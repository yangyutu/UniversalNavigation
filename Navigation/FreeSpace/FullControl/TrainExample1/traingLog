/home/yangyutu/anaconda3/bin/python /home/yangyutu/Dropbox/UniversalNavigationProject/activeParticleModel/Navigation/FreeSpace/FullControl/DDPGHER_MLP.py
episode index:0
target Thresh 6.399999999999999
target distance 3.0
model initialize at round 0
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([15.01231737,  6.50704871,  4.69258662]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 3.339932791500753}
done in step count: 99
reward sum = -0.6223509573711064
running average episode reward sum: -0.6223509573711064
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([13.72191463, 15.46100089,  0.15966036]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 8.600497003335741}
episode index:1
target Thresh 6.6547242560212965
target distance 4.0
model initialize at round 1
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 19.]), 'previousTarget': array([21., 19.]), 'currentState': array([24.54069266, 23.1963088 ,  2.65944907]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 5.490492878274882}
done in step count: 99
reward sum = -0.7741496099714698
running average episode reward sum: -0.698250283671288
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 19.]), 'previousTarget': array([21., 19.]), 'currentState': array([29.3439844 , 24.20762522,  3.99268557]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 9.835722453439102}
episode index:2
target Thresh 6.9069139633470655
target distance 5.0
model initialize at round 2
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  6.]), 'previousTarget': array([11.,  6.]), 'currentState': array([6.        , 7.        , 2.46369511]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 5.0990195135927845}
done in step count: 99
reward sum = -0.7221822805730843
running average episode reward sum: -0.7062276159718869
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  6.]), 'previousTarget': array([11.,  6.]), 'currentState': array([5.79039748, 9.6466888 , 5.46121566]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 6.3591113858961075}
episode index:3
target Thresh 7.15659434115819
target distance 4.0
model initialize at round 3
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([24.4454096 , 12.85254853,  5.96344835]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 4.936923572191084}
done in step count: 99
reward sum = -0.8263453011482498
running average episode reward sum: -0.7362570372659776
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([26.65629469, 14.61755256,  3.7425509 ]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 6.6672726882798665}
episode index:4
target Thresh 7.403790357700526
target distance 7.0
model initialize at round 4
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([13.37805869, 11.31922878,  0.44872785]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 7.496786218685331}
done in step count: 99
reward sum = -1.0321808009903244
running average episode reward sum: -0.7954417900108469
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([13.27039176,  4.80152111,  3.15375784]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.9063002832114444}
episode index:5
target Thresh 7.648526732781718
target distance 2.0
model initialize at round 5
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 16.]), 'previousTarget': array([26., 16.]), 'currentState': array([23.87322683, 17.62216735,  4.13616514]), 'targetState': array([26, 16], dtype=int32), 'currentDistance': 2.674806732413731}
done in step count: 98
reward sum = -0.4231520347198584
running average episode reward sum: -0.7333934974623487
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 16.]), 'previousTarget': array([26., 16.]), 'currentState': array([25.10285666, 16.84464681,  5.16480713]), 'targetState': array([26, 16], dtype=int32), 'currentDistance': 1.2321908977793092}
episode index:6
target Thresh 7.890827940243231
target distance 7.0
model initialize at round 6
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 3.65379759, 17.33757212,  2.11631536]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 7.345734866731115}
done in step count: 99
reward sum = -0.9863618063127747
running average episode reward sum: -0.7695318272981239
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 2.62176466, 18.09164463,  3.07347455]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 8.20818161478754}
episode index:7
target Thresh 8.130718210407721
target distance 6.0
model initialize at round 7
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 22.]), 'previousTarget': array([ 8., 22.]), 'currentState': array([ 2.40206329, 17.71158971,  5.40842581]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 7.051762778403239}
done in step count: 99
reward sum = -0.9837016972992151
running average episode reward sum: -0.7963030610482602
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 22.]), 'previousTarget': array([ 8., 22.]), 'currentState': array([ 5.77673889, 13.16910124,  3.16222363]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 9.106462699938827}
episode index:8
target Thresh 8.368221532502123
target distance 7.0
model initialize at round 8
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([ 2.17050187, 11.83376159,  5.25794744]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 6.8839207327256675}
done in step count: 99
reward sum = -0.9125939948943077
running average episode reward sum: -0.8092242759200432
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([ 1.45013966, 14.75041487,  6.11580354]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 9.872824177425816}
episode index:9
target Thresh 8.603361657056556
target distance 1.0
model initialize at round 9
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 15.]), 'previousTarget': array([27., 15.]), 'currentState': array([27.07462075, 16.49373062,  1.51085517]), 'targetState': array([27, 15], dtype=int32), 'currentDistance': 1.4955933361443323}
done in step count: 99
reward sum = -0.9383463381486405
running average episode reward sum: -0.8221364821429029
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 15.]), 'previousTarget': array([27., 15.]), 'currentState': array([27.27386858, 23.99638064,  5.38546565]), 'targetState': array([27, 15], dtype=int32), 'currentDistance': 9.000548234924096}
episode index:10
target Thresh 8.836162098279434
target distance 4.0
model initialize at round 10
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 28.]), 'previousTarget': array([22., 28.]), 'currentState': array([26.22680583, 26.31918644,  0.73459095]), 'targetState': array([22, 28], dtype=int32), 'currentDistance': 4.548738474638739}
done in step count: 99
reward sum = -0.8834971073712257
running average episode reward sum: -0.8277147208000231
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 28.]), 'previousTarget': array([22., 28.]), 'currentState': array([23.86935476, 21.66844159,  3.73254862]), 'targetState': array([22, 28], dtype=int32), 'currentDistance': 6.601751212463764}
episode index:11
target Thresh 9.066646136408878
target distance 4.0
model initialize at round 11
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 25.]), 'previousTarget': array([26., 25.]), 'currentState': array([22.92220385, 21.03676679,  2.44760084]), 'targetState': array([26, 25], dtype=int32), 'currentDistance': 5.017972359669892}
done in step count: 99
reward sum = -0.9369253020117548
running average episode reward sum: -0.8368156025676674
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 25.]), 'previousTarget': array([26., 25.]), 'currentState': array([22.56524177, 16.62150593,  4.2499223 ]), 'targetState': array([26, 25], dtype=int32), 'currentDistance': 9.055204412152884}
episode index:12
target Thresh 9.294836820040768
target distance 8.0
model initialize at round 12
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([17.00976066,  2.57513286,  4.64848906]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 9.322038203651653}
done in step count: 99
reward sum = -0.8760851862365677
running average episode reward sum: -0.8398363397729673
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([16.37171563,  2.2369925 ,  3.6681075 ]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 9.910162289355247}
episode index:13
target Thresh 9.52075696843363
target distance 7.0
model initialize at round 13
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 18.]), 'previousTarget': array([24., 18.]), 'currentState': array([22.07020716, 24.75067385,  4.73436928]), 'targetState': array([24, 18], dtype=int32), 'currentDistance': 7.021089500668649}
done in step count: 99
reward sum = -1.0352420847781236
running average episode reward sum: -0.8537938929876214
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 18.]), 'previousTarget': array([24., 18.]), 'currentState': array([15.4906167 , 18.38211183,  4.01652276]), 'targetState': array([24, 18], dtype=int32), 'currentDistance': 8.517958295192054}
episode index:14
target Thresh 9.744429173790568
target distance 7.0
model initialize at round 14
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 19.]), 'previousTarget': array([12., 19.]), 'currentState': array([11.85631043, 25.7139604 ,  4.06910917]), 'targetState': array([12, 19], dtype=int32), 'currentDistance': 6.715497817507122}
done in step count: 99
reward sum = -0.8440150686027983
running average episode reward sum: -0.8531419713619666
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 19.]), 'previousTarget': array([12., 19.]), 'currentState': array([16.38522078, 22.02991641,  3.74208399]), 'targetState': array([12, 19], dtype=int32), 'currentDistance': 5.330155229023471}
episode index:15
target Thresh 9.965875803518518
target distance 5.0
model initialize at round 15
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  2.]), 'previousTarget': array([19.,  2.]), 'currentState': array([20.03482095,  6.86586145,  4.71387243]), 'targetState': array([19,  2], dtype=int32), 'currentDistance': 4.974682101593517}
done in step count: 99
reward sum = -0.8460099800579811
running average episode reward sum: -0.8526962219054676
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  2.]), 'previousTarget': array([19.,  2.]), 'currentState': array([2.05467296e+01, 8.04068086e+00, 3.33048501e-03]), 'targetState': array([19,  2], dtype=int32), 'currentDistance': 6.235559144024574}
episode index:16
target Thresh 10.185119002464987
target distance 7.0
model initialize at round 16
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 25.]), 'previousTarget': array([ 5., 25.]), 'currentState': array([12.49158767, 18.93895555,  5.92105575]), 'targetState': array([ 5, 25], dtype=int32), 'currentDistance': 9.636396918567783}
done in step count: 99
reward sum = -0.8837662896110271
running average episode reward sum: -0.8545238729469711
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 25.]), 'previousTarget': array([ 5., 25.]), 'currentState': array([11.45104504, 19.71415727,  5.74395159]), 'targetState': array([ 5, 25], dtype=int32), 'currentDistance': 8.340030904241386}
episode index:17
target Thresh 10.402180695132575
target distance 7.0
model initialize at round 17
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 23.]), 'previousTarget': array([23., 23.]), 'currentState': array([16.13884399, 15.82832946,  5.13996172]), 'targetState': array([23, 23], dtype=int32), 'currentDistance': 9.925135774356592}
done in step count: 99
reward sum = -0.7865591284333109
running average episode reward sum: -0.8507480538073232
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 23.]), 'previousTarget': array([23., 23.]), 'currentState': array([19.87391766, 18.37956706,  5.18204269]), 'targetState': array([23, 23], dtype=int32), 'currentDistance': 5.578601198486171}
episode index:18
target Thresh 10.617082587871437
target distance 8.0
model initialize at round 18
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 18.]), 'previousTarget': array([17., 18.]), 'currentState': array([24.40164344, 10.09838156,  6.27090305]), 'targetState': array([17, 18], dtype=int32), 'currentDistance': 10.826813917400562}
done in step count: 99
reward sum = -1.0525019192282248
running average episode reward sum: -0.86136667830316
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 18.]), 'previousTarget': array([17., 18.]), 'currentState': array([23.73308098,  9.032508  ,  0.98785052]), 'targetState': array([17, 18], dtype=int32), 'currentDistance': 11.213843773217814}
episode index:19
target Thresh 10.829846171049923
target distance 10.0
model initialize at round 19
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([12.66204152, 13.75733802,  3.55963475]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 10.421909291851195}
done in step count: 99
reward sum = -1.0589304323791142
running average episode reward sum: -0.8712448660069578
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([11.74520933, 18.08845888,  0.32718738]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 14.353426346638289}
episode index:20
target Thresh 11.040492721203663
target distance 6.0
model initialize at round 20
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([9.81170218, 3.9504244 , 3.25587797]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 6.563215184271191}
done in step count: 99
reward sum = -0.7853981796657118
running average episode reward sum: -0.8671569285621366
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([8.22697352, 2.80396302, 3.61725413]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 5.956008016368356}
episode index:21
target Thresh 11.249043303163209
target distance 8.0
model initialize at round 21
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 27.]), 'previousTarget': array([10., 27.]), 'currentState': array([ 4.61748136, 18.6861292 ,  3.57623148]), 'targetState': array([10, 27], dtype=int32), 'currentDistance': 9.90413825385315}
done in step count: 99
reward sum = -0.9261045880383633
running average episode reward sum: -0.8698363676292378
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 27.]), 'previousTarget': array([10., 27.]), 'currentState': array([ 2.6960468 , 18.45431208,  1.87977561]), 'targetState': array([10, 27], dtype=int32), 'currentDistance': 11.241730930990343}
episode index:22
target Thresh 11.455518772160548
target distance 11.0
model initialize at round 22
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([4.47020959, 5.16427793, 0.18273631]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 10.593961916130558}
done in step count: 99
reward sum = -0.7912185424602064
running average episode reward sum: -0.8664182013175408
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([10.07452706, 15.77661915,  0.37633923]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 12.76514951866009}
episode index:23
target Thresh 11.659939775914648
target distance 4.0
model initialize at round 23
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 22.]), 'previousTarget': array([20., 22.]), 'currentState': array([22.84363899, 17.89010542,  3.87519771]), 'targetState': array([20, 22], dtype=int32), 'currentDistance': 4.9977511066837845}
done in step count: 99
reward sum = -0.27222410878702236
running average episode reward sum: -0.8416601141287692
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 22.]), 'previousTarget': array([20., 22.]), 'currentState': array([23.07426454, 22.2623138 ,  4.42929194]), 'targetState': array([20, 22], dtype=int32), 'currentDistance': 3.085435295928847}
episode index:24
target Thresh 11.86232675669623
target distance 5.0
model initialize at round 24
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 28.]), 'previousTarget': array([ 5., 28.]), 'currentState': array([ 8.75796685, 23.2460209 ,  2.60052395]), 'targetState': array([ 5, 28], dtype=int32), 'currentDistance': 6.059920142052769}
done in step count: 37
reward sum = 0.4395361212977968
running average episode reward sum: -0.7904122647117066
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 28.]), 'previousTarget': array([ 5., 28.]), 'currentState': array([ 5.69031178, 28.99202874,  5.52394244]), 'targetState': array([ 5, 28], dtype=int32), 'currentDistance': 1.2085741076965915}
episode index:25
target Thresh 12.062699953372032
target distance 7.0
model initialize at round 25
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 16.]), 'previousTarget': array([12., 16.]), 'currentState': array([17.52845337,  9.15655826,  3.01738206]), 'targetState': array([12, 16], dtype=int32), 'currentDistance': 8.797527581992671}
done in step count: 40
reward sum = 0.37942470083536883
running average episode reward sum: -0.7454185352675883
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 16.]), 'previousTarget': array([12., 16.]), 'currentState': array([11.14225268, 15.80045387,  6.22435365]), 'targetState': array([12, 16], dtype=int32), 'currentDistance': 0.8806526700762625}
episode index:26
target Thresh 12.261079403428703
target distance 8.0
model initialize at round 26
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 15.]), 'previousTarget': array([25., 15.]), 'currentState': array([19.        , 23.        ,  0.90524483]), 'targetState': array([25, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 50
reward sum = 0.2967219254467697
running average episode reward sum: -0.7068207404263157
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 15.]), 'previousTarget': array([25., 15.]), 'currentState': array([25.95581206, 14.73030349,  3.3523369 ]), 'targetState': array([25, 15], dtype=int32), 'currentDistance': 0.9931328706184706}
episode index:27
target Thresh 12.457484944976557
target distance 12.0
model initialize at round 27
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([19.        , 17.        ,  5.61748317]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 13.0}
done in step count: 30
reward sum = 0.4513679219727637
running average episode reward sum: -0.6654568596263486
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.89758143,  5.14624377,  3.71529528]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.9094172107696145}
episode index:28
target Thresh 12.651936218733425
target distance 9.0
model initialize at round 28
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 19.]), 'previousTarget': array([14., 19.]), 'currentState': array([10.37770296, 27.68035041,  5.32834387]), 'targetState': array([14, 19], dtype=int32), 'currentDistance': 9.405823680104215}
done in step count: 22
reward sum = 0.6160900470930157
running average episode reward sum: -0.6212655869808532
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 19.]), 'previousTarget': array([14., 19.]), 'currentState': array([13.81649059, 19.97716607,  4.6363823 ]), 'targetState': array([14, 19], dtype=int32), 'currentDistance': 0.9942480787550312}
episode index:29
target Thresh 12.84445266998873
target distance 2.0
model initialize at round 29
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([14.        ,  8.        ,  1.25265169]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 2.0}
done in step count: 24
reward sum = 0.6001967654011644
running average episode reward sum: -0.5805501752347859
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.17364637,  7.33217291,  0.35321843]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.06247510472466}
episode index:30
target Thresh 13.035053550548021
target distance 3.0
model initialize at round 30
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([2.99100883, 6.97176424, 4.15160894]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 3.6287419525099884}
done in step count: 14
reward sum = 0.7487574021113816
running average episode reward sum: -0.5376692856429741
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([5.33854532, 8.64875247, 0.496149  ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.7489306485411704}
episode index:31
target Thresh 13.223757920658194
target distance 9.0
model initialize at round 31
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 12.]), 'previousTarget': array([11., 12.]), 'currentState': array([ 2.44160564, 12.77679641,  6.067698  ]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 8.593574729393016}
done in step count: 17
reward sum = 0.6644238600351251
running average episode reward sum: -0.5001038748405335
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 12.]), 'previousTarget': array([11., 12.]), 'currentState': array([10.01765753, 11.27589943,  0.33323533]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 1.2203763169710837}
episode index:32
target Thresh 13.41058465091351
target distance 10.0
model initialize at round 32
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 25.]), 'previousTarget': array([ 9., 25.]), 'currentState': array([19.       , 27.       ,  1.5079107]), 'targetState': array([ 9, 25], dtype=int32), 'currentDistance': 10.198039027185569}
done in step count: 24
reward sum = 0.5800070066789413
running average episode reward sum: -0.4673732420672161
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 25.]), 'previousTarget': array([ 9., 25.]), 'currentState': array([ 9.64163828, 25.83790792,  3.81311744]), 'targetState': array([ 9, 25], dtype=int32), 'currentDistance': 1.0553621922602425}
episode index:33
target Thresh 13.595552424142689
target distance 8.0
model initialize at round 33
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 17.]), 'previousTarget': array([24., 17.]), 'currentState': array([16.05042598, 19.15741414,  1.08431667]), 'targetState': array([24, 17], dtype=int32), 'currentDistance': 8.237121034123865}
done in step count: 16
reward sum = 0.6659226950151209
running average episode reward sum: -0.43404100862361794
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 17.]), 'previousTarget': array([24., 17.]), 'currentState': array([23.08403249, 17.50885078,  6.07259762]), 'targetState': array([24, 17], dtype=int32), 'currentDistance': 1.04781945392368}
episode index:34
target Thresh 13.77867973727719
target distance 11.0
model initialize at round 34
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 26.]), 'previousTarget': array([20., 26.]), 'currentState': array([ 8.7823665 , 15.05842384,  2.68232882]), 'targetState': array([20, 26], dtype=int32), 'currentDistance': 15.670143277260015}
done in step count: 39
reward sum = 0.28465196036212553
running average episode reward sum: -0.41350692379545384
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 26.]), 'previousTarget': array([20., 26.]), 'currentState': array([20.59268131, 25.0731172 ,  1.82346331]), 'targetState': array([20, 26], dtype=int32), 'currentDistance': 1.1001740115233716}
episode index:35
target Thresh 13.959984903200937
target distance 11.0
model initialize at round 35
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 29.]), 'previousTarget': array([23., 29.]), 'currentState': array([17.4615058 , 18.17845946,  0.62147943]), 'targetState': array([23, 29], dtype=int32), 'currentDistance': 12.156506801978317}
done in step count: 52
reward sum = 0.3295367279697396
running average episode reward sum: -0.3928668223575318
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 29.]), 'previousTarget': array([23., 29.]), 'currentState': array([23.14122518, 28.13239198,  2.68570912]), 'targetState': array([23, 29], dtype=int32), 'currentDistance': 0.8790268657109154}
episode index:36
target Thresh 14.139486052581606
target distance 3.0
model initialize at round 36
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 3.40753718, 12.97854551,  6.06248951]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 3.036897357136327}
done in step count: 5
reward sum = 0.8627306412369786
running average episode reward sum: -0.35893175577389635
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 3.94364845, 10.95132629,  4.39971983]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.9529938177032099}
episode index:37
target Thresh 14.317201135683721
target distance 13.0
model initialize at round 37
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([ 4.94008488, 20.99999871,  3.08191438]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 18.427190492399333}
done in step count: 55
reward sum = 0.19955031700833242
running average episode reward sum: -0.3442348591217324
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([17.16082542,  7.61030419,  6.02225687]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 0.9252441881283917}
episode index:38
target Thresh 14.49314792416369
target distance 5.0
model initialize at round 38
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 16.]), 'previousTarget': array([12., 16.]), 'currentState': array([10.78269672, 21.4460371 ,  1.80692947]), 'targetState': array([12, 16], dtype=int32), 'currentDistance': 5.58042537917989}
done in step count: 31
reward sum = 0.5026113253132131
running average episode reward sum: -0.32252085439263123
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 16.]), 'previousTarget': array([12., 16.]), 'currentState': array([12.19864023, 16.98968264,  5.03841668]), 'targetState': array([12, 16], dtype=int32), 'currentDistance': 1.0094204596919931}
episode index:39
target Thresh 14.667344012846986
target distance 14.0
model initialize at round 39
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  6.]), 'previousTarget': array([27.,  6.]), 'currentState': array([19.49616101, 19.95659965,  6.04155717]), 'targetState': array([27,  6], dtype=int32), 'currentDistance': 15.845954481569102}
done in step count: 41
reward sum = 0.3126857203990872
running average episode reward sum: -0.3066406900228382
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  6.]), 'previousTarget': array([27.,  6.]), 'currentState': array([26.21507319,  5.9610126 ,  5.65033075]), 'targetState': array([27,  6], dtype=int32), 'currentDistance': 0.7858944681910478}
episode index:40
target Thresh 14.839806821487631
target distance 12.0
model initialize at round 40
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 15.]), 'previousTarget': array([13., 15.]), 'currentState': array([25.02270921,  9.84804327,  4.60823631]), 'targetState': array([13, 15], dtype=int32), 'currentDistance': 13.08006861120388}
done in step count: 37
reward sum = 0.4192341755543638
running average episode reward sum: -0.28893642500876016
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 15.]), 'previousTarget': array([13., 15.]), 'currentState': array([13.94223671, 15.45785323,  3.11341219]), 'targetState': array([13, 15], dtype=int32), 'currentDistance': 1.0475875175514628}
episode index:41
target Thresh 15.010553596510224
target distance 7.0
model initialize at round 41
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 19.]), 'previousTarget': array([ 2., 19.]), 'currentState': array([ 8.97244817, 22.0480005 ,  2.24626929]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 7.609555866463188}
done in step count: 27
reward sum = 0.5863054137073551
running average episode reward sum: -0.2680973336107574
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 19.]), 'previousTarget': array([ 2., 19.]), 'currentState': array([ 2.01131911, 19.99582889,  4.03067883]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 0.995893212834962}
episode index:42
target Thresh 15.179601412734545
target distance 5.0
model initialize at round 42
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 9.09573792, 12.97565095,  5.78163528]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 5.465321731628367}
done in step count: 26
reward sum = 0.6588252585767219
running average episode reward sum: -0.2465409942575602
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.92358881, 11.51690557,  3.39184359]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.0583986307196067}
episode index:43
target Thresh 15.346967175083098
target distance 6.0
model initialize at round 43
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 17.]), 'previousTarget': array([23., 17.]), 'currentState': array([19.        , 11.        ,  3.35262278]), 'targetState': array([23, 17], dtype=int32), 'currentDistance': 7.211102550927979}
done in step count: 21
reward sum = 0.6388686896549546
running average episode reward sum: -0.22641804689591216
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 17.]), 'previousTarget': array([23., 17.]), 'currentState': array([22.30008476, 16.15418809,  1.02249419]), 'targetState': array([23, 17], dtype=int32), 'currentDistance': 1.09785205600428}
episode index:44
target Thresh 15.512667620271579
target distance 2.0
model initialize at round 44
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 6.]), 'previousTarget': array([8., 6.]), 'currentState': array([10.        ,  8.        ,  6.21989549]), 'targetState': array([8, 6], dtype=int32), 'currentDistance': 2.8284271247461903}
done in step count: 11
reward sum = 0.810444190670327
running average episode reward sum: -0.20337666383888459
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 6.]), 'previousTarget': array([8., 6.]), 'currentState': array([8.88829321, 6.08304209, 3.45873716]), 'targetState': array([8, 6], dtype=int32), 'currentDistance': 0.8921663589736041}
episode index:45
target Thresh 15.6767193184826
target distance 7.0
model initialize at round 45
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  4.]), 'previousTarget': array([22.,  4.]), 'currentState': array([15.        ,  8.        ,  4.41493464]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 8.062257748298551}
done in step count: 19
reward sum = 0.6346592133164995
running average episode reward sum: -0.18515849259637623
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  4.]), 'previousTarget': array([22.,  4.]), 'currentState': array([21.10610259,  3.6751355 ,  5.92839409]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 0.9510991123224797}
episode index:46
target Thresh 15.839138675022696
target distance 14.0
model initialize at round 46
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 22.]), 'previousTarget': array([25., 22.]), 'currentState': array([19.22458126,  8.27555515,  0.70700663]), 'targetState': array([25, 22], dtype=int32), 'currentDistance': 14.890125858694192}
done in step count: 46
reward sum = 0.33590705245058156
running average episode reward sum: -0.17407199163793033
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 22.]), 'previousTarget': array([25., 22.]), 'currentState': array([25.26136491, 21.0331254 ,  1.93271058]), 'targetState': array([25, 22], dtype=int32), 'currentDistance': 1.0015778136299365}
episode index:47
target Thresh 15.999941931962859
target distance 12.0
model initialize at round 47
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 7.]), 'previousTarget': array([6., 7.]), 'currentState': array([18.        , 12.        ,  5.04631633]), 'targetState': array([6, 7], dtype=int32), 'currentDistance': 13.000000000000002}
done in step count: 33
reward sum = 0.4689200112068548
running average episode reward sum: -0.16067632491199732
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 7.]), 'previousTarget': array([6., 7.]), 'currentState': array([6.74218387, 6.81001687, 3.7314912 ]), 'targetState': array([6, 7], dtype=int32), 'currentDistance': 0.7661138819424906}
episode index:48
target Thresh 16.159145169762795
target distance 11.0
model initialize at round 48
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([4.80755996, 4.02794212, 2.79357657]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 11.234572112280915}
done in step count: 27
reward sum = 0.4911636534853601
running average episode reward sum: -0.14737346821001043
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.18360598,  4.63560266,  6.25131373]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.8940271848442362}
episode index:49
target Thresh 16.316764308878945
target distance 7.0
model initialize at round 49
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'previousTarget': array([27.,  8.]), 'currentState': array([20.43987594, 11.22659335,  0.22317833]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 7.310686178648416}
done in step count: 23
reward sum = 0.637065782330448
running average episode reward sum: -0.13168468319920126
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'previousTarget': array([27.,  8.]), 'currentState': array([26.00647843,  8.48388502,  5.86190081]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 1.105092674411666}
episode index:50
target Thresh 16.47281511135658
target distance 12.0
model initialize at round 50
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 29.]), 'previousTarget': array([ 9., 29.]), 'currentState': array([20.68361196, 24.224488  ,  2.72982082]), 'targetState': array([ 9, 29], dtype=int32), 'currentDistance': 12.621897767682158}
done in step count: 26
reward sum = 0.5071441154752949
running average episode reward sum: -0.11915862832323075
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 29.]), 'previousTarget': array([ 9., 29.]), 'currentState': array([ 9.95578636, 28.9844043 ,  3.02238949]), 'targetState': array([ 9, 29], dtype=int32), 'currentDistance': 0.955913590220586}
episode index:51
target Thresh 16.62731318240599
target distance 7.0
model initialize at round 51
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 26.]), 'previousTarget': array([11., 26.]), 'currentState': array([16.17167679, 19.26369152,  1.13463527]), 'targetState': array([11, 26], dtype=int32), 'currentDistance': 8.492590465269172}
done in step count: 25
reward sum = 0.6217445091095188
running average episode reward sum: -0.10491049106490864
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 26.]), 'previousTarget': array([11., 26.]), 'currentState': array([11.83548382, 25.45667764,  2.52188157]), 'targetState': array([11, 26], dtype=int32), 'currentDistance': 0.996610454626376}
episode index:52
target Thresh 16.780273971963023
target distance 13.0
model initialize at round 52
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 15.]), 'previousTarget': array([26., 15.]), 'currentState': array([23.46168361, 27.82200104,  5.66270351]), 'targetState': array([26, 15], dtype=int32), 'currentDistance': 13.070836272558692}
done in step count: 27
reward sum = 0.48483421163810336
running average episode reward sum: -0.09378323252334238
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 15.]), 'previousTarget': array([26., 15.]), 'currentState': array([25.18890397, 15.97993948,  5.1779731 ]), 'targetState': array([26, 15], dtype=int32), 'currentDistance': 1.2720684562199915}
episode index:53
target Thresh 16.931712776234107
target distance 11.0
model initialize at round 53
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 13.]), 'previousTarget': array([ 2., 13.]), 'currentState': array([ 5.78172123, 23.63130118,  3.97870693]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 11.283881431647085}
done in step count: 24
reward sum = 0.5416975149850036
running average episode reward sum: -0.08201507053244708
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 13.]), 'previousTarget': array([ 2., 13.]), 'currentState': array([ 1.65298595, 13.78024802,  4.86386536]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 0.8539354360051137}
episode index:54
target Thresh 17.081644739225865
target distance 2.0
model initialize at round 54
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'previousTarget': array([7., 3.]), 'currentState': array([4.92726968, 2.06467081, 2.16228414]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 2.273994648690961}
done in step count: 8
reward sum = 0.8616805385944063
running average episode reward sum: -0.06485696854832247
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'previousTarget': array([7., 3.]), 'currentState': array([6.09296266, 2.69737645, 6.27331894]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 0.9561891828105111}
episode index:55
target Thresh 17.23008485425954
target distance 8.0
model initialize at round 55
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([14.        , 20.        ,  3.47927323]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 10.63014581273465}
done in step count: 33
reward sum = 0.4581429570563725
running average episode reward sum: -0.055517684162524346
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([20.90718493, 12.89825136,  4.3368809 ]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 0.9030338564016636}
episode index:56
target Thresh 17.37704796547034
target distance 9.0
model initialize at round 56
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([ 3.93821295, 15.50906442,  4.33469152]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 8.509288741614098}
done in step count: 20
reward sum = 0.6397135217077162
running average episode reward sum: -0.043320645463046446
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.05074407, 7.58071502, 5.29303986]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.1127968181943013}
episode index:57
target Thresh 17.522548769291852
target distance 11.0
model initialize at round 57
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([22.004082  ,  8.360129  ,  1.75080618]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 11.836407634076426}
done in step count: 29
reward sum = 0.4566045240509678
running average episode reward sum: -0.03470124598866688
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([11.55098573,  4.91806868,  4.15687175]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 1.0707172232475366}
episode index:58
target Thresh 17.66660181592571
target distance 8.0
model initialize at round 58
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([16.06542503, 12.93188025,  5.59557661]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 8.068385010323922}
done in step count: 44
reward sum = 0.5005917373809651
running average episode reward sum: -0.025628483558673122
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([16.2240167 , 20.0442628 ,  1.90212566]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 0.9816399930167777}
episode index:59
target Thresh 17.80922151079662
target distance 7.0
model initialize at round 59
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  2.]), 'previousTarget': array([13.,  2.]), 'currentState': array([19.76280993,  7.56574632,  3.95996666]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 8.75860321523438}
done in step count: 20
reward sum = 0.628651781547138
running average episode reward sum: -0.01472381247357627
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  2.]), 'previousTarget': array([13.,  2.]), 'currentState': array([13.85446101,  2.02785543,  3.96685959]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 0.8549149363057422}
episode index:60
target Thresh 17.95042211599292
target distance 4.0
model initialize at round 60
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 2.]), 'previousTarget': array([8., 2.]), 'currentState': array([8.97978434, 5.5056047 , 4.41902208]), 'targetState': array([8, 2], dtype=int32), 'currentDistance': 3.63995077784158}
done in step count: 13
reward sum = 0.8235644177215893
running average episode reward sum: -0.0009813824703768345
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 2.]), 'previousTarget': array([8., 2.]), 'currentState': array([8.67213641, 2.91329601, 4.92812181]), 'targetState': array([8, 2], dtype=int32), 'currentDistance': 1.1339651477920627}
episode index:61
target Thresh 18.090217751692805
target distance 7.0
model initialize at round 61
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 16.]), 'previousTarget': array([19., 16.]), 'currentState': array([11.95421441, 20.9174317 ,  4.45858383]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 8.592102718861595}
done in step count: 18
reward sum = 0.6362860582322375
running average episode reward sum: 0.009297124637729848
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 16.]), 'previousTarget': array([19., 16.]), 'currentState': array([18.08727983, 16.6050598 ,  6.01775555]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 1.095059571022853}
episode index:62
target Thresh 18.22862239757633
target distance 11.0
model initialize at round 62
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 23.]), 'previousTarget': array([11., 23.]), 'currentState': array([ 2.        , 12.        ,  5.48202157]), 'targetState': array([11, 23], dtype=int32), 'currentDistance': 14.212670403551893}
done in step count: 42
reward sum = 0.41248868261061944
running average episode reward sum: 0.015696990637299523
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 23.]), 'previousTarget': array([11., 23.]), 'currentState': array([10.6509712 , 22.16724604,  1.45601914]), 'targetState': array([11, 23], dtype=int32), 'currentDistance': 0.9029397899683621}
episode index:63
target Thresh 18.36564989422343
target distance 16.0
model initialize at round 63
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([ 5.97231118, 24.12652415,  1.53374195]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 18.989979571931}
done in step count: 55
reward sum = 0.15774938561884505
running average episode reward sum: 0.017916559308886172
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.20257175,  8.99986414,  5.59327381]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.2789136449714462}
episode index:64
target Thresh 18.50131394449796
target distance 10.0
model initialize at round 64
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 13.]), 'previousTarget': array([ 9., 13.]), 'currentState': array([19.00802979, 15.99535062,  5.50583744]), 'targetState': array([ 9, 13], dtype=int32), 'currentDistance': 10.446663850513387}
done in step count: 35
reward sum = 0.4451798759332694
running average episode reward sum: 0.024489841103107455
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 13.]), 'previousTarget': array([ 9., 13.]), 'currentState': array([ 9.99495066, 12.91905896,  3.71538904]), 'targetState': array([ 9, 13], dtype=int32), 'currentDistance': 0.998237579219736}
episode index:65
target Thresh 18.63562811491799
target distance 3.0
model initialize at round 65
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 27.]), 'previousTarget': array([ 6., 27.]), 'currentState': array([ 3.02522966, 26.91633786,  5.22112286]), 'targetState': array([ 6, 27], dtype=int32), 'currentDistance': 2.975946561085634}
done in step count: 7
reward sum = 0.8802654576021055
running average episode reward sum: 0.03745613832278925
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 27.]), 'previousTarget': array([ 6., 27.]), 'currentState': array([ 5.02271227, 26.74956298,  0.34277771]), 'targetState': array([ 6, 27], dtype=int32), 'currentDistance': 1.0088657024643932}
episode index:66
target Thresh 18.7686058370125
target distance 16.0
model initialize at round 66
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([ 3.39636082, 27.82561842,  5.61621475]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 18.43953179965649}
done in step count: 40
reward sum = 0.30526888062638086
running average episode reward sum: 0.04145334343179807
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([18.01805942, 18.54305232,  0.42737298]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 1.1221021056127152}
episode index:67
target Thresh 18.900260408664515
target distance 10.0
model initialize at round 67
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([13.        , 15.        ,  1.40224883]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 13.45362404707371}
done in step count: 98
reward sum = 0.04454205200956807
running average episode reward sum: 0.04149876561676528
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.87347153, 5.47219561, 4.29124995]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 1.0205537652000853}
episode index:68
target Thresh 19.030604995440907
target distance 18.0
model initialize at round 68
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'previousTarget': array([22., 18.]), 'currentState': array([ 4.22147814, 19.55681397,  5.41227978]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 17.846554552224266}
done in step count: 95
reward sum = 0.06878793505883174
running average episode reward sum: 0.04189426082607058
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'previousTarget': array([22., 18.]), 'currentState': array([21.10266409, 18.77982377,  0.85326698]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 1.1888384439849329}
episode index:69
target Thresh 19.159652631908976
target distance 8.0
model initialize at round 69
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 22.]), 'previousTarget': array([24., 22.]), 'currentState': array([16.1803735 , 21.73919377,  5.55995592]), 'targetState': array([24, 22], dtype=int32), 'currentDistance': 7.823974599739073}
done in step count: 29
reward sum = 0.5932099454862174
running average episode reward sum: 0.04977019917835839
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 22.]), 'previousTarget': array([24., 22.]), 'currentState': array([23.17763439, 22.5440776 ,  0.22073743]), 'targetState': array([24, 22], dtype=int32), 'currentDistance': 0.9860555990036073}
episode index:70
target Thresh 19.287416222939918
target distance 5.0
model initialize at round 70
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  8.]), 'previousTarget': array([21.,  8.]), 'currentState': array([26.29432239,  4.11654356,  0.62952954]), 'targetState': array([21,  8], dtype=int32), 'currentDistance': 6.565903101994822}
done in step count: 35
reward sum = 0.5421513818351023
running average episode reward sum: 0.056705145412960414
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  8.]), 'previousTarget': array([21.,  8.]), 'currentState': array([21.9140307 ,  7.106125  ,  3.27292883]), 'targetState': array([21,  8], dtype=int32), 'currentDistance': 1.2784618193346322}
episode index:71
target Thresh 19.413908544999295
target distance 8.0
model initialize at round 71
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  4.]), 'previousTarget': array([10.,  4.]), 'currentState': array([1.98665173, 7.79846512, 4.8036105 ]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 8.868037418710461}
done in step count: 55
reward sum = 0.3934054392699503
running average episode reward sum: 0.06138153838319639
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  4.]), 'previousTarget': array([10.,  4.]), 'currentState': array([10.59279475,  4.93252683,  5.21021404]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 1.1049940783557686}
episode index:72
target Thresh 19.539142247424724
target distance 12.0
model initialize at round 72
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 17.]), 'previousTarget': array([26., 17.]), 'currentState': array([13.81375117, 24.84165999,  4.00977466]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 14.491248805557086}
done in step count: 99
reward sum = -0.357545007538935
running average episode reward sum: 0.055642818576043904
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 17.]), 'previousTarget': array([26., 17.]), 'currentState': array([27.481318  , 17.16006501,  0.63722969]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 1.4899408777262735}
episode index:73
target Thresh 19.66312985369082
target distance 17.0
model initialize at round 73
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([10.70106817, 23.60569663,  4.31620717]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 17.43065802386056}
done in step count: 82
reward sum = 0.13519241891414974
running average episode reward sum: 0.0567178131752075
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.96432559,  6.41229297,  3.8803916 ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.129302172083815}
episode index:74
target Thresh 19.78588376266152
target distance 19.0
model initialize at round 74
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  8.]), 'previousTarget': array([25.,  8.]), 'currentState': array([6.        , 6.        , 2.20592666]), 'targetState': array([25,  8], dtype=int32), 'currentDistance': 19.1049731745428}
done in step count: 99
reward sum = -0.31618283147400905
running average episode reward sum: 0.051745804579884605
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  8.]), 'previousTarget': array([25.,  8.]), 'currentState': array([24.04703566,  9.33773295,  5.79310173]), 'targetState': array([25,  8], dtype=int32), 'currentDistance': 1.6424586729840618}
episode index:75
target Thresh 19.90741624983002
target distance 18.0
model initialize at round 75
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.89113454,  7.33642942]), 'previousTarget': array([11.90599608,  7.35899411]), 'currentState': array([22.98670375, 23.97639167,  4.08167773]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 79
reward sum = 0.05128567445664739
running average episode reward sum: 0.051739750236157796
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  6.]), 'previousTarget': array([11.,  6.]), 'currentState': array([10.08411153,  5.03038331,  2.07994977]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 1.3337946686164184}
episode index:76
target Thresh 20.027739468546322
target distance 4.0
model initialize at round 76
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 14.]), 'previousTarget': array([ 2., 14.]), 'currentState': array([2.19382441e+00, 1.80465668e+01, 1.70400739e-02]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 4.051206089836774}
done in step count: 12
reward sum = 0.8059475840244021
running average episode reward sum: 0.06153465716847266
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 14.]), 'previousTarget': array([ 2., 14.]), 'currentState': array([ 2.97029265, 14.96801606,  4.91507823]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 1.3705921781596104}
episode index:77
target Thresh 20.14686545123256
target distance 10.0
model initialize at round 77
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([21.25977138,  9.4270268 ,  1.06949135]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 9.601549576949596}
done in step count: 29
reward sum = 0.5891259218241636
running average episode reward sum: 0.06829864774098152
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([21.20312094, 18.0389451 ,  1.43991369]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 1.248456151394888}
episode index:78
target Thresh 20.264806110586278
target distance 8.0
model initialize at round 78
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 20.]), 'previousTarget': array([20., 20.]), 'currentState': array([12.0146519 , 21.02037139,  0.69477451]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 8.050275894864061}
done in step count: 64
reward sum = 0.4033936223784612
running average episode reward sum: 0.07254035628069645
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 20.]), 'previousTarget': array([20., 20.]), 'currentState': array([19.08863934, 20.66656321,  0.53079638]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 1.1291079522395864}
episode index:79
target Thresh 20.38157324077169
target distance 18.0
model initialize at round 79
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 13.]), 'previousTarget': array([27., 13.]), 'currentState': array([ 9.48980856, 18.89976502,  6.10360451]), 'targetState': array([27, 13], dtype=int32), 'currentDistance': 18.4773924417746}
done in step count: 92
reward sum = 0.11153943282526713
running average episode reward sum: 0.07302784473750358
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 13.]), 'previousTarget': array([27., 13.]), 'currentState': array([26.92233061, 13.92588643,  5.52143695]), 'targetState': array([27, 13], dtype=int32), 'currentDistance': 0.9291384222265029}
episode index:80
target Thresh 20.497178518599128
target distance 17.0
model initialize at round 80
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 12.]), 'previousTarget': array([26., 12.]), 'currentState': array([15.81272467, 28.88062339,  3.96158695]), 'targetState': array([26, 12], dtype=int32), 'currentDistance': 19.71638974627704}
done in step count: 87
reward sum = 0.07140556709015461
running average episode reward sum: 0.0730078166184005
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 12.]), 'previousTarget': array([26., 12.]), 'currentState': array([25.77915351, 12.90820945,  5.66309887]), 'targetState': array([26, 12], dtype=int32), 'currentDistance': 0.9346751153643738}
episode index:81
target Thresh 20.611633504692705
target distance 12.0
model initialize at round 81
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 2.]), 'previousTarget': array([5., 2.]), 'currentState': array([13.51574086, 13.89568016,  3.59223354]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 14.629594964715725}
done in step count: 46
reward sum = 0.38737055988569546
running average episode reward sum: 0.07684150860946508
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 2.]), 'previousTarget': array([5., 2.]), 'currentState': array([5.88575941, 2.11998204, 4.24776108]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 0.8938486571108007}
episode index:82
target Thresh 20.724949644646415
target distance 9.0
model initialize at round 82
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 17.]), 'previousTarget': array([12., 17.]), 'currentState': array([16.71701354, 26.08858394,  3.0907228 ]), 'targetState': array([12, 17], dtype=int32), 'currentDistance': 10.239754622671093}
done in step count: 34
reward sum = 0.5255829121956325
running average episode reward sum: 0.08224803154423818
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 17.]), 'previousTarget': array([12., 17.]), 'currentState': array([12.98114402, 17.40387323,  4.32415354]), 'targetState': array([12, 17], dtype=int32), 'currentDistance': 1.061017048573597}
episode index:83
target Thresh 20.837138270168687
target distance 14.0
model initialize at round 83
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 18.]), 'previousTarget': array([ 2., 18.]), 'currentState': array([10.16595007,  4.46615015,  1.48128515]), 'targetState': array([ 2, 18], dtype=int32), 'currentDistance': 15.80657560420259}
done in step count: 64
reward sum = 0.2780712797373043
running average episode reward sum: 0.08457926068939373
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 18.]), 'previousTarget': array([ 2., 18.]), 'currentState': array([ 1.72132957, 17.10599697,  2.08467837]), 'targetState': array([ 2, 18], dtype=int32), 'currentDistance': 0.9364286562867192}
episode index:84
target Thresh 20.94821060021556
target distance 12.0
model initialize at round 84
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 14.]), 'previousTarget': array([13., 14.]), 'currentState': array([24.61725639, 19.68185358,  3.66733429]), 'targetState': array([13, 14], dtype=int32), 'currentDistance': 12.932289278063283}
done in step count: 64
reward sum = 0.32622989766775834
running average episode reward sum: 0.08742220935972743
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 14.]), 'previousTarget': array([13., 14.]), 'currentState': array([13.95159072, 13.74815053,  4.37606179]), 'targetState': array([13, 14], dtype=int32), 'currentDistance': 0.984354131270426}
episode index:85
target Thresh 21.058177742112598
target distance 14.0
model initialize at round 85
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([15.09218503, 22.51160264,  4.70780754]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 18.814056306236658}
done in step count: 66
reward sum = 0.24254811339019333
running average episode reward sum: 0.08922599894147704
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.66448611, 9.95552334, 4.58729973]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.1638585124939342}
episode index:86
target Thresh 21.16705069266563
target distance 7.0
model initialize at round 86
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 14.]), 'previousTarget': array([19., 14.]), 'currentState': array([26.        ,  8.        ,  6.22062165]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 59
reward sum = 0.41098107614382734
running average episode reward sum: 0.0929243331621937
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 14.]), 'previousTarget': array([19., 14.]), 'currentState': array([19.30814081, 13.00728502,  2.75756753]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 1.039439169733932}
episode index:87
target Thresh 21.274840339260443
target distance 14.0
model initialize at round 87
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 3.]), 'previousTarget': array([6., 3.]), 'currentState': array([ 8.285935  , 16.59446271,  5.1107198 ]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 13.785315200573216}
done in step count: 77
reward sum = 0.23819300751498004
running average episode reward sum: 0.09457511355256627
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 3.]), 'previousTarget': array([6., 3.]), 'currentState': array([6.35939039, 3.97312656, 4.6665476 ]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 1.0373701098266002}
episode index:88
target Thresh 21.381557460951516
target distance 15.0
model initialize at round 88
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 23.]), 'previousTarget': array([ 2., 23.]), 'currentState': array([3.86463084, 7.89369756, 3.6331355 ]), 'targetState': array([ 2, 23], dtype=int32), 'currentDistance': 15.220946804924676}
done in step count: 69
reward sum = 0.23638652980913832
running average episode reward sum: 0.09616850025207832
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 23.]), 'previousTarget': array([ 2., 23.]), 'currentState': array([ 1.53570263, 22.02357016,  1.16496831]), 'targetState': array([ 2, 23], dtype=int32), 'currentDistance': 1.0811971574116115}
episode index:89
target Thresh 21.487212729539955
target distance 11.0
model initialize at round 89
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 16.]), 'previousTarget': array([17., 16.]), 'currentState': array([24.03091635, 27.02570506,  0.44111943]), 'targetState': array([17, 16], dtype=int32), 'currentDistance': 13.07669518004826}
done in step count: 43
reward sum = 0.38430362426504633
running average episode reward sum: 0.09937000163000018
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 16.]), 'previousTarget': array([17., 16.]), 'currentState': array([17.81070673, 16.93719538,  4.65093483]), 'targetState': array([17, 16], dtype=int32), 'currentDistance': 1.2391854558073494}
episode index:90
target Thresh 21.591816710640664
target distance 15.0
model initialize at round 90
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 25.]), 'previousTarget': array([24., 25.]), 'currentState': array([21.65960953, 10.04938358,  2.80227053]), 'targetState': array([24, 25], dtype=int32), 'currentDistance': 15.132691725069868}
done in step count: 99
reward sum = -0.2205991491323335
running average episode reward sum: 0.09585385711612839
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 25.]), 'previousTarget': array([24., 25.]), 'currentState': array([23.26258887, 23.90688033,  1.28625684]), 'targetState': array([24, 25], dtype=int32), 'currentDistance': 1.3185923481038078}
episode index:91
target Thresh 21.695379864738918
target distance 12.0
model initialize at round 91
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 12.]), 'previousTarget': array([26., 12.]), 'currentState': array([27.        , 24.        ,  3.67944935]), 'targetState': array([26, 12], dtype=int32), 'currentDistance': 12.041594578792296}
done in step count: 68
reward sum = 0.32837898661960385
running average episode reward sum: 0.09838130417594879
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 12.]), 'previousTarget': array([26., 12.]), 'currentState': array([25.11312815, 12.94932305,  4.67776853]), 'targetState': array([26, 12], dtype=int32), 'currentDistance': 1.2991366082944726}
episode index:92
target Thresh 21.79791254823644
target distance 7.0
model initialize at round 92
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  2.]), 'previousTarget': array([13.,  2.]), 'currentState': array([6.18669248, 7.22286245, 0.65002444]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 8.584838470803929}
done in step count: 63
reward sum = 0.37332568869505856
running average episode reward sum: 0.10133769540733704
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  2.]), 'previousTarget': array([13.,  2.]), 'currentState': array([12.00015644,  1.25535215,  0.07680747]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 1.246670592006339}
episode index:93
target Thresh 21.89942501448701
target distance 3.0
model initialize at round 93
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 12.]), 'previousTarget': array([ 7., 12.]), 'currentState': array([ 8.03920685, 14.95618762,  5.18986845]), 'targetState': array([ 7, 12], dtype=int32), 'currentDistance': 3.1335277415613643}
done in step count: 24
reward sum = 0.7343555700941742
running average episode reward sum: 0.10807192811677148
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 12.]), 'previousTarget': array([ 7., 12.]), 'currentState': array([ 6.90381654, 12.9582622 ,  3.71970454]), 'targetState': array([ 7, 12], dtype=int32), 'currentDistance': 0.9630772078169887}
episode index:94
target Thresh 21.999927414821858
target distance 14.0
model initialize at round 94
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'previousTarget': array([27.,  8.]), 'currentState': array([25.09725555, 21.98258739,  5.85352262]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 14.11145586140578}
done in step count: 52
reward sum = 0.3742935239174573
running average episode reward sum: 0.11087426070414712
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'previousTarget': array([27.,  8.]), 'currentState': array([26.60476991,  8.98918065,  4.39641052]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 1.0652160239898727}
episode index:95
target Thresh 22.099429799564767
target distance 7.0
model initialize at round 95
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([ 7.3606723 , 12.68727864,  5.7286754 ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 7.594517498584747}
done in step count: 56
reward sum = 0.4745670894420453
running average episode reward sum: 0.11466272767016689
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.11486523,  9.28891985,  5.73647038]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.9310951837978076}
episode index:96
target Thresh 22.19794211903713
target distance 4.0
model initialize at round 96
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([ 9.54869362, 16.11477795,  2.84749855]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 4.018368894398801}
done in step count: 19
reward sum = 0.7760766163471289
running average episode reward sum: 0.12148142755343454
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([ 6.93018233, 18.10379234,  2.04387872]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 0.9359551348451757}
episode index:97
target Thresh 22.29547422455299
target distance 14.0
model initialize at round 97
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 24.]), 'previousTarget': array([20., 24.]), 'currentState': array([ 5.78568904, 10.21792017,  2.14955744]), 'targetState': array([20, 24], dtype=int32), 'currentDistance': 19.798796948297202}
done in step count: 99
reward sum = -0.3062399658999819
running average episode reward sum: 0.11711692353860376
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 24.]), 'previousTarget': array([20., 24.]), 'currentState': array([19.66790082, 22.51068142,  3.61381683]), 'targetState': array([20, 24], dtype=int32), 'currentDistance': 1.5258963555592193}
episode index:98
target Thresh 22.392035869404168
target distance 4.0
model initialize at round 98
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 21.]), 'previousTarget': array([ 2., 21.]), 'currentState': array([ 6.00952504, 24.00464132,  0.20091617]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 5.010405219605303}
done in step count: 64
reward sum = 0.4070312611343406
running average episode reward sum: 0.12004535119108595
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 21.]), 'previousTarget': array([ 2., 21.]), 'currentState': array([ 2.18285786, 21.99856585,  3.25440154]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 1.0151703072162637}
episode index:99
target Thresh 22.487636709835627
target distance 20.0
model initialize at round 99
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9.98118886, 2.33100875]), 'previousTarget': array([9.99875234, 2.02495322]), 'currentState': array([ 8.84642514, 22.29879058,  2.28134224]), 'targetState': array([10,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.33080488698984245
running average episode reward sum: 0.11553684880927667
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  2.]), 'previousTarget': array([10.,  2.]), 'currentState': array([8.89129225, 4.18880361, 4.67939616]), 'targetState': array([10,  2], dtype=int32), 'currentDistance': 2.4535880100656953}
episode index:100
target Thresh 22.582286306011078
target distance 11.0
model initialize at round 100
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([ 9.01011091, 10.3957161 ,  1.29275095]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 11.915414065812245}
done in step count: 99
reward sum = -0.18115489845139363
running average episode reward sum: 0.11259930675719082
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([19.23720316, 13.0284372 ,  4.3986162 ]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 2.113981765884578}
episode index:101
target Thresh 22.675994122969
target distance 8.0
model initialize at round 101
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 14.]), 'previousTarget': array([ 5., 14.]), 'currentState': array([13.        , 21.        ,  2.53590083]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 10.630145812734648}
done in step count: 99
reward sum = -0.24812859440454849
running average episode reward sum: 0.10906275870658554
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 14.]), 'previousTarget': array([ 5., 14.]), 'currentState': array([ 2.91435173, 16.21082916,  4.85768019]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 3.0393575439451097}
episode index:102
target Thresh 22.768769531569195
target distance 12.0
model initialize at round 102
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 20.]), 'previousTarget': array([20., 20.]), 'currentState': array([ 8.36639019, 24.99746531,  6.07955954]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 12.661577184827244}
done in step count: 99
reward sum = -0.19706496600315343
running average episode reward sum: 0.10609064487445215
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 20.]), 'previousTarget': array([20., 20.]), 'currentState': array([17.54509011, 17.92334966,  6.22347062]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 3.215440747011255}
episode index:103
target Thresh 22.860621809429826
target distance 10.0
model initialize at round 103
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  8.]), 'previousTarget': array([19.,  8.]), 'currentState': array([ 9.        , 13.        ,  5.38814116]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 11.180339887498949}
done in step count: 99
reward sum = -0.09450699373771489
running average episode reward sum: 0.10416182142625822
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  8.]), 'previousTarget': array([19.,  8.]), 'currentState': array([15.61789664,  8.60526007,  5.34810909]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 3.435835112812407}
episode index:104
target Thresh 22.951560141855225
target distance 2.0
model initialize at round 104
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([2.00153481, 4.13486599, 1.39370728]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 2.003010719178714}
done in step count: 99
reward sum = -0.2933956973818802
running average episode reward sum: 0.10037555934237119
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([5.23896939, 2.2492006 , 0.81479485]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 2.1448411755143217}
episode index:105
target Thresh 23.041593622754423
target distance 18.0
model initialize at round 105
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 29.]), 'previousTarget': array([13., 29.]), 'currentState': array([ 5.        , 11.        ,  2.83833504]), 'targetState': array([13, 29], dtype=int32), 'currentDistance': 19.69771560359221}
done in step count: 99
reward sum = -0.22148738222434314
running average episode reward sum: 0.09733911649740219
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 29.]), 'previousTarget': array([13., 29.]), 'currentState': array([ 8.55604585, 24.37392882,  0.42567556]), 'targetState': array([13, 29], dtype=int32), 'currentDistance': 6.414769135461623}
episode index:106
target Thresh 23.13073125555053
target distance 18.0
model initialize at round 106
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([10.9640838 , 20.96051063,  4.14501113]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 19.289480520033482}
done in step count: 99
reward sum = -0.20904754927824679
running average episode reward sum: 0.09447568971445221
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([15.47614738,  8.22811556,  5.03238175]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 5.805430589762399}
episode index:107
target Thresh 23.21898195408111
target distance 14.0
model initialize at round 107
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 18.]), 'previousTarget': array([21., 18.]), 'currentState': array([9.        , 4.        , 5.31136209]), 'targetState': array([21, 18], dtype=int32), 'currentDistance': 18.439088914585774}
done in step count: 99
reward sum = -0.1838800458581466
running average episode reward sum: 0.09189832179248371
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 18.]), 'previousTarget': array([21., 18.]), 'currentState': array([16.62945748, 12.24062315,  0.56345285]), 'targetState': array([21, 18], dtype=int32), 'currentDistance': 7.229942158015856}
episode index:108
target Thresh 23.306354543489558
target distance 11.0
model initialize at round 108
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([13.      ,  3.      ,  5.077106]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 13.038404810405298}
done in step count: 99
reward sum = -0.1297466993308263
running average episode reward sum: 0.08986488123171939
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([5.07542688, 8.11349102, 2.53535638]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 3.6079310705308045}
episode index:109
target Thresh 23.392857761107628
target distance 10.0
model initialize at round 109
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  3.]), 'previousTarget': array([17.,  3.]), 'currentState': array([ 9.95310699, 12.83478978,  4.68832445]), 'targetState': array([17,  3], dtype=int32), 'currentDistance': 12.098834290785843}
done in step count: 99
reward sum = -0.10142487473727212
running average episode reward sum: 0.0881258834501831
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  3.]), 'previousTarget': array([17.,  3.]), 'currentState': array([15.70563474,  6.33835396,  4.9018369 ]), 'targetState': array([17,  3], dtype=int32), 'currentDistance': 3.5805011600177807}
episode index:110
target Thresh 23.478500257329163
target distance 16.0
model initialize at round 110
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 28.]), 'previousTarget': array([21., 28.]), 'currentState': array([ 5.02135571, 18.98624142,  5.9418174 ]), 'targetState': array([21, 28], dtype=int32), 'currentDistance': 18.34570568762759}
done in step count: 99
reward sum = -0.19819189211091853
running average episode reward sum: 0.08554644403071372
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 28.]), 'previousTarget': array([21., 28.]), 'currentState': array([18.46952616, 27.47922445,  5.96352187]), 'targetState': array([21, 28], dtype=int32), 'currentDistance': 2.583506346880102}
episode index:111
target Thresh 23.56329059647516
target distance 18.0
model initialize at round 111
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 21.]), 'previousTarget': array([27., 21.]), 'currentState': array([ 9.        , 22.        ,  5.80306447]), 'targetState': array([27, 21], dtype=int32), 'currentDistance': 18.027756377319943}
done in step count: 99
reward sum = -0.13897610683770842
running average episode reward sum: 0.08354177839795995
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 21.]), 'previousTarget': array([27., 21.]), 'currentState': array([22.04097366, 23.82543972,  6.18023039]), 'targetState': array([27, 21], dtype=int32), 'currentDistance': 5.707455815823015}
episode index:112
target Thresh 23.64723725765019
target distance 4.0
model initialize at round 112
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 26.]), 'previousTarget': array([16., 26.]), 'currentState': array([19.76367762, 22.07303732,  2.79583229]), 'targetState': array([16, 26], dtype=int32), 'currentDistance': 5.4393294707466255}
done in step count: 97
reward sum = 0.28981396289457767
running average episode reward sum: 0.08536719595987692
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 26.]), 'previousTarget': array([16., 26.]), 'currentState': array([16.87616465, 25.61741943,  2.32662982]), 'targetState': array([16, 26], dtype=int32), 'currentDistance': 0.9560504105582576}
episode index:113
target Thresh 23.730348635590325
target distance 7.0
model initialize at round 113
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 14.]), 'previousTarget': array([19., 14.]), 'currentState': array([13.16100503,  6.98232102,  0.08633095]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 9.129166476116916}
done in step count: 99
reward sum = -0.03278365934464762
running average episode reward sum: 0.08433078494843371
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 14.]), 'previousTarget': array([19., 14.]), 'currentState': array([14.99149549,  8.51315642,  0.91491192]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 6.795113014292054}
episode index:114
target Thresh 23.812633041502618
target distance 8.0
model initialize at round 114
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 15.]), 'previousTarget': array([27., 15.]), 'currentState': array([23.        , 23.        ,  6.12126726]), 'targetState': array([27, 15], dtype=int32), 'currentDistance': 8.94427190999916}
done in step count: 99
reward sum = -0.06609051269297497
running average episode reward sum: 0.08302277366459537
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 15.]), 'previousTarget': array([27., 15.]), 'currentState': array([23.81682295, 15.92044049,  4.92866675]), 'targetState': array([27, 15], dtype=int32), 'currentDistance': 3.313582169856547}
episode index:115
target Thresh 23.894098703896233
target distance 10.0
model initialize at round 115
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 16.]), 'previousTarget': array([ 6., 16.]), 'currentState': array([12.        ,  6.        ,  1.73553848]), 'targetState': array([ 6, 16], dtype=int32), 'currentDistance': 11.661903789690601}
done in step count: 99
reward sum = -0.0882950525549459
running average episode reward sum: 0.08154589585235796
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 16.]), 'previousTarget': array([ 6., 16.]), 'currentState': array([ 8.78855707, 14.86558768,  1.31211821]), 'targetState': array([ 6, 16], dtype=int32), 'currentDistance': 3.0104720263123825}
episode index:116
target Thresh 23.974753769405304
target distance 2.0
model initialize at round 116
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 7.]), 'previousTarget': array([7., 7.]), 'currentState': array([5.01350118, 9.00805061, 0.28518701]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 2.824614106280913}
done in step count: 99
reward sum = -0.269931296815866
running average episode reward sum: 0.07854181728254407
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 7.]), 'previousTarget': array([7., 7.]), 'currentState': array([8.38804224, 4.92255857, 1.00903084]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 2.498484374872237}
episode index:117
target Thresh 24.054606303603585
target distance 8.0
model initialize at round 117
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([21.91662265, 27.96447091,  3.59529392]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 9.912004207507495}
done in step count: 99
reward sum = -0.06645886811253411
running average episode reward sum: 0.07731299791478918
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([16.17692792, 23.20042173,  2.55068589]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 2.4859661117991028}
episode index:118
target Thresh 24.13366429181104
target distance 8.0
model initialize at round 118
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([16.00698815, 12.0049933 ,  0.46739471]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 8.252752222255467}
done in step count: 99
reward sum = -0.07272575734427457
running average episode reward sum: 0.07605216803866259
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([15.12533749,  5.63819944,  4.14722204]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.98748128866506}
episode index:119
target Thresh 24.211935639892374
target distance 2.0
model initialize at round 119
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([9.00882262, 9.012981  , 0.7213546 ]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 2.243774745153011}
done in step count: 26
reward sum = 0.7189252128147811
running average episode reward sum: 0.08140944341179691
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([9.09334068, 7.99843381, 4.2140094 ]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 1.3486664480603794}
episode index:120
target Thresh 24.289428175047625
target distance 8.0
model initialize at round 120
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 25.]), 'previousTarget': array([23., 25.]), 'currentState': array([23.19798228, 16.8239993 ,  5.73843539]), 'targetState': array([23, 25], dtype=int32), 'currentDistance': 8.178397421182769}
done in step count: 99
reward sum = -0.06324547257091843
running average episode reward sum: 0.08021394823838605
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 25.]), 'previousTarget': array([23., 25.]), 'currentState': array([24.58936366, 21.20470398,  1.33319816]), 'targetState': array([23, 25], dtype=int32), 'currentDistance': 4.114650500764837}
episode index:121
target Thresh 24.366149646594884
target distance 8.0
model initialize at round 121
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 21.]), 'previousTarget': array([ 7., 21.]), 'currentState': array([10.94598005, 12.98956379,  3.0799334 ]), 'targetState': array([ 7, 21], dtype=int32), 'currentDistance': 8.929605078439964}
done in step count: 99
reward sum = -0.05296030578124797
running average episode reward sum: 0.07912235599232348
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 21.]), 'previousTarget': array([ 7., 21.]), 'currentState': array([ 9.79125777, 17.59720056,  1.7247053 ]), 'targetState': array([ 7, 21], dtype=int32), 'currentDistance': 4.401154844395302}
episode index:122
target Thresh 24.442107726745235
target distance 18.0
model initialize at round 122
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  7.]), 'previousTarget': array([27.,  7.]), 'currentState': array([22.24562971, 24.79295998,  5.36173546]), 'targetState': array([27,  7], dtype=int32), 'currentDistance': 18.41720558833412}
done in step count: 99
reward sum = -0.06085663786492592
running average episode reward sum: 0.07798431539185803
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  7.]), 'previousTarget': array([27.,  7.]), 'currentState': array([21.67472036, 18.56506359,  4.51708124]), 'targetState': array([27,  7], dtype=int32), 'currentDistance': 12.732215011634919}
episode index:123
target Thresh 24.51731001137
target distance 11.0
model initialize at round 123
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([ 3.02132934, 24.99598507,  5.97210325]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 11.03945125365112}
done in step count: 99
reward sum = -0.04578314691538
running average episode reward sum: 0.07698619069583192
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([ 2.44119711, 21.1907432 ,  4.45162509]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 7.357761496013138}
episode index:124
target Thresh 24.591764020760305
target distance 9.0
model initialize at round 124
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 14.]), 'previousTarget': array([19., 14.]), 'currentState': array([10.        , 13.        ,  5.60532618]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 9.055385138137417}
done in step count: 99
reward sum = -0.03570396714176338
running average episode reward sum: 0.07608466943313115
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 14.]), 'previousTarget': array([19., 14.]), 'currentState': array([15.54004978, 11.94113893,  0.09611497]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 4.0261848482967295}
episode index:125
target Thresh 24.665477200379133
target distance 24.0
model initialize at round 125
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9.24178455, 9.66566533]), 'previousTarget': array([ 9.4000212 , 10.04003392]), 'currentState': array([19.97928991, 26.53889794,  4.47624227]), 'targetState': array([5, 3], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.05593014560899941
running average episode reward sum: 0.07503693280581267
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7.11599229, 6.60767198]), 'previousTarget': array([7.16027243, 6.67318676]), 'currentState': array([17.23447509, 23.85923149,  3.84760814]), 'targetState': array([5, 3], dtype=int32), 'currentDistance': 20.0}
episode index:126
target Thresh 24.738456921605877
target distance 11.0
model initialize at round 126
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  2.]), 'previousTarget': array([20.,  2.]), 'currentState': array([ 9.00207711, 11.94150689,  5.00038433]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 14.825244254470103}
done in step count: 99
reward sum = -0.037960315780995256
running average episode reward sum: 0.0741471906909559
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  2.]), 'previousTarget': array([20.,  2.]), 'currentState': array([10.89422003,  8.38346483,  5.18668294]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 11.120424994029976}
episode index:127
target Thresh 24.81071048247348
target distance 17.0
model initialize at round 127
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([18.        , 21.        ,  1.84407336]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 17.46424919657298}
done in step count: 99
reward sum = -0.06230419261607405
running average episode reward sum: 0.07308116425886974
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([16.24308405, 17.59346192,  4.24264091]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 13.77728685339298}
episode index:128
target Thresh 24.88224510839823
target distance 2.0
model initialize at round 128
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 25.]), 'previousTarget': array([16., 25.]), 'currentState': array([13.96370792, 24.02053433,  2.45036969]), 'targetState': array([16, 25], dtype=int32), 'currentDistance': 2.2596102389349015}
done in step count: 47
reward sum = 0.5357606665328589
running average episode reward sum: 0.07666782706719523
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 25.]), 'previousTarget': array([16., 25.]), 'currentState': array([16.93613824, 24.09233899,  3.59402983]), 'targetState': array([16, 25], dtype=int32), 'currentDistance': 1.3039184454497474}
episode index:129
target Thresh 24.95306795290234
target distance 15.0
model initialize at round 129
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 11.]), 'previousTarget': array([17., 11.]), 'currentState': array([ 4.96302796, 26.04776158,  2.39924747]), 'targetState': array([17, 11], dtype=int32), 'currentDistance': 19.269764513622665}
done in step count: 99
reward sum = -0.07011893088443019
running average episode reward sum: 0.07553869815987505
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 11.]), 'previousTarget': array([17., 11.]), 'currentState': array([ 9.84154558, 20.47560328,  4.91773377]), 'targetState': array([17, 11], dtype=int32), 'currentDistance': 11.87562744604941}
episode index:130
target Thresh 25.023186098329276
target distance 17.0
model initialize at round 130
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.47424026, 10.38094912]), 'previousTarget': array([21.33935727, 10.46633605]), 'currentState': array([ 5.27871355, 22.11572285,  0.14104378]), 'targetState': array([22, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.10859542836094228
running average episode reward sum: 0.07413309414063216
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 10.]), 'previousTarget': array([22., 10.]), 'currentState': array([16.29609577, 19.72439563,  4.9039206 ]), 'targetState': array([22, 10], dtype=int32), 'currentDistance': 11.273792346868657}
episode index:131
target Thresh 25.092606556552017
target distance 7.0
model initialize at round 131
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([15.94974417, 17.99196506,  3.55263162]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 7.258724251380594}
done in step count: 99
reward sum = -0.32844852616509956
running average episode reward sum: 0.07108323338074025
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([16.9421429 , 15.04239065,  5.79035873]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 4.999712692390666}
episode index:132
target Thresh 25.16133626967423
target distance 16.0
model initialize at round 132
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 10.]), 'previousTarget': array([23., 10.]), 'currentState': array([7.       , 3.       , 4.6672473]), 'targetState': array([23, 10], dtype=int32), 'currentDistance': 17.46424919657298}
done in step count: 99
reward sum = -0.5469787869370691
running average episode reward sum: 0.06643615052120785
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 10.]), 'previousTarget': array([23., 10.]), 'currentState': array([25.56667571,  9.37599772,  3.73842028]), 'targetState': array([23, 10], dtype=int32), 'currentDistance': 2.6414395777428057}
episode index:133
target Thresh 25.229382110724508
target distance 14.0
model initialize at round 133
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 14.]), 'previousTarget': array([13., 14.]), 'currentState': array([24.      , 28.      ,  3.248981]), 'targetState': array([13, 14], dtype=int32), 'currentDistance': 17.804493814764854}
done in step count: 99
reward sum = -0.3496918221502883
running average episode reward sum: 0.063330717889331
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 14.]), 'previousTarget': array([13., 14.]), 'currentState': array([14.2254843 , 13.38437231,  2.56942353]), 'targetState': array([13, 14], dtype=int32), 'currentDistance': 1.3714259806786604}
episode index:134
target Thresh 25.296750884343655
target distance 18.0
model initialize at round 134
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  7.]), 'previousTarget': array([20.,  7.]), 'currentState': array([22.        , 25.        ,  2.98769855]), 'targetState': array([20,  7], dtype=int32), 'currentDistance': 18.110770276274835}
done in step count: 72
reward sum = 0.17824442455757356
running average episode reward sum: 0.064181930531318
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  7.]), 'previousTarget': array([20.,  7.]), 'currentState': array([19.73670594,  7.91077356,  4.39454219]), 'targetState': array([20,  7], dtype=int32), 'currentDistance': 0.9480676380259794}
episode index:135
target Thresh 25.363449327465176
target distance 7.0
model initialize at round 135
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([10.47586996, 24.90271211,  5.82902209]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 8.253192538324932}
done in step count: 32
reward sum = 0.5822594430177483
running average episode reward sum: 0.06799132400548291
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([14.00336727, 18.67432899,  5.00080824]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 1.2033272121628973}
episode index:136
target Thresh 25.42948410998897
target distance 5.0
model initialize at round 136
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([20.0512584 , 20.02748381,  0.71769398]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 5.154700224192106}
done in step count: 26
reward sum = 0.6651569152806417
running average episode reward sum: 0.07235019693449868
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([15.99316283, 19.79276301,  3.27859043]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 1.2707657576586515}
episode index:137
target Thresh 25.49486183544831
target distance 22.0
model initialize at round 137
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.41817361,  4.52211171]), 'previousTarget': array([22.43242207,  4.49734288]), 'currentState': array([17.92244625, 24.01027313,  3.25127444]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.16781781238040433
running average episode reward sum: 0.07304199124932408
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  2.]), 'previousTarget': array([23.,  2.]), 'currentState': array([22.10860409,  2.99520398,  5.3969801 ]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 1.3360455178251436}
episode index:138
target Thresh 25.559589041670232
target distance 18.0
model initialize at round 138
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.69353217, 23.82565014]), 'previousTarget': array([23.48314552, 23.71285862]), 'currentState': array([ 6.30977356, 13.9360123 ,  0.04880207]), 'targetState': array([24, 24], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.1933847865838526
running average episode reward sum: 0.07390776675532788
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 24.]), 'previousTarget': array([24., 24.]), 'currentState': array([23.20564508, 23.09447299,  0.64343904]), 'targetState': array([24, 24], dtype=int32), 'currentDistance': 1.2045658524434097}
episode index:139
target Thresh 25.623672201429294
target distance 9.0
model initialize at round 139
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 20.]), 'previousTarget': array([23., 20.]), 'currentState': array([23.0419589 , 10.99311336,  0.07068491]), 'targetState': array([23, 20], dtype=int32), 'currentDistance': 9.006984377183784}
done in step count: 19
reward sum = 0.6198597082335625
running average episode reward sum: 0.07780742348017242
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 20.]), 'previousTarget': array([23., 20.]), 'currentState': array([23.00538105, 19.03005345,  1.42392771]), 'targetState': array([23, 20], dtype=int32), 'currentDistance': 0.9699614781978343}
episode index:140
target Thresh 25.68711772309487
target distance 20.0
model initialize at round 140
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4.22127294, 14.03319094]), 'currentState': array([23.50048749, 16.98890808,  3.09699942]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 19.728217961860633}
done in step count: 51
reward sum = 0.20078970784873346
running average episode reward sum: 0.07867963826292816
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([ 4.88782427, 13.9548717 ,  3.27158908]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 0.8889704681406717}
episode index:141
target Thresh 25.74993195127201
target distance 2.0
model initialize at round 141
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 19.]), 'previousTarget': array([ 7., 19.]), 'currentState': array([ 5.        , 19.        ,  4.97748423]), 'targetState': array([ 7, 19], dtype=int32), 'currentDistance': 2.0}
done in step count: 7
reward sum = 0.8921130507793176
running average episode reward sum: 0.08440804257642386
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 19.]), 'previousTarget': array([ 7., 19.]), 'currentState': array([ 6.0382271 , 18.91235648,  0.04335412]), 'targetState': array([ 7, 19], dtype=int32), 'currentDistance': 0.9657579958251703}
episode index:142
target Thresh 25.812121167435865
target distance 10.0
model initialize at round 142
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 16.]), 'previousTarget': array([ 9., 16.]), 'currentState': array([19.        ,  9.        ,  6.13053074]), 'targetState': array([ 9, 16], dtype=int32), 'currentDistance': 12.206555615733702}
done in step count: 34
reward sum = 0.3643027167011196
running average episode reward sum: 0.08636534798988327
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 16.]), 'previousTarget': array([ 9., 16.]), 'currentState': array([ 9.47046525, 15.26431164,  1.5527399 ]), 'targetState': array([ 9, 16], dtype=int32), 'currentDistance': 0.8732553560702203}
episode index:143
target Thresh 25.873691590559883
target distance 24.0
model initialize at round 143
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7.66597696, 21.06238299]), 'previousTarget': array([ 7.90599608, 20.64100589]), 'currentState': array([18.82762146,  4.46666993,  1.74916381]), 'targetState': array([ 3, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 72
reward sum = -0.03852263662164024
running average episode reward sum: 0.0854980703189699
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 28.]), 'previousTarget': array([ 3., 28.]), 'currentState': array([ 3.58342371, 27.05786366,  2.09229899]), 'targetState': array([ 3, 28], dtype=int32), 'currentDistance': 1.1081534725260782}
episode index:144
target Thresh 25.93464937773768
target distance 9.0
model initialize at round 144
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([7.00621267, 6.98881427, 5.47187662]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 9.484484699084772}
done in step count: 21
reward sum = 0.5755377353043964
running average episode reward sum: 0.08887765421542113
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.04636212,  9.98982515,  6.08514623]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.9536921549472702}
episode index:145
target Thresh 25.99500062479878
target distance 11.0
model initialize at round 145
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([ 3.        , 29.        ,  4.38021371]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 13.038404810405298}
done in step count: 30
reward sum = 0.3774898233900759
running average episode reward sum: 0.0908544498946996
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([13.46389232, 22.87175509,  4.73544537]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 1.0234101703441076}
episode index:146
target Thresh 26.054751366918175
target distance 15.0
model initialize at round 146
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 17.]), 'previousTarget': array([ 9., 17.]), 'currentState': array([24.00945449, 12.97822695,  4.8695507 ]), 'targetState': array([ 9, 17], dtype=int32), 'currentDistance': 15.53893119289471}
done in step count: 39
reward sum = 0.26879007955176915
running average episode reward sum: 0.09206489635495177
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 17.]), 'previousTarget': array([ 9., 17.]), 'currentState': array([ 9.35778251, 16.00504776,  1.69980085]), 'targetState': array([ 9, 17], dtype=int32), 'currentDistance': 1.0573260079610467}
episode index:147
target Thresh 26.11390757921987
target distance 6.0
model initialize at round 147
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  4.]), 'previousTarget': array([20.,  4.]), 'currentState': array([19.81967262,  9.88619434,  3.93734768]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 5.888955917057678}
done in step count: 12
reward sum = 0.751190142032362
running average episode reward sum: 0.09651844531223157
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  4.]), 'previousTarget': array([20.,  4.]), 'currentState': array([20.19196291,  4.77505491,  4.54085476]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 0.7984734634664034}
episode index:148
target Thresh 26.172475177374395
target distance 11.0
model initialize at round 148
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 28.]), 'previousTarget': array([10., 28.]), 'currentState': array([ 1.98040148, 16.97723038,  4.20241672]), 'targetState': array([10, 28], dtype=int32), 'currentDistance': 13.63141264277034}
done in step count: 37
reward sum = 0.31140734616676374
running average episode reward sum: 0.09796065270051701
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 28.]), 'previousTarget': array([10., 28.]), 'currentState': array([ 9.05975368, 27.92657372,  0.05584723]), 'targetState': array([10, 28], dtype=int32), 'currentDistance': 0.9431089862205787}
episode index:149
target Thresh 26.230460018190367
target distance 15.0
model initialize at round 149
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 24.]), 'previousTarget': array([24., 24.]), 'currentState': array([ 8.9717308 , 17.02552663,  2.16813636]), 'targetState': array([24, 24], dtype=int32), 'currentDistance': 16.567804744174996}
done in step count: 44
reward sum = 0.21635275803804016
running average episode reward sum: 0.09874993340276716
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 24.]), 'previousTarget': array([24., 24.]), 'currentState': array([23.55353621, 24.81970008,  5.36340559]), 'targetState': array([24, 24], dtype=int32), 'currentDistance': 0.9334013793390533}
episode index:150
target Thresh 26.287867900200197
target distance 26.0
model initialize at round 150
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2.42322688, 8.5790347 ]), 'previousTarget': array([2.46607002, 9.05891029]), 'currentState': array([ 3.93608471, 28.52173415,  4.69049503]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 55
reward sum = 0.1006372409367029
running average episode reward sum: 0.09876243212815747
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.9981091 , 3.83553959, 3.70477761]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.3016713026161701}
episode index:151
target Thresh 26.34470456423992
target distance 18.0
model initialize at round 151
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.42900019, 26.06563667]), 'previousTarget': array([26.42900019, 26.06563667]), 'currentState': array([16.        ,  9.        ,  4.65504074]), 'targetState': array([27, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.19693978088674957
running average episode reward sum: 0.09940833573841136
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 27.]), 'previousTarget': array([27., 27.]), 'currentState': array([26.15618596, 26.27333617,  0.39894417]), 'targetState': array([27, 27], dtype=int32), 'currentDistance': 1.1135809138437092}
episode index:152
target Thresh 26.400975694023302
target distance 24.0
model initialize at round 152
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.41995419, 19.35379704]), 'previousTarget': array([13.43965318, 19.32048962]), 'currentState': array([24.98583332,  3.03722755,  2.02597213]), 'targetState': array([ 8, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.06548626705992389
running average episode reward sum: 0.09918662287123171
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 27.]), 'previousTarget': array([ 8., 27.]), 'currentState': array([ 8.16424381, 26.2466893 ,  1.42872535]), 'targetState': array([ 8, 27], dtype=int32), 'currentDistance': 0.7710078130882301}
episode index:153
target Thresh 26.45668691671022
target distance 22.0
model initialize at round 153
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.29128166,  6.70642952]), 'previousTarget': array([24.26234812,  6.70472358]), 'currentState': array([19.22481924, 26.05406396,  6.2666817 ]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.16648069358756834
running average episode reward sum: 0.09962359735640272
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  4.]), 'previousTarget': array([25.,  4.]), 'currentState': array([25.4031012 ,  4.90344881,  4.05487618]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 0.989297901711182}
episode index:154
target Thresh 26.511843803469365
target distance 13.0
model initialize at round 154
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  6.]), 'previousTarget': array([19.,  6.]), 'currentState': array([27.04223654, 18.50587439,  4.57409692]), 'targetState': array([19,  6], dtype=int32), 'currentDistance': 14.868572990946321}
done in step count: 34
reward sum = 0.444757897844614
running average episode reward sum: 0.10185027026277828
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  6.]), 'previousTarget': array([19.,  6.]), 'currentState': array([19.89957839,  6.57379825,  3.59431575]), 'targetState': array([19,  6], dtype=int32), 'currentDistance': 1.0669984641286148}
episode index:155
target Thresh 26.56645187003538
target distance 7.0
model initialize at round 155
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([12.08840674, 11.69113306,  4.73866463]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 7.582064838163013}
done in step count: 16
reward sum = 0.6847652248629864
running average episode reward sum: 0.10558690458713858
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([5.91257604, 9.29516519, 3.43929245]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 0.9591233104197641}
episode index:156
target Thresh 26.620516577260425
target distance 11.0
model initialize at round 156
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  4.]), 'previousTarget': array([18.,  4.]), 'currentState': array([7.44253556e+00, 8.96769889e+00, 4.11697825e-03]), 'targetState': array([18,  4], dtype=int32), 'currentDistance': 11.667822750835972}
done in step count: 25
reward sum = 0.5679081444478341
running average episode reward sum: 0.10853162586013665
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  4.]), 'previousTarget': array([18.,  4.]), 'currentState': array([17.35829527,  4.88135541,  5.46940934]), 'targetState': array([18,  4], dtype=int32), 'currentDistance': 1.0902166355428127}
episode index:157
target Thresh 26.674043331660275
target distance 16.0
model initialize at round 157
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 27.]), 'previousTarget': array([ 5., 27.]), 'currentState': array([15.64182794, 11.29171577,  2.39577834]), 'targetState': array([ 5, 27], dtype=int32), 'currentDistance': 18.973631584794123}
done in step count: 42
reward sum = 0.3339657042083047
running average episode reward sum: 0.10995842382436555
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 27.]), 'previousTarget': array([ 5., 27.]), 'currentState': array([ 5.26205153, 26.00007673,  1.61338921]), 'targetState': array([ 5, 27], dtype=int32), 'currentDistance': 1.0336912218173024}
episode index:158
target Thresh 26.727037485954984
target distance 23.0
model initialize at round 158
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9.00626665, 5.04294931]), 'previousTarget': array([9., 5.]), 'currentState': array([ 9.04745456, 25.0429069 ,  0.49893051]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.18826854720086206
running average episode reward sum: 0.1104509403235888
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 2.]), 'previousTarget': array([9., 2.]), 'currentState': array([9.71240237, 2.98816463, 4.01541512]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 1.2181898328528946}
episode index:159
target Thresh 26.779504339604138
target distance 4.0
model initialize at round 159
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  5.]), 'previousTarget': array([13.,  5.]), 'currentState': array([17.        ,  7.        ,  2.13183579]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 4.47213595499958}
done in step count: 9
reward sum = 0.8161722553563893
running average episode reward sum: 0.1148616985425438
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  5.]), 'previousTarget': array([13.,  5.]), 'currentState': array([13.80437573,  5.52341189,  3.9374692 ]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 0.9596771926837709}
episode index:160
target Thresh 26.831449139336822
target distance 12.0
model initialize at round 160
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 25.]), 'previousTarget': array([ 2., 25.]), 'currentState': array([ 3.68731958, 13.38988016,  2.19432507]), 'targetState': array([ 2, 25], dtype=int32), 'currentDistance': 11.732089760336102}
done in step count: 26
reward sum = 0.5670623051000838
running average episode reward sum: 0.11767039796215585
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 25.]), 'previousTarget': array([ 2., 25.]), 'currentState': array([ 1.76943282, 24.21824563,  1.67734404]), 'targetState': array([ 2, 25], dtype=int32), 'currentDistance': 0.8150466956184219}
episode index:161
target Thresh 26.8828770796763
target distance 6.0
model initialize at round 161
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([10.        , 24.        ,  0.55211949]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 6.324555320336759}
done in step count: 18
reward sum = 0.6846286645440488
running average episode reward sum: 0.12117014034846384
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([ 4.91859555, 22.41133951,  3.72657797]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 1.006487936698071}
episode index:162
target Thresh 26.933793303459463
target distance 14.0
model initialize at round 162
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  3.]), 'previousTarget': array([20.,  3.]), 'currentState': array([6.18907524, 3.39077608, 0.87058228]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 13.81645210287184}
done in step count: 32
reward sum = 0.49163282265628955
running average episode reward sum: 0.12344291754053638
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  3.]), 'previousTarget': array([20.,  3.]), 'currentState': array([19.30325386,  3.26096999,  6.23196846]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 0.7440164745027886}
episode index:163
target Thresh 26.98420290235112
target distance 13.0
model initialize at round 163
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 8.]), 'previousTarget': array([8., 8.]), 'currentState': array([ 6.00056237, 21.00102966,  0.9913072 ]), 'targetState': array([8, 8], dtype=int32), 'currentDistance': 13.15387863139503}
done in step count: 32
reward sum = 0.47412719416210347
running average episode reward sum: 0.125581236300424
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 8.]), 'previousTarget': array([8., 8.]), 'currentState': array([8.09357326, 8.77997724, 4.83471159]), 'targetState': array([8, 8], dtype=int32), 'currentDistance': 0.785570146302384}
episode index:164
target Thresh 27.034110917353168
target distance 10.0
model initialize at round 164
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'previousTarget': array([11., 21.]), 'currentState': array([ 3.03232589, 11.01236453,  0.58026513]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 12.776411581211818}
done in step count: 27
reward sum = 0.5538231039270141
running average episode reward sum: 0.12817664155876698
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'previousTarget': array([11., 21.]), 'currentState': array([10.16330594, 20.2799097 ,  0.75025289]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 1.1038962782807464}
episode index:165
target Thresh 27.083522339308693
target distance 6.0
model initialize at round 165
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  6.]), 'previousTarget': array([13.,  6.]), 'currentState': array([19.02709953,  6.03874753,  1.21298164]), 'targetState': array([13,  6], dtype=int32), 'currentDistance': 6.027224083622841}
done in step count: 14
reward sum = 0.7293589039798049
running average episode reward sum: 0.1317982214528696
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  6.]), 'previousTarget': array([13.,  6.]), 'currentState': array([13.8877246 ,  5.85552483,  3.52101898]), 'targetState': array([13,  6], dtype=int32), 'currentDistance': 0.899404272611368}
episode index:166
target Thresh 27.132442109401072
target distance 7.0
model initialize at round 166
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([6.01202242, 3.02029028, 1.28839439]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 7.052697149343287}
done in step count: 16
reward sum = 0.7327474814785246
running average episode reward sum: 0.13539672001589748
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([5.28840731, 9.11424036, 2.00312541]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.9315304169523712}
episode index:167
target Thresh 27.180875119648082
target distance 4.0
model initialize at round 167
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 24.]), 'previousTarget': array([21., 24.]), 'currentState': array([24.83332076, 27.66528495,  4.14437272]), 'targetState': array([21, 24], dtype=int32), 'currentDistance': 5.303646086759062}
done in step count: 12
reward sum = 0.8208517564192127
running average episode reward sum: 0.13947680951829816
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 24.]), 'previousTarget': array([21., 24.]), 'currentState': array([21.75591295, 24.83379595,  4.0294374 ]), 'targetState': array([21, 24], dtype=int32), 'currentDistance': 1.1254421711455982}
episode index:168
target Thresh 27.228826213391105
target distance 6.0
model initialize at round 168
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([22.        , 10.        ,  2.33661959]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 7.211102550927978}
done in step count: 16
reward sum = 0.7125085403869771
running average episode reward sum: 0.14286752981929626
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.91949429,  6.631428  ,  3.86030598]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.1154241674917584}
episode index:169
target Thresh 27.276300185779476
target distance 6.0
model initialize at round 169
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 22.]), 'previousTarget': array([18., 22.]), 'currentState': array([23.24741587, 27.83552212,  5.44944221]), 'targetState': array([18, 22], dtype=int32), 'currentDistance': 7.847846312275614}
done in step count: 16
reward sum = 0.6934148190425774
running average episode reward sum: 0.14610604328531557
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 22.]), 'previousTarget': array([18., 22.]), 'currentState': array([18.9822193 , 22.85864455,  3.76361102]), 'targetState': array([18, 22], dtype=int32), 'currentDistance': 1.3046168846426076}
episode index:170
target Thresh 27.323301784249992
target distance 20.0
model initialize at round 170
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 26.]), 'previousTarget': array([13.96680906, 25.77872706]), 'currentState': array([10.73230531,  6.41614293,  1.88993001]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 19.854603655671205}
done in step count: 42
reward sum = 0.3305236594329222
running average episode reward sum: 0.14718450887682205
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 26.]), 'previousTarget': array([14., 26.]), 'currentState': array([13.6054436 , 25.03074669,  1.20660843]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 1.046483032795901}
episode index:171
target Thresh 27.369835709001674
target distance 5.0
model initialize at round 171
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 21.]), 'previousTarget': array([ 6., 21.]), 'currentState': array([ 9.98083166, 15.94620122,  4.16882735]), 'targetState': array([ 6, 21], dtype=int32), 'currentDistance': 6.4333430485145415}
done in step count: 14
reward sum = 0.7179970473316507
running average episode reward sum: 0.15050318642597804
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 21.]), 'previousTarget': array([ 6., 21.]), 'currentState': array([ 6.70550451, 20.06504949,  2.16173172]), 'targetState': array([ 6, 21], dtype=int32), 'currentDistance': 1.171268147733436}
episode index:172
target Thresh 27.415906613465772
target distance 20.0
model initialize at round 172
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.50718977, 23.42273282]), 'previousTarget': array([13.53075311, 23.38463841]), 'currentState': array([24.98596854,  7.04477086,  1.80967043]), 'targetState': array([11, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 85
reward sum = 0.15978468469639667
running average episode reward sum: 0.1505568367049978
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 27.]), 'previousTarget': array([11., 27.]), 'currentState': array([11.25382141, 26.2444205 ,  2.11486434]), 'targetState': array([11, 27], dtype=int32), 'currentDistance': 0.7970731994843345}
episode index:173
target Thresh 27.461519104771128
target distance 11.0
model initialize at round 173
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 14.]), 'previousTarget': array([19., 14.]), 'currentState': array([11.        ,  3.        ,  0.24895897]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 13.601470508735444}
done in step count: 30
reward sum = 0.5124972807399695
running average episode reward sum: 0.15263695419945164
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 14.]), 'previousTarget': array([19., 14.]), 'currentState': array([18.21337467, 13.26209803,  0.83171812]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 1.0785539988239736}
episode index:174
target Thresh 27.50667774420488
target distance 4.0
model initialize at round 174
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 27.]), 'previousTarget': array([27., 27.]), 'currentState': array([23.04092804, 26.94562277,  5.57851866]), 'targetState': array([27, 27], dtype=int32), 'currentDistance': 3.9594453701701258}
done in step count: 8
reward sum = 0.8612811024274284
running average episode reward sum: 0.15668634933218295
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 27.]), 'previousTarget': array([27., 27.]), 'currentState': array([26.015584  , 26.80972996,  0.32351669]), 'targetState': array([27, 27], dtype=int32), 'currentDistance': 1.0026353009956637}
episode index:175
target Thresh 27.551387047668605
target distance 24.0
model initialize at round 175
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.16738911, 23.98266146]), 'previousTarget': array([26.16738911, 23.98266146]), 'currentState': array([27.        ,  4.        ,  0.13274699]), 'targetState': array([26, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 75
reward sum = 0.15846903579251126
running average episode reward sum: 0.15669647823252572
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 28.]), 'previousTarget': array([26., 28.]), 'currentState': array([25.59982011, 27.13633997,  1.40586377]), 'targetState': array([26, 28], dtype=int32), 'currentDistance': 0.9518679525631402}
episode index:176
target Thresh 27.595651486129906
target distance 5.0
model initialize at round 176
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 14.]), 'previousTarget': array([ 9., 14.]), 'currentState': array([ 4.        , 14.        ,  4.20751461]), 'targetState': array([ 9, 14], dtype=int32), 'currentDistance': 5.0}
done in step count: 13
reward sum = 0.7609234101022038
running average episode reward sum: 0.16011018971201543
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 14.]), 'previousTarget': array([ 9., 14.]), 'currentState': array([ 8.29086095, 14.20234279,  0.50907851]), 'targetState': array([ 9, 14], dtype=int32), 'currentDistance': 0.7374420611812148}
episode index:177
target Thresh 27.639475486069518
target distance 15.0
model initialize at round 177
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.07674355, 10.07996484]), 'previousTarget': array([13.35363499, 10.37889464]), 'currentState': array([26.92524918, 24.50975666,  4.33702126]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.30701501862287656
running average episode reward sum: 0.16093549773960455
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13.84394953, 10.11118234,  3.76238513]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.8512416389388656}
episode index:178
target Thresh 27.682863429923955
target distance 1.0
model initialize at round 178
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 28.]), 'previousTarget': array([14., 28.]), 'currentState': array([12.91652031, 27.85087814,  4.45157462]), 'targetState': array([14, 28], dtype=int32), 'currentDistance': 1.093693539289348}
done in step count: 3
reward sum = 0.9271460308598706
running average episode reward sum: 0.16521600351122614
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 28.]), 'previousTarget': array([14., 28.]), 'currentState': array([13.16074337, 27.43999088,  5.92700705]), 'targetState': array([14, 28], dtype=int32), 'currentDistance': 1.0089409838946897}
episode index:179
target Thresh 27.725819656523758
target distance 24.0
model initialize at round 179
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.13798208, 16.13509722]), 'previousTarget': array([17.14213562, 16.14213562]), 'currentState': array([2.99791474, 1.99089361, 4.73978186]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 87
reward sum = -0.12053284276022436
running average episode reward sum: 0.1636285099208292
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 26.]), 'previousTarget': array([27., 26.]), 'currentState': array([26.22803277, 26.19984352,  0.4267481 ]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 0.7974150993038562}
episode index:180
target Thresh 27.768348461527385
target distance 10.0
model initialize at round 180
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 14.]), 'previousTarget': array([24., 14.]), 'currentState': array([15.13950186,  4.07136535,  0.48832289]), 'targetState': array([24, 14], dtype=int32), 'currentDistance': 13.30737440090491}
done in step count: 30
reward sum = 0.5580035476085307
running average episode reward sum: 0.16580737753236346
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 14.]), 'previousTarget': array([24., 14.]), 'currentState': array([23.25711027, 13.07371588,  0.96725164]), 'targetState': array([24, 14], dtype=int32), 'currentDistance': 1.1873868066793498}
episode index:181
target Thresh 27.810454097850776
target distance 7.0
model initialize at round 181
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([8.65840216, 1.6798898 , 3.67324305]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 6.666092545566738}
done in step count: 14
reward sum = 0.750022621510231
running average episode reward sum: 0.16901735140037372
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.70177756, 1.78144135, 3.10030614]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.735023556438721}
episode index:182
target Thresh 27.852140776092654
target distance 8.0
model initialize at round 182
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  7.]), 'previousTarget': array([13.,  7.]), 'currentState': array([20.75591095,  7.56892807,  3.95904016]), 'targetState': array([13,  7], dtype=int32), 'currentDistance': 7.776749568016853}
done in step count: 17
reward sum = 0.7033861023479295
running average episode reward sum: 0.17193739921975926
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  7.]), 'previousTarget': array([13.,  7.]), 'currentState': array([13.93794959,  7.20209205,  3.38891972]), 'targetState': array([13,  7], dtype=int32), 'currentDistance': 0.9594741405763536}
episode index:183
target Thresh 27.893412664955576
target distance 15.0
model initialize at round 183
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([ 5.24283174, 28.56704643,  5.01346976]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 14.619968280391802}
done in step count: 33
reward sum = 0.4733342083486759
running average episode reward sum: 0.17357542535632947
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([ 4.22113474, 14.94819307,  4.33947542]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 0.973637856439784}
episode index:184
target Thresh 27.93427389166283
target distance 12.0
model initialize at round 184
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 27.]), 'previousTarget': array([22., 27.]), 'currentState': array([10.3751718 , 25.07931624,  0.29134172]), 'targetState': array([22, 27], dtype=int32), 'currentDistance': 11.78243000735484}
done in step count: 24
reward sum = 0.5825613167071154
running average episode reward sum: 0.17578615990417157
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 27.]), 'previousTarget': array([22., 27.]), 'currentState': array([21.12877145, 27.23370401,  6.21604235]), 'targetState': array([22, 27], dtype=int32), 'currentDistance': 0.9020292364017165}
episode index:185
target Thresh 27.974728542371132
target distance 11.0
model initialize at round 185
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 17.]), 'previousTarget': array([ 7., 17.]), 'currentState': array([11.04252857,  5.98025863,  6.10109329]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 11.737833573806569}
done in step count: 32
reward sum = 0.5304966885650053
running average episode reward sum: 0.17769320575718678
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 17.]), 'previousTarget': array([ 7., 17.]), 'currentState': array([ 7.19749491, 16.12015513,  1.95510104]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 0.9017379004015389}
episode index:186
target Thresh 28.01478066257927
target distance 5.0
model initialize at round 186
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 15.]), 'previousTarget': array([13., 15.]), 'currentState': array([18.        , 13.        ,  1.58223319]), 'targetState': array([13, 15], dtype=int32), 'currentDistance': 5.385164807134504}
done in step count: 13
reward sum = 0.7729317046954486
running average episode reward sum: 0.18087629933439678
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 15.]), 'previousTarget': array([13., 15.]), 'currentState': array([13.79705567, 14.62455205,  2.88962097]), 'targetState': array([13, 15], dtype=int32), 'currentDistance': 0.8810555581499704}
episode index:187
target Thresh 28.054434257532634
target distance 2.0
model initialize at round 187
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 20.]), 'previousTarget': array([ 6., 20.]), 'currentState': array([ 4.06632668, 19.93310302,  5.69546258]), 'targetState': array([ 6, 20], dtype=int32), 'currentDistance': 1.9348301498618394}
done in step count: 5
reward sum = 0.9286026648940751
running average episode reward sum: 0.18485356723630994
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 20.]), 'previousTarget': array([ 6., 20.]), 'currentState': array([ 5.1148482 , 19.88961125,  0.16091923]), 'targetState': array([ 6, 20], dtype=int32), 'currentDistance': 0.8920086268822951}
episode index:188
target Thresh 28.093693292623772
target distance 17.0
model initialize at round 188
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([25.00248564, 10.01539098,  1.66317904]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 17.463150545921504}
done in step count: 89
reward sum = 0.24473635145466413
running average episode reward sum: 0.18517040736444937
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([ 8.95817357, 13.64706016,  2.84245268]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 1.021108769478617}
episode index:189
target Thresh 28.132561693788904
target distance 7.0
model initialize at round 189
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([22.        , 14.        ,  2.06207931]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 9.899494936611665}
done in step count: 24
reward sum = 0.5930326385877668
running average episode reward sum: 0.18731705068667734
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.89674009,  7.1648751 ,  3.57515245]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.9117711277070533}
episode index:190
target Thresh 28.171043347900543
target distance 12.0
model initialize at round 190
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 17.]), 'previousTarget': array([23., 17.]), 'currentState': array([16.98574408, 28.99594641,  3.67112541]), 'targetState': array([23, 17], dtype=int32), 'currentDistance': 13.41916556643094}
done in step count: 36
reward sum = 0.4942710715577988
running average episode reward sum: 0.18892413980118583
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 17.]), 'previousTarget': array([23., 17.]), 'currentState': array([22.68306345, 17.79379382,  5.1324803 ]), 'targetState': array([23, 17], dtype=int32), 'currentDistance': 0.8547265106587594}
episode index:191
target Thresh 28.20914210315616
target distance 13.0
model initialize at round 191
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 20.]), 'previousTarget': array([21., 20.]), 'currentState': array([ 7.93025887, 23.99665673,  3.417867  ]), 'targetState': array([21, 20], dtype=int32), 'currentDistance': 13.667164962810231}
done in step count: 39
reward sum = 0.45839248704519686
running average episode reward sum: 0.19032762077641505
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 20.]), 'previousTarget': array([21., 20.]), 'currentState': array([20.05573072, 20.52299881,  5.70586724]), 'targetState': array([21, 20], dtype=int32), 'currentDistance': 1.0794314350435867}
episode index:192
target Thresh 28.246861769463035
target distance 16.0
model initialize at round 192
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 13.]), 'previousTarget': array([27., 13.]), 'currentState': array([19.        , 29.        ,  2.37578586]), 'targetState': array([27, 13], dtype=int32), 'currentDistance': 17.88854381999832}
done in step count: 99
reward sum = -0.07126771197081347
running average episode reward sum: 0.18897220454456412
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 13.]), 'previousTarget': array([27., 13.]), 'currentState': array([18.47568114, 24.26084373,  4.84248055]), 'targetState': array([27, 13], dtype=int32), 'currentDistance': 14.123406579069664}
episode index:193
target Thresh 28.28420611881923
target distance 8.0
model initialize at round 193
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([5.95557209, 3.49443876, 1.4511511 ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 11.002102973064119}
done in step count: 24
reward sum = 0.5722348994204429
running average episode reward sum: 0.19094778544598617
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.28794664, 11.5278744 ,  0.19730227]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.8863810531833504}
episode index:194
target Thresh 28.321178885690806
target distance 21.0
model initialize at round 194
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.34349072,  7.53990768]), 'previousTarget': array([16.01582747,  7.81486795]), 'currentState': array([ 3.47481006, 22.84993897,  5.81889558]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.19009169843346843
running average episode reward sum: 0.1889937368107069
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  2.]), 'previousTarget': array([21.,  2.]), 'currentState': array([ 8.10060574, 16.30079364,  4.78435591]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 19.258947813287133}
episode index:195
target Thresh 28.35778376738525
target distance 18.0
model initialize at round 195
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.13274091, 21.1386185 ]), 'previousTarget': array([23.14213562, 21.14213562]), 'currentState': array([8.97985437, 7.00724198, 2.54399633]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.15128504156096934
running average episode reward sum: 0.18725762059452492
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.51372357, 24.46286842]), 'previousTarget': array([26.51372357, 24.46286842]), 'currentState': array([13.09090057,  9.63627667,  0.16367546]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 20.0}
episode index:196
target Thresh 28.394024424421247
target distance 13.0
model initialize at round 196
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 15.]), 'previousTarget': array([11., 15.]), 'currentState': array([ 4.       , 28.       ,  2.1518454]), 'targetState': array([11, 15], dtype=int32), 'currentDistance': 14.7648230602334}
done in step count: 43
reward sum = 0.40187872320611157
running average episode reward sum: 0.18834706781590352
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 15.]), 'previousTarget': array([11., 15.]), 'currentState': array([11.27919427, 15.92666758,  4.71896833]), 'targetState': array([11, 15], dtype=int32), 'currentDistance': 0.9678131288220491}
episode index:197
target Thresh 28.429904480894695
target distance 11.0
model initialize at round 197
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([22.71494688, 19.59006861,  4.0120893 ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 13.733148417287994}
done in step count: 32
reward sum = 0.5159082638821678
running average episode reward sum: 0.19000141729098566
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.92133404, 11.61115455,  3.94193907]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.105606756892831}
episode index:198
target Thresh 28.465427524841143
target distance 14.0
model initialize at round 198
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([ 4.       , 22.       ,  4.0213294]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 17.204650534085253}
done in step count: 50
reward sum = 0.3790208104485947
running average episode reward sum: 0.19095126348775754
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.51187443,  8.79790346,  5.26562273]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.9353697167785273}
episode index:199
target Thresh 28.50059710859459
target distance 6.0
model initialize at round 199
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([23.03700975, 15.97307711,  5.83680078]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 7.855917540908311}
done in step count: 20
reward sum = 0.6548019331662615
running average episode reward sum: 0.19327051683615007
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([17.77667465, 20.08631448,  2.45188362]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 1.1991850274578189}
episode index:200
target Thresh 28.535416749142716
target distance 16.0
model initialize at round 200
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 23.]), 'previousTarget': array([21., 23.]), 'currentState': array([ 4.84331091, 16.43518784,  1.68222162]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 17.439477089213973}
done in step count: 42
reward sum = 0.3718576886557584
running average episode reward sum: 0.19415901022828744
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 23.]), 'previousTarget': array([21., 23.]), 'currentState': array([20.22248695, 22.72896621,  0.24835961]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 0.8233989637572929}
episode index:201
target Thresh 28.56988992847859
target distance 17.0
model initialize at round 201
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 12.]), 'previousTarget': array([25., 12.]), 'currentState': array([8.02448008, 5.9931131 , 6.15456298]), 'targetState': array([25, 12], dtype=int32), 'currentDistance': 18.006969954727733}
done in step count: 61
reward sum = 0.3652843555680465
running average episode reward sum: 0.1950061654032367
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 12.]), 'previousTarget': array([25., 12.]), 'currentState': array([24.0412318 , 11.56777156,  0.6009271 ]), 'targetState': array([25, 12], dtype=int32), 'currentDistance': 1.0516928637029448}
episode index:202
target Thresh 28.604020093948883
target distance 14.0
model initialize at round 202
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'previousTarget': array([19.,  3.]), 'currentState': array([10.46406774, 17.09863117,  6.26563657]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 16.481308819092906}
done in step count: 38
reward sum = 0.4276868741791742
running average episode reward sum: 0.19615237579129552
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'previousTarget': array([19.,  3.]), 'currentState': array([18.53825645,  3.88258377,  5.25553693]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 0.9960729027270592}
episode index:203
target Thresh 28.637810658598575
target distance 5.0
model initialize at round 203
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 17.]), 'previousTarget': array([12., 17.]), 'currentState': array([ 7.00480481, 11.99345205,  5.57127148]), 'targetState': array([12, 17], dtype=int32), 'currentDistance': 7.072304957218501}
done in step count: 15
reward sum = 0.731330423862463
running average episode reward sum: 0.19877579759556593
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 17.]), 'previousTarget': array([12., 17.]), 'currentState': array([11.02932859, 16.25030943,  0.93068369]), 'targetState': array([12, 17], dtype=int32), 'currentDistance': 1.2264741891970603}
episode index:204
target Thresh 28.671265001512296
target distance 6.0
model initialize at round 204
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 16.]), 'previousTarget': array([13., 16.]), 'currentState': array([18.02711602, 21.99531659,  5.89766565]), 'targetState': array([13, 16], dtype=int32), 'currentDistance': 7.824047320768953}
done in step count: 20
reward sum = 0.6603028243393962
running average episode reward sum: 0.20102714894553583
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 16.]), 'previousTarget': array([13., 16.]), 'currentState': array([13.93524717, 16.22891698,  3.7594363 ]), 'targetState': array([13, 16], dtype=int32), 'currentDistance': 0.9628552649469764}
episode index:205
target Thresh 28.70438646815221
target distance 19.0
model initialize at round 205
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.38900334, 12.30356511]), 'previousTarget': array([22.39288577, 12.30234469]), 'currentState': array([ 9.98757859, 27.99447535,  3.81258726]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.13671514250863195
running average episode reward sum: 0.1993876232588651
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([21.30143155, 13.60429799,  5.40629361]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 5.905841902268468}
episode index:206
target Thresh 28.73717837069259
target distance 8.0
model initialize at round 206
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  6.]), 'previousTarget': array([10.,  6.]), 'currentState': array([ 8.97880299, 14.04934194,  2.21430165]), 'targetState': array([10,  6], dtype=int32), 'currentDistance': 8.113861538267388}
done in step count: 21
reward sum = 0.6361832217545884
running average episode reward sum: 0.20149774692309563
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  6.]), 'previousTarget': array([10.,  6.]), 'currentState': array([10.16440243,  6.94217062,  4.71389281]), 'targetState': array([10,  6], dtype=int32), 'currentDistance': 0.9564066237061027}
episode index:207
target Thresh 28.769643988351007
target distance 8.0
model initialize at round 207
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 20.]), 'previousTarget': array([ 4., 20.]), 'currentState': array([12.00193193, 25.06154579,  1.78093582]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 9.468376859226618}
done in step count: 24
reward sum = 0.6112976212187815
running average episode reward sum: 0.20346793862644028
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 20.]), 'previousTarget': array([ 4., 20.]), 'currentState': array([ 4.8772663 , 20.17006786,  3.64461871]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 0.8935990416602267}
episode index:208
target Thresh 28.80178656771629
target distance 16.0
model initialize at round 208
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 26.]), 'previousTarget': array([27., 26.]), 'currentState': array([20.98265971,  9.9539792 ,  4.56756577]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 17.137186690593218}
done in step count: 47
reward sum = 0.3432218381238505
running average episode reward sum: 0.20413661757140397
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 26.]), 'previousTarget': array([27., 26.]), 'currentState': array([26.8604252 , 25.0679218 ,  1.36691266]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 0.9424706321205967}
episode index:209
target Thresh 28.833609323073155
target distance 10.0
model initialize at round 209
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 16.]), 'previousTarget': array([11., 16.]), 'currentState': array([20.54353867,  5.80900669,  3.28537822]), 'targetState': array([11, 16], dtype=int32), 'currentDistance': 13.961929484771387}
done in step count: 30
reward sum = 0.48631497331194207
running average episode reward sum: 0.20548032402731128
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 16.]), 'previousTarget': array([11., 16.]), 'currentState': array([11.41062069, 15.10588306,  1.87210873]), 'targetState': array([11, 16], dtype=int32), 'currentDistance': 0.9838975813252323}
episode index:210
target Thresh 28.865115436723663
target distance 16.0
model initialize at round 210
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 19.]), 'previousTarget': array([ 8., 19.]), 'currentState': array([23.52305137, 18.85667304,  3.27767953]), 'targetState': array([ 8, 19], dtype=int32), 'currentDistance': 15.52371303206332}
done in step count: 34
reward sum = 0.46901905904345453
running average episode reward sum: 0.20672932277146364
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 19.]), 'previousTarget': array([ 8., 19.]), 'currentState': array([ 8.9368064 , 18.17850494,  2.47436979]), 'targetState': array([ 8, 19], dtype=int32), 'currentDistance': 1.2459776682146786}
episode index:211
target Thresh 28.89630805930543
target distance 12.0
model initialize at round 211
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 14.]), 'previousTarget': array([ 5., 14.]), 'currentState': array([10.02496273,  2.08569113,  1.53983045]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 12.930622812858818}
done in step count: 27
reward sum = 0.5417106994814948
running average episode reward sum: 0.2083094236050015
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 14.]), 'previousTarget': array([ 5., 14.]), 'currentState': array([ 4.91470861, 13.0506294 ,  1.3156819 ]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 0.9531941880319421}
episode index:212
target Thresh 28.92719031010671
target distance 10.0
model initialize at round 212
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([24.04303035,  5.93954935,  5.0785079 ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 10.098862024938736}
done in step count: 24
reward sum = 0.5752552833196006
running average episode reward sum: 0.2100321741200935
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.92105997,  6.88218393,  2.85023735]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.9285645334989702}
episode index:213
target Thresh 28.957765277378314
target distance 18.0
model initialize at round 213
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([ 2.97946811, 15.02475035,  2.03046519]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 18.27262116740406}
done in step count: 45
reward sum = 0.3491202297050336
running average episode reward sum: 0.2106821183050699
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([20.02841168, 12.82695997,  5.65810409]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 1.2758709383461762}
episode index:214
target Thresh 28.988036018642454
target distance 8.0
model initialize at round 214
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'previousTarget': array([13., 22.]), 'currentState': array([19.54604097, 14.20860482,  2.64038981]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 10.176270994480218}
done in step count: 24
reward sum = 0.6325763229827222
running average episode reward sum: 0.21264441693147756
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'previousTarget': array([13., 22.]), 'currentState': array([13.64234298, 21.02468538,  1.80560169]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 1.1678369426081137}
episode index:215
target Thresh 29.01800556099848
target distance 3.0
model initialize at round 215
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([ 5.98931798, 24.05219632,  1.62996578]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 4.287202891643421}
done in step count: 16
reward sum = 0.7617191264990085
running average episode reward sum: 0.21518642947577168
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([ 8.74873677, 21.95765678,  5.03777629]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 0.9900705602146865}
episode index:216
target Thresh 29.0476769014256
target distance 13.0
model initialize at round 216
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([24.99596995, 19.92331451,  4.40829593]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 13.04049411028276}
done in step count: 34
reward sum = 0.5028907871884342
running average episode reward sum: 0.21651225600900975
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([12.9744377 , 20.09167682,  2.23626768]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 1.3321335585554526}
episode index:217
target Thresh 29.077053007082586
target distance 20.0
model initialize at round 217
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9.8507125, 8.59715  ]), 'previousTarget': array([9.8507125, 8.59715  ]), 'currentState': array([ 5.        , 28.        ,  3.62791634]), 'targetState': array([10,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 80
reward sum = 0.24294696127207735
running average episode reward sum: 0.2166335161248954
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  8.]), 'previousTarget': array([10.,  8.]), 'currentState': array([10.81081153,  8.98287027,  4.17814473]), 'targetState': array([10,  8], dtype=int32), 'currentDistance': 1.2741464954314845}
episode index:218
target Thresh 29.106136815604483
target distance 5.0
model initialize at round 218
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 13.]), 'previousTarget': array([17., 13.]), 'currentState': array([17.74772791,  8.39879953,  2.12540942]), 'targetState': array([17, 13], dtype=int32), 'currentDistance': 4.661560125129983}
done in step count: 18
reward sum = 0.7734204256925656
running average episode reward sum: 0.21917592210465642
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 13.]), 'previousTarget': array([17., 13.]), 'currentState': array([17.02502516, 12.05006419,  1.34775296]), 'targetState': array([17, 13], dtype=int32), 'currentDistance': 0.9502653849014513}
episode index:219
target Thresh 29.13493123539638
target distance 14.0
model initialize at round 219
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 26.]), 'previousTarget': array([13., 26.]), 'currentState': array([20.5320989 , 12.16422871,  2.57932445]), 'targetState': array([13, 26], dtype=int32), 'currentDistance': 15.753129244410367}
done in step count: 43
reward sum = 0.40470101368254907
running average episode reward sum: 0.22001921797546503
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 26.]), 'previousTarget': array([13., 26.]), 'currentState': array([12.15405662, 25.03536021,  1.07097066]), 'targetState': array([13, 26], dtype=int32), 'currentDistance': 1.2830238213265863}
episode index:220
target Thresh 29.16343914592425
target distance 15.0
model initialize at round 220
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 27.]), 'previousTarget': array([27., 27.]), 'currentState': array([27.        , 12.        ,  0.51487069]), 'targetState': array([27, 27], dtype=int32), 'currentDistance': 14.999999999999998}
done in step count: 43
reward sum = 0.455473782959289
running average episode reward sum: 0.22108462324688505
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 27.]), 'previousTarget': array([27., 27.]), 'currentState': array([26.35802472, 26.02941885,  1.23143785]), 'targetState': array([27, 27], dtype=int32), 'currentDistance': 1.163683815526142}
episode index:221
target Thresh 29.19166339800291
target distance 6.0
model initialize at round 221
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  2.]), 'previousTarget': array([19.,  2.]), 'currentState': array([13.42389518,  7.93910436,  6.12237325]), 'targetState': array([19,  2], dtype=int32), 'currentDistance': 8.14652720872864}
done in step count: 25
reward sum = 0.622892460666756
running average episode reward sum: 0.22289456846048805
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  2.]), 'previousTarget': array([19.,  2.]), 'currentState': array([19.06057993,  2.91364026,  4.30773691]), 'targetState': array([19,  2], dtype=int32), 'currentDistance': 0.9156464678401691}
episode index:222
target Thresh 29.219606814081075
target distance 5.0
model initialize at round 222
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 12.]), 'previousTarget': array([ 3., 12.]), 'currentState': array([8.02466744, 6.99288776, 6.23045337]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 7.093550297127856}
done in step count: 21
reward sum = 0.6799360292653542
running average episode reward sum: 0.22494408173763994
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 12.]), 'previousTarget': array([ 3., 12.]), 'currentState': array([ 3.79598241, 11.16240505,  2.625816  ]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 1.1554883400521476}
episode index:223
target Thresh 29.24727218852365
target distance 22.0
model initialize at round 223
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7.94666312, 5.91924506]), 'previousTarget': array([8.05572809, 6.11145618]), 'currentState': array([16.8435208 , 23.83141755,  3.82941586]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.11231610099684995
running average episode reward sum: 0.22343845592186096
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 2.]), 'previousTarget': array([6., 2.]), 'currentState': array([ 8.60166398, 10.81009077,  4.69416016]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 9.186204593789258}
episode index:224
target Thresh 29.27466228789113
target distance 18.0
model initialize at round 224
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 29.]), 'previousTarget': array([ 2.05572809, 28.88854382]), 'currentState': array([10.64097616, 11.34656557,  2.26343779]), 'targetState': array([ 2, 29], dtype=int32), 'currentDistance': 19.65477591457543}
done in step count: 53
reward sum = 0.32014657757539955
running average episode reward sum: 0.22386826979587668
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 29.]), 'previousTarget': array([ 2., 29.]), 'currentState': array([ 1.26606854, 28.03745783,  0.99568034]), 'targetState': array([ 2, 29], dtype=int32), 'currentDistance': 1.2104308378399216}
episode index:225
target Thresh 29.301779851216274
target distance 13.0
model initialize at round 225
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  2.]), 'previousTarget': array([18.,  2.]), 'currentState': array([5.        , 4.        , 2.55644277]), 'targetState': array([18,  2], dtype=int32), 'currentDistance': 13.152946437965904}
done in step count: 40
reward sum = 0.43341613065626683
running average episode reward sum: 0.22479547272003772
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  2.]), 'previousTarget': array([18.,  2.]), 'currentState': array([17.02194591,  2.16721968,  5.63564417]), 'targetState': array([18,  2], dtype=int32), 'currentDistance': 0.9922460446888544}
episode index:226
target Thresh 29.328627590278014
target distance 13.0
model initialize at round 226
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 26.]), 'previousTarget': array([24., 26.]), 'currentState': array([15.        , 13.        ,  5.19009894]), 'targetState': array([24, 26], dtype=int32), 'currentDistance': 15.811388300841898}
done in step count: 40
reward sum = 0.41504727052637114
running average episode reward sum: 0.22563358636676165
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 26.]), 'previousTarget': array([24., 26.]), 'currentState': array([23.03010425, 25.59972608,  0.4025404 ]), 'targetState': array([24, 26], dtype=int32), 'currentDistance': 1.049245906100129}
episode index:227
target Thresh 29.355208189872634
target distance 18.0
model initialize at round 227
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 7.]), 'previousTarget': array([8., 7.]), 'currentState': array([25.99452543,  7.01539583,  2.14450374]), 'targetState': array([8, 7], dtype=int32), 'currentDistance': 17.994532014803205}
done in step count: 49
reward sum = 0.3486654397917806
running average episode reward sum: 0.22617319975897665
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 7.]), 'previousTarget': array([8., 7.]), 'currentState': array([8.12704315, 6.0490876 , 2.11001771]), 'targetState': array([8, 7], dtype=int32), 'currentDistance': 0.9593614306144566}
episode index:228
target Thresh 29.38152430808224
target distance 19.0
model initialize at round 228
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 27.]), 'previousTarget': array([ 7., 27.]), 'currentState': array([9.86007181, 8.47793589, 1.69898766]), 'targetState': array([ 7, 27], dtype=int32), 'currentDistance': 18.741581299985317}
done in step count: 60
reward sum = 0.3403472650335007
running average episode reward sum: 0.22667177646323222
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 27.]), 'previousTarget': array([ 7., 27.]), 'currentState': array([ 6.05195882, 26.32576583,  1.08831109]), 'targetState': array([ 7, 27], dtype=int32), 'currentDistance': 1.1633459451472237}
episode index:229
target Thresh 29.407578576540587
target distance 15.0
model initialize at round 229
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([23.0155007 ,  7.97131826,  4.96115226]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 15.317903722826129}
done in step count: 38
reward sum = 0.3880681002787871
running average episode reward sum: 0.22737349961025635
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.12678352, 10.02041787,  2.09722332]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.9877526097233108}
episode index:230
target Thresh 29.433373600696225
target distance 5.0
model initialize at round 230
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([21.        , 15.        ,  1.17048019]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 6.403124237432849}
done in step count: 23
reward sum = 0.6710152584169021
running average episode reward sum: 0.22929402670465743
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.95546324, 11.23329903,  3.80738751]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.9835336528279645}
episode index:231
target Thresh 29.45891196007307
target distance 11.0
model initialize at round 231
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 26.]), 'previousTarget': array([14., 26.]), 'currentState': array([ 3.        , 26.        ,  1.44490552]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 11.0}
done in step count: 26
reward sum = 0.5718623957617592
running average episode reward sum: 0.23077061450231737
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 26.]), 'previousTarget': array([14., 26.]), 'currentState': array([13.39037694, 26.98393808,  5.76580541]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 1.1574862524144824}
episode index:232
target Thresh 29.484196208528346
target distance 18.0
model initialize at round 232
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 24.]), 'previousTarget': array([20., 24.]), 'currentState': array([ 2.01295103, 26.98527255,  5.52478698]), 'targetState': array([20, 24], dtype=int32), 'currentDistance': 18.233095808818934}
done in step count: 66
reward sum = 0.348608250674028
running average episode reward sum: 0.23127635543009292
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 24.]), 'previousTarget': array([20., 24.]), 'currentState': array([19.25351679, 24.99631781,  5.85175714]), 'targetState': array([20, 24], dtype=int32), 'currentDistance': 1.2449443171539982}
episode index:233
target Thresh 29.50922887450796
target distance 15.0
model initialize at round 233
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 25.]), 'previousTarget': array([ 9., 25.]), 'currentState': array([12.78734013, 10.43328561,  1.94429094]), 'targetState': array([ 9, 25], dtype=int32), 'currentDistance': 15.051017019079287}
done in step count: 36
reward sum = 0.4822815968338774
running average episode reward sum: 0.23234902740190397
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 25.]), 'previousTarget': array([ 9., 25.]), 'currentState': array([ 8.65217139, 24.14047215,  1.55127026]), 'targetState': array([ 9, 25], dtype=int32), 'currentDistance': 0.927239383608261}
episode index:234
target Thresh 29.534012461299376
target distance 13.0
model initialize at round 234
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  6.]), 'previousTarget': array([10.,  6.]), 'currentState': array([22.99590873, 17.0000832 ,  3.34623456]), 'targetState': array([10,  6], dtype=int32), 'currentDistance': 17.026317108048005}
done in step count: 49
reward sum = 0.38589412402325796
running average episode reward sum: 0.23300241079178208
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  6.]), 'previousTarget': array([10.,  6.]), 'currentState': array([10.93145533,  5.94287193,  3.04674252]), 'targetState': array([10,  6], dtype=int32), 'currentDistance': 0.9332055743684962}
episode index:235
target Thresh 29.55854944728193
target distance 15.0
model initialize at round 235
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.64776312,  6.37722968]), 'previousTarget': array([15.64636501,  6.37889464]), 'currentState': array([ 1.99817073, 20.99532216,  4.58206236]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.11132890106677666
running average episode reward sum: 0.23154337980933057
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([14.0324619 , 10.53249318,  5.91728119]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 4.941123418989306}
episode index:236
target Thresh 29.58284228617466
target distance 19.0
model initialize at round 236
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([25.04411116, 24.02047341,  0.21605623]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 19.677943643841708}
done in step count: 63
reward sum = 0.28415079247892977
running average episode reward sum: 0.23176535201468754
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([20.14179724,  5.97123917,  4.6227627 ]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 0.981535521237417}
episode index:237
target Thresh 29.606893407281703
target distance 20.0
model initialize at round 237
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.77918929, 20.9026903 ]), 'previousTarget': array([23.76887233, 20.89976701]), 'currentState': array([7.01931708, 9.98908945, 6.02154016]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.08670427557484413
running average episode reward sum: 0.2304272443357399
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 23.]), 'previousTarget': array([27., 23.]), 'currentState': array([20.15362426, 16.76227383,  0.75988543]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 9.261862039424074}
episode index:238
target Thresh 29.63070521573521
target distance 2.0
model initialize at round 238
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([16.        , 13.        ,  0.58710268]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 2.0}
done in step count: 8
reward sum = 0.905051777574351
running average episode reward sum: 0.23324994112753328
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([16.32110651, 14.17096968,  1.53316478]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 0.8890448044667532}
episode index:239
target Thresh 29.654280092735878
target distance 4.0
model initialize at round 239
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 22.]), 'previousTarget': array([ 9., 22.]), 'currentState': array([13.03108474, 19.06741162,  1.39123482]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 4.984949231238249}
done in step count: 12
reward sum = 0.8068476507000294
running average episode reward sum: 0.23563993158408533
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 22.]), 'previousTarget': array([ 9., 22.]), 'currentState': array([ 9.93342095, 21.2948233 ,  2.9130741 ]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 1.169849925205269}
episode index:240
target Thresh 29.67762039579104
target distance 16.0
model initialize at round 240
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 27.]), 'previousTarget': array([17., 27.]), 'currentState': array([27.02957834, 10.99340084,  0.03298729]), 'targetState': array([17, 27], dtype=int32), 'currentDistance': 18.889247160233644}
done in step count: 84
reward sum = 0.2625444970116252
running average episode reward sum: 0.23575156878502948
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 27.]), 'previousTarget': array([17., 27.]), 'currentState': array([17.53231271, 26.11203295,  2.17114873]), 'targetState': array([17, 27], dtype=int32), 'currentDistance': 1.0352981763846625}
episode index:241
target Thresh 29.70072845895046
target distance 16.0
model initialize at round 241
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5.82990784, 17.05153389]), 'previousTarget': array([ 5.82990784, 17.05153389]), 'currentState': array([19.        ,  2.        ,  4.83634579]), 'targetState': array([ 5, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 85
reward sum = 0.1910025396942131
running average episode reward sum: 0.23556665544167898
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 18.]), 'previousTarget': array([ 5., 18.]), 'currentState': array([ 4.79191792, 17.10966384,  1.83243382]), 'targetState': array([ 5, 18], dtype=int32), 'currentDistance': 0.9143285173015349}
episode index:242
target Thresh 29.723606593039708
target distance 10.0
model initialize at round 242
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 17.]), 'previousTarget': array([20., 17.]), 'currentState': array([ 9.99502915, 10.95651404,  4.85107374]), 'targetState': array([20, 17], dtype=int32), 'currentDistance': 11.688591201434663}
done in step count: 41
reward sum = 0.4938245371042802
running average episode reward sum: 0.2366294450781506
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 17.]), 'previousTarget': array([20., 17.]), 'currentState': array([19.27903293, 16.00592613,  0.67577677]), 'targetState': array([20, 17], dtype=int32), 'currentDistance': 1.2279969008890605}
episode index:243
target Thresh 29.746257085891262
target distance 8.0
model initialize at round 243
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  6.]), 'previousTarget': array([24.,  6.]), 'currentState': array([16.        ,  9.        ,  5.12342215]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 8.544003745317532}
done in step count: 21
reward sum = 0.6864738368522731
running average episode reward sum: 0.2384730696346019
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  6.]), 'previousTarget': array([24.,  6.]), 'currentState': array([23.1149612 ,  6.65117648,  6.14422646]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 1.09878318906281}
episode index:244
target Thresh 29.768682202573277
target distance 21.0
model initialize at round 244
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.32455532, 23.97366596]), 'previousTarget': array([26.32455532, 23.97366596]), 'currentState': array([20.        ,  5.        ,  5.14919043]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 83
reward sum = 0.20310950926886187
running average episode reward sum: 0.2383287285718846
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 26.]), 'previousTarget': array([27., 26.]), 'currentState': array([26.04784284, 25.45830049,  0.85291712]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 1.0954641161538585}
episode index:245
target Thresh 29.790884185616115
target distance 1.0
model initialize at round 245
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 14.]), 'previousTarget': array([27., 14.]), 'currentState': array([27.00411596, 12.98489372,  4.95244367]), 'targetState': array([27, 14], dtype=int32), 'currentDistance': 1.0151146240667168}
done in step count: 10
reward sum = 0.8817726285624973
running average episode reward sum: 0.24094435418160254
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 14.]), 'previousTarget': array([27., 14.]), 'currentState': array([26.9636999 , 13.00070462,  2.63021787]), 'targetState': array([27, 14], dtype=int32), 'currentDistance': 0.9999544768980263}
episode index:246
target Thresh 29.812865255236577
target distance 20.0
model initialize at round 246
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.94277086, 24.71162596]), 'previousTarget': array([18.9223227 , 24.61161351]), 'currentState': array([15.04960394,  5.09420382,  1.15248361]), 'targetState': array([19, 25], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 62
reward sum = 0.33911611749881077
running average episode reward sum: 0.24134181071325117
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 25.]), 'previousTarget': array([19., 25.]), 'currentState': array([18.81620036, 24.01792936,  1.26082849]), 'targetState': array([19, 25], dtype=int32), 'currentDistance': 0.9991221432425873}
episode index:247
target Thresh 29.834627609559945
target distance 13.0
model initialize at round 247
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 29.]), 'previousTarget': array([23., 29.]), 'currentState': array([10.30940098, 26.01839431,  0.12548963]), 'targetState': array([23, 29], dtype=int32), 'currentDistance': 13.036152650824116}
done in step count: 34
reward sum = 0.5481659810796848
running average episode reward sum: 0.24257900494859969
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 29.]), 'previousTarget': array([23., 29.]), 'currentState': array([22.04785547, 28.23896131,  0.78585439]), 'targetState': array([23, 29], dtype=int32), 'currentDistance': 1.2189171812563016}
episode index:248
target Thresh 29.856173424839785
target distance 23.0
model initialize at round 248
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.95036054, 21.52740918]), 'previousTarget': array([23.7042351, 21.5731765]), 'currentState': array([ 4.24290207, 24.93564632,  5.96849804]), 'targetState': array([27, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 91
reward sum = 0.24405722441059546
running average episode reward sum: 0.24258494157294505
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 21.]), 'previousTarget': array([27., 21.]), 'currentState': array([26.14338922, 21.05529419,  0.26569219]), 'targetState': array([27, 21], dtype=int32), 'currentDistance': 0.8583935398352847}
episode index:249
target Thresh 29.87750485567558
target distance 7.0
model initialize at round 249
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 24.]), 'previousTarget': array([ 5., 24.]), 'currentState': array([ 8.        , 17.        ,  6.14751101]), 'targetState': array([ 5, 24], dtype=int32), 'currentDistance': 7.615773105863909}
done in step count: 24
reward sum = 0.6621835212074944
running average episode reward sum: 0.24426333589148325
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 24.]), 'previousTarget': array([ 5., 24.]), 'currentState': array([ 5.9247666 , 23.33786236,  2.26741441]), 'targetState': array([ 5, 24], dtype=int32), 'currentDistance': 1.1373739560743454}
episode index:250
target Thresh 29.898624035228192
target distance 17.0
model initialize at round 250
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 19.]), 'previousTarget': array([ 7., 19.]), 'currentState': array([24.02509451, 27.9597142 ,  5.01698542]), 'targetState': array([ 7, 19], dtype=int32), 'currentDistance': 19.23877131172558}
done in step count: 48
reward sum = 0.33236821710634173
running average episode reward sum: 0.24461435135449067
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 19.]), 'previousTarget': array([ 7., 19.]), 'currentState': array([ 7.86817231, 19.77307294,  3.90393793]), 'targetState': array([ 7, 19], dtype=int32), 'currentDistance': 1.162482229039872}
episode index:251
target Thresh 29.919533075433172
target distance 13.0
model initialize at round 251
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([10.96110613, 23.68231363,  4.71990609]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 12.724793883340922}
done in step count: 32
reward sum = 0.5468661251373353
running average episode reward sum: 0.24581376315521622
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.67065771, 11.95767559,  4.90175734]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.0127234985573021}
episode index:252
target Thresh 29.940234067211968
target distance 7.0
model initialize at round 252
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 19.]), 'previousTarget': array([20., 19.]), 'currentState': array([15.33993639, 25.75805707,  5.44996032]), 'targetState': array([20, 19], dtype=int32), 'currentDistance': 8.208990689814819}
done in step count: 25
reward sum = 0.6716536749995093
running average episode reward sum: 0.24749692486211067
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 19.]), 'previousTarget': array([20., 19.]), 'currentState': array([19.78138948, 19.86663846,  5.04633607]), 'targetState': array([20, 19], dtype=int32), 'currentDistance': 0.8937856477731673}
episode index:253
target Thresh 29.96072908068101
target distance 17.0
model initialize at round 253
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.87889489, 14.08255678]), 'previousTarget': array([22.79140314, 14.13497444]), 'currentState': array([ 6.35340281, 25.34790683,  0.55368522]), 'targetState': array([23, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.29787832847833523
running average episode reward sum: 0.2476952768448517
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 14.]), 'previousTarget': array([23., 14.]), 'currentState': array([22.14756275, 14.56954355,  5.78218497]), 'targetState': array([23, 14], dtype=int32), 'currentDistance': 1.0251971174159333}
episode index:254
target Thresh 29.98102016535872
target distance 19.0
model initialize at round 254
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  8.]), 'previousTarget': array([12.,  8.]), 'currentState': array([14.        , 27.        ,  3.44328189]), 'targetState': array([12,  8], dtype=int32), 'currentDistance': 19.104973174542803}
done in step count: 56
reward sum = 0.34600192276015673
running average episode reward sum: 0.2480807931033431
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  8.]), 'previousTarget': array([12.,  8.]), 'currentState': array([12.16723615,  8.97781816,  4.47148419]), 'targetState': array([12,  8], dtype=int32), 'currentDistance': 0.9920162700974202}
episode index:255
target Thresh 30.001109350370477
target distance 11.0
model initialize at round 255
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 18.]), 'previousTarget': array([13., 18.]), 'currentState': array([11.       ,  7.       ,  4.2703895]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 11.180339887498947}
done in step count: 28
reward sum = 0.5346242698666294
running average episode reward sum: 0.2492001035594497
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 18.]), 'previousTarget': array([13., 18.]), 'currentState': array([12.59141175, 17.01854653,  1.32137818]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 1.0631064272716069}
episode index:256
target Thresh 30.020998644651527
target distance 18.0
model initialize at round 256
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 20.]), 'previousTarget': array([ 3., 20.]), 'currentState': array([8.        , 2.        , 3.67353025]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 18.681541692269406}
done in step count: 48
reward sum = 0.33780742984001855
running average episode reward sum: 0.24954487914809004
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 20.]), 'previousTarget': array([ 3., 20.]), 'currentState': array([ 3.13996578, 19.11119099,  1.70945482]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 0.8997621207692}
episode index:257
target Thresh 30.040690037147865
target distance 10.0
model initialize at round 257
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([22.01011843, 13.98763085,  5.15415999]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 12.806432670723057}
done in step count: 32
reward sum = 0.5178350300930487
running average episode reward sum: 0.25058476345407826
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([12.88757418,  6.14790735,  3.36524356]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 0.8998135921234491}
episode index:258
target Thresh 30.060185497015155
target distance 19.0
model initialize at round 258
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 26.]), 'previousTarget': array([11., 26.]), 'currentState': array([14.66111124,  7.33521751,  2.14055523]), 'targetState': array([11, 26], dtype=int32), 'currentDistance': 19.020458482128163}
done in step count: 53
reward sum = 0.36283905062683086
running average episode reward sum: 0.2510181776902665
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 26.]), 'previousTarget': array([11., 26.]), 'currentState': array([11.62549441, 25.05197691,  1.97500372]), 'targetState': array([11, 26], dtype=int32), 'currentDistance': 1.1357777200940145}
episode index:259
target Thresh 30.07948697381563
target distance 21.0
model initialize at round 259
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.04820293,  9.01794307]), 'previousTarget': array([15.04869701,  9.02263725]), 'currentState': array([15.99420833, 28.99555738,  4.04384851]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.3164332180553072
running average episode reward sum: 0.25126977399936284
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.08045112,  8.85085867,  4.58767853]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.8546536471874375}
episode index:260
target Thresh 30.098596397713052
target distance 6.0
model initialize at round 260
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 16.]), 'previousTarget': array([21., 16.]), 'currentState': array([17.97292225, 21.97231655,  4.19055104]), 'targetState': array([21, 16], dtype=int32), 'currentDistance': 6.695652672618309}
done in step count: 16
reward sum = 0.7385911932905157
running average episode reward sum: 0.2531369058740416
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 16.]), 'previousTarget': array([21., 16.]), 'currentState': array([20.7466718 , 16.83917641,  5.00664453]), 'targetState': array([21, 16], dtype=int32), 'currentDistance': 0.8765798453907948}
episode index:261
target Thresh 30.11751567966574
target distance 23.0
model initialize at round 261
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.09211044, 15.8764257 ]), 'previousTarget': array([17.09211044, 15.8764257 ]), 'currentState': array([ 2.        , 29.        ,  5.13586494]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 99
reward sum = -0.14726332679070608
running average episode reward sum: 0.25160866071119903
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([18.03477912, 13.53048414,  5.64223741]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 8.309006470312799}
episode index:262
target Thresh 30.13624671161765
target distance 17.0
model initialize at round 262
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 19.]), 'previousTarget': array([ 2., 19.]), 'currentState': array([7.       , 2.       , 0.9412235]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 17.72004514666935}
done in step count: 52
reward sum = 0.3797908063352254
running average episode reward sum: 0.25209604529532076
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 19.]), 'previousTarget': array([ 2., 19.]), 'currentState': array([ 2.30546364, 18.09897129,  1.75906612]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 0.951399378999428}
episode index:263
target Thresh 30.154791366687586
target distance 12.0
model initialize at round 263
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 17.]), 'previousTarget': array([27., 17.]), 'currentState': array([15.17654789, 28.81267546,  5.46046334]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 16.713267820642216}
done in step count: 44
reward sum = 0.4605578925991075
running average episode reward sum: 0.25288567350480484
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 17.]), 'previousTarget': array([27., 17.]), 'currentState': array([26.20160818, 17.94058081,  5.20060536]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 1.2337429900502304}
episode index:264
target Thresh 30.173151499356518
target distance 7.0
model initialize at round 264
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.        , 16.        ,  2.83086082]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 7.0710678118654755}
done in step count: 21
reward sum = 0.6908801878170937
running average episode reward sum: 0.25453848299277576
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.93678658,  9.85361889,  4.78066273]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.8559562726099121}
episode index:265
target Thresh 30.191328945653
target distance 25.0
model initialize at round 265
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.61599723,  7.1446813 ]), 'previousTarget': array([15.61709559,  7.14246323]), 'currentState': array([17.99370937, 27.00284045,  2.9699614 ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 89
reward sum = 0.18520762577334698
running average episode reward sum: 0.25427784067240194
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.24943527,  2.80372773,  4.46639746]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.8415439538879141}
episode index:266
target Thresh 30.209325523336823
target distance 17.0
model initialize at round 266
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.28298614, 11.54990798]), 'previousTarget': array([22.88715666, 11.85099785]), 'currentState': array([ 7.4129683 , 23.72128972,  5.54177034]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.33979430077316297
running average episode reward sum: 0.25459812703982054
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 11.]), 'previousTarget': array([24., 11.]), 'currentState': array([23.13826292, 11.92019738,  5.48133248]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 1.2606958410961422}
episode index:267
target Thresh 30.227143032080743
target distance 20.0
model initialize at round 267
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.1556299 , 13.64395443]), 'previousTarget': array([12.13411708, 13.62070537]), 'currentState': array([27.01593991, 27.0294386 ,  1.32703096]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 95
reward sum = 0.14820865051072493
running average episode reward sum: 0.2542011513811299
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([7.88532494, 9.44312514, 3.55714201]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.990030374051836}
episode index:268
target Thresh 30.244783253650485
target distance 14.0
model initialize at round 268
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 20.]), 'previousTarget': array([ 4., 20.]), 'currentState': array([2.        , 6.        , 0.89313346]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 14.142135623730951}
done in step count: 33
reward sum = 0.5127780442249789
running average episode reward sum: 0.25516240377088395
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 20.]), 'previousTarget': array([ 4., 20.]), 'currentState': array([ 3.7427172 , 19.10432025,  1.23398944]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 0.9318994913589232}
episode index:269
target Thresh 30.26224795208291
target distance 13.0
model initialize at round 269
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 27.]), 'previousTarget': array([10., 27.]), 'currentState': array([22.99856288, 16.02230635,  1.87504444]), 'targetState': array([10, 27], dtype=int32), 'currentDistance': 17.013888288250776}
done in step count: 46
reward sum = 0.43294615274969683
running average episode reward sum: 0.2558208621004351
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 27.]), 'previousTarget': array([10., 27.]), 'currentState': array([10.8387327 , 26.39027335,  2.30988635]), 'targetState': array([10, 27], dtype=int32), 'currentDistance': 1.036937379267027}
episode index:270
target Thresh 30.279538873862407
target distance 3.0
model initialize at round 270
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 27.]), 'previousTarget': array([ 4., 27.]), 'currentState': array([ 6.65861692, 27.67232238,  3.69099119]), 'targetState': array([ 4, 27], dtype=int32), 'currentDistance': 2.742309485562004}
done in step count: 5
reward sum = 0.9035905084656032
running average episode reward sum: 0.25821115599846156
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 27.]), 'previousTarget': array([ 4., 27.]), 'currentState': array([ 4.88384053, 27.37346753,  3.10760546]), 'targetState': array([ 4, 27], dtype=int32), 'currentDistance': 0.9595061640892262}
episode index:271
target Thresh 30.29665774809557
target distance 15.0
model initialize at round 271
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([4.02609485, 1.96942189, 5.6549871 ]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 19.216989432623123}
done in step count: 64
reward sum = 0.32966598353126664
running average episode reward sum: 0.25847385757027336
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([15.07949876, 16.53445971,  0.84112596]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 1.0315281368324072}
episode index:272
target Thresh 30.313606286684085
target distance 12.0
model initialize at round 272
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([14.59234973, 12.28798071,  2.42219616]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 12.978666003667232}
done in step count: 28
reward sum = 0.556684569719699
running average episode reward sum: 0.2595662045012236
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([ 9.42328781, 23.03009967,  1.91310753]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 1.05824346244937}
episode index:273
target Thresh 30.330386184495936
target distance 19.0
model initialize at round 273
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 17.]), 'previousTarget': array([27., 17.]), 'currentState': array([ 8.34246795, 23.35714295,  0.55387104]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 19.710828716206866}
done in step count: 51
reward sum = 0.33117068881840656
running average episode reward sum: 0.2598275347359578
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 17.]), 'previousTarget': array([27., 17.]), 'currentState': array([26.10650124, 17.16808841,  5.72874702]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 0.9091720089244577}
episode index:274
target Thresh 30.346999119534885
target distance 13.0
model initialize at round 274
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 29.]), 'previousTarget': array([17., 29.]), 'currentState': array([ 7.90867336, 16.48656721,  1.51014167]), 'targetState': array([17, 29], dtype=int32), 'currentDistance': 15.46732750986688}
done in step count: 33
reward sum = 0.47157524382786686
running average episode reward sum: 0.260597526405383
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 29.]), 'previousTarget': array([17., 29.]), 'currentState': array([16.13297413, 28.1606687 ,  0.46076471]), 'targetState': array([17, 29], dtype=int32), 'currentDistance': 1.2067356388658768}
episode index:275
target Thresh 30.363446753108285
target distance 16.0
model initialize at round 275
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  3.]), 'previousTarget': array([13.,  3.]), 'currentState': array([10.99320572, 19.03672034,  1.88214254]), 'targetState': array([13,  3], dtype=int32), 'currentDistance': 16.161795147548343}
done in step count: 47
reward sum = 0.3840657384405967
running average episode reward sum: 0.26104487499971346
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  3.]), 'previousTarget': array([13.,  3.]), 'currentState': array([12.77414642,  3.91816008,  4.69640212]), 'targetState': array([13,  3], dtype=int32), 'currentDistance': 0.9455304230322572}
episode index:276
target Thresh 30.3797307299932
target distance 19.0
model initialize at round 276
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([27.        , 15.        ,  1.80047679]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 19.1049731745428}
done in step count: 53
reward sum = 0.3493960405671487
running average episode reward sum: 0.26136383227613014
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([ 8.98172228, 16.89813841,  2.75345304]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 0.9869926085720812}
episode index:277
target Thresh 30.39585267860088
target distance 18.0
model initialize at round 277
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6.78641543, 16.29018892]), 'previousTarget': array([ 6.78641543, 16.29018892]), 'currentState': array([23.        , 28.        ,  5.84522213]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.24365803141092246
running average episode reward sum: 0.26130014234496035
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 15.]), 'previousTarget': array([ 5., 15.]), 'currentState': array([ 5.90405246, 15.44593906,  3.74339954]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 1.0080538122797884}
episode index:278
target Thresh 30.411814211139628
target distance 6.0
model initialize at round 278
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 17.]), 'previousTarget': array([11., 17.]), 'currentState': array([16.99717244, 19.01288391,  2.03933597]), 'targetState': array([11, 17], dtype=int32), 'currentDistance': 6.325960707928859}
done in step count: 14
reward sum = 0.7357200338879746
running average episode reward sum: 0.2630005720637525
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 17.]), 'previousTarget': array([11., 17.]), 'currentState': array([11.9664903 , 17.0373541 ,  3.08802728]), 'targetState': array([11, 17], dtype=int32), 'currentDistance': 0.9672118837239302}
episode index:279
target Thresh 30.427616923775997
target distance 6.0
model initialize at round 279
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  5.]), 'previousTarget': array([18.,  5.]), 'currentState': array([23.97961329,  8.06462123,  2.12889385]), 'targetState': array([18,  5], dtype=int32), 'currentDistance': 6.719202211054963}
done in step count: 17
reward sum = 0.7296932617464884
running average episode reward sum: 0.26466733166976225
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  5.]), 'previousTarget': array([18.,  5.]), 'currentState': array([18.85866638,  5.84322869,  3.88304831]), 'targetState': array([18,  5], dtype=int32), 'currentDistance': 1.2034710552385806}
episode index:280
target Thresh 30.44326239679442
target distance 13.0
model initialize at round 280
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 25.]), 'previousTarget': array([ 2., 25.]), 'currentState': array([14.99706197, 12.01414622,  1.97753897]), 'targetState': array([ 2, 25], dtype=int32), 'currentDistance': 18.37269762853891}
done in step count: 50
reward sum = 0.3950609328321152
running average episode reward sum: 0.2651313658375998
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 25.]), 'previousTarget': array([ 2., 25.]), 'currentState': array([ 2.32551107, 24.12619666,  2.01910744]), 'targetState': array([ 2, 25], dtype=int32), 'currentDistance': 0.932464331805447}
episode index:281
target Thresh 30.458752194755238
target distance 13.0
model initialize at round 281
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([19.97767842,  9.97259724,  3.78641742]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 14.333382778961603}
done in step count: 31
reward sum = 0.4716439584527822
running average episode reward sum: 0.2658636799958097
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([14.31350254, 22.08898051,  1.67192629]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 0.9634523065930435}
episode index:282
target Thresh 30.474087866651153
target distance 8.0
model initialize at round 282
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 17.]), 'previousTarget': array([ 7., 17.]), 'currentState': array([ 2.06017311, 25.08517641,  0.72046292]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 9.474806977521732}
done in step count: 21
reward sum = 0.6184750974220412
running average episode reward sum: 0.2671096567358317
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 17.]), 'previousTarget': array([ 7., 17.]), 'currentState': array([ 6.87925432, 17.90891441,  4.80583272]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 0.9168996248971922}
episode index:283
target Thresh 30.489270946062135
target distance 10.0
model initialize at round 283
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 24.]), 'previousTarget': array([21., 24.]), 'currentState': array([11.38222105, 22.19555261,  0.34396589]), 'targetState': array([21, 24], dtype=int32), 'currentDistance': 9.785586456689428}
done in step count: 23
reward sum = 0.657188225103183
running average episode reward sum: 0.2684831728216322
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 24.]), 'previousTarget': array([21., 24.]), 'currentState': array([20.08320676, 24.04225482,  0.12457961]), 'targetState': array([21, 24], dtype=int32), 'currentDistance': 0.9177664848710252}
episode index:284
target Thresh 30.50430295130878
target distance 3.0
model initialize at round 284
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([ 5.07566856, 11.47370114,  1.23438492]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 3.8644404895907156}
done in step count: 7
reward sum = 0.8661265822464498
running average episode reward sum: 0.2705801672406667
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([ 7.06633006, 13.46690358,  0.48936114]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 1.075142474114975}
episode index:285
target Thresh 30.519185385604136
target distance 7.0
model initialize at round 285
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 29.]), 'previousTarget': array([18., 29.]), 'currentState': array([13.08421109, 22.49163497,  1.2796524 ]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 8.156212110396078}
done in step count: 19
reward sum = 0.7010680358747908
running average episode reward sum: 0.27208536957854823
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 29.]), 'previousTarget': array([18., 29.]), 'currentState': array([17.717074  , 28.00227765,  0.83452484]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 1.0370617175809762}
episode index:286
target Thresh 30.533919737204037
target distance 16.0
model initialize at round 286
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 20.]), 'previousTarget': array([10., 20.]), 'currentState': array([3.0078291 , 4.00254813, 0.56380333]), 'targetState': array([10, 20], dtype=int32), 'currentDistance': 17.45877774189384}
done in step count: 46
reward sum = 0.4126016062718514
running average episode reward sum: 0.27257497319072005
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 20.]), 'previousTarget': array([10., 20.]), 'currentState': array([ 9.83789993, 19.0499623 ,  1.34145197]), 'targetState': array([10, 20], dtype=int32), 'currentDistance': 0.9637676437871598}
episode index:287
target Thresh 30.54850747955592
target distance 20.0
model initialize at round 287
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7.12283287, 23.60700849]), 'previousTarget': array([ 7.12283287, 23.60700849]), 'currentState': array([26.        , 17.        ,  4.85554233]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.27525150522122144
running average episode reward sum: 0.2725842667047148
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 24.]), 'previousTarget': array([ 6., 24.]), 'currentState': array([ 6.98747442, 23.55240568,  2.61425538]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 1.0841800640796038}
episode index:288
target Thresh 30.562950071446178
target distance 2.0
model initialize at round 288
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([12.01697651,  9.06485343,  1.4751339 ]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 2.886488163851505}
done in step count: 10
reward sum = 0.8337751252726524
running average episode reward sum: 0.2745261035855727
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([10.27397089,  7.89129556,  4.13953092]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 0.9324525889737705}
episode index:289
target Thresh 30.577248957146033
target distance 15.0
model initialize at round 289
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 29.]), 'previousTarget': array([16., 29.]), 'currentState': array([15.95979767, 13.9692213 ,  3.54250002]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 15.030832462843199}
done in step count: 41
reward sum = 0.4204851653231737
running average episode reward sum: 0.27502941069501274
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 29.]), 'previousTarget': array([16., 29.]), 'currentState': array([16.25606539, 28.13796198,  1.05260095]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 0.8992658322200628}
episode index:290
target Thresh 30.591405566555974
target distance 21.0
model initialize at round 290
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.17456142, 22.150361  ]), 'previousTarget': array([16.12086431, 21.9086344 ]), 'currentState': array([3.78810713, 6.44763034, 1.77139235]), 'targetState': array([20, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 68
reward sum = 0.1621629439562996
running average episode reward sum: 0.27464155342099655
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 27.]), 'previousTarget': array([20., 27.]), 'currentState': array([20.07718569, 26.08886416,  0.64008104]), 'targetState': array([20, 27], dtype=int32), 'currentDistance': 0.9143993348711366}
episode index:291
target Thresh 30.60542131534874
target distance 8.0
model initialize at round 291
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 23.]), 'previousTarget': array([ 7., 23.]), 'currentState': array([15.        , 18.        ,  1.37367716]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 9.433981132056603}
done in step count: 23
reward sum = 0.6476061693910143
running average episode reward sum: 0.27591882950308566
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 23.]), 'previousTarget': array([ 7., 23.]), 'currentState': array([ 7.96463414, 22.65357292,  2.58709023]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 1.0249540233441672}
episode index:292
target Thresh 30.619297605110887
target distance 17.0
model initialize at round 292
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 22.]), 'previousTarget': array([10., 22.]), 'currentState': array([27.03395371, 20.03241124,  1.01466012]), 'targetState': array([10, 22], dtype=int32), 'currentDistance': 17.14721506475415}
done in step count: 46
reward sum = 0.3800088454340715
running average episode reward sum: 0.27627408553015387
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 22.]), 'previousTarget': array([10., 22.]), 'currentState': array([10.83446845, 22.40062922,  2.9978143 ]), 'targetState': array([10, 22], dtype=int32), 'currentDistance': 0.9256572579887548}
episode index:293
target Thresh 30.633035823482956
target distance 16.0
model initialize at round 293
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  6.]), 'previousTarget': array([18.,  6.]), 'currentState': array([24.95355185, 22.00944245,  3.17599693]), 'targetState': array([18,  6], dtype=int32), 'currentDistance': 17.454344180011894}
done in step count: 47
reward sum = 0.4055077810204216
running average episode reward sum: 0.2767136559229779
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  6.]), 'previousTarget': array([18.,  6.]), 'currentState': array([18.22927972,  6.93441989,  4.3192706 ]), 'targetState': array([18,  6], dtype=int32), 'currentDistance': 0.9621381024208985}
episode index:294
target Thresh 30.64663734429823
target distance 12.0
model initialize at round 294
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 26.]), 'previousTarget': array([12., 26.]), 'currentState': array([23.62590248, 23.17812905,  2.84027401]), 'targetState': array([12, 26], dtype=int32), 'currentDistance': 11.963467897474606}
done in step count: 27
reward sum = 0.5874097320060663
running average episode reward sum: 0.27776686296054776
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 26.]), 'previousTarget': array([12., 26.]), 'currentState': array([12.86667352, 25.77483815,  3.04213763]), 'targetState': array([12, 26], dtype=int32), 'currentDistance': 0.8954444939329345}
episode index:295
target Thresh 30.660103527720132
target distance 7.0
model initialize at round 295
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 17.]), 'previousTarget': array([18., 17.]), 'currentState': array([25.00318957, 19.02701554,  1.69779214]), 'targetState': array([18, 17], dtype=int32), 'currentDistance': 7.290641688519944}
done in step count: 21
reward sum = 0.6864828435104215
running average episode reward sum: 0.27914766019213516
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 17.]), 'previousTarget': array([18., 17.]), 'currentState': array([18.90410941, 16.95215641,  3.99721673]), 'targetState': array([18, 17], dtype=int32), 'currentDistance': 0.9053744184547872}
episode index:296
target Thresh 30.67343572037822
target distance 11.0
model initialize at round 296
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 12.]), 'previousTarget': array([ 4., 12.]), 'currentState': array([ 1.96201558, 22.92370542,  4.50295067]), 'targetState': array([ 4, 12], dtype=int32), 'currentDistance': 11.112187926929103}
done in step count: 25
reward sum = 0.5930615517698222
running average episode reward sum: 0.2802046093220264
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 12.]), 'previousTarget': array([ 4., 12.]), 'currentState': array([ 3.72415112, 12.96084386,  4.79885509]), 'targetState': array([ 4, 12], dtype=int32), 'currentDistance': 0.999656702885538}
episode index:297
target Thresh 30.68663525550287
target distance 5.0
model initialize at round 297
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([8.07792517, 7.06031229, 0.88020676]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 5.187445051461094}
done in step count: 16
reward sum = 0.7404456957290628
running average episode reward sum: 0.2817490424978889
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.86953942, 6.2123696 , 3.45874925]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.895097562630427}
episode index:298
target Thresh 30.699703453058596
target distance 17.0
model initialize at round 298
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.13497444,  9.20859686]), 'currentState': array([25.7012661 , 25.68993101,  3.79838452]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 19.826015570539635}
done in step count: 50
reward sum = 0.34059296518474913
running average episode reward sum: 0.2819458449149019
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.00446353,  9.82354029,  4.46028101]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.8235523870917956}
episode index:299
target Thresh 30.71264161987605
target distance 1.0
model initialize at round 299
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 11.]), 'previousTarget': array([19., 11.]), 'currentState': array([19.09779537, 11.80466245,  5.13820872]), 'targetState': array([19, 11], dtype=int32), 'currentDistance': 0.8105834939074191}
done in step count: 0
reward sum = 0.9978596214625242
running average episode reward sum: 0.28433222417006065
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 11.]), 'previousTarget': array([19., 11.]), 'currentState': array([19.09779537, 11.80466245,  5.13820872]), 'targetState': array([19, 11], dtype=int32), 'currentDistance': 0.8105834939074191}
episode index:300
target Thresh 30.725451049782684
target distance 7.0
model initialize at round 300
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 18.]), 'previousTarget': array([14., 18.]), 'currentState': array([12.01237454, 11.49954484,  1.48522811]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 6.797541633096625}
done in step count: 18
reward sum = 0.7462326514209096
running average episode reward sum: 0.2858667770845153
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 18.]), 'previousTarget': array([14., 18.]), 'currentState': array([13.57264486, 17.05505745,  0.98136172]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 1.0370867090985005}
episode index:301
target Thresh 30.738133023732168
target distance 3.0
model initialize at round 301
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 25.]), 'previousTarget': array([22., 25.]), 'currentState': array([25.        , 28.        ,  2.04421708]), 'targetState': array([22, 25], dtype=int32), 'currentDistance': 4.242640687119286}
done in step count: 16
reward sum = 0.7739437647729782
running average episode reward sum: 0.2874829260503711
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 25.]), 'previousTarget': array([22., 25.]), 'currentState': array([22.27433758, 25.92637275,  4.13727241]), 'targetState': array([22, 25], dtype=int32), 'currentDistance': 0.9661405608569521}
episode index:302
target Thresh 30.75068880993247
target distance 13.0
model initialize at round 302
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 21.]), 'previousTarget': array([ 5., 21.]), 'currentState': array([18.        ,  9.        ,  1.67709899]), 'targetState': array([ 5, 21], dtype=int32), 'currentDistance': 17.69180601295413}
done in step count: 45
reward sum = 0.39360888573544706
running average episode reward sum: 0.2878331767424011
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 21.]), 'previousTarget': array([ 5., 21.]), 'currentState': array([ 5.96629678, 20.8023924 ,  2.77840973]), 'targetState': array([ 5, 21], dtype=int32), 'currentDistance': 0.9862952065981001}
episode index:303
target Thresh 30.763119663972663
target distance 4.0
model initialize at round 303
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 18.]), 'previousTarget': array([21., 18.]), 'currentState': array([16.99712602, 19.99706751,  4.18956733]), 'targetState': array([21, 18], dtype=int32), 'currentDistance': 4.473396779807269}
done in step count: 15
reward sum = 0.7852284610901749
running average episode reward sum: 0.28946934544091346
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 18.]), 'previousTarget': array([21., 18.]), 'currentState': array([2.00483489e+01, 1.84587869e+01, 1.82961931e-02]), 'targetState': array([21, 18], dtype=int32), 'currentDistance': 1.0564683343556143}
episode index:304
target Thresh 30.77542682894852
target distance 11.0
model initialize at round 304
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 27.]), 'previousTarget': array([ 8., 27.]), 'currentState': array([19.        , 24.        ,  5.11693865]), 'targetState': array([ 8, 27], dtype=int32), 'currentDistance': 11.40175425099138}
done in step count: 35
reward sum = 0.5355631435266679
running average episode reward sum: 0.29027621035267004
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 27.]), 'previousTarget': array([ 8., 27.]), 'currentState': array([ 8.96367949, 27.01582332,  2.99306466]), 'targetState': array([ 8, 27], dtype=int32), 'currentDistance': 0.9638093931894508}
episode index:305
target Thresh 30.78761153558679
target distance 10.0
model initialize at round 305
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 14.]), 'previousTarget': array([15., 14.]), 'currentState': array([10.05398409, 24.01001214,  6.23828018]), 'targetState': array([15, 14], dtype=int32), 'currentDistance': 11.165277268613284}
done in step count: 28
reward sum = 0.585199659704245
running average episode reward sum: 0.291240012474734
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 14.]), 'previousTarget': array([15., 14.]), 'currentState': array([14.30203526, 14.91988875,  5.50594189]), 'targetState': array([15, 14], dtype=int32), 'currentDistance': 1.1547077988555488}
episode index:306
target Thresh 30.799675002368296
target distance 8.0
model initialize at round 306
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 26.]), 'previousTarget': array([25., 26.]), 'currentState': array([17.        , 21.        ,  5.84963697]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 9.433981132056603}
done in step count: 21
reward sum = 0.6547313893333351
running average episode reward sum: 0.2924240234742735
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 26.]), 'previousTarget': array([25., 26.]), 'currentState': array([24.13213604, 25.54995067,  0.69417014]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 0.9776155952604609}
episode index:307
target Thresh 30.81161843564976
target distance 11.0
model initialize at round 307
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 21.]), 'previousTarget': array([23., 21.]), 'currentState': array([12.0350361 , 24.06106017,  0.79975766]), 'targetState': array([23, 21], dtype=int32), 'currentDistance': 11.384222532099251}
done in step count: 26
reward sum = 0.5798390533716387
running average episode reward sum: 0.2933571891557585
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 21.]), 'previousTarget': array([23., 21.]), 'currentState': array([22.01177361, 20.81827503,  0.05321549]), 'targetState': array([23, 21], dtype=int32), 'currentDistance': 1.0047961810008392}
episode index:308
target Thresh 30.823443029784467
target distance 19.0
model initialize at round 308
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.30789389, 17.34531844]), 'previousTarget': array([24.07475678, 17.43827311]), 'currentState': array([ 6.41175843, 26.27439093,  0.33530974]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.2799440165632972
running average episode reward sum: 0.29331378083021653
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([24.05455814, 16.89891482,  6.2757087 ]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 0.9508304358588188}
episode index:309
target Thresh 30.835149967241687
target distance 9.0
model initialize at round 309
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 16.]), 'previousTarget': array([21., 16.]), 'currentState': array([12.30770661, 12.06201256,  0.3247221 ]), 'targetState': array([21, 16], dtype=int32), 'currentDistance': 9.542730718633717}
done in step count: 29
reward sum = 0.6442133898882078
running average episode reward sum: 0.2944457150529842
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 16.]), 'previousTarget': array([21., 16.]), 'currentState': array([20.14708667, 15.53584007,  0.51485135]), 'targetState': array([21, 16], dtype=int32), 'currentDistance': 0.9710332561996519}
episode index:310
target Thresh 30.84674041872492
target distance 24.0
model initialize at round 310
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5.00049685, 9.00084458]), 'previousTarget': array([5., 9.]), 'currentState': array([5.00298059e+00, 2.90008444e+01, 2.51711309e-02]), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.22158828353175863
running average episode reward sum: 0.29421144678442723
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.12061404, 5.98178412, 4.74568087]), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.9891652087022864}
episode index:311
target Thresh 30.85821554328897
target distance 8.0
model initialize at round 311
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 18.]), 'previousTarget': array([23., 18.]), 'currentState': array([17.46635161, 25.82745898,  5.74507394]), 'targetState': array([23, 18], dtype=int32), 'currentDistance': 9.585946934069327}
done in step count: 22
reward sum = 0.6556101546563575
running average episode reward sum: 0.29536977597632447
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 18.]), 'previousTarget': array([23., 18.]), 'currentState': array([22.05602142, 18.57356682,  5.46424572]), 'targetState': array([23, 18], dtype=int32), 'currentDistance': 1.1045698082092772}
episode index:312
target Thresh 30.869576488455863
target distance 15.0
model initialize at round 312
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([ 3.43676615, 19.77683413,  5.55831575]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 14.787564346478984}
done in step count: 38
reward sum = 0.47622396327362165
running average episode reward sum: 0.2959475848814277
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.27960988, 5.90248245, 4.8484496 ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.9448048811358741}
episode index:313
target Thresh 30.880824390329575
target distance 17.0
model initialize at round 313
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.32264852, 22.52170912]), 'previousTarget': array([15.46633605, 22.33935727]), 'currentState': array([26.50739411,  5.94155634,  3.03981695]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.3142693813165639
running average episode reward sum: 0.2960059345516033
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 23.]), 'previousTarget': array([15., 23.]), 'currentState': array([15.49948503, 22.10438809,  2.36113119]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 1.0254784205479286}
episode index:314
target Thresh 30.891960373709672
target distance 9.0
model initialize at round 314
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 16.]), 'previousTarget': array([19., 16.]), 'currentState': array([11.        ,  7.        ,  5.85377216]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 12.041594578792296}
done in step count: 55
reward sum = 0.39836715347021834
running average episode reward sum: 0.29633089080213854
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 16.]), 'previousTarget': array([19., 16.]), 'currentState': array([18.03782332, 16.37093381,  0.2870838 ]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 1.031201168092313}
episode index:315
target Thresh 30.90298555220377
target distance 11.0
model initialize at round 315
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.88919365, 25.91586608,  3.91901672]), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 11.96078370005881}
done in step count: 30
reward sum = 0.58350096719
running average episode reward sum: 0.2972396568666571
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.74834894, 15.88993757,  4.39489699]), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9248335718690428}
episode index:316
target Thresh 30.91390102833891
target distance 14.0
model initialize at round 316
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 16.]), 'previousTarget': array([26., 16.]), 'currentState': array([12.37355715, 18.99207612,  6.22641633]), 'targetState': array([26, 16], dtype=int32), 'currentDistance': 13.951073945774729}
done in step count: 37
reward sum = 0.5183188027242474
running average episode reward sum: 0.2979370674214129
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 16.]), 'previousTarget': array([26., 16.]), 'currentState': array([25.45259858, 16.96564597,  5.91336032]), 'targetState': array([26, 16], dtype=int32), 'currentDistance': 1.1100092126663719}
episode index:317
target Thresh 30.924707893671794
target distance 16.0
model initialize at round 317
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 20.]), 'previousTarget': array([20., 20.]), 'currentState': array([ 3.957086  , 24.99918697,  3.41303587]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 16.803778143572426}
done in step count: 45
reward sum = 0.3798899736946531
running average episode reward sum: 0.29819478096315266
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 20.]), 'previousTarget': array([20., 20.]), 'currentState': array([19.08872433, 20.50745888,  6.12323355]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 1.0430425937188148}
episode index:318
target Thresh 30.93540722889797
target distance 14.0
model initialize at round 318
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 28.]), 'previousTarget': array([ 9., 28.]), 'currentState': array([ 5.95053097, 14.41402972,  1.58883299]), 'targetState': array([ 9, 28], dtype=int32), 'currentDistance': 13.924002647638963}
done in step count: 32
reward sum = 0.5261269106498596
running average episode reward sum: 0.29890930174586966
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 28.]), 'previousTarget': array([ 9., 28.]), 'currentState': array([ 8.07651699, 27.00445047,  1.91977593]), 'targetState': array([ 9, 28], dtype=int32), 'currentDistance': 1.3579174216022714}
episode index:319
target Thresh 30.946000103959864
target distance 5.0
model initialize at round 319
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  4.]), 'previousTarget': array([22.,  4.]), 'currentState': array([17.       ,  3.       ,  2.1329149]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 5.099019513592785}
done in step count: 14
reward sum = 0.7713986207754497
running average episode reward sum: 0.30038583086783704
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  4.]), 'previousTarget': array([22.,  4.]), 'currentState': array([21.11668914,  4.32864393,  6.14220852]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 0.9424674583934868}
episode index:320
target Thresh 30.956487578153826
target distance 23.0
model initialize at round 320
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6.79520378, 10.69785275]), 'previousTarget': array([ 7.11006407, 10.5704125 ]), 'currentState': array([25.71269313,  4.20719819,  2.54952487]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 78
reward sum = 0.23867744272960345
running average episode reward sum: 0.30019359289855907
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 12.]), 'previousTarget': array([ 3., 12.]), 'currentState': array([ 3.9092938 , 11.73369894,  3.0450874 ]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 0.9474869279581226}
episode index:321
target Thresh 30.966870700236
target distance 20.0
model initialize at round 321
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.09984976,  7.20632261]), 'previousTarget': array([11.38262381,  7.50609905]), 'currentState': array([26.85415534, 19.52711218,  4.16216972]), 'targetState': array([7, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.20189893839562972
running average episode reward sum: 0.2998883299963761
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 4.]), 'previousTarget': array([7., 4.]), 'currentState': array([7.67130427, 4.87676308, 3.98838558]), 'targetState': array([7, 4], dtype=int32), 'currentDistance': 1.1042476749700356}
episode index:322
target Thresh 30.97715050852726
target distance 16.0
model initialize at round 322
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'previousTarget': array([25.,  3.]), 'currentState': array([27.04467637, 18.99954456,  6.02049145]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 16.129666071238496}
done in step count: 38
reward sum = 0.4240127084776951
running average episode reward sum: 0.3002726159978662
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'previousTarget': array([25.,  3.]), 'currentState': array([25.07543113,  3.87301991,  5.02274105]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 0.8762725708847933}
episode index:323
target Thresh 30.987328031016993
target distance 11.0
model initialize at round 323
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 25.]), 'previousTarget': array([17., 25.]), 'currentState': array([ 6.40844195, 22.12716326,  0.32483884]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 10.974255912299876}
done in step count: 23
reward sum = 0.6255236926993273
running average episode reward sum: 0.3012764773457102
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 25.]), 'previousTarget': array([17., 25.]), 'currentState': array([16.07033804, 24.68885826,  0.37784515]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 0.9803471551683267}
episode index:324
target Thresh 30.99740428546593
target distance 18.0
model initialize at round 324
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2.62855213, 13.38726277]), 'previousTarget': array([ 2.93436333, 13.57099981]), 'currentState': array([19.65613616, 23.87827711,  3.48885583]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.3463209567949039
running average episode reward sum: 0.30141507574401544
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 13.]), 'previousTarget': array([ 2., 13.]), 'currentState': array([ 2.85887792, 13.59186089,  3.84117116]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 1.043058294273787}
episode index:325
target Thresh 31.007380279507917
target distance 7.0
model initialize at round 325
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 19.]), 'previousTarget': array([21., 19.]), 'currentState': array([14.42559503, 21.01326298,  6.20624282]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 6.875756576248321}
done in step count: 14
reward sum = 0.7653036995098244
running average episode reward sum: 0.3028380469825609
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 19.]), 'previousTarget': array([21., 19.]), 'currentState': array([20.06796355, 19.4033431 ,  6.10511132]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 1.015567624478821}
episode index:326
target Thresh 31.01725701075067
target distance 14.0
model initialize at round 326
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([ 9.98964223, 16.96729515,  4.65817833]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 14.838767429333721}
done in step count: 38
reward sum = 0.5021413999307074
running average episode reward sum: 0.30344753735854907
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.38353964,  3.93118536,  5.22041113]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.116749548079026}
episode index:327
target Thresh 31.02703546687554
target distance 15.0
model initialize at round 327
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 23.]), 'previousTarget': array([18., 23.]), 'currentState': array([ 2.99700019, 10.99425891,  4.44801366]), 'targetState': array([18, 23], dtype=int32), 'currentDistance': 19.215301780150813}
done in step count: 59
reward sum = 0.27662340422938636
running average episode reward sum: 0.3033657564648626
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 23.]), 'previousTarget': array([18., 23.]), 'currentState': array([17.01169048, 23.02832059,  0.66759077]), 'targetState': array([18, 23], dtype=int32), 'currentDistance': 0.9887152053798599}
episode index:328
target Thresh 31.03671662573629
target distance 15.0
model initialize at round 328
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([13.07502797, 24.58001288,  4.65454298]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 17.173610808137738}
done in step count: 50
reward sum = 0.39372112658673897
running average episode reward sum: 0.30364039284821176
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.97417643, 10.20015957,  4.18930676]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.994526806086576}
episode index:329
target Thresh 31.046301455456874
target distance 22.0
model initialize at round 329
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.05572809, 20.88854382]), 'previousTarget': array([10.05572809, 20.88854382]), 'currentState': array([19.        ,  3.        ,  3.70788527]), 'targetState': array([ 8, 25], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 66
reward sum = 0.19117645778776216
running average episode reward sum: 0.30329959304499826
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 25.]), 'previousTarget': array([ 8., 25.]), 'currentState': array([ 8.09705329, 24.07203497,  2.02554095]), 'targetState': array([ 8, 25], dtype=int32), 'currentDistance': 0.9330264956670116}
episode index:330
target Thresh 31.055790914528256
target distance 7.0
model initialize at round 330
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 29.]), 'previousTarget': array([11., 29.]), 'currentState': array([ 4.        , 22.        ,  2.03767917]), 'targetState': array([11, 29], dtype=int32), 'currentDistance': 9.899494936611665}
done in step count: 20
reward sum = 0.637241659065819
running average episode reward sum: 0.3043084814619796
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 29.]), 'previousTarget': array([11., 29.]), 'currentState': array([10.13533868, 28.26300106,  0.86548385]), 'targetState': array([11, 29], dtype=int32), 'currentDistance': 1.1361367114525722}
episode index:331
target Thresh 31.065185951904247
target distance 10.0
model initialize at round 331
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 15.]), 'previousTarget': array([12., 15.]), 'currentState': array([16.07425451, 24.96380253,  5.57710505]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 10.764613817835036}
done in step count: 23
reward sum = 0.5892570058158239
running average episode reward sum: 0.3051667601497924
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 15.]), 'previousTarget': array([12., 15.]), 'currentState': array([12.14127178, 15.78284341,  4.50563816]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 0.7954882284271425}
episode index:332
target Thresh 31.07448750709641
target distance 19.0
model initialize at round 332
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.62822773,  7.77385068]), 'previousTarget': array([11.56172689,  7.92524322]), 'currentState': array([ 2.96747355, 25.80136679,  4.58601894]), 'targetState': array([12,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.32820745504228394
running average episode reward sum: 0.30523595142574583
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  7.]), 'previousTarget': array([12.,  7.]), 'currentState': array([12.27446455,  7.93964158,  5.69827928]), 'targetState': array([12,  7], dtype=int32), 'currentDistance': 0.9789060682210035}
episode index:333
target Thresh 31.083696510268023
target distance 11.0
model initialize at round 333
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 29.]), 'previousTarget': array([23., 29.]), 'currentState': array([20.07682197, 18.47129808,  1.46096907]), 'targetState': array([23, 29], dtype=int32), 'currentDistance': 10.92696362136811}
done in step count: 22
reward sum = 0.6257816646754208
running average episode reward sum: 0.30619566913008617
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 29.]), 'previousTarget': array([23., 29.]), 'currentState': array([22.5386809 , 28.02980454,  1.69893451]), 'targetState': array([23, 29], dtype=int32), 'currentDistance': 1.074287923331695}
episode index:334
target Thresh 31.092813882327075
target distance 8.0
model initialize at round 334
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 29.]), 'previousTarget': array([16., 29.]), 'currentState': array([24.06113023, 23.98428408,  6.24855283]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 9.494168044700876}
done in step count: 24
reward sum = 0.589249324037514
running average episode reward sum: 0.3070406054133919
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 29.]), 'previousTarget': array([16., 29.]), 'currentState': array([16.42663926, 28.0016839 ,  2.96941875]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 1.0856592870152435}
episode index:335
target Thresh 31.10184053501837
target distance 18.0
model initialize at round 335
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.29235656, 27.47289834]), 'previousTarget': array([12.28714138, 27.48314552]), 'currentState': array([21.99310059,  9.98302799,  4.07686633]), 'targetState': array([12, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.28270732754979505
running average episode reward sum: 0.30696818494355976
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 28.]), 'previousTarget': array([12., 28.]), 'currentState': array([11.92443577, 27.06603561,  2.11661574]), 'targetState': array([12, 28], dtype=int32), 'currentDistance': 0.9370162377384563}
episode index:336
target Thresh 31.11077737101469
target distance 5.0
model initialize at round 336
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 23.]), 'previousTarget': array([25., 23.]), 'currentState': array([20.35205814, 22.23965032,  0.54607508]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 4.709723465759972}
done in step count: 13
reward sum = 0.8073529874745624
running average episode reward sum: 0.3084530063160553
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 23.]), 'previousTarget': array([25., 23.]), 'currentState': array([24.00624514, 23.72876978,  0.09485951]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 1.2323368520928462}
episode index:337
target Thresh 31.1196252840071
target distance 18.0
model initialize at round 337
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([24.        , 27.        ,  1.48503765]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 18.110770276274835}
done in step count: 48
reward sum = 0.36671944395511546
running average episode reward sum: 0.3086253922262301
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([ 6.84932587, 24.65920063,  2.97662308]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 0.9151495200668682}
episode index:338
target Thresh 31.128385158794256
target distance 6.0
model initialize at round 338
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 20.]), 'previousTarget': array([21., 20.]), 'currentState': array([15.13684157, 16.00265856,  0.1920067 ]), 'targetState': array([21, 20], dtype=int32), 'currentDistance': 7.096151446767826}
done in step count: 14
reward sum = 0.7462295477183135
running average episode reward sum: 0.309916259941546
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 20.]), 'previousTarget': array([21., 20.]), 'currentState': array([20.14974996, 19.29735982,  1.42085405]), 'targetState': array([21, 20], dtype=int32), 'currentDistance': 1.103008770255216}
episode index:339
target Thresh 31.137057871370946
target distance 8.0
model initialize at round 339
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'previousTarget': array([16., 23.]), 'currentState': array([ 8.01864131, 23.05300392,  0.98738158]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 7.981534683085911}
done in step count: 16
reward sum = 0.7025581951719355
running average episode reward sum: 0.31107108916281184
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'previousTarget': array([16., 23.]), 'currentState': array([15.06375809, 22.97113898,  6.22867805]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 0.9366866483411138}
episode index:340
target Thresh 31.145644289015653
target distance 10.0
model initialize at round 340
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 15.]), 'previousTarget': array([12., 15.]), 'currentState': array([ 1.93504542, 10.01840545,  2.62059087]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 11.230298080011295}
done in step count: 25
reward sum = 0.560027686156843
running average episode reward sum: 0.3118011671598618
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 15.]), 'previousTarget': array([12., 15.]), 'currentState': array([11.02207978, 14.53420556,  0.55303454]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 1.0831862347431997}
episode index:341
target Thresh 31.154145270377295
target distance 10.0
model initialize at round 341
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 20.]), 'previousTarget': array([11., 20.]), 'currentState': array([9.0025399 , 9.99805112, 5.88120031]), 'targetState': array([11, 20], dtype=int32), 'currentDistance': 10.199452350888361}
done in step count: 24
reward sum = 0.5938423112821543
running average episode reward sum: 0.31262584886782174
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 20.]), 'previousTarget': array([11., 20.]), 'currentState': array([10.25515042, 19.29770068,  1.13453941]), 'targetState': array([11, 20], dtype=int32), 'currentDistance': 1.0237310362327336}
episode index:342
target Thresh 31.162561665561093
target distance 20.0
model initialize at round 342
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.78123306,  6.21634738]), 'previousTarget': array([24.638375  ,  6.47568183]), 'currentState': array([15.14421864, 23.7414138 ,  5.29931395]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 76
reward sum = 0.20706561914583382
running average episode reward sum: 0.31231809309603753
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  4.]), 'previousTarget': array([26.,  4.]), 'currentState': array([26.35457889,  4.9773133 ,  5.35795267]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 1.0396477666396198}
episode index:343
target Thresh 31.17089431621358
target distance 11.0
model initialize at round 343
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 28.]), 'previousTarget': array([ 7., 28.]), 'currentState': array([17.62739335, 28.00758256,  3.18763939]), 'targetState': array([ 7, 28], dtype=int32), 'currentDistance': 10.627396059295394}
done in step count: 22
reward sum = 0.6364489738787132
running average episode reward sum: 0.3132603340285453
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 28.]), 'previousTarget': array([ 7., 28.]), 'currentState': array([ 7.93368574, 27.71403578,  3.29657621]), 'targetState': array([ 7, 28], dtype=int32), 'currentDistance': 0.9764960822251727}
episode index:344
target Thresh 31.179144055606763
target distance 11.0
model initialize at round 344
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 27.]), 'previousTarget': array([14., 27.]), 'currentState': array([25.        , 26.        ,  5.37071562]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 11.045361017187261}
done in step count: 26
reward sum = 0.5558673797963668
running average episode reward sum: 0.3139635428568578
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 27.]), 'previousTarget': array([14., 27.]), 'currentState': array([14.90472668, 26.62452207,  3.20248801]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 0.9795478736883899}
episode index:345
target Thresh 31.18731170872146
target distance 4.0
model initialize at round 345
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 18.]), 'previousTarget': array([17., 18.]), 'currentState': array([13.00196363, 16.99522762,  5.32521501]), 'targetState': array([17, 18], dtype=int32), 'currentDistance': 4.122361259724025}
done in step count: 10
reward sum = 0.8386655647056374
running average episode reward sum: 0.31548002268879066
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 18.]), 'previousTarget': array([17., 18.]), 'currentState': array([16.128511  , 17.08632474,  0.47332318]), 'targetState': array([17, 18], dtype=int32), 'currentDistance': 1.2626541716025883}
episode index:346
target Thresh 31.195398092329786
target distance 4.0
model initialize at round 346
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 17.]), 'previousTarget': array([ 3., 17.]), 'currentState': array([ 4.        , 21.        ,  0.52246526]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 4.123105625617661}
done in step count: 15
reward sum = 0.7607081570027635
running average episode reward sum: 0.3167631008856609
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 17.]), 'previousTarget': array([ 3., 17.]), 'currentState': array([ 3.97715445, 17.24146223,  4.37349899]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 1.0065459865196336}
episode index:347
target Thresh 31.203404015076842
target distance 6.0
model initialize at round 347
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([22.78377909, 24.55110869,  4.11713624]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 6.321412504163386}
done in step count: 16
reward sum = 0.7425095026969958
running average episode reward sum: 0.31798651008626816
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([17.96780781, 21.55683371,  3.35406667]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 1.0644474180212768}
episode index:348
target Thresh 31.211330277561572
target distance 14.0
model initialize at round 348
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 3.]), 'previousTarget': array([6., 3.]), 'currentState': array([16.01099188, 16.99557438,  5.64791679]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 17.2074420132176}
done in step count: 37
reward sum = 0.40401131838214693
running average episode reward sum: 0.3182329995083194
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 3.]), 'previousTarget': array([6., 3.]), 'currentState': array([6.63557777, 3.88330771, 3.93480283]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 1.0882056848403245}
episode index:349
target Thresh 31.219177672416834
target distance 1.0
model initialize at round 349
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  8.]), 'previousTarget': array([17.,  8.]), 'currentState': array([17.03166429,  6.98912924,  6.07839383]), 'targetState': array([17,  8], dtype=int32), 'currentDistance': 1.0113665639012717}
done in step count: 3
reward sum = 0.9600380140679441
running average episode reward sum: 0.3200667281213469
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  8.]), 'previousTarget': array([17.,  8.]), 'currentState': array([17.14780268,  7.00980269,  0.55449675]), 'targetState': array([17,  8], dtype=int32), 'currentDistance': 1.0011674886937723}
episode index:350
target Thresh 31.226946984388647
target distance 8.0
model initialize at round 350
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 15.]), 'previousTarget': array([11., 15.]), 'currentState': array([ 2.9173941 , 23.01502589,  3.19729996]), 'targetState': array([11, 15], dtype=int32), 'currentDistance': 11.382844900123153}
done in step count: 30
reward sum = 0.5244849431093406
running average episode reward sum: 0.3206491161982358
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 15.]), 'previousTarget': array([11., 15.]), 'currentState': array([10.02453085, 15.6725282 ,  0.54733027]), 'targetState': array([11, 15], dtype=int32), 'currentDistance': 1.1848351135884152}
episode index:351
target Thresh 31.234638990414687
target distance 19.0
model initialize at round 351
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.31997356, 21.8763135 ]), 'previousTarget': array([15.33589719, 21.90482627]), 'currentState': array([2.00335634, 6.95425915, 5.03813505]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 75
reward sum = 0.19188513325988193
running average episode reward sum: 0.32028330942852457
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 26.]), 'previousTarget': array([19., 26.]), 'currentState': array([18.14399099, 25.05441181,  0.91532915]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 1.2754953752769116}
episode index:352
target Thresh 31.242254459701964
target distance 23.0
model initialize at round 352
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.33118508, 24.86578278]), 'previousTarget': array([26.24859289, 24.54352728]), 'currentState': array([22.15732982,  5.30615773,  1.19309169]), 'targetState': array([27, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.2805895597128754
running average episode reward sum: 0.32017086254547744
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 28.]), 'previousTarget': array([27., 28.]), 'currentState': array([27.10312166, 27.06598954,  1.65753411]), 'targetState': array([27, 28], dtype=int32), 'currentDistance': 0.9396859166492967}
episode index:353
target Thresh 31.249794153803755
target distance 19.0
model initialize at round 353
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7.89888325, 17.86398076]), 'previousTarget': array([ 7.89888325, 17.86398076]), 'currentState': array([24.        ,  6.        ,  1.06317905]), 'targetState': array([ 5, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.23263033824676277
running average episode reward sum: 0.3199235729288144
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 20.]), 'previousTarget': array([ 5., 20.]), 'currentState': array([ 5.79476031, 20.0825918 ,  2.55440023]), 'targetState': array([ 5, 20], dtype=int32), 'currentDistance': 0.7990402685140142}
episode index:354
target Thresh 31.257258826695754
target distance 12.0
model initialize at round 354
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 17.]), 'previousTarget': array([15., 17.]), 'currentState': array([27.03858928, 26.96422037,  5.28304911]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 15.627326049451254}
done in step count: 41
reward sum = 0.43172426496586747
running average episode reward sum: 0.3202385044556793
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 17.]), 'previousTarget': array([15., 17.]), 'currentState': array([15.89862151, 16.7511663 ,  3.3694317 ]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 0.932437033207999}
episode index:355
target Thresh 31.26464922485147
target distance 7.0
model initialize at round 355
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  4.]), 'previousTarget': array([13.,  4.]), 'currentState': array([10.        , 11.        ,  0.71745312]), 'targetState': array([13,  4], dtype=int32), 'currentDistance': 7.615773105863909}
done in step count: 17
reward sum = 0.6828938015607013
running average episode reward sum: 0.3212571991104687
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  4.]), 'previousTarget': array([13.,  4.]), 'currentState': array([12.48536461,  4.72414987,  5.08862642]), 'targetState': array([13,  4], dtype=int32), 'currentDistance': 0.8883932828912846}
episode index:356
target Thresh 31.271966087316876
target distance 18.0
model initialize at round 356
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 11.]), 'previousTarget': array([20., 11.]), 'currentState': array([15.99570313, 29.00090757,  3.16797015]), 'targetState': array([20, 11], dtype=int32), 'currentDistance': 18.440907426928764}
done in step count: 49
reward sum = 0.36257031141788026
running average episode reward sum: 0.32137292211413093
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 11.]), 'previousTarget': array([20., 11.]), 'currentState': array([20.61155371, 11.87626152,  5.07530954]), 'targetState': array([20, 11], dtype=int32), 'currentDistance': 1.0685654795817892}
episode index:357
target Thresh 31.27921014578432
target distance 17.0
model initialize at round 357
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 3.]), 'previousTarget': array([9.13497444, 3.20859686]), 'currentState': array([19.99564244, 19.5032486 ,  4.50458556]), 'targetState': array([9, 3], dtype=int32), 'currentDistance': 19.83081861288974}
done in step count: 51
reward sum = 0.36018206813109965
running average episode reward sum: 0.32148132754993247
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 3.]), 'previousTarget': array([9., 3.]), 'currentState': array([9.90497621, 3.71656598, 4.15176623]), 'targetState': array([9, 3], dtype=int32), 'currentDistance': 1.1543174367807845}
episode index:358
target Thresh 31.286382124665675
target distance 10.0
model initialize at round 358
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'previousTarget': array([10., 12.]), 'currentState': array([20.        , 18.        ,  2.61999607]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 11.661903789690601}
done in step count: 26
reward sum = 0.5895044212200397
running average episode reward sum: 0.32222790998355394
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'previousTarget': array([10., 12.]), 'currentState': array([10.96806637, 12.71974722,  3.92052977]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 1.2063119695505231}
episode index:359
target Thresh 31.29348274116482
target distance 17.0
model initialize at round 359
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 22.]), 'previousTarget': array([19., 22.]), 'currentState': array([ 2.00446883, 23.96956266,  5.0968174 ]), 'targetState': array([19, 22], dtype=int32), 'currentDistance': 17.109273997223617}
done in step count: 43
reward sum = 0.3999293346199132
running average episode reward sum: 0.3224437472742105
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 22.]), 'previousTarget': array([19., 22.]), 'currentState': array([18.25327358, 21.03257126,  0.2818869 ]), 'targetState': array([19, 22], dtype=int32), 'currentDistance': 1.2220960277329702}
episode index:360
target Thresh 31.30051270534931
target distance 7.0
model initialize at round 360
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  9.]), 'previousTarget': array([19.,  9.]), 'currentState': array([12.34202468,  4.04654667,  0.17405209]), 'targetState': array([19,  9], dtype=int32), 'currentDistance': 8.29851404051666}
done in step count: 16
reward sum = 0.7136462470737955
running average episode reward sum: 0.3235274107085584
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  9.]), 'previousTarget': array([19.,  9.]), 'currentState': array([18.45422365,  8.014408  ,  0.93718684]), 'targetState': array([19,  9], dtype=int32), 'currentDistance': 1.126615909856318}
episode index:361
target Thresh 31.30747272022143
target distance 13.0
model initialize at round 361
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 15.]), 'previousTarget': array([ 2., 15.]), 'currentState': array([12.00624815, 27.99691816,  5.57245922]), 'targetState': array([ 2, 15], dtype=int32), 'currentDistance': 16.402587708146843}
done in step count: 41
reward sum = 0.4244741672579133
running average episode reward sum: 0.3238062691520649
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 15.]), 'previousTarget': array([ 2., 15.]), 'currentState': array([ 2.88292925, 15.74202693,  4.45794297]), 'targetState': array([ 2, 15], dtype=int32), 'currentDistance': 1.1533291088081092}
episode index:362
target Thresh 31.31436348178846
target distance 13.0
model initialize at round 362
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([22.03038354,  7.00331513,  0.36117953]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 13.93623249380933}
done in step count: 40
reward sum = 0.4766245313003592
running average episode reward sum: 0.3242272561001318
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([16.91962962, 19.06602589,  2.20556012]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 0.9374257480107467}
episode index:363
target Thresh 31.32118567913231
target distance 7.0
model initialize at round 363
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 18.]), 'previousTarget': array([11., 18.]), 'currentState': array([18.        , 11.        ,  4.11810207]), 'targetState': array([11, 18], dtype=int32), 'currentDistance': 9.899494936611665}
done in step count: 21
reward sum = 0.6275382091599196
running average episode reward sum: 0.32506052794919715
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 18.]), 'previousTarget': array([11., 18.]), 'currentState': array([11.90604956, 17.0804773 ,  2.37303174]), 'targetState': array([11, 18], dtype=int32), 'currentDistance': 1.2909096811792105}
episode index:364
target Thresh 31.32793999447839
target distance 16.0
model initialize at round 364
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 21.]), 'previousTarget': array([ 8., 21.]), 'currentState': array([12.95798254,  4.93376851,  3.90452027]), 'targetState': array([ 8, 21], dtype=int32), 'currentDistance': 16.81384504487358}
done in step count: 44
reward sum = 0.38171610693806196
running average episode reward sum: 0.3252157487135502
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 21.]), 'previousTarget': array([ 8., 21.]), 'currentState': array([ 7.06402612, 20.15279693,  1.48142377]), 'targetState': array([ 8, 21], dtype=int32), 'currentDistance': 1.2624579762277164}
episode index:365
target Thresh 31.334627103263863
target distance 4.0
model initialize at round 365
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 14.]), 'previousTarget': array([ 9., 14.]), 'currentState': array([ 5.98468085, 17.7793919 ,  4.88593921]), 'targetState': array([ 9, 14], dtype=int32), 'currentDistance': 4.8348684250373735}
done in step count: 8
reward sum = 0.8317851906893137
running average episode reward sum: 0.3265998182271452
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 14.]), 'previousTarget': array([ 9., 14.]), 'currentState': array([ 8.42664697, 14.72006458,  5.51714877]), 'targetState': array([ 9, 14], dtype=int32), 'currentDistance': 0.9204491778726229}
episode index:366
target Thresh 31.341247674205185
target distance 2.0
model initialize at round 366
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 26.]), 'previousTarget': array([18., 26.]), 'currentState': array([15.99859382, 25.92858248,  4.91462246]), 'targetState': array([18, 26], dtype=int32), 'currentDistance': 2.0026799968507834}
done in step count: 4
reward sum = 0.919813476450514
running average episode reward sum: 0.32821620421685466
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 26.]), 'previousTarget': array([18., 26.]), 'currentState': array([17.0469971 , 25.22019104,  6.0674333 ]), 'targetState': array([18, 26], dtype=int32), 'currentDistance': 1.2313880499958236}
episode index:367
target Thresh 31.347802369364967
target distance 22.0
model initialize at round 367
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7.56157909, 7.40465794]), 'previousTarget': array([7.49734288, 7.43242207]), 'currentState': array([27.04236697,  2.87708647,  4.79182482]), 'targetState': array([5, 8], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 60
reward sum = 0.2563012209883074
running average episode reward sum: 0.32802078306677707
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 8.]), 'previousTarget': array([5., 8.]), 'currentState': array([5.84170228, 7.33247511, 2.8972766 ]), 'targetState': array([5, 8], dtype=int32), 'currentDistance': 1.0742682190270423}
episode index:368
target Thresh 31.35429184421818
target distance 20.0
model initialize at round 368
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4.25304229, 27.1565257 ]), 'previousTarget': array([ 4.25304229, 27.1565257 ]), 'currentState': array([10.        ,  8.        ,  2.91221571]), 'targetState': array([ 4, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.3198321888881187
running average episode reward sum: 0.32799859175463975
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 28.]), 'previousTarget': array([ 4., 28.]), 'currentState': array([ 4.25659569, 27.01196077,  2.10291413]), 'targetState': array([ 4, 28], dtype=int32), 'currentDistance': 1.0208148103689563}
episode index:369
target Thresh 31.36071674771773
target distance 22.0
model initialize at round 369
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.82541376,  9.78146944]), 'previousTarget': array([15.82541376,  9.78146944]), 'currentState': array([26.        , 27.        ,  2.01861024]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.18548725130940868
running average episode reward sum: 0.32761342596965265
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  5.]), 'previousTarget': array([13.,  5.]), 'currentState': array([13.15181733,  5.74055911,  4.23003296]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 0.7559605080636163}
episode index:370
target Thresh 31.367077722359312
target distance 12.0
model initialize at round 370
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 19.]), 'previousTarget': array([25., 19.]), 'currentState': array([13.01401375, 17.97914642,  5.55657959]), 'targetState': array([25, 19], dtype=int32), 'currentDistance': 12.029381045518203}
done in step count: 27
reward sum = 0.5649597414737279
running average episode reward sum: 0.32825317345079574
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 19.]), 'previousTarget': array([25., 19.]), 'currentState': array([24.13644603, 19.43487626,  0.3091941 ]), 'targetState': array([25, 19], dtype=int32), 'currentDistance': 0.9668727002228147}
episode index:371
target Thresh 31.373375404245692
target distance 9.0
model initialize at round 371
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  4.]), 'previousTarget': array([27.,  4.]), 'currentState': array([27.        , 13.        ,  0.88002923]), 'targetState': array([27,  4], dtype=int32), 'currentDistance': 9.0}
done in step count: 21
reward sum = 0.624755974023694
running average episode reward sum: 0.32905022398997025
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  4.]), 'previousTarget': array([27.,  4.]), 'currentState': array([27.30726945,  4.95636381,  4.59123955]), 'targetState': array([27,  4], dtype=int32), 'currentDistance': 1.0045129405320221}
episode index:372
target Thresh 31.379610423150307
target distance 6.0
model initialize at round 372
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 19.]), 'previousTarget': array([26., 19.]), 'currentState': array([22.        , 13.        ,  3.64962354]), 'targetState': array([26, 19], dtype=int32), 'currentDistance': 7.211102550927978}
done in step count: 24
reward sum = 0.6292325297893868
running average episode reward sum: 0.3298550022897006
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 19.]), 'previousTarget': array([26., 19.]), 'currentState': array([25.01536465, 19.54791075,  0.55202884]), 'targetState': array([26, 19], dtype=int32), 'currentDistance': 1.1268154062353821}
episode index:373
target Thresh 31.385783402580245
target distance 22.0
model initialize at round 373
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.2008914 , 13.16812954]), 'previousTarget': array([24.91786413, 13.18928508]), 'currentState': array([ 5.2876555 , 15.02905292,  0.24057823]), 'targetState': array([27, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.30635720436412534
running average episode reward sum: 0.3297921739530012
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 13.]), 'previousTarget': array([27., 13.]), 'currentState': array([26.10309277, 13.5613694 ,  6.05318171]), 'targetState': array([27, 13], dtype=int32), 'currentDistance': 1.0581012120437447}
episode index:374
target Thresh 31.39189495983859
target distance 19.0
model initialize at round 374
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 15.]), 'previousTarget': array([ 6., 15.]), 'currentState': array([25.        , 11.        ,  1.33971721]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 19.4164878389476}
done in step count: 53
reward sum = 0.32569337023646877
running average episode reward sum: 0.3297812438097571
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 15.]), 'previousTarget': array([ 6., 15.]), 'currentState': array([ 6.60472169, 15.42278353,  3.36733421]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 0.7378578659407451}
episode index:375
target Thresh 31.397945706086166
target distance 16.0
model initialize at round 375
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  2.]), 'previousTarget': array([25.,  2.]), 'currentState': array([8.96073508, 4.99946862, 3.31530741]), 'targetState': array([25,  2], dtype=int32), 'currentDistance': 16.317316907338927}
done in step count: 45
reward sum = 0.39075292496523917
running average episode reward sum: 0.32994340253623444
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  2.]), 'previousTarget': array([25.,  2.]), 'currentState': array([24.15499688,  2.86271173,  6.10324206]), 'targetState': array([25,  2], dtype=int32), 'currentDistance': 1.2076016774136682}
episode index:376
target Thresh 31.403936246402637
target distance 13.0
model initialize at round 376
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([11.       , 15.       ,  5.9909488]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 13.341664064126334}
done in step count: 29
reward sum = 0.5286438529038697
running average episode reward sum: 0.33047045943376135
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.95626617,  2.85342474,  5.05161128]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.8545445744519474}
episode index:377
target Thresh 31.409867179847026
target distance 9.0
model initialize at round 377
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 23.]), 'previousTarget': array([ 7., 23.]), 'currentState': array([15.97872944, 20.0346767 ,  2.35134894]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 9.455724434242995}
done in step count: 19
reward sum = 0.6694049116983212
running average episode reward sum: 0.3313671114238792
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 23.]), 'previousTarget': array([ 7., 23.]), 'currentState': array([ 7.94567589, 22.51378594,  3.10416223]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 1.0633470754348189}
episode index:378
target Thresh 31.41573909951762
target distance 21.0
model initialize at round 378
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9.86900621, 26.38830801]), 'previousTarget': array([ 9.87838597, 26.3829006 ]), 'currentState': array([1.92123226, 8.03530038, 2.46777177]), 'targetState': array([11, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.26689321951411044
running average episode reward sum: 0.3311969956140909
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 29.]), 'previousTarget': array([11., 29.]), 'currentState': array([10.37520693, 28.11170817,  1.13180683]), 'targetState': array([11, 29], dtype=int32), 'currentDistance': 1.0860150801250144}
episode index:379
target Thresh 31.42155259261128
target distance 13.0
model initialize at round 379
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  9.]), 'previousTarget': array([20.,  9.]), 'currentState': array([14.00551027, 21.78690361,  4.85544817]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 14.122280663841133}
done in step count: 35
reward sum = 0.5175766405106046
running average episode reward sum: 0.33168746836381857
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  9.]), 'previousTarget': array([20.,  9.]), 'currentState': array([19.17921223,  9.66202389,  5.35818595]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 1.0544990253665119}
episode index:380
target Thresh 31.427308240482162
target distance 9.0
model initialize at round 380
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 22.]), 'previousTarget': array([15., 22.]), 'currentState': array([20.       , 13.       ,  0.8119936]), 'targetState': array([15, 22], dtype=int32), 'currentDistance': 10.295630140987}
done in step count: 27
reward sum = 0.6153554047906048
running average episode reward sum: 0.3324320036300306
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 22.]), 'previousTarget': array([15., 22.]), 'currentState': array([15.97069017, 21.09773805,  2.55459845]), 'targetState': array([15, 22], dtype=int32), 'currentDistance': 1.325260740806319}
episode index:381
target Thresh 31.433006618699846
target distance 12.0
model initialize at round 381
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 24.]), 'previousTarget': array([18., 24.]), 'currentState': array([ 8.        , 12.        ,  4.84234667]), 'targetState': array([18, 24], dtype=int32), 'currentDistance': 15.620499351813308}
done in step count: 39
reward sum = 0.4313630765352936
running average episode reward sum: 0.33269098549627474
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 24.]), 'previousTarget': array([18., 24.]), 'currentState': array([17.01908212, 23.33195359,  0.89058721]), 'targetState': array([18, 24], dtype=int32), 'currentDistance': 1.1867964811516127}
episode index:382
target Thresh 31.4386482971069
target distance 18.0
model initialize at round 382
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.26382675, 20.5287039 ]), 'previousTarget': array([14.28714138, 20.48314552]), 'currentState': array([24.03310125,  3.07701986,  1.40194124]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.3383823139365949
running average episode reward sum: 0.33270584536165415
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([14.74004495, 20.03949124,  2.11037897]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 1.2125360262242728}
episode index:383
target Thresh 31.444233839875878
target distance 17.0
model initialize at round 383
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 28.]), 'previousTarget': array([ 3., 28.]), 'currentState': array([19.7940415 , 19.24557932,  2.33366641]), 'targetState': array([ 3, 28], dtype=int32), 'currentDistance': 18.938841340842373}
done in step count: 52
reward sum = 0.3785415237420944
running average episode reward sum: 0.33282520910743657
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 28.]), 'previousTarget': array([ 3., 28.]), 'currentState': array([ 3.19930828, 27.05472043,  2.46138113]), 'targetState': array([ 3, 28], dtype=int32), 'currentDistance': 0.9660627614876698}
episode index:384
target Thresh 31.4497638055657
target distance 20.0
model initialize at round 384
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.638375  ,  7.47568183]), 'previousTarget': array([15.638375  ,  7.47568183]), 'currentState': array([ 6.        , 25.        ,  4.63505131]), 'targetState': array([17,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.28751750667962483
running average episode reward sum: 0.3327075267634682
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  5.]), 'previousTarget': array([17.,  5.]), 'currentState': array([16.76722081,  5.90695019,  5.13696962]), 'targetState': array([17,  5], dtype=int32), 'currentDistance': 0.93634651781594}
episode index:385
target Thresh 31.455238747177546
target distance 18.0
model initialize at round 385
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 11.]), 'previousTarget': array([27., 11.]), 'currentState': array([24.01496319, 29.07851829,  1.12998486]), 'targetState': array([27, 11], dtype=int32), 'currentDistance': 18.32329851412069}
done in step count: 47
reward sum = 0.3405196080348146
running average episode reward sum: 0.33272776531598464
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 11.]), 'previousTarget': array([27., 11.]), 'currentState': array([27.35889684, 11.85491773,  4.79481511]), 'targetState': array([27, 11], dtype=int32), 'currentDistance': 0.927195379072124}
episode index:386
target Thresh 31.46065921221014
target distance 16.0
model initialize at round 386
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.78625236,  5.27402434]), 'previousTarget': array([23.6118525,  5.47772  ]), 'currentState': array([11.48530671, 21.0438289 ,  6.12075273]), 'targetState': array([24,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.3294875889629665
running average episode reward sum: 0.3327193927672688
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  5.]), 'previousTarget': array([24.,  5.]), 'currentState': array([23.23300609,  5.39596594,  5.55497224]), 'targetState': array([24,  5], dtype=int32), 'currentDistance': 0.8631736107680053}
episode index:387
target Thresh 31.466025742714503
target distance 5.0
model initialize at round 387
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 14.]), 'previousTarget': array([ 9., 14.]), 'currentState': array([ 4.32000329, 14.73045307,  5.67323193]), 'targetState': array([ 9, 14], dtype=int32), 'currentDistance': 4.73665820196333}
done in step count: 9
reward sum = 0.8381348136936042
running average episode reward sum: 0.334022009831512
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 14.]), 'previousTarget': array([ 9., 14.]), 'currentState': array([ 8.01620483, 13.88223629,  0.12296135]), 'targetState': array([ 9, 14], dtype=int32), 'currentDistance': 0.9908184590141803}
episode index:388
target Thresh 31.47133887534816
target distance 2.0
model initialize at round 388
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 24.]), 'previousTarget': array([ 8., 24.]), 'currentState': array([ 6.06455844, 25.56913577,  5.00602701]), 'targetState': array([ 8, 24], dtype=int32), 'currentDistance': 2.4916101418080023}
done in step count: 4
reward sum = 0.9126662172219043
running average episode reward sum: 0.33550952707416076
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 24.]), 'previousTarget': array([ 8., 24.]), 'currentState': array([ 7.14430698, 24.14997703,  5.68511446]), 'targetState': array([ 8, 24], dtype=int32), 'currentDistance': 0.8687368122128596}
episode index:389
target Thresh 31.476599141428796
target distance 18.0
model initialize at round 389
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 14.]), 'previousTarget': array([25., 14.]), 'currentState': array([7.        , 9.        , 2.52504063]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 18.681541692269406}
done in step count: 49
reward sum = 0.3420093062937232
running average episode reward sum: 0.33552619317472376
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 14.]), 'previousTarget': array([25., 14.]), 'currentState': array([24.23828875, 13.9718756 ,  0.21508487]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 0.7622302843796115}
episode index:390
target Thresh 31.481807066987407
target distance 23.0
model initialize at round 390
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.0902613 ,  9.46528005]), 'previousTarget': array([21.62485559,  9.28798697]), 'currentState': array([3.47320836, 2.15738435, 0.44756359]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.22025733264103337
running average episode reward sum: 0.33523138790481666
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'previousTarget': array([26., 11.]), 'currentState': array([26.16205847, 10.29121014,  1.1776506 ]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 0.7270804720352956}
episode index:391
target Thresh 31.48696317282089
target distance 9.0
model initialize at round 391
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 14.]), 'previousTarget': array([23., 14.]), 'currentState': array([15.03177324,  5.49839917,  1.42219514]), 'targetState': array([23, 14], dtype=int32), 'currentDistance': 11.652032201068435}
done in step count: 27
reward sum = 0.5668806985461808
running average episode reward sum: 0.33582233002379974
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 14.]), 'previousTarget': array([23., 14.]), 'currentState': array([22.04832574, 14.18921403,  0.71422807]), 'targetState': array([23, 14], dtype=int32), 'currentDistance': 0.9703019383261408}
episode index:392
target Thresh 31.49206797454412
target distance 22.0
model initialize at round 392
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.74862134, 23.87769644]), 'previousTarget': array([20.73750984, 23.87322975]), 'currentState': array([10.03052044,  6.99213122,  6.27458984]), 'targetState': array([24, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.2002982610767488
running average episode reward sum: 0.3354774850646469
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 29.]), 'previousTarget': array([24., 29.]), 'currentState': array([23.53560883, 28.21000538,  0.80985364]), 'targetState': array([24, 29], dtype=int32), 'currentDistance': 0.9163791006586791}
episode index:393
target Thresh 31.497121982641527
target distance 11.0
model initialize at round 393
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 24.]), 'previousTarget': array([15., 24.]), 'currentState': array([20.        , 13.        ,  5.95974106]), 'targetState': array([15, 24], dtype=int32), 'currentDistance': 12.083045973594572}
done in step count: 30
reward sum = 0.5262197887580924
running average episode reward sum: 0.3359616025867115
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 24.]), 'previousTarget': array([15., 24.]), 'currentState': array([15.05289005, 23.14994919,  1.96670714]), 'targetState': array([15, 24], dtype=int32), 'currentDistance': 0.8516946251928172}
episode index:394
target Thresh 31.502125702518136
target distance 23.0
model initialize at round 394
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6.11006407, 21.5704125 ]), 'previousTarget': array([ 6.11006407, 21.5704125 ]), 'currentState': array([25.       , 15.       ,  5.2357918]), 'targetState': array([ 2, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.20687268369848855
running average episode reward sum: 0.335634795197121
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 23.]), 'previousTarget': array([ 2., 23.]), 'currentState': array([ 2.78787482, 22.32339198,  2.72522866]), 'targetState': array([ 2, 23], dtype=int32), 'currentDistance': 1.0385302815232975}
episode index:395
target Thresh 31.507079634550095
target distance 18.0
model initialize at round 395
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  9.]), 'previousTarget': array([21.,  9.]), 'currentState': array([ 3.        , 10.        ,  1.98849219]), 'targetState': array([21,  9], dtype=int32), 'currentDistance': 18.027756377319943}
done in step count: 45
reward sum = 0.36979976528813496
running average episode reward sum: 0.3357210703741185
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  9.]), 'previousTarget': array([21.,  9.]), 'currentState': array([20.07799273,  9.4287078 ,  6.1570321 ]), 'targetState': array([21,  9], dtype=int32), 'currentDistance': 1.0168027289502248}
episode index:396
target Thresh 31.511984274134743
target distance 13.0
model initialize at round 396
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 24.]), 'previousTarget': array([13., 24.]), 'currentState': array([ 2.16170643, 11.46254144,  1.13763447]), 'targetState': array([13, 24], dtype=int32), 'currentDistance': 16.57276303567836}
done in step count: 43
reward sum = 0.4499961396334613
running average episode reward sum: 0.3360089168961824
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 24.]), 'previousTarget': array([13., 24.]), 'currentState': array([12.12312287, 23.1498824 ,  0.98032493]), 'targetState': array([13, 24], dtype=int32), 'currentDistance': 1.2213162674056712}
episode index:397
target Thresh 31.516840111740123
target distance 20.0
model initialize at round 397
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5.38429193, 14.61813311]), 'previousTarget': array([ 5.61536159, 14.46924689]), 'currentState': array([21.73967395,  3.10720444,  2.77599092]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 70
reward sum = 0.26955814490750346
running average episode reward sum: 0.33584195515751736
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'previousTarget': array([ 2., 17.]), 'currentState': array([ 2.78351787, 16.48690785,  2.42566409]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 0.9365702361876834}
episode index:398
target Thresh 31.521647632954043
target distance 16.0
model initialize at round 398
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 23.]), 'previousTarget': array([21., 23.]), 'currentState': array([16.95387485,  7.03754732,  2.23635265]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 16.46727130852101}
done in step count: 39
reward sum = 0.4416542010777944
running average episode reward sum: 0.33610714875631503
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 23.]), 'previousTarget': array([21., 23.]), 'currentState': array([20.37587482, 22.24569962,  1.33561765]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 0.9790307936040111}
episode index:399
target Thresh 31.526407318532627
target distance 9.0
model initialize at round 399
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([22.98996063,  2.93909684,  4.31548151]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 9.223161823253001}
done in step count: 21
reward sum = 0.6393123609420028
running average episode reward sum: 0.3368651617867793
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.80444874,  4.71870125,  2.80112081]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.8522128592424225}
episode index:400
target Thresh 31.531119644448406
target distance 21.0
model initialize at round 400
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6.99469707, 10.47290771]), 'previousTarget': array([ 6.99469707, 10.47290771]), 'currentState': array([24.        , 21.        ,  2.86171365]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.22265231690304632
running average episode reward sum: 0.33658034172472506
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.80629416, 8.41430636, 3.93148162]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.9065098072192567}
episode index:401
target Thresh 31.53578508193789
target distance 8.0
model initialize at round 401
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 18.]), 'previousTarget': array([ 2., 18.]), 'currentState': array([10.        , 21.        ,  2.69769359]), 'targetState': array([ 2, 18], dtype=int32), 'currentDistance': 8.54400374531753}
done in step count: 20
reward sum = 0.6800046982134019
running average episode reward sum: 0.33743463116872674
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 18.]), 'previousTarget': array([ 2., 18.]), 'currentState': array([ 2.94356096, 17.92111465,  3.30862514]), 'targetState': array([ 2, 18], dtype=int32), 'currentDistance': 0.9468527800215445}
episode index:402
target Thresh 31.540404097548727
target distance 15.0
model initialize at round 402
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 24.]), 'previousTarget': array([10., 24.]), 'currentState': array([25.       , 27.       ,  2.9042646]), 'targetState': array([10, 24], dtype=int32), 'currentDistance': 15.297058540778355}
done in step count: 36
reward sum = 0.4743934112173699
running average episode reward sum: 0.33777447925817744
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 24.]), 'previousTarget': array([10., 24.]), 'currentState': array([10.80294324, 23.72975999,  2.93044116]), 'targetState': array([10, 24], dtype=int32), 'currentDistance': 0.847199800969737}
episode index:403
target Thresh 31.54497715318632
target distance 8.0
model initialize at round 403
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  3.]), 'previousTarget': array([10.,  3.]), 'currentState': array([2.33232639, 4.35315366, 0.70135567]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 7.78615716460728}
done in step count: 16
reward sum = 0.7140958092014295
running average episode reward sum: 0.33870596769863104
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  3.]), 'previousTarget': array([10.,  3.]), 'currentState': array([9.19778647, 3.77732384, 5.90901678]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 1.1170402385614304}
episode index:404
target Thresh 31.54950470616004
target distance 21.0
model initialize at round 404
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.40441301, 19.47901662]), 'previousTarget': array([20.14213562, 19.14213562]), 'currentState': array([6.18207069, 5.41754519, 1.06633025]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 75
reward sum = 0.15219829024649095
running average episode reward sum: 0.3382454549147986
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 26.]), 'previousTarget': array([27., 26.]), 'currentState': array([26.08957606, 25.80855536,  0.75909452]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 0.9303347788370899}
episode index:405
target Thresh 31.553987209228968
target distance 8.0
model initialize at round 405
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([12.00030898,  3.06815387,  1.81678078]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 8.054395131308675}
done in step count: 18
reward sum = 0.6860694883325457
running average episode reward sum: 0.33910216435671425
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.70899278, 3.59523059, 3.07991248]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.8164000538495281}
episode index:406
target Thresh 31.558425110647136
target distance 10.0
model initialize at round 406
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  6.]), 'previousTarget': array([23.,  6.]), 'currentState': array([13.03323471,  9.04952481,  0.78203636]), 'targetState': array([23,  6], dtype=int32), 'currentDistance': 10.422860062073466}
done in step count: 24
reward sum = 0.6018841348946263
running average episode reward sum: 0.3397478203039819
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  6.]), 'previousTarget': array([23.,  6.]), 'currentState': array([22.23188634,  6.68392019,  5.99913192]), 'targetState': array([23,  6], dtype=int32), 'currentDistance': 1.0284675109966712}
episode index:407
target Thresh 31.562818854208388
target distance 11.0
model initialize at round 407
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 15.]), 'previousTarget': array([13., 15.]), 'currentState': array([ 1.99701467, 26.01957875,  1.46960914]), 'targetState': array([13, 15], dtype=int32), 'currentDistance': 15.572308825944834}
done in step count: 35
reward sum = 0.4190409495316286
running average episode reward sum: 0.33994216620895157
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 15.]), 'previousTarget': array([13., 15.]), 'currentState': array([12.43643738, 15.96420255,  5.53603332]), 'targetState': array([13, 15], dtype=int32), 'currentDistance': 1.1168211061366626}
episode index:408
target Thresh 31.567168879290744
target distance 7.0
model initialize at round 408
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([14.00020965,  5.9999208 ,  6.17176768]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 8.60220070504687}
done in step count: 19
reward sum = 0.6843993362123583
running average episode reward sum: 0.340784359778642
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([20.22347389, 10.55734771,  0.64209021]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 0.8938309968409691}
episode index:409
target Thresh 31.571475620900337
target distance 10.0
model initialize at round 409
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([ 8.03190399, 23.99013727,  5.74480429]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 9.990188214063782}
done in step count: 21
reward sum = 0.6286032079078178
running average episode reward sum: 0.341486356969201
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([ 8.70314554, 14.87005406,  4.38330731]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 1.118663364618728}
episode index:410
target Thresh 31.57573950971491
target distance 18.0
model initialize at round 410
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 26.]), 'previousTarget': array([ 4., 26.]), 'currentState': array([7.        , 8.        , 1.28428522]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 18.248287590894662}
done in step count: 47
reward sum = 0.40376064943342116
running average episode reward sum: 0.34163787592896794
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 26.]), 'previousTarget': array([ 4., 26.]), 'currentState': array([ 3.56522961, 25.21600468,  1.43815405]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 0.896478642870508}
episode index:411
target Thresh 31.57996097212691
target distance 11.0
model initialize at round 411
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([10.77593109, 13.69123546,  4.07152283]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 13.831828596610215}
done in step count: 33
reward sum = 0.534830236456129
running average episode reward sum: 0.3421067894253931
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.81568918, 3.55292937, 3.64670214]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.9854337749752446}
episode index:412
target Thresh 31.584140430286084
target distance 23.0
model initialize at round 412
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.51669868, 12.9444182 ]), 'previousTarget': array([23.13347761, 12.82323232]), 'currentState': array([4.37626542, 7.14408939, 0.45663537]), 'targetState': array([27, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.2688224780770856
running average episode reward sum: 0.34192934557224947
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 14.]), 'previousTarget': array([27., 14.]), 'currentState': array([26.03247785, 13.85251245,  0.36523863]), 'targetState': array([27, 14], dtype=int32), 'currentDistance': 0.9786989816752074}
episode index:413
target Thresh 31.588278302141735
target distance 13.0
model initialize at round 413
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 12.]), 'previousTarget': array([26., 12.]), 'currentState': array([19.93069171, 24.94620227,  4.05415773]), 'targetState': array([26, 12], dtype=int32), 'currentDistance': 14.298274591927534}
done in step count: 34
reward sum = 0.4909970835284734
running average episode reward sum: 0.3422894125721437
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 12.]), 'previousTarget': array([26., 12.]), 'currentState': array([25.41329012, 12.71791521,  5.24039898]), 'targetState': array([26, 12], dtype=int32), 'currentDistance': 0.9271627308981332}
episode index:414
target Thresh 31.592375001484502
target distance 21.0
model initialize at round 414
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.23047895, 12.49442256]), 'previousTarget': array([22.23047895, 12.49442256]), 'currentState': array([3.        , 7.        , 2.11151692]), 'targetState': array([24, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.2640005504292323
running average episode reward sum: 0.3421007647115584
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 13.]), 'previousTarget': array([24., 13.]), 'currentState': array([23.00854905, 13.4553891 ,  0.36247802]), 'targetState': array([24, 13], dtype=int32), 'currentDistance': 1.0910335522351353}
episode index:415
target Thresh 31.596430937987726
target distance 16.0
model initialize at round 415
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.16881121,  3.94896325]), 'previousTarget': array([25.17009216,  3.94846611]), 'currentState': array([11.99112474, 18.99384873,  4.00015926]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.29268943544645354
running average episode reward sum: 0.34198198747774805
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  3.]), 'previousTarget': array([26.,  3.]), 'currentState': array([25.37871543,  3.86781549,  5.22324759]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 1.0672854524953626}
episode index:416
target Thresh 31.60044651724844
target distance 1.0
model initialize at round 416
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.91575529, 5.07154922, 2.49497223]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.074855759182306}
done in step count: 7
reward sum = 0.9116075138947766
running average episode reward sum: 0.3433479959343836
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.2136755 , 4.89293626, 4.12064634]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.1898072918095983}
episode index:417
target Thresh 31.604422140827918
target distance 15.0
model initialize at round 417
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 21.]), 'previousTarget': array([ 7., 21.]), 'currentState': array([21.52279743, 16.865105  ,  3.19282648]), 'targetState': array([ 7, 21], dtype=int32), 'currentDistance': 15.099966947777732}
done in step count: 37
reward sum = 0.48029156160097086
running average episode reward sum: 0.3436756121201889
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 21.]), 'previousTarget': array([ 7., 21.]), 'currentState': array([ 7.62799425, 20.18508346,  2.40220383]), 'targetState': array([ 7, 21], dtype=int32), 'currentDistance': 1.0288176501357047}
episode index:418
target Thresh 31.608358206291825
target distance 10.0
model initialize at round 418
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([14.99230181,  4.00637075,  2.19776845]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 12.205755178971845}
done in step count: 26
reward sum = 0.5539927785028008
running average episode reward sum: 0.3441775623979516
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([21.57538594, 13.20969481,  0.98641403]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 0.8971507097334747}
episode index:419
target Thresh 31.612255107249997
target distance 3.0
model initialize at round 419
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 20.]), 'previousTarget': array([ 9., 20.]), 'currentState': array([12.03812046, 18.97404853,  5.47288796]), 'targetState': array([ 9, 20], dtype=int32), 'currentDistance': 3.206673094505781}
done in step count: 10
reward sum = 0.8261786824827635
running average episode reward sum: 0.3453251841124393
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 20.]), 'previousTarget': array([ 9., 20.]), 'currentState': array([ 9.8974334 , 19.10425954,  2.57116891]), 'targetState': array([ 9, 20], dtype=int32), 'currentDistance': 1.2679659630953144}
episode index:420
target Thresh 31.616113233395772
target distance 20.0
model initialize at round 420
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.57218647, 21.56953382]), 'previousTarget': array([12.57218647, 21.56953382]), 'currentState': array([20.        ,  3.        ,  0.98199767]), 'targetState': array([12, 23], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 57
reward sum = 0.2982430922267499
running average episode reward sum: 0.34521335016496735
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 23.]), 'previousTarget': array([12., 23.]), 'currentState': array([12.48620172, 22.09579877,  2.1611388 ]), 'targetState': array([12, 23], dtype=int32), 'currentDistance': 1.0266313710094892}
episode index:421
target Thresh 31.619932970544976
target distance 14.0
model initialize at round 421
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([10.97174395, 21.97174357,  4.1578418 ]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 17.79977668300311}
done in step count: 42
reward sum = 0.3909019479649664
running average episode reward sum: 0.34532161698439867
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([21.73623377,  8.94415093,  5.18753527]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 0.9803028136806813}
episode index:422
target Thresh 31.623714700674515
target distance 13.0
model initialize at round 422
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 18.]), 'previousTarget': array([ 7., 18.]), 'currentState': array([18.        ,  5.        ,  1.41051608]), 'targetState': array([ 7, 18], dtype=int32), 'currentDistance': 17.029386365926403}
done in step count: 38
reward sum = 0.4281388831146299
running average episode reward sum: 0.3455174024835245
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 18.]), 'previousTarget': array([ 7., 18.]), 'currentState': array([ 7.78882806, 17.41064282,  2.32267514]), 'targetState': array([ 7, 18], dtype=int32), 'currentDistance': 0.984678424562572}
episode index:423
target Thresh 31.627458801960547
target distance 4.0
model initialize at round 423
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([19.50304307,  3.95700713,  3.34847221]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 4.012628521665143}
done in step count: 6
reward sum = 0.876090591507726
running average episode reward sum: 0.34676875434443066
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.95520862,  2.73961056,  3.72280268]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.2080758645689018}
episode index:424
target Thresh 31.631165648816324
target distance 7.0
model initialize at round 424
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 22.]), 'previousTarget': array([11., 22.]), 'currentState': array([ 3.97846814, 21.95327558,  4.53306818]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 7.021687317594512}
done in step count: 17
reward sum = 0.7006049926125304
running average episode reward sum: 0.34760131019917917
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 22.]), 'previousTarget': array([11., 22.]), 'currentState': array([10.36237055, 21.56677754,  6.22188883]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 0.770878078660136}
episode index:425
target Thresh 31.63483561192962
target distance 18.0
model initialize at round 425
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 24.]), 'previousTarget': array([21., 24.]), 'currentState': array([14.8157981 ,  6.45924399,  1.69974899]), 'targetState': array([21, 24], dtype=int32), 'currentDistance': 18.598991218914936}
done in step count: 47
reward sum = 0.37982818654197653
running average episode reward sum: 0.3476769601436458
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 24.]), 'previousTarget': array([21., 24.]), 'currentState': array([20.56785221, 23.10516917,  1.16000754]), 'targetState': array([21, 24], dtype=int32), 'currentDistance': 0.9937172315955999}
episode index:426
target Thresh 31.6384690582998
target distance 22.0
model initialize at round 426
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.49513674, 14.58454583]), 'previousTarget': array([21.51093912, 14.57265691]), 'currentState': array([2.95035258, 7.09515461, 1.79919624]), 'targetState': array([25, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.22218200127499865
running average episode reward sum: 0.3473830609425483
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 16.]), 'previousTarget': array([25., 16.]), 'currentState': array([24.18175892, 15.6826551 ,  0.71108881]), 'targetState': array([25, 16], dtype=int32), 'currentDistance': 0.877625352500447}
episode index:427
target Thresh 31.64206635127454
target distance 16.0
model initialize at round 427
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 18.]), 'previousTarget': array([ 9., 18.]), 'currentState': array([24.99771238, 25.01804954,  1.94936562]), 'targetState': array([ 9, 18], dtype=int32), 'currentDistance': 17.469396685860936}
done in step count: 44
reward sum = 0.40089030106846524
running average episode reward sum: 0.34750807785873034
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 18.]), 'previousTarget': array([ 9., 18.]), 'currentState': array([ 9.83643096, 18.94579523,  3.5543728 ]), 'targetState': array([ 9, 18], dtype=int32), 'currentDistance': 1.2625946966234711}
episode index:428
target Thresh 31.645627850586123
target distance 12.0
model initialize at round 428
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 12.]), 'previousTarget': array([18., 12.]), 'currentState': array([20.96694548, 23.99175364,  3.61907607]), 'targetState': array([18, 12], dtype=int32), 'currentDistance': 12.353336420113138}
done in step count: 28
reward sum = 0.5476454765937344
running average episode reward sum: 0.3479745986017024
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 12.]), 'previousTarget': array([18., 12.]), 'currentState': array([18.23097368, 12.87534443,  4.91737435]), 'targetState': array([18, 12], dtype=int32), 'currentDistance': 0.905304764778362}
episode index:429
target Thresh 31.649153912387458
target distance 14.0
model initialize at round 429
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 14.]), 'previousTarget': array([ 3., 14.]), 'currentState': array([16.75061391, 17.88910971,  3.51389744]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 14.290016002134607}
done in step count: 32
reward sum = 0.5213815827967935
running average episode reward sum: 0.3483778706579701
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 14.]), 'previousTarget': array([ 3., 14.]), 'currentState': array([ 3.86555096, 14.73175039,  3.52158146]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 1.133418323817405}
episode index:430
target Thresh 31.652644889287657
target distance 5.0
model initialize at round 430
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'previousTarget': array([22., 18.]), 'currentState': array([17.        , 22.        ,  4.10405424]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 6.403124237432849}
done in step count: 15
reward sum = 0.741146294525164
running average episode reward sum: 0.3492891663049937
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'previousTarget': array([22., 18.]), 'currentState': array([21.14699466, 18.38928979,  5.79296542]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 0.9376378045339451}
episode index:431
target Thresh 31.65610113038732
target distance 10.0
model initialize at round 431
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 20.]), 'previousTarget': array([13., 20.]), 'currentState': array([ 3.44380623, 24.2280092 ,  0.36123756]), 'targetState': array([13, 20], dtype=int32), 'currentDistance': 10.449732114041646}
done in step count: 22
reward sum = 0.6199874122728792
running average episode reward sum: 0.34991578261510453
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 20.]), 'previousTarget': array([13., 20.]), 'currentState': array([12.19692037, 20.2465049 ,  5.84698648]), 'targetState': array([13, 20], dtype=int32), 'currentDistance': 0.8400604523296287}
episode index:432
target Thresh 31.65952298131344
target distance 4.0
model initialize at round 432
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([11.        ,  7.        ,  3.99890473]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 5.0}
done in step count: 13
reward sum = 0.7822476482685508
running average episode reward sum: 0.3509142395796621
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([7.30808456, 9.02722091, 1.94046233]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 1.0203995571975073}
episode index:433
target Thresh 31.66291078425396
target distance 19.0
model initialize at round 433
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([ 5.        , 10.        ,  5.70326042]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 19.646882704388503}
done in step count: 54
reward sum = 0.2927301767699212
running average episode reward sum: 0.35078017491881014
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([ 9.81674761, 28.18630098,  1.35087714]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 0.8340788558158114}
episode index:434
target Thresh 31.666264877991996
target distance 2.0
model initialize at round 434
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 12.]), 'previousTarget': array([26., 12.]), 'currentState': array([24.33931919, 13.98532943,  6.09488537]), 'targetState': array([26, 12], dtype=int32), 'currentDistance': 2.588318701076384}
done in step count: 5
reward sum = 0.9097494420457873
running average episode reward sum: 0.3520651617397917
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 12.]), 'previousTarget': array([26., 12.]), 'currentState': array([25.65581294, 12.86007218,  5.13785746]), 'targetState': array([26, 12], dtype=int32), 'currentDistance': 0.9263848434039434}
episode index:435
target Thresh 31.669585597939715
target distance 13.0
model initialize at round 435
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([ 8.        , 19.        ,  2.09581113]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 14.7648230602334}
done in step count: 35
reward sum = 0.4455254574601928
running average episode reward sum: 0.35227952021621467
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([20.13822643, 12.62100066,  5.64951897]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 1.062212549022884}
episode index:436
target Thresh 31.672873276171885
target distance 19.0
model initialize at round 436
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 21.]), 'previousTarget': array([15., 21.]), 'currentState': array([13.        ,  2.        ,  6.14332724]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 19.104973174542796}
done in step count: 48
reward sum = 0.3253977102668746
running average episode reward sum: 0.3522180057769713
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 21.]), 'previousTarget': array([15., 21.]), 'currentState': array([14.78226295, 20.24171836,  1.5766939 ]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 0.7889236175866315}
episode index:437
target Thresh 31.676128241459065
target distance 12.0
model initialize at round 437
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  5.]), 'previousTarget': array([17.,  5.]), 'currentState': array([17.06446447, 17.02274899,  0.08831602]), 'targetState': array([17,  5], dtype=int32), 'currentDistance': 12.022921818329506}
done in step count: 26
reward sum = 0.5393310956328283
running average episode reward sum: 0.3526452046122587
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  5.]), 'previousTarget': array([17.,  5.]), 'currentState': array([16.94015589,  5.83040685,  4.77413756]), 'targetState': array([17,  5], dtype=int32), 'currentDistance': 0.8325604179706115}
episode index:438
target Thresh 31.679350819300495
target distance 19.0
model initialize at round 438
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.84802805, 26.91551466]), 'previousTarget': array([10.89888325, 26.86398076]), 'currentState': array([26.98711572, 15.1032393 ,  1.94745457]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.2393468626984463
running average episode reward sum: 0.35238712182885595
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 29.]), 'previousTarget': array([ 8., 29.]), 'currentState': array([ 8.63242992, 28.27728218,  2.24734645]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 0.9603586058617019}
episode index:439
target Thresh 31.68254133195665
target distance 18.0
model initialize at round 439
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5.78641543, 17.70981108]), 'previousTarget': array([ 5.78641543, 17.70981108]), 'currentState': array([22.        ,  6.        ,  5.09390402]), 'targetState': array([ 4, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 70
reward sum = 0.17175471340802967
running average episode reward sum: 0.3519765936278995
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 19.]), 'previousTarget': array([ 4., 19.]), 'currentState': array([ 4.68545854, 18.75401144,  2.65252412]), 'targetState': array([ 4, 19], dtype=int32), 'currentDistance': 0.7282607929345306}
episode index:440
target Thresh 31.685700098481448
target distance 11.0
model initialize at round 440
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([14.03605966, 16.81804822,  4.83735186]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 12.972293337030042}
done in step count: 30
reward sum = 0.5062601321599796
running average episode reward sum: 0.3523264429216231
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.8178102 , 9.05052764, 2.83578482]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.253120620318462}
episode index:441
target Thresh 31.688827434754177
target distance 17.0
model initialize at round 441
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 22.]), 'previousTarget': array([ 3., 22.]), 'currentState': array([20.05286321, 16.03020512,  0.77161142]), 'targetState': array([ 3, 22], dtype=int32), 'currentDistance': 18.0676117567984}
done in step count: 44
reward sum = 0.3596208326438943
running average episode reward sum: 0.3523429460657911
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 22.]), 'previousTarget': array([ 3., 22.]), 'currentState': array([ 3.82169906, 21.59839327,  2.88149317]), 'targetState': array([ 3, 22], dtype=int32), 'currentDistance': 0.9145913339989025}
episode index:442
target Thresh 31.691923653511072
target distance 16.0
model initialize at round 442
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  9.]), 'previousTarget': array([18.,  9.]), 'currentState': array([ 2.        , 11.        ,  1.85313341]), 'targetState': array([18,  9], dtype=int32), 'currentDistance': 16.1245154965971}
done in step count: 43
reward sum = 0.4177485057520881
running average episode reward sum: 0.35249058841271275
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  9.]), 'previousTarget': array([18.,  9.]), 'currentState': array([17.14419145,  9.119072  ,  5.80667785]), 'targetState': array([18,  9], dtype=int32), 'currentDistance': 0.8640523193524763}
episode index:443
target Thresh 31.694989064376585
target distance 13.0
model initialize at round 443
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 25.]), 'previousTarget': array([27., 25.]), 'currentState': array([20.00620664, 11.99141804,  5.57211918]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 14.769439741012818}
done in step count: 36
reward sum = 0.47302856663307363
running average episode reward sum: 0.3527620703456415
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 25.]), 'previousTarget': array([27., 25.]), 'currentState': array([26.0668809 , 24.0309586 ,  0.89369656]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 1.3452704101047017}
episode index:444
target Thresh 31.69802397389436
target distance 2.0
model initialize at round 444
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([22.08165489,  6.97143617,  5.70665398]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 2.8670276647957533}
done in step count: 9
reward sum = 0.8507443782535372
running average episode reward sum: 0.35388113171172664
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([20.96042831,  5.07400241,  3.60350804]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 0.9632750834384545}
episode index:445
target Thresh 31.701028685557876
target distance 23.0
model initialize at round 445
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.45374769, 12.6225407 ]), 'previousTarget': array([19.09211044, 12.8764257 ]), 'currentState': array([ 4.42154923, 25.81469782,  5.61653519]), 'targetState': array([27,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.08700020645599549
running average episode reward sum: 0.353282743986938
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  6.]), 'previousTarget': array([27.,  6.]), 'currentState': array([26.24740911,  6.64564678,  5.49849814]), 'targetState': array([27,  6], dtype=int32), 'currentDistance': 0.9915910524018734}
episode index:446
target Thresh 31.704003499840805
target distance 19.0
model initialize at round 446
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([4.96184751, 3.01761233, 2.46436465]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 19.006740705675174}
done in step count: 42
reward sum = 0.36129143231560984
running average episode reward sum: 0.35330066051563747
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([ 4.35331363, 21.1876551 ,  1.53223477]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 0.8858525608371725}
episode index:447
target Thresh 31.706948714227053
target distance 21.0
model initialize at round 447
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.41826648, 13.16928442]), 'previousTarget': array([16.41826648, 13.16928442]), 'currentState': array([ 3.        , 28.        ,  3.29751149]), 'targetState': array([22,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 78
reward sum = 0.09289329969803933
running average episode reward sum: 0.352719394085241
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  7.]), 'previousTarget': array([22.,  7.]), 'currentState': array([21.0076822 ,  7.12769514,  5.7519106 ]), 'targetState': array([22,  7], dtype=int32), 'currentDistance': 1.0005002059619967}
episode index:448
target Thresh 31.709864623240517
target distance 11.0
model initialize at round 448
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 17.]), 'previousTarget': array([13., 17.]), 'currentState': array([ 7.99971738, 27.99834761,  4.77234927]), 'targetState': array([13, 17], dtype=int32), 'currentDistance': 12.08165867581049}
done in step count: 27
reward sum = 0.5835667953374974
running average episode reward sum: 0.35323353083635967
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 17.]), 'previousTarget': array([13., 17.]), 'currentState': array([12.66784799, 17.74276062,  5.12109767]), 'targetState': array([13, 17], dtype=int32), 'currentDistance': 0.8136450670294338}
episode index:449
target Thresh 31.71275151847452
target distance 10.0
model initialize at round 449
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 13.]), 'previousTarget': array([ 4., 13.]), 'currentState': array([13.53335927,  9.90574006,  3.11376569]), 'targetState': array([ 4, 13], dtype=int32), 'currentDistance': 10.02294286096326}
done in step count: 23
reward sum = 0.6319075409927011
running average episode reward sum: 0.35385280641448486
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 13.]), 'previousTarget': array([ 4., 13.]), 'currentState': array([ 4.96640412, 12.27287695,  3.13535838]), 'targetState': array([ 4, 13], dtype=int32), 'currentDistance': 1.209398547701706}
episode index:450
target Thresh 31.715609688620997
target distance 5.0
model initialize at round 450
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([24.        ,  6.        ,  6.25383615]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 5.830951894845301}
done in step count: 15
reward sum = 0.7313667548879672
running average episode reward sum: 0.35468986616719766
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([21.21909833, 10.17906511,  2.24415164]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 0.8496694454127868}
episode index:451
target Thresh 31.718439419499344
target distance 16.0
model initialize at round 451
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([4.        , 6.        , 4.66510105]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 16.0312195418814}
done in step count: 36
reward sum = 0.4263701584240222
running average episode reward sum: 0.35484845088458
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([19.03433949,  4.7502299 ,  0.15136447]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 0.997439384709704}
episode index:452
target Thresh 31.721240994085004
target distance 18.0
model initialize at round 452
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 6.]), 'previousTarget': array([9., 6.]), 'currentState': array([27.        ,  9.        ,  1.17214715]), 'targetState': array([9, 6], dtype=int32), 'currentDistance': 18.248287590894662}
done in step count: 48
reward sum = 0.36375827510478065
running average episode reward sum: 0.3548681193707173
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 6.]), 'previousTarget': array([9., 6.]), 'currentState': array([9.97335845, 5.98840691, 3.18779107]), 'targetState': array([9, 6], dtype=int32), 'currentDistance': 0.9734274903664936}
episode index:453
target Thresh 31.72401469253778
target distance 16.0
model initialize at round 453
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 2.]), 'previousTarget': array([5., 2.]), 'currentState': array([ 3.01060703, 17.50018646,  4.76369207]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 15.627330699415749}
done in step count: 39
reward sum = 0.4621546446449356
running average episode reward sum: 0.35510443330303937
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 2.]), 'previousTarget': array([5., 2.]), 'currentState': array([5.84758973, 2.96961135, 4.41868755]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 1.2878487985302067}
episode index:454
target Thresh 31.726760792229815
target distance 6.0
model initialize at round 454
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  8.]), 'previousTarget': array([20.,  8.]), 'currentState': array([18.04881163, 13.57854963,  4.92534606]), 'targetState': array([20,  8], dtype=int32), 'currentDistance': 5.909936723234709}
done in step count: 12
reward sum = 0.7882911696692415
running average episode reward sum: 0.35605649206428375
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  8.]), 'previousTarget': array([20.,  8.]), 'currentState': array([19.92597507,  8.71158188,  4.7919978 ]), 'targetState': array([20,  8], dtype=int32), 'currentDistance': 0.7154218747036595}
episode index:455
target Thresh 31.72947956777337
target distance 14.0
model initialize at round 455
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 15.]), 'previousTarget': array([22., 15.]), 'currentState': array([27.03595928, 28.98338745,  5.59856009]), 'targetState': array([22, 15], dtype=int32), 'currentDistance': 14.862570789592029}
done in step count: 36
reward sum = 0.48206135494747115
running average episode reward sum: 0.35633281851797494
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 15.]), 'previousTarget': array([22., 15.]), 'currentState': array([22.89831468, 15.93501485,  4.20708691]), 'targetState': array([22, 15], dtype=int32), 'currentDistance': 1.2966194652523304}
episode index:456
target Thresh 31.732171291048274
target distance 17.0
model initialize at round 456
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 12.]), 'previousTarget': array([ 3., 12.]), 'currentState': array([19.98373397,  4.96888233,  3.98940063]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 18.381616770501793}
done in step count: 39
reward sum = 0.3747976100227573
running average episode reward sum: 0.35637322287575346
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 12.]), 'previousTarget': array([ 3., 12.]), 'currentState': array([ 3.99540384, 11.13673093,  2.59709892]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 1.3175971641779476}
episode index:457
target Thresh 31.73483623122909
target distance 10.0
model initialize at round 457
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 21.]), 'previousTarget': array([15., 21.]), 'currentState': array([ 5.        , 28.        ,  0.93993115]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 12.206555615733702}
done in step count: 27
reward sum = 0.5384932023214175
running average episode reward sum: 0.3567708647522724
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 21.]), 'previousTarget': array([15., 21.]), 'currentState': array([14.22654344, 21.35499791,  5.67693117]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 0.8510338235222933}
episode index:458
target Thresh 31.737474654812058
target distance 18.0
model initialize at round 458
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 26.]), 'previousTarget': array([21.88854382, 25.94427191]), 'currentState': array([ 4.42435168, 17.26388926,  0.49693818]), 'targetState': array([22, 26], dtype=int32), 'currentDistance': 19.627099749331865}
done in step count: 42
reward sum = 0.36496393106899155
running average episode reward sum: 0.3567887145699558
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 26.]), 'previousTarget': array([22., 26.]), 'currentState': array([21.22092081, 25.76597745,  0.48786947]), 'targetState': array([22, 26], dtype=int32), 'currentDistance': 0.8134684603772278}
episode index:459
target Thresh 31.740086825641733
target distance 4.0
model initialize at round 459
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 16.]), 'previousTarget': array([ 5., 16.]), 'currentState': array([ 7.0003065 , 11.9952147 ,  4.54173324]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 4.476553512259177}
done in step count: 15
reward sum = 0.7574988559578669
running average episode reward sum: 0.357659823572973
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 16.]), 'previousTarget': array([ 5., 16.]), 'currentState': array([ 4.64684215, 15.1492642 ,  1.46626473]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 0.9211253255419106}
episode index:460
target Thresh 31.74267300493738
target distance 6.0
model initialize at round 460
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 13.]), 'previousTarget': array([25., 13.]), 'currentState': array([22.01989813, 19.00156619,  6.20679655]), 'targetState': array([25, 13], dtype=int32), 'currentDistance': 6.70073159493753}
done in step count: 15
reward sum = 0.7389138995293963
running average episode reward sum: 0.35848683892211924
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 13.]), 'previousTarget': array([25., 13.]), 'currentState': array([25.28422987, 13.82704433,  4.78084923]), 'targetState': array([25, 13], dtype=int32), 'currentDistance': 0.8745221228470534}
episode index:461
target Thresh 31.74523345131908
target distance 8.0
model initialize at round 461
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([10.63163807,  3.3378107 ,  2.35076186]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 8.47926529362629}
done in step count: 18
reward sum = 0.6768393196215934
running average episode reward sum: 0.3591759135556679
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.44639934, 10.2773882 ,  1.45560752]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.9102974804734517}
episode index:462
target Thresh 31.747768420833605
target distance 12.0
model initialize at round 462
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([25.97237097,  5.94864878,  4.00090981]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 13.922967469236896}
done in step count: 33
reward sum = 0.46351269975774656
running average episode reward sum: 0.35940126298591
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([18.88184705, 17.09084668,  1.58424886]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 0.9167987088341816}
episode index:463
target Thresh 31.75027816698002
target distance 3.0
model initialize at round 463
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 15.]), 'previousTarget': array([18., 15.]), 'currentState': array([17.01050435, 18.04202446,  1.07335722]), 'targetState': array([18, 15], dtype=int32), 'currentDistance': 3.1989083190150986}
done in step count: 12
reward sum = 0.8111310252210374
running average episode reward sum: 0.3603748185079685
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 15.]), 'previousTarget': array([18., 15.]), 'currentState': array([18.23806732, 15.81256168,  4.57145244]), 'targetState': array([18, 15], dtype=int32), 'currentDistance': 0.8467186803907555}
episode index:464
target Thresh 31.752762940735032
target distance 21.0
model initialize at round 464
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.05458696, 13.40600405]), 'previousTarget': array([24.05721038, 13.40132839]), 'currentState': array([ 6.00553491, 22.02178726,  1.07736111]), 'targetState': array([27, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.22996638020911608
running average episode reward sum: 0.36009437025356233
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 12.]), 'previousTarget': array([27., 12.]), 'currentState': array([26.33333637, 12.86498901,  5.57371778]), 'targetState': array([27, 12], dtype=int32), 'currentDistance': 1.0920835099418686}
episode index:465
target Thresh 31.755222990578087
target distance 7.0
model initialize at round 465
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 16.]), 'previousTarget': array([20., 16.]), 'currentState': array([27.0021085 , 18.00807644,  1.54552752]), 'targetState': array([20, 16], dtype=int32), 'currentDistance': 7.284359580350394}
done in step count: 17
reward sum = 0.6934994531609827
running average episode reward sum: 0.3608098318048658
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 16.]), 'previousTarget': array([20., 16.]), 'currentState': array([20.75051663, 16.33952065,  3.58932593]), 'targetState': array([20, 16], dtype=int32), 'currentDistance': 0.8237411516092769}
episode index:466
target Thresh 31.757658562516216
target distance 10.0
model initialize at round 466
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([12.        , 16.        ,  5.04554319]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 10.0}
done in step count: 24
reward sum = 0.6200169443695638
running average episode reward sum: 0.3613648791551114
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([21.17562882, 16.20000914,  6.09461968]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 0.8482873956647753}
episode index:467
target Thresh 31.760069900108643
target distance 3.0
model initialize at round 467
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 10.]), 'previousTarget': array([25., 10.]), 'currentState': array([23.41830695,  6.99145064,  0.19902712]), 'targetState': array([25, 10], dtype=int32), 'currentDistance': 3.3989884047347414}
done in step count: 7
reward sum = 0.8606826326157879
running average episode reward sum: 0.36243179743173676
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 10.]), 'previousTarget': array([25., 10.]), 'currentState': array([25.02445336,  9.21946468,  1.26659305]), 'targetState': array([25, 10], dtype=int32), 'currentDistance': 0.7809182702228841}
episode index:468
target Thresh 31.76245724449114
target distance 8.0
model initialize at round 468
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([24.80876853, 18.14133884,  2.68212861]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 8.02692265083233}
done in step count: 15
reward sum = 0.7287083293103289
running average episode reward sum: 0.36321277084725617
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([17.89903385, 19.37042185,  2.97444452]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 1.0975566108326116}
episode index:469
target Thresh 31.764820834400137
target distance 5.0
model initialize at round 469
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 28.]), 'previousTarget': array([17., 28.]), 'currentState': array([11.90139808, 23.99910206,  2.9196291 ]), 'targetState': array([17, 28], dtype=int32), 'currentDistance': 6.480966428185589}
done in step count: 20
reward sum = 0.6780993383886771
running average episode reward sum: 0.36388274226755707
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 28.]), 'previousTarget': array([17., 28.]), 'currentState': array([16.14831549, 28.23085526,  0.07771088]), 'targetState': array([17, 28], dtype=int32), 'currentDistance': 0.8824175095069392}
episode index:470
target Thresh 31.767160906196587
target distance 11.0
model initialize at round 470
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 24.]), 'previousTarget': array([14., 24.]), 'currentState': array([ 3.23261312, 15.27410155,  0.81266447]), 'targetState': array([14, 24], dtype=int32), 'currentDistance': 13.859218017580568}
done in step count: 35
reward sum = 0.5190636377718214
running average episode reward sum: 0.36421221338327736
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 24.]), 'previousTarget': array([14., 24.]), 'currentState': array([13.1187882 , 24.12116554,  0.14246936]), 'targetState': array([14, 24], dtype=int32), 'currentDistance': 0.8895028503296053}
episode index:471
target Thresh 31.769477693889623
target distance 10.0
model initialize at round 471
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 21.]), 'previousTarget': array([ 7., 21.]), 'currentState': array([17.        , 28.        ,  5.63887358]), 'targetState': array([ 7, 21], dtype=int32), 'currentDistance': 12.206555615733704}
done in step count: 28
reward sum = 0.5242725106997023
running average episode reward sum: 0.3645513241826765
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 21.]), 'previousTarget': array([ 7., 21.]), 'currentState': array([ 7.82028037, 21.06616267,  3.66778073]), 'targetState': array([ 7, 21], dtype=int32), 'currentDistance': 0.8229443409060293}
episode index:472
target Thresh 31.771771429159948
target distance 7.0
model initialize at round 472
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 23.]), 'previousTarget': array([13., 23.]), 'currentState': array([ 6.        , 20.        ,  2.87056077]), 'targetState': array([13, 23], dtype=int32), 'currentDistance': 7.615773105863909}
done in step count: 19
reward sum = 0.6532236001296626
running average episode reward sum: 0.36516162497749044
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 23.]), 'previousTarget': array([13., 23.]), 'currentState': array([12.15482492, 23.21906224,  0.08373493]), 'targetState': array([13, 23], dtype=int32), 'currentDistance': 0.8731031959034795}
episode index:473
target Thresh 31.774042341382994
target distance 25.0
model initialize at round 473
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9.47339648, 11.469143  ]), 'previousTarget': array([ 9.85014149, 11.71008489]), 'currentState': array([26.63832948, 21.73389214,  3.63367492]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 79
reward sum = 0.09282634458513728
running average episode reward sum: 0.3645870779724433
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.79835163, 6.99012339, 3.15888576]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.7984127241600995}
episode index:474
target Thresh 31.776290657651884
target distance 6.0
model initialize at round 474
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 26.]), 'previousTarget': array([18., 26.]), 'currentState': array([12.04232705, 22.95793476,  5.7431426 ]), 'targetState': array([18, 26], dtype=int32), 'currentDistance': 6.689396672966367}
done in step count: 14
reward sum = 0.7442176042130509
running average episode reward sum: 0.3653863001329498
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 26.]), 'previousTarget': array([18., 26.]), 'currentState': array([17.15483522, 25.7363308 ,  0.70242711]), 'targetState': array([18, 26], dtype=int32), 'currentDistance': 0.8853388885843974}
episode index:475
target Thresh 31.77851660280011
target distance 25.0
model initialize at round 475
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.36679142,  8.85147692]), 'previousTarget': array([15.39259851,  8.74071961]), 'currentState': array([ 9.99008431, 28.11519983,  1.4140445 ]), 'targetState': array([17,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.11825309629689551
running average episode reward sum: 0.3648671127299329
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  3.]), 'previousTarget': array([17.,  3.]), 'currentState': array([16.86048751,  3.97160074,  5.161391  ]), 'targetState': array([17,  3], dtype=int32), 'currentDistance': 0.9815659557967001}
episode index:476
target Thresh 31.780720399424048
target distance 4.0
model initialize at round 476
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 16.]), 'previousTarget': array([17., 16.]), 'currentState': array([15.14146851, 19.56225001,  5.01270689]), 'targetState': array([17, 16], dtype=int32), 'currentDistance': 4.017930365309243}
done in step count: 7
reward sum = 0.8758828603228702
running average episode reward sum: 0.36593842456975034
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 16.]), 'previousTarget': array([17., 16.]), 'currentState': array([16.35317188, 16.92949474,  5.17951006]), 'targetState': array([17, 16], dtype=int32), 'currentDistance': 1.1324076504818454}
episode index:477
target Thresh 31.782902267905197
target distance 6.0
model initialize at round 477
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 3.88109281, 16.61283356,  4.60990718]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 5.723273007535356}
done in step count: 11
reward sum = 0.8023145458190402
running average episode reward sum: 0.36685134532550195
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.77611679, 11.99118652,  5.13893783]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.0161566844061922}
episode index:478
target Thresh 31.78506242643222
target distance 17.0
model initialize at round 478
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 13.]), 'previousTarget': array([ 8., 13.]), 'currentState': array([25.01272511,  7.01359888,  1.0710786 ]), 'targetState': array([ 8, 13], dtype=int32), 'currentDistance': 18.035238124476734}
done in step count: 44
reward sum = 0.3580590144595733
running average episode reward sum: 0.3668329897287046
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 13.]), 'previousTarget': array([ 8., 13.]), 'currentState': array([ 8.74159799, 13.46086969,  3.22989676]), 'targetState': array([ 8, 13], dtype=int32), 'currentDistance': 0.8731371300971927}
episode index:479
target Thresh 31.78720109102277
target distance 8.0
model initialize at round 479
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 16.]), 'previousTarget': array([13., 16.]), 'currentState': array([21.        , 22.        ,  1.89599192]), 'targetState': array([13, 16], dtype=int32), 'currentDistance': 10.0}
done in step count: 24
reward sum = 0.5953627765593978
running average episode reward sum: 0.36730909345126855
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 16.]), 'previousTarget': array([13., 16.]), 'currentState': array([13.43643168, 16.75790908,  4.25548852]), 'targetState': array([13, 16], dtype=int32), 'currentDistance': 0.8745849170835426}
episode index:480
target Thresh 31.789318475545087
target distance 22.0
model initialize at round 480
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.74108151, 23.92344798]), 'previousTarget': array([21.57704261, 23.55791146]), 'currentState': array([12.04735514,  6.42968712,  1.28267294]), 'targetState': array([24, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.2447169709043427
running average episode reward sum: 0.3670542241736242
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 28.]), 'previousTarget': array([24., 28.]), 'currentState': array([23.1640831 , 27.08568874,  0.7557525 ]), 'targetState': array([24, 28], dtype=int32), 'currentDistance': 1.2388390333185444}
episode index:481
target Thresh 31.791414791739392
target distance 23.0
model initialize at round 481
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.09721053,  8.40460768]), 'previousTarget': array([18.75010002,  8.67383477]), 'currentState': array([ 3.34979548, 20.73420295,  5.64372246]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.1343162837056845
running average episode reward sum: 0.3665713653759729
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  3.]), 'previousTarget': array([26.,  3.]), 'currentState': array([25.20210856,  3.965978  ,  5.55959725]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 1.2528943462053532}
episode index:482
target Thresh 31.79349024923905
target distance 8.0
model initialize at round 482
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 29.]), 'previousTarget': array([ 9., 29.]), 'currentState': array([10.05614962, 21.18100586,  1.50007877]), 'targetState': array([ 9, 29], dtype=int32), 'currentDistance': 7.890001358137905}
done in step count: 17
reward sum = 0.7137130420759736
running average episode reward sum: 0.3672900852035091
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 29.]), 'previousTarget': array([ 9., 29.]), 'currentState': array([ 8.29446587, 28.02725026,  1.38146873]), 'targetState': array([ 9, 29], dtype=int32), 'currentDistance': 1.201674022675731}
episode index:483
target Thresh 31.79554505559154
target distance 18.0
model initialize at round 483
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.64100589,  7.90599608]), 'previousTarget': array([23.64100589,  7.90599608]), 'currentState': array([ 7.        , 19.        ,  4.84897184]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.29375589949490527
running average episode reward sum: 0.3671381550677475
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([24.01771599,  7.7167241 ,  5.77560813]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 1.2159668207274092}
episode index:484
target Thresh 31.79757941627921
target distance 2.0
model initialize at round 484
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  7.]), 'previousTarget': array([11.,  7.]), 'currentState': array([10.        ,  9.        ,  1.48641527]), 'targetState': array([11,  7], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 10
reward sum = 0.8447253074978919
running average episode reward sum: 0.36812287084595396
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  7.]), 'previousTarget': array([11.,  7.]), 'currentState': array([10.98249571,  7.98746491,  5.03172762]), 'targetState': array([11,  7], dtype=int32), 'currentDistance': 0.9876200437193375}
episode index:485
target Thresh 31.79959353473982
target distance 19.0
model initialize at round 485
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 29.]), 'previousTarget': array([ 4., 29.]), 'currentState': array([ 9.        , 10.        ,  1.39355171]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 19.6468827043885}
done in step count: 48
reward sum = 0.3649504692680788
running average episode reward sum: 0.36811634327069087
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 29.]), 'previousTarget': array([ 4., 29.]), 'currentState': array([ 4.30163825, 28.18551277,  1.94344637]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 0.8685476896897678}
episode index:486
target Thresh 31.8015876123869
target distance 14.0
model initialize at round 486
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 27.]), 'previousTarget': array([10., 27.]), 'currentState': array([24.        , 25.        ,  1.80116904]), 'targetState': array([10, 27], dtype=int32), 'currentDistance': 14.142135623730951}
done in step count: 33
reward sum = 0.4866805094343225
running average episode reward sum: 0.36835980151743347
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 27.]), 'previousTarget': array([10., 27.]), 'currentState': array([10.87830758, 27.05803911,  2.83827497]), 'targetState': array([10, 27], dtype=int32), 'currentDistance': 0.8802231184094252}
episode index:487
target Thresh 31.803561848629876
target distance 11.0
model initialize at round 487
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 22.]), 'previousTarget': array([18., 22.]), 'currentState': array([24.98118979, 10.94578029,  4.12595558]), 'targetState': array([18, 22], dtype=int32), 'currentDistance': 13.074126519748269}
done in step count: 33
reward sum = 0.4987782105350912
running average episode reward sum: 0.3686270523555844
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 22.]), 'previousTarget': array([18., 22.]), 'currentState': array([18.29527281, 21.1105106 ,  2.44906002]), 'targetState': array([18, 22], dtype=int32), 'currentDistance': 0.9372179153701212}
episode index:488
target Thresh 31.805516440894014
target distance 23.0
model initialize at round 488
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.14888374, 10.83049497]), 'previousTarget': array([18.14213562, 10.85786438]), 'currentState': array([ 3.9902644 , 24.95612764,  4.74418852]), 'targetState': array([27,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 98
reward sum = 0.05708133055828729
running average episode reward sum: 0.36798994454004796
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  2.]), 'previousTarget': array([27.,  2.]), 'currentState': array([26.5758044 ,  2.89356943,  5.33232614]), 'targetState': array([27,  2], dtype=int32), 'currentDistance': 0.9891452029071593}
episode index:489
target Thresh 31.807451584640173
target distance 13.0
model initialize at round 489
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([20.        ,  8.        ,  3.13718009]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 17.029386365926403}
done in step count: 39
reward sum = 0.42692605140596995
running average episode reward sum: 0.36811022230916207
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([ 9.22560968, 20.04027732,  2.10488446]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 0.9858840437712031}
episode index:490
target Thresh 31.809367473384338
target distance 2.0
model initialize at round 490
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 28.]), 'previousTarget': array([20., 28.]), 'currentState': array([17.83794411, 28.66323243,  4.45324436]), 'targetState': array([20, 28], dtype=int32), 'currentDistance': 2.261495723727562}
done in step count: 7
reward sum = 0.8435118353885188
running average episode reward sum: 0.36907845370036235
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 28.]), 'previousTarget': array([20., 28.]), 'currentState': array([19.82510663, 27.01285377,  0.19149945]), 'targetState': array([20, 28], dtype=int32), 'currentDistance': 1.0025195154286437}
episode index:491
target Thresh 31.811264298716978
target distance 9.0
model initialize at round 491
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 13.]), 'previousTarget': array([ 9., 13.]), 'currentState': array([11.85998521, 21.55653249,  4.38266741]), 'targetState': array([ 9, 13], dtype=int32), 'currentDistance': 9.021849237482796}
done in step count: 18
reward sum = 0.6932425768433201
running average episode reward sum: 0.3697373238693521
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 13.]), 'previousTarget': array([ 9., 13.]), 'currentState': array([ 9.68637866, 13.80838254,  4.30427838]), 'targetState': array([ 9, 13], dtype=int32), 'currentDistance': 1.0604706447133596}
episode index:492
target Thresh 31.813142250322212
target distance 21.0
model initialize at round 492
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.82179448, 19.49572125]), 'previousTarget': array([17.79310345, 19.48275862]), 'currentState': array([4.04781224, 4.99477599, 0.14367073]), 'targetState': array([24, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.12729702746969973
running average episode reward sum: 0.36924555856225344
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 26.]), 'previousTarget': array([24., 26.]), 'currentState': array([23.01709977, 25.33056849,  0.80318032]), 'targetState': array([24, 26], dtype=int32), 'currentDistance': 1.1892146183163352}
episode index:493
target Thresh 31.815001515996762
target distance 8.0
model initialize at round 493
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 10.]), 'previousTarget': array([22., 10.]), 'currentState': array([19.09602879,  2.40575766,  1.29843645]), 'targetState': array([22, 10], dtype=int32), 'currentDistance': 8.130532920175552}
done in step count: 17
reward sum = 0.7151402374631619
running average episode reward sum: 0.3699457502199476
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 10.]), 'previousTarget': array([22., 10.]), 'currentState': array([21.74535172,  9.25521916,  1.13604078]), 'targetState': array([22, 10], dtype=int32), 'currentDistance': 0.7871113290843635}
episode index:494
target Thresh 31.816842281668748
target distance 6.0
model initialize at round 494
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 8.]), 'previousTarget': array([6., 8.]), 'currentState': array([12.        ,  3.        ,  5.14909887]), 'targetState': array([6, 8], dtype=int32), 'currentDistance': 7.810249675906655}
done in step count: 23
reward sum = 0.6399059701963273
running average episode reward sum: 0.37049112440171805
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 8.]), 'previousTarget': array([6., 8.]), 'currentState': array([6.59744057, 7.19839905, 2.29153936]), 'targetState': array([6, 8], dtype=int32), 'currentDistance': 0.9997496283924359}
episode index:495
target Thresh 31.818664731416266
target distance 4.0
model initialize at round 495
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([18.01975169,  5.98898443,  5.52194738]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 4.471173630196537}
done in step count: 12
reward sum = 0.805550981207656
running average episode reward sum: 0.37136826120979455
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.95499687,  2.24721445,  3.77433058]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.9864755466208762}
episode index:496
target Thresh 31.820469047485812
target distance 23.0
model initialize at round 496
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.42577946, 19.11798553]), 'previousTarget': array([23.98112317, 19.13125551]), 'currentState': array([ 4.44675352, 20.03369412,  0.07824432]), 'targetState': array([27, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.28209157245006256
running average episode reward sum: 0.37118863004528807
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 19.]), 'previousTarget': array([27., 19.]), 'currentState': array([26.05545646, 19.02067882,  0.08423602]), 'targetState': array([27, 19], dtype=int32), 'currentDistance': 0.944769872508495}
episode index:497
target Thresh 31.8222554103105
target distance 21.0
model initialize at round 497
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.17157288, 21.79898987]), 'previousTarget': array([10.17157288, 21.79898987]), 'currentState': array([13.        ,  2.        ,  1.24189061]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.2609210182483405
running average episode reward sum: 0.3709672091380653
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 23.]), 'previousTarget': array([10., 23.]), 'currentState': array([ 9.05309788, 22.70328311,  0.66787401]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 0.9923026466812637}
episode index:498
target Thresh 31.824023998528094
target distance 13.0
model initialize at round 498
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  6.]), 'previousTarget': array([25.,  6.]), 'currentState': array([12.        , 18.        ,  0.56640446]), 'targetState': array([25,  6], dtype=int32), 'currentDistance': 17.69180601295413}
done in step count: 40
reward sum = 0.3868912663781278
running average episode reward sum: 0.37099912107642213
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  6.]), 'previousTarget': array([25.,  6.]), 'currentState': array([24.02570941,  6.46350926,  5.93096698]), 'targetState': array([25,  6], dtype=int32), 'currentDistance': 1.0789267735664165}
episode index:499
target Thresh 31.825774988998898
target distance 10.0
model initialize at round 499
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  9.]), 'previousTarget': array([20.,  9.]), 'currentState': array([21.        , 19.        ,  3.13639939]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 10.049875621120888}
done in step count: 23
reward sum = 0.6042639426904657
running average episode reward sum: 0.3714656507196502
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  9.]), 'previousTarget': array([20.,  9.]), 'currentState': array([19.95816925,  9.78088527,  4.77260604]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 0.782004872793156}
episode index:500
target Thresh 31.82750855682341
target distance 14.0
model initialize at round 500
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 24.]), 'previousTarget': array([22., 24.]), 'currentState': array([9.0105603 , 9.99597661, 6.12484822]), 'targetState': array([22, 24], dtype=int32), 'currentDistance': 19.10073859178204}
done in step count: 57
reward sum = 0.3165704491820707
running average episode reward sum: 0.3713560794590962
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 24.]), 'previousTarget': array([22., 24.]), 'currentState': array([21.1480632 , 24.237392  ,  0.06232518]), 'targetState': array([22, 24], dtype=int32), 'currentDistance': 0.8843931636271426}
episode index:501
target Thresh 31.829224875359866
target distance 9.0
model initialize at round 501
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 21.]), 'previousTarget': array([ 5., 21.]), 'currentState': array([ 7.        , 12.        ,  2.84300518]), 'targetState': array([ 5, 21], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 22
reward sum = 0.6472514707872463
running average episode reward sum: 0.37190567187210044
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 21.]), 'previousTarget': array([ 5., 21.]), 'currentState': array([ 4.6514257 , 20.14663329,  1.66146638]), 'targetState': array([ 5, 21], dtype=int32), 'currentDistance': 0.9218127725064753}
episode index:502
target Thresh 31.83092411624154
target distance 7.0
model initialize at round 502
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 22.]), 'previousTarget': array([19., 22.]), 'currentState': array([14.06072852, 14.96027922,  5.95645332]), 'targetState': array([19, 22], dtype=int32), 'currentDistance': 8.599655310235823}
done in step count: 21
reward sum = 0.6385661137905368
running average episode reward sum: 0.37243581191567593
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 22.]), 'previousTarget': array([19., 22.]), 'currentState': array([18.15437223, 21.7556174 ,  0.23266247]), 'targetState': array([19, 22], dtype=int32), 'currentDistance': 0.8802324563391579}
episode index:503
target Thresh 31.832606449393943
target distance 12.0
model initialize at round 503
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 16.]), 'previousTarget': array([ 4., 16.]), 'currentState': array([15.       ,  4.       ,  0.3865495]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 16.278820596099706}
done in step count: 37
reward sum = 0.41093441377988427
running average episode reward sum: 0.37251219803048585
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 16.]), 'previousTarget': array([ 4., 16.]), 'currentState': array([ 4.82301127, 15.73390252,  2.36358709]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 0.864959780320092}
episode index:504
target Thresh 31.83427204305179
target distance 15.0
model initialize at round 504
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 26.]), 'previousTarget': array([ 2., 26.]), 'currentState': array([17.00159288, 20.97911679,  4.53601742]), 'targetState': array([ 2, 26], dtype=int32), 'currentDistance': 15.819515076815067}
done in step count: 38
reward sum = 0.434745229424423
running average episode reward sum: 0.37263543175601843
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 26.]), 'previousTarget': array([ 2., 26.]), 'currentState': array([ 2.79287066, 25.19566757,  2.51663045]), 'targetState': array([ 2, 26], dtype=int32), 'currentDistance': 1.1294222154439082}
episode index:505
target Thresh 31.835921063775835
target distance 11.0
model initialize at round 505
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([13.        ,  2.        ,  5.57168028]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 11.045361017187261}
done in step count: 29
reward sum = 0.5377277292261555
running average episode reward sum: 0.37296170111860766
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.77797176, 3.26500564, 3.56164205]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.8218686331396086}
episode index:506
target Thresh 31.837553676469525
target distance 17.0
model initialize at round 506
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  3.]), 'previousTarget': array([24.,  3.]), 'currentState': array([7.      , 8.      , 5.546134]), 'targetState': array([24,  3], dtype=int32), 'currentDistance': 17.72004514666935}
done in step count: 41
reward sum = 0.3992562462688776
running average episode reward sum: 0.37301356412679365
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  3.]), 'previousTarget': array([24.,  3.]), 'currentState': array([23.22796807,  3.47181087,  6.0393944 ]), 'targetState': array([24,  3], dtype=int32), 'currentDistance': 0.9047865991403742}
episode index:507
target Thresh 31.83917004439549
target distance 5.0
model initialize at round 507
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 23.]), 'previousTarget': array([19., 23.]), 'currentState': array([20.64730404, 18.35398267,  2.293572  ]), 'targetState': array([19, 23], dtype=int32), 'currentDistance': 4.929410471274029}
done in step count: 9
reward sum = 0.8266757436662177
running average episode reward sum: 0.37390659991328856
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 23.]), 'previousTarget': array([19., 23.]), 'currentState': array([18.82339857, 22.02420241,  1.68042677]), 'targetState': array([19, 23], dtype=int32), 'currentDistance': 0.9916496395785793}
episode index:508
target Thresh 31.84077032919187
target distance 8.0
model initialize at round 508
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 12.]), 'previousTarget': array([15., 12.]), 'currentState': array([19.03587257,  4.18585101,  1.608042  ]), 'targetState': array([15, 12], dtype=int32), 'currentDistance': 8.794838929484392}
done in step count: 21
reward sum = 0.6628521434397541
running average episode reward sum: 0.37447427288681795
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 12.]), 'previousTarget': array([15., 12.]), 'currentState': array([14.70707197, 11.0513744 ,  1.7654648 ]), 'targetState': array([15, 12], dtype=int32), 'currentDistance': 0.9928229223395783}
episode index:509
target Thresh 31.842354690888477
target distance 10.0
model initialize at round 509
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 16.]), 'previousTarget': array([24., 16.]), 'currentState': array([14.        , 13.        ,  5.69130182]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 10.44030650891055}
done in step count: 25
reward sum = 0.6045213965837146
running average episode reward sum: 0.3749253456783805
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 16.]), 'previousTarget': array([24., 16.]), 'currentState': array([23.19139987, 16.10639267,  6.25714394]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 0.8155694732354327}
episode index:510
target Thresh 31.8439232879228
target distance 24.0
model initialize at round 510
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.25687065,  8.4099328 ]), 'previousTarget': array([16.4,  8.8]), 'currentState': array([21.73875081, 27.64399087,  4.1255586 ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.22686460958314347
running average episode reward sum: 0.37463559864101215
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.89812007,  4.95177337,  4.39462311]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.3086222565108823}
episode index:511
target Thresh 31.84547627715585
target distance 12.0
model initialize at round 511
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([9.        , 8.        , 4.92029047]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 12.649110640673516}
done in step count: 29
reward sum = 0.5223283901584129
running average episode reward sum: 0.37492406112444454
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([20.0614662 , 11.96809665,  0.21824495]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 0.9390758855514215}
episode index:512
target Thresh 31.84701381388785
target distance 3.0
model initialize at round 512
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 10.]), 'previousTarget': array([18., 10.]), 'currentState': array([20.51149227, 12.9173693 ,  3.42974555]), 'targetState': array([18, 10], dtype=int32), 'currentDistance': 3.84949829124153}
done in step count: 7
reward sum = 0.8568026589346931
running average episode reward sum: 0.3758633956231
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 10.]), 'previousTarget': array([18., 10.]), 'currentState': array([18.13189681, 10.7822988 ,  4.06037278]), 'targetState': array([18, 10], dtype=int32), 'currentDistance': 0.7933398874196234}
episode index:513
target Thresh 31.848536051873744
target distance 20.0
model initialize at round 513
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.72879537,  9.20783911]), 'previousTarget': array([14.46924689,  9.61536159]), 'currentState': array([ 3.17188715, 25.53076386,  5.12126655]), 'targetState': array([17,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.2526379010754918
running average episode reward sum: 0.37562365730685954
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  6.]), 'previousTarget': array([17.,  6.]), 'currentState': array([16.23110287,  6.91920316,  5.64918949]), 'targetState': array([17,  6], dtype=int32), 'currentDistance': 1.1983894334677654}
episode index:514
target Thresh 31.85004314333861
target distance 19.0
model initialize at round 514
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 12.]), 'previousTarget': array([ 7., 12.]), 'currentState': array([26.        , 15.        ,  5.93009109]), 'targetState': array([ 7, 12], dtype=int32), 'currentDistance': 19.235384061671343}
done in step count: 55
reward sum = 0.29809448605791944
running average episode reward sum: 0.3754731152267645
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 12.]), 'previousTarget': array([ 7., 12.]), 'currentState': array([ 7.88224048, 11.20189301,  2.94096676]), 'targetState': array([ 7, 12], dtype=int32), 'currentDistance': 1.1896734991735312}
episode index:515
target Thresh 31.85153523899284
target distance 13.0
model initialize at round 515
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  2.]), 'previousTarget': array([12.,  2.]), 'currentState': array([ 7.02944937, 15.00021752,  6.07367558]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 13.918046889730146}
done in step count: 33
reward sum = 0.5117691698699908
running average episode reward sum: 0.37573725486754594
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  2.]), 'previousTarget': array([12.,  2.]), 'currentState': array([12.07478499,  2.92209627,  4.72491   ]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 0.9251239492905194}
episode index:516
target Thresh 31.853012488047252
target distance 24.0
model initialize at round 516
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6.92091492, 7.42039161]), 'previousTarget': array([6.92091492, 7.42039161]), 'currentState': array([11.        , 27.        ,  0.09550548]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 20.0}
done in step count: 73
reward sum = 0.1906343934032224
running average episode reward sum: 0.375379222253495
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 3.]), 'previousTarget': array([6., 3.]), 'currentState': array([6.46091924, 3.80356418, 4.30017819]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 0.926370300381582}
episode index:517
target Thresh 31.854475038227978
target distance 5.0
model initialize at round 517
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([20.7556687 ,  7.14955366,  2.83954141]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 6.3115117332775394}
done in step count: 12
reward sum = 0.7550893245157358
running average episode reward sum: 0.3761122533389433
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.25212514,  3.96701647,  4.29294297]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.9993437593323776}
episode index:518
target Thresh 31.855923035791257
target distance 12.0
model initialize at round 518
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 21.]), 'previousTarget': array([ 8., 21.]), 'currentState': array([2.93845677, 9.18095136, 1.75630471]), 'targetState': array([ 8, 21], dtype=int32), 'currentDistance': 12.857259840408183}
done in step count: 32
reward sum = 0.529945880841328
running average episode reward sum: 0.376408657245499
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 21.]), 'previousTarget': array([ 8., 21.]), 'currentState': array([ 7.14011788, 20.99761829,  0.41467081]), 'targetState': array([ 8, 21], dtype=int32), 'currentDistance': 0.8598854214555447}
episode index:519
target Thresh 31.85735662553805
target distance 8.0
model initialize at round 519
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 13.]), 'previousTarget': array([19., 13.]), 'currentState': array([15.0152546 ,  4.98107138,  5.63164604]), 'targetState': array([19, 13], dtype=int32), 'currentDistance': 8.954407416136306}
done in step count: 19
reward sum = 0.6566688393710999
running average episode reward sum: 0.37694761913420205
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 13.]), 'previousTarget': array([19., 13.]), 'currentState': array([18.04318785, 12.04542035,  1.10650481]), 'targetState': array([19, 13], dtype=int32), 'currentDistance': 1.3515590257040038}
episode index:520
target Thresh 31.858775950828523
target distance 10.0
model initialize at round 520
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([22.99109211,  3.03640011,  2.0029279 ]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 13.420620706205693}
done in step count: 30
reward sum = 0.5323166864901798
running average episode reward sum: 0.3772458323153076
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([14.00079899, 12.08147555,  1.92204026]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 0.9185248004640159}
episode index:521
target Thresh 31.860181153596397
target distance 12.0
model initialize at round 521
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 19.]), 'previousTarget': array([21., 19.]), 'currentState': array([12.00725061,  6.99648112,  6.08384943]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 14.998466821534697}
done in step count: 36
reward sum = 0.4646082000552455
running average episode reward sum: 0.37741319317304695
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 19.]), 'previousTarget': array([21., 19.]), 'currentState': array([20.14005149, 18.85045175,  0.7499096 ]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 0.8728551519746075}
episode index:522
target Thresh 31.86157237436311
target distance 23.0
model initialize at round 522
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.23029012, 21.48538728]), 'previousTarget': array([12.23029012, 21.48538728]), 'currentState': array([27.        ,  8.        ,  1.40418971]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 96
reward sum = 0.08836291988507955
running average episode reward sum: 0.37686051578626306
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 29.]), 'previousTarget': array([ 4., 29.]), 'currentState': array([ 4.80510592, 28.29334928,  2.11290515]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 1.071237969370336}
episode index:523
target Thresh 31.862949752251907
target distance 11.0
model initialize at round 523
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([19.70088048,  4.21105669,  2.43774039]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 12.166376110471415}
done in step count: 31
reward sum = 0.5707273602802362
running average episode reward sum: 0.37723049068033554
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([9.87284884, 9.21023935, 2.86505425]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 1.1771095866720487}
episode index:524
target Thresh 31.86431342500172
target distance 19.0
model initialize at round 524
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([9.        , 3.        , 0.91854334]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 19.235384061671343}
done in step count: 53
reward sum = 0.3316618825504504
running average episode reward sum: 0.3771436933315167
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([11.04709552, 21.80933832,  0.98739434]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 0.9717915572860798}
episode index:525
target Thresh 31.865663528980956
target distance 8.0
model initialize at round 525
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 12.]), 'previousTarget': array([12., 12.]), 'currentState': array([3.97729746, 9.03471154, 1.91483071]), 'targetState': array([12, 12], dtype=int32), 'currentDistance': 8.55316851760113}
done in step count: 82
reward sum = 0.3534801464211902
running average episode reward sum: 0.37709870559974806
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 12.]), 'previousTarget': array([12., 12.]), 'currentState': array([11.03008935, 12.87495687,  0.09880032]), 'targetState': array([12, 12], dtype=int32), 'currentDistance': 1.3062450743789729}
episode index:526
target Thresh 31.867000199201147
target distance 25.0
model initialize at round 526
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.22340312, 18.18598016]), 'previousTarget': array([16.14213562, 18.14213562]), 'currentState': array([2.10580122, 4.01935329, 0.17786465]), 'targetState': array([27, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.184599100375093
running average episode reward sum: 0.3760328653607066
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 29.]), 'previousTarget': array([27., 29.]), 'currentState': array([16.97065981, 21.18375166,  0.66450777]), 'targetState': array([27, 29], dtype=int32), 'currentDistance': 12.715400221904417}
episode index:527
target Thresh 31.86832356933042
target distance 10.0
model initialize at round 527
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([24.81517489, 14.74317161,  3.99701534]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 11.371968967243214}
done in step count: 28
reward sum = 0.5895656003989508
running average episode reward sum: 0.3764372834194912
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.8703451 ,  9.16254929,  3.02459295]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.8853941885062081}
episode index:528
target Thresh 31.8696337717069
target distance 7.0
model initialize at round 528
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([18.        , 18.        ,  5.21928549]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 8.602325267042627}
done in step count: 23
reward sum = 0.6191820730440499
running average episode reward sum: 0.3768961582581009
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([13.79039985, 24.11879128,  2.16665916]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 1.1837485937663612}
episode index:529
target Thresh 31.87093093735191
target distance 22.0
model initialize at round 529
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.42598884,  5.48718522]), 'previousTarget': array([11.42734309,  5.48906088]), 'currentState': array([18.9959766 , 23.99921605,  3.58652997]), 'targetState': array([10,  2], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 90
reward sum = 0.19688842105814586
running average episode reward sum: 0.376556521018101
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  2.]), 'previousTarget': array([10.,  2.]), 'currentState': array([10.52887526,  2.88056156,  4.62810612]), 'targetState': array([10,  2], dtype=int32), 'currentDistance': 1.027179486675568}
episode index:530
target Thresh 31.8722151959831
target distance 2.0
model initialize at round 530
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 20.]), 'previousTarget': array([ 6., 20.]), 'currentState': array([ 7.51468631, 20.88226626,  3.46600401]), 'targetState': array([ 6, 20], dtype=int32), 'currentDistance': 1.752902845610303}
done in step count: 2
reward sum = 0.9593064121723114
running average episode reward sum: 0.37765397844023696
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 20.]), 'previousTarget': array([ 6., 20.]), 'currentState': array([ 6.8908144 , 20.61776218,  3.61245102]), 'targetState': array([ 6, 20], dtype=int32), 'currentDistance': 1.084057381530251}
episode index:531
target Thresh 31.873486676027397
target distance 10.0
model initialize at round 531
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([19.        ,  8.        ,  5.81401253]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 10.049875621120892}
done in step count: 27
reward sum = 0.5860057878736112
running average episode reward sum: 0.37804561717977336
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([19.20555224, 17.04059689,  1.37036877]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 1.245632998506522}
episode index:532
target Thresh 31.874745504633868
target distance 17.0
model initialize at round 532
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.8186522 , 10.82695388]), 'previousTarget': array([12.85786438, 10.85786438]), 'currentState': array([26.93997698, 24.98986976,  3.31836506]), 'targetState': array([10,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 90
reward sum = 0.18836956927149598
running average episode reward sum: 0.37768975217431694
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  8.]), 'previousTarget': array([10.,  8.]), 'currentState': array([10.89292122,  8.12204102,  3.6306543 ]), 'targetState': array([10,  8], dtype=int32), 'currentDistance': 0.9012226813129431}
episode index:533
target Thresh 31.875991807686425
target distance 21.0
model initialize at round 533
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7.67190354, 5.13712332]), 'previousTarget': array([7.6170994 , 5.12161403]), 'currentState': array([26.07464117, 12.96906101,  5.63773918]), 'targetState': array([5, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.2143468321023524
running average episode reward sum: 0.37738386655620465
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 4.]), 'previousTarget': array([5., 4.]), 'currentState': array([5.84824759, 3.79122314, 3.03210957]), 'targetState': array([5, 4], dtype=int32), 'currentDistance': 0.8735626756359602}
episode index:534
target Thresh 31.87722570981641
target distance 17.0
model initialize at round 534
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.30518641, 12.49088885]), 'previousTarget': array([21.33935727, 12.46633605]), 'currentState': array([ 4.97061762, 24.03133348,  2.07157063]), 'targetState': array([22, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 81
reward sum = 0.17099970989397573
running average episode reward sum: 0.37699810177739673
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 12.]), 'previousTarget': array([22., 12.]), 'currentState': array([21.77407305, 12.99762429,  5.81414535]), 'targetState': array([22, 12], dtype=int32), 'currentDistance': 1.022886703427435}
episode index:535
target Thresh 31.878447334415068
target distance 3.0
model initialize at round 535
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  2.]), 'previousTarget': array([13.,  2.]), 'currentState': array([10.3783769 ,  4.83710531,  5.85218214]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 3.8629100740497972}
done in step count: 19
reward sum = 0.7787838555797774
running average episode reward sum: 0.3777477020643415
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  2.]), 'previousTarget': array([13.,  2.]), 'currentState': array([13.40665592,  2.91046595,  4.93333276]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 0.9971545948421651}
episode index:536
target Thresh 31.87965680364587
target distance 25.0
model initialize at round 536
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.76505278, 10.20266734]), 'previousTarget': array([15.78107393, 10.15457199]), 'currentState': array([ 5.00550895, 27.06185524,  1.2294687 ]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.08659564417366643
running average episode reward sum: 0.37688300309555567
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.31956679,  3.5577047 ]), 'previousTarget': array([20.18232721,  3.85864431]), 'currentState': array([12.31368519, 21.88543942,  5.27462309]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 20.0}
episode index:537
target Thresh 31.88085423845675
target distance 19.0
model initialize at round 537
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 24.]), 'previousTarget': array([27., 24.]), 'currentState': array([ 8.11019803, 21.06292467,  0.39802685]), 'targetState': array([27, 24], dtype=int32), 'currentDistance': 19.116773527773596}
done in step count: 46
reward sum = 0.36807878660272586
running average episode reward sum: 0.37686663838088497
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 24.]), 'previousTarget': array([27., 24.]), 'currentState': array([26.62573782, 23.14053454,  0.98253015]), 'targetState': array([27, 24], dtype=int32), 'currentDistance': 0.9374182951677389}
episode index:538
target Thresh 31.882039758592185
target distance 13.0
model initialize at round 538
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  5.]), 'previousTarget': array([18.,  5.]), 'currentState': array([22.87103969, 17.83766085,  4.2149879 ]), 'targetState': array([18,  5], dtype=int32), 'currentDistance': 13.73071606663816}
done in step count: 29
reward sum = 0.5226727768179257
running average episode reward sum: 0.3771371506970947
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  5.]), 'previousTarget': array([18.,  5.]), 'currentState': array([18.91329177,  5.61101029,  3.78219803]), 'targetState': array([18,  5], dtype=int32), 'currentDistance': 1.098833666294656}
episode index:539
target Thresh 31.883213482605182
target distance 7.0
model initialize at round 539
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 20.]), 'previousTarget': array([19., 20.]), 'currentState': array([25.96021532, 24.04504341,  2.54552183]), 'targetState': array([19, 20], dtype=int32), 'currentDistance': 8.050277850783713}
done in step count: 23
reward sum = 0.6544331508271933
running average episode reward sum: 0.37765066180844675
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 20.]), 'previousTarget': array([19., 20.]), 'currentState': array([19.98754745, 20.0318599 ,  4.51629188]), 'targetState': array([19, 20], dtype=int32), 'currentDistance': 0.9880612393891047}
episode index:540
target Thresh 31.884375527869114
target distance 7.0
model initialize at round 540
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 21.]), 'previousTarget': array([15., 21.]), 'currentState': array([22.02474182, 27.04763356,  1.10671673]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 9.26935107729168}
done in step count: 22
reward sum = 0.6201004390699663
running average episode reward sum: 0.37809881296789505
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 21.]), 'previousTarget': array([15., 21.]), 'currentState': array([15.94295182, 21.84321234,  3.56631258]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 1.264976353341927}
episode index:541
target Thresh 31.885526010589484
target distance 10.0
model initialize at round 541
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 12.]), 'previousTarget': array([15., 12.]), 'currentState': array([24.83717998, 16.64205665,  4.13854054]), 'targetState': array([15, 12], dtype=int32), 'currentDistance': 10.877444546997861}
done in step count: 24
reward sum = 0.6137257092305556
running average episode reward sum: 0.3785335489388593
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 12.]), 'previousTarget': array([15., 12.]), 'currentState': array([15.91571916, 12.12665604,  3.97088959]), 'targetState': array([15, 12], dtype=int32), 'currentDistance': 0.924436763466883}
episode index:542
target Thresh 31.886665045815512
target distance 8.0
model initialize at round 542
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 17.]), 'previousTarget': array([ 5., 17.]), 'currentState': array([12.73742181,  8.99062284,  2.98361301]), 'targetState': array([ 5, 17], dtype=int32), 'currentDistance': 11.136328780492779}
done in step count: 21
reward sum = 0.6206220942760259
running average episode reward sum: 0.37897938419730715
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 17.]), 'previousTarget': array([ 5., 17.]), 'currentState': array([ 5.81867163, 16.24126492,  2.27803801]), 'targetState': array([ 5, 17], dtype=int32), 'currentDistance': 1.1161998736279326}
episode index:543
target Thresh 31.88779274745168
target distance 4.0
model initialize at round 543
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.05212676, 3.98936946, 0.01797023]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 4.010969275274815}
done in step count: 9
reward sum = 0.8197715673201611
running average episode reward sum: 0.3797896639456948
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.92013284, 7.24165067, 2.09001806]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.1923666179181427}
episode index:544
target Thresh 31.888909228269085
target distance 18.0
model initialize at round 544
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.64100589,  4.90599608]), 'previousTarget': array([24.64100589,  4.90599608]), 'currentState': array([ 8.       , 16.       ,  0.3474443]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.2875820611691226
running average episode reward sum: 0.3796204756837194
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  4.]), 'previousTarget': array([26.,  4.]), 'currentState': array([25.0778231 ,  3.86721036,  5.31556095]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 0.9316884310483998}
episode index:545
target Thresh 31.89001459991674
target distance 15.0
model initialize at round 545
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 27.]), 'previousTarget': array([ 9., 27.]), 'currentState': array([ 4.99235646, 12.00474331,  2.33368945]), 'targetState': array([ 9, 27], dtype=int32), 'currentDistance': 15.521563387369106}
done in step count: 33
reward sum = 0.4618584470144426
running average episode reward sum: 0.37977109467883063
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 27.]), 'previousTarget': array([ 9., 27.]), 'currentState': array([ 8.92249912, 26.19423227,  1.11301549]), 'targetState': array([ 9, 27], dtype=int32), 'currentDistance': 0.8094862726136932}
episode index:546
target Thresh 31.891108972932734
target distance 15.0
model initialize at round 546
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 12.]), 'previousTarget': array([26., 12.]), 'currentState': array([11.0064818 , 24.0273657 ,  1.09259489]), 'targetState': array([26, 12], dtype=int32), 'currentDistance': 19.221423299414248}
done in step count: 58
reward sum = 0.3362478170105938
running average episode reward sum: 0.37969152744360535
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 12.]), 'previousTarget': array([26., 12.]), 'currentState': array([25.30661292, 12.66977414,  5.26971204]), 'targetState': array([26, 12], dtype=int32), 'currentDistance': 0.9640451426771737}
episode index:547
target Thresh 31.892192456755275
target distance 10.0
model initialize at round 547
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 11.]), 'previousTarget': array([17., 11.]), 'currentState': array([23.01836267, 20.98227285,  5.26289463]), 'targetState': array([17, 11], dtype=int32), 'currentDistance': 11.656176922457322}
done in step count: 23
reward sum = 0.5953800066234258
running average episode reward sum: 0.380085119558897
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 11.]), 'previousTarget': array([17., 11.]), 'currentState': array([17.88220781, 11.97552273,  4.0252681 ]), 'targetState': array([17, 11], dtype=int32), 'currentDistance': 1.3152700168075406}
episode index:548
target Thresh 31.893265159733655
target distance 16.0
model initialize at round 548
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6.86380892, 7.864798  ]), 'previousTarget': array([6.85786438, 7.85786438]), 'currentState': array([21.00219258, 22.01068459,  1.61716321]), 'targetState': array([5, 6], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.24340550894669405
running average episode reward sum: 0.3798361585195305
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 6.]), 'previousTarget': array([5., 6.]), 'currentState': array([5.90079183, 6.98933326, 4.04810917]), 'targetState': array([5, 6], dtype=int32), 'currentDistance': 1.3379858791355557}
episode index:549
target Thresh 31.894327189139062
target distance 7.0
model initialize at round 549
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 29.]), 'previousTarget': array([26., 29.]), 'currentState': array([26.87593586, 22.4842928 ,  1.79260285]), 'targetState': array([26, 29], dtype=int32), 'currentDistance': 6.57432156016352}
done in step count: 12
reward sum = 0.7740430097276171
running average episode reward sum: 0.38055289824899974
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 29.]), 'previousTarget': array([26., 29.]), 'currentState': array([25.75753752, 28.01730854,  1.68072863]), 'targetState': array([26, 29], dtype=int32), 'currentDistance': 1.01216133512846}
episode index:550
target Thresh 31.89537865117532
target distance 15.0
model initialize at round 550
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 25.]), 'previousTarget': array([25., 25.]), 'currentState': array([25.        , 10.        ,  4.00234079]), 'targetState': array([25, 25], dtype=int32), 'currentDistance': 15.0}
done in step count: 37
reward sum = 0.43821860068953167
running average episode reward sum: 0.3806575546962603
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 25.]), 'previousTarget': array([25., 25.]), 'currentState': array([25.14982732, 24.23342255,  1.38275278]), 'targetState': array([25, 25], dtype=int32), 'currentDistance': 0.7810820753447918}
episode index:551
target Thresh 31.89641965098951
target distance 13.0
model initialize at round 551
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 26.]), 'previousTarget': array([14., 26.]), 'currentState': array([10.        , 13.        ,  2.65568018]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 13.601470508735444}
done in step count: 31
reward sum = 0.5129152901683508
running average episode reward sum: 0.38089715204312996
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 26.]), 'previousTarget': array([14., 26.]), 'currentState': array([13.12908781, 25.23808807,  1.17586865]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 1.1571507405241246}
episode index:552
target Thresh 31.897450292682485
target distance 20.0
model initialize at round 552
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8.43046618, 20.57218647]), 'previousTarget': array([ 8.43046618, 20.57218647]), 'currentState': array([27.       , 28.       ,  4.7606777]), 'targetState': array([ 7, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.2846991065109677
running average episode reward sum: 0.38072319536043164
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 20.]), 'previousTarget': array([ 7., 20.]), 'currentState': array([ 7.99477712, 20.34538796,  3.29710592]), 'targetState': array([ 7, 20], dtype=int32), 'currentDistance': 1.0530310321301266}
episode index:553
target Thresh 31.898470679319267
target distance 6.0
model initialize at round 553
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([11.85969756, 18.69227015,  4.17176877]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 5.900448576233027}
done in step count: 12
reward sum = 0.7788673179246822
running average episode reward sum: 0.3814418670618112
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([ 6.87556872, 18.26897438,  2.71688575]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 0.9159518596195833}
episode index:554
target Thresh 31.899480912939374
target distance 12.0
model initialize at round 554
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 25.]), 'previousTarget': array([12., 25.]), 'currentState': array([23.66071345, 23.09742101,  2.78485814]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 11.814907744402545}
done in step count: 24
reward sum = 0.5920426295125333
running average episode reward sum: 0.38182132789505574
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 25.]), 'previousTarget': array([12., 25.]), 'currentState': array([12.74708616, 25.23554633,  3.24129185]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 0.7833388846634407}
episode index:555
target Thresh 31.90048109456701
target distance 7.0
model initialize at round 555
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 19.]), 'previousTarget': array([16., 19.]), 'currentState': array([22.69854346, 23.94324496,  3.50503045]), 'targetState': array([16, 19], dtype=int32), 'currentDistance': 8.3250318476734}
done in step count: 18
reward sum = 0.7077285452125385
running average episode reward sum: 0.3824074919549793
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 19.]), 'previousTarget': array([16., 19.]), 'currentState': array([16.89559655, 19.49068461,  3.70909132]), 'targetState': array([16, 19], dtype=int32), 'currentDistance': 1.0212074031512008}
episode index:556
target Thresh 31.90147132422117
target distance 12.0
model initialize at round 556
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([14.79397961, 17.14152818,  2.4404719 ]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 12.790840252057189}
done in step count: 31
reward sum = 0.5598871522820087
running average episode reward sum: 0.38272612689272983
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([10.10027004, 28.02347791,  1.60214546]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 0.9816564923041391}
episode index:557
target Thresh 31.902451700925642
target distance 23.0
model initialize at round 557
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3.75140711, 21.54352728]), 'previousTarget': array([ 3.75140711, 21.54352728]), 'currentState': array([8.        , 2.        , 3.17214486]), 'targetState': array([ 3, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 86
reward sum = 0.21206798314049047
running average episode reward sum: 0.38242028792543187
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 25.]), 'previousTarget': array([ 3., 25.]), 'currentState': array([ 2.95283259, 24.22910204,  1.88687455]), 'targetState': array([ 3, 25], dtype=int32), 'currentDistance': 0.7723395785606498}
episode index:558
target Thresh 31.90342232271892
target distance 14.0
model initialize at round 558
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  3.]), 'previousTarget': array([26.,  3.]), 'currentState': array([12.02729596, 10.9488375 ,  5.3784433 ]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 16.07546191142109}
done in step count: 35
reward sum = 0.4693463091128929
running average episode reward sum: 0.38257579064669744
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  3.]), 'previousTarget': array([26.,  3.]), 'currentState': array([25.03443231,  3.66201647,  5.69983516]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 1.170720620272669}
episode index:559
target Thresh 31.904383286663986
target distance 1.0
model initialize at round 559
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 16.]), 'previousTarget': array([16., 16.]), 'currentState': array([15.96624395, 17.01290561,  2.93355715]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 1.0134679281193948}
done in step count: 2
reward sum = 0.9664590765344768
running average episode reward sum: 0.38361843937149703
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 16.]), 'previousTarget': array([16., 16.]), 'currentState': array([15.78453326, 16.98072329,  3.60760918]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 1.0041135794132299}
episode index:560
target Thresh 31.905334688858037
target distance 21.0
model initialize at round 560
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.04871219,  8.03991134]), 'previousTarget': array([23.04869701,  8.02263725]), 'currentState': array([23.98453889, 28.01800505,  2.53282499]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.2667235260880697
running average episode reward sum: 0.3834100705421148
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([23.84357463,  7.94449527,  5.19610827]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 1.266368611800238}
episode index:561
target Thresh 31.906276624442086
target distance 10.0
model initialize at round 561
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 10.]), 'previousTarget': array([18., 10.]), 'currentState': array([7.98438046, 9.05524042, 1.64413738]), 'targetState': array([18, 10], dtype=int32), 'currentDistance': 10.060079791236975}
done in step count: 25
reward sum = 0.5755163017228047
running average episode reward sum: 0.38375189657624414
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 10.]), 'previousTarget': array([18., 10.]), 'currentState': array([17.19399948,  9.22446907,  5.73113363]), 'targetState': array([18, 10], dtype=int32), 'currentDistance': 1.1185191393138907}
episode index:562
target Thresh 31.907209187610476
target distance 16.0
model initialize at round 562
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([20.09426797,  3.94901425,  5.55620843]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 16.12854712601795}
done in step count: 47
reward sum = 0.38948089015973975
running average episode reward sum: 0.38376207240854165
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.68222135, 5.40186805, 3.23911174]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.7917852651870498}
episode index:563
target Thresh 31.908132471620302
target distance 13.0
model initialize at round 563
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 15.]), 'previousTarget': array([ 4., 15.]), 'currentState': array([ 4.03649022, 27.98737474,  5.80210364]), 'targetState': array([ 4, 15], dtype=int32), 'currentDistance': 12.987426006012226}
done in step count: 28
reward sum = 0.5440560433441519
running average episode reward sum: 0.38404628157686715
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 15.]), 'previousTarget': array([ 4., 15.]), 'currentState': array([ 3.55243077, 15.76606092,  4.74361785]), 'targetState': array([ 4, 15], dtype=int32), 'currentDistance': 0.8872246338887152}
episode index:564
target Thresh 31.909046568800733
target distance 11.0
model initialize at round 564
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  8.]), 'previousTarget': array([17.,  8.]), 'currentState': array([ 9.89844575, 19.03782818,  3.03752232]), 'targetState': array([17,  8], dtype=int32), 'currentDistance': 13.125003759209987}
done in step count: 30
reward sum = 0.49888280517569583
running average episode reward sum: 0.3842495320611129
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  8.]), 'previousTarget': array([17.,  8.]), 'currentState': array([16.52868574,  8.90937466,  5.2763481 ]), 'targetState': array([17,  8], dtype=int32), 'currentDistance': 1.0242555412675058}
episode index:565
target Thresh 31.90995157056225
target distance 7.0
model initialize at round 565
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 2.]), 'previousTarget': array([9., 2.]), 'currentState': array([10.97934394,  9.04815201,  2.20688498]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 7.3208093222909865}
done in step count: 18
reward sum = 0.6801973010150045
running average episode reward sum: 0.38477240797799256
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 2.]), 'previousTarget': array([9., 2.]), 'currentState': array([8.7759422 , 2.73285266, 4.53934989]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 0.7663386452539823}
episode index:566
target Thresh 31.91084756740578
target distance 9.0
model initialize at round 566
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 16.]), 'previousTarget': array([12., 16.]), 'currentState': array([17.55282355,  7.1561839 ,  2.73275538]), 'targetState': array([12, 16], dtype=int32), 'currentDistance': 10.44255392967469}
done in step count: 21
reward sum = 0.6374854009325736
running average episode reward sum: 0.385218109905602
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 16.]), 'previousTarget': array([12., 16.]), 'currentState': array([12.39311572, 15.05443523,  1.99853534]), 'targetState': array([12, 16], dtype=int32), 'currentDistance': 1.0240276888856659}
episode index:567
target Thresh 31.91173464893176
target distance 6.0
model initialize at round 567
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 18.]), 'previousTarget': array([25., 18.]), 'currentState': array([21.99270574, 24.00867917,  2.29551974]), 'targetState': array([25, 18], dtype=int32), 'currentDistance': 6.719229435810621}
done in step count: 17
reward sum = 0.6892584668313504
running average episode reward sum: 0.38575339222413324
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 18.]), 'previousTarget': array([25., 18.]), 'currentState': array([24.14005883, 18.426595  ,  5.25433812]), 'targetState': array([25, 18], dtype=int32), 'currentDistance': 0.9599385930582327}
episode index:568
target Thresh 31.912612903849077
target distance 11.0
model initialize at round 568
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 20.]), 'previousTarget': array([ 9., 20.]), 'currentState': array([18.95329735,  8.94114296,  3.7891283 ]), 'targetState': array([ 9, 20], dtype=int32), 'currentDistance': 14.878388593761187}
done in step count: 36
reward sum = 0.47183661057551435
running average episode reward sum: 0.38590468083283513
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 20.]), 'previousTarget': array([ 9., 20.]), 'currentState': array([ 9.80355471, 19.38601246,  2.33036441]), 'targetState': array([ 9, 20], dtype=int32), 'currentDistance': 1.0112768541138488}
episode index:569
target Thresh 31.913482419983957
target distance 5.0
model initialize at round 569
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 27.]), 'previousTarget': array([11., 27.]), 'currentState': array([ 6.        , 23.        ,  4.99146971]), 'targetState': array([11, 27], dtype=int32), 'currentDistance': 6.4031242374328485}
done in step count: 16
reward sum = 0.709519936455238
running average episode reward sum: 0.38647242689533057
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 27.]), 'previousTarget': array([11., 27.]), 'currentState': array([11.15202764, 26.13242491,  0.85160497]), 'targetState': array([11, 27], dtype=int32), 'currentDistance': 0.8807944982913741}
episode index:570
target Thresh 31.914343284288737
target distance 13.0
model initialize at round 570
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  2.]), 'previousTarget': array([20.,  2.]), 'currentState': array([13.        , 15.        ,  3.09793782]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 14.7648230602334}
done in step count: 40
reward sum = 0.4481305870975356
running average episode reward sum: 0.38658040966275997
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  2.]), 'previousTarget': array([20.,  2.]), 'currentState': array([19.76951316,  2.89760842,  5.28324344]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 0.9267281482082682}
episode index:571
target Thresh 31.91519558285056
target distance 10.0
model initialize at round 571
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([18.74188843,  9.36629491,  2.15096084]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 10.334892489488595}
done in step count: 21
reward sum = 0.6462019747732415
running average episode reward sum: 0.38703429351784824
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([15.70300321, 18.24119341,  1.94144641]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 1.0344085026133596}
episode index:572
target Thresh 31.916039400900004
target distance 3.0
model initialize at round 572
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 20.]), 'previousTarget': array([ 9., 20.]), 'currentState': array([ 6.        , 22.        ,  0.61792955]), 'targetState': array([ 9, 20], dtype=int32), 'currentDistance': 3.6055512754639896}
done in step count: 8
reward sum = 0.8592497184246778
running average episode reward sum: 0.3878584042070399
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 20.]), 'previousTarget': array([ 9., 20.]), 'currentState': array([ 8.32620593, 20.73935304,  5.1452297 ]), 'targetState': array([ 9, 20], dtype=int32), 'currentDistance': 1.0003206280083714}
episode index:573
target Thresh 31.91687482281957
target distance 18.0
model initialize at round 573
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8.33977985, 7.89657783]), 'previousTarget': array([8.35899411, 7.90599608]), 'currentState': array([24.96134213, 19.01969193,  2.86986926]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 64
reward sum = 0.29735583080617733
running average episode reward sum: 0.3877007342185366
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 7.]), 'previousTarget': array([7., 7.]), 'currentState': array([7.74128646, 7.51209519, 3.88160691]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 0.9009700880358171}
episode index:574
target Thresh 31.917701932152145
target distance 17.0
model initialize at round 574
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9.46353383, 24.33993163]), 'previousTarget': array([ 9.46633605, 24.33935727]), 'currentState': array([20.95749694,  7.97262238,  3.50432992]), 'targetState': array([ 9, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.3098851273564388
running average episode reward sum: 0.38756540272834167
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 25.]), 'previousTarget': array([ 9., 25.]), 'currentState': array([ 9.79129121, 24.24167708,  2.23171001]), 'targetState': array([ 9, 25], dtype=int32), 'currentDistance': 1.0959906148280012}
episode index:575
target Thresh 31.91852081160935
target distance 11.0
model initialize at round 575
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 28.]), 'previousTarget': array([17., 28.]), 'currentState': array([ 7.0231248 , 16.98321807,  5.88169062]), 'targetState': array([17, 28], dtype=int32), 'currentDistance': 14.862958075999908}
done in step count: 43
reward sum = 0.4397228604361002
running average episode reward sum: 0.3876559538701954
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 28.]), 'previousTarget': array([17., 28.]), 'currentState': array([17.84943014, 27.23262783,  1.55324921]), 'targetState': array([17, 28], dtype=int32), 'currentDistance': 1.1447233805615156}
episode index:576
target Thresh 31.91933154307982
target distance 3.0
model initialize at round 576
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([18.63082092, 12.17825515,  2.83907562]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 3.4155547409944225}
done in step count: 10
reward sum = 0.8317010941887333
running average episode reward sum: 0.38842552950332976
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.57691343, 10.85505005,  4.35553072]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.9539983432812549}
episode index:577
target Thresh 31.920134207637375
target distance 5.0
model initialize at round 577
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 20.]), 'previousTarget': array([14., 20.]), 'currentState': array([15.17125296, 15.10070238,  0.73658162]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 5.0373555236900875}
done in step count: 11
reward sum = 0.8028366930990836
running average episode reward sum: 0.3891425038348103
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 20.]), 'previousTarget': array([14., 20.]), 'currentState': array([14.96391725, 19.47501695,  2.12531245]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 1.097608155384556}
episode index:578
target Thresh 31.920928885549138
target distance 12.0
model initialize at round 578
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 12.]), 'previousTarget': array([22., 12.]), 'currentState': array([10.        , 13.        ,  3.87742507]), 'targetState': array([22, 12], dtype=int32), 'currentDistance': 12.041594578792296}
done in step count: 33
reward sum = 0.5191160881896617
running average episode reward sum: 0.38936698325511226
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 12.]), 'previousTarget': array([22., 12.]), 'currentState': array([21.17279682, 11.76224468,  0.08397002]), 'targetState': array([22, 12], dtype=int32), 'currentDistance': 0.8606931476146458}
episode index:579
target Thresh 31.921715656283563
target distance 19.0
model initialize at round 579
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  5.]), 'previousTarget': array([13.,  5.]), 'currentState': array([16.        , 24.        ,  6.03171063]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 19.235384061671343}
done in step count: 64
reward sum = 0.3150583334497604
running average episode reward sum: 0.38923886489337894
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  5.]), 'previousTarget': array([13.,  5.]), 'currentState': array([12.85473637,  5.78687217,  4.72867581]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 0.8001683166895238}
episode index:580
target Thresh 31.92249459851838
target distance 3.0
model initialize at round 580
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([26.        ,  2.        ,  4.70746344]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 9
reward sum = 0.8526141281542732
running average episode reward sum: 0.39003641267868167
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([23.81265587,  2.18023084,  2.57715596]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 1.1543097630365158}
episode index:581
target Thresh 31.92326579014846
target distance 8.0
model initialize at round 581
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'previousTarget': array([13., 22.]), 'currentState': array([ 5.        , 16.        ,  1.80602816]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 10.0}
done in step count: 22
reward sum = 0.6470284135179369
running average episode reward sum: 0.39047797969043296
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'previousTarget': array([13., 22.]), 'currentState': array([12.11743348, 21.26785244,  0.51651379]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 1.1467186708982195}
episode index:582
target Thresh 31.92402930829361
target distance 8.0
model initialize at round 582
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  5.]), 'previousTarget': array([23.,  5.]), 'currentState': array([15.        ,  4.        ,  5.42593461]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 8.062257748298551}
done in step count: 19
reward sum = 0.6967903619517195
running average episode reward sum: 0.39100338686412295
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  5.]), 'previousTarget': array([23.,  5.]), 'currentState': array([22.13592237,  4.92098965,  0.50908396]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 0.867682419702508}
episode index:583
target Thresh 31.924785229306277
target distance 4.0
model initialize at round 583
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 19.]), 'previousTarget': array([14., 19.]), 'currentState': array([10.        , 21.        ,  2.35186496]), 'targetState': array([14, 19], dtype=int32), 'currentDistance': 4.47213595499958}
done in step count: 13
reward sum = 0.7681569785418413
running average episode reward sum: 0.3916491978087766
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 19.]), 'previousTarget': array([14., 19.]), 'currentState': array([13.28855473, 19.74157937,  5.30107011]), 'targetState': array([14, 19], dtype=int32), 'currentDistance': 1.0276645078616344}
episode index:584
target Thresh 31.9255336287792
target distance 14.0
model initialize at round 584
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 29.]), 'previousTarget': array([23., 29.]), 'currentState': array([25.88472696, 15.35072426,  1.78824474]), 'targetState': array([23, 29], dtype=int32), 'currentDistance': 13.950784131341496}
done in step count: 42
reward sum = 0.4914274699309279
running average episode reward sum: 0.3918197589577033
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 29.]), 'previousTarget': array([23., 29.]), 'currentState': array([23.50192817, 28.15293823,  2.31083374]), 'targetState': array([23, 29], dtype=int32), 'currentDistance': 0.9846042484509988}
episode index:585
target Thresh 31.926274581552942
target distance 5.0
model initialize at round 585
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([22.653213  , 15.77644939,  3.86029637]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 4.8209082157654715}
done in step count: 10
reward sum = 0.8207586729043385
running average episode reward sum: 0.39255173662655424
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([22.09034835, 11.8559081 ,  4.81910958]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 0.8606634027370105}
episode index:586
target Thresh 31.9270081617234
target distance 19.0
model initialize at round 586
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 24.]), 'previousTarget': array([ 4., 24.]), 'currentState': array([6.        , 5.        , 6.00471556]), 'targetState': array([ 4, 24], dtype=int32), 'currentDistance': 19.104973174542796}
done in step count: 68
reward sum = 0.29283267587351436
running average episode reward sum: 0.3923818574770601
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 24.]), 'previousTarget': array([ 4., 24.]), 'currentState': array([ 4.9675895 , 23.32723829,  1.74914377]), 'targetState': array([ 4, 24], dtype=int32), 'currentDistance': 1.1784896134394804}
episode index:587
target Thresh 31.927734442649207
target distance 9.0
model initialize at round 587
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  7.]), 'previousTarget': array([11.,  7.]), 'currentState': array([20.        ,  3.        ,  5.60350809]), 'targetState': array([11,  7], dtype=int32), 'currentDistance': 9.848857801796104}
done in step count: 27
reward sum = 0.5885103512735987
running average episode reward sum: 0.39271540933725835
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  7.]), 'previousTarget': array([11.,  7.]), 'currentState': array([11.8644291 ,  6.5976217 ,  2.39516784]), 'targetState': array([11,  7], dtype=int32), 'currentDistance': 0.9534914593234226}
episode index:588
target Thresh 31.928453496959058
target distance 13.0
model initialize at round 588
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 8.]), 'previousTarget': array([5., 8.]), 'currentState': array([ 3.        , 21.        ,  6.11932964]), 'targetState': array([5, 8], dtype=int32), 'currentDistance': 13.152946437965905}
done in step count: 35
reward sum = 0.5150920319740429
running average episode reward sum: 0.3929231794945364
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 8.]), 'previousTarget': array([5., 8.]), 'currentState': array([4.7858612 , 8.84899548, 4.95617237]), 'targetState': array([5, 8], dtype=int32), 'currentDistance': 0.875584803153426}
episode index:589
target Thresh 31.92916539655898
target distance 8.0
model initialize at round 589
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 13.]), 'previousTarget': array([18., 13.]), 'currentState': array([18.23025559, 20.88140559,  5.76294184]), 'targetState': array([18, 13], dtype=int32), 'currentDistance': 7.8847683359924865}
done in step count: 18
reward sum = 0.7019577048673895
running average episode reward sum: 0.39344696682567687
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 13.]), 'previousTarget': array([18., 13.]), 'currentState': array([18.15406535, 13.84264463,  4.58939971]), 'targetState': array([18, 13], dtype=int32), 'currentDistance': 0.8566131588878363}
episode index:590
target Thresh 31.92987021263953
target distance 12.0
model initialize at round 590
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 18.]), 'previousTarget': array([11., 18.]), 'currentState': array([14.        ,  6.        ,  0.18451184]), 'targetState': array([11, 18], dtype=int32), 'currentDistance': 12.36931687685298}
done in step count: 34
reward sum = 0.5216703715528413
running average episode reward sum: 0.3936639269013573
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 18.]), 'previousTarget': array([11., 18.]), 'currentState': array([11.13207928, 17.03320645,  2.07282078]), 'targetState': array([11, 18], dtype=int32), 'currentDistance': 0.975773899629683}
episode index:591
target Thresh 31.9305680156829
target distance 15.0
model initialize at round 591
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([17.04785877,  6.00566588,  0.37033909]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 17.01758168440537}
done in step count: 60
reward sum = 0.35937604880052465
running average episode reward sum: 0.3936060081883491
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([ 9.99776132, 20.27752719,  2.07713141]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 1.231866311343418}
episode index:592
target Thresh 31.931258875469982
target distance 15.0
model initialize at round 592
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  3.]), 'previousTarget': array([11.,  3.]), 'currentState': array([26.        ,  9.        ,  5.33271646]), 'targetState': array([11,  3], dtype=int32), 'currentDistance': 16.15549442140351}
done in step count: 67
reward sum = 0.32465868917172935
running average episode reward sum: 0.39348973952221655
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  3.]), 'previousTarget': array([11.,  3.]), 'currentState': array([11.85290714,  3.19109706,  3.51132806]), 'targetState': array([11,  3], dtype=int32), 'currentDistance': 0.8740530192652635}
episode index:593
target Thresh 31.931942861087325
target distance 20.0
model initialize at round 593
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7.50001133, 14.22501076]), 'previousTarget': array([ 7.50001133, 14.22501076]), 'currentState': array([22.        , 28.        ,  2.85246897]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.09967814777392137
running average episode reward sum: 0.3926594905537045
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([ 7.68357898, 21.20542352,  3.82110204]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 13.463856554828897}
episode index:594
target Thresh 31.93262004093406
target distance 8.0
model initialize at round 594
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([ 4.10558268, 26.80837331,  5.32370403]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 9.243499264161349}
done in step count: 18
reward sum = 0.6863207694767491
running average episode reward sum: 0.3931530389216424
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([11.14975794, 22.73536724,  5.6845654 ]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 1.1241336826992658}
episode index:595
target Thresh 31.933290482728744
target distance 22.0
model initialize at round 595
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.18956241, 21.89210966]), 'previousTarget': array([22.18928508, 21.91786413]), 'currentState': array([23.98093159,  1.9724963 ,  3.87116095]), 'targetState': array([22, 24], dtype=int32), 'currentDistance': 20.0}
done in step count: 94
reward sum = 0.197022159728644
running average episode reward sum: 0.3928239602652783
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 24.]), 'previousTarget': array([22., 24.]), 'currentState': array([22.22232686, 23.10703086,  1.76004329]), 'targetState': array([22, 24], dtype=int32), 'currentDistance': 0.9202299307788109}
episode index:596
target Thresh 31.933954253516106
target distance 22.0
model initialize at round 596
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.06271518, 15.91231371]), 'previousTarget': array([21.79586847, 15.83486126]), 'currentState': array([3.30733036, 8.96713264, 6.22176636]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 78
reward sum = 0.25272411275092976
running average episode reward sum: 0.39258928715386404
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([24.02260122, 16.7639901 ,  0.32641772]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 1.0054894556679452}
episode index:597
target Thresh 31.93461141967378
target distance 14.0
model initialize at round 597
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 27.]), 'previousTarget': array([22., 27.]), 'currentState': array([ 8.        , 25.        ,  5.87505043]), 'targetState': array([22, 27], dtype=int32), 'currentDistance': 14.142135623730953}
done in step count: 55
reward sum = 0.4363692428647361
running average episode reward sum: 0.3926624977821431
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 27.]), 'previousTarget': array([22., 27.]), 'currentState': array([21.01111944, 26.39101928,  0.36152763]), 'targetState': array([22, 27], dtype=int32), 'currentDistance': 1.1613536355514336}
episode index:598
target Thresh 31.935262046918933
target distance 18.0
model initialize at round 598
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9.19631201, 17.36442559]), 'previousTarget': array([ 9.19631201, 17.36442559]), 'currentState': array([22.        ,  2.        ,  2.69168997]), 'targetState': array([ 7, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 78
reward sum = 0.2546815799871818
running average episode reward sum: 0.392432145665624
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 20.]), 'previousTarget': array([ 7., 20.]), 'currentState': array([ 7.7322024 , 19.14800236,  2.32741786]), 'targetState': array([ 7, 20], dtype=int32), 'currentDistance': 1.1233967789607242}
episode index:599
target Thresh 31.935906200314825
target distance 10.0
model initialize at round 599
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([14.18521062, 26.93574504,  5.83068438]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 10.100123263606344}
done in step count: 23
reward sum = 0.6329729223989142
running average episode reward sum: 0.3928330469601795
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([15.78961772, 17.94733331,  4.7382083 ]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 0.9704128482669386}
episode index:600
target Thresh 31.936543944277343
target distance 5.0
model initialize at round 600
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 22.]), 'previousTarget': array([11., 22.]), 'currentState': array([14.        , 17.        ,  0.53925721]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 5.8309518948453}
done in step count: 12
reward sum = 0.7687906076344341
running average episode reward sum: 0.3934586003057273
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 22.]), 'previousTarget': array([11., 22.]), 'currentState': array([11.99114681, 21.26175432,  2.47678362]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 1.2358716267145762}
episode index:601
target Thresh 31.937175342581405
target distance 12.0
model initialize at round 601
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([12.13810698, 17.58727349,  4.96265213]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 11.588096501024904}
done in step count: 26
reward sum = 0.5966241618386532
running average episode reward sum: 0.3937960846272106
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([11.98152681,  6.90637783,  4.65411097]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 0.9065660632028493}
episode index:602
target Thresh 31.93780045836737
target distance 9.0
model initialize at round 602
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 17.]), 'previousTarget': array([ 4., 17.]), 'currentState': array([ 1.91026185, 25.66037067,  4.57697682]), 'targetState': array([ 4, 17], dtype=int32), 'currentDistance': 8.908929544939479}
done in step count: 19
reward sum = 0.6879861015409685
running average episode reward sum: 0.3942839619355253
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 17.]), 'previousTarget': array([ 4., 17.]), 'currentState': array([ 3.449125  , 17.84887158,  4.83039778]), 'targetState': array([ 4, 17], dtype=int32), 'currentDistance': 1.0119516958286103}
episode index:603
target Thresh 31.938419354147342
target distance 22.0
model initialize at round 603
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.57878994,  9.42137338]), 'previousTarget': array([24.57704261,  9.44208854]), 'currentState': array([14.97255309, 26.96332918,  4.28495649]), 'targetState': array([27,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.18510455845561957
running average episode reward sum: 0.39393763841983004
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  5.]), 'previousTarget': array([27.,  5.]), 'currentState': array([26.11970532,  5.59646484,  5.45750983]), 'targetState': array([27,  5], dtype=int32), 'currentDistance': 1.063338625327664}
episode index:604
target Thresh 31.93903209181141
target distance 23.0
model initialize at round 604
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.98112317,  7.13125551]), 'previousTarget': array([21.98112317,  7.13125551]), 'currentState': array([2.        , 8.        , 2.23184061]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.2269518659336398
running average episode reward sum: 0.39366162887853057
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([24.18485999,  6.87416777,  0.21144349]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 0.8247951228305148}
episode index:605
target Thresh 31.93963873263385
target distance 9.0
model initialize at round 605
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 19.]), 'previousTarget': array([ 3., 19.]), 'currentState': array([11.95681325, 15.93430101,  4.033869  ]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 9.466943221759117}
done in step count: 64
reward sum = 0.3940307377143777
running average episode reward sum: 0.3936622379690188
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 19.]), 'previousTarget': array([ 3., 19.]), 'currentState': array([ 3.21026052, 18.06778313,  1.65716561]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 0.9556347476237799}
episode index:606
target Thresh 31.940239337279255
target distance 6.0
model initialize at round 606
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 17.]), 'previousTarget': array([19., 17.]), 'currentState': array([12.97472573, 12.04936311,  1.8665176 ]), 'targetState': array([19, 17], dtype=int32), 'currentDistance': 7.798252082934208}
done in step count: 32
reward sum = 0.5891532163153284
running average episode reward sum: 0.39398429888886444
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 17.]), 'previousTarget': array([19., 17.]), 'currentState': array([18.2647945 , 16.79677556,  0.13198575]), 'targetState': array([19, 17], dtype=int32), 'currentDistance': 0.7627760461544356}
episode index:607
target Thresh 31.940833965808586
target distance 12.0
model initialize at round 607
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([19.82651315, 15.77286227,  4.1363641 ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 13.135773350623886}
done in step count: 27
reward sum = 0.5554876585225074
running average episode reward sum: 0.3942499294145777
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.40689773,  4.78764936,  4.71119015]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.8865423119885086}
episode index:608
target Thresh 31.941422677685193
target distance 21.0
model initialize at round 608
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.00117274, 13.47142148]), 'previousTarget': array([22.00530293, 13.47290771]), 'currentState': array([ 4.98817619, 23.98607547,  4.2608974 ]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.2103056344518613
running average episode reward sum: 0.393947886237299
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'previousTarget': array([26., 11.]), 'currentState': array([2.52843848e+01, 1.05777098e+01, 1.96721924e-02]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 0.8309237149983825}
episode index:609
target Thresh 31.942005531780755
target distance 11.0
model initialize at round 609
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 12.]), 'previousTarget': array([ 8., 12.]), 'currentState': array([19.02331988,  6.05460614,  1.38812765]), 'targetState': array([ 8, 12], dtype=int32), 'currentDistance': 12.52442769962367}
done in step count: 28
reward sum = 0.5400926083216956
running average episode reward sum: 0.3941874677489127
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 12.]), 'previousTarget': array([ 8., 12.]), 'currentState': array([ 8.67154857, 11.86584338,  3.04815127]), 'targetState': array([ 8, 12], dtype=int32), 'currentDistance': 0.6848178428709911}
episode index:610
target Thresh 31.942582586381164
target distance 22.0
model initialize at round 610
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.19989685, 20.939654  ]), 'previousTarget': array([13.42295739, 20.55791146]), 'currentState': array([22.72737718,  3.35479919,  2.26874299]), 'targetState': array([11, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 78
reward sum = 0.2233157992326783
running average episode reward sum: 0.39390780871697123
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 25.]), 'previousTarget': array([11., 25.]), 'currentState': array([11.76592571, 24.02818622,  2.02277274]), 'targetState': array([11, 25], dtype=int32), 'currentDistance': 1.2373617977295834}
episode index:611
target Thresh 31.943153899192364
target distance 16.0
model initialize at round 611
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 12.]), 'previousTarget': array([18., 12.]), 'currentState': array([23.96818362, 28.04822412,  2.4065026 ]), 'targetState': array([18, 12], dtype=int32), 'currentDistance': 17.12205341355654}
done in step count: 43
reward sum = 0.3843344239090543
running average episode reward sum: 0.39389216593133736
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 12.]), 'previousTarget': array([18., 12.]), 'currentState': array([17.84103065, 12.82318343,  4.49680947]), 'targetState': array([18, 12], dtype=int32), 'currentDistance': 0.8383926395090122}
episode index:612
target Thresh 31.94371952734611
target distance 10.0
model initialize at round 612
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([16.85480425, 18.22186337,  2.35461548]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 11.960781867258433}
done in step count: 23
reward sum = 0.5960235292767418
running average episode reward sum: 0.3942219071439726
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([ 7.7545088 , 24.28449903,  2.79868344]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 1.039819777066064}
episode index:613
target Thresh 31.944279527405694
target distance 22.0
model initialize at round 613
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.0337022 , 25.98080136]), 'previousTarget': array([18.06407315, 26.0585156 ]), 'currentState': array([11.93730501,  6.93260088,  3.71173766]), 'targetState': array([19, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 70
reward sum = 0.21231289871258813
running average episode reward sum: 0.39392563840059897
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 29.]), 'previousTarget': array([19., 29.]), 'currentState': array([18.87974315, 28.18540978,  1.31912382]), 'targetState': array([19, 29], dtype=int32), 'currentDistance': 0.823419057166617}
episode index:614
target Thresh 31.944833955371582
target distance 14.0
model initialize at round 614
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 21.]), 'previousTarget': array([ 3., 21.]), 'currentState': array([17.        , 14.        ,  1.88090974]), 'targetState': array([ 3, 21], dtype=int32), 'currentDistance': 15.652475842498529}
done in step count: 36
reward sum = 0.4644454308922652
running average episode reward sum: 0.3940403047298537
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 21.]), 'previousTarget': array([ 3., 21.]), 'currentState': array([ 3.80399185, 20.54470267,  2.96196347]), 'targetState': array([ 3, 21], dtype=int32), 'currentDistance': 0.9239580956399919}
episode index:615
target Thresh 31.945382866687034
target distance 15.0
model initialize at round 615
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 13.]), 'previousTarget': array([ 7., 13.]), 'currentState': array([21.7613093 , 24.90867129,  3.62919512]), 'targetState': array([ 7, 13], dtype=int32), 'currentDistance': 18.966093536168426}
done in step count: 50
reward sum = 0.3790887681899017
running average episode reward sum: 0.3940160327549512
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 13.]), 'previousTarget': array([ 7., 13.]), 'currentState': array([ 7.30818454, 13.82731405,  3.93357874]), 'targetState': array([ 7, 13], dtype=int32), 'currentDistance': 0.8828512057644511}
episode index:616
target Thresh 31.945926316243643
target distance 15.0
model initialize at round 616
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'previousTarget': array([11., 19.]), 'currentState': array([25.98147715, 25.03168544,  2.3472257 ]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 16.150104855201434}
done in step count: 36
reward sum = 0.4517818522223695
running average episode reward sum: 0.39410965644938784
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'previousTarget': array([11., 19.]), 'currentState': array([11.78311134, 19.87042287,  3.54042047]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 1.1708541084329498}
episode index:617
target Thresh 31.946464358386812
target distance 10.0
model initialize at round 617
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 25.]), 'previousTarget': array([17., 25.]), 'currentState': array([ 6.99634553, 22.97699905,  4.78133807]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 10.206156752856755}
done in step count: 24
reward sum = 0.6049661392096723
running average episode reward sum: 0.39445084816906467
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 25.]), 'previousTarget': array([17., 25.]), 'currentState': array([16.11701882, 24.44563953,  0.50192861]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 1.0425791587725586}
episode index:618
target Thresh 31.946997046921208
target distance 24.0
model initialize at round 618
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.28797975,  7.27212152]), 'previousTarget': array([12.28797975,  7.27212152]), 'currentState': array([ 9.        , 27.        ,  0.11652991]), 'targetState': array([13,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 77
reward sum = 0.22380891239407352
running average episode reward sum: 0.3941751746056156
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  3.]), 'previousTarget': array([13.,  3.]), 'currentState': array([13.03079458,  3.90320001,  4.93482224]), 'targetState': array([13,  3], dtype=int32), 'currentDistance': 0.9037248316152398}
episode index:619
target Thresh 31.94752443511613
target distance 7.0
model initialize at round 619
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 24.]), 'previousTarget': array([ 8., 24.]), 'currentState': array([14.76699652, 23.24647459,  2.55899322]), 'targetState': array([ 8, 24], dtype=int32), 'currentDistance': 6.808820926215481}
done in step count: 16
reward sum = 0.7465935948436584
running average episode reward sum: 0.39474359141245113
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 24.]), 'previousTarget': array([ 8., 24.]), 'currentState': array([ 8.89619469, 24.05242084,  3.35809047]), 'targetState': array([ 8, 24], dtype=int32), 'currentDistance': 0.8977265022448463}
episode index:620
target Thresh 31.94804657571083
target distance 5.0
model initialize at round 620
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 28.]), 'previousTarget': array([15., 28.]), 'currentState': array([14.        , 23.        ,  3.09345371]), 'targetState': array([15, 28], dtype=int32), 'currentDistance': 5.0990195135927845}
done in step count: 13
reward sum = 0.7801652616437823
running average episode reward sum: 0.39536423822441785
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 28.]), 'previousTarget': array([15., 28.]), 'currentState': array([14.66034932, 27.07683255,  1.30654449]), 'targetState': array([15, 28], dtype=int32), 'currentDistance': 0.9836669801060676}
episode index:621
target Thresh 31.948563520919805
target distance 5.0
model initialize at round 621
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  7.]), 'previousTarget': array([12.,  7.]), 'currentState': array([16.80161707, 10.14626012,  2.75879407]), 'targetState': array([12,  7], dtype=int32), 'currentDistance': 5.740599206938313}
done in step count: 11
reward sum = 0.7874896207889313
running average episode reward sum: 0.3959946648844894
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  7.]), 'previousTarget': array([12.,  7.]), 'currentState': array([12.70629219,  7.82989913,  3.99570341]), 'targetState': array([12,  7], dtype=int32), 'currentDistance': 1.0897620025553079}
episode index:622
target Thresh 31.949075322438013
target distance 12.0
model initialize at round 622
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 15.]), 'previousTarget': array([27., 15.]), 'currentState': array([19.00434855, 27.01854785,  1.10426524]), 'targetState': array([27, 15], dtype=int32), 'currentDistance': 14.435232405276302}
done in step count: 37
reward sum = 0.4700309140145216
running average episode reward sum: 0.396113503165597
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 15.]), 'previousTarget': array([27., 15.]), 'currentState': array([26.31533069, 15.79229322,  5.45363717]), 'targetState': array([27, 15], dtype=int32), 'currentDistance': 1.0471392518071179}
episode index:623
target Thresh 31.949582031446024
target distance 13.0
model initialize at round 623
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 16.]), 'previousTarget': array([12., 16.]), 'currentState': array([21.        ,  3.        ,  4.76854134]), 'targetState': array([12, 16], dtype=int32), 'currentDistance': 15.811388300841896}
done in step count: 45
reward sum = 0.41051493090762115
running average episode reward sum: 0.39613658237672206
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 16.]), 'previousTarget': array([12., 16.]), 'currentState': array([12.99034432, 15.56358446,  2.37690513]), 'targetState': array([12, 16], dtype=int32), 'currentDistance': 1.082238600052339}
episode index:624
target Thresh 31.950083698615163
target distance 10.0
model initialize at round 624
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  3.]), 'previousTarget': array([12.,  3.]), 'currentState': array([ 2.20331721, 11.02450035,  6.16007424]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 12.663632949495883}
done in step count: 31
reward sum = 0.5573790521720474
running average episode reward sum: 0.3963945703283946
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  3.]), 'previousTarget': array([12.,  3.]), 'currentState': array([11.10738908,  3.58085474,  5.59014294]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 1.0649631399823718}
episode index:625
target Thresh 31.95058037411257
target distance 16.0
model initialize at round 625
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 13.]), 'previousTarget': array([ 8., 13.]), 'currentState': array([23.89863433, 24.84182854,  4.10361124]), 'targetState': array([ 8, 13], dtype=int32), 'currentDistance': 19.824113517210492}
done in step count: 52
reward sum = 0.365784236694471
running average episode reward sum: 0.3963456720318548
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 13.]), 'previousTarget': array([ 8., 13.]), 'currentState': array([ 8.86344581, 13.7821119 ,  3.9881784 ]), 'targetState': array([ 8, 13], dtype=int32), 'currentDistance': 1.1650054480029322}
episode index:626
target Thresh 31.951072107606205
target distance 6.0
model initialize at round 626
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 28.]), 'previousTarget': array([16., 28.]), 'currentState': array([22.0053783 , 21.943227  ,  4.61158296]), 'targetState': array([16, 28], dtype=int32), 'currentDistance': 8.529306399737942}
done in step count: 24
reward sum = 0.6356637869331366
running average episode reward sum: 0.39672735961542943
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 28.]), 'previousTarget': array([16., 28.]), 'currentState': array([16.24331026, 27.07386486,  2.45962868]), 'targetState': array([16, 28], dtype=int32), 'currentDistance': 0.9575626286551295}
episode index:627
target Thresh 31.951558948269824
target distance 9.0
model initialize at round 627
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 15.]), 'previousTarget': array([ 6., 15.]), 'currentState': array([11.        ,  6.        ,  5.85903081]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 10.295630140987}
done in step count: 25
reward sum = 0.5892740465281363
running average episode reward sum: 0.3970339626200675
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 15.]), 'previousTarget': array([ 6., 15.]), 'currentState': array([ 6.94830276, 14.0554603 ,  2.15776938]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 1.3384443901873702}
episode index:628
target Thresh 31.952040944787907
target distance 13.0
model initialize at round 628
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 17.]), 'previousTarget': array([19., 17.]), 'currentState': array([8.       , 4.       , 0.4432998]), 'targetState': array([19, 17], dtype=int32), 'currentDistance': 17.029386365926403}
done in step count: 43
reward sum = 0.44302127375828876
running average episode reward sum: 0.3971070744024812
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 17.]), 'previousTarget': array([19., 17.]), 'currentState': array([18.32901575, 16.19950466,  1.00076503]), 'targetState': array([19, 17], dtype=int32), 'currentDistance': 1.0445155095078176}
episode index:629
target Thresh 31.9525181453605
target distance 7.0
model initialize at round 629
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([20.88574778, 18.91973323,  3.63795246]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 8.00388027377662}
done in step count: 18
reward sum = 0.7073814677798205
running average episode reward sum: 0.39759957343958807
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([14.80339212, 22.51609568,  2.81304276]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 0.9378711461928906}
episode index:630
target Thresh 31.95299059770806
target distance 4.0
model initialize at round 630
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 25.]), 'previousTarget': array([10., 25.]), 'currentState': array([ 9.13589542, 20.99238574,  0.16725742]), 'targetState': array([10, 25], dtype=int32), 'currentDistance': 4.099713254998078}
done in step count: 10
reward sum = 0.8273480918850675
running average episode reward sum: 0.3982806328982972
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 25.]), 'previousTarget': array([10., 25.]), 'currentState': array([10.59793277, 24.11956693,  1.81873017]), 'targetState': array([10, 25], dtype=int32), 'currentDistance': 1.064277208758069}
episode index:631
target Thresh 31.953458349076218
target distance 13.0
model initialize at round 631
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 23.]), 'previousTarget': array([ 7., 23.]), 'currentState': array([20.        , 18.        ,  0.60095298]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 13.92838827718412}
done in step count: 38
reward sum = 0.4743799108569927
running average episode reward sum: 0.3984010431482318
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 23.]), 'previousTarget': array([ 7., 23.]), 'currentState': array([ 7.9755263 , 22.59138171,  3.03684249]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 1.057648558549472}
episode index:632
target Thresh 31.953921446240496
target distance 13.0
model initialize at round 632
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 13.]), 'previousTarget': array([ 2., 13.]), 'currentState': array([15.02588345, 23.00352305,  6.18867118]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 16.42388848910938}
done in step count: 47
reward sum = 0.39787780928456196
running average episode reward sum: 0.39840021655445035
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 13.]), 'previousTarget': array([ 2., 13.]), 'currentState': array([ 2.76381576, 13.98472267,  3.93208746]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 1.2462316243520517}
episode index:633
target Thresh 31.954379935511003
target distance 8.0
model initialize at round 633
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  7.]), 'previousTarget': array([11.,  7.]), 'currentState': array([3.36781851, 9.7457389 , 5.68492992]), 'targetState': array([11,  7], dtype=int32), 'currentDistance': 8.111058897408684}
done in step count: 18
reward sum = 0.7131632173890423
running average episode reward sum: 0.39889668816459956
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  7.]), 'previousTarget': array([11.,  7.]), 'currentState': array([10.12060974,  7.06957562,  6.05887606]), 'targetState': array([11,  7], dtype=int32), 'currentDistance': 0.8821383109495082}
episode index:634
target Thresh 31.95483386273704
target distance 6.0
model initialize at round 634
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 25.]), 'previousTarget': array([19., 25.]), 'currentState': array([24.85127607, 18.96985649,  3.16754302]), 'targetState': array([19, 25], dtype=int32), 'currentDistance': 8.402384329640842}
done in step count: 20
reward sum = 0.7012687865373284
running average episode reward sum: 0.39937286469746996
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 25.]), 'previousTarget': array([19., 25.]), 'currentState': array([19.88072736, 24.2336999 ,  2.43999766]), 'targetState': array([19, 25], dtype=int32), 'currentDistance': 1.167431591456279}
episode index:635
target Thresh 31.955283273311714
target distance 8.0
model initialize at round 635
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 20.]), 'previousTarget': array([ 8., 20.]), 'currentState': array([15.65595789, 23.80871727,  3.62381619]), 'targetState': array([ 8, 20], dtype=int32), 'currentDistance': 8.55102440796826}
done in step count: 22
reward sum = 0.685578179710747
running average episode reward sum: 0.3998228730544091
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 20.]), 'previousTarget': array([ 8., 20.]), 'currentState': array([ 8.38515451, 20.9314933 ,  3.99938165]), 'targetState': array([ 8, 20], dtype=int32), 'currentDistance': 1.0079800445222773}
episode index:636
target Thresh 31.955728212176457
target distance 19.0
model initialize at round 636
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.31492866, 24.69836445]), 'previousTarget': array([24.31492866, 24.69836445]), 'currentState': array([15.        ,  7.        ,  5.56014729]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.28541741081554983
running average episode reward sum: 0.39964327264273114
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 26.]), 'previousTarget': array([25., 26.]), 'currentState': array([24.81042158, 25.0137465 ,  1.31608498]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 1.0043086855359447}
episode index:637
target Thresh 31.956168723825524
target distance 18.0
model initialize at round 637
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([5.99779714, 8.99832955, 4.04289198]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 19.314655682197976}
done in step count: 61
reward sum = 0.3179291051705446
running average episode reward sum: 0.3995151940103296
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([23.10906289,  2.29354474,  6.26024538]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 0.9380498084541168}
episode index:638
target Thresh 31.956604852310445
target distance 10.0
model initialize at round 638
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([10.81305661, 15.74105964,  4.05312152]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 12.487277387615544}
done in step count: 29
reward sum = 0.5741152496704269
running average episode reward sum: 0.3997884335340543
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.30217428, 6.88775821, 4.06930574]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.9377760573073038}
episode index:639
target Thresh 31.957036641244436
target distance 13.0
model initialize at round 639
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 18.]), 'previousTarget': array([24., 18.]), 'currentState': array([11.        , 28.        ,  0.26466471]), 'targetState': array([24, 18], dtype=int32), 'currentDistance': 16.401219466856727}
done in step count: 42
reward sum = 0.4351234489908937
running average episode reward sum: 0.39984364449570564
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 18.]), 'previousTarget': array([24., 18.]), 'currentState': array([23.02660215, 18.19655222,  5.74377022]), 'targetState': array([24, 18], dtype=int32), 'currentDistance': 0.9930438762033865}
episode index:640
target Thresh 31.95746413380675
target distance 8.0
model initialize at round 640
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 10.]), 'previousTarget': array([17., 10.]), 'currentState': array([ 8.9412527 , 14.0427055 ,  2.30829877]), 'targetState': array([17, 10], dtype=int32), 'currentDistance': 9.015923457470159}
done in step count: 25
reward sum = 0.6182754602823659
running average episode reward sum: 0.40018441175902336
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 10.]), 'previousTarget': array([17., 10.]), 'currentState': array([16.1577118 , 10.82211067,  5.7802871 ]), 'targetState': array([17, 10], dtype=int32), 'currentDistance': 1.1769942138396006}
episode index:641
target Thresh 31.957887372746992
target distance 17.0
model initialize at round 641
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8.54809982, 10.28085576]), 'previousTarget': array([ 8.56139529, 10.28585494]), 'currentState': array([23.95760101, 23.03025861,  2.77425528]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.2958649281465951
running average episode reward sum: 0.4000219203515274
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([7.61585496, 9.88596849, 4.07271019]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 1.0789891061075416}
episode index:642
target Thresh 31.95830640038942
target distance 6.0
model initialize at round 642
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([18.        , 28.        ,  0.35786027]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 7.211102550927978}
done in step count: 20
reward sum = 0.6851101830314232
running average episode reward sum: 0.40046529245522866
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([14.86106974, 22.77869668,  3.95078894]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 1.1609520297238205}
episode index:643
target Thresh 31.95872125863714
target distance 3.0
model initialize at round 643
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([17.06472277,  9.99711297,  6.02005417]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 3.658002986205798}
done in step count: 13
reward sum = 0.8047666903508387
running average episode reward sum: 0.40109308965692997
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.98877676,  8.39825457,  3.42366355]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.0659672482846687}
episode index:644
target Thresh 31.959131988976328
target distance 3.0
model initialize at round 644
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  4.]), 'previousTarget': array([10.,  4.]), 'currentState': array([6.99881176, 4.99478282, 4.67645824]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 3.161759588446391}
done in step count: 9
reward sum = 0.8627753822347769
running average episode reward sum: 0.40180887615705063
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  4.]), 'previousTarget': array([10.,  4.]), 'currentState': array([9.07960419, 3.94961961, 0.14454271]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 0.9217736342549864}
episode index:645
target Thresh 31.959538632480353
target distance 24.0
model initialize at round 645
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6.04146451, 15.48939795]), 'previousTarget': array([ 6.15444247, 15.48069469]), 'currentState': array([25.88373024, 12.98250929,  3.21284766]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.26704847999349285
running average episode reward sum: 0.4016002687326488
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 16.]), 'previousTarget': array([ 2., 16.]), 'currentState': array([ 2.89667737, 15.95052548,  3.306189  ]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 0.8980412164438565}
episode index:646
target Thresh 31.95994122981391
target distance 11.0
model initialize at round 646
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 25.]), 'previousTarget': array([20., 25.]), 'currentState': array([16.0107931 , 13.97027102,  5.31314182]), 'targetState': array([20, 25], dtype=int32), 'currentDistance': 11.72896811357504}
done in step count: 27
reward sum = 0.5378913390195001
running average episode reward sum: 0.4018109195368016
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 25.]), 'previousTarget': array([20., 25.]), 'currentState': array([20.00894845, 24.14479017,  1.30217241]), 'targetState': array([20, 25], dtype=int32), 'currentDistance': 0.855256639887029}
episode index:647
target Thresh 31.960339821237064
target distance 16.0
model initialize at round 647
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 15.]), 'previousTarget': array([18., 15.]), 'currentState': array([2.08493688, 7.12243159, 0.87335464]), 'targetState': array([18, 15], dtype=int32), 'currentDistance': 17.757964921685847}
done in step count: 48
reward sum = 0.4233738656968403
running average episode reward sum: 0.4018441956882831
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 15.]), 'previousTarget': array([18., 15.]), 'currentState': array([17.08939232, 14.20825804,  0.58398442]), 'targetState': array([18, 15], dtype=int32), 'currentDistance': 1.206673803519821}
episode index:648
target Thresh 31.960734446609294
target distance 18.0
model initialize at round 648
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([ 8.        , 11.        ,  3.95180535]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 18.110770276274835}
done in step count: 53
reward sum = 0.35136045340271216
running average episode reward sum: 0.4017664087202006
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([25.07364258,  8.52800387,  6.15881671]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 1.0396722632726947}
episode index:649
target Thresh 31.961125145393463
target distance 19.0
model initialize at round 649
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.69834038, 18.94641429]), 'previousTarget': array([19.67985983, 18.90977806]), 'currentState': array([8.9911037 , 2.0539579 , 1.48170102]), 'targetState': array([21, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 92
reward sum = 0.22892106978312451
running average episode reward sum: 0.40150049281414346
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 21.]), 'previousTarget': array([21., 21.]), 'currentState': array([20.85468801, 20.17913579,  1.48420993]), 'targetState': array([21, 21], dtype=int32), 'currentDistance': 0.8336267896928681}
episode index:650
target Thresh 31.961511956659773
target distance 20.0
model initialize at round 650
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.9290837 , 24.00325976]), 'previousTarget': array([26.97504678, 24.00124766]), 'currentState': array([ 6.95017933, 24.92161721,  4.39869833]), 'targetState': array([27, 24], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.3025997956166057
running average episode reward sum: 0.4013485716202917
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 24.]), 'previousTarget': array([27., 24.]), 'currentState': array([26.06273964, 23.72352989,  0.09474344]), 'targetState': array([27, 24], dtype=int32), 'currentDistance': 0.9771861172655301}
episode index:651
target Thresh 31.96189491908968
target distance 17.0
model initialize at round 651
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.71414506, 24.43860471]), 'previousTarget': array([22.71414506, 24.43860471]), 'currentState': array([10.        ,  9.        ,  2.68617868]), 'targetState': array([24, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 83
reward sum = 0.21677302322351108
running average episode reward sum: 0.40106548028839484
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 26.]), 'previousTarget': array([24., 26.]), 'currentState': array([24.68048481, 25.07048301,  1.4052188 ]), 'targetState': array([24, 26], dtype=int32), 'currentDistance': 1.1519815151477892}
episode index:652
target Thresh 31.96227407097974
target distance 7.0
model initialize at round 652
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 12.]), 'previousTarget': array([13., 12.]), 'currentState': array([ 6.00936738, 17.01776065,  0.86919528]), 'targetState': array([13, 12], dtype=int32), 'currentDistance': 8.605048886755988}
done in step count: 24
reward sum = 0.6611170189377916
running average episode reward sum: 0.4014637215420692
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 12.]), 'previousTarget': array([13., 12.]), 'currentState': array([12.16253167, 12.40542614,  5.5209384 ]), 'targetState': array([13, 12], dtype=int32), 'currentDistance': 0.9304426704079457}
episode index:653
target Thresh 31.962649450245458
target distance 6.0
model initialize at round 653
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([14.93294871,  9.6359892 ,  4.4156599 ]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 8.18316898768646}
done in step count: 18
reward sum = 0.7145422088369209
running average episode reward sum: 0.4019424348253947
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([9.95604188, 4.65948482, 3.74469517]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 1.1614371703626478}
episode index:654
target Thresh 31.963021094425073
target distance 13.0
model initialize at round 654
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  7.]), 'previousTarget': array([17.,  7.]), 'currentState': array([ 5.01364326, 19.95353463,  5.14897931]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 17.64842223078165}
done in step count: 49
reward sum = 0.42598469184802085
running average episode reward sum: 0.4019791405613071
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  7.]), 'previousTarget': array([17.,  7.]), 'currentState': array([16.16882806,  7.67445676,  5.50619688]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 1.0703918472095708}
episode index:655
target Thresh 31.963389040683317
target distance 7.0
model initialize at round 655
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 20.]), 'previousTarget': array([20., 20.]), 'currentState': array([24.00273322, 12.98366266,  4.62565279]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 8.07780061904542}
done in step count: 27
reward sum = 0.6335758738871973
running average episode reward sum: 0.40233218436210877
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 20.]), 'previousTarget': array([20., 20.]), 'currentState': array([20.17925485, 19.08167585,  1.81836137]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 0.935655678025507}
episode index:656
target Thresh 31.963753325815123
target distance 12.0
model initialize at round 656
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([18.07042495, 10.35223895,  1.50629607]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 11.696843470222772}
done in step count: 26
reward sum = 0.6040015917696357
running average episode reward sum: 0.40263913932011114
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([17.27277997, 21.00525914,  1.63841519]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 1.0314641463280452}
episode index:657
target Thresh 31.964113986249302
target distance 12.0
model initialize at round 657
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([ 9.98840272, 15.98849594,  4.17512488]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 12.17508050716989}
done in step count: 33
reward sum = 0.5294352067750344
running average episode reward sum: 0.4028318385107721
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([21.17213363, 13.93261989,  6.24239311]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 0.8306038764309902}
episode index:658
target Thresh 31.9644710580522
target distance 5.0
model initialize at round 658
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.94813045, 3.47582462, 1.68486254]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 4.52447270810004}
done in step count: 11
reward sum = 0.835202034656059
running average episode reward sum: 0.40348793896015794
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.71960354, 7.13507505, 1.6048077 ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.9092399763454011}
episode index:659
target Thresh 31.964824576931296
target distance 4.0
model initialize at round 659
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  3.]), 'previousTarget': array([21.,  3.]), 'currentState': array([24.66478023,  3.74430488,  3.77852892]), 'targetState': array([21,  3], dtype=int32), 'currentDistance': 3.7395994309244722}
done in step count: 10
reward sum = 0.8602236171158286
running average episode reward sum: 0.4041799627149392
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  3.]), 'previousTarget': array([21.,  3.]), 'currentState': array([21.84833091,  2.88183402,  3.17972447]), 'targetState': array([21,  3], dtype=int32), 'currentDistance': 0.8565211835567805}
episode index:660
target Thresh 31.965174578238774
target distance 13.0
model initialize at round 660
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 28.]), 'previousTarget': array([10., 28.]), 'currentState': array([ 6.14763304, 15.15675232,  1.05139673]), 'targetState': array([10, 28], dtype=int32), 'currentDistance': 13.408569733049717}
done in step count: 34
reward sum = 0.5340460903731691
running average episode reward sum: 0.4043764318944525
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 28.]), 'previousTarget': array([10., 28.]), 'currentState': array([ 9.84018608, 27.00102237,  1.377275  ]), 'targetState': array([10, 28], dtype=int32), 'currentDistance': 1.0116801835176248}
episode index:661
target Thresh 31.965521096975053
target distance 11.0
model initialize at round 661
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 24.]), 'previousTarget': array([18., 24.]), 'currentState': array([18.        , 13.        ,  2.78585732]), 'targetState': array([18, 24], dtype=int32), 'currentDistance': 11.000000000000002}
done in step count: 29
reward sum = 0.5901571803086605
running average episode reward sum: 0.40465706746607516
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 24.]), 'previousTarget': array([18., 24.]), 'currentState': array([17.72411556, 23.13537175,  0.98350133]), 'targetState': array([18, 24], dtype=int32), 'currentDistance': 0.9075760260459614}
episode index:662
target Thresh 31.965864167792297
target distance 13.0
model initialize at round 662
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  7.]), 'previousTarget': array([21.,  7.]), 'currentState': array([18.0161059 , 20.01525038,  0.50562108]), 'targetState': array([21,  7], dtype=int32), 'currentDistance': 13.352916025442795}
done in step count: 34
reward sum = 0.5072958014754481
running average episode reward sum: 0.4048118770196338
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  7.]), 'previousTarget': array([21.,  7.]), 'currentState': array([20.7530213 ,  7.90317136,  5.00377306]), 'targetState': array([21,  7], dtype=int32), 'currentDistance': 0.9363316643903697}
episode index:663
target Thresh 31.966203824997876
target distance 12.0
model initialize at round 663
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 24.]), 'previousTarget': array([15., 24.]), 'currentState': array([ 6.90348704, 11.98330459,  3.08400291]), 'targetState': array([15, 24], dtype=int32), 'currentDistance': 14.489806436495124}
done in step count: 40
reward sum = 0.46907904014641716
running average episode reward sum: 0.40490866491590904
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 24.]), 'previousTarget': array([15., 24.]), 'currentState': array([14.207614  , 23.06759245,  0.70831855]), 'targetState': array([15, 24], dtype=int32), 'currentDistance': 1.2236255201889252}
episode index:664
target Thresh 31.96654010255779
target distance 24.0
model initialize at round 664
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.73174251, 17.49921127]), 'previousTarget': array([17.7430828 , 17.51449257]), 'currentState': array([2.99253075, 3.98049695, 4.57280394]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.20274819345153555
running average episode reward sum: 0.4039948952040783
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 26.]), 'previousTarget': array([27., 26.]), 'currentState': array([22.0366718 , 18.71924737,  0.88011861]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 8.811582478179716}
episode index:665
target Thresh 31.966873034100075
target distance 6.0
model initialize at round 665
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 27.]), 'previousTarget': array([ 8., 27.]), 'currentState': array([14.0266096 , 20.94494414,  4.91875431]), 'targetState': array([ 8, 27], dtype=int32), 'currentDistance': 8.543051257492367}
done in step count: 24
reward sum = 0.6232731045672796
running average episode reward sum: 0.4043241417646837
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 27.]), 'previousTarget': array([ 8., 27.]), 'currentState': array([ 8.4996508 , 26.2055071 ,  1.86808605]), 'targetState': array([ 8, 27], dtype=int32), 'currentDistance': 0.9385466901339244}
episode index:666
target Thresh 31.967202652918164
target distance 19.0
model initialize at round 666
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.16794445, 13.85194781]), 'previousTarget': array([18.90977806, 13.67985983]), 'currentState': array([2.22053546, 3.23190409, 0.87409946]), 'targetState': array([21, 15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 72
reward sum = 0.29067560680429827
running average episode reward sum: 0.40415375415604743
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 15.]), 'previousTarget': array([21., 15.]), 'currentState': array([20.2278868 , 14.18105036,  0.65237974]), 'targetState': array([21, 15], dtype=int32), 'currentDistance': 1.125538672485332}
episode index:667
target Thresh 31.967528991974216
target distance 11.0
model initialize at round 667
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([15.08138573, 19.69745963,  4.9398902 ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 10.751978316417004}
done in step count: 28
reward sum = 0.6213489953706736
running average episode reward sum: 0.40447889673271603
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.28988234,  9.85525165,  4.8628205 ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.9030432725180865}
episode index:668
target Thresh 31.967852083902404
target distance 18.0
model initialize at round 668
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.71285862, 25.48314552]), 'previousTarget': array([22.71285862, 25.48314552]), 'currentState': array([13.        ,  8.        ,  0.68914239]), 'targetState': array([23, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.34205743825265644
running average episode reward sum: 0.40438559111465916
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 26.]), 'previousTarget': array([23., 26.]), 'currentState': array([22.76863172, 25.02605345,  1.09547005]), 'targetState': array([23, 26], dtype=int32), 'currentDistance': 1.0010510247190807}
episode index:669
target Thresh 31.968171961012192
target distance 20.0
model initialize at round 669
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.99839706, 23.96594613]), 'previousTarget': array([11.99875234, 23.97504678]), 'currentState': array([11.05802284,  3.98806595,  0.02789008]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.3254682472002599
running average episode reward sum: 0.4042678040341899
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 24.]), 'previousTarget': array([12., 24.]), 'currentState': array([12.27121077, 23.12168269,  1.65523175]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 0.9192369549882395}
episode index:670
target Thresh 31.96848865529156
target distance 19.0
model initialize at round 670
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 13.]), 'previousTarget': array([23., 13.]), 'currentState': array([ 4.07442462, 18.0164393 ,  0.08733782]), 'targetState': array([23, 13], dtype=int32), 'currentDistance': 19.579123235321926}
done in step count: 63
reward sum = 0.35851591712669373
running average episode reward sum: 0.40419961940392535
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 13.]), 'previousTarget': array([23., 13.]), 'currentState': array([22.02954941, 13.15212002,  6.07355759]), 'targetState': array([23, 13], dtype=int32), 'currentDistance': 0.9823007947373943}
episode index:671
target Thresh 31.968802198410195
target distance 21.0
model initialize at round 671
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.89618185, 21.90990945]), 'previousTarget': array([24.89618185, 21.90990945]), 'currentState': array([23.       ,  2.       ,  5.2744171]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 71
reward sum = 0.26384387738193527
running average episode reward sum: 0.4039907566925831
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 23.]), 'previousTarget': array([25., 23.]), 'currentState': array([25.05158415, 22.01303521,  1.79948999]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 0.9883119070206469}
episode index:672
target Thresh 31.969112621722672
target distance 12.0
model initialize at round 672
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([10.        , 25.        ,  0.41001588]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 16.278820596099706}
done in step count: 50
reward sum = 0.43711863462153433
running average episode reward sum: 0.4040399808796989
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([21.20779434, 14.97225333,  5.68943936]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 1.2541396826523272}
episode index:673
target Thresh 31.969419956271587
target distance 18.0
model initialize at round 673
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  9.]), 'previousTarget': array([21.,  9.]), 'currentState': array([ 3.01629793, 17.00672662,  0.15508813]), 'targetState': array([21,  9], dtype=int32), 'currentDistance': 19.685558441778134}
done in step count: 62
reward sum = 0.3509394966402388
running average episode reward sum: 0.40396119677845344
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  9.]), 'previousTarget': array([21.,  9.]), 'currentState': array([20.09484236,  9.23705986,  5.87395163]), 'targetState': array([21,  9], dtype=int32), 'currentDistance': 0.9356857000819294}
episode index:674
target Thresh 31.969724232790643
target distance 5.0
model initialize at round 674
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 25.]), 'previousTarget': array([18., 25.]), 'currentState': array([14.        , 20.        ,  2.81213403]), 'targetState': array([18, 25], dtype=int32), 'currentDistance': 6.403124237432849}
done in step count: 17
reward sum = 0.7221068428758177
running average episode reward sum: 0.40443252366156063
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 25.]), 'previousTarget': array([18., 25.]), 'currentState': array([17.01490189, 24.48462223,  0.25329561]), 'targetState': array([18, 25], dtype=int32), 'currentDistance': 1.1117699971923507}
episode index:675
target Thresh 31.970025481707747
target distance 15.0
model initialize at round 675
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 24.]), 'previousTarget': array([24., 24.]), 'currentState': array([ 9.        , 16.        ,  4.35245144]), 'targetState': array([24, 24], dtype=int32), 'currentDistance': 17.0}
done in step count: 53
reward sum = 0.37286874144614557
running average episode reward sum: 0.40438583167603487
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 24.]), 'previousTarget': array([24., 24.]), 'currentState': array([23.11348348, 23.43504207,  0.28629138]), 'targetState': array([24, 24], dtype=int32), 'currentDistance': 1.0512321367287558}
episode index:676
target Thresh 31.970323733148042
target distance 20.0
model initialize at round 676
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([4.        , 2.        , 1.39077222]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.324531798179733
running average episode reward sum: 0.4042678788939133
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([2.31273690e+01, 1.75526711e+00, 7.62075884e-04]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 0.9062996461345888}
episode index:677
target Thresh 31.970619016936922
target distance 10.0
model initialize at round 677
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 7.]), 'previousTarget': array([9., 7.]), 'currentState': array([ 7.        , 17.        ,  1.01334155]), 'targetState': array([9, 7], dtype=int32), 'currentDistance': 10.198039027185569}
done in step count: 25
reward sum = 0.582260524035358
running average episode reward sum: 0.4045304049191957
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 7.]), 'previousTarget': array([9., 7.]), 'currentState': array([8.72795379, 7.78974004, 4.81734468]), 'targetState': array([9, 7], dtype=int32), 'currentDistance': 0.8352834667444262}
episode index:678
target Thresh 31.970911362603015
target distance 16.0
model initialize at round 678
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([10.        , 21.        ,  4.86421967]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 16.0}
done in step count: 45
reward sum = 0.4247128491866978
running average episode reward sum: 0.404560128695731
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([25.09219202, 20.57346456,  6.11459836]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 1.003019341952353}
episode index:679
target Thresh 31.971200799381123
target distance 17.0
model initialize at round 679
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([23.       , 16.       ,  4.3567667]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 18.384776310850235}
done in step count: 53
reward sum = 0.37446641452860685
running average episode reward sum: 0.40451587323372057
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.9809788 , 9.35555444, 3.12023328]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 1.0434262606109588}
episode index:680
target Thresh 31.971487356215174
target distance 5.0
model initialize at round 680
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 6.]), 'previousTarget': array([7., 6.]), 'currentState': array([11.82078547,  3.86009251,  3.62645727]), 'targetState': array([7, 6], dtype=int32), 'currentDistance': 5.274388740540365}
done in step count: 10
reward sum = 0.8141854486393278
running average episode reward sum: 0.40511744382902976
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 6.]), 'previousTarget': array([7., 6.]), 'currentState': array([7.82828688, 5.07275748, 2.3480667 ]), 'targetState': array([7, 6], dtype=int32), 'currentDistance': 1.243317278991461}
episode index:681
target Thresh 31.971771061761082
target distance 15.0
model initialize at round 681
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 20.]), 'previousTarget': array([24., 20.]), 'currentState': array([ 8.93888812, 21.10758594,  1.83487558]), 'targetState': array([24, 20], dtype=int32), 'currentDistance': 15.10178260148977}
done in step count: 41
reward sum = 0.4441704060144482
running average episode reward sum: 0.4051747062369263
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 20.]), 'previousTarget': array([24., 20.]), 'currentState': array([23.01479105, 20.06692393,  6.07630178]), 'targetState': array([24, 20], dtype=int32), 'currentDistance': 0.9874793645063976}
episode index:682
target Thresh 31.972051944389644
target distance 3.0
model initialize at round 682
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 21.]), 'previousTarget': array([10., 21.]), 'currentState': array([ 9.        , 18.        ,  4.97420235]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 3.162277660168379}
done in step count: 20
reward sum = 0.7394472025562905
running average episode reward sum: 0.4056641242403221
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 21.]), 'previousTarget': array([10., 21.]), 'currentState': array([ 9.56819738, 20.13034729,  0.84139001]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 0.9709528035206745}
episode index:683
target Thresh 31.972330032189355
target distance 7.0
model initialize at round 683
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  4.]), 'previousTarget': array([27.,  4.]), 'currentState': array([20.21719995,  2.27408194,  0.8163996 ]), 'targetState': array([27,  4], dtype=int32), 'currentDistance': 6.998940615444061}
done in step count: 15
reward sum = 0.7434620104843276
running average episode reward sum: 0.4061579807991584
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  4.]), 'previousTarget': array([27.,  4.]), 'currentState': array([26.12272208,  3.94998794,  5.86743055]), 'targetState': array([27,  4], dtype=int32), 'currentDistance': 0.8787023151898682}
episode index:684
target Thresh 31.972605352969225
target distance 17.0
model initialize at round 684
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 6.]), 'previousTarget': array([9., 6.]), 'currentState': array([26.04684054, 12.91017626,  4.94056606]), 'targetState': array([9, 6], dtype=int32), 'currentDistance': 18.3941650649766}
done in step count: 51
reward sum = 0.36901360157375035
running average episode reward sum: 0.4061037554280264
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 6.]), 'previousTarget': array([9., 6.]), 'currentState': array([9.97827203, 6.46698658, 3.21030191]), 'targetState': array([9, 6], dtype=int32), 'currentDistance': 1.0840169008073761}
episode index:685
target Thresh 31.972877934261565
target distance 24.0
model initialize at round 685
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.5900478 , 11.33837977]), 'previousTarget': array([11.58594111, 11.31908351]), 'currentState': array([23.99118165, 27.02951992,  2.08659476]), 'targetState': array([5, 3], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.21578183431467685
running average episode reward sum: 0.4051972166674685
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 3.]), 'previousTarget': array([5., 3.]), 'currentState': array([6.97153516, 7.33635049, 4.25706864]), 'targetState': array([5, 3], dtype=int32), 'currentDistance': 4.763495199259767}
episode index:686
target Thresh 31.97314780332473
target distance 4.0
model initialize at round 686
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 24.]), 'previousTarget': array([11., 24.]), 'currentState': array([12.16257114, 27.85262108,  5.35780737]), 'targetState': array([11, 24], dtype=int32), 'currentDistance': 4.024209348647777}
done in step count: 10
reward sum = 0.8447496111216568
running average episode reward sum: 0.40583703092431594
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 24.]), 'previousTarget': array([11., 24.]), 'currentState': array([11.61057462, 24.85473827,  3.87769259]), 'targetState': array([11, 24], dtype=int32), 'currentDistance': 1.0504184280556743}
episode index:687
target Thresh 31.973414987145848
target distance 14.0
model initialize at round 687
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 26.]), 'previousTarget': array([12., 26.]), 'currentState': array([ 2.98184739, 12.00653348,  2.54360819]), 'targetState': array([12, 26], dtype=int32), 'currentDistance': 16.647647932778874}
done in step count: 42
reward sum = 0.40635279040399275
running average episode reward sum: 0.4058377805747225
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 26.]), 'previousTarget': array([12., 26.]), 'currentState': array([11.01338239, 25.5816331 ,  1.14654991]), 'targetState': array([12, 26], dtype=int32), 'currentDistance': 1.071655339714556}
episode index:688
target Thresh 31.97367951244353
target distance 11.0
model initialize at round 688
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([10.95075901,  6.01084287,  2.6723516 ]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 11.226863880831354}
done in step count: 32
reward sum = 0.5487110328744216
running average episode reward sum: 0.40604514378560735
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([21.00155477,  7.83676905,  0.2156948 ]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 1.0117001670477772}
episode index:689
target Thresh 31.973941405670516
target distance 6.0
model initialize at round 689
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 15.]), 'previousTarget': array([ 7., 15.]), 'currentState': array([12.        , 21.        ,  5.97523623]), 'targetState': array([ 7, 15], dtype=int32), 'currentDistance': 7.810249675906653}
done in step count: 23
reward sum = 0.6599166821427858
running average episode reward sum: 0.4064130735513424
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 15.]), 'previousTarget': array([ 7., 15.]), 'currentState': array([ 7.92856657, 15.05976371,  3.80985742]), 'targetState': array([ 7, 15], dtype=int32), 'currentDistance': 0.9304878149721284}
episode index:690
target Thresh 31.974200693016357
target distance 9.0
model initialize at round 690
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  6.]), 'previousTarget': array([20.,  6.]), 'currentState': array([11.27774253,  8.12994297,  0.31588691]), 'targetState': array([20,  6], dtype=int32), 'currentDistance': 8.978554030418476}
done in step count: 19
reward sum = 0.6812403263291429
running average episode reward sum: 0.40681079750615834
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  6.]), 'previousTarget': array([20.,  6.]), 'currentState': array([19.05705274,  6.70800653,  5.98142486]), 'targetState': array([20,  6], dtype=int32), 'currentDistance': 1.1791619005722864}
episode index:691
target Thresh 31.974457400410003
target distance 12.0
model initialize at round 691
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 15.]), 'previousTarget': array([22., 15.]), 'currentState': array([13.        ,  3.        ,  0.10538262]), 'targetState': array([22, 15], dtype=int32), 'currentDistance': 15.0}
done in step count: 38
reward sum = 0.4823206519453403
running average episode reward sum: 0.4069199157929201
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 15.]), 'previousTarget': array([22., 15.]), 'currentState': array([21.1084491 , 14.26232455,  1.06666338]), 'targetState': array([22, 15], dtype=int32), 'currentDistance': 1.1571638056376565}
episode index:692
target Thresh 31.9747115535224
target distance 16.0
model initialize at round 692
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 29.]), 'previousTarget': array([ 4., 29.]), 'currentState': array([ 5.79681309, 13.26640116,  2.12716663]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 15.83586655679603}
done in step count: 41
reward sum = 0.4680069187201017
running average episode reward sum: 0.40700806442629267
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 29.]), 'previousTarget': array([ 4., 29.]), 'currentState': array([ 4.40571805, 28.31287395,  1.68690117]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 0.7979657588091174}
episode index:693
target Thresh 31.974963177769077
target distance 17.0
model initialize at round 693
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([12.07326362, 28.04269912,  0.27886856]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 18.452226194094457}
done in step count: 53
reward sum = 0.34297641556936875
running average episode reward sum: 0.4069157998025795
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.11361414, 11.97160422,  4.29464156]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.9782243747867448}
episode index:694
target Thresh 31.97521229831267
target distance 8.0
model initialize at round 694
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.14963174, 18.57536998,  5.00183389]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 7.576847627053196}
done in step count: 17
reward sum = 0.7249766044672231
running average episode reward sum: 0.4073734412481401
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.93932153, 11.83353298,  4.44591445]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.8357386561561131}
episode index:695
target Thresh 31.975458940065433
target distance 9.0
model initialize at round 695
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 24.]), 'previousTarget': array([11., 24.]), 'currentState': array([20.        , 29.        ,  5.11995649]), 'targetState': array([11, 24], dtype=int32), 'currentDistance': 10.295630140986999}
done in step count: 24
reward sum = 0.6106652476316428
running average episode reward sum: 0.407665527176852
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 24.]), 'previousTarget': array([11., 24.]), 'currentState': array([11.84483303, 24.39487481,  3.32020864]), 'targetState': array([11, 24], dtype=int32), 'currentDistance': 0.9325604326886338}
episode index:696
target Thresh 31.975703127691755
target distance 20.0
model initialize at round 696
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.99007438, 22.9007438 ]), 'previousTarget': array([21.99007438, 22.9007438 ]), 'currentState': array([20.        ,  3.        ,  3.91167879]), 'targetState': array([22, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.2861078960291382
running average episode reward sum: 0.4074911259843876
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 23.]), 'previousTarget': array([22., 23.]), 'currentState': array([22.25406765, 22.16921366,  1.79626123]), 'targetState': array([22, 23], dtype=int32), 'currentDistance': 0.868767119906409}
episode index:697
target Thresh 31.975944885610595
target distance 17.0
model initialize at round 697
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 16.]), 'previousTarget': array([27., 16.]), 'currentState': array([10.       , 11.       ,  5.2610631]), 'targetState': array([27, 16], dtype=int32), 'currentDistance': 17.720045146669353}
done in step count: 47
reward sum = 0.3864115486447258
running average episode reward sum: 0.40746092601685224
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 16.]), 'previousTarget': array([27., 16.]), 'currentState': array([26.03503885, 15.64891449,  0.24162813]), 'targetState': array([27, 16], dtype=int32), 'currentDistance': 1.0268451987067124}
episode index:698
target Thresh 31.976184237997952
target distance 19.0
model initialize at round 698
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 27.]), 'previousTarget': array([ 4., 27.]), 'currentState': array([2.        , 8.        , 4.15469763]), 'targetState': array([ 4, 27], dtype=int32), 'currentDistance': 19.1049731745428}
done in step count: 57
reward sum = 0.3195491533315762
running average episode reward sum: 0.40733515810170884
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 27.]), 'previousTarget': array([ 4., 27.]), 'currentState': array([ 3.55351223, 26.17743155,  1.20533141]), 'targetState': array([ 4, 27], dtype=int32), 'currentDistance': 0.9359327892441396}
episode index:699
target Thresh 31.976421208789265
target distance 10.0
model initialize at round 699
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 19.]), 'previousTarget': array([26., 19.]), 'currentState': array([15.97799632,  9.38073202,  1.4315283 ]), 'targetState': array([26, 19], dtype=int32), 'currentDistance': 13.891395692627222}
done in step count: 35
reward sum = 0.5157802323463789
running average episode reward sum: 0.4074900796363441
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 19.]), 'previousTarget': array([26., 19.]), 'currentState': array([25.42003977, 18.13313536,  0.81131359]), 'targetState': array([26, 19], dtype=int32), 'currentDistance': 1.0429804319820808}
episode index:700
target Thresh 31.976655821681803
target distance 8.0
model initialize at round 700
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 16.]), 'previousTarget': array([ 9., 16.]), 'currentState': array([11.99962041, 24.00089668,  2.22374606]), 'targetState': array([ 9, 16], dtype=int32), 'currentDistance': 8.544710080105148}
done in step count: 20
reward sum = 0.6390312105520652
running average episode reward sum: 0.4078203808216732
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 16.]), 'previousTarget': array([ 9., 16.]), 'currentState': array([ 8.95991127, 16.94889772,  4.68458775]), 'targetState': array([ 9, 16], dtype=int32), 'currentDistance': 0.9497441682081224}
episode index:701
target Thresh 31.97688810013706
target distance 18.0
model initialize at round 701
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 29.]), 'previousTarget': array([ 5., 29.]), 'currentState': array([23.01043527, 29.00451835,  0.66111778]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 18.01043583285746}
done in step count: 50
reward sum = 0.3475083274634274
running average episode reward sum: 0.40773446621574977
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 29.]), 'previousTarget': array([ 5., 29.]), 'currentState': array([ 5.76912394, 29.17930358,  3.19350652]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 0.7897476831114241}
episode index:702
target Thresh 31.977118067383067
target distance 14.0
model initialize at round 702
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  4.]), 'previousTarget': array([26.,  4.]), 'currentState': array([11.94285907, 16.97243372,  3.83581042]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 19.128179411130752}
done in step count: 50
reward sum = 0.35536761877324663
running average episode reward sum: 0.40765997567884715
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  4.]), 'previousTarget': array([26.,  4.]), 'currentState': array([25.14922981,  4.95991505,  5.73333158]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 1.2826717542858497}
episode index:703
target Thresh 31.97734574641675
target distance 9.0
model initialize at round 703
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([16.97236623, 23.01184079,  2.97447932]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 12.716785372672177}
done in step count: 27
reward sum = 0.5518762484886083
running average episode reward sum: 0.4078648283390883
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([ 8.29880413, 14.89132218,  3.87927086]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 0.9400740088565724}
episode index:704
target Thresh 31.97757116000619
target distance 4.0
model initialize at round 704
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([11.17665897, 10.74019539,  5.1052452 ]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 3.920916712521725}
done in step count: 8
reward sum = 0.8599634882079464
running average episode reward sum: 0.40850610303393775
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([10.59060556,  7.76649895,  4.2080791 ]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 0.9676443399235043}
episode index:705
target Thresh 31.97779433069294
target distance 26.0
model initialize at round 705
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.6152431 , 10.18964118]), 'previousTarget': array([11.48199646, 10.390578  ]), 'currentState': array([ 2.17508238, 27.82152609,  5.48333436]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 94
reward sum = 0.16139716108902102
running average episode reward sum: 0.4081560903682934
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.30005284,  2.94062021,  5.37948937]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.1724727757400304}
episode index:706
target Thresh 31.978015280794256
target distance 11.0
model initialize at round 706
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  4.]), 'previousTarget': array([23.,  4.]), 'currentState': array([12.00437473, 12.92949376,  4.9619092 ]), 'targetState': array([23,  4], dtype=int32), 'currentDistance': 14.16473204889386}
done in step count: 36
reward sum = 0.5096077175406531
running average episode reward sum: 0.40829958630488794
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  4.]), 'previousTarget': array([23.,  4.]), 'currentState': array([22.1281192 ,  4.19178402,  5.81601669]), 'targetState': array([23,  4], dtype=int32), 'currentDistance': 0.8927246163225185}
episode index:707
target Thresh 31.978234032405332
target distance 17.0
model initialize at round 707
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 17.]), 'previousTarget': array([ 6., 17.]), 'currentState': array([23.        , 19.        ,  4.63043976]), 'targetState': array([ 6, 17], dtype=int32), 'currentDistance': 17.11724276862369}
done in step count: 44
reward sum = 0.3903192416251175
running average episode reward sum: 0.4082741903378261
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 17.]), 'previousTarget': array([ 6., 17.]), 'currentState': array([ 6.88447134, 17.01312846,  2.87782971]), 'targetState': array([ 6, 17], dtype=int32), 'currentDistance': 0.8845687711573393}
episode index:708
target Thresh 31.97845060740151
target distance 12.0
model initialize at round 708
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 20.]), 'previousTarget': array([ 5., 20.]), 'currentState': array([11.        ,  8.        ,  5.46105227]), 'targetState': array([ 5, 20], dtype=int32), 'currentDistance': 13.416407864998739}
done in step count: 34
reward sum = 0.48001182281235316
running average episode reward sum: 0.40837537176585786
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 20.]), 'previousTarget': array([ 5., 20.]), 'currentState': array([ 5.56132578, 19.14664574,  2.20387235]), 'targetState': array([ 5, 20], dtype=int32), 'currentDistance': 1.0214206401270636}
episode index:709
target Thresh 31.978665027440467
target distance 1.0
model initialize at round 709
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 24.]), 'previousTarget': array([ 7., 24.]), 'currentState': array([ 6.98715183, 23.2264527 ,  1.64654007]), 'targetState': array([ 7, 24], dtype=int32), 'currentDistance': 0.7736539905740376}
done in step count: 0
reward sum = 0.997884892289489
running average episode reward sum: 0.40920566686518695
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 24.]), 'previousTarget': array([ 7., 24.]), 'currentState': array([ 6.98715183, 23.2264527 ,  1.64654007]), 'targetState': array([ 7, 24], dtype=int32), 'currentDistance': 0.7736539905740376}
episode index:710
target Thresh 31.978877313964393
target distance 12.0
model initialize at round 710
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 12.]), 'previousTarget': array([16., 12.]), 'currentState': array([ 4.        , 12.        ,  5.05206919]), 'targetState': array([16, 12], dtype=int32), 'currentDistance': 12.0}
done in step count: 28
reward sum = 0.5589064092552491
running average episode reward sum: 0.4094162164325429
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 12.]), 'previousTarget': array([16., 12.]), 'currentState': array([15.14408182, 11.77320805,  0.24483746]), 'targetState': array([16, 12], dtype=int32), 'currentDistance': 0.885454977703321}
episode index:711
target Thresh 31.97908748820211
target distance 8.0
model initialize at round 711
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 15.]), 'previousTarget': array([ 6., 15.]), 'currentState': array([ 2.        , 23.        ,  3.28993282]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 8.944271909999157}
done in step count: 22
reward sum = 0.6438373293535761
running average episode reward sum: 0.4097454595686679
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 15.]), 'previousTarget': array([ 6., 15.]), 'currentState': array([ 5.32703181, 15.8483075 ,  5.31386666]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 1.0828258338551704}
episode index:712
target Thresh 31.97929557117122
target distance 15.0
model initialize at round 712
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 17.]), 'previousTarget': array([15., 17.]), 'currentState': array([2.9644672 , 2.07167319, 1.77910143]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 19.17573964963757}
done in step count: 51
reward sum = 0.36396460805148634
running average episode reward sum: 0.4096812508007617
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 17.]), 'previousTarget': array([15., 17.]), 'currentState': array([14.5651738 , 16.09947139,  1.27710884]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 1.0000128032920481}
episode index:713
target Thresh 31.979501583680197
target distance 14.0
model initialize at round 713
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 20.]), 'previousTarget': array([21., 20.]), 'currentState': array([25.0592438 ,  5.99959225,  0.24561756]), 'targetState': array([21, 20], dtype=int32), 'currentDistance': 14.576998227042445}
done in step count: 42
reward sum = 0.4434557793022933
running average episode reward sum: 0.40972855406196823
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 20.]), 'previousTarget': array([21., 20.]), 'currentState': array([20.9786414 , 19.04831515,  1.82729428]), 'targetState': array([21, 20], dtype=int32), 'currentDistance': 0.9519244924659922}
episode index:714
target Thresh 31.979705546330457
target distance 14.0
model initialize at round 714
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  5.]), 'previousTarget': array([17.,  5.]), 'currentState': array([17.31920701, 18.78130972,  5.4488284 ]), 'targetState': array([17,  5], dtype=int32), 'currentDistance': 13.785006013856126}
done in step count: 38
reward sum = 0.5010567657484298
running average episode reward sum: 0.40985628582656475
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  5.]), 'previousTarget': array([17.,  5.]), 'currentState': array([16.93325618,  5.98156425,  4.817907  ]), 'targetState': array([17,  5], dtype=int32), 'currentDistance': 0.9838308379360048}
episode index:715
target Thresh 31.97990747951844
target distance 5.0
model initialize at round 715
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 28.]), 'previousTarget': array([ 8., 28.]), 'currentState': array([12.9973917 , 29.02485766,  1.92784309]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 5.1013975581575925}
done in step count: 12
reward sum = 0.798017959363581
running average episode reward sum: 0.4103984110689348
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 28.]), 'previousTarget': array([ 8., 28.]), 'currentState': array([ 8.95419978, 28.95364386,  3.57388417]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 1.3490492316639389}
episode index:716
target Thresh 31.98010740343763
target distance 21.0
model initialize at round 716
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.97366596,  8.32455532]), 'previousTarget': array([22.97366596,  8.32455532]), 'currentState': array([4.        , 2.        , 2.49145126]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.25484707084352104
running average episode reward sum: 0.4101814635930277
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([24.07324815,  8.82335613,  0.41924117]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 0.9434362981129316}
episode index:717
target Thresh 31.980305338080587
target distance 17.0
model initialize at round 717
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 20.]), 'previousTarget': array([ 8., 20.]), 'currentState': array([15.02592971,  3.04152138,  1.21652061]), 'targetState': array([ 8, 20], dtype=int32), 'currentDistance': 18.35629824456347}
done in step count: 49
reward sum = 0.3871724632249745
running average episode reward sum: 0.41014941763151236
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 20.]), 'previousTarget': array([ 8., 20.]), 'currentState': array([ 8.279597  , 19.08982398,  2.33280408]), 'targetState': array([ 8, 20], dtype=int32), 'currentDistance': 0.952152757816883}
episode index:718
target Thresh 31.98050130324094
target distance 7.0
model initialize at round 718
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 22.]), 'previousTarget': array([ 3., 22.]), 'currentState': array([ 8.        , 15.        ,  4.63572252]), 'targetState': array([ 3, 22], dtype=int32), 'currentDistance': 8.602325267042627}
done in step count: 22
reward sum = 0.6199320086017166
running average episode reward sum: 0.4104411875772289
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 22.]), 'previousTarget': array([ 3., 22.]), 'currentState': array([ 3.11289574, 21.10928737,  1.64365858]), 'targetState': array([ 3, 22], dtype=int32), 'currentDistance': 0.8978387578480469}
episode index:719
target Thresh 31.980695318515366
target distance 10.0
model initialize at round 719
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 20.]), 'previousTarget': array([16., 20.]), 'currentState': array([26.02619502, 21.99557892,  5.86348531]), 'targetState': array([16, 20], dtype=int32), 'currentDistance': 10.222862702120146}
done in step count: 27
reward sum = 0.5573313771710666
running average episode reward sum: 0.4106452017294426
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 20.]), 'previousTarget': array([16., 20.]), 'currentState': array([16.8440407 , 19.71600459,  2.75757817]), 'targetState': array([16, 20], dtype=int32), 'currentDistance': 0.8905380970440482}
episode index:720
target Thresh 31.980887403305555
target distance 4.0
model initialize at round 720
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 16.]), 'previousTarget': array([27., 16.]), 'currentState': array([23.01160195, 17.01945496,  0.801094  ]), 'targetState': array([27, 16], dtype=int32), 'currentDistance': 4.116625736312756}
done in step count: 10
reward sum = 0.8454544212073799
running average episode reward sum: 0.41124826583412766
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 16.]), 'previousTarget': array([27., 16.]), 'currentState': array([26.01038033, 16.70788449,  5.82461054]), 'targetState': array([27, 16], dtype=int32), 'currentDistance': 1.216736430343012}
episode index:721
target Thresh 31.98107757682015
target distance 8.0
model initialize at round 721
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 19.]), 'previousTarget': array([14., 19.]), 'currentState': array([22.        , 25.        ,  2.49878561]), 'targetState': array([14, 19], dtype=int32), 'currentDistance': 10.0}
done in step count: 21
reward sum = 0.636104955966905
running average episode reward sum: 0.4115597016930373
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 19.]), 'previousTarget': array([14., 19.]), 'currentState': array([14.96440502, 19.87171309,  3.81210715]), 'targetState': array([14, 19], dtype=int32), 'currentDistance': 1.2999849085291353}
episode index:722
target Thresh 31.98126585807666
target distance 8.0
model initialize at round 722
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 23.]), 'previousTarget': array([17., 23.]), 'currentState': array([24.61875095, 19.13400617,  2.74141113]), 'targetState': array([17, 23], dtype=int32), 'currentDistance': 8.54349309669948}
done in step count: 18
reward sum = 0.6914429640299611
running average episode reward sum: 0.41194681547220324
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 23.]), 'previousTarget': array([17., 23.]), 'currentState': array([17.91633464, 23.39187974,  2.72280687]), 'targetState': array([17, 23], dtype=int32), 'currentDistance': 0.9966137151757106}
episode index:723
target Thresh 31.981452265903364
target distance 15.0
model initialize at round 723
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 25.]), 'previousTarget': array([12., 25.]), 'currentState': array([26.99645111, 26.04082321,  1.89827505]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 15.032526695718278}
done in step count: 33
reward sum = 0.46365443494945535
running average episode reward sum: 0.41201823483612204
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 25.]), 'previousTarget': array([12., 25.]), 'currentState': array([12.84012475, 24.89424553,  3.54909743]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 0.8467547440694091}
episode index:724
target Thresh 31.9816368189412
target distance 21.0
model initialize at round 724
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.3687563 ,  2.02682333]), 'previousTarget': array([23.97736275,  2.04869701]), 'currentState': array([4.38678829, 2.8759136 , 5.94319651]), 'targetState': array([25,  2], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 46
reward sum = 0.33644174597646204
running average episode reward sum: 0.41191399140321217
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  2.]), 'previousTarget': array([25.,  2.]), 'currentState': array([24.07168542,  1.89219662,  0.31147742]), 'targetState': array([25,  2], dtype=int32), 'currentDistance': 0.9345531145372821}
episode index:725
target Thresh 31.98181953564563
target distance 26.0
model initialize at round 725
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.19966821, 18.14782245]), 'previousTarget': array([11.19966821, 18.14782245]), 'currentState': array([23.        ,  2.        ,  0.83268875]), 'targetState': array([ 4, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.26839502435221874
running average episode reward sum: 0.41097692664321844
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 28.]), 'previousTarget': array([ 4., 28.]), 'currentState': array([ 7.87825751, 26.23232615,  2.69919391]), 'targetState': array([ 4, 28], dtype=int32), 'currentDistance': 4.262106533904656}
episode index:726
target Thresh 31.982000434288473
target distance 12.0
model initialize at round 726
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 13.]), 'previousTarget': array([18., 13.]), 'currentState': array([ 6.42901866, 14.74823449,  5.92979512]), 'targetState': array([18, 13], dtype=int32), 'currentDistance': 11.702304605832797}
done in step count: 25
reward sum = 0.5852296388086531
running average episode reward sum: 0.41121661400520665
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 13.]), 'previousTarget': array([18., 13.]), 'currentState': array([17.06165229, 12.75097927,  6.23372065]), 'targetState': array([18, 13], dtype=int32), 'currentDistance': 0.9708283883412105}
episode index:727
target Thresh 31.982179532959748
target distance 6.0
model initialize at round 727
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 18.]), 'previousTarget': array([ 9., 18.]), 'currentState': array([11.        , 12.        ,  4.00868511]), 'targetState': array([ 9, 18], dtype=int32), 'currentDistance': 6.324555320336759}
done in step count: 20
reward sum = 0.7045529405331793
running average episode reward sum: 0.41161954851966814
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 18.]), 'previousTarget': array([ 9., 18.]), 'currentState': array([ 9.19530487, 17.0226831 ,  2.00713269]), 'targetState': array([ 9, 18], dtype=int32), 'currentDistance': 0.9966405183093091}
episode index:728
target Thresh 31.982356849569463
target distance 10.0
model initialize at round 728
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([18.        , 11.        ,  4.41402155]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 11.661903789690601}
done in step count: 31
reward sum = 0.5477800044446313
running average episode reward sum: 0.4118063255511153
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([ 8.99394769, 16.10444521,  2.48678897]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 1.337890279216041}
episode index:729
target Thresh 31.982532401849436
target distance 12.0
model initialize at round 729
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 26.]), 'previousTarget': array([18., 26.]), 'currentState': array([10.        , 14.        ,  6.02114105]), 'targetState': array([18, 26], dtype=int32), 'currentDistance': 14.422205101855956}
done in step count: 34
reward sum = 0.4760603299951705
running average episode reward sum: 0.41189434473528524
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 26.]), 'previousTarget': array([18., 26.]), 'currentState': array([17.82116376, 25.08422383,  1.2778135 ]), 'targetState': array([18, 26], dtype=int32), 'currentDistance': 0.9330746991783052}
episode index:730
target Thresh 31.982706207355037
target distance 5.0
model initialize at round 730
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([18.       ,  6.       ,  0.2790102]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 7.071067811865475}
done in step count: 17
reward sum = 0.713446870196927
running average episode reward sum: 0.4123068652899523
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.98721534, 10.12291496,  2.7406583 ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.3205575687616338}
episode index:731
target Thresh 31.982878283466963
target distance 15.0
model initialize at round 731
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([24.00561861,  5.99302069,  5.14927527]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 15.30393787580574}
done in step count: 37
reward sum = 0.4253640161942391
running average episode reward sum: 0.4123247029277997
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([9.84288871, 8.96032623, 2.99478634]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.8438218890845338}
episode index:732
target Thresh 31.983048647392966
target distance 8.0
model initialize at round 732
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 20.]), 'previousTarget': array([ 5., 20.]), 'currentState': array([ 5.9553113 , 28.01455435,  3.06992799]), 'targetState': array([ 5, 20], dtype=int32), 'currentDistance': 8.071288693458097}
done in step count: 26
reward sum = 0.6366306011912826
running average episode reward sum: 0.4126307137030568
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 20.]), 'previousTarget': array([ 5., 20.]), 'currentState': array([ 4.88498352, 20.9871365 ,  4.91560513]), 'targetState': array([ 5, 20], dtype=int32), 'currentDistance': 0.9938145018142966}
episode index:733
target Thresh 31.983217316169583
target distance 16.0
model initialize at round 733
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  9.]), 'previousTarget': array([18.,  9.]), 'currentState': array([14.05554924, 24.94976818,  5.46199125]), 'targetState': array([18,  9], dtype=int32), 'currentDistance': 16.43027074679539}
done in step count: 41
reward sum = 0.4432191583867998
running average episode reward sum: 0.41267238733341616
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  9.]), 'previousTarget': array([18.,  9.]), 'currentState': array([17.54775777,  9.96261766,  5.33676312]), 'targetState': array([18,  9], dtype=int32), 'currentDistance': 1.063558078712274}
episode index:734
target Thresh 31.98338430666383
target distance 7.0
model initialize at round 734
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 16.]), 'previousTarget': array([ 5., 16.]), 'currentState': array([12.        , 15.        ,  1.00794381]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 7.071067811865476}
done in step count: 18
reward sum = 0.6897689485735119
running average episode reward sum: 0.41304938945755226
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 16.]), 'previousTarget': array([ 5., 16.]), 'currentState': array([ 5.84740399, 16.0510775 ,  2.93930262]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 0.8489419456929674}
episode index:735
target Thresh 31.9835496355749
target distance 5.0
model initialize at round 735
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 25.]), 'previousTarget': array([18., 25.]), 'currentState': array([12.9847495 , 23.010684  ,  2.27798629]), 'targetState': array([18, 25], dtype=int32), 'currentDistance': 5.395379110472911}
done in step count: 14
reward sum = 0.7569992866057146
running average episode reward sum: 0.41351671268737317
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 25.]), 'previousTarget': array([18., 25.]), 'currentState': array([17.10127167, 25.03308387,  0.10505469]), 'targetState': array([18, 25], dtype=int32), 'currentDistance': 0.899337061430997}
episode index:736
target Thresh 31.983713319435815
target distance 18.0
model initialize at round 736
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  5.]), 'previousTarget': array([19.,  5.]), 'currentState': array([22.        , 23.        ,  6.27852837]), 'targetState': array([19,  5], dtype=int32), 'currentDistance': 18.248287590894655}
done in step count: 47
reward sum = 0.3516387992820985
running average episode reward sum: 0.4134327535104325
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  5.]), 'previousTarget': array([19.,  5.]), 'currentState': array([19.90396035,  5.79472079,  4.61262386]), 'targetState': array([19,  5], dtype=int32), 'currentDistance': 1.2036301130412388}
episode index:737
target Thresh 31.983875374615103
target distance 18.0
model initialize at round 737
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 24.]), 'previousTarget': array([ 3., 24.]), 'currentState': array([21.00352147, 26.98197996,  4.69314834]), 'targetState': array([ 3, 24], dtype=int32), 'currentDistance': 18.24880790301142}
done in step count: 49
reward sum = 0.36093415442987786
running average episode reward sum: 0.4133616171973152
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 24.]), 'previousTarget': array([ 3., 24.]), 'currentState': array([ 3.93483938, 24.07441798,  3.67476762]), 'targetState': array([ 3, 24], dtype=int32), 'currentDistance': 0.9377967305834003}
episode index:738
target Thresh 31.984035817318414
target distance 14.0
model initialize at round 738
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  2.]), 'previousTarget': array([17.,  2.]), 'currentState': array([ 6.19510611, 15.67499763,  5.23820721]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 17.428462128504787}
done in step count: 38
reward sum = 0.43492538455258967
running average episode reward sum: 0.4133907968554414
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  2.]), 'previousTarget': array([17.,  2.]), 'currentState': array([16.43251203,  2.87880075,  5.50853431]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 1.0461038932472837}
episode index:739
target Thresh 31.984194663590156
target distance 9.0
model initialize at round 739
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([ 4.97673379, 15.00577065,  2.67370948]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 12.05516636367391}
done in step count: 31
reward sum = 0.5326898152442033
running average episode reward sum: 0.413552011745156
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([13.01791313, 22.69929846,  0.19684019]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 1.0270910539017373}
episode index:740
target Thresh 31.984351929315082
target distance 13.0
model initialize at round 740
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([12.        , 15.        ,  0.26873368]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 15.811388300841898}
done in step count: 45
reward sum = 0.3916901482389443
running average episode reward sum: 0.4135225085555389
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.93242591, 2.5376015 , 3.9649715 ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 1.0763054602452917}
episode index:741
target Thresh 31.984507630219902
target distance 14.0
model initialize at round 741
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 27.]), 'previousTarget': array([13., 27.]), 'currentState': array([ 4.24442029, 13.23186049,  0.84598288]), 'targetState': array([13, 27], dtype=int32), 'currentDistance': 16.316306005912534}
done in step count: 37
reward sum = 0.4641370568089961
running average episode reward sum: 0.41359072223243043
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 27.]), 'previousTarget': array([13., 27.]), 'currentState': array([12.69717043, 26.11378828,  1.15783733]), 'targetState': array([13, 27], dtype=int32), 'currentDistance': 0.93652386623429}
episode index:742
target Thresh 31.984661781874834
target distance 24.0
model initialize at round 742
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.59642493, 23.20310332]), 'previousTarget': array([24.6, 23.2]), 'currentState': array([18.97990463,  4.00792945,  2.51325798]), 'targetState': array([26, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.20908955994132455
running average episode reward sum: 0.4133154851364801
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 28.]), 'previousTarget': array([26., 28.]), 'currentState': array([25.96271598, 27.0723728 ,  1.44543185]), 'targetState': array([26, 28], dtype=int32), 'currentDistance': 0.9283761773024636}
episode index:743
target Thresh 31.98481439969517
target distance 8.0
model initialize at round 743
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'previousTarget': array([10., 17.]), 'currentState': array([ 1.91954262, 14.01498589,  2.70494413]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 8.614180213353077}
done in step count: 22
reward sum = 0.633343713181111
running average episode reward sum: 0.4136112220021314
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'previousTarget': array([10., 17.]), 'currentState': array([ 9.09139101, 16.65063988,  0.2965429 ]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 0.9734591897664864}
episode index:744
target Thresh 31.984965498942824
target distance 19.0
model initialize at round 744
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.29367831, 10.49385478]), 'previousTarget': array([17.29367831, 10.49385478]), 'currentState': array([ 6.       , 27.       ,  3.0766573]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 68
reward sum = 0.24094726593113863
running average episode reward sum: 0.4133794583027073
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  8.]), 'previousTarget': array([19.,  8.]), 'currentState': array([18.09370422,  8.76975741,  5.5142089 ]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 1.1890746438044775}
episode index:745
target Thresh 31.98511509472784
target distance 13.0
model initialize at round 745
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 23.]), 'previousTarget': array([13., 23.]), 'currentState': array([26.0149223 , 29.05774515,  1.54623261]), 'targetState': array([13, 23], dtype=int32), 'currentDistance': 14.355642748784634}
done in step count: 32
reward sum = 0.4607951413197778
running average episode reward sum: 0.41344301819951296
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 23.]), 'previousTarget': array([13., 23.]), 'currentState': array([13.97270958, 23.43387979,  3.79431768]), 'targetState': array([13, 23], dtype=int32), 'currentDistance': 1.0650894781446067}
episode index:746
target Thresh 31.985263202009925
target distance 12.0
model initialize at round 746
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 13.]), 'previousTarget': array([ 8., 13.]), 'currentState': array([19.66852695, 17.82473925,  3.60132706]), 'targetState': array([ 8, 13], dtype=int32), 'currentDistance': 12.62666345968047}
done in step count: 26
reward sum = 0.5807603418450485
running average episode reward sum: 0.41366700390720446
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 13.]), 'previousTarget': array([ 8., 13.]), 'currentState': array([ 8.9788051 , 13.6060882 ,  3.57782072]), 'targetState': array([ 8, 13], dtype=int32), 'currentDistance': 1.151261192346721}
episode index:747
target Thresh 31.98540983559993
target distance 9.0
model initialize at round 747
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 17.]), 'previousTarget': array([18., 17.]), 'currentState': array([26.        ,  8.        ,  0.49463433]), 'targetState': array([18, 17], dtype=int32), 'currentDistance': 12.041594578792294}
done in step count: 25
reward sum = 0.5521014838415155
running average episode reward sum: 0.4138520767413412
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 17.]), 'previousTarget': array([18., 17.]), 'currentState': array([18.89573308, 16.08668364,  2.44412199]), 'targetState': array([18, 17], dtype=int32), 'currentDistance': 1.2792515450932762}
episode index:748
target Thresh 31.985555010161338
target distance 18.0
model initialize at round 748
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.36442559, 12.19631201]), 'previousTarget': array([21.36442559, 12.19631201]), 'currentState': array([ 6.        , 25.        ,  3.34614801]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 75
reward sum = 0.21170736068859197
running average episode reward sum: 0.4135821906050893
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([23.02778656, 10.71591345,  5.7931548 ]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 1.2073653313507648}
episode index:749
target Thresh 31.985698740211724
target distance 22.0
model initialize at round 749
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8.12062748, 22.26317437]), 'previousTarget': array([ 8.17429997, 22.22895002]), 'currentState': array([23.93725875, 10.0224973 ,  2.67439532]), 'targetState': array([ 2, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 93
reward sum = 0.1645364296903868
running average episode reward sum: 0.4132501295905363
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 27.]), 'previousTarget': array([ 2., 27.]), 'currentState': array([ 2.8865887 , 26.74560247,  2.70979076]), 'targetState': array([ 2, 27], dtype=int32), 'currentDistance': 0.9223652332837327}
episode index:750
target Thresh 31.985841040124214
target distance 21.0
model initialize at round 750
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.74058628, 26.64140195]), 'previousTarget': array([17.74224216, 26.64677133]), 'currentState': array([13.98950997,  6.99631537,  3.23320144]), 'targetState': array([18, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.2602948068047226
running average episode reward sum: 0.41304646071865103
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 28.]), 'previousTarget': array([18., 28.]), 'currentState': array([18.42040573, 27.0946085 ,  1.62086405]), 'targetState': array([18, 28], dtype=int32), 'currentDistance': 0.9982358155733414}
episode index:751
target Thresh 31.985981924128918
target distance 13.0
model initialize at round 751
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 20.]), 'previousTarget': array([24., 20.]), 'currentState': array([10.95933378, 28.0454792 ,  2.04788208]), 'targetState': array([24, 20], dtype=int32), 'currentDistance': 15.32281667492891}
done in step count: 38
reward sum = 0.4207906987681847
running average episode reward sum: 0.41305675890754673
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 20.]), 'previousTarget': array([24., 20.]), 'currentState': array([23.38500957, 20.89341113,  5.54798556]), 'targetState': array([24, 20], dtype=int32), 'currentDistance': 1.0846182121080667}
episode index:752
target Thresh 31.986121406314354
target distance 20.0
model initialize at round 752
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.63148859, 18.48301262]), 'previousTarget': array([17.61737619, 18.49390095]), 'currentState': array([2.05276771, 5.94094517, 5.69004834]), 'targetState': array([22, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 82
reward sum = 0.19588482504896143
running average episode reward sum: 0.4127683499648394
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 22.]), 'previousTarget': array([22., 22.]), 'currentState': array([21.04383052, 21.68625527,  0.63516393]), 'targetState': array([22, 22], dtype=int32), 'currentDistance': 1.006327895213705}
episode index:753
target Thresh 31.98625950062885
target distance 22.0
model initialize at round 753
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.43242207,  9.49734288]), 'previousTarget': array([21.43242207,  9.49734288]), 'currentState': array([17.        , 29.        ,  0.86760187]), 'targetState': array([22,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 72
reward sum = 0.21331093589216293
running average episode reward sum: 0.41250381758543264
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  7.]), 'previousTarget': array([22.,  7.]), 'currentState': array([21.6302869 ,  7.94212125,  5.12518495]), 'targetState': array([22,  7], dtype=int32), 'currentDistance': 1.0120673075818296}
episode index:754
target Thresh 31.986396220881964
target distance 23.0
model initialize at round 754
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.82606538,  8.54553682]), 'previousTarget': array([23.7042351,  8.5731765]), 'currentState': array([ 4.115105  , 11.93346253,  5.68214485]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 72
reward sum = 0.26601703501404605
running average episode reward sum: 0.4123097953568613
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'previousTarget': array([27.,  8.]), 'currentState': array([26.14458686,  7.89653487,  0.24701755]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 0.8616476529392016}
episode index:755
target Thresh 31.98653158074583
target distance 7.0
model initialize at round 755
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([8.02405389, 4.4992278 , 1.47403333]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 6.7944390670578185}
done in step count: 14
reward sum = 0.7572828983711657
running average episode reward sum: 0.41276610898518706
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.5901313 , 10.13897312,  1.33625083]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.953603497702979}
episode index:756
target Thresh 31.986665593756545
target distance 11.0
model initialize at round 756
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 27.]), 'previousTarget': array([14., 27.]), 'currentState': array([15.99975091, 15.99970027,  3.77086127]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 11.180590231466798}
done in step count: 28
reward sum = 0.5641266091240511
running average episode reward sum: 0.41296605680571397
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 27.]), 'previousTarget': array([14., 27.]), 'currentState': array([14.41243663, 26.14966145,  1.73549465]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 0.9450818040523882}
episode index:757
target Thresh 31.98679827331552
target distance 7.0
model initialize at round 757
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 20.]), 'previousTarget': array([10., 20.]), 'currentState': array([16.84361891, 23.60538909,  4.19656229]), 'targetState': array([10, 20], dtype=int32), 'currentDistance': 7.7352407992423275}
done in step count: 15
reward sum = 0.7266891108297979
running average episode reward sum: 0.4133799394627378
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 20.]), 'previousTarget': array([10., 20.]), 'currentState': array([10.96450799, 20.57905189,  3.46876203]), 'targetState': array([10, 20], dtype=int32), 'currentDistance': 1.1249785624448445}
episode index:758
target Thresh 31.98692963269083
target distance 10.0
model initialize at round 758
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([14.      , 10.      ,  2.212594]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 10.44030650891055}
done in step count: 23
reward sum = 0.6129028573381743
running average episode reward sum: 0.4136428155073695
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.9748507 , 7.188874  , 3.73321011]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.9929789885280567}
episode index:759
target Thresh 31.987059685018515
target distance 2.0
model initialize at round 759
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.04029256, 6.94711311, 5.57411692]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 2.266138334829416}
done in step count: 10
reward sum = 0.862578563878717
running average episode reward sum: 0.41423352043943706
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.26704308, 8.03957928, 1.31289962]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.9968550398465976}
episode index:760
target Thresh 31.98718844330392
target distance 13.0
model initialize at round 760
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 15.]), 'previousTarget': array([19., 15.]), 'currentState': array([6.03787171, 4.05217136, 0.8017301 ]), 'targetState': array([19, 15], dtype=int32), 'currentDistance': 16.966782889316093}
done in step count: 37
reward sum = 0.4378376582180258
running average episode reward sum: 0.4142645377032723
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 15.]), 'previousTarget': array([19., 15.]), 'currentState': array([18.70693334, 14.18203981,  0.88248862]), 'targetState': array([19, 15], dtype=int32), 'currentDistance': 0.8688768227398099}
episode index:761
target Thresh 31.98731592042298
target distance 2.0
model initialize at round 761
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 29.]), 'previousTarget': array([15., 29.]), 'currentState': array([13.42550264, 29.16319692,  0.31332617]), 'targetState': array([15, 29], dtype=int32), 'currentDistance': 1.582932459479265}
done in step count: 2
reward sum = 0.9637483729474697
running average episode reward sum: 0.41498564509860586
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 29.]), 'previousTarget': array([15., 29.]), 'currentState': array([14.00751657, 29.31946594,  0.21693627]), 'targetState': array([15, 29], dtype=int32), 'currentDistance': 1.042632169602347}
episode index:762
target Thresh 31.987442129123508
target distance 21.0
model initialize at round 762
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.51291586,  9.78822411]), 'previousTarget': array([17.50557744,  9.76952105]), 'currentState': array([23.02716152, 29.01302826,  0.20361593]), 'targetState': array([17,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.26054173760754407
running average episode reward sum: 0.41478322844396487
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  8.]), 'previousTarget': array([17.,  8.]), 'currentState': array([17.11595085,  8.83781212,  4.76759212]), 'targetState': array([17,  8], dtype=int32), 'currentDistance': 0.8457976945970944}
episode index:763
target Thresh 31.987567082026487
target distance 15.0
model initialize at round 763
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.37889464, 13.35363499]), 'previousTarget': array([11.37889464, 13.35363499]), 'currentState': array([26.        , 27.        ,  2.76939797]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.3317907754815091
running average episode reward sum: 0.4146745995788308
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 13.]), 'previousTarget': array([11., 13.]), 'currentState': array([11.93755438, 13.46186855,  4.28804469]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 1.045146296910131}
episode index:764
target Thresh 31.987690791627305
target distance 8.0
model initialize at round 764
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 24.]), 'previousTarget': array([12., 24.]), 'currentState': array([ 4.47723946, 19.13641996,  0.48991308]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 8.958031983813495}
done in step count: 18
reward sum = 0.6827483355781852
running average episode reward sum: 0.4150250227631437
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 24.]), 'previousTarget': array([12., 24.]), 'currentState': array([11.13621193, 23.2471277 ,  0.5389081 ]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 1.1458387930611866}
episode index:765
target Thresh 31.98781327029703
target distance 6.0
model initialize at round 765
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([9.        , 3.        , 6.02737847]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 6.324555320336759}
done in step count: 16
reward sum = 0.738132364213526
running average episode reward sum: 0.4154468339138622
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.83427561,  8.1687174 ,  1.49103766]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.8476410375119849}
episode index:766
target Thresh 31.987934530283628
target distance 19.0
model initialize at round 766
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.27926583, 19.00088553]), 'previousTarget': array([22.30852571, 19.02072541]), 'currentState': array([4.98339306, 8.9583368 , 4.58559036]), 'targetState': array([24, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.26248307198339066
running average episode reward sum: 0.4152474026727534
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 20.]), 'previousTarget': array([24., 20.]), 'currentState': array([23.27372634, 19.05991851,  0.62060107]), 'targetState': array([24, 20], dtype=int32), 'currentDistance': 1.187950602568917}
episode index:767
target Thresh 31.9880545837132
target distance 5.0
model initialize at round 767
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  9.]), 'previousTarget': array([22.,  9.]), 'currentState': array([27.        , 14.        ,  1.76741615]), 'targetState': array([22,  9], dtype=int32), 'currentDistance': 7.071067811865475}
done in step count: 17
reward sum = 0.6918223580706864
running average episode reward sum: 0.41560752631259446
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  9.]), 'previousTarget': array([22.,  9.]), 'currentState': array([22.38292479,  9.78523416,  4.3412018 ]), 'targetState': array([22,  9], dtype=int32), 'currentDistance': 0.8736269722794606}
episode index:768
target Thresh 31.98817344259119
target distance 4.0
model initialize at round 768
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 14.]), 'previousTarget': array([26., 14.]), 'currentState': array([24.93926967,  9.98211347,  3.17579713]), 'targetState': array([26, 14], dtype=int32), 'currentDistance': 4.155545808476819}
done in step count: 13
reward sum = 0.796666274941521
running average episode reward sum: 0.41610305134332126
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 14.]), 'previousTarget': array([26., 14.]), 'currentState': array([25.56342298, 13.07608023,  0.88286518]), 'targetState': array([26, 14], dtype=int32), 'currentDistance': 1.021874371770865}
episode index:769
target Thresh 31.988291118803584
target distance 4.0
model initialize at round 769
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 24.]), 'previousTarget': array([25., 24.]), 'currentState': array([23.92034048, 27.91369587,  4.10147235]), 'targetState': array([25, 24], dtype=int32), 'currentDistance': 4.059886708328578}
done in step count: 9
reward sum = 0.8520846952961508
running average episode reward sum: 0.4166692612705327
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 24.]), 'previousTarget': array([25., 24.]), 'currentState': array([24.55971227, 24.82766802,  5.4092273 ]), 'targetState': array([25, 24], dtype=int32), 'currentDistance': 0.9374900759125522}
episode index:770
target Thresh 31.988407624118096
target distance 14.0
model initialize at round 770
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'previousTarget': array([11., 21.]), 'currentState': array([8.94555187, 7.09970499, 2.07494997]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 14.051297392804058}
done in step count: 37
reward sum = 0.5103067495119995
running average episode reward sum: 0.4167907106716241
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'previousTarget': array([11., 21.]), 'currentState': array([10.82418631, 20.05158027,  1.44530599]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 0.9645778544910437}
episode index:771
target Thresh 31.988522970185365
target distance 8.0
model initialize at round 771
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([24.00286968, 12.99539114,  5.01679325]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 9.43397349151128}
done in step count: 22
reward sum = 0.6382165326178424
running average episode reward sum: 0.4170775316845078
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.87483177,  8.49892609,  3.43122981]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.007103699604142}
episode index:772
target Thresh 31.988637168540087
target distance 7.0
model initialize at round 772
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([ 2.97563596, 11.96098798,  4.39016163]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 8.599598358825057}
done in step count: 18
reward sum = 0.6773343610416289
running average episode reward sum: 0.4174142158104549
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([9.00489212, 7.38691997, 5.68801455]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 1.0676828886408976}
episode index:773
target Thresh 31.988750230602193
target distance 7.0
model initialize at round 773
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.95284123, 14.01516674,  3.07182235]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 7.079581260250749}
done in step count: 16
reward sum = 0.7136782813881932
running average episode reward sum: 0.41779698592102044
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.66561575,  7.83180547,  4.88016184]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.8965005135309946}
episode index:774
target Thresh 31.988862167677983
target distance 5.0
model initialize at round 774
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 18.]), 'previousTarget': array([ 8., 18.]), 'currentState': array([ 2.96631606, 20.11524523,  1.60265613]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 5.460058282770202}
done in step count: 12
reward sum = 0.7538236639181845
running average episode reward sum: 0.41823056873133935
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 18.]), 'previousTarget': array([ 8., 18.]), 'currentState': array([ 7.31078843, 18.87802423,  5.3684059 ]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 1.1162164423765415}
episode index:775
target Thresh 31.988972990961262
target distance 5.0
model initialize at round 775
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([23.00276465, 14.98238233,  5.10323718]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 6.41516776921519}
done in step count: 15
reward sum = 0.730460430510989
running average episode reward sum: 0.41863292680064307
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([26.40173142, 19.03013436,  1.26209515]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 1.1395458050637144}
episode index:776
target Thresh 31.989082711534447
target distance 21.0
model initialize at round 776
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.10732562,  5.12187854]), 'previousTarget': array([11.10381815,  5.09009055]), 'currentState': array([13.01194976, 25.03098215,  0.95017946]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.23735418762545796
running average episode reward sum: 0.4183996208300186
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([11.56241499,  4.87558439,  4.45309187]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 1.0406529954007873}
episode index:777
target Thresh 31.989191340369686
target distance 12.0
model initialize at round 777
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 18.]), 'previousTarget': array([25., 18.]), 'currentState': array([20.        ,  6.        ,  2.60889757]), 'targetState': array([25, 18], dtype=int32), 'currentDistance': 13.000000000000002}
done in step count: 30
reward sum = 0.5203140330191037
running average episode reward sum: 0.4185306162184364
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 18.]), 'previousTarget': array([25., 18.]), 'currentState': array([24.52186556, 17.12594543,  1.51760901]), 'targetState': array([25, 18], dtype=int32), 'currentDistance': 0.9962850705739679}
episode index:778
target Thresh 31.98929888832996
target distance 23.0
model initialize at round 778
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9.21768818, 25.02201457]), 'previousTarget': array([ 9.26740767, 24.92481176]), 'currentState': array([10.6757806 ,  5.07523622,  2.68347976]), 'targetState': array([ 9, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.24362472810116725
running average episode reward sum: 0.41830609004627045
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 28.]), 'previousTarget': array([ 9., 28.]), 'currentState': array([ 9.30191075, 27.19735303,  1.76145431]), 'targetState': array([ 9, 28], dtype=int32), 'currentDistance': 0.857550155081058}
episode index:779
target Thresh 31.989405366170143
target distance 9.0
model initialize at round 779
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([13.94923612, 26.98962704,  3.5528096 ]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 9.050828637470511}
done in step count: 19
reward sum = 0.6610545727028438
running average episode reward sum: 0.4186173060496763
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([14.09460033, 18.78443299,  5.00563974]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 1.197949777507603}
episode index:780
target Thresh 31.98951078453812
target distance 11.0
model initialize at round 780
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  2.]), 'previousTarget': array([20.,  2.]), 'currentState': array([8.96655503, 7.99008459, 3.66112921]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 12.554601597227816}
done in step count: 32
reward sum = 0.5131887685211787
running average episode reward sum: 0.41873839627051046
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  2.]), 'previousTarget': array([20.,  2.]), 'currentState': array([19.1143071 ,  2.26143078,  5.92040721]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 0.9234706103696106}
episode index:781
target Thresh 31.989615153975805
target distance 7.0
model initialize at round 781
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  8.]), 'previousTarget': array([20.,  8.]), 'currentState': array([27.00372471, 10.98273709,  4.67239475]), 'targetState': array([20,  8], dtype=int32), 'currentDistance': 7.612416192398238}
done in step count: 33
reward sum = 0.5649738765003405
running average episode reward sum: 0.41892539816338753
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  8.]), 'previousTarget': array([20.,  8.]), 'currentState': array([19.60361523,  7.03438278,  2.03856456]), 'targetState': array([20,  8], dtype=int32), 'currentDistance': 1.0438091303764152}
episode index:782
target Thresh 31.989718484920235
target distance 2.0
model initialize at round 782
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 28.]), 'previousTarget': array([16., 28.]), 'currentState': array([17.6918472 , 26.00174743,  3.03763743]), 'targetState': array([16, 28], dtype=int32), 'currentDistance': 2.618274298098355}
done in step count: 9
reward sum = 0.8849287553015724
running average episode reward sum: 0.4195205493219293
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 28.]), 'previousTarget': array([16., 28.]), 'currentState': array([15.96583111, 27.01239209,  2.46545242]), 'targetState': array([16, 28], dtype=int32), 'currentDistance': 0.9881988181027141}
episode index:783
target Thresh 31.989820787704588
target distance 11.0
model initialize at round 783
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([ 6.        , 14.        ,  2.68584248]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 11.045361017187261}
done in step count: 28
reward sum = 0.5893684020477961
running average episode reward sum: 0.41973719199122245
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([ 6.15167969, 24.0649694 ,  1.07064597]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 1.2625092342020312}
episode index:784
target Thresh 31.98992207255923
target distance 21.0
model initialize at round 784
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8.0913656 , 23.12086431]), 'previousTarget': array([ 8.0913656 , 23.12086431]), 'currentState': array([24.        , 11.        ,  1.66599435]), 'targetState': array([ 3, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 72
reward sum = 0.20870638151539583
running average episode reward sum: 0.4194683629332915
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 27.]), 'previousTarget': array([ 3., 27.]), 'currentState': array([ 3.98884843, 26.56770679,  2.64120752]), 'targetState': array([ 3, 27], dtype=int32), 'currentDistance': 1.0792120422616958}
episode index:785
target Thresh 31.99002234961273
target distance 8.0
model initialize at round 785
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  7.]), 'previousTarget': array([12.,  7.]), 'currentState': array([3.98862634, 9.99262327, 3.94582736]), 'targetState': array([12,  7], dtype=int32), 'currentDistance': 8.552070035328562}
done in step count: 20
reward sum = 0.6465591375244901
running average episode reward sum: 0.4197572824938401
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  7.]), 'previousTarget': array([12.,  7.]), 'currentState': array([11.19766579,  7.11864403,  6.19868216]), 'targetState': array([12,  7], dtype=int32), 'currentDistance': 0.8110589350469166}
episode index:786
target Thresh 31.990121628892876
target distance 16.0
model initialize at round 786
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 29.]), 'previousTarget': array([ 5., 29.]), 'currentState': array([20.69981183, 24.05789871,  2.88423213]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 16.459296964346}
done in step count: 35
reward sum = 0.453880589200066
running average episode reward sum: 0.4198006412063004
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 29.]), 'previousTarget': array([ 5., 29.]), 'currentState': array([ 5.83693271, 28.79178937,  2.94133313]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 0.8624430540281794}
episode index:787
target Thresh 31.99021992032768
target distance 14.0
model initialize at round 787
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 28.]), 'previousTarget': array([ 5., 28.]), 'currentState': array([ 5.037372  , 14.40039158,  1.60443151]), 'targetState': array([ 5, 28], dtype=int32), 'currentDistance': 13.599659766296158}
done in step count: 29
reward sum = 0.5371500580836797
running average episode reward sum: 0.419949561786094
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 28.]), 'previousTarget': array([ 5., 28.]), 'currentState': array([ 5.6117385 , 27.12167726,  1.75648919]), 'targetState': array([ 5, 28], dtype=int32), 'currentDistance': 1.0703620126765052}
episode index:788
target Thresh 31.990317233746364
target distance 15.0
model initialize at round 788
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 14.]), 'previousTarget': array([ 9., 14.]), 'currentState': array([18.        , 29.        ,  0.19341454]), 'targetState': array([ 9, 14], dtype=int32), 'currentDistance': 17.4928556845359}
done in step count: 42
reward sum = 0.37406089913336327
running average episode reward sum: 0.4198914012504125
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 14.]), 'previousTarget': array([ 9., 14.]), 'currentState': array([ 9.76349621, 14.8757931 ,  4.28308459]), 'targetState': array([ 9, 14], dtype=int32), 'currentDistance': 1.1618691894068824}
episode index:789
target Thresh 31.990413578880357
target distance 20.0
model initialize at round 789
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.96680906, 23.77872706]), 'previousTarget': array([21.96680906, 23.77872706]), 'currentState': array([19.        ,  4.        ,  3.52881992]), 'targetState': array([22, 24], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.29683839363889464
running average episode reward sum: 0.4197356379496384
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 24.]), 'previousTarget': array([22., 24.]), 'currentState': array([22.10648689, 23.13416461,  1.57378617]), 'targetState': array([22, 24], dtype=int32), 'currentDistance': 0.872359091935824}
episode index:790
target Thresh 31.990508965364246
target distance 13.0
model initialize at round 790
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 18.]), 'previousTarget': array([12., 18.]), 'currentState': array([24.7079553 ,  8.27275036,  2.41775587]), 'targetState': array([12, 18], dtype=int32), 'currentDistance': 16.003484412491932}
done in step count: 35
reward sum = 0.4732887971665903
running average episode reward sum: 0.41980334105863587
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 18.]), 'previousTarget': array([12., 18.]), 'currentState': array([12.90823328, 17.50772079,  2.513703  ]), 'targetState': array([12, 18], dtype=int32), 'currentDistance': 1.0330665579915046}
episode index:791
target Thresh 31.990603402736767
target distance 18.0
model initialize at round 791
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  4.]), 'previousTarget': array([10.,  4.]), 'currentState': array([ 4.98758977, 22.01507085,  2.50242576]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 18.699385929168635}
done in step count: 49
reward sum = 0.333461927142978
running average episode reward sum: 0.4196943241218737
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  4.]), 'previousTarget': array([10.,  4.]), 'currentState': array([10.0556405 ,  4.97119748,  5.13719281]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 0.9727900102123451}
episode index:792
target Thresh 31.990696900441726
target distance 16.0
model initialize at round 792
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 10.]), 'previousTarget': array([25., 10.]), 'currentState': array([ 8.96897085, 19.0453952 ,  1.92638782]), 'targetState': array([25, 10], dtype=int32), 'currentDistance': 18.40687561465376}
done in step count: 49
reward sum = 0.32807486879284214
running average episode reward sum: 0.419578788869252
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 10.]), 'previousTarget': array([25., 10.]), 'currentState': array([24.08176373, 10.43345755,  5.93020166]), 'targetState': array([25, 10], dtype=int32), 'currentDistance': 1.0154030190978998}
episode index:793
target Thresh 31.99078946782898
target distance 5.0
model initialize at round 793
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([22.33065134, 13.71778239,  5.55718698]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 6.6377923754320705}
done in step count: 13
reward sum = 0.7855904673179441
running average episode reward sum: 0.42003976075646693
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([26.10097472,  9.95670387,  5.34938794]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 1.312832339528153}
episode index:794
target Thresh 31.99088111415534
target distance 6.0
model initialize at round 794
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  8.]), 'previousTarget': array([24.,  8.]), 'currentState': array([18.0332623 ,  4.30848795,  1.29738244]), 'targetState': array([24,  8], dtype=int32), 'currentDistance': 7.0163537504717555}
done in step count: 15
reward sum = 0.7485120224687887
running average episode reward sum: 0.42045293341270884
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  8.]), 'previousTarget': array([24.,  8.]), 'currentState': array([23.1720352 ,  7.49351931,  0.39671521]), 'targetState': array([24,  8], dtype=int32), 'currentDistance': 0.9705917786792584}
episode index:795
target Thresh 31.990971848585517
target distance 12.0
model initialize at round 795
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 17.]), 'previousTarget': array([ 5., 17.]), 'currentState': array([16.69427972, 18.90782914,  3.35381031]), 'targetState': array([ 5, 17], dtype=int32), 'currentDistance': 11.848881386234387}
done in step count: 24
reward sum = 0.5983680330939032
running average episode reward sum: 0.42067644484446914
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 17.]), 'previousTarget': array([ 5., 17.]), 'currentState': array([ 5.93744993, 17.24457051,  3.3118436 ]), 'targetState': array([ 5, 17], dtype=int32), 'currentDistance': 0.9688276927946691}
episode index:796
target Thresh 31.991061680193027
target distance 22.0
model initialize at round 796
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.24434856, 11.68866087]), 'previousTarget': array([21.07239741, 11.86353984]), 'currentState': array([ 8.19912989, 26.84855104,  5.5969914 ]), 'targetState': array([27,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 68
reward sum = 0.16711879474661773
running average episode reward sum: 0.420358304756517
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  5.]), 'previousTarget': array([27.,  5.]), 'currentState': array([26.15208367,  5.98854812,  5.78748769]), 'targetState': array([27,  5], dtype=int32), 'currentDistance': 1.3023783973855805}
episode index:797
target Thresh 31.99115061796111
target distance 1.0
model initialize at round 797
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  7.]), 'previousTarget': array([20.,  7.]), 'currentState': array([21.        ,  6.        ,  5.96410054]), 'targetState': array([20,  7], dtype=int32), 'currentDistance': 1.414213562373095}
done in step count: 16
reward sum = 0.8242596462622983
running average episode reward sum: 0.4208644467884791
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  7.]), 'previousTarget': array([20.,  7.]), 'currentState': array([20.99764744,  6.641989  ,  2.17664536]), 'targetState': array([20,  7], dtype=int32), 'currentDistance': 1.0599397636662449}
episode index:798
target Thresh 31.991238670783616
target distance 8.0
model initialize at round 798
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  6.]), 'previousTarget': array([13.,  6.]), 'currentState': array([ 5.00430349, 13.90687141,  4.9711858 ]), 'targetState': array([13,  6], dtype=int32), 'currentDistance': 11.244989029789714}
done in step count: 23
reward sum = 0.616204151051999
running average episode reward sum: 0.4211089270190968
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  6.]), 'previousTarget': array([13.,  6.]), 'currentState': array([12.21829161,  6.90038786,  5.72117431]), 'targetState': array([13,  6], dtype=int32), 'currentDistance': 1.1923784242334539}
episode index:799
target Thresh 31.991325847465895
target distance 10.0
model initialize at round 799
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([14.        ,  8.        ,  5.92417806]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 10.770329614269007}
done in step count: 23
reward sum = 0.6195220666228969
running average episode reward sum: 0.42135694344360153
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([23.019394  , 11.63230155,  0.44391217]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 1.0472775502250937}
episode index:800
target Thresh 31.991412156725694
target distance 15.0
model initialize at round 800
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([14.02858565, 19.9106225 ,  4.89689368]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 15.047984019311286}
done in step count: 35
reward sum = 0.4906380791221781
running average episode reward sum: 0.42144343674657103
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([11.89790067,  5.92332992,  5.13676115]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 0.9289576997535848}
episode index:801
target Thresh 31.99149760719401
target distance 21.0
model initialize at round 801
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.41603543,  8.45510259]), 'previousTarget': array([13.41603543,  8.45510259]), 'currentState': array([26.        , 24.        ,  1.58250093]), 'targetState': array([9, 3], dtype=int32), 'currentDistance': 20.0}
done in step count: 98
reward sum = 0.10815603259157047
running average episode reward sum: 0.42105280407306106
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 3.]), 'previousTarget': array([9., 3.]), 'currentState': array([9.95889315, 3.02525885, 3.74672801]), 'targetState': array([9, 3], dtype=int32), 'currentDistance': 0.9592257676024091}
episode index:802
target Thresh 31.99158220741596
target distance 22.0
model initialize at round 802
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.74344656, 19.87826844]), 'previousTarget': array([18.73750984, 19.87322975]), 'currentState': array([8.01234116, 3.00096484, 0.25513646]), 'targetState': array([22, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.18266931477115445
running average episode reward sum: 0.42075593795936006
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 25.]), 'previousTarget': array([22., 25.]), 'currentState': array([21.06032658, 25.15171649,  0.92076169]), 'targetState': array([22, 25], dtype=int32), 'currentDistance': 0.9518424367899323}
episode index:803
target Thresh 31.991665965851638
target distance 21.0
model initialize at round 803
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8.26083524, 4.85478255]), 'previousTarget': array([8.63513716, 5.07722123]), 'currentState': array([25.64530392, 14.74317214,  3.79963699]), 'targetState': array([5, 3], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.2638236816899612
running average episode reward sum: 0.42056074858589065
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 3.]), 'previousTarget': array([5., 3.]), 'currentState': array([5.95698934, 3.69622416, 3.55247696]), 'targetState': array([5, 3], dtype=int32), 'currentDistance': 1.1834511698814858}
episode index:804
target Thresh 31.991748890876952
target distance 8.0
model initialize at round 804
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([18.92279129,  2.98134517,  3.22116846]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 9.949614451802566}
done in step count: 22
reward sum = 0.650643791446938
running average episode reward sum: 0.4208465660304386
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([11.91279281,  8.55470914,  2.49034711]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 1.0156154137930404}
episode index:805
target Thresh 31.991830990784482
target distance 6.0
model initialize at round 805
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 19.]), 'previousTarget': array([19., 19.]), 'currentState': array([13.        , 24.        ,  0.76774096]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 7.810249675906655}
done in step count: 24
reward sum = 0.6598497267905121
running average episode reward sum: 0.42114309600656774
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 19.]), 'previousTarget': array([19., 19.]), 'currentState': array([19.16933148, 19.95985745,  5.21096888]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 0.9746791640629504}
episode index:806
target Thresh 31.99191227378428
target distance 11.0
model initialize at round 806
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 10.]), 'previousTarget': array([22., 10.]), 'currentState': array([12.14836637, 20.93406247,  5.79965511]), 'targetState': array([22, 10], dtype=int32), 'currentDistance': 14.717622339262336}
done in step count: 35
reward sum = 0.5047903573095602
running average episode reward sum: 0.42124674812714147
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 10.]), 'previousTarget': array([22., 10.]), 'currentState': array([21.98657906, 10.9642156 ,  5.78320285]), 'targetState': array([22, 10], dtype=int32), 'currentDistance': 0.9643089976876501}
episode index:807
target Thresh 31.99199274800472
target distance 13.0
model initialize at round 807
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([15.1192591 , 11.04215717,  0.14680901]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 15.933473118665368}
done in step count: 43
reward sum = 0.40849076831639775
running average episode reward sum: 0.4212309610234153
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.97255979, 2.5046278 , 3.71735582]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 1.0956832437095672}
episode index:808
target Thresh 31.992072421493283
target distance 7.0
model initialize at round 808
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 18.]), 'previousTarget': array([14., 18.]), 'currentState': array([ 7.38943177, 22.87563588,  5.97666602]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 8.214099927589565}
done in step count: 21
reward sum = 0.6675926765447884
running average episode reward sum: 0.42153548724779283
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 18.]), 'previousTarget': array([14., 18.]), 'currentState': array([14.46247786, 18.97070071,  5.07501562]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 1.0752421276877422}
episode index:809
target Thresh 31.99215130221739
target distance 14.0
model initialize at round 809
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 10.]), 'previousTarget': array([22., 10.]), 'currentState': array([24.03765042, 23.98360437,  5.71404004]), 'targetState': array([22, 10], dtype=int32), 'currentDistance': 14.131284810382652}
done in step count: 35
reward sum = 0.505811864541631
running average episode reward sum: 0.42163953215803207
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 10.]), 'previousTarget': array([22., 10.]), 'currentState': array([21.73328779, 10.878471  ,  4.76498644]), 'targetState': array([22, 10], dtype=int32), 'currentDistance': 0.9180668249671002}
episode index:810
target Thresh 31.99222939806518
target distance 4.0
model initialize at round 810
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 24.]), 'previousTarget': array([ 7., 24.]), 'currentState': array([ 8.        , 28.        ,  2.86332905]), 'targetState': array([ 7, 24], dtype=int32), 'currentDistance': 4.123105625617661}
done in step count: 12
reward sum = 0.812596372561505
running average episode reward sum: 0.4221215997787516
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 24.]), 'previousTarget': array([ 7., 24.]), 'currentState': array([ 6.47674042, 24.99960899,  4.94501536]), 'targetState': array([ 7, 24], dtype=int32), 'currentDistance': 1.1282813139924595}
episode index:811
target Thresh 31.9923067168463
target distance 9.0
model initialize at round 811
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([25.        , 15.        ,  2.07619429]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 9.055385138137417}
done in step count: 19
reward sum = 0.6598885795895596
running average episode reward sum: 0.42241441625635107
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([16.87317995, 14.28804524,  3.44342288]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 0.9194635866824827}
episode index:812
target Thresh 31.992383266292695
target distance 17.0
model initialize at round 812
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([25.98451596, 28.0218956 ,  2.42120734]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 19.73411991592253}
done in step count: 80
reward sum = 0.2687356284124742
running average episode reward sum: 0.42222538945703514
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.98908756, 11.82648707,  3.56816749]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.2889434024194217}
episode index:813
target Thresh 31.99245905405937
target distance 6.0
model initialize at round 813
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 22.]), 'previousTarget': array([19., 22.]), 'currentState': array([24.        , 16.        ,  1.18431506]), 'targetState': array([19, 22], dtype=int32), 'currentDistance': 7.810249675906654}
done in step count: 19
reward sum = 0.713886272664783
running average episode reward sum: 0.4225836952103616
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 22.]), 'previousTarget': array([19., 22.]), 'currentState': array([19.99281917, 21.16968268,  2.29095849]), 'targetState': array([19, 22], dtype=int32), 'currentDistance': 1.294263018872563}
episode index:814
target Thresh 31.99253408772517
target distance 14.0
model initialize at round 814
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([26.        , 18.        ,  4.03717148]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 14.317821063276353}
done in step count: 48
reward sum = 0.44816692576832795
running average episode reward sum: 0.4226150856773039
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([12.35473105, 20.17429182,  2.21690019]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 0.89868131881664}
episode index:815
target Thresh 31.99260837479352
target distance 4.0
model initialize at round 815
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([ 5.98479722, 14.94219596,  4.70770788]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 4.124268002046469}
done in step count: 11
reward sum = 0.8151287478888655
running average episode reward sum: 0.423096107322171
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([ 9.05439285, 13.56288272,  0.12438648]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 1.0417506399887517}
episode index:816
target Thresh 31.992681922693187
target distance 7.0
model initialize at round 816
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  7.]), 'previousTarget': array([22.,  7.]), 'currentState': array([24.        , 14.        ,  5.41523534]), 'targetState': array([22,  7], dtype=int32), 'currentDistance': 7.280109889280518}
done in step count: 18
reward sum = 0.7269979736878214
running average episode reward sum: 0.42346808023081933
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  7.]), 'previousTarget': array([22.,  7.]), 'currentState': array([22.09373399,  7.92527664,  4.3577822 ]), 'targetState': array([22,  7], dtype=int32), 'currentDistance': 0.9300123193242535}
episode index:817
target Thresh 31.992754738779027
target distance 19.0
model initialize at round 817
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.53214757, 22.78228855]), 'previousTarget': array([22.51905368, 22.75489296]), 'currentState': array([7.98875379, 9.05313354, 1.57746786]), 'targetState': array([27, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 82
reward sum = 0.18701017855698582
running average episode reward sum: 0.42317901189136475
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 27.]), 'previousTarget': array([27., 27.]), 'currentState': array([26.03246518, 26.56978164,  0.60841625]), 'targetState': array([27, 27], dtype=int32), 'currentDistance': 1.058872731085932}
episode index:818
target Thresh 31.992826830332707
target distance 5.0
model initialize at round 818
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  8.]), 'previousTarget': array([26.,  8.]), 'currentState': array([21.37709894, 10.96261256,  6.12293091]), 'targetState': array([26,  8], dtype=int32), 'currentDistance': 5.490745610605639}
done in step count: 11
reward sum = 0.817833485349044
running average episode reward sum: 0.4236608854853302
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  8.]), 'previousTarget': array([26.,  8.]), 'currentState': array([25.25344227,  8.91515958,  5.4907433 ]), 'targetState': array([26,  8], dtype=int32), 'currentDistance': 1.1810442439495497}
episode index:819
target Thresh 31.99289820456344
target distance 20.0
model initialize at round 819
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.39933211, 19.15024705]), 'previousTarget': array([21.40285  , 19.1492875]), 'currentState': array([ 1.99709089, 24.00339402,  2.02691555]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.2636031595421563
running average episode reward sum: 0.42346569313661897
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([21.02527187, 18.65974407,  6.07366137]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 1.032409330690805}
episode index:820
target Thresh 31.992968868608713
target distance 18.0
model initialize at round 820
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([9.20104506e+00, 2.60519501e+01, 2.60075927e-02]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 18.79970471907265}
done in step count: 48
reward sum = 0.38478999383128093
running average episode reward sum: 0.4234185850984883
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([26.06697426, 20.36745271,  6.02534638]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 1.0027754094344734}
episode index:821
target Thresh 31.99303882953499
target distance 18.0
model initialize at round 821
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 22.]), 'previousTarget': array([21., 22.]), 'currentState': array([14.        ,  4.        ,  5.76914945]), 'targetState': array([21, 22], dtype=int32), 'currentDistance': 19.313207915827967}
done in step count: 62
reward sum = 0.32444905626471654
running average episode reward sum: 0.4232981842118292
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 22.]), 'previousTarget': array([21., 22.]), 'currentState': array([20.05912161, 21.28210056,  0.79731997]), 'targetState': array([21, 22], dtype=int32), 'currentDistance': 1.1834828849547754}
episode index:822
target Thresh 31.993108094338414
target distance 6.0
model initialize at round 822
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 20.]), 'previousTarget': array([ 3., 20.]), 'currentState': array([ 6.04173616, 14.46481971,  1.67773387]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 6.315883129807643}
done in step count: 12
reward sum = 0.7805019813454664
running average episode reward sum: 0.4237322106968032
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 20.]), 'previousTarget': array([ 3., 20.]), 'currentState': array([ 3.72526845, 19.12362766,  2.19584142]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 1.1375600193343713}
episode index:823
target Thresh 31.993176669945534
target distance 18.0
model initialize at round 823
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  4.]), 'previousTarget': array([18.,  4.]), 'currentState': array([ 9.99707675, 21.99427564,  4.37908444]), 'targetState': array([18,  4], dtype=int32), 'currentDistance': 19.693672498706185}
done in step count: 53
reward sum = 0.3667593279675546
running average episode reward sum: 0.4236630688488308
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  4.]), 'previousTarget': array([18.,  4.]), 'currentState': array([17.00542795,  4.90421645,  5.33148755]), 'targetState': array([18,  4], dtype=int32), 'currentDistance': 1.3441655220685527}
episode index:824
target Thresh 31.99324456321396
target distance 12.0
model initialize at round 824
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 25.]), 'previousTarget': array([18., 25.]), 'currentState': array([ 6.0418946 , 15.13685875,  1.14192599]), 'targetState': array([18, 25], dtype=int32), 'currentDistance': 15.500898042322975}
done in step count: 41
reward sum = 0.47462133724320876
running average episode reward sum: 0.42372483644688463
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 25.]), 'previousTarget': array([18., 25.]), 'currentState': array([17.52512864, 24.0353309 ,  0.90415161]), 'targetState': array([18, 25], dtype=int32), 'currentDistance': 1.0752159217544417}
episode index:825
target Thresh 31.993311780933077
target distance 11.0
model initialize at round 825
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 14.]), 'previousTarget': array([27., 14.]), 'currentState': array([16.        , 16.        ,  3.59758905]), 'targetState': array([27, 14], dtype=int32), 'currentDistance': 11.180339887498949}
done in step count: 28
reward sum = 0.5428842259740636
running average episode reward sum: 0.4238690972090241
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 14.]), 'previousTarget': array([27., 14.]), 'currentState': array([26.0587747 , 14.08778463,  0.15115537]), 'targetState': array([27, 14], dtype=int32), 'currentDistance': 0.9453101153920783}
episode index:826
target Thresh 31.993378329824715
target distance 10.0
model initialize at round 826
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 12.]), 'previousTarget': array([22., 12.]), 'currentState': array([17.08765086,  2.23547993,  1.28460258]), 'targetState': array([22, 12], dtype=int32), 'currentDistance': 10.930554710973281}
done in step count: 36
reward sum = 0.5804985140604575
running average episode reward sum: 0.4240584919089654
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 12.]), 'previousTarget': array([22., 12.]), 'currentState': array([21.05665302, 11.23971702,  0.60683597]), 'targetState': array([22, 12], dtype=int32), 'currentDistance': 1.2115831474030259}
episode index:827
target Thresh 31.993444216543818
target distance 18.0
model initialize at round 827
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 24.]), 'previousTarget': array([15., 24.]), 'currentState': array([13.01945289,  6.00134241,  0.25290805]), 'targetState': array([15, 24], dtype=int32), 'currentDistance': 18.10729803301493}
done in step count: 65
reward sum = 0.34923366409845424
running average episode reward sum: 0.42396812375943577
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 24.]), 'previousTarget': array([15., 24.]), 'currentState': array([14.83895405, 23.03481099,  1.00474302]), 'targetState': array([15, 24], dtype=int32), 'currentDistance': 0.9785323844588686}
episode index:828
target Thresh 31.99350944767911
target distance 14.0
model initialize at round 828
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([13.        , 18.        ,  5.56193495]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 14.560219778561036}
done in step count: 51
reward sum = 0.43456520873374843
running average episode reward sum: 0.4239809067328668
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([9.27449727, 4.91088073, 4.28751098]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 0.9513424500815613}
episode index:829
target Thresh 31.993574029753763
target distance 13.0
model initialize at round 829
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 18.]), 'previousTarget': array([12., 18.]), 'currentState': array([13.092551  ,  5.42420744,  1.55676699]), 'targetState': array([12, 18], dtype=int32), 'currentDistance': 12.623162289142034}
done in step count: 32
reward sum = 0.5560448873982844
running average episode reward sum: 0.4241400199625842
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 18.]), 'previousTarget': array([12., 18.]), 'currentState': array([11.95053833, 17.07935338,  1.17055683]), 'targetState': array([12, 18], dtype=int32), 'currentDistance': 0.9219743199340031}
episode index:830
target Thresh 31.993637969226036
target distance 8.0
model initialize at round 830
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 19.]), 'previousTarget': array([19., 19.]), 'currentState': array([26.81318096, 25.74392117,  4.02738572]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 10.321156403076248}
done in step count: 20
reward sum = 0.657179503398793
running average episode reward sum: 0.4244204525539635
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 19.]), 'previousTarget': array([19., 19.]), 'currentState': array([19.82438812, 19.97527749,  4.12277926]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 1.2770207281931825}
episode index:831
target Thresh 31.99370127248993
target distance 1.0
model initialize at round 831
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 28.]), 'previousTarget': array([ 8., 28.]), 'currentState': array([ 8.83519679, 29.00346944,  3.25103551]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 1.3055667702988873}
done in step count: 1
reward sum = 0.9828731757267467
running average episode reward sum: 0.4250916697693154
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 28.]), 'previousTarget': array([ 8., 28.]), 'currentState': array([ 8.64885071, 28.96213502,  3.47091095]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 1.1604787972672859}
episode index:832
target Thresh 31.993763945875823
target distance 19.0
model initialize at round 832
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([10.00679925, 27.00763556,  0.60196486]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 19.934187892397457}
done in step count: 64
reward sum = 0.2986788731373994
running average episode reward sum: 0.42493991371093376
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.89306063, 8.85589639, 4.19376   ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.2369785441700984}
episode index:833
target Thresh 31.993825995651108
target distance 17.0
model initialize at round 833
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 4.]), 'previousTarget': array([7., 4.]), 'currentState': array([23.72189078,  5.90782442,  3.49060677]), 'targetState': array([7, 4], dtype=int32), 'currentDistance': 16.830372107460676}
done in step count: 46
reward sum = 0.42923698565919516
running average episode reward sum: 0.42494506607538013
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 4.]), 'previousTarget': array([7., 4.]), 'currentState': array([7.84195914, 3.88938355, 3.0761682 ]), 'targetState': array([7, 4], dtype=int32), 'currentDistance': 0.8491944398955645}
episode index:834
target Thresh 31.993887428020813
target distance 20.0
model initialize at round 834
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.7655179 , 10.51702236]), 'previousTarget': array([21.77498924, 10.50001133]), 'currentState': array([ 7.99977784, 25.02579214,  1.32690954]), 'targetState': array([27,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 95
reward sum = 0.1329101621299979
running average episode reward sum: 0.4245953236754455
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  5.]), 'previousTarget': array([27.,  5.]), 'currentState': array([26.02666721,  5.70043966,  5.30871628]), 'targetState': array([27,  5], dtype=int32), 'currentDistance': 1.1991632207883753}
episode index:835
target Thresh 31.993948249128227
target distance 2.0
model initialize at round 835
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 27.]), 'previousTarget': array([ 5., 27.]), 'currentState': array([ 7.        , 27.        ,  2.04403535]), 'targetState': array([ 5, 27], dtype=int32), 'currentDistance': 2.0}
done in step count: 9
reward sum = 0.8921948501550239
running average episode reward sum: 0.4251546532525742
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 27.]), 'previousTarget': array([ 5., 27.]), 'currentState': array([ 5.90873248, 27.2885267 ,  3.27189178]), 'targetState': array([ 5, 27], dtype=int32), 'currentDistance': 0.9534371379202958}
episode index:836
target Thresh 31.994008465055508
target distance 4.0
model initialize at round 836
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 24.]), 'previousTarget': array([23., 24.]), 'currentState': array([19.        , 21.        ,  2.49110317]), 'targetState': array([23, 24], dtype=int32), 'currentDistance': 5.000000000000001}
done in step count: 17
reward sum = 0.7619180880996241
running average episode reward sum: 0.4255569990528693
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 24.]), 'previousTarget': array([23., 24.]), 'currentState': array([22.01992518, 23.9341555 ,  0.42941161]), 'targetState': array([23, 24], dtype=int32), 'currentDistance': 0.9822841485984916}
episode index:837
target Thresh 31.994068081824306
target distance 10.0
model initialize at round 837
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([ 9.84740303, 17.54038967,  4.61251405]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 10.047780983018502}
done in step count: 21
reward sum = 0.6353345584600608
running average episode reward sum: 0.4258073302693457
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([12.38335456,  8.82686307,  5.11443288]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 1.0314815226535674}
episode index:838
target Thresh 31.994127105396338
target distance 24.0
model initialize at round 838
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.16181997, 23.58864597]), 'previousTarget': array([22.2, 23.6]), 'currentState': array([ 2.96205273, 17.98784801,  3.44460614]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 96
reward sum = 0.15547148227576799
running average episode reward sum: 0.4254851182931912
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 25.]), 'previousTarget': array([27., 25.]), 'currentState': array([26.00922439, 25.5078521 ,  6.05317311]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 1.1133508262005205}
episode index:839
target Thresh 31.99418554167402
target distance 7.0
model initialize at round 839
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 23.]), 'previousTarget': array([25., 23.]), 'currentState': array([21.28909969, 16.00196513,  0.25718429]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 7.9210651555181295}
done in step count: 20
reward sum = 0.684705016155348
running average episode reward sum: 0.4257937134096938
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 23.]), 'previousTarget': array([25., 23.]), 'currentState': array([24.92268985, 22.02831357,  1.04026633]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 0.9747570901091571}
episode index:840
target Thresh 31.994243396501023
target distance 16.0
model initialize at round 840
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  7.]), 'previousTarget': array([21.,  7.]), 'currentState': array([5.17468756, 5.92073003, 6.00186414]), 'targetState': array([21,  7], dtype=int32), 'currentDistance': 15.862072298315306}
done in step count: 49
reward sum = 0.4376685899756019
running average episode reward sum: 0.425807833358048
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  7.]), 'previousTarget': array([21.,  7.]), 'currentState': array([20.04854   ,  7.41888488,  6.26417495]), 'targetState': array([21,  7], dtype=int32), 'currentDistance': 1.0395867778232963}
episode index:841
target Thresh 31.994300675662878
target distance 19.0
model initialize at round 841
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.91722021, 18.35609413]), 'previousTarget': array([22.88271492, 18.29822396]), 'currentState': array([9.99965785, 3.08728334, 1.38230723]), 'targetState': array([26, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.08091016575011259
running average episode reward sum: 0.4252060305087509
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 22.]), 'previousTarget': array([26., 22.]), 'currentState': array([13.77074562,  7.15789871,  0.1734182 ]), 'targetState': array([26, 22], dtype=int32), 'currentDistance': 19.23129307770469}
episode index:842
target Thresh 31.994357384887547
target distance 10.0
model initialize at round 842
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  6.]), 'previousTarget': array([13.,  6.]), 'currentState': array([22.92974944, 12.85008667,  4.11619261]), 'targetState': array([13,  6], dtype=int32), 'currentDistance': 12.063316759510288}
done in step count: 25
reward sum = 0.5920890823279044
running average episode reward sum: 0.42540399379679256
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  6.]), 'previousTarget': array([13.,  6.]), 'currentState': array([13.81995232,  6.95182246,  4.01102137]), 'targetState': array([13,  6], dtype=int32), 'currentDistance': 1.256299244569221}
episode index:843
target Thresh 31.994413529846007
target distance 15.0
model initialize at round 843
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 25.]), 'previousTarget': array([25., 25.]), 'currentState': array([10.        , 21.        ,  4.07506111]), 'targetState': array([25, 25], dtype=int32), 'currentDistance': 15.524174696260024}
done in step count: 81
reward sum = 0.22214883972276453
running average episode reward sum: 0.42516317015452476
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 25.]), 'previousTarget': array([25., 25.]), 'currentState': array([24.00828452, 24.3925083 ,  0.46750825]), 'targetState': array([25, 25], dtype=int32), 'currentDistance': 1.1629900057179048}
episode index:844
target Thresh 31.994469116152793
target distance 18.0
model initialize at round 844
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([21.72807464,  9.82730615,  3.56037566]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 18.37355477296955}
done in step count: 99
reward sum = -0.4630940287660744
running average episode reward sum: 0.42411197820313945
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([6.2316546 , 5.51908236, 3.40524474]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 2.2912286525568937}
episode index:845
target Thresh 31.994524149366587
target distance 8.0
model initialize at round 845
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([11.63734689,  9.15929723,  2.6452677 ]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 9.042204979280497}
done in step count: 18
reward sum = 0.6872678834577985
running average episode reward sum: 0.42442303719280217
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([ 4.79110096, 13.55436895,  2.57354848]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 0.9079800422797324}
episode index:846
target Thresh 31.994578634990756
target distance 3.0
model initialize at round 846
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 10.]), 'previousTarget': array([20., 10.]), 'currentState': array([20.89762268,  7.00426495,  2.97628307]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 3.1273239287082286}
done in step count: 10
reward sum = 0.8660308035044708
running average episode reward sum: 0.4249444159015527
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 10.]), 'previousTarget': array([20., 10.]), 'currentState': array([19.83240198,  9.02477613,  1.7410146 ]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 0.9895204375823158}
episode index:847
target Thresh 31.994632578473905
target distance 22.0
model initialize at round 847
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7.70226409, 23.81660336]), 'previousTarget': array([ 7.70226409, 23.81660336]), 'currentState': array([5.        , 4.        , 2.03220707]), 'targetState': array([ 8, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.28192065860482113
running average episode reward sum: 0.4247757558104009
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 26.]), 'previousTarget': array([ 8., 26.]), 'currentState': array([ 7.94785168, 25.01340355,  1.5632523 ]), 'targetState': array([ 8, 26], dtype=int32), 'currentDistance': 0.9879736892331459}
episode index:848
target Thresh 31.994685985210428
target distance 16.0
model initialize at round 848
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 26.]), 'previousTarget': array([13., 26.]), 'currentState': array([ 7.        , 10.        ,  4.57958537]), 'targetState': array([13, 26], dtype=int32), 'currentDistance': 17.088007490635064}
done in step count: 51
reward sum = 0.33220130191797065
running average episode reward sum: 0.42466671640652287
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 26.]), 'previousTarget': array([13., 26.]), 'currentState': array([12.10366998, 25.11721799,  1.47016559]), 'targetState': array([13, 26], dtype=int32), 'currentDistance': 1.2580586532712872}
episode index:849
target Thresh 31.99473886054104
target distance 10.0
model initialize at round 849
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 18.]), 'previousTarget': array([21., 18.]), 'currentState': array([17.19252752, 27.85894831,  5.49557722]), 'targetState': array([21, 18], dtype=int32), 'currentDistance': 10.568619037893493}
done in step count: 29
reward sum = 0.5788881214204813
running average episode reward sum: 0.42484815335359816
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 18.]), 'previousTarget': array([21., 18.]), 'currentState': array([21.36014273, 18.99811334,  5.02689719]), 'targetState': array([21, 18], dtype=int32), 'currentDistance': 1.0610999086963364}
episode index:850
target Thresh 31.994791209753327
target distance 7.0
model initialize at round 850
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 29.]), 'previousTarget': array([11., 29.]), 'currentState': array([13.63999798, 22.31974138,  2.32223082]), 'targetState': array([11, 29], dtype=int32), 'currentDistance': 7.182996903986733}
done in step count: 21
reward sum = 0.6945946466792092
running average episode reward sum: 0.42516512925644845
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 29.]), 'previousTarget': array([11., 29.]), 'currentState': array([10.48392251, 28.052042  ,  1.43349165]), 'targetState': array([11, 29], dtype=int32), 'currentDistance': 1.0793332860230982}
episode index:851
target Thresh 31.994843038082248
target distance 21.0
model initialize at round 851
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.37960957, 21.14723806]), 'previousTarget': array([14.32455532, 20.97366596]), 'currentState': array([8.02923642, 2.18219756, 1.48408663]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.2973934944701908
running average episode reward sum: 0.42501516254895283
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 23.]), 'previousTarget': array([15., 23.]), 'currentState': array([15.12807818, 22.04121632,  1.24839325]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 0.9673004516626443}
episode index:852
target Thresh 31.994894350710677
target distance 19.0
model initialize at round 852
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.0943677 , 21.74245404]), 'previousTarget': array([13.08589282, 21.76686234]), 'currentState': array([19.97522979,  2.96337694,  3.86520863]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.27599637262301563
running average episode reward sum: 0.4248404629124629
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'previousTarget': array([13., 22.]), 'currentState': array([12.7014888 , 21.03463032,  1.82723019]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 1.0104689760418561}
episode index:853
target Thresh 31.994945152769926
target distance 11.0
model initialize at round 853
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([11.00196972, 11.02359845,  1.25348917]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 10.99805560026796}
done in step count: 24
reward sum = 0.5946699613511485
running average episode reward sum: 0.4250393264937728
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([21.04927355, 11.07925787,  6.02386574]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 0.95402441898323}
episode index:854
target Thresh 31.994995449340237
target distance 5.0
model initialize at round 854
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([22.09275451,  7.14790456,  0.82992175]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 5.245123050423718}
done in step count: 12
reward sum = 0.8072848044302032
running average episode reward sum: 0.42548639722820136
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([26.0731899 ,  9.00958582,  0.19603685]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 0.9268596670503924}
episode index:855
target Thresh 31.995045245451312
target distance 14.0
model initialize at round 855
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([12.9629121 , 20.95674474,  4.23663074]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 14.069655191762278}
done in step count: 30
reward sum = 0.47421584079763596
running average episode reward sum: 0.42554332414825913
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([26.07879143, 20.15829407,  0.06240094]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 0.9347097105755715}
episode index:856
target Thresh 31.995094546082804
target distance 11.0
model initialize at round 856
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 15.]), 'previousTarget': array([ 8., 15.]), 'currentState': array([17.0178506 ,  4.03199405,  1.29985613]), 'targetState': array([ 8, 15], dtype=int32), 'currentDistance': 14.199252934865132}
done in step count: 32
reward sum = 0.5017284038371264
running average episode reward sum: 0.4256322215574644
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 15.]), 'previousTarget': array([ 8., 15.]), 'currentState': array([ 8.94048035, 14.44765407,  2.31623104]), 'targetState': array([ 8, 15], dtype=int32), 'currentDistance': 1.0906829612969147}
episode index:857
target Thresh 31.995143356164817
target distance 13.0
model initialize at round 857
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 10.]), 'previousTarget': array([18., 10.]), 'currentState': array([ 8.23449934, 22.88309621,  5.5803068 ]), 'targetState': array([18, 10], dtype=int32), 'currentDistance': 16.16598809270244}
done in step count: 39
reward sum = 0.45871525567437954
running average episode reward sum: 0.42567077987228596
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 10.]), 'previousTarget': array([18., 10.]), 'currentState': array([17.37458721, 10.98135026,  5.35758657]), 'targetState': array([18, 10], dtype=int32), 'currentDistance': 1.1636964792248161}
episode index:858
target Thresh 31.995191680578397
target distance 10.0
model initialize at round 858
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 23.]), 'previousTarget': array([21., 23.]), 'currentState': array([11.17076474, 13.1550188 ,  0.79056296]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 13.911776323285169}
done in step count: 32
reward sum = 0.5322131195363454
running average episode reward sum: 0.42579481053545715
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 23.]), 'previousTarget': array([21., 23.]), 'currentState': array([20.58449816, 22.00296698,  0.85300296]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 1.0801465714268468}
episode index:859
target Thresh 31.99523952415603
target distance 17.0
model initialize at round 859
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([18.99610282, 27.00055233,  3.19507629]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 17.463894678797256}
done in step count: 42
reward sum = 0.40263345557684266
running average episode reward sum: 0.4257678787273657
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.14753432, 10.98372399,  4.50658053]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.9947257246480006}
episode index:860
target Thresh 31.99528689168211
target distance 13.0
model initialize at round 860
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 8.]), 'previousTarget': array([9., 8.]), 'currentState': array([16.97735278, 21.03788346,  2.34619075]), 'targetState': array([9, 8], dtype=int32), 'currentDistance': 15.284782053014526}
done in step count: 38
reward sum = 0.44396066881649704
running average episode reward sum: 0.4257890085648676
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 8.]), 'previousTarget': array([9., 8.]), 'currentState': array([9.91562768, 8.99072718, 4.33431558]), 'targetState': array([9, 8], dtype=int32), 'currentDistance': 1.3490420235906093}
episode index:861
target Thresh 31.995333787893433
target distance 2.0
model initialize at round 861
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'previousTarget': array([16., 23.]), 'currentState': array([14.99985565, 25.00067955,  1.85327151]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 2.2367403529149277}
done in step count: 16
reward sum = 0.7985851509665456
running average episode reward sum: 0.42622148668830345
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'previousTarget': array([16., 23.]), 'currentState': array([15.03312489, 23.30693201,  5.49597322]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 1.0144233508473715}
episode index:862
target Thresh 31.995380217479653
target distance 14.0
model initialize at round 862
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  5.]), 'previousTarget': array([24.,  5.]), 'currentState': array([24.        , 19.        ,  3.66749913]), 'targetState': array([24,  5], dtype=int32), 'currentDistance': 14.0}
done in step count: 31
reward sum = 0.49885060324539743
running average episode reward sum: 0.4263056455719154
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  5.]), 'previousTarget': array([24.,  5.]), 'currentState': array([24.08501742,  5.99980508,  4.9444269 ]), 'targetState': array([24,  5], dtype=int32), 'currentDistance': 1.0034132553089559}
episode index:863
target Thresh 31.995426185083772
target distance 16.0
model initialize at round 863
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  4.]), 'previousTarget': array([23.,  4.]), 'currentState': array([20.14120948, 19.73697187,  5.12761757]), 'targetState': array([23,  4], dtype=int32), 'currentDistance': 15.994529277799371}
done in step count: 42
reward sum = 0.45955482227616984
running average episode reward sum: 0.4263441284153231
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  4.]), 'previousTarget': array([23.,  4.]), 'currentState': array([22.8248272 ,  4.843007  ,  5.12014559]), 'targetState': array([23,  4], dtype=int32), 'currentDistance': 0.8610147003644881}
episode index:864
target Thresh 31.99547169530259
target distance 8.0
model initialize at round 864
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 15.]), 'previousTarget': array([18., 15.]), 'currentState': array([26.02770479, 10.97496252,  5.3350175 ]), 'targetState': array([18, 15], dtype=int32), 'currentDistance': 8.980254497091796}
done in step count: 26
reward sum = 0.6056953510806073
running average episode reward sum: 0.4265514708692714
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 15.]), 'previousTarget': array([18., 15.]), 'currentState': array([18.66366609, 14.01556304,  2.95861066]), 'targetState': array([18, 15], dtype=int32), 'currentDistance': 1.1872526321120789}
episode index:865
target Thresh 31.995516752687163
target distance 6.0
model initialize at round 865
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([15.        ,  9.        ,  0.60083354]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 6.7082039324993685}
done in step count: 14
reward sum = 0.7508441019322304
running average episode reward sum: 0.42692594272962126
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([20.3411506 ,  6.99733969,  5.68378159]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 1.1953112528079923}
episode index:866
target Thresh 31.99556136174327
target distance 10.0
model initialize at round 866
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([ 3.03000342, 20.99781984,  5.97244332]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 13.431948491357682}
done in step count: 28
reward sum = 0.5350628834287372
running average episode reward sum: 0.4270506681514195
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.5624788 , 11.93835235,  5.78404633]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.0353404939922004}
episode index:867
target Thresh 31.99560552693185
target distance 5.0
model initialize at round 867
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 24.]), 'previousTarget': array([10., 24.]), 'currentState': array([ 8.90133381, 19.26239694,  1.80216843]), 'targetState': array([10, 24], dtype=int32), 'currentDistance': 4.8633270684396805}
done in step count: 10
reward sum = 0.8326985297516929
running average episode reward sum: 0.42751800439750276
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 24.]), 'previousTarget': array([10., 24.]), 'currentState': array([ 9.48546498, 23.06046075,  1.17102595]), 'targetState': array([10, 24], dtype=int32), 'currentDistance': 1.0712050619337186}
episode index:868
target Thresh 31.995649252669466
target distance 9.0
model initialize at round 868
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 23.]), 'previousTarget': array([ 4., 23.]), 'currentState': array([13.        , 18.        ,  5.76740459]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 10.295630140986999}
done in step count: 26
reward sum = 0.5613302771735157
running average episode reward sum: 0.42767198860092737
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 23.]), 'previousTarget': array([ 4., 23.]), 'currentState': array([ 4.90217474, 22.68986071,  2.73805682]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 0.9539945720782257}
episode index:869
target Thresh 31.99569254332872
target distance 9.0
model initialize at round 869
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 28.]), 'previousTarget': array([12., 28.]), 'currentState': array([15.97687916, 19.49695366,  1.79267305]), 'targetState': array([12, 28], dtype=int32), 'currentDistance': 9.387085010734706}
done in step count: 18
reward sum = 0.6740043148095543
running average episode reward sum: 0.4279551292057649
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 28.]), 'previousTarget': array([12., 28.]), 'currentState': array([12.54871333, 27.00498187,  2.29181875]), 'targetState': array([12, 28], dtype=int32), 'currentDistance': 1.1362866732473889}
episode index:870
target Thresh 31.995735403238715
target distance 13.0
model initialize at round 870
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([22.64374566,  7.26221479,  2.65269765]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 14.326969457489596}
done in step count: 37
reward sum = 0.5056881785266298
running average episode reward sum: 0.4280443749569943
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([10.91801519, 13.13122745,  2.81498678]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 1.2639294359873807}
episode index:871
target Thresh 31.995777836685484
target distance 21.0
model initialize at round 871
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.24759706, 26.50109098]), 'previousTarget': array([24.23047895, 26.49442256]), 'currentState': array([ 5.01197259, 21.02470977,  0.86710238]), 'targetState': array([26, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.2943865034111031
running average episode reward sum: 0.4278910975813683
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 27.]), 'previousTarget': array([26., 27.]), 'currentState': array([25.05439607, 27.30019497,  0.54685918]), 'targetState': array([26, 27], dtype=int32), 'currentDistance': 0.9921107886667877}
episode index:872
target Thresh 31.9958198479124
target distance 13.0
model initialize at round 872
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 18.]), 'previousTarget': array([12., 18.]), 'currentState': array([24.98210636,  8.98433048,  3.61730507]), 'targetState': array([12, 18], dtype=int32), 'currentDistance': 15.805612371290342}
done in step count: 39
reward sum = 0.4430565317944445
running average episode reward sum: 0.4279084692127693
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 18.]), 'previousTarget': array([12., 18.]), 'currentState': array([12.78978153, 17.93846052,  2.73138378]), 'targetState': array([12, 18], dtype=int32), 'currentDistance': 0.7921754640408891}
episode index:873
target Thresh 31.995861441120628
target distance 25.0
model initialize at round 873
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.24575629, 22.66701617]), 'previousTarget': array([22.33254095, 22.44774604]), 'currentState': array([26.79517429,  3.1913186 ,  2.14950302]), 'targetState': array([21, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.20501153486351503
running average episode reward sum: 0.42765343839543607
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 28.]), 'previousTarget': array([21., 28.]), 'currentState': array([20.58877334, 27.19753602,  1.67689721]), 'targetState': array([21, 28], dtype=int32), 'currentDistance': 0.9016960690364216}
episode index:874
target Thresh 31.995902620469515
target distance 19.0
model initialize at round 874
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.07341346,  8.1966285 ]), 'previousTarget': array([12.08589282,  8.23313766]), 'currentState': array([19.0689554 , 26.93328757,  5.26666471]), 'targetState': array([12,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.3326980736622982
running average episode reward sum: 0.4275449179785982
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  8.]), 'previousTarget': array([12.,  8.]), 'currentState': array([12.32752986,  8.84005573,  4.69439018]), 'targetState': array([12,  8], dtype=int32), 'currentDistance': 0.9016481783236094}
episode index:875
target Thresh 31.995943390077038
target distance 22.0
model initialize at round 875
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.18935107, 26.91379568]), 'previousTarget': array([21.18928508, 26.91786413]), 'currentState': array([22.99718859,  6.99567018,  3.91849968]), 'targetState': array([21, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.25709906209432
running average episode reward sum: 0.42735034508375314
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 29.]), 'previousTarget': array([21., 29.]), 'currentState': array([20.69223599, 28.07962322,  1.42898065]), 'targetState': array([21, 29], dtype=int32), 'currentDistance': 0.970470042870608}
episode index:876
target Thresh 31.995983754020184
target distance 23.0
model initialize at round 876
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.38758108, 16.16789261]), 'previousTarget': array([21.41125677, 16.15885487]), 'currentState': array([ 3.97055238, 25.99881888,  3.42720273]), 'targetState': array([27, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 78
reward sum = 0.15750322230938868
running average episode reward sum: 0.4270426516712396
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 13.]), 'previousTarget': array([27., 13.]), 'currentState': array([26.0685969 , 13.96014025,  5.70469811]), 'targetState': array([27, 13], dtype=int32), 'currentDistance': 1.3376774721788154}
episode index:877
target Thresh 31.996023716335387
target distance 11.0
model initialize at round 877
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 15.]), 'previousTarget': array([18., 15.]), 'currentState': array([12.14594189,  4.30488557,  1.1355734 ]), 'targetState': array([18, 15], dtype=int32), 'currentDistance': 12.192434907142877}
done in step count: 27
reward sum = 0.5771124803885292
running average episode reward sum: 0.4272135740274096
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 15.]), 'previousTarget': array([18., 15.]), 'currentState': array([17.99432869, 14.0847089 ,  1.23961212]), 'targetState': array([18, 15], dtype=int32), 'currentDistance': 0.915308672814218}
episode index:878
target Thresh 31.996063281018913
target distance 12.0
model initialize at round 878
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 27.]), 'previousTarget': array([16., 27.]), 'currentState': array([21.83386264, 15.12527035,  2.38444097]), 'targetState': array([16, 27], dtype=int32), 'currentDistance': 13.230387658442286}
done in step count: 29
reward sum = 0.553851637339896
running average episode reward sum: 0.427357644634136
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 27.]), 'previousTarget': array([16., 27.]), 'currentState': array([16.47062793, 26.12795138,  2.01359358]), 'targetState': array([16, 27], dtype=int32), 'currentDistance': 0.9909386692899801}
episode index:879
target Thresh 31.996102452027255
target distance 13.0
model initialize at round 879
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 14.]), 'previousTarget': array([ 2., 14.]), 'currentState': array([15.00263071, 22.99196815,  4.77641106]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 15.808981509991268}
done in step count: 36
reward sum = 0.4562478107036069
running average episode reward sum: 0.42739047436830585
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 14.]), 'previousTarget': array([ 2., 14.]), 'currentState': array([ 2.90188589, 14.91428445,  3.84466067]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 1.2842562923560181}
episode index:880
target Thresh 31.996141233277555
target distance 15.0
model initialize at round 880
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 17.]), 'previousTarget': array([23., 17.]), 'currentState': array([24.99686944,  2.24828668,  1.58863426]), 'targetState': array([23, 17], dtype=int32), 'currentDistance': 14.886253168185279}
done in step count: 35
reward sum = 0.5088141928640848
running average episode reward sum: 0.4274828962962238
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 17.]), 'previousTarget': array([23., 17.]), 'currentState': array([23.17553857, 16.07195276,  1.98803321]), 'targetState': array([23, 17], dtype=int32), 'currentDistance': 0.9445027661141332}
episode index:881
target Thresh 31.99617962864797
target distance 4.0
model initialize at round 881
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 25.]), 'previousTarget': array([ 9., 25.]), 'currentState': array([ 5.40882524, 24.71679952,  5.8581866 ]), 'targetState': array([ 9, 25], dtype=int32), 'currentDistance': 3.6023240612138685}
done in step count: 8
reward sum = 0.8619631818017706
running average episode reward sum: 0.4279755043296769
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 25.]), 'previousTarget': array([ 9., 25.]), 'currentState': array([ 8.03665424, 24.79972238,  0.32038746]), 'targetState': array([ 9, 25], dtype=int32), 'currentDistance': 0.9839441990543208}
episode index:882
target Thresh 31.99621764197806
target distance 14.0
model initialize at round 882
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([17.97834323, 12.02970079,  2.44249603]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 16.120489273064123}
done in step count: 48
reward sum = 0.392087732578435
running average episode reward sum: 0.4279348613265611
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.98502598, 4.06073689, 3.93331693]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.9868967316764953}
episode index:883
target Thresh 31.9962552770692
target distance 22.0
model initialize at round 883
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.90815322,  6.0206292 ]), 'previousTarget': array([11.90815322,  6.0206292 ]), 'currentState': array([11.        , 26.        ,  6.01179913]), 'targetState': array([12,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.2878831662181071
running average episode reward sum: 0.4277764318072076
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  4.]), 'previousTarget': array([12.,  4.]), 'currentState': array([12.00347733,  4.92269438,  4.88900529]), 'targetState': array([12,  4], dtype=int32), 'currentDistance': 0.9227009313363883}
episode index:884
target Thresh 31.996292537684923
target distance 19.0
model initialize at round 884
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4.62681634, 20.68478446]), 'previousTarget': array([ 4.60711423, 20.69765531]), 'currentState': array([17.04747436,  5.00909455,  0.43332186]), 'targetState': array([ 2, 24], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 99
reward sum = -0.21440204387528375
running average episode reward sum: 0.4270508064109562
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 24.]), 'previousTarget': array([ 2., 24.]), 'currentState': array([ 2.78939667, 22.9709184 ,  2.20012686]), 'targetState': array([ 2, 24], dtype=int32), 'currentDistance': 1.296979584321572}
episode index:885
target Thresh 31.996329427551327
target distance 20.0
model initialize at round 885
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9.42781353, 25.56953382]), 'previousTarget': array([ 9.42781353, 25.56953382]), 'currentState': array([2.        , 7.        , 2.86254203]), 'targetState': array([10, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.1831646236372998
running average episode reward sum: 0.4263620756772674
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 27.]), 'previousTarget': array([10., 27.]), 'currentState': array([ 8.96091059, 26.51199051,  1.66951269]), 'targetState': array([10, 27], dtype=int32), 'currentDistance': 1.14798086596092}
episode index:886
target Thresh 31.996365950357426
target distance 15.0
model initialize at round 886
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 15.]), 'previousTarget': array([ 8., 15.]), 'currentState': array([22.84259564,  3.28060726,  2.33450139]), 'targetState': array([ 8, 15], dtype=int32), 'currentDistance': 18.911552331045353}
done in step count: 50
reward sum = 0.3765398136291837
running average episode reward sum: 0.4263059062724781
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 15.]), 'previousTarget': array([ 8., 15.]), 'currentState': array([ 8.94831254, 14.0707746 ,  2.50861767]), 'targetState': array([ 8, 15], dtype=int32), 'currentDistance': 1.3276884100737036}
episode index:887
target Thresh 31.996402109755536
target distance 6.0
model initialize at round 887
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  8.]), 'previousTarget': array([17.,  8.]), 'currentState': array([19.        ,  2.        ,  5.96424571]), 'targetState': array([17,  8], dtype=int32), 'currentDistance': 6.324555320336759}
done in step count: 16
reward sum = 0.7281272452539086
running average episode reward sum: 0.4266457951677275
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  8.]), 'previousTarget': array([17.,  8.]), 'currentState': array([17.67560983,  7.14713224,  2.0335461 ]), 'targetState': array([17,  8], dtype=int32), 'currentDistance': 1.088040468193897}
episode index:888
target Thresh 31.996437909361617
target distance 13.0
model initialize at round 888
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 27.]), 'previousTarget': array([16., 27.]), 'currentState': array([ 3.48937581, 21.10224004,  0.23267276]), 'targetState': array([16, 27], dtype=int32), 'currentDistance': 13.83109866502884}
done in step count: 29
reward sum = 0.532429128459674
running average episode reward sum: 0.42676478654375893
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 27.]), 'previousTarget': array([16., 27.]), 'currentState': array([15.08838128, 26.18224241,  0.85957177]), 'targetState': array([16, 27], dtype=int32), 'currentDistance': 1.2246534851798012}
episode index:889
target Thresh 31.99647335275567
target distance 22.0
model initialize at round 889
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8.09184678, 6.0206292 ]), 'previousTarget': array([8.09184678, 6.0206292 ]), 'currentState': array([ 9.       , 26.       ,  6.0850412]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.12150636733826312
running average episode reward sum: 0.42614875153939713
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 4.]), 'previousTarget': array([8., 4.]), 'currentState': array([10.19149215, 14.36411706,  4.49941163]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 10.593279013664883}
episode index:890
target Thresh 31.99650844348206
target distance 7.0
model initialize at round 890
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([12.        ,  2.        ,  2.76779607]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 7.280109889280518}
done in step count: 17
reward sum = 0.7290647817829522
running average episode reward sum: 0.4264887246373135
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.43966547,  8.09375175,  1.87866112]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 1.007269383368635}
episode index:891
target Thresh 31.996543185049887
target distance 5.0
model initialize at round 891
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([18.72349178,  5.00594509,  3.04939583]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 5.592471669693092}
done in step count: 11
reward sum = 0.8166651161440304
running average episode reward sum: 0.42692614211658114
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.85957898,  7.19983327,  2.35437818]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.1743691156053921}
episode index:892
target Thresh 31.99657758093334
target distance 17.0
model initialize at round 892
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  7.]), 'previousTarget': array([13.,  7.]), 'currentState': array([ 4.99910608, 24.02011887,  1.36269927]), 'targetState': array([13,  7], dtype=int32), 'currentDistance': 18.806880386689553}
done in step count: 54
reward sum = 0.3279939529996688
running average episode reward sum: 0.4268153557905824
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  7.]), 'previousTarget': array([13.,  7.]), 'currentState': array([12.10768916,  7.96597712,  5.25894392]), 'targetState': array([13,  7], dtype=int32), 'currentDistance': 1.315040084089561}
episode index:893
target Thresh 31.996611634572034
target distance 17.0
model initialize at round 893
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([20.83327515, 26.98086169,  3.25727375]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 17.554718320033682}
done in step count: 48
reward sum = 0.40570584293551915
running average episode reward sum: 0.42679174336009573
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([ 4.98544683, 21.87206407,  3.39554215]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 0.9937167930750201}
episode index:894
target Thresh 31.99664534937136
target distance 25.0
model initialize at round 894
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.20084688, 22.00892484]), 'previousTarget': array([15.20063923, 21.98401917]), 'currentState': array([16.00502015,  2.02509874,  1.37110414]), 'targetState': array([15, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.04466071116851286
running average episode reward sum: 0.42626498084106934
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 27.]), 'previousTarget': array([15., 27.]), 'currentState': array([18.57458301, 12.19813781,  1.4689976 ]), 'targetState': array([15, 27], dtype=int32), 'currentDistance': 15.227369044233507}
episode index:895
target Thresh 31.99667872870283
target distance 16.0
model initialize at round 895
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 29.]), 'previousTarget': array([11., 29.]), 'currentState': array([18.        , 13.        ,  2.74377745]), 'targetState': array([11, 29], dtype=int32), 'currentDistance': 17.46424919657298}
done in step count: 44
reward sum = 0.41413060277858904
running average episode reward sum: 0.4262514380084103
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 29.]), 'previousTarget': array([11., 29.]), 'currentState': array([10.69503848, 28.07297404,  2.12761452]), 'targetState': array([11, 29], dtype=int32), 'currentDistance': 0.9758989015367691}
episode index:896
target Thresh 31.996711775904398
target distance 17.0
model initialize at round 896
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([ 9.02866633, 26.03425131,  0.64382079]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 17.03427542585709}
done in step count: 48
reward sum = 0.3845344633720994
running average episode reward sum: 0.4262049307903097
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([8.69936065, 9.93461399, 5.00660261]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.9817776326737175}
episode index:897
target Thresh 31.996744494280822
target distance 14.0
model initialize at round 897
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  8.]), 'previousTarget': array([11.,  8.]), 'currentState': array([ 9.97968926, 22.00896425,  2.97322395]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 14.046071100776222}
done in step count: 37
reward sum = 0.4730115625311243
running average episode reward sum: 0.42625705398823927
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  8.]), 'previousTarget': array([11.,  8.]), 'currentState': array([10.78737686,  8.89530581,  5.12756731]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 0.9202070924348136}
episode index:898
target Thresh 31.99677688710396
target distance 8.0
model initialize at round 898
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 16.]), 'previousTarget': array([ 2., 16.]), 'currentState': array([10.        , 15.        ,  1.20342541]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 20
reward sum = 0.6667764042282721
running average episode reward sum: 0.42652459497849515
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 16.]), 'previousTarget': array([ 2., 16.]), 'currentState': array([ 2.90140395, 16.05992665,  3.28407964]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 0.903393755363374}
episode index:899
target Thresh 31.99680895761312
target distance 11.0
model initialize at round 899
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 28.]), 'previousTarget': array([16., 28.]), 'currentState': array([26.63685849, 21.08505447,  2.88014137]), 'targetState': array([16, 28], dtype=int32), 'currentDistance': 12.686970882368422}
done in step count: 30
reward sum = 0.5678208279831198
running average episode reward sum: 0.4266815907929447
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 28.]), 'previousTarget': array([16., 28.]), 'currentState': array([16.98451316, 27.64119015,  2.58315289]), 'targetState': array([16, 28], dtype=int32), 'currentDistance': 1.0478600421214035}
episode index:900
target Thresh 31.99684070901538
target distance 3.0
model initialize at round 900
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 7.]), 'previousTarget': array([9., 7.]), 'currentState': array([5.99719539, 5.09365952, 1.42383689]), 'targetState': array([9, 7], dtype=int32), 'currentDistance': 3.5568201447663648}
done in step count: 9
reward sum = 0.8662255437397632
running average episode reward sum: 0.42716943091830184
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 7.]), 'previousTarget': array([9., 7.]), 'currentState': array([8.02405155, 6.36250104, 0.32234719]), 'targetState': array([9, 7], dtype=int32), 'currentDistance': 1.1657102155031074}
episode index:901
target Thresh 31.996872144485913
target distance 3.0
model initialize at round 901
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 24.]), 'previousTarget': array([ 2., 24.]), 'currentState': array([ 5.        , 26.        ,  2.80531886]), 'targetState': array([ 2, 24], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 8
reward sum = 0.8719820896476699
running average episode reward sum: 0.42766257133817914
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 24.]), 'previousTarget': array([ 2., 24.]), 'currentState': array([ 2.9237012 , 24.75617125,  3.94243633]), 'targetState': array([ 2, 24], dtype=int32), 'currentDistance': 1.1937415426473559}
episode index:902
target Thresh 31.996903267168285
target distance 14.0
model initialize at round 902
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([23.99811623, 11.99646872,  3.96983814]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 14.320879482086243}
done in step count: 40
reward sum = 0.4415671635837967
running average episode reward sum: 0.42767796955772025
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([21.00234801, 25.11950471,  1.68189589]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 0.8804984216322252}
episode index:903
target Thresh 31.996934080174796
target distance 11.0
model initialize at round 903
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 18.]), 'previousTarget': array([25., 18.]), 'currentState': array([21.9928532 , 28.98872647,  4.39988327]), 'targetState': array([25, 18], dtype=int32), 'currentDistance': 11.392762667774747}
done in step count: 28
reward sum = 0.588285987734652
running average episode reward sum: 0.42785563329464166
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 18.]), 'previousTarget': array([25., 18.]), 'currentState': array([24.37632785, 18.95819077,  5.11100299]), 'targetState': array([25, 18], dtype=int32), 'currentDistance': 1.1432832136421087}
episode index:904
target Thresh 31.99696458658677
target distance 19.0
model initialize at round 904
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  6.]), 'previousTarget': array([26.,  6.]), 'currentState': array([24.07772436, 24.90303181,  5.26197778]), 'targetState': array([26,  6], dtype=int32), 'currentDistance': 19.000519866684787}
done in step count: 48
reward sum = 0.38553039872944717
running average episode reward sum: 0.42780886507965243
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  6.]), 'previousTarget': array([26.,  6.]), 'currentState': array([25.74856786,  6.88782427,  4.8791753 ]), 'targetState': array([26,  6], dtype=int32), 'currentDistance': 0.9227405103309718}
episode index:905
target Thresh 31.99699478945487
target distance 6.0
model initialize at round 905
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 5.]), 'previousTarget': array([7., 5.]), 'currentState': array([ 7.04011821, 10.98235356,  5.66879839]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 5.982488074981641}
done in step count: 14
reward sum = 0.7669770853705726
running average episode reward sum: 0.4281832229386932
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 5.]), 'previousTarget': array([7., 5.]), 'currentState': array([6.8959471 , 5.83547531, 4.70873381]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 0.8419299262728931}
episode index:906
target Thresh 31.997024691799414
target distance 9.0
model initialize at round 906
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 18.]), 'previousTarget': array([26., 18.]), 'currentState': array([16.98549925, 25.05723499,  1.56642985]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 11.448396810903578}
done in step count: 29
reward sum = 0.5416733158104858
running average episode reward sum: 0.42830834983270843
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 18.]), 'previousTarget': array([26., 18.]), 'currentState': array([25.04597401, 18.49747779,  5.77521909]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 1.0759413268861477}
episode index:907
target Thresh 31.997054296610656
target distance 21.0
model initialize at round 907
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.25775784,  7.35322867]), 'previousTarget': array([19.25775784,  7.35322867]), 'currentState': array([23.        , 27.        ,  5.52286965]), 'targetState': array([19,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.29878710561601396
running average episode reward sum: 0.42816570529061954
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  6.]), 'previousTarget': array([19.,  6.]), 'currentState': array([19.10724416,  6.97458235,  4.47327939]), 'targetState': array([19,  6], dtype=int32), 'currentDistance': 0.9804652251733152}
episode index:908
target Thresh 31.997083606849106
target distance 20.0
model initialize at round 908
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5.22127294, 24.03319094]), 'previousTarget': array([ 5.22127294, 24.03319094]), 'currentState': array([25.        , 27.        ,  4.02470002]), 'targetState': array([ 5, 24], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.3420958865642766
running average episode reward sum: 0.4280710190213936
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 24.]), 'previousTarget': array([ 5., 24.]), 'currentState': array([ 5.87541384, 24.0665927 ,  3.36675046]), 'targetState': array([ 5, 24], dtype=int32), 'currentDistance': 0.8779430372306464}
episode index:909
target Thresh 31.997112625445812
target distance 8.0
model initialize at round 909
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  8.]), 'previousTarget': array([19.,  8.]), 'currentState': array([11.1240857 ,  4.2276177 ,  0.92783234]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 8.732748382917764}
done in step count: 18
reward sum = 0.6960086901211083
running average episode reward sum: 0.4283654560226021
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  8.]), 'previousTarget': array([19.,  8.]), 'currentState': array([18.06614901,  7.42244758,  0.65813226]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 1.0980184246005915}
episode index:910
target Thresh 31.997141355302652
target distance 1.0
model initialize at round 910
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.19331196, 3.17985979, 0.81121306]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.1503806188805328}
done in step count: 0
reward sum = 0.9966077910788798
running average episode reward sum: 0.42898921270213697
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.19331196, 3.17985979, 0.81121306]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.1503806188805328}
episode index:911
target Thresh 31.997169799292642
target distance 10.0
model initialize at round 911
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([22.04261191, 13.81557401,  4.85877079]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 11.035099822397004}
done in step count: 23
reward sum = 0.6189302358340938
running average episode reward sum: 0.42919748136785185
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([17.62610539,  4.89513366,  4.03135915]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 1.092370000595003}
episode index:912
target Thresh 31.997197960260205
target distance 23.0
model initialize at round 912
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.13125551,  8.01887683]), 'previousTarget': array([23.13125551,  8.01887683]), 'currentState': array([24.        , 28.        ,  3.46126044]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 74
reward sum = 0.24556406091459013
running average episode reward sum: 0.4289963494725033
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  5.]), 'previousTarget': array([23.,  5.]), 'currentState': array([22.81412676,  5.90841975,  5.02123758]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 0.9272406906499825}
episode index:913
target Thresh 31.997225841021457
target distance 8.0
model initialize at round 913
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 13.]), 'previousTarget': array([ 8., 13.]), 'currentState': array([7.18983857, 5.46044422, 1.33458319]), 'targetState': array([ 8, 13], dtype=int32), 'currentDistance': 7.582958712154914}
done in step count: 15
reward sum = 0.7290605362839947
running average episode reward sum: 0.42932464726989006
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 13.]), 'previousTarget': array([ 8., 13.]), 'currentState': array([ 8.24350085, 12.07426815,  1.60615177]), 'targetState': array([ 8, 13], dtype=int32), 'currentDistance': 0.9572210425491258}
episode index:914
target Thresh 31.9972534443645
target distance 14.0
model initialize at round 914
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([23.70110711, 24.9253073 ,  3.53829148]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 17.562114814105453}
done in step count: 48
reward sum = 0.4117806909302716
running average episode reward sum: 0.42930547354711457
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.12823924, 11.96000498,  4.33770014]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.9685323253110829}
episode index:915
target Thresh 31.99728077304969
target distance 21.0
model initialize at round 915
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9.67468862, 8.7918067 ]), 'previousTarget': array([9.87913569, 9.0913656 ]), 'currentState': array([21.84534307, 24.66238231,  4.25963185]), 'targetState': array([6, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.2008300623352866
running average episode reward sum: 0.42905604624229815
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 4.]), 'previousTarget': array([6., 4.]), 'currentState': array([6.25126284, 4.87936608, 4.12428016]), 'targetState': array([6, 4], dtype=int32), 'currentDistance': 0.9145587565724377}
episode index:916
target Thresh 31.99730782980992
target distance 22.0
model initialize at round 916
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.92680982, 11.81056399]), 'previousTarget': array([24.91786413, 11.81071492]), 'currentState': array([5.00978298, 9.99066236, 5.60405262]), 'targetState': array([27, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 80
reward sum = 0.2711789773295872
running average episode reward sum: 0.42888387931872923
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 12.]), 'previousTarget': array([27., 12.]), 'currentState': array([26.08578654, 11.64274418,  0.53091004]), 'targetState': array([27, 12], dtype=int32), 'currentDistance': 0.9815385745526141}
episode index:917
target Thresh 31.997334617350887
target distance 15.0
model initialize at round 917
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.64636501, 28.62110536]), 'previousTarget': array([26.64636501, 28.62110536]), 'currentState': array([13.        , 14.        ,  5.43304062]), 'targetState': array([27, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 68
reward sum = 0.28673059169062987
running average episode reward sum: 0.4287290282428816
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 29.]), 'previousTarget': array([27., 29.]), 'currentState': array([26.87824498, 28.05483443,  1.11411734]), 'targetState': array([27, 29], dtype=int32), 'currentDistance': 0.95297547132058}
episode index:918
target Thresh 31.99736113835137
target distance 6.0
model initialize at round 918
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  6.]), 'previousTarget': array([20.,  6.]), 'currentState': array([18.09742214, 11.95515571,  5.66632497]), 'targetState': array([20,  6], dtype=int32), 'currentDistance': 6.251694338253151}
done in step count: 13
reward sum = 0.77157232490801
running average episode reward sum: 0.42910208950149437
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  6.]), 'previousTarget': array([20.,  6.]), 'currentState': array([20.00903246,  6.83004328,  4.64598933]), 'targetState': array([20,  6], dtype=int32), 'currentDistance': 0.830092425514749}
episode index:919
target Thresh 31.997387395463484
target distance 12.0
model initialize at round 919
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([14.08261987, 21.94488022,  5.47314924]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 11.993841302210875}
done in step count: 31
reward sum = 0.560388681959675
running average episode reward sum: 0.42924479231938373
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([12.78707352, 10.9742331 ,  4.71183278]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.9972300707207263}
episode index:920
target Thresh 31.99741339131297
target distance 20.0
model initialize at round 920
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3.7615699 , 20.79270645]), 'previousTarget': array([ 3.7615699 , 20.79270645]), 'currentState': array([22.        , 29.        ,  1.51170757]), 'targetState': array([ 2, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 81
reward sum = 0.2383910097539199
running average episode reward sum: 0.42903756779976865
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 20.]), 'previousTarget': array([ 2., 20.]), 'currentState': array([ 2.89190861, 20.68410384,  3.80372843]), 'targetState': array([ 2, 20], dtype=int32), 'currentDistance': 1.1240547244140258}
episode index:921
target Thresh 31.997439128499433
target distance 8.0
model initialize at round 921
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([ 6.        , 17.        ,  2.57150558]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 10.0}
done in step count: 28
reward sum = 0.5960151633725117
running average episode reward sum: 0.4292186714826024
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([13.09577035, 22.45943939,  0.47028038]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 1.0534880342859179}
episode index:922
target Thresh 31.99746460959661
target distance 9.0
model initialize at round 922
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([15.79095644, 14.79064308,  3.83452493]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 10.526749855233758}
done in step count: 22
reward sum = 0.645010385987296
running average episode reward sum: 0.4294524653228025
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([7.68395345, 9.84996944, 3.8104464 ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 1.0909813825541865}
episode index:923
target Thresh 31.997489837152635
target distance 21.0
model initialize at round 923
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4.13795588, 7.10758138]), 'previousTarget': array([4.09009055, 7.10381815]), 'currentState': array([24.04917405,  8.989971  ,  5.82949557]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.2693981115274682
running average episode reward sum: 0.42927924632518855
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.98657879, 6.84648497, 2.98643296]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.9984510871365282}
episode index:924
target Thresh 31.997514813690284
target distance 13.0
model initialize at round 924
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([25.        ,  5.        ,  0.67299454]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 13.038404810405298}
done in step count: 34
reward sum = 0.49479570121662303
running average episode reward sum: 0.42935007492507116
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([12.96379591,  5.75604046,  3.42298641]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 0.9941925387467854}
episode index:925
target Thresh 31.99753954170723
target distance 17.0
model initialize at round 925
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.33935727, 11.46633605]), 'previousTarget': array([23.33935727, 11.46633605]), 'currentState': array([ 7.        , 23.        ,  3.58221722]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.27859628290287775
running average episode reward sum: 0.4291872738537729
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 11.]), 'previousTarget': array([24., 11.]), 'currentState': array([23.1252074 , 11.55648315,  5.72915549]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 1.0367910014186337}
episode index:926
target Thresh 31.997564023676293
target distance 9.0
model initialize at round 926
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  9.]), 'previousTarget': array([24.,  9.]), 'currentState': array([21.0074942 , 18.0383706 ,  1.13122448]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 9.520884104847124}
done in step count: 24
reward sum = 0.6008174360267602
running average episode reward sum: 0.4293724196597848
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  9.]), 'previousTarget': array([24.,  9.]), 'currentState': array([23.19768739,  9.95476762,  5.1158982 ]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 1.247111351511628}
episode index:927
target Thresh 31.9975882620457
target distance 10.0
model initialize at round 927
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 16.]), 'previousTarget': array([10., 16.]), 'currentState': array([20.00638168, 10.00373451,  0.7404767 ]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 11.665456452099193}
done in step count: 28
reward sum = 0.5625729906608925
running average episode reward sum: 0.4295159547578463
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 16.]), 'previousTarget': array([10., 16.]), 'currentState': array([10.84617185, 15.33019226,  2.63230991]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 1.0791891388580803}
episode index:928
target Thresh 31.997612259239297
target distance 16.0
model initialize at round 928
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 20.]), 'previousTarget': array([ 4., 20.]), 'currentState': array([7.06471   , 3.9873186 , 0.05897969]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 16.303325215816514}
done in step count: 45
reward sum = 0.4171788267592383
running average episode reward sum: 0.4295026747492364
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 20.]), 'previousTarget': array([ 4., 20.]), 'currentState': array([ 4.43224712, 19.02590684,  2.01044959]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 1.0656899471839782}
episode index:929
target Thresh 31.99763601765683
target distance 10.0
model initialize at round 929
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 17.]), 'previousTarget': array([ 4., 17.]), 'currentState': array([13.6243965 , 16.01501178,  3.03321035]), 'targetState': array([ 4, 17], dtype=int32), 'currentDistance': 9.674668458983858}
done in step count: 22
reward sum = 0.660443978088386
running average episode reward sum: 0.4297509987313215
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 17.]), 'previousTarget': array([ 4., 17.]), 'currentState': array([ 4.99948453, 17.14965712,  3.11714313]), 'targetState': array([ 4, 17], dtype=int32), 'currentDistance': 1.0106268283725872}
episode index:930
target Thresh 31.99765953967416
target distance 18.0
model initialize at round 930
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([6.00246173, 2.99190302, 5.21812299]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 18.024851046966788}
done in step count: 48
reward sum = 0.3887003188471607
running average episode reward sum: 0.4297069056272569
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([23.07972668,  1.90528897,  6.21505151]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 0.9251341307269185}
episode index:931
target Thresh 31.997682827643505
target distance 10.0
model initialize at round 931
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 18.]), 'previousTarget': array([14., 18.]), 'currentState': array([24.        , 10.        ,  0.85210308]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 12.806248474865697}
done in step count: 34
reward sum = 0.5356077603490152
running average episode reward sum: 0.4298205331537824
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 18.]), 'previousTarget': array([14., 18.]), 'currentState': array([14.90256294, 17.26356232,  2.44904594]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 1.1648863937419038}
episode index:932
target Thresh 31.997705883893683
target distance 16.0
model initialize at round 932
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 24.]), 'previousTarget': array([23., 24.]), 'currentState': array([11.       ,  8.       ,  6.1010631]), 'targetState': array([23, 24], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.33229455807031033
running average episode reward sum: 0.4297160037056758
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 24.]), 'previousTarget': array([23., 24.]), 'currentState': array([22.10668249, 23.45830364,  0.62469704]), 'targetState': array([23, 24], dtype=int32), 'currentDistance': 1.044725375873518}
episode index:933
target Thresh 31.99772871073034
target distance 15.0
model initialize at round 933
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  8.]), 'previousTarget': array([26.,  8.]), 'currentState': array([11.30972333,  8.81500129,  5.9258073 ]), 'targetState': array([26,  8], dtype=int32), 'currentDistance': 14.71286701781403}
done in step count: 38
reward sum = 0.4952160398642396
running average episode reward sum: 0.4297861322240469
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  8.]), 'previousTarget': array([26.,  8.]), 'currentState': array([25.16010509,  7.87769971,  0.07915726]), 'targetState': array([26,  8], dtype=int32), 'currentDistance': 0.848752511032609}
episode index:934
target Thresh 31.997751310436175
target distance 14.0
model initialize at round 934
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 10.]), 'previousTarget': array([20., 10.]), 'currentState': array([5.99011119, 2.04757482, 1.53025618]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 16.109563952390065}
done in step count: 40
reward sum = 0.4563405345590225
running average episode reward sum: 0.42981453265435166
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 10.]), 'previousTarget': array([20., 10.]), 'currentState': array([19.04495522,  9.3185987 ,  0.4998133 ]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 1.1732085286110043}
episode index:935
target Thresh 31.997773685271184
target distance 18.0
model initialize at round 935
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([16.        , 29.        ,  2.00180483]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 18.973665961010276}
done in step count: 60
reward sum = 0.3147585055219684
running average episode reward sum: 0.429691609548441
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.00801673, 11.900749  ,  4.79626217]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.900784678800615}
episode index:936
target Thresh 31.99779583747286
target distance 8.0
model initialize at round 936
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 24.]), 'previousTarget': array([19., 24.]), 'currentState': array([27.        , 23.        ,  2.12197399]), 'targetState': array([19, 24], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 20
reward sum = 0.6927504038829972
running average episode reward sum: 0.429972355326813
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 24.]), 'previousTarget': array([19., 24.]), 'currentState': array([19.89693007, 24.02273838,  3.08618473]), 'targetState': array([19, 24], dtype=int32), 'currentDistance': 0.8972182503828535}
episode index:937
target Thresh 31.99781776925645
target distance 22.0
model initialize at round 937
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4.93592685, 24.0585156 ]), 'previousTarget': array([ 4.93592685, 24.0585156 ]), 'currentState': array([11.        ,  5.        ,  6.12257344]), 'targetState': array([ 4, 27], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 74
reward sum = 0.20741487626730792
running average episode reward sum: 0.42973508722547027
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 27.]), 'previousTarget': array([ 4., 27.]), 'currentState': array([ 4.19698742, 26.22718939,  1.88239442]), 'targetState': array([ 4, 27], dtype=int32), 'currentDistance': 0.7975213366587087}
episode index:938
target Thresh 31.997839482815145
target distance 12.0
model initialize at round 938
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 17.]), 'previousTarget': array([ 9., 17.]), 'currentState': array([10.08235021, 28.97160218,  5.71566391]), 'targetState': array([ 9, 17], dtype=int32), 'currentDistance': 12.020430142966251}
done in step count: 29
reward sum = 0.5633565326558139
running average episode reward sum: 0.4298773890842885
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 17.]), 'previousTarget': array([ 9., 17.]), 'currentState': array([ 9.24835942, 17.81615144,  4.46824947]), 'targetState': array([ 9, 17], dtype=int32), 'currentDistance': 0.853103496391045}
episode index:939
target Thresh 31.997860980320322
target distance 22.0
model initialize at round 939
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4.70226409, 24.81660336]), 'previousTarget': array([ 4.70226409, 24.81660336]), 'currentState': array([2.        , 5.        , 1.04546273]), 'targetState': array([ 5, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.28008078596399677
running average episode reward sum: 0.42971803099586264
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 27.]), 'previousTarget': array([ 5., 27.]), 'currentState': array([ 5.23409063, 26.07776133,  1.76502536]), 'targetState': array([ 5, 27], dtype=int32), 'currentDistance': 0.9514844159030842}
episode index:940
target Thresh 31.99788226392175
target distance 17.0
model initialize at round 940
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 28.]), 'previousTarget': array([21., 28.]), 'currentState': array([25.98538841, 11.01307951,  2.36477047]), 'targetState': array([21, 28], dtype=int32), 'currentDistance': 17.70337722909192}
done in step count: 47
reward sum = 0.407092408019746
running average episode reward sum: 0.4296939867631569
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 28.]), 'previousTarget': array([21., 28.]), 'currentState': array([21.20754549, 27.22587324,  1.79358427]), 'targetState': array([21, 28], dtype=int32), 'currentDistance': 0.8014657642832852}
episode index:941
target Thresh 31.997903335747804
target distance 21.0
model initialize at round 941
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.89702028,  5.11762449]), 'previousTarget': array([14.89618185,  5.09009055]), 'currentState': array([13.0619615 , 25.03326054,  0.24607459]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.08717525436032458
running average episode reward sum: 0.4291452933012424
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.02442574, 13.31724791,  4.75583171]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 9.31727992405432}
episode index:942
target Thresh 31.997924197905686
target distance 14.0
model initialize at round 942
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 28.]), 'previousTarget': array([12., 28.]), 'currentState': array([ 6.2842721 , 14.2626643 ,  0.88542861]), 'targetState': array([12, 28], dtype=int32), 'currentDistance': 14.878976359321152}
done in step count: 36
reward sum = 0.4997339859532211
running average episode reward sum: 0.429220148754744
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 28.]), 'previousTarget': array([12., 28.]), 'currentState': array([11.67855077, 27.05180611,  1.3386958 ]), 'targetState': array([12, 28], dtype=int32), 'currentDistance': 1.0011999117040409}
episode index:943
target Thresh 31.997944852481627
target distance 5.0
model initialize at round 943
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([11.        ,  4.        ,  4.00320646]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 5.385164807134504}
done in step count: 16
reward sum = 0.7503191472487396
running average episode reward sum: 0.4295602959989113
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([9.28112681, 8.06888854, 1.91355048]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.9726257438951877}
episode index:944
target Thresh 31.997965301541107
target distance 7.0
model initialize at round 944
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 27.]), 'previousTarget': array([20., 27.]), 'currentState': array([13.       , 26.       ,  4.1979847]), 'targetState': array([20, 27], dtype=int32), 'currentDistance': 7.0710678118654755}
done in step count: 19
reward sum = 0.6816991350358008
running average episode reward sum: 0.42982710958519377
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 27.]), 'previousTarget': array([20., 27.]), 'currentState': array([19.06622727, 26.86362613,  0.23468102]), 'targetState': array([20, 27], dtype=int32), 'currentDistance': 0.9436786253694186}
episode index:945
target Thresh 31.99798554712904
target distance 20.0
model initialize at round 945
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.28114913,  5.32211619]), 'previousTarget': array([10.361625  ,  5.47568183]), 'currentState': array([19.94258944, 22.83372866,  4.47920571]), 'targetState': array([9, 3], dtype=int32), 'currentDistance': 20.0}
done in step count: 68
reward sum = 0.2924488751767298
running average episode reward sum: 0.42968188946425456
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 3.]), 'previousTarget': array([9., 3.]), 'currentState': array([9.62993852, 3.9381468 , 3.99807083]), 'targetState': array([9, 3], dtype=int32), 'currentDistance': 1.1300185650700656}
episode index:946
target Thresh 31.99800559127001
target distance 10.0
model initialize at round 946
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  6.]), 'previousTarget': array([27.,  6.]), 'currentState': array([17.        ,  7.        ,  4.37514448]), 'targetState': array([27,  6], dtype=int32), 'currentDistance': 10.04987562112089}
done in step count: 24
reward sum = 0.6041760902367083
running average episode reward sum: 0.4298661494439509
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  6.]), 'previousTarget': array([27.,  6.]), 'currentState': array([26.02701463,  6.11323935,  6.19110417]), 'targetState': array([27,  6], dtype=int32), 'currentDistance': 0.9795527923611183}
episode index:947
target Thresh 31.998025435968447
target distance 21.0
model initialize at round 947
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8.09079324, 5.97475466]), 'previousTarget': array([8.10381815, 6.09009055]), 'currentState': array([ 9.94565846, 25.88855578,  4.21747678]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.3271159618364363
running average episode reward sum: 0.42975776317010334
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 5.]), 'previousTarget': array([8., 5.]), 'currentState': array([8.40126684, 5.95117478, 4.43221942]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 1.0323509792258927}
episode index:948
target Thresh 31.998045083208837
target distance 15.0
model initialize at round 948
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([27.        , 27.        ,  3.28370857]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 16.15549442140351}
done in step count: 41
reward sum = 0.4459072536215206
running average episode reward sum: 0.42977478054676443
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([21.3500555 , 12.85589897,  4.40277557]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 0.9247172001667936}
episode index:949
target Thresh 31.998064534955912
target distance 12.0
model initialize at round 949
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 23.]), 'previousTarget': array([ 6., 23.]), 'currentState': array([18.00448323, 17.96399939,  4.60349673]), 'targetState': array([ 6, 23], dtype=int32), 'currentDistance': 13.018022887538528}
done in step count: 32
reward sum = 0.5189321028158685
running average episode reward sum: 0.4298686303596793
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 23.]), 'previousTarget': array([ 6., 23.]), 'currentState': array([ 6.86694016, 22.3799956 ,  2.63048911]), 'targetState': array([ 6, 23], dtype=int32), 'currentDistance': 1.06582864467959}
episode index:950
target Thresh 31.998083793154876
target distance 16.0
model initialize at round 950
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 19.]), 'previousTarget': array([27., 19.]), 'currentState': array([26.        ,  3.        ,  0.96958417]), 'targetState': array([27, 19], dtype=int32), 'currentDistance': 16.0312195418814}
done in step count: 37
reward sum = 0.46503010062152006
running average episode reward sum: 0.4299056035145288
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 19.]), 'previousTarget': array([27., 19.]), 'currentState': array([26.57540316, 18.0406713 ,  1.56874541]), 'targetState': array([27, 19], dtype=int32), 'currentDistance': 1.0490920023237789}
episode index:951
target Thresh 31.998102859731556
target distance 22.0
model initialize at round 951
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.80656079,  8.76024166]), 'previousTarget': array([15.82541376,  8.78146944]), 'currentState': array([25.96421701, 25.98876511,  3.57994115]), 'targetState': array([13,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 78
reward sum = 0.2115640321598273
running average episode reward sum: 0.4296762531244503
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  4.]), 'previousTarget': array([13.,  4.]), 'currentState': array([13.21865442,  4.95460405,  4.33351617]), 'targetState': array([13,  4], dtype=int32), 'currentDistance': 0.9793256122204833}
episode index:952
target Thresh 31.998121736592626
target distance 13.0
model initialize at round 952
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  4.]), 'previousTarget': array([18.,  4.]), 'currentState': array([5.        , 6.        , 1.06464732]), 'targetState': array([18,  4], dtype=int32), 'currentDistance': 13.152946437965904}
done in step count: 33
reward sum = 0.5143444880687872
running average episode reward sum: 0.429765097022608
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  4.]), 'previousTarget': array([18.,  4.]), 'currentState': array([17.06639689,  4.38558859,  5.69997672]), 'targetState': array([18,  4], dtype=int32), 'currentDistance': 1.0100957070397185}
episode index:953
target Thresh 31.99814042562579
target distance 22.0
model initialize at round 953
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.81376734, 26.29702003]), 'previousTarget': array([22.81660336, 26.29773591]), 'currentState': array([ 2.99582834, 28.98947126,  4.58082801]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 84
reward sum = 0.23199017912927516
running average episode reward sum: 0.42955778578791903
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 26.]), 'previousTarget': array([25., 26.]), 'currentState': array([24.11303899, 25.98206087,  0.36966241]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 0.8871424006032137}
episode index:954
target Thresh 31.998158928699972
target distance 18.0
model initialize at round 954
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 19.]), 'previousTarget': array([ 4., 19.]), 'currentState': array([22.04398023, 13.00821407,  0.43714016]), 'targetState': array([ 4, 19], dtype=int32), 'currentDistance': 19.012804132334267}
done in step count: 50
reward sum = 0.3250419127111921
running average episode reward sum: 0.4294483450831266
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 19.]), 'previousTarget': array([ 4., 19.]), 'currentState': array([ 4.77211818, 19.07140172,  3.22360121]), 'targetState': array([ 4, 19], dtype=int32), 'currentDistance': 0.7754125973421321}
episode index:955
target Thresh 31.998177247665488
target distance 9.0
model initialize at round 955
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.19171809, 23.98910986,  5.98108243]), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.762535896238804}
done in step count: 22
reward sum = 0.6431934290609562
running average episode reward sum: 0.4296719278069528
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.64501708, 15.91717253,  5.16440728]), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9834725830028339}
episode index:956
target Thresh 31.99819538435425
target distance 16.0
model initialize at round 956
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([11.10929113, 22.9483836 ,  5.88063155]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 19.893212334739864}
done in step count: 49
reward sum = 0.3668007844185881
running average episode reward sum: 0.42960623173235685
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([22.08227586,  7.80270369,  5.13864928]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 1.2192419025695849}
episode index:957
target Thresh 31.998213340579944
target distance 12.0
model initialize at round 957
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 27.]), 'previousTarget': array([27., 27.]), 'currentState': array([15.01239256, 26.08229328,  1.1688292 ]), 'targetState': array([27, 27], dtype=int32), 'currentDistance': 12.022683465377616}
done in step count: 27
reward sum = 0.558096179508949
running average episode reward sum: 0.4297403548511215
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 27.]), 'previousTarget': array([27., 27.]), 'currentState': array([26.12512234, 27.04153258,  0.22209166]), 'targetState': array([27, 27], dtype=int32), 'currentDistance': 0.8758629352103826}
episode index:958
target Thresh 31.998231118138207
target distance 4.0
model initialize at round 958
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 29.]), 'previousTarget': array([ 9., 29.]), 'currentState': array([ 5.33328741, 26.25105301,  0.72115648]), 'targetState': array([ 9, 29], dtype=int32), 'currentDistance': 4.58273834659136}
done in step count: 9
reward sum = 0.844709408068808
running average episode reward sum: 0.43017306502131725
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 29.]), 'previousTarget': array([ 9., 29.]), 'currentState': array([ 8.10560608, 28.67878668,  0.5795723 ]), 'targetState': array([ 9, 29], dtype=int32), 'currentDistance': 0.9503254644543453}
episode index:959
target Thresh 31.99824871880681
target distance 24.0
model initialize at round 959
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7.16689345, 22.48483813]), 'previousTarget': array([ 7.15444247, 22.48069469]), 'currentState': array([27.01577463, 20.03087888,  1.33798882]), 'targetState': array([ 3, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 75
reward sum = 0.2147665200592952
running average episode reward sum: 0.4299486832036485
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 23.]), 'previousTarget': array([ 3., 23.]), 'currentState': array([ 3.92629792, 23.44525506,  3.45039884]), 'targetState': array([ 3, 23], dtype=int32), 'currentDistance': 1.0277547920018646}
episode index:960
target Thresh 31.998266144345834
target distance 9.0
model initialize at round 960
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 2.]), 'previousTarget': array([6., 2.]), 'currentState': array([14.52402184,  4.99827148,  3.19861116]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 9.035960394590726}
done in step count: 19
reward sum = 0.6805207857566928
running average episode reward sum: 0.43020942420526453
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 2.]), 'previousTarget': array([6., 2.]), 'currentState': array([6.79586415, 2.29365524, 3.76968892]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 0.8483119427008804}
episode index:961
target Thresh 31.998283396497847
target distance 14.0
model initialize at round 961
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  2.]), 'previousTarget': array([10.,  2.]), 'currentState': array([ 9.82211743, 15.58218568,  4.4129561 ]), 'targetState': array([10,  2], dtype=int32), 'currentDistance': 13.583350471052372}
done in step count: 28
reward sum = 0.541692780078243
running average episode reward sum: 0.43032531126958157
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  2.]), 'previousTarget': array([10.,  2.]), 'currentState': array([9.8563135 , 2.96919762, 4.78913024]), 'targetState': array([10,  2], dtype=int32), 'currentDistance': 0.9797907136696227}
episode index:962
target Thresh 31.99830047698808
target distance 22.0
model initialize at round 962
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.74573165, 12.41485602]), 'previousTarget': array([23.6773982 , 12.42229124]), 'currentState': array([ 4.07604023, 16.03469569,  0.36112328]), 'targetState': array([26, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 96
reward sum = 0.23540988342896063
running average episode reward sum: 0.43012290687930055
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 12.]), 'previousTarget': array([26., 12.]), 'currentState': array([25.02432714, 12.28217803,  5.75561835]), 'targetState': array([26, 12], dtype=int32), 'currentDistance': 1.0156583948166713}
episode index:963
target Thresh 31.998317387524597
target distance 5.0
model initialize at round 963
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([ 9.08030272, 11.9528969 ,  5.92774954]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 5.328606987023527}
done in step count: 11
reward sum = 0.8120859589602507
running average episode reward sum: 0.4305191341117497
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([13.15126108, 13.30457751,  0.56588372]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 1.0972557605831805}
episode index:964
target Thresh 31.99833412979846
target distance 20.0
model initialize at round 964
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8.71008489, 4.85014149]), 'previousTarget': array([8.71008489, 4.85014149]), 'currentState': array([19.        , 22.        ,  4.92385423]), 'targetState': array([7, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.1502422197864912
running average episode reward sum: 0.42991730887454943
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 2.]), 'previousTarget': array([7., 2.]), 'currentState': array([7.97895225, 3.00429628, 3.97853028]), 'targetState': array([7, 2], dtype=int32), 'currentDistance': 1.4024829853095357}
episode index:965
target Thresh 31.998350705483922
target distance 12.0
model initialize at round 965
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 25.]), 'previousTarget': array([14., 25.]), 'currentState': array([12.28854389, 13.38009235,  1.06784695]), 'targetState': array([14, 25], dtype=int32), 'currentDistance': 11.74526865403881}
done in step count: 24
reward sum = 0.5829210652105392
running average episode reward sum: 0.4300756978562637
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 25.]), 'previousTarget': array([14., 25.]), 'currentState': array([13.52860395, 24.17232337,  1.04161957]), 'targetState': array([14, 25], dtype=int32), 'currentDistance': 0.9525034594209368}
episode index:966
target Thresh 31.99836711623855
target distance 20.0
model initialize at round 966
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.62757583, 11.49298366]), 'previousTarget': array([19.638375  , 11.47568183]), 'currentState': array([ 9.98227804, 29.01349251,  2.74337077]), 'targetState': array([21,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.21435516383087994
running average episode reward sum: 0.42985261560804716
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  9.]), 'previousTarget': array([21.,  9.]), 'currentState': array([20.88480895,  9.86212584,  4.94253013]), 'targetState': array([21,  9], dtype=int32), 'currentDistance': 0.869787294556704}
episode index:967
target Thresh 31.998383363703446
target distance 5.0
model initialize at round 967
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 13.]), 'previousTarget': array([24., 13.]), 'currentState': array([21.22468687, 18.04690903,  6.28155998]), 'targetState': array([24, 13], dtype=int32), 'currentDistance': 5.759657435287691}
done in step count: 14
reward sum = 0.7775741315449852
running average episode reward sum: 0.4302118320501308
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 13.]), 'previousTarget': array([24., 13.]), 'currentState': array([23.7264163 , 13.83845908,  5.11896421]), 'targetState': array([24, 13], dtype=int32), 'currentDistance': 0.8819646662200931}
episode index:968
target Thresh 31.998399449503363
target distance 15.0
model initialize at round 968
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 11.]), 'previousTarget': array([23., 11.]), 'currentState': array([ 8.28002156, 19.95281077,  6.15033612]), 'targetState': array([23, 11], dtype=int32), 'currentDistance': 17.22877203457678}
done in step count: 46
reward sum = 0.4330800610367245
running average episode reward sum: 0.43021479203876506
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 11.]), 'previousTarget': array([23., 11.]), 'currentState': array([22.06808833, 11.58598275,  5.42114372]), 'targetState': array([23, 11], dtype=int32), 'currentDistance': 1.1008338397697586}
episode index:969
target Thresh 31.9984153752469
target distance 18.0
model initialize at round 969
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.29878023, 25.96763409]), 'previousTarget': array([23.28727678, 25.94818637]), 'currentState': array([ 9.99560003, 11.03359926,  1.5365898 ]), 'targetState': array([26, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.05811580540577324
running average episode reward sum: 0.42971135843315217
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 29.]), 'previousTarget': array([26., 29.]), 'currentState': array([15.61057568, 22.28807937,  0.82978085]), 'targetState': array([26, 29], dtype=int32), 'currentDistance': 12.36891329922497}
episode index:970
target Thresh 31.998431142526638
target distance 7.0
model initialize at round 970
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 19.]), 'previousTarget': array([ 4., 19.]), 'currentState': array([10.66579001, 18.82390144,  3.52849525]), 'targetState': array([ 4, 19], dtype=int32), 'currentDistance': 6.668115710213142}
done in step count: 13
reward sum = 0.7628299914693875
running average episode reward sum: 0.4300544260263924
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 19.]), 'previousTarget': array([ 4., 19.]), 'currentState': array([ 4.80457927, 19.04571863,  3.1268902 ]), 'targetState': array([ 4, 19], dtype=int32), 'currentDistance': 0.8058771617301865}
episode index:971
target Thresh 31.99844675291932
target distance 19.0
model initialize at round 971
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 29.]), 'previousTarget': array([ 6., 29.]), 'currentState': array([ 2.27051497, 10.40703239,  0.98732878]), 'targetState': array([ 6, 29], dtype=int32), 'currentDistance': 18.963319944151817}
done in step count: 48
reward sum = 0.37201540889809354
running average episode reward sum: 0.4299947151034209
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 29.]), 'previousTarget': array([ 6., 29.]), 'currentState': array([ 5.66462219, 28.062385  ,  1.35573989]), 'targetState': array([ 6, 29], dtype=int32), 'currentDistance': 0.9957912244674205}
episode index:972
target Thresh 31.998462207986
target distance 26.0
model initialize at round 972
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.48199646, 10.390578  ]), 'previousTarget': array([15.48199646, 10.390578  ]), 'currentState': array([ 6.        , 28.        ,  0.57667899]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.15884859691153444
running average episode reward sum: 0.42938953184338496
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  2.]), 'previousTarget': array([20.,  2.]), 'currentState': array([17.86914679,  9.79386071,  5.0597659 ]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 8.079901001430384}
episode index:973
target Thresh 31.998477509272202
target distance 16.0
model initialize at round 973
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 20.]), 'previousTarget': array([18., 20.]), 'currentState': array([ 2.        , 19.        ,  2.61414868]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 16.0312195418814}
done in step count: 44
reward sum = 0.4048822531885108
running average episode reward sum: 0.42936437036632663
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 20.]), 'previousTarget': array([18., 20.]), 'currentState': array([17.13944773, 19.86695681,  6.2479798 ]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 0.870775919706517}
episode index:974
target Thresh 31.99849265830806
target distance 3.0
model initialize at round 974
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  3.]), 'previousTarget': array([10.,  3.]), 'currentState': array([12.07455181,  5.68896796,  4.79349867]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 3.396220532716197}
done in step count: 7
reward sum = 0.8824347133157014
running average episode reward sum: 0.42982905789755677
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  3.]), 'previousTarget': array([10.,  3.]), 'currentState': array([10.91069959,  3.56415447,  3.86238079]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 1.07128148438214}
episode index:975
target Thresh 31.99850765660849
target distance 9.0
model initialize at round 975
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([20.03195748,  6.98061889,  5.50649947]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 9.254953048098558}
done in step count: 23
reward sum = 0.6064656389944397
running average episode reward sum: 0.4300100380011396
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([11.91893924,  8.88326087,  2.79263547]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.9263246456372911}
episode index:976
target Thresh 31.99852250567334
target distance 20.0
model initialize at round 976
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.98901829, 21.91768106]), 'previousTarget': array([14.10023299, 21.76887233]), 'currentState': array([24.83319338,  5.11280479,  2.57564721]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 99
reward sum = -0.014263668064862177
running average episode reward sum: 0.4295553054463126
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.07379678, 24.85161804]), 'previousTarget': array([12.14491957, 24.71070633]), 'currentState': array([20.97998931,  6.94408514,  2.37755155]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 20.0}
episode index:977
target Thresh 31.998537206987525
target distance 5.0
model initialize at round 977
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  9.]), 'previousTarget': array([17.,  9.]), 'currentState': array([22.        , 11.        ,  1.74352068]), 'targetState': array([17,  9], dtype=int32), 'currentDistance': 5.385164807134504}
done in step count: 14
reward sum = 0.7718846829251848
running average episode reward sum: 0.42990533548463455
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  9.]), 'previousTarget': array([17.,  9.]), 'currentState': array([17.95779421,  9.73040198,  3.62043487]), 'targetState': array([17,  9], dtype=int32), 'currentDistance': 1.204515173193176}
episode index:978
target Thresh 31.998551762021187
target distance 22.0
model initialize at round 978
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.9793708 ,  7.09184678]), 'previousTarget': array([22.9793708 ,  7.09184678]), 'currentState': array([3.        , 8.        , 2.11753702]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 87
reward sum = 0.2020999043170699
running average episode reward sum: 0.4296726435222571
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([24.07553453,  6.97064135,  5.99527883]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 0.9249315300192931}
episode index:979
target Thresh 31.998566172229847
target distance 8.0
model initialize at round 979
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 17.]), 'previousTarget': array([23., 17.]), 'currentState': array([15.        , 22.        ,  2.11509433]), 'targetState': array([23, 17], dtype=int32), 'currentDistance': 9.433981132056603}
done in step count: 26
reward sum = 0.5972375320747381
running average episode reward sum: 0.4298436281024127
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 17.]), 'previousTarget': array([23., 17.]), 'currentState': array([22.08561884, 17.77238823,  5.40848089]), 'targetState': array([23, 17], dtype=int32), 'currentDistance': 1.1969446406416884}
episode index:980
target Thresh 31.998580439054535
target distance 11.0
model initialize at round 980
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([18.99914618, 22.85953385,  4.50613815]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 14.1234753813235}
done in step count: 33
reward sum = 0.5152353955197168
running average episode reward sum: 0.429930673736885
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([ 8.85434334, 14.18519526,  3.73291444]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 0.87418523146148}
episode index:981
target Thresh 31.998594563921948
target distance 12.0
model initialize at round 981
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 12.]), 'previousTarget': array([ 6., 12.]), 'currentState': array([17.98071256, 12.04024802,  2.25709701]), 'targetState': array([ 6, 12], dtype=int32), 'currentDistance': 11.980780160074863}
done in step count: 27
reward sum = 0.567743747210768
running average episode reward sum: 0.4300710129155753
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 12.]), 'previousTarget': array([ 6., 12.]), 'currentState': array([ 6.94181405, 12.10958829,  3.22748024]), 'targetState': array([ 6, 12], dtype=int32), 'currentDistance': 0.9481683872080581}
episode index:982
target Thresh 31.99860854824458
target distance 13.0
model initialize at round 982
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 27.]), 'previousTarget': array([23., 27.]), 'currentState': array([ 9.98902425, 22.975105  ,  4.54964352]), 'targetState': array([23, 27], dtype=int32), 'currentDistance': 13.61929769826882}
done in step count: 31
reward sum = 0.4953593515983375
running average episode reward sum: 0.4301374303506544
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 27.]), 'previousTarget': array([23., 27.]), 'currentState': array([22.02453268, 26.27737077,  0.30580676]), 'targetState': array([23, 27], dtype=int32), 'currentDistance': 1.2139726100382588}
episode index:983
target Thresh 31.998622393420874
target distance 5.0
model initialize at round 983
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 15.]), 'previousTarget': array([ 4., 15.]), 'currentState': array([ 4.90418794, 19.99556634,  3.41013259]), 'targetState': array([ 4, 15], dtype=int32), 'currentDistance': 5.076735061068922}
done in step count: 13
reward sum = 0.7895825125971103
running average episode reward sum: 0.4305027200683846
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 15.]), 'previousTarget': array([ 4., 15.]), 'currentState': array([ 3.76073801, 15.85008002,  4.78830895]), 'targetState': array([ 4, 15], dtype=int32), 'currentDistance': 0.8831094755216148}
episode index:984
target Thresh 31.998636100835366
target distance 19.0
model initialize at round 984
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  3.]), 'previousTarget': array([20.,  3.]), 'currentState': array([15.97335995, 21.98116809,  3.97939426]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 19.403571118907973}
done in step count: 47
reward sum = 0.35232151705947345
running average episode reward sum: 0.43042334828868006
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  3.]), 'previousTarget': array([20.,  3.]), 'currentState': array([19.80993086,  3.88561388,  4.85216375]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 0.9057804471920644}
episode index:985
target Thresh 31.998649671858804
target distance 6.0
model initialize at round 985
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  9.]), 'previousTarget': array([22.,  9.]), 'currentState': array([20.97392965, 14.99979799,  3.37052602]), 'targetState': array([22,  9], dtype=int32), 'currentDistance': 6.0869036727886545}
done in step count: 16
reward sum = 0.7451793269858012
running average episode reward sum: 0.4307425734192045
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  9.]), 'previousTarget': array([22.,  9.]), 'currentState': array([21.37955456,  9.94504068,  4.93750258]), 'targetState': array([22,  9], dtype=int32), 'currentDistance': 1.130510696463108}
episode index:986
target Thresh 31.9986631078483
target distance 18.0
model initialize at round 986
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([9.91706419, 3.1100279 , 2.07151088]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 18.349971842830566}
done in step count: 47
reward sum = 0.3902776579213606
running average episode reward sum: 0.4307015755311622
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([13.56652665, 20.04639241,  1.12193794]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 1.0475049282940374}
episode index:987
target Thresh 31.99867641014747
target distance 25.0
model initialize at round 987
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9.8754239 , 8.97301241]), 'previousTarget': array([9.77438937, 9.18225176]), 'currentState': array([ 3.17288262, 27.81647123,  5.46937488]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 81
reward sum = 0.18498626812743674
running average episode reward sum: 0.43045287582731223
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  3.]), 'previousTarget': array([12.,  3.]), 'currentState': array([12.05325386,  3.96320339,  4.45735031]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 0.9646744248903336}
episode index:988
target Thresh 31.998689580086552
target distance 6.0
model initialize at round 988
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([7.99629911, 3.02549568, 1.47407317]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 6.08227614031738}
done in step count: 17
reward sum = 0.7398019801993425
running average episode reward sum: 0.43076566561939716
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.04874458,  3.79512144,  6.12720502]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.9730683909528105}
episode index:989
target Thresh 31.99870261898255
target distance 13.0
model initialize at round 989
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 29.]), 'previousTarget': array([16., 29.]), 'currentState': array([ 3.36558962, 25.1661249 ,  0.38382483]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 13.203292159554268}
done in step count: 31
reward sum = 0.5322757802912041
running average episode reward sum: 0.43086820108876267
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 29.]), 'previousTarget': array([16., 29.]), 'currentState': array([15.17404447, 28.8875748 ,  0.09106608]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 0.833571811360962}
episode index:990
target Thresh 31.99871552813936
target distance 13.0
model initialize at round 990
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  3.]), 'previousTarget': array([12.,  3.]), 'currentState': array([ 2.98990429, 15.55330022,  4.82705218]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 15.4520927723139}
done in step count: 33
reward sum = 0.48175864591272666
running average episode reward sum: 0.4309195537071522
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  3.]), 'previousTarget': array([12.,  3.]), 'currentState': array([11.00724792,  3.91484888,  5.14826063]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 1.3500019083374084}
episode index:991
target Thresh 31.99872830884792
target distance 9.0
model initialize at round 991
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 26.]), 'previousTarget': array([11., 26.]), 'currentState': array([19.89450022, 28.58955826,  4.22870576]), 'targetState': array([11, 26], dtype=int32), 'currentDistance': 9.263797608434947}
done in step count: 23
reward sum = 0.6381917643709791
running average episode reward sum: 0.431128497467902
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 26.]), 'previousTarget': array([11., 26.]), 'currentState': array([11.98977419, 26.26863397,  3.13577587]), 'targetState': array([11, 26], dtype=int32), 'currentDistance': 1.0255813723036258}
episode index:992
target Thresh 31.998740962386304
target distance 16.0
model initialize at round 992
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'previousTarget': array([10., 17.]), 'currentState': array([26.       , 12.       ,  5.1577487]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 16.76305461424021}
done in step count: 46
reward sum = 0.37957542555865453
running average episode reward sum: 0.4310765809805815
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'previousTarget': array([10., 17.]), 'currentState': array([10.87417673, 17.17333297,  2.92244284]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 0.8911954185988465}
episode index:993
target Thresh 31.998753490019876
target distance 23.0
model initialize at round 993
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.66693225, 12.60864988]), 'previousTarget': array([10.8764257 , 12.90788956]), 'currentState': array([23.84750583, 27.65100607,  4.36398944]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.08188213440299055
running average episode reward sum: 0.43056052593492394
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([17.03072427, 14.05324219,  3.93370473]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 15.866977314776962}
episode index:994
target Thresh 31.998765893001412
target distance 14.0
model initialize at round 994
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 11.]), 'previousTarget': array([24., 11.]), 'currentState': array([15.39342748, 25.04482214,  6.14412353]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 16.472101246312146}
done in step count: 37
reward sum = 0.44229240212748877
running average episode reward sum: 0.43057231676526825
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 11.]), 'previousTarget': array([24., 11.]), 'currentState': array([23.20203895, 11.78699392,  5.1481094 ]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 1.1207592334005656}
episode index:995
target Thresh 31.998778172571217
target distance 25.0
model initialize at round 995
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.17552164,  8.59138013]), 'previousTarget': array([21.93630557,  8.59490445]), 'currentState': array([2.24687415, 6.90347922, 5.9061527 ]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 71
reward sum = 0.224054668903585
running average episode reward sum: 0.4303649697292625
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([26.09814925,  8.52802434,  0.26183597]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 1.017887905515873}
episode index:996
target Thresh 31.998790329957263
target distance 19.0
model initialize at round 996
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 29.]), 'previousTarget': array([ 4.08589282, 28.76686234]), 'currentState': array([10.73472814, 10.24735931,  2.52247286]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 19.925312948289065}
done in step count: 50
reward sum = 0.3335235533550019
running average episode reward sum: 0.4302678369144438
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 29.]), 'previousTarget': array([ 4., 29.]), 'currentState': array([ 4.15628409, 28.03147571,  2.02740647]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 0.9810525055595201}
episode index:997
target Thresh 31.998802366375298
target distance 25.0
model initialize at round 997
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.04848294, 19.09551454]), 'previousTarget': array([21.04848294, 19.09551454]), 'currentState': array([ 2.       , 13.       ,  5.3367486]), 'targetState': array([27, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 79
reward sum = 0.17347732738973437
running average episode reward sum: 0.4300105317946796
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 21.]), 'previousTarget': array([27., 21.]), 'currentState': array([26.13481049, 20.4999353 ,  0.4574452 ]), 'targetState': array([27, 21], dtype=int32), 'currentDistance': 0.9993085585567859}
episode index:998
target Thresh 31.99881428302897
target distance 12.0
model initialize at round 998
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([13.91132427,  6.33828431,  1.94104168]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 14.09200711105109}
done in step count: 33
reward sum = 0.5374444280076778
running average episode reward sum: 0.4301180732323302
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([ 6.93214944, 17.17263937,  2.05498404]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 1.2463659907994036}
episode index:999
target Thresh 31.99882608110996
target distance 15.0
model initialize at round 999
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 22.]), 'previousTarget': array([21., 22.]), 'currentState': array([21.03556352,  6.95407541,  5.62381744]), 'targetState': array([21, 22], dtype=int32), 'currentDistance': 15.045966622129153}
done in step count: 48
reward sum = 0.4101640196255194
running average episode reward sum: 0.4300981191787234
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 22.]), 'previousTarget': array([21., 22.]), 'currentState': array([21.32171387, 21.02643493,  1.88128876]), 'targetState': array([21, 22], dtype=int32), 'currentDistance': 1.025343234389679}
episode index:1000
target Thresh 31.99883776179808
target distance 11.0
model initialize at round 1000
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.        , 19.        ,  3.68917549]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 11.0}
done in step count: 27
reward sum = 0.6005761341344882
running average episode reward sum: 0.43026842688597194
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.35742846,  8.93456758,  4.84213282]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.134158170319654}
episode index:1001
target Thresh 31.998849326261414
target distance 13.0
model initialize at round 1001
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 26.]), 'previousTarget': array([24., 26.]), 'currentState': array([13.        , 13.        ,  2.11895323]), 'targetState': array([24, 26], dtype=int32), 'currentDistance': 17.029386365926403}
done in step count: 42
reward sum = 0.423725197623239
running average episode reward sum: 0.43026189671704707
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 26.]), 'previousTarget': array([24., 26.]), 'currentState': array([23.29744318, 25.11255991,  0.94675341]), 'targetState': array([24, 26], dtype=int32), 'currentDistance': 1.1318727843194838}
episode index:1002
target Thresh 31.99886077565641
target distance 15.0
model initialize at round 1002
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  9.]), 'previousTarget': array([17.,  9.]), 'currentState': array([13.97411343, 24.01690098,  2.81569123]), 'targetState': array([17,  9], dtype=int32), 'currentDistance': 15.318723988481814}
done in step count: 39
reward sum = 0.4458202731059039
running average episode reward sum: 0.4302774085579133
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  9.]), 'previousTarget': array([17.,  9.]), 'currentState': array([16.08552607,  9.94083985,  5.17566514]), 'targetState': array([17,  9], dtype=int32), 'currentDistance': 1.3120374198606226}
episode index:1003
target Thresh 31.998872111128026
target distance 6.0
model initialize at round 1003
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 23.]), 'previousTarget': array([19., 23.]), 'currentState': array([17.8984543 , 28.91485879,  3.96847469]), 'targetState': array([19, 23], dtype=int32), 'currentDistance': 6.016556940282939}
done in step count: 13
reward sum = 0.7832203169825727
running average episode reward sum: 0.43062894531929247
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 23.]), 'previousTarget': array([19., 23.]), 'currentState': array([18.10284204, 23.81417801,  5.0776659 ]), 'targetState': array([19, 23], dtype=int32), 'currentDistance': 1.2115189814912024}
episode index:1004
target Thresh 31.998883333809815
target distance 18.0
model initialize at round 1004
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.40408132, 14.6665781 ]), 'previousTarget': array([21.48314552, 14.71285862]), 'currentState': array([3.95030927, 4.90103452, 4.47334477]), 'targetState': array([22, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.29673741984731367
running average episode reward sum: 0.43049571992081287
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 15.]), 'previousTarget': array([22., 15.]), 'currentState': array([21.3578841 , 14.0527569 ,  0.53708314]), 'targetState': array([22, 15], dtype=int32), 'currentDistance': 1.1443698387712757}
episode index:1005
target Thresh 31.998894444824057
target distance 11.0
model initialize at round 1005
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 20.]), 'previousTarget': array([ 3., 20.]), 'currentState': array([13.89638395, 24.7491546 ,  4.14468065]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 11.886364147931252}
done in step count: 28
reward sum = 0.5723082488216193
running average episode reward sum: 0.4306366866493425
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 20.]), 'previousTarget': array([ 3., 20.]), 'currentState': array([ 3.93890032, 20.72446058,  3.43937525]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 1.1859076485337592}
episode index:1006
target Thresh 31.998905445281856
target distance 19.0
model initialize at round 1006
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 23.]), 'previousTarget': array([15., 23.]), 'currentState': array([18.        ,  4.        ,  0.79813471]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 19.235384061671347}
done in step count: 57
reward sum = 0.32002288409539065
running average episode reward sum: 0.4305268417610069
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 23.]), 'previousTarget': array([15., 23.]), 'currentState': array([15.83541054, 22.00122864,  1.75382776]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 1.302096387406861}
episode index:1007
target Thresh 31.998916336283273
target distance 8.0
model initialize at round 1007
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([ 8.        , 20.        ,  2.18241453]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 8.246211251235321}
done in step count: 22
reward sum = 0.6394762640806426
running average episode reward sum: 0.430734132854578
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([15.05678548, 18.28320664,  6.05238953]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 0.9848145174510458}
episode index:1008
target Thresh 31.998927118917415
target distance 7.0
model initialize at round 1008
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([19.03274324, 18.9705755 ,  5.80270866]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 7.299514230115111}
done in step count: 20
reward sum = 0.6890787993008964
running average episode reward sum: 0.4309901731582909
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([21.35922482, 25.04674562,  1.28199939]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 1.0186934666522618}
episode index:1009
target Thresh 31.998937794262552
target distance 15.0
model initialize at round 1009
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.12161343, 20.12760775]), 'previousTarget': array([21.14213562, 20.14213562]), 'currentState': array([6.9311411 , 6.03397462, 2.43074322]), 'targetState': array([22, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.28101796862668316
running average episode reward sum: 0.43084168582707144
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 21.]), 'previousTarget': array([22., 21.]), 'currentState': array([21.05401475, 20.25125336,  0.38763717]), 'targetState': array([22, 21], dtype=int32), 'currentDistance': 1.2064450373993347}
episode index:1010
target Thresh 31.998948363386233
target distance 21.0
model initialize at round 1010
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7.35322867, 22.74224216]), 'previousTarget': array([ 7.35322867, 22.74224216]), 'currentState': array([27.        , 19.        ,  1.95651484]), 'targetState': array([ 6, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.28180288330169184
running average episode reward sum: 0.43069426861389104
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 23.]), 'previousTarget': array([ 6., 23.]), 'currentState': array([ 6.94171278, 23.35353103,  3.10406139]), 'targetState': array([ 6, 23], dtype=int32), 'currentDistance': 1.005886253865372}
episode index:1011
target Thresh 31.998958827345376
target distance 4.0
model initialize at round 1011
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 20.]), 'previousTarget': array([14., 20.]), 'currentState': array([12.15456243, 23.56568081,  5.03981824]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 4.0149370471855805}
done in step count: 9
reward sum = 0.8662429899870743
running average episode reward sum: 0.4311246527259199
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 20.]), 'previousTarget': array([14., 20.]), 'currentState': array([13.13600888, 20.87548458,  5.24577358]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 1.2300219093344014}
episode index:1012
target Thresh 31.998969187186383
target distance 4.0
model initialize at round 1012
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  4.]), 'previousTarget': array([27.,  4.]), 'currentState': array([23.40937597,  6.71299779,  5.6493228 ]), 'targetState': array([27,  4], dtype=int32), 'currentDistance': 4.5003264239333145}
done in step count: 14
reward sum = 0.8104205951387919
running average episode reward sum: 0.43149908109947654
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  4.]), 'previousTarget': array([27.,  4.]), 'currentState': array([26.00633116,  4.20047821,  5.52806039]), 'targetState': array([27,  4], dtype=int32), 'currentDistance': 1.0136909162195413}
episode index:1013
target Thresh 31.998979443945252
target distance 17.0
model initialize at round 1013
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([6.        , 9.        , 3.51772577]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 18.027756377319946}
done in step count: 49
reward sum = 0.34995796706021354
running average episode reward sum: 0.43141866579963506
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([22.02676099,  2.88970563,  0.05311326]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 0.9794687428248745}
episode index:1014
target Thresh 31.998989598647665
target distance 8.0
model initialize at round 1014
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 18.]), 'previousTarget': array([13., 18.]), 'currentState': array([12.08831233, 25.54894119,  4.85424473]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 7.603794285763879}
done in step count: 21
reward sum = 0.7090702759417039
running average episode reward sum: 0.4316922141840115
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 18.]), 'previousTarget': array([13., 18.]), 'currentState': array([12.44115117, 18.95876666,  4.99323571]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 1.1097502044731742}
episode index:1015
target Thresh 31.998999652309102
target distance 5.0
model initialize at round 1015
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([23.16026009, 13.67193375,  5.11412824]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 5.0211162043707445}
done in step count: 15
reward sum = 0.8076650136759818
running average episode reward sum: 0.43206226615201543
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([24.35773391,  9.96361064,  5.1773325 ]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 1.158037649613227}
episode index:1016
target Thresh 31.999009605934933
target distance 7.0
model initialize at round 1016
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 25.]), 'previousTarget': array([23., 25.]), 'currentState': array([16.37521308, 22.91391563,  6.21876386]), 'targetState': array([23, 25], dtype=int32), 'currentDistance': 6.945469725005354}
done in step count: 17
reward sum = 0.7410962355991988
running average episode reward sum: 0.43236613436189464
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 25.]), 'previousTarget': array([23., 25.]), 'currentState': array([22.13233731, 24.22323801,  0.34234652]), 'targetState': array([23, 25], dtype=int32), 'currentDistance': 1.1645590272870774}
episode index:1017
target Thresh 31.999019460520536
target distance 12.0
model initialize at round 1017
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 19.]), 'previousTarget': array([ 8., 19.]), 'currentState': array([20.0029457 , 27.00590888,  1.32262582]), 'targetState': array([ 8, 19], dtype=int32), 'currentDistance': 14.427934098094417}
done in step count: 67
reward sum = 0.3271737777430744
running average episode reward sum: 0.4322628019880058
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 19.]), 'previousTarget': array([ 8., 19.]), 'currentState': array([ 8.39902635, 19.97368829,  3.98295982]), 'targetState': array([ 8, 19], dtype=int32), 'currentDistance': 1.05227891268733}
episode index:1018
target Thresh 31.999029217051373
target distance 9.0
model initialize at round 1018
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  4.]), 'previousTarget': array([22.,  4.]), 'currentState': array([13.22083423,  3.81753601,  5.72090882]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 8.781061705349437}
done in step count: 19
reward sum = 0.6834656602971065
running average episode reward sum: 0.4325093209853651
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  4.]), 'previousTarget': array([22.,  4.]), 'currentState': array([21.05566692,  3.87994408,  6.25532605]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 0.9519340271851588}
episode index:1019
target Thresh 31.999038876503104
target distance 2.0
model initialize at round 1019
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 27.]), 'previousTarget': array([ 8., 27.]), 'currentState': array([ 8.        , 29.        ,  3.34147587]), 'targetState': array([ 8, 27], dtype=int32), 'currentDistance': 2.0}
done in step count: 7
reward sum = 0.9004113686419063
running average episode reward sum: 0.4329680484830676
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 27.]), 'previousTarget': array([ 8., 27.]), 'currentState': array([ 7.89091006, 27.95942563,  4.9314304 ]), 'targetState': array([ 8, 27], dtype=int32), 'currentDistance': 0.9656076587791922}
episode index:1020
target Thresh 31.999048439841687
target distance 4.0
model initialize at round 1020
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([17.45430081, 13.0106668 ,  6.26854827]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 3.6869269171299783}
done in step count: 7
reward sum = 0.8819317177483886
running average episode reward sum: 0.43340777783592294
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([20.10506786, 12.67594858,  5.94330741]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 1.1215212929919731}
episode index:1021
target Thresh 31.99905790802346
target distance 8.0
model initialize at round 1021
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 24.]), 'previousTarget': array([17., 24.]), 'currentState': array([ 9.        , 21.        ,  1.52926213]), 'targetState': array([17, 24], dtype=int32), 'currentDistance': 8.54400374531753}
done in step count: 22
reward sum = 0.6688171400704475
running average episode reward sum: 0.43363811967763977
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 24.]), 'previousTarget': array([17., 24.]), 'currentState': array([16.12256238, 23.65504204,  0.3599468 ]), 'targetState': array([17, 24], dtype=int32), 'currentDistance': 0.9428110989727237}
episode index:1022
target Thresh 31.99906728199525
target distance 17.0
model initialize at round 1022
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.33649578, 21.53180834]), 'previousTarget': array([25.33935727, 21.53366395]), 'currentState': array([ 8.99523283, 10.00084458,  2.71374631]), 'targetState': array([26, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.28691009219283337
running average episode reward sum: 0.4334946905207631
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 22.]), 'previousTarget': array([26., 22.]), 'currentState': array([25.03126642, 21.11935714,  0.6369577 ]), 'targetState': array([26, 22], dtype=int32), 'currentDistance': 1.3091892891222714}
episode index:1023
target Thresh 31.999076562694462
target distance 8.0
model initialize at round 1023
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 26.]), 'previousTarget': array([15., 26.]), 'currentState': array([21.01343287, 18.00164607,  0.37443274]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 10.006749745440956}
done in step count: 24
reward sum = 0.6051856156054797
running average episode reward sum: 0.43366235743979115
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 26.]), 'previousTarget': array([15., 26.]), 'currentState': array([15.93297203, 25.28227129,  2.25794962]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 1.1771029287086396}
episode index:1024
target Thresh 31.999085751049176
target distance 7.0
model initialize at round 1024
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([ 7.97874416, 15.02028041,  2.1271739 ]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 7.627369294731437}
done in step count: 19
reward sum = 0.6774667354002262
running average episode reward sum: 0.4339002153695086
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([14.16150047, 17.65135975,  0.23592608]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 0.9080922250612109}
episode index:1025
target Thresh 31.99909484797823
target distance 8.0
model initialize at round 1025
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([ 5.82132153, 13.36828947,  1.76999974]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 8.267224587548014}
done in step count: 18
reward sum = 0.6869816189635465
running average episode reward sum: 0.4341468834042007
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([ 9.02840597, 20.15823207,  1.29967961]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 0.8422470828223386}
episode index:1026
target Thresh 31.99910385439133
target distance 5.0
model initialize at round 1026
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([10.73104528, 20.42096468,  2.21375252]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 5.906628765769212}
done in step count: 12
reward sum = 0.7998221332330658
running average episode reward sum: 0.4345029449911811
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([ 7.79572493, 24.04502513,  2.27512596]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 1.2430427117828684}
episode index:1027
target Thresh 31.999112771189118
target distance 19.0
model initialize at round 1027
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 26.]), 'previousTarget': array([17., 26.]), 'currentState': array([22.88086074,  7.06000226,  2.59157072]), 'targetState': array([17, 26], dtype=int32), 'currentDistance': 19.83199529224706}
done in step count: 53
reward sum = 0.350897506330747
running average episode reward sum: 0.4344216167434569
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 26.]), 'previousTarget': array([17., 26.]), 'currentState': array([17.20756027, 25.0398676 ,  2.05995891]), 'targetState': array([17, 26], dtype=int32), 'currentDistance': 0.9823113036895885}
episode index:1028
target Thresh 31.99912159926329
target distance 15.0
model initialize at round 1028
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 28.]), 'previousTarget': array([18., 28.]), 'currentState': array([14.93607932, 13.10785952,  2.0013127 ]), 'targetState': array([18, 28], dtype=int32), 'currentDistance': 15.20406058019618}
done in step count: 34
reward sum = 0.4885695956845101
running average episode reward sum: 0.43447423868606244
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 28.]), 'previousTarget': array([18., 28.]), 'currentState': array([17.92119273, 27.09091725,  1.30824534]), 'targetState': array([18, 28], dtype=int32), 'currentDistance': 0.9124922125273776}
episode index:1029
target Thresh 31.999130339496656
target distance 20.0
model initialize at round 1029
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3.35345468, 17.54691139]), 'previousTarget': array([ 3.43046618, 17.57218647]), 'currentState': array([21.89675008, 25.03998791,  2.90882123]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.29695405846155054
running average episode reward sum: 0.4343407239479804
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'previousTarget': array([ 2., 17.]), 'currentState': array([ 2.99619014, 17.3159267 ,  3.39637889]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 1.045085869591832}
episode index:1030
target Thresh 31.999138992763246
target distance 6.0
model initialize at round 1030
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 12.]), 'previousTarget': array([ 2., 12.]), 'currentState': array([ 8.        , 16.        ,  5.30411381]), 'targetState': array([ 2, 12], dtype=int32), 'currentDistance': 7.211102550927978}
done in step count: 18
reward sum = 0.7101945053749033
running average episode reward sum: 0.4346082833868038
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 12.]), 'previousTarget': array([ 2., 12.]), 'currentState': array([ 2.88265672, 12.66346537,  3.71081874]), 'targetState': array([ 2, 12], dtype=int32), 'currentDistance': 1.1042052287589887}
episode index:1031
target Thresh 31.999147559928396
target distance 13.0
model initialize at round 1031
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 17.]), 'previousTarget': array([26., 17.]), 'currentState': array([17.94757174,  4.13669204,  1.81188765]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 15.175845693993722}
done in step count: 43
reward sum = 0.45924128781308193
running average episode reward sum: 0.43463215257713933
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 17.]), 'previousTarget': array([26., 17.]), 'currentState': array([25.86794459, 16.1310602 ,  1.17660991]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 0.8789169480624793}
episode index:1032
target Thresh 31.999156041848824
target distance 14.0
model initialize at round 1032
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 20.]), 'previousTarget': array([ 2., 20.]), 'currentState': array([16.0228302 , 10.02355413,  1.05350411]), 'targetState': array([ 2, 20], dtype=int32), 'currentDistance': 17.209568240026098}
done in step count: 48
reward sum = 0.3833938403098003
running average episode reward sum: 0.43458255111318256
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 20.]), 'previousTarget': array([ 2., 20.]), 'currentState': array([ 2.98299178, 19.4479629 ,  2.32776621]), 'targetState': array([ 2, 20], dtype=int32), 'currentDistance': 1.1273942547911762}
episode index:1033
target Thresh 31.999164439372738
target distance 9.0
model initialize at round 1033
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([11.00311025, 12.84665358,  4.53467074]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 11.28303295417051}
done in step count: 23
reward sum = 0.6099556884205786
running average episode reward sum: 0.43475215762895375
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.23429665, 4.91220513, 4.0167588 ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.9418137388602839}
episode index:1034
target Thresh 31.99917275333989
target distance 12.0
model initialize at round 1034
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([16.05846951, 21.09454129,  0.94297144]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 15.730566996474462}
done in step count: 46
reward sum = 0.3885472089565317
running average episode reward sum: 0.43470751516646833
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.67985938, 9.93074707, 3.9806025 ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 1.1526052651707606}
episode index:1035
target Thresh 31.99918098458169
target distance 4.0
model initialize at round 1035
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 22.]), 'previousTarget': array([23., 22.]), 'currentState': array([20.        , 26.        ,  0.44450462]), 'targetState': array([23, 22], dtype=int32), 'currentDistance': 5.0}
done in step count: 13
reward sum = 0.7911766317083365
running average episode reward sum: 0.43505159732529247
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 22.]), 'previousTarget': array([23., 22.]), 'currentState': array([22.41484761, 22.85529495,  5.2652961 ]), 'targetState': array([23, 22], dtype=int32), 'currentDistance': 1.0363072827812148}
episode index:1036
target Thresh 31.999189133921266
target distance 7.0
model initialize at round 1036
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([18.        ,  4.        ,  0.54063225]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 7.615773105863908}
done in step count: 19
reward sum = 0.6971125926682783
running average episode reward sum: 0.4353043080247554
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.35545456, 10.16974156,  2.24828737]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.903148392697352}
episode index:1037
target Thresh 31.999197202173555
target distance 4.0
model initialize at round 1037
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([13.02607689,  8.96309023,  5.57996655]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 4.1527300283027255}
done in step count: 21
reward sum = 0.7242059381602882
running average episode reward sum: 0.4355826332946355
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([14.12798345, 12.01554445,  1.64327444]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 0.9927398956862867}
episode index:1038
target Thresh 31.999205190145396
target distance 6.0
model initialize at round 1038
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([ 8.        , 19.        ,  6.16345692]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 6.08276253029822}
done in step count: 21
reward sum = 0.7033007300504232
running average episode reward sum: 0.43584030230017523
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([ 7.51448242, 24.09351556,  1.70934501]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 1.0423081177135696}
episode index:1039
target Thresh 31.999213098635586
target distance 23.0
model initialize at round 1039
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8.11847993, 19.64924133]), 'previousTarget': array([ 8.37514441, 19.71201303]), 'currentState': array([26.68513747, 27.0842415 ,  2.90950624]), 'targetState': array([ 4, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 72
reward sum = 0.2433846450694371
running average episode reward sum: 0.43565524878360723
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 18.]), 'previousTarget': array([ 4., 18.]), 'currentState': array([ 4.76555624, 18.78739622,  3.54697193]), 'targetState': array([ 4, 18], dtype=int32), 'currentDistance': 1.098211802776855}
episode index:1040
target Thresh 31.999220928434983
target distance 12.0
model initialize at round 1040
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 3.]), 'previousTarget': array([9., 3.]), 'currentState': array([ 7.90180784, 14.55400055,  4.59315518]), 'targetState': array([9, 3], dtype=int32), 'currentDistance': 11.606074043915829}
done in step count: 23
reward sum = 0.6062401483676083
running average episode reward sum: 0.4358191151616898
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 3.]), 'previousTarget': array([9., 3.]), 'currentState': array([8.66218139, 3.99953278, 4.89099864]), 'targetState': array([9, 3], dtype=int32), 'currentDistance': 1.0550768651585012}
episode index:1041
target Thresh 31.999228680326578
target distance 6.0
model initialize at round 1041
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([21.64068746,  9.65275948,  3.97236879]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 7.313663235700523}
done in step count: 14
reward sum = 0.7549519167503361
running average episode reward sum: 0.43612538464498024
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([17.88769051,  4.86072437,  3.90117901]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 1.236463050986488}
episode index:1042
target Thresh 31.99923635508556
target distance 8.0
model initialize at round 1042
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 6.]), 'previousTarget': array([6., 6.]), 'currentState': array([ 9.9650906, 14.005721 ,  3.2078768]), 'targetState': array([6, 6], dtype=int32), 'currentDistance': 8.93384084488605}
done in step count: 21
reward sum = 0.6600634728621991
running average episode reward sum: 0.4363400903863199
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 6.]), 'previousTarget': array([6., 6.]), 'currentState': array([6.10576569, 6.93737107, 4.41214302]), 'targetState': array([6, 6], dtype=int32), 'currentDistance': 0.9433190850829701}
episode index:1043
target Thresh 31.999243953479418
target distance 10.0
model initialize at round 1043
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 20.]), 'previousTarget': array([ 9., 20.]), 'currentState': array([15.64420691, 10.0408731 ,  2.85959259]), 'targetState': array([ 9, 20], dtype=int32), 'currentDistance': 11.972038006931747}
done in step count: 25
reward sum = 0.5735367101531679
running average episode reward sum: 0.43647150477306973
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 20.]), 'previousTarget': array([ 9., 20.]), 'currentState': array([ 9.7205186 , 19.02640963,  1.81978308]), 'targetState': array([ 9, 20], dtype=int32), 'currentDistance': 1.211208186854292}
episode index:1044
target Thresh 31.99925147626799
target distance 7.0
model initialize at round 1044
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 17.]), 'previousTarget': array([20., 17.]), 'currentState': array([26.83321725, 17.21485918,  2.33334917]), 'targetState': array([20, 17], dtype=int32), 'currentDistance': 6.836594360557367}
done in step count: 14
reward sum = 0.7556360876652565
running average episode reward sum: 0.4367769254265551
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 17.]), 'previousTarget': array([20., 17.]), 'currentState': array([20.99030757, 17.57220217,  3.09555745]), 'targetState': array([20, 17], dtype=int32), 'currentDistance': 1.1437326670332892}
episode index:1045
target Thresh 31.999258924203566
target distance 10.0
model initialize at round 1045
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 13.]), 'previousTarget': array([ 4., 13.]), 'currentState': array([14.04926852, 18.89446065,  4.89664698]), 'targetState': array([ 4, 13], dtype=int32), 'currentDistance': 11.650427631359973}
done in step count: 26
reward sum = 0.5667367642522245
running average episode reward sum: 0.43690117001434253
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 13.]), 'previousTarget': array([ 4., 13.]), 'currentState': array([ 4.85692727, 13.82332826,  3.54138382]), 'targetState': array([ 4, 13], dtype=int32), 'currentDistance': 1.1883575885133935}
episode index:1046
target Thresh 31.999266298030946
target distance 7.0
model initialize at round 1046
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 16.]), 'previousTarget': array([ 8., 16.]), 'currentState': array([14.63942428, 17.73610901,  3.60709184]), 'targetState': array([ 8, 16], dtype=int32), 'currentDistance': 6.862654683988596}
done in step count: 14
reward sum = 0.7564973962382843
running average episode reward sum: 0.43720641951407885
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 16.]), 'previousTarget': array([ 8., 16.]), 'currentState': array([ 8.92411734, 16.6612473 ,  3.25543828]), 'targetState': array([ 8, 16], dtype=int32), 'currentDistance': 1.136327794311867}
episode index:1047
target Thresh 31.999273598487516
target distance 12.0
model initialize at round 1047
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 18.]), 'previousTarget': array([ 8., 18.]), 'currentState': array([20.        , 16.        ,  0.90544671]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 12.165525060596439}
done in step count: 36
reward sum = 0.502998606634955
running average episode reward sum: 0.43726919831858346
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 18.]), 'previousTarget': array([ 8., 18.]), 'currentState': array([ 8.91418623, 17.75669305,  3.18004839]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 0.9460099053705634}
episode index:1048
target Thresh 31.99928082630333
target distance 8.0
model initialize at round 1048
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 11.]), 'previousTarget': array([19., 11.]), 'currentState': array([27.        ,  6.        ,  4.63981932]), 'targetState': array([19, 11], dtype=int32), 'currentDistance': 9.433981132056603}
done in step count: 23
reward sum = 0.620367504536829
running average episode reward sum: 0.43744374389171814
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 11.]), 'previousTarget': array([19., 11.]), 'currentState': array([19.97976991, 10.47102875,  2.33649595]), 'targetState': array([19, 11], dtype=int32), 'currentDistance': 1.113444948026637}
episode index:1049
target Thresh 31.999287982201174
target distance 5.0
model initialize at round 1049
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 25.]), 'previousTarget': array([15., 25.]), 'currentState': array([10.14808634, 19.9613339 ,  6.14232564]), 'targetState': array([15, 25], dtype=int32), 'currentDistance': 6.994942617416264}
done in step count: 20
reward sum = 0.7242265591611956
running average episode reward sum: 0.43771687038245094
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 25.]), 'previousTarget': array([15., 25.]), 'currentState': array([14.74421416, 24.09211654,  1.04103285]), 'targetState': array([15, 25], dtype=int32), 'currentDistance': 0.9432278498256265}
episode index:1050
target Thresh 31.999295066896646
target distance 4.0
model initialize at round 1050
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 10.]), 'previousTarget': array([18., 10.]), 'currentState': array([17.        , 14.        ,  3.17396617]), 'targetState': array([18, 10], dtype=int32), 'currentDistance': 4.1231056256176615}
done in step count: 12
reward sum = 0.814965329236215
running average episode reward sum: 0.4380758127790768
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 10.]), 'previousTarget': array([18., 10.]), 'currentState': array([17.28776301, 10.80727488,  5.09975555]), 'targetState': array([18, 10], dtype=int32), 'currentDistance': 1.076556668965772}
episode index:1051
target Thresh 31.99930208109822
target distance 7.0
model initialize at round 1051
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  8.]), 'previousTarget': array([11.,  8.]), 'currentState': array([18.00126881,  6.99696441,  4.87618759]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 7.072753733682917}
done in step count: 18
reward sum = 0.7017590200373326
running average episode reward sum: 0.4383264622156341
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  8.]), 'previousTarget': array([11.,  8.]), 'currentState': array([11.78496652,  7.58717085,  2.7677124 ]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 0.8869049221085168}
episode index:1052
target Thresh 31.999309025507323
target distance 22.0
model initialize at round 1052
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.26628369, 18.52348754]), 'previousTarget': array([19.26673649, 18.52454686]), 'currentState': array([8.00010225, 1.99856225, 5.03588915]), 'targetState': array([23, 24], dtype=int32), 'currentDistance': 20.0}
done in step count: 94
reward sum = 0.11031374824964835
running average episode reward sum: 0.4380149591634347
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 24.]), 'previousTarget': array([23., 24.]), 'currentState': array([22.08662866, 23.02327575,  0.7065078 ]), 'targetState': array([23, 24], dtype=int32), 'currentDistance': 1.3372499647274392}
episode index:1053
target Thresh 31.9993159008184
target distance 18.0
model initialize at round 1053
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 25.]), 'previousTarget': array([12., 25.]), 'currentState': array([11.97320948,  6.89035228,  4.23039737]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 18.1096675340544}
done in step count: 54
reward sum = 0.3486356379144413
running average episode reward sum: 0.4379301590483977
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 25.]), 'previousTarget': array([12., 25.]), 'currentState': array([12.26296724, 24.0163389 ,  1.60501758]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 1.0182047526850801}
episode index:1054
target Thresh 31.99932270771899
target distance 21.0
model initialize at round 1054
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.11990655,  9.3102453 ]), 'previousTarget': array([14.11990655,  9.3102453 ]), 'currentState': array([ 7.        , 28.        ,  2.69786024]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 73
reward sum = 0.22208059518926826
running average episode reward sum: 0.43772556230540327
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.14240529,  7.79500458,  5.4371994 ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.1694019712942472}
episode index:1055
target Thresh 31.999329446889785
target distance 7.0
model initialize at round 1055
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([10.        , 17.        ,  3.48034477]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 20
reward sum = 0.6784432894213716
running average episode reward sum: 0.43795351469850546
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.12589031, 10.92472324,  5.25423884]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.2724703569583777}
episode index:1056
target Thresh 31.999336119004713
target distance 7.0
model initialize at round 1056
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 28.]), 'previousTarget': array([23., 28.]), 'currentState': array([23.64624513, 21.19989415,  2.37862533]), 'targetState': array([23, 28], dtype=int32), 'currentDistance': 6.830744643106225}
done in step count: 19
reward sum = 0.7221510142946181
running average episode reward sum: 0.43822238650512435
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 28.]), 'previousTarget': array([23., 28.]), 'currentState': array([23.23454795, 27.09089672,  1.42356521]), 'targetState': array([23, 28], dtype=int32), 'currentDistance': 0.9388724691077678}
episode index:1057
target Thresh 31.999342724730987
target distance 11.0
model initialize at round 1057
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  2.]), 'previousTarget': array([13.,  2.]), 'currentState': array([24.        ,  6.        ,  5.52663633]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 11.704699910719626}
done in step count: 30
reward sum = 0.5378718070731572
running average episode reward sum: 0.43831657310301475
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  2.]), 'previousTarget': array([13.,  2.]), 'currentState': array([13.87560345,  2.37889747,  3.49707081]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 0.9540674463511367}
episode index:1058
target Thresh 31.999349264729187
target distance 1.0
model initialize at round 1058
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.07168054,  9.12888163,  1.15681876]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.874062534193371}
done in step count: 0
reward sum = 0.9977536360184764
running average episode reward sum: 0.43884484228423803
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.07168054,  9.12888163,  1.15681876]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.874062534193371}
episode index:1059
target Thresh 31.999355739653314
target distance 11.0
model initialize at round 1059
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 27.]), 'previousTarget': array([14., 27.]), 'currentState': array([ 3.11026352, 18.35153839,  1.1896745 ]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 13.906194617555379}
done in step count: 31
reward sum = 0.524354846508515
running average episode reward sum: 0.43892551209954395
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 27.]), 'previousTarget': array([14., 27.]), 'currentState': array([13.41222027, 26.07677907,  0.6064707 ]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 1.0944505015449881}
episode index:1060
target Thresh 31.999362150150873
target distance 16.0
model initialize at round 1060
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 25.]), 'previousTarget': array([ 9., 25.]), 'currentState': array([24.70889549, 22.13258831,  2.70722531]), 'targetState': array([ 9, 25], dtype=int32), 'currentDistance': 15.96845162307135}
done in step count: 41
reward sum = 0.46695976840827036
running average episode reward sum: 0.4389519345842836
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 25.]), 'previousTarget': array([ 9., 25.]), 'currentState': array([ 9.85457928, 24.21510387,  2.94426088]), 'targetState': array([ 9, 25], dtype=int32), 'currentDistance': 1.1603308468692464}
episode index:1061
target Thresh 31.999368496862914
target distance 16.0
model initialize at round 1061
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3.91232139, 16.20755433]), 'previousTarget': array([ 3.94846611, 16.17009216]), 'currentState': array([19.0116189 ,  3.09224974,  1.54516572]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.27102115956970296
running average episode reward sum: 0.4387938076774902
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 17.]), 'previousTarget': array([ 3., 17.]), 'currentState': array([ 3.6794891 , 16.2716605 ,  2.86902438]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 0.9960842609848052}
episode index:1062
target Thresh 31.999374780424116
target distance 14.0
model initialize at round 1062
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 28.]), 'previousTarget': array([12., 28.]), 'currentState': array([12.9792148 , 13.96035815,  3.98580059]), 'targetState': array([12, 28], dtype=int32), 'currentDistance': 14.07374878890864}
done in step count: 35
reward sum = 0.4722187287953183
running average episode reward sum: 0.43882525162962366
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 28.]), 'previousTarget': array([12., 28.]), 'currentState': array([12.05313128, 27.07325124,  1.60205865]), 'targetState': array([12, 28], dtype=int32), 'currentDistance': 0.928270536678683}
episode index:1063
target Thresh 31.99938100146284
target distance 4.0
model initialize at round 1063
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 27.]), 'previousTarget': array([26., 27.]), 'currentState': array([22.       , 28.       ,  3.6629512]), 'targetState': array([26, 27], dtype=int32), 'currentDistance': 4.123105625617661}
done in step count: 13
reward sum = 0.7820418514094553
running average episode reward sum: 0.43914782362189797
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 27.]), 'previousTarget': array([26., 27.]), 'currentState': array([25.1851925 , 26.45196483,  0.14754359]), 'targetState': array([26, 27], dtype=int32), 'currentDistance': 0.9819642604371208}
episode index:1064
target Thresh 31.999387160601195
target distance 11.0
model initialize at round 1064
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  5.]), 'previousTarget': array([26.,  5.]), 'currentState': array([15.        ,  6.        ,  3.52328897]), 'targetState': array([26,  5], dtype=int32), 'currentDistance': 11.045361017187261}
done in step count: 44
reward sum = 0.45974495867003995
running average episode reward sum: 0.439167163654807
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  5.]), 'previousTarget': array([26.,  5.]), 'currentState': array([25.32684204,  4.27791199,  0.38361514]), 'targetState': array([26,  5], dtype=int32), 'currentDistance': 0.9871943771353342}
episode index:1065
target Thresh 31.999393258455097
target distance 21.0
model initialize at round 1065
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.48387745,  9.64835375]), 'previousTarget': array([13.50557744,  9.76952105]), 'currentState': array([19.11721149, 28.8385999 ,  5.21277094]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.310560525333047
running average episode reward sum: 0.43904651952880164
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([13.37699793,  8.80604905,  4.27726898]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 0.8898553279825112}
episode index:1066
target Thresh 31.99939929563434
target distance 4.0
model initialize at round 1066
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  7.]), 'previousTarget': array([11.,  7.]), 'currentState': array([10.97634465, 10.99671645,  3.50194329]), 'targetState': array([11,  7], dtype=int32), 'currentDistance': 3.9967864538291975}
done in step count: 10
reward sum = 0.8371662686975089
running average episode reward sum: 0.4394196401934396
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  7.]), 'previousTarget': array([11.,  7.]), 'currentState': array([10.23732931,  7.80229257,  4.75441927]), 'targetState': array([11,  7], dtype=int32), 'currentDistance': 1.106950743352131}
episode index:1067
target Thresh 31.999405272742646
target distance 3.0
model initialize at round 1067
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  3.]), 'previousTarget': array([10.,  3.]), 'currentState': array([10.        ,  6.        ,  0.81362098]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 3.0}
done in step count: 10
reward sum = 0.8338195046547032
running average episode reward sum: 0.4397889284560437
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  3.]), 'previousTarget': array([10.,  3.]), 'currentState': array([10.3111395 ,  3.88028364,  4.1187713 ]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 0.9336525494671943}
episode index:1068
target Thresh 31.99941119037773
target distance 8.0
model initialize at round 1068
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 14.]), 'previousTarget': array([11., 14.]), 'currentState': array([18.72089259,  9.87922941,  3.29746962]), 'targetState': array([11, 14], dtype=int32), 'currentDistance': 8.751738838253155}
done in step count: 18
reward sum = 0.6840389874513434
running average episode reward sum: 0.4400174130762451
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 14.]), 'previousTarget': array([11., 14.]), 'currentState': array([11.92892704, 13.65614474,  2.75404582]), 'targetState': array([11, 14], dtype=int32), 'currentDistance': 0.9905260601388288}
episode index:1069
target Thresh 31.999417049131363
target distance 2.0
model initialize at round 1069
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 16.]), 'previousTarget': array([24., 16.]), 'currentState': array([21.99748264, 15.9965157 ,  4.29267865]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 2.00252039302127}
done in step count: 7
reward sum = 0.8863959650233745
running average episode reward sum: 0.44043458929301815
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 16.]), 'previousTarget': array([24., 16.]), 'currentState': array([23.00304644, 15.60694583,  0.16690605]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 1.071637994909384}
episode index:1070
target Thresh 31.999422849589422
target distance 13.0
model initialize at round 1070
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 29.]), 'previousTarget': array([24., 29.]), 'currentState': array([11.        , 26.        ,  5.35125446]), 'targetState': array([24, 29], dtype=int32), 'currentDistance': 13.341664064126334}
done in step count: 30
reward sum = 0.5159149551505793
running average episode reward sum: 0.440505065825098
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 29.]), 'previousTarget': array([24., 29.]), 'currentState': array([23.08056192, 28.81059545,  0.11851604]), 'targetState': array([24, 29], dtype=int32), 'currentDistance': 0.9387440913363193}
episode index:1071
target Thresh 31.99942859233196
target distance 15.0
model initialize at round 1071
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 20.]), 'previousTarget': array([18., 20.]), 'currentState': array([ 3.18014597, 17.15678369,  0.55139586]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 15.090127644953837}
done in step count: 32
reward sum = 0.49942522556993685
running average episode reward sum: 0.44056002866068095
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 20.]), 'previousTarget': array([18., 20.]), 'currentState': array([17.03458462, 19.59203881,  0.40327784]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 1.048074041463375}
episode index:1072
target Thresh 31.999434277933254
target distance 5.0
model initialize at round 1072
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 22.]), 'previousTarget': array([15., 22.]), 'currentState': array([19.        , 17.        ,  5.59119469]), 'targetState': array([15, 22], dtype=int32), 'currentDistance': 6.4031242374328485}
done in step count: 18
reward sum = 0.6887050965645308
running average episode reward sum: 0.4407912915385037
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 22.]), 'previousTarget': array([15., 22.]), 'currentState': array([15.15309759, 21.07475305,  2.52074409]), 'targetState': array([15, 22], dtype=int32), 'currentDistance': 0.9378276944135568}
episode index:1073
target Thresh 31.999439906961868
target distance 19.0
model initialize at round 1073
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6.76960082, 24.98312553]), 'previousTarget': array([ 6.89888325, 24.86398076]), 'currentState': array([22.93704622, 13.20969319,  1.94906646]), 'targetState': array([ 4, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 76
reward sum = 0.24746097048724142
running average episode reward sum: 0.44061128192858634
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 27.]), 'previousTarget': array([ 4., 27.]), 'currentState': array([ 4.96401649, 26.60932819,  2.48896757]), 'targetState': array([ 4, 27], dtype=int32), 'currentDistance': 1.0401693401730443}
episode index:1074
target Thresh 31.999445479980714
target distance 18.0
model initialize at round 1074
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8.5517509 , 27.90803601]), 'previousTarget': array([ 8.78641543, 27.70981108]), 'currentState': array([24.90792325, 16.39823035,  1.75988667]), 'targetState': array([ 7, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.25521419544485613
running average episode reward sum: 0.440438819522555
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 29.]), 'previousTarget': array([ 7., 29.]), 'currentState': array([ 7.82051053, 28.66421722,  2.90428444]), 'targetState': array([ 7, 29], dtype=int32), 'currentDistance': 0.8865594171466901}
episode index:1075
target Thresh 31.999450997547097
target distance 6.0
model initialize at round 1075
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([13.        , 11.        ,  3.12609172]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 6.708203932499369}
done in step count: 20
reward sum = 0.6956323694684525
running average episode reward sum: 0.4406759882492705
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([16.03540267, 16.04025152,  0.93324735]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 0.9604012126568672}
episode index:1076
target Thresh 31.999456460212773
target distance 11.0
model initialize at round 1076
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  6.]), 'previousTarget': array([24.,  6.]), 'currentState': array([25.9173419 , 16.58269521,  4.53484851]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 10.754982002615714}
done in step count: 25
reward sum = 0.6162696690129792
running average episode reward sum: 0.44083902787857754
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  6.]), 'previousTarget': array([24.,  6.]), 'currentState': array([23.91342947,  6.9946082 ,  4.30809553]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 0.9983686357204697}
episode index:1077
target Thresh 31.99946186852402
target distance 14.0
model initialize at round 1077
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([17.82910937,  4.86435063,  3.60025215]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 14.751920567446858}
done in step count: 35
reward sum = 0.49024552607347116
running average episode reward sum: 0.44088485950955614
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.54339056, 9.00292408, 2.7535571 ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.1355323353035043}
episode index:1078
target Thresh 31.99946722302167
target distance 26.0
model initialize at round 1078
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.52673148,  8.44613953]), 'previousTarget': array([14.50280987,  8.51217609]), 'currentState': array([19.1360998 , 27.90773566,  5.55887491]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 92
reward sum = 0.1646740645173389
running average episode reward sum: 0.44062887174774684
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  2.]), 'previousTarget': array([13.,  2.]), 'currentState': array([13.06662204,  2.99670882,  4.36585148]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 0.9989329099286616}
episode index:1079
target Thresh 31.99947252424118
target distance 18.0
model initialize at round 1079
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.84499093, 12.6837115 ]), 'previousTarget': array([19.78704435, 12.72118773]), 'currentState': array([ 4.08489609, 24.9970949 ,  6.18271747]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 70
reward sum = 0.272505058863236
running average episode reward sum: 0.4404732015506316
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([21.06186233, 11.75889551,  5.63539654]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 1.2066584763409662}
episode index:1080
target Thresh 31.999477772712673
target distance 20.0
model initialize at round 1080
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.40628661, 21.2948796 ]), 'previousTarget': array([24.2384301 , 21.20729355]), 'currentState': array([ 6.11646605, 13.20275604,  1.05176264]), 'targetState': array([26, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 68
reward sum = 0.3011365322872622
running average episode reward sum: 0.4403443054643565
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 22.]), 'previousTarget': array([26., 22.]), 'currentState': array([25.084191  , 22.01569783,  0.3237618 ]), 'targetState': array([26, 22], dtype=int32), 'currentDistance': 0.9159435314637145}
episode index:1081
target Thresh 31.999482968961004
target distance 14.0
model initialize at round 1081
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 25.]), 'previousTarget': array([27., 25.]), 'currentState': array([12.98471961, 22.95404463,  4.64388418]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 14.16382779221835}
done in step count: 35
reward sum = 0.47585189427438596
running average episode reward sum: 0.44037712208987406
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 25.]), 'previousTarget': array([27., 25.]), 'currentState': array([26.08944999, 25.18664341,  0.36310384]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 0.9294821588436643}
episode index:1082
target Thresh 31.999488113505798
target distance 5.0
model initialize at round 1082
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 28.]), 'previousTarget': array([16., 28.]), 'currentState': array([17.        , 23.        ,  5.67293739]), 'targetState': array([16, 28], dtype=int32), 'currentDistance': 5.0990195135927845}
done in step count: 13
reward sum = 0.7627735031584045
running average episode reward sum: 0.4406748103457084
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 28.]), 'previousTarget': array([16., 28.]), 'currentState': array([16.47525984, 27.02987124,  1.97204988]), 'targetState': array([16, 28], dtype=int32), 'currentDistance': 1.0802878009678072}
episode index:1083
target Thresh 31.999493206861516
target distance 12.0
model initialize at round 1083
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([17.99267677,  8.01179868,  2.27735871]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 12.156366259566363}
done in step count: 27
reward sum = 0.5777330448965238
running average episode reward sum: 0.4408012478314564
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 6.96405199, 10.44706035,  2.86187004]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 1.0626660774237864}
episode index:1084
target Thresh 31.9994982495375
target distance 1.0
model initialize at round 1084
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 13.]), 'previousTarget': array([ 3., 13.]), 'currentState': array([ 2.03530725, 13.04642804,  0.85665441]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 0.9658093272967216}
done in step count: 0
reward sum = 0.9992218483255463
running average episode reward sum: 0.4413159211959671
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 13.]), 'previousTarget': array([ 3., 13.]), 'currentState': array([ 2.03530725, 13.04642804,  0.85665441]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 0.9658093272967216}
episode index:1085
target Thresh 31.999503242038017
target distance 18.0
model initialize at round 1085
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.8032423 ,  3.65344009]), 'previousTarget': array([13.70981108,  3.78641543]), 'currentState': array([ 2.07665233, 19.85489335,  5.19973071]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.3209764281215329
running average episode reward sum: 0.44120511134967383
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.23095426,  2.72905582,  5.14636532]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 1.0596951132993595}
episode index:1086
target Thresh 31.999508184862325
target distance 11.0
model initialize at round 1086
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([15.00670424,  5.93660012,  4.56614661]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 11.006886833867506}
done in step count: 24
reward sum = 0.5785410389969724
running average episode reward sum: 0.44133145534934937
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.92489885, 6.02303006, 2.86971488]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.9251855336856695}
episode index:1087
target Thresh 31.999513078504712
target distance 17.0
model initialize at round 1087
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.14213562, 11.85786438]), 'previousTarget': array([21.14213562, 11.85786438]), 'currentState': array([ 7.        , 26.        ,  1.49920109]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 89
reward sum = 0.1925389728701603
running average episode reward sum: 0.4411027857882472
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  9.]), 'previousTarget': array([24.,  9.]), 'currentState': array([23.1250149 ,  9.76362418,  5.2625079 ]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 1.161344398050336}
episode index:1088
target Thresh 31.99951792345454
target distance 5.0
model initialize at round 1088
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([8.64580787, 1.82174911, 3.45121628]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 4.792891182409115}
done in step count: 9
reward sum = 0.8253359812828636
running average episode reward sum: 0.44145561700541397
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.89323455, 3.06378787, 2.63865771]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.8955092687165012}
episode index:1089
target Thresh 31.999522720196314
target distance 8.0
model initialize at round 1089
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 10.]), 'previousTarget': array([25., 10.]), 'currentState': array([16.96260234, 11.88828965,  4.61250496]), 'targetState': array([25, 10], dtype=int32), 'currentDistance': 8.256233939246227}
done in step count: 17
reward sum = 0.6841303505749429
running average episode reward sum: 0.4416782543756612
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 10.]), 'previousTarget': array([25., 10.]), 'currentState': array([24.00039313,  9.90826057,  5.98737657]), 'targetState': array([25, 10], dtype=int32), 'currentDistance': 1.0038077639110716}
episode index:1090
target Thresh 31.999527469209706
target distance 8.0
model initialize at round 1090
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 22.]), 'previousTarget': array([23., 22.]), 'currentState': array([15.22676346, 20.18847899,  0.55009195]), 'targetState': array([23, 22], dtype=int32), 'currentDistance': 7.981529594209094}
done in step count: 15
reward sum = 0.7296339563685548
running average episode reward sum: 0.4419421917743715
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 22.]), 'previousTarget': array([23., 22.]), 'currentState': array([22.04285561, 21.19732419,  0.087752  ]), 'targetState': array([23, 22], dtype=int32), 'currentDistance': 1.2491652570530487}
episode index:1091
target Thresh 31.99953217096963
target distance 17.0
model initialize at round 1091
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([13.79014052,  5.05986762,  2.6975033 ]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 17.034455907256003}
done in step count: 43
reward sum = 0.42959549833186117
running average episode reward sum: 0.441930885278545
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([12.19533461, 21.02648927,  1.95183542]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 0.9929142699947192}
episode index:1092
target Thresh 31.999536825946258
target distance 17.0
model initialize at round 1092
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 15.]), 'previousTarget': array([ 5., 15.]), 'currentState': array([22.01434132, 20.97152695,  4.9849565 ]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 18.031831422968626}
done in step count: 47
reward sum = 0.3890033188251353
running average episode reward sum: 0.44188246115553187
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 15.]), 'previousTarget': array([ 5., 15.]), 'currentState': array([ 5.92462115, 15.87005873,  3.505176  ]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 1.2696166592171536}
episode index:1093
target Thresh 31.999541434605096
target distance 21.0
model initialize at round 1093
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.93847454,  9.62337985]), 'previousTarget': array([12.92277877,  9.63513716]), 'currentState': array([ 3.04818067, 27.00676522,  6.21587402]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 86
reward sum = 0.21236996102921663
running average episode reward sum: 0.4416726691078844
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.01893571,  6.58994953,  5.06187874]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 1.1447827644160407}
episode index:1094
target Thresh 31.99954599740701
target distance 13.0
model initialize at round 1094
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([18.79555975, 23.10956956,  2.75104162]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 12.9344530829337}
done in step count: 30
reward sum = 0.5634917994128678
running average episode reward sum: 0.44178391945519485
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([ 6.93609958, 25.37501386,  3.04999436]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 1.0084234332682982}
episode index:1095
target Thresh 31.99955051480829
target distance 16.0
model initialize at round 1095
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([6.16222603, 2.01077568, 0.12826027]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 18.211019693980365}
done in step count: 48
reward sum = 0.3954500757555458
running average episode reward sum: 0.4417416440503594
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([21.92074736, 10.00877066,  0.76946676]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 0.994392570264723}
episode index:1096
target Thresh 31.999554987260677
target distance 10.0
model initialize at round 1096
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([ 6.98092952, 27.97344855,  4.34202194]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 12.206996268858743}
done in step count: 28
reward sum = 0.5597775330477412
running average episode reward sum: 0.44184924285527954
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([16.01784085, 21.51144348,  5.51180686]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 1.1073441307255145}
episode index:1097
target Thresh 31.999559415211415
target distance 1.0
model initialize at round 1097
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 29.]), 'previousTarget': array([18., 29.]), 'currentState': array([17.0460642 , 28.94136497,  5.43132892]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 0.9557361442039503}
done in step count: 0
reward sum = 0.9993362147469196
running average episode reward sum: 0.44235697233787663
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 29.]), 'previousTarget': array([18., 29.]), 'currentState': array([17.0460642 , 28.94136497,  5.43132892]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 0.9557361442039503}
episode index:1098
target Thresh 31.99956379910331
target distance 3.0
model initialize at round 1098
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 15.]), 'previousTarget': array([ 9., 15.]), 'currentState': array([12.        , 17.        ,  0.59242857]), 'targetState': array([ 9, 15], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 19
reward sum = 0.7398121669078374
running average episode reward sum: 0.44262763220554724
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 15.]), 'previousTarget': array([ 9., 15.]), 'currentState': array([ 8.82835173, 15.96076922,  3.80899307]), 'targetState': array([ 9, 15], dtype=int32), 'currentDistance': 0.9759818788344368}
episode index:1099
target Thresh 31.99956813937475
target distance 15.0
model initialize at round 1099
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([11.06645188,  4.30436598,  1.39185978]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 18.316771921772084}
done in step count: 48
reward sum = 0.394231083761174
running average episode reward sum: 0.44258363534332507
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([21.72665752, 18.07176449,  0.71758187]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 0.9676452261212679}
episode index:1100
target Thresh 31.99957243645977
target distance 22.0
model initialize at round 1100
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.54573816, 18.30753542]), 'previousTarget': array([20.52454686, 18.26673649]), 'currentState': array([3.9840405 , 7.09548072, 1.67975466]), 'targetState': array([26, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 95
reward sum = 0.17600263400090052
running average episode reward sum: 0.4423415090932411
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 22.]), 'previousTarget': array([26., 22.]), 'currentState': array([25.12007266, 22.11485901,  0.1559424 ]), 'targetState': array([26, 22], dtype=int32), 'currentDistance': 0.8873920823856077}
episode index:1101
target Thresh 31.99957669078808
target distance 20.0
model initialize at round 1101
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2.84313238, 19.74739399]), 'previousTarget': array([ 2.8434743 , 19.74695771]), 'currentState': array([22.00174383, 14.00739335,  1.56006706]), 'targetState': array([ 2, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.31178801552224433
running average episode reward sum: 0.4422230394983491
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 20.]), 'previousTarget': array([ 2., 20.]), 'currentState': array([ 2.93576276, 20.51097219,  2.98113566]), 'targetState': array([ 2, 20], dtype=int32), 'currentDistance': 1.0661822185371455}
episode index:1102
target Thresh 31.999580902785112
target distance 13.0
model initialize at round 1102
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  5.]), 'previousTarget': array([25.,  5.]), 'currentState': array([12.        ,  8.        ,  5.07011253]), 'targetState': array([25,  5], dtype=int32), 'currentDistance': 13.341664064126332}
done in step count: 32
reward sum = 0.5178260496732285
running average episode reward sum: 0.44229158257194373
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  5.]), 'previousTarget': array([25.,  5.]), 'currentState': array([24.02847037,  4.32428193,  6.02923555]), 'targetState': array([25,  5], dtype=int32), 'currentDistance': 1.183412331187795}
episode index:1103
target Thresh 31.999585072872076
target distance 10.0
model initialize at round 1103
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  6.]), 'previousTarget': array([20.,  6.]), 'currentState': array([17.        , 16.        ,  0.66047621]), 'targetState': array([20,  6], dtype=int32), 'currentDistance': 10.44030650891055}
done in step count: 24
reward sum = 0.5871099822537429
running average episode reward sum: 0.442422758658612
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  6.]), 'previousTarget': array([20.,  6.]), 'currentState': array([19.83103379,  6.8842042 ,  4.40855888]), 'targetState': array([20,  6], dtype=int32), 'currentDistance': 0.9002036661263128}
episode index:1104
target Thresh 31.99958920146598
target distance 20.0
model initialize at round 1104
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.96980461,  6.14868799]), 'previousTarget': array([18.9223227 ,  6.38838649]), 'currentState': array([14.98947415, 25.74861067,  4.64866708]), 'targetState': array([19,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.3652642601661969
running average episode reward sum: 0.4423529319631438
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  6.]), 'previousTarget': array([19.,  6.]), 'currentState': array([18.4156191,  6.9052221,  4.9195877]), 'targetState': array([19,  6], dtype=int32), 'currentDistance': 1.0774637279456885}
episode index:1105
target Thresh 31.99959328897969
target distance 16.0
model initialize at round 1105
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.65536023, 10.4225371 ]), 'previousTarget': array([25.6118525, 10.47772  ]), 'currentState': array([13.01418118, 25.92094321,  4.94426523]), 'targetState': array([26, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.3534116256685799
running average episode reward sum: 0.4422725148688449
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 10.]), 'previousTarget': array([26., 10.]), 'currentState': array([25.00640826, 10.23194525,  5.63930611]), 'targetState': array([26, 10], dtype=int32), 'currentDistance': 1.020305415276629}
episode index:1106
target Thresh 31.999597335821957
target distance 8.0
model initialize at round 1106
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 22.]), 'previousTarget': array([23., 22.]), 'currentState': array([15.        , 29.        ,  4.79567635]), 'targetState': array([23, 22], dtype=int32), 'currentDistance': 10.63014581273465}
done in step count: 25
reward sum = 0.622005792097653
running average episode reward sum: 0.44243487555288175
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 22.]), 'previousTarget': array([23., 22.]), 'currentState': array([22.08384136, 22.42236974,  5.25124035]), 'targetState': array([23, 22], dtype=int32), 'currentDistance': 1.0088324202682495}
episode index:1107
target Thresh 31.999601342397472
target distance 20.0
model initialize at round 1107
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.14068384, 12.87079282]), 'previousTarget': array([18.14213562, 12.85786438]), 'currentState': array([ 4.01239156, 27.02675825,  0.88460779]), 'targetState': array([24,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.22701379530480453
running average episode reward sum: 0.44183068000156617
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  7.]), 'previousTarget': array([24.,  7.]), 'currentState': array([23.0583829 ,  8.3307993 ,  5.43340338]), 'targetState': array([24,  7], dtype=int32), 'currentDistance': 1.6302360336904436}
episode index:1108
target Thresh 31.999605309106894
target distance 10.0
model initialize at round 1108
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([ 8.        , 14.        ,  0.87281865]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 13.45362404707371}
done in step count: 32
reward sum = 0.521076681860104
running average episode reward sum: 0.4419021371718624
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([16.09744302,  4.97189341,  5.33278147]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 1.3263430521871105}
episode index:1109
target Thresh 31.9996092363469
target distance 21.0
model initialize at round 1109
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.02341746, 11.48003363]), 'previousTarget': array([23.00530293, 11.47290771]), 'currentState': array([ 6.0532355 , 22.06364947,  0.62175941]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.11990447282003269
running average episode reward sum: 0.44139600509078863
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([21.14524538, 15.02579251,  5.3825182 ]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 8.401685964762905}
episode index:1110
target Thresh 31.999613124510212
target distance 8.0
model initialize at round 1110
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  8.]), 'previousTarget': array([25.,  8.]), 'currentState': array([27.00530842, 16.00933022,  0.96954164]), 'targetState': array([25,  8], dtype=int32), 'currentDistance': 8.256550875300055}
done in step count: 23
reward sum = 0.6357966277891826
running average episode reward sum: 0.44157098314902304
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  8.]), 'previousTarget': array([25.,  8.]), 'currentState': array([24.81575269,  8.9327068 ,  4.29880947]), 'targetState': array([25,  8], dtype=int32), 'currentDistance': 0.9507307911442907}
episode index:1111
target Thresh 31.999616973985653
target distance 12.0
model initialize at round 1111
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 17.]), 'previousTarget': array([ 3., 17.]), 'currentState': array([14.9459269 , 21.02555973,  2.81645566]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 12.605962903308404}
done in step count: 31
reward sum = 0.5498684139578452
running average episode reward sum: 0.4416683729249303
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 17.]), 'previousTarget': array([ 3., 17.]), 'currentState': array([ 3.33980845, 17.95720955,  3.65613126]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 1.0157361413897088}
episode index:1112
target Thresh 31.999620785158175
target distance 24.0
model initialize at round 1112
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.98556283, 20.83183709]), 'previousTarget': array([22.98266146, 20.83261089]), 'currentState': array([ 3.0030871 , 19.99478044,  5.43350068]), 'targetState': array([27, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 70
reward sum = 0.25917653012098807
running average episode reward sum: 0.4415044090050705
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 21.]), 'previousTarget': array([27., 21.]), 'currentState': array([2.60795632e+01, 2.05325727e+01, 1.53116541e-02]), 'targetState': array([27, 21], dtype=int32), 'currentDistance': 1.0323236345253413}
episode index:1113
target Thresh 31.999624558408897
target distance 22.0
model initialize at round 1113
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.59821281,  5.45941246]), 'previousTarget': array([11.57265691,  5.48906088]), 'currentState': array([ 4.08722763, 23.99546117,  6.18493021]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 70
reward sum = 0.23394127217122207
running average episode reward sum: 0.44131808662012084
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  2.]), 'previousTarget': array([13.,  2.]), 'currentState': array([13.5551489 ,  2.93036998,  4.76724275]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 1.0834106305010116}
episode index:1114
target Thresh 31.999628294115144
target distance 18.0
model initialize at round 1114
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([26.9848285 ,  9.99281173,  3.40140453]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 17.984829933434707}
done in step count: 42
reward sum = 0.41673716687122897
running average episode reward sum: 0.4412960409521846
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([9.87937707, 9.78847117, 3.19533029]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.904460325867448}
episode index:1115
target Thresh 31.999631992650496
target distance 19.0
model initialize at round 1115
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 17.]), 'previousTarget': array([ 6., 17.]), 'currentState': array([25.       , 16.       ,  0.5292532]), 'targetState': array([ 6, 17], dtype=int32), 'currentDistance': 19.026297590440446}
done in step count: 53
reward sum = 0.32359006415826397
running average episode reward sum: 0.44119056964681375
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 17.]), 'previousTarget': array([ 6., 17.]), 'currentState': array([ 6.92291451, 16.71649052,  3.00652842]), 'targetState': array([ 6, 17], dtype=int32), 'currentDistance': 0.9654785438400801}
episode index:1116
target Thresh 31.999635654384804
target distance 2.0
model initialize at round 1116
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 15.]), 'previousTarget': array([14., 15.]), 'currentState': array([16.        , 15.        ,  4.96415124]), 'targetState': array([14, 15], dtype=int32), 'currentDistance': 2.0}
done in step count: 15
reward sum = 0.8422685184327318
running average episode reward sum: 0.4415496367451001
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 15.]), 'previousTarget': array([14., 15.]), 'currentState': array([14.93431632, 14.33846844,  3.24405382]), 'targetState': array([14, 15], dtype=int32), 'currentDistance': 1.1448017286600265}
episode index:1117
target Thresh 31.99963927968425
target distance 18.0
model initialize at round 1117
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 24.]), 'previousTarget': array([15., 24.]), 'currentState': array([16.88116139,  6.10480812,  2.25820878]), 'targetState': array([15, 24], dtype=int32), 'currentDistance': 17.993795055895664}
done in step count: 42
reward sum = 0.4049286388667226
running average episode reward sum: 0.44151688093304436
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 24.]), 'previousTarget': array([15., 24.]), 'currentState': array([15.35835463, 23.09226014,  1.42959055]), 'targetState': array([15, 24], dtype=int32), 'currentDistance': 0.9759147943904158}
episode index:1118
target Thresh 31.99964286891136
target distance 16.0
model initialize at round 1118
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([14.06797631, 18.76197084,  4.90890674]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 15.879938293923393}
done in step count: 44
reward sum = 0.46902621362867325
running average episode reward sum: 0.4415414647871066
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.97549356,  3.991271  ,  4.69973341]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.9915738760908175}
episode index:1119
target Thresh 31.999646422425066
target distance 11.0
model initialize at round 1119
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 13.]), 'previousTarget': array([ 6., 13.]), 'currentState': array([17.        ,  3.        ,  3.97771454]), 'targetState': array([ 6, 13], dtype=int32), 'currentDistance': 14.866068747318508}
done in step count: 41
reward sum = 0.4619120060356159
running average episode reward sum: 0.4415596527703642
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 13.]), 'previousTarget': array([ 6., 13.]), 'currentState': array([ 6.87309433, 12.71042509,  2.43409906]), 'targetState': array([ 6, 13], dtype=int32), 'currentDistance': 0.9198626697884547}
episode index:1120
target Thresh 31.999649940580717
target distance 14.0
model initialize at round 1120
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  4.]), 'previousTarget': array([10.,  4.]), 'currentState': array([21.        , 18.        ,  5.71480137]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 17.804493814764857}
done in step count: 48
reward sum = 0.3957633536983949
running average episode reward sum: 0.44151879969358276
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  4.]), 'previousTarget': array([10.,  4.]), 'currentState': array([10.7293815 ,  4.8318465 ,  4.02379486]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 1.1063299536188713}
episode index:1121
target Thresh 31.999653423730138
target distance 19.0
model initialize at round 1121
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([20.95101983, 28.9734757 ,  3.78617331]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 19.608808667498696}
done in step count: 50
reward sum = 0.364278300783931
running average episode reward sum: 0.44144995789419805
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.77259947, 10.83014655,  4.542994  ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.8607289269008069}
episode index:1122
target Thresh 31.999656872221642
target distance 20.0
model initialize at round 1122
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.21170113, 11.18904592]), 'previousTarget': array([12.361625  , 11.47568183]), 'currentState': array([21.89745494, 28.6872222 ,  4.25165069]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.3043178355673242
running average episode reward sum: 0.4413278455858036
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([11.12590234,  9.92677635,  3.91424945]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.9352891533595485}
episode index:1123
target Thresh 31.999660286400083
target distance 17.0
model initialize at round 1123
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([2.94479335, 2.97295532, 3.83415911]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 17.175243345133907}
done in step count: 54
reward sum = 0.3560910561129802
running average episode reward sum: 0.44125201214321214
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([19.00069637,  5.07948073,  0.28805996]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 1.00245944528593}
episode index:1124
target Thresh 31.999663666606878
target distance 8.0
model initialize at round 1124
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 28.]), 'previousTarget': array([20., 28.]), 'currentState': array([22.00559017, 20.34254546,  1.61976768]), 'targetState': array([20, 28], dtype=int32), 'currentDistance': 7.915743928112624}
done in step count: 19
reward sum = 0.7244832378001473
running average episode reward sum: 0.44150377323268497
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 28.]), 'previousTarget': array([20., 28.]), 'currentState': array([20.79117733, 27.14036419,  2.08354178]), 'targetState': array([20, 28], dtype=int32), 'currentDistance': 1.1683044512089034}
episode index:1125
target Thresh 31.999667013180055
target distance 10.0
model initialize at round 1125
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 17.]), 'previousTarget': array([ 4., 17.]), 'currentState': array([9.        , 7.        , 4.34209585]), 'targetState': array([ 4, 17], dtype=int32), 'currentDistance': 11.180339887498949}
done in step count: 26
reward sum = 0.5569370728844398
running average episode reward sum: 0.44160628948459596
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 17.]), 'previousTarget': array([ 4., 17.]), 'currentState': array([ 4.85217824, 16.47580004,  2.1967903 ]), 'targetState': array([ 4, 17], dtype=int32), 'currentDistance': 1.0004965510935444}
episode index:1126
target Thresh 31.999670326454275
target distance 10.0
model initialize at round 1126
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 16.]), 'previousTarget': array([12., 16.]), 'currentState': array([ 1.98779593, 11.05749192,  1.53071943]), 'targetState': array([12, 16], dtype=int32), 'currentDistance': 11.165689253149573}
done in step count: 24
reward sum = 0.5966619542226888
running average episode reward sum: 0.44174387215073446
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 16.]), 'previousTarget': array([12., 16.]), 'currentState': array([11.25836144, 15.67077183,  0.42806047]), 'targetState': array([12, 16], dtype=int32), 'currentDistance': 0.8114301873392709}
episode index:1127
target Thresh 31.99967360676086
target distance 5.0
model initialize at round 1127
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([16.00824858, 15.0499142 ,  1.65369233]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 5.434583310400569}
done in step count: 15
reward sum = 0.7312805183376669
running average episode reward sum: 0.44200055357465906
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.05375753, 10.94408275,  4.40081544]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.9456120329003453}
episode index:1128
target Thresh 31.999676854427854
target distance 21.0
model initialize at round 1128
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6.39026603, 24.6744179 ]), 'previousTarget': array([ 6.54387571, 24.63241055]), 'currentState': array([25.8634057 , 20.11406357,  2.50808445]), 'targetState': array([ 5, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.32328014676558325
running average episode reward sum: 0.4418953982099034
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 25.]), 'previousTarget': array([ 5., 25.]), 'currentState': array([ 5.80812376, 25.25306957,  2.94666407]), 'targetState': array([ 5, 25], dtype=int32), 'currentDistance': 0.8468224210592223}
episode index:1129
target Thresh 31.99968006978002
target distance 12.0
model initialize at round 1129
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 25.]), 'previousTarget': array([ 3., 25.]), 'currentState': array([15.        , 24.        ,  4.92426062]), 'targetState': array([ 3, 25], dtype=int32), 'currentDistance': 12.041594578792294}
done in step count: 31
reward sum = 0.5207309878944211
running average episode reward sum: 0.44196516421847376
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 25.]), 'previousTarget': array([ 3., 25.]), 'currentState': array([ 3.69163713, 25.98708967,  3.54968914]), 'targetState': array([ 3, 25], dtype=int32), 'currentDistance': 1.2052833388916697}
episode index:1130
target Thresh 31.999683253138897
target distance 8.0
model initialize at round 1130
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 7.]), 'previousTarget': array([7., 7.]), 'currentState': array([12.75199499, 14.66168582,  4.06328034]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 9.580546744913995}
done in step count: 19
reward sum = 0.6750743135508541
running average episode reward sum: 0.44217127310382515
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 7.]), 'previousTarget': array([7., 7.]), 'currentState': array([7.43444972, 7.97453652, 4.02859993]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 1.066990155395457}
episode index:1131
target Thresh 31.999686404822825
target distance 3.0
model initialize at round 1131
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 22.]), 'previousTarget': array([ 8., 22.]), 'currentState': array([11.00455951, 21.02122263,  1.60666159]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 3.159965661097066}
done in step count: 10
reward sum = 0.856712170190357
running average episode reward sum: 0.442537475309732
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 22.]), 'previousTarget': array([ 8., 22.]), 'currentState': array([ 8.94937788, 22.23088577,  2.9999985 ]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 0.9770499510824251}
episode index:1132
target Thresh 31.999689525146973
target distance 10.0
model initialize at round 1132
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 14.]), 'previousTarget': array([17., 14.]), 'currentState': array([27.        , 13.        ,  4.79175138]), 'targetState': array([17, 14], dtype=int32), 'currentDistance': 10.049875621120892}
done in step count: 27
reward sum = 0.6073939260916822
running average episode reward sum: 0.44268297967935416
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 14.]), 'previousTarget': array([17., 14.]), 'currentState': array([17.99186334, 13.6257105 ,  2.95578064]), 'targetState': array([17, 14], dtype=int32), 'currentDistance': 1.060134670790384}
episode index:1133
target Thresh 31.999692614423378
target distance 4.0
model initialize at round 1133
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([ 5.28479305, 23.61763792,  5.42681545]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 4.544754226497612}
done in step count: 8
reward sum = 0.8459137820108884
running average episode reward sum: 0.44303856239745953
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([ 8.21828   , 21.44028811,  5.70546963]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 0.8971843633822766}
episode index:1134
target Thresh 31.999695672960968
target distance 16.0
model initialize at round 1134
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([3.97134252, 4.02475826, 2.19097963]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 18.37043344299736}
done in step count: 44
reward sum = 0.37643496981347196
running average episode reward sum: 0.4429798808180904
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([19.13405321, 12.43914363,  0.52994851]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 1.0317091199578081}
episode index:1135
target Thresh 31.9996987010656
target distance 9.0
model initialize at round 1135
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 14.]), 'previousTarget': array([ 9., 14.]), 'currentState': array([6.       , 5.       , 6.1727066]), 'targetState': array([ 9, 14], dtype=int32), 'currentDistance': 9.486832980505138}
done in step count: 24
reward sum = 0.6292750135896994
running average episode reward sum: 0.4431438730124316
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 14.]), 'previousTarget': array([ 9., 14.]), 'currentState': array([ 9.32199928, 13.04644826,  1.21416134]), 'targetState': array([ 9, 14], dtype=int32), 'currentDistance': 1.0064514177287094}
episode index:1136
target Thresh 31.99970169904009
target distance 17.0
model initialize at round 1136
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 22.]), 'previousTarget': array([ 7., 22.]), 'currentState': array([7.90086747, 5.14521744, 1.92716941]), 'targetState': array([ 7, 22], dtype=int32), 'currentDistance': 16.87884052411381}
done in step count: 40
reward sum = 0.435494838572
running average episode reward sum: 0.4431371456294585
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 22.]), 'previousTarget': array([ 7., 22.]), 'currentState': array([ 6.84339384, 21.05372698,  1.60762675]), 'targetState': array([ 7, 22], dtype=int32), 'currentDistance': 0.9591444716252229}
episode index:1137
target Thresh 31.999704667184233
target distance 16.0
model initialize at round 1137
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 26.]), 'previousTarget': array([23., 26.]), 'currentState': array([ 7.        , 23.        ,  5.48571205]), 'targetState': array([23, 26], dtype=int32), 'currentDistance': 16.278820596099706}
done in step count: 36
reward sum = 0.44021823555763595
running average episode reward sum: 0.44313458068211947
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 26.]), 'previousTarget': array([23., 26.]), 'currentState': array([22.07545712, 25.49920952,  0.48043278]), 'targetState': array([23, 26], dtype=int32), 'currentDistance': 1.0514611961341618}
episode index:1138
target Thresh 31.99970760579485
target distance 9.0
model initialize at round 1138
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 28.]), 'previousTarget': array([20., 28.]), 'currentState': array([20.        , 19.        ,  0.24806109]), 'targetState': array([20, 28], dtype=int32), 'currentDistance': 9.0}
done in step count: 21
reward sum = 0.6534964579223442
running average episode reward sum: 0.4433192706533576
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 28.]), 'previousTarget': array([20., 28.]), 'currentState': array([19.82125425, 27.0972056 ,  1.84752524]), 'targetState': array([20, 28], dtype=int32), 'currentDistance': 0.9203193902565316}
episode index:1139
target Thresh 31.999710515165802
target distance 20.0
model initialize at round 1139
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  6.]), 'previousTarget': array([24.9007438 ,  5.99007438]), 'currentState': array([5.17786684, 3.9694819 , 6.20231748]), 'targetState': array([25,  6], dtype=int32), 'currentDistance': 19.92586175981342}
done in step count: 49
reward sum = 0.36554242269149073
running average episode reward sum: 0.44325104534812787
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  6.]), 'previousTarget': array([25.,  6.]), 'currentState': array([24.27802262,  5.83807749,  0.18659404]), 'targetState': array([25,  6], dtype=int32), 'currentDistance': 0.7399123203419891}
episode index:1140
target Thresh 31.999713395588028
target distance 20.0
model initialize at round 1140
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9.56870361, 20.5880072 ]), 'previousTarget': array([ 9.57218647, 20.56953382]), 'currentState': array([17.04074284,  2.03622481,  0.82791218]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.16194735534041924
running average episode reward sum: 0.4427206348304341
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9.48297119, 21.11864752]), 'previousTarget': array([ 9.48297119, 21.11864752]), 'currentState': array([19.09425077,  3.57945413,  0.84593986]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 20.0}
episode index:1141
target Thresh 31.99971624734958
target distance 4.0
model initialize at round 1141
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 12.]), 'previousTarget': array([14., 12.]), 'currentState': array([10.        , 15.        ,  3.31977272]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 5.000000000000001}
done in step count: 13
reward sum = 0.7676166209959718
running average episode reward sum: 0.4430051321913496
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 12.]), 'previousTarget': array([14., 12.]), 'currentState': array([13.06320117, 12.6695247 ,  5.96910205]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 1.151457936105888}
episode index:1142
target Thresh 31.999719070735622
target distance 14.0
model initialize at round 1142
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 17.]), 'previousTarget': array([14., 17.]), 'currentState': array([11.00415855,  3.00279608,  0.70684516]), 'targetState': array([14, 17], dtype=int32), 'currentDistance': 14.314216131511653}
done in step count: 33
reward sum = 0.516204365901064
running average episode reward sum: 0.44306917351568004
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 17.]), 'previousTarget': array([14., 17.]), 'currentState': array([14.13417484, 16.07227358,  1.32978589]), 'targetState': array([14, 17], dtype=int32), 'currentDistance': 0.9373788996478614}
episode index:1143
target Thresh 31.99972186602851
target distance 3.0
model initialize at round 1143
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 21.]), 'previousTarget': array([10., 21.]), 'currentState': array([ 8.28711467, 23.6188086 ,  5.30847847]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 3.1292386663350866}
done in step count: 5
reward sum = 0.9083289429007888
running average episode reward sum: 0.4434758691182894
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 21.]), 'previousTarget': array([10., 21.]), 'currentState': array([ 9.27926578, 21.84794326,  5.28974142]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 1.1128636874973865}
episode index:1144
target Thresh 31.999724633507764
target distance 12.0
model initialize at round 1144
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 13.]), 'previousTarget': array([19., 13.]), 'currentState': array([21.        , 25.        ,  0.16547397]), 'targetState': array([19, 13], dtype=int32), 'currentDistance': 12.165525060596439}
done in step count: 30
reward sum = 0.555324121013342
running average episode reward sum: 0.4435735531810798
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 13.]), 'previousTarget': array([19., 13.]), 'currentState': array([19.32289879, 13.91474829,  4.56915848]), 'targetState': array([19, 13], dtype=int32), 'currentDistance': 0.9700660059799024}
episode index:1145
target Thresh 31.999727373450142
target distance 24.0
model initialize at round 1145
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.52897976, 19.57106303]), 'previousTarget': array([14.52566297, 19.58583933]), 'currentState': array([5.02728528, 1.97226188, 5.74205732]), 'targetState': array([18, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 95
reward sum = 0.16460223426922968
running average episode reward sum: 0.44333012271082517
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 26.]), 'previousTarget': array([18., 26.]), 'currentState': array([17.65720914, 25.03290436,  1.08375725]), 'targetState': array([18, 26], dtype=int32), 'currentDistance': 1.0260504604981018}
episode index:1146
target Thresh 31.999730086129638
target distance 12.0
model initialize at round 1146
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 6.]), 'previousTarget': array([8., 6.]), 'currentState': array([15.97967058, 17.84016801,  4.46514651]), 'targetState': array([8, 6], dtype=int32), 'currentDistance': 14.278120361847598}
done in step count: 49
reward sum = 0.367068351668976
running average episode reward sum: 0.44326363468027435
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 6.]), 'previousTarget': array([8., 6.]), 'currentState': array([8.91925937, 6.47575701, 3.08933026]), 'targetState': array([8, 6], dtype=int32), 'currentDistance': 1.0350760955820728}
episode index:1147
target Thresh 31.99973277181752
target distance 14.0
model initialize at round 1147
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 15.]), 'previousTarget': array([23., 15.]), 'currentState': array([ 8.99303596, 21.00188282,  3.13004279]), 'targetState': array([23, 15], dtype=int32), 'currentDistance': 15.238688886659373}
done in step count: 45
reward sum = 0.41526892825898853
running average episode reward sum: 0.44323924904750317
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 15.]), 'previousTarget': array([23., 15.]), 'currentState': array([22.04124703, 15.14365082,  5.90378291]), 'targetState': array([23, 15], dtype=int32), 'currentDistance': 0.9694549107210342}
episode index:1148
target Thresh 31.999735430782362
target distance 16.0
model initialize at round 1148
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 21.]), 'previousTarget': array([20., 21.]), 'currentState': array([21.79241868,  5.13647432,  2.40318587]), 'targetState': array([20, 21], dtype=int32), 'currentDistance': 15.964467160574186}
done in step count: 37
reward sum = 0.46524696183164455
running average episode reward sum: 0.4432584028445303
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 21.]), 'previousTarget': array([20., 21.]), 'currentState': array([20.25317266, 20.21002464,  1.57879636]), 'targetState': array([20, 21], dtype=int32), 'currentDistance': 0.8295525733555855}
episode index:1149
target Thresh 31.999738063290064
target distance 6.0
model initialize at round 1149
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  9.]), 'previousTarget': array([19.,  9.]), 'currentState': array([12.96695842, 13.98999306,  3.6759235 ]), 'targetState': array([19,  9], dtype=int32), 'currentDistance': 7.829279752601188}
done in step count: 20
reward sum = 0.6778624199264552
running average episode reward sum: 0.443462406337645
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  9.]), 'previousTarget': array([19.,  9.]), 'currentState': array([18.11550406,  9.65983963,  5.63018363]), 'targetState': array([19,  9], dtype=int32), 'currentDistance': 1.103504146221235}
episode index:1150
target Thresh 31.999740669603874
target distance 6.0
model initialize at round 1150
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([11.98210764, 27.93922057,  4.52955574]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 7.775190336900533}
done in step count: 15
reward sum = 0.7298397453290751
running average episode reward sum: 0.4437112137564038
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([16.3049046 , 22.87346588,  5.39327718]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 1.1162886112598258}
episode index:1151
target Thresh 31.99974324998443
target distance 4.0
model initialize at round 1151
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([14.00225359,  4.98587072,  5.10987499]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 4.137359797713896}
done in step count: 15
reward sum = 0.7633498276493931
running average episode reward sum: 0.4439886778309637
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.39986676,  8.16324031,  1.97297897]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.9273943074866708}
episode index:1152
target Thresh 31.99974580468977
target distance 13.0
model initialize at round 1152
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  2.]), 'previousTarget': array([23.,  2.]), 'currentState': array([10.        ,  6.        ,  0.68277675]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 13.601470508735444}
done in step count: 55
reward sum = 0.3648680853192104
running average episode reward sum: 0.4439200563283516
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  2.]), 'previousTarget': array([23.,  2.]), 'currentState': array([22.49133238,  2.97248694,  4.33829787]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 1.0974851215004842}
episode index:1153
target Thresh 31.99974833397537
target distance 6.0
model initialize at round 1153
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 10.]), 'previousTarget': array([18., 10.]), 'currentState': array([23.56289648, 10.16664691,  2.88172488]), 'targetState': array([18, 10], dtype=int32), 'currentDistance': 5.565392033465416}
done in step count: 11
reward sum = 0.8053091081444365
running average episode reward sum: 0.44423321841831354
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 10.]), 'previousTarget': array([18., 10.]), 'currentState': array([18.88682322, 10.3800794 ,  3.12205304]), 'targetState': array([18, 10], dtype=int32), 'currentDistance': 0.9648397639746473}
episode index:1154
target Thresh 31.99975083809415
target distance 17.0
model initialize at round 1154
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.76756726, 18.99675711]), 'previousTarget': array([10.76756726, 18.99675711]), 'currentState': array([24.        ,  4.        ,  4.43563136]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 82
reward sum = 0.1706017637189745
running average episode reward sum: 0.4439963080679245
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([ 8.06871966, 20.05973147,  1.88353862]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 1.3234001616826887}
episode index:1155
target Thresh 31.999753317296538
target distance 14.0
model initialize at round 1155
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 27.]), 'previousTarget': array([ 7., 27.]), 'currentState': array([20.81785028, 20.87397203,  3.49435639]), 'targetState': array([ 7, 27], dtype=int32), 'currentDistance': 15.114933180379214}
done in step count: 34
reward sum = 0.4818242000839317
running average episode reward sum: 0.44402903115790376
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 27.]), 'previousTarget': array([ 7., 27.]), 'currentState': array([ 7.9454832 , 26.71758924,  2.63749359]), 'targetState': array([ 7, 27], dtype=int32), 'currentDistance': 0.9867595032382859}
episode index:1156
target Thresh 31.99975577183045
target distance 8.0
model initialize at round 1156
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([6.99399461, 2.00169653, 3.11876631]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 8.549031878475601}
done in step count: 28
reward sum = 0.5881540547031764
running average episode reward sum: 0.4441535990261365
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.19259752,  4.18045308,  0.14431316]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.1504590068003937}
episode index:1157
target Thresh 31.99975820194134
target distance 15.0
model initialize at round 1157
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 24.]), 'previousTarget': array([12., 24.]), 'currentState': array([16.96635409,  9.32853936,  1.72745731]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 15.489235951421916}
done in step count: 39
reward sum = 0.4854801228433277
running average episode reward sum: 0.4441892868705382
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 24.]), 'previousTarget': array([12., 24.]), 'currentState': array([12.42934988, 23.23467489,  1.67629651]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 0.8775328174281668}
episode index:1158
target Thresh 31.999760607872222
target distance 10.0
model initialize at round 1158
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  9.]), 'previousTarget': array([19.,  9.]), 'currentState': array([8.98220732, 5.9745558 , 4.31295061]), 'targetState': array([19,  9], dtype=int32), 'currentDistance': 10.464677866709604}
done in step count: 28
reward sum = 0.5764843641494839
running average episode reward sum: 0.4443034327525735
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  9.]), 'previousTarget': array([19.,  9.]), 'currentState': array([18.01076309,  8.21952901,  0.26925054]), 'targetState': array([19,  9], dtype=int32), 'currentDistance': 1.2600494510147142}
episode index:1159
target Thresh 31.999762989863694
target distance 25.0
model initialize at round 1159
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.1791931 , 21.79047029]), 'previousTarget': array([23.15981002, 21.74881264]), 'currentState': array([20.0664185 ,  2.03418941,  0.65719649]), 'targetState': array([24, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 74
reward sum = 0.22936666520975696
running average episode reward sum: 0.4441181424357263
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 27.]), 'previousTarget': array([24., 27.]), 'currentState': array([23.22964006, 26.26555641,  1.47351337]), 'targetState': array([24, 27], dtype=int32), 'currentDistance': 1.064359817429988}
episode index:1160
target Thresh 31.999765348153954
target distance 12.0
model initialize at round 1160
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 28.]), 'previousTarget': array([18., 28.]), 'currentState': array([ 9.00546218, 15.99891527,  0.05646151]), 'targetState': array([18, 28], dtype=int32), 'currentDistance': 14.997591315129073}
done in step count: 35
reward sum = 0.48516229751952694
running average episode reward sum: 0.44415349485181915
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 28.]), 'previousTarget': array([18., 28.]), 'currentState': array([17.0693929 , 27.43958484,  0.9758472 ]), 'targetState': array([18, 28], dtype=int32), 'currentDistance': 1.086321650531039}
episode index:1161
target Thresh 31.99976768297883
target distance 10.0
model initialize at round 1161
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 24.]), 'previousTarget': array([25., 24.]), 'currentState': array([14.99397583, 16.99285613,  4.2456114 ]), 'targetState': array([25, 24], dtype=int32), 'currentDistance': 12.215587783029306}
done in step count: 30
reward sum = 0.5247063862413566
running average episode reward sum: 0.44422281747779985
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 24.]), 'previousTarget': array([25., 24.]), 'currentState': array([24.07455548, 23.04219696,  0.6720221 ]), 'targetState': array([25, 24], dtype=int32), 'currentDistance': 1.3318536845430278}
episode index:1162
target Thresh 31.999769994571814
target distance 8.0
model initialize at round 1162
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([21.5626022 ,  7.15139049,  2.69829018]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 8.485561105270692}
done in step count: 16
reward sum = 0.7137004344955331
running average episode reward sum: 0.44445452652080736
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.90647781, 10.1967187 ,  2.91617864]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.2111824299385128}
episode index:1163
target Thresh 31.999772283164063
target distance 11.0
model initialize at round 1163
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 28.]), 'previousTarget': array([ 9., 28.]), 'currentState': array([20.02466629, 22.01776255,  0.87659839]), 'targetState': array([ 9, 28], dtype=int32), 'currentDistance': 12.543142815074999}
done in step count: 31
reward sum = 0.5323333078848956
running average episode reward sum: 0.4445300237556562
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 28.]), 'previousTarget': array([ 9., 28.]), 'currentState': array([ 9.88677243, 27.33278866,  2.62483518]), 'targetState': array([ 9, 28], dtype=int32), 'currentDistance': 1.1097460605191338}
episode index:1164
target Thresh 31.99977454898444
target distance 20.0
model initialize at round 1164
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.03319094,  9.22127294]), 'previousTarget': array([14.03319094,  9.22127294]), 'currentState': array([17.        , 29.        ,  2.00037432]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.28814756621818083
running average episode reward sum: 0.44439578988652534
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.12215661,  9.93287275,  4.64352682]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.9408367531260097}
episode index:1165
target Thresh 31.999776792259524
target distance 6.0
model initialize at round 1165
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 27.]), 'previousTarget': array([ 8., 27.]), 'currentState': array([14.02012027, 21.98852199,  5.98878467]), 'targetState': array([ 8, 27], dtype=int32), 'currentDistance': 7.833055595497523}
done in step count: 20
reward sum = 0.6560490839559203
running average episode reward sum: 0.44457731072191936
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 27.]), 'previousTarget': array([ 8., 27.]), 'currentState': array([ 8.78085863, 26.04035852,  2.67871359]), 'targetState': array([ 8, 27], dtype=int32), 'currentDistance': 1.2371952071074839}
episode index:1166
target Thresh 31.99977901321365
target distance 8.0
model initialize at round 1166
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([21.02854918,  9.04838235,  1.23722738]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 9.408216091449882}
done in step count: 19
reward sum = 0.6711348044983252
running average episode reward sum: 0.44477144739182206
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([16.67376292, 16.08233148,  2.26701777]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 1.1384515722015334}
episode index:1167
target Thresh 31.999781212068914
target distance 11.0
model initialize at round 1167
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([12.97084227, 19.04830961,  2.36632907]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 14.231695481136642}
done in step count: 36
reward sum = 0.47722712273886914
running average episode reward sum: 0.4447992347850987
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.2316251 , 8.96831282, 4.38465407]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.9956304070203441}
episode index:1168
target Thresh 31.999783389045202
target distance 16.0
model initialize at round 1168
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  6.]), 'previousTarget': array([23.,  6.]), 'currentState': array([6.99573889, 8.98142276, 4.7394166 ]), 'targetState': array([23,  6], dtype=int32), 'currentDistance': 16.279596288594032}
done in step count: 39
reward sum = 0.4365550667120065
running average episode reward sum: 0.444792182459972
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  6.]), 'previousTarget': array([23.,  6.]), 'currentState': array([22.00739173,  5.92492484,  6.26663249]), 'targetState': array([23,  6], dtype=int32), 'currentDistance': 0.9954433452898018}
episode index:1169
target Thresh 31.999785544360215
target distance 12.0
model initialize at round 1169
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 13.]), 'previousTarget': array([ 5., 13.]), 'currentState': array([17.        ,  2.        ,  5.30807127]), 'targetState': array([ 5, 13], dtype=int32), 'currentDistance': 16.27882059609971}
done in step count: 40
reward sum = 0.40815902701843465
running average episode reward sum: 0.4447608720707057
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 13.]), 'previousTarget': array([ 5., 13.]), 'currentState': array([ 5.9428338 , 12.24880687,  2.53077544]), 'targetState': array([ 5, 13], dtype=int32), 'currentDistance': 1.2054985235292235}
episode index:1170
target Thresh 31.999787678229485
target distance 13.0
model initialize at round 1170
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([26.99502362, 10.98064421,  4.21799794]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 12.995038031014454}
done in step count: 32
reward sum = 0.5292806811206803
running average episode reward sum: 0.44483304953360064
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.90992661, 11.04879057,  3.45608057]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.9112337550963228}
episode index:1171
target Thresh 31.9997897908664
target distance 9.0
model initialize at round 1171
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 27.]), 'previousTarget': array([17., 27.]), 'currentState': array([14.        , 18.        ,  5.91578102]), 'targetState': array([17, 27], dtype=int32), 'currentDistance': 9.486832980505138}
done in step count: 23
reward sum = 0.6229993654352205
running average episode reward sum: 0.44498506857447234
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 27.]), 'previousTarget': array([17., 27.]), 'currentState': array([16.76785219, 26.2357111 ,  1.35182977]), 'targetState': array([17, 27], dtype=int32), 'currentDistance': 0.7987678841979627}
episode index:1172
target Thresh 31.999791882482228
target distance 12.0
model initialize at round 1172
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 2.99782987, 19.02017199,  1.42826283]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 15.013846632205933}
done in step count: 36
reward sum = 0.4507338196127385
running average episode reward sum: 0.4449899694704982
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.10728787, 10.37906623,  5.72205772]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.9698588293930906}
episode index:1173
target Thresh 31.99979395328613
target distance 16.0
model initialize at round 1173
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 20.]), 'previousTarget': array([14., 20.]), 'currentState': array([8.00573625, 3.99373116, 5.70595217]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 17.091864735754132}
done in step count: 41
reward sum = 0.3988186139848383
running average episode reward sum: 0.44495064122902833
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 20.]), 'previousTarget': array([14., 20.]), 'currentState': array([13.96632506, 19.02302841,  1.23823821]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 0.9775517858172679}
episode index:1174
target Thresh 31.999796003485187
target distance 10.0
model initialize at round 1174
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([12.00449332,  4.0019859 ,  0.62488931]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 12.209098572555025}
done in step count: 28
reward sum = 0.5403103700106588
running average episode reward sum: 0.4450317984450126
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.87450073, 10.54246402,  2.72631099]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.9869603347310781}
episode index:1175
target Thresh 31.999798033284424
target distance 10.0
model initialize at round 1175
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 7.]), 'previousTarget': array([9., 7.]), 'currentState': array([19.0133476 , 12.98530624,  5.20131227]), 'targetState': array([9, 7], dtype=int32), 'currentDistance': 11.665805628549776}
done in step count: 28
reward sum = 0.5717149936448572
running average episode reward sum: 0.44513952225045467
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 7.]), 'previousTarget': array([9., 7.]), 'currentState': array([9.93846329, 7.37797507, 3.66736386]), 'targetState': array([9, 7], dtype=int32), 'currentDistance': 1.0117205691030762}
episode index:1176
target Thresh 31.99980004288682
target distance 6.0
model initialize at round 1176
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 2.]), 'previousTarget': array([6., 2.]), 'currentState': array([12.00893618,  2.9461133 ,  4.62422609]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 6.082963450237109}
done in step count: 13
reward sum = 0.7613494159766345
running average episode reward sum: 0.4454081797642407
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 2.]), 'previousTarget': array([6., 2.]), 'currentState': array([6.99635515, 2.10006371, 3.08416653]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 1.0013672284261652}
episode index:1177
target Thresh 31.99980203249334
target distance 18.0
model initialize at round 1177
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  6.]), 'previousTarget': array([27.,  6.]), 'currentState': array([ 9.06627331, 12.89237165,  5.3816053 ]), 'targetState': array([27,  6], dtype=int32), 'currentDistance': 19.212582856057754}
done in step count: 50
reward sum = 0.38354153309011596
running average episode reward sum: 0.4453556613884562
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  6.]), 'previousTarget': array([27.,  6.]), 'currentState': array([26.13588284,  6.4572576 ,  6.09779704]), 'targetState': array([27,  6], dtype=int32), 'currentDistance': 0.9776415364480617}
episode index:1178
target Thresh 31.999804002302945
target distance 1.0
model initialize at round 1178
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([16.96983581, 15.01891519,  2.79168457]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 1.4066874759732781}
done in step count: 2
reward sum = 0.9634632896818343
running average episode reward sum: 0.4457951080621571
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([16.78507856, 14.99987963,  3.45111087]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 1.2712622144833357}
episode index:1179
target Thresh 31.999805952512613
target distance 15.0
model initialize at round 1179
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([14.96624347,  2.01921913,  2.38406128]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 18.030531479514867}
done in step count: 45
reward sum = 0.38755804329037724
running average episode reward sum: 0.44574575461743526
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([24.72779261, 16.0919901 ,  1.14106772]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 0.947933984331338}
episode index:1180
target Thresh 31.999807883317374
target distance 13.0
model initialize at round 1180
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 28.]), 'previousTarget': array([26., 28.]), 'currentState': array([13.        , 23.        ,  5.01371828]), 'targetState': array([26, 28], dtype=int32), 'currentDistance': 13.928388277184121}
done in step count: 32
reward sum = 0.4923159821651905
running average episode reward sum: 0.4457851874942751
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 28.]), 'previousTarget': array([26., 28.]), 'currentState': array([25.2126771 , 27.39881897,  0.46357364]), 'targetState': array([26, 28], dtype=int32), 'currentDistance': 0.9906038463670783}
episode index:1181
target Thresh 31.999809794910306
target distance 16.0
model initialize at round 1181
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 23.]), 'previousTarget': array([18., 23.]), 'currentState': array([2.31944967e+00, 2.09729683e+01, 4.21379110e-03]), 'targetState': array([18, 23], dtype=int32), 'currentDistance': 15.811025155626195}
done in step count: 37
reward sum = 0.4702385702855916
running average episode reward sum: 0.44580587563538443
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 23.]), 'previousTarget': array([18., 23.]), 'currentState': array([17.02909476, 22.97538384,  0.28440036]), 'targetState': array([18, 23], dtype=int32), 'currentDistance': 0.9712172421004374}
episode index:1182
target Thresh 31.99981168748257
target distance 17.0
model initialize at round 1182
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([12.        , 26.        ,  1.84431286]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 17.029386365926403}
done in step count: 47
reward sum = 0.38256556396124874
running average episode reward sum: 0.4457524180600048
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.86840807,  9.95592965,  4.81932289]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.9649445201583733}
episode index:1183
target Thresh 31.999813561223426
target distance 8.0
model initialize at round 1183
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 16.]), 'previousTarget': array([10., 16.]), 'currentState': array([16.98830335, 23.7035087 ,  4.61147165]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 10.400982161089548}
done in step count: 23
reward sum = 0.6443410145261799
running average episode reward sum: 0.44592014491512827
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 16.]), 'previousTarget': array([10., 16.]), 'currentState': array([10.93547483, 16.78324876,  3.97225829]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 1.2200785917743944}
episode index:1184
target Thresh 31.99981541632025
target distance 10.0
model initialize at round 1184
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 21.]), 'previousTarget': array([ 7., 21.]), 'currentState': array([10.        , 11.        ,  0.34523886]), 'targetState': array([ 7, 21], dtype=int32), 'currentDistance': 10.44030650891055}
done in step count: 25
reward sum = 0.600639205774273
running average episode reward sum: 0.4460507095234483
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 21.]), 'previousTarget': array([ 7., 21.]), 'currentState': array([ 7.12615681, 20.00812572,  2.08158488]), 'targetState': array([ 7, 21], dtype=int32), 'currentDistance': 0.9998650523207624}
episode index:1185
target Thresh 31.99981725295855
target distance 24.0
model initialize at round 1185
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.21074628, 23.99869196]), 'previousTarget': array([23., 24.]), 'currentState': array([ 3.21074748, 23.99178804,  0.06731724]), 'targetState': array([27, 24], dtype=int32), 'currentDistance': 20.0}
done in step count: 86
reward sum = 0.24174472302038064
running average episode reward sum: 0.44587844477934785
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 24.]), 'previousTarget': array([27., 24.]), 'currentState': array([26.13664794, 23.64903564,  0.2873394 ]), 'targetState': array([27, 24], dtype=int32), 'currentDistance': 0.9319617811315016}
episode index:1186
target Thresh 31.999819071321994
target distance 10.0
model initialize at round 1186
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([13.32023525, 14.89222386,  5.89149052]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 11.882785677007996}
done in step count: 24
reward sum = 0.5978409082703958
running average episode reward sum: 0.4460064670737801
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([22.01620601,  8.3801672 ,  5.88281336]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 1.0546931840587714}
episode index:1187
target Thresh 31.99982087159242
target distance 22.0
model initialize at round 1187
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.27178448, 23.19378422]), 'previousTarget': array([16.27605889, 23.20732955]), 'currentState': array([8.00319104, 4.98306313, 5.15111399]), 'targetState': array([18, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 72
reward sum = 0.21438475813395313
running average episode reward sum: 0.4458114993053123
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 27.]), 'previousTarget': array([18., 27.]), 'currentState': array([18.12219084, 26.01780466,  1.19959251]), 'targetState': array([18, 27], dtype=int32), 'currentDistance': 0.9897667881982041}
episode index:1188
target Thresh 31.999822653949856
target distance 14.0
model initialize at round 1188
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 25.]), 'previousTarget': array([22., 25.]), 'currentState': array([ 9.99644682, 10.99970266,  2.97258186]), 'targetState': array([22, 25], dtype=int32), 'currentDistance': 18.44162722555449}
done in step count: 59
reward sum = 0.3345237093999078
running average episode reward sum: 0.4457179015005138
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 25.]), 'previousTarget': array([22., 25.]), 'currentState': array([21.81106437, 24.07142244,  1.17629508]), 'targetState': array([22, 25], dtype=int32), 'currentDistance': 0.9476037997665893}
episode index:1189
target Thresh 31.999824418572537
target distance 16.0
model initialize at round 1189
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5.47636753, 28.61270935]), 'previousTarget': array([ 5.47772  , 28.6118525]), 'currentState': array([20.99478798, 15.99610811,  3.53048706]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.33748596915720047
running average episode reward sum: 0.445626950296864
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 29.]), 'previousTarget': array([ 5., 29.]), 'currentState': array([ 5.84581974, 28.22999207,  2.68808232]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 1.143819590612526}
episode index:1190
target Thresh 31.999826165636932
target distance 11.0
model initialize at round 1190
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 13.]), 'previousTarget': array([ 5., 13.]), 'currentState': array([16.05133049, 22.01920556,  0.14226505]), 'targetState': array([ 5, 13], dtype=int32), 'currentDistance': 14.264570609932132}
done in step count: 42
reward sum = 0.46296926977347064
running average episode reward sum: 0.4456415114383221
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 13.]), 'previousTarget': array([ 5., 13.]), 'currentState': array([ 5.75902227, 13.93978349,  3.80499556]), 'targetState': array([ 5, 13], dtype=int32), 'currentDistance': 1.208018133977527}
episode index:1191
target Thresh 31.999827895317743
target distance 22.0
model initialize at round 1191
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9.16058577, 9.01438595]), 'previousTarget': array([9.26249016, 9.12677025]), 'currentState': array([19.82501652, 25.93389885,  3.64742848]), 'targetState': array([6, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 85
reward sum = 0.20281193436219194
running average episode reward sum: 0.445437795350171
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 4.]), 'previousTarget': array([6., 4.]), 'currentState': array([6.3034406 , 4.93208443, 4.15210484]), 'targetState': array([6, 4], dtype=int32), 'currentDistance': 0.980233434703427}
episode index:1192
target Thresh 31.999829607787944
target distance 4.0
model initialize at round 1192
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'previousTarget': array([7., 3.]), 'currentState': array([7.        , 7.        , 3.55077794]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 3.9999999999999996}
done in step count: 10
reward sum = 0.8376005570953236
running average episode reward sum: 0.44576651518398924
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'previousTarget': array([7., 3.]), 'currentState': array([6.7009202 , 3.9666537 , 4.85172005]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 1.0118636739888498}
episode index:1193
target Thresh 31.999831303218784
target distance 15.0
model initialize at round 1193
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 23.]), 'previousTarget': array([ 4., 23.]), 'currentState': array([19.        , 16.        ,  2.10104996]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 16.55294535724685}
done in step count: 43
reward sum = 0.4497560280140728
running average episode reward sum: 0.445769856484517
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 23.]), 'previousTarget': array([ 4., 23.]), 'currentState': array([ 4.97598216, 22.85006711,  2.8006884 ]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 0.9874315391634239}
episode index:1194
target Thresh 31.9998329817798
target distance 8.0
model initialize at round 1194
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 28.]), 'previousTarget': array([18., 28.]), 'currentState': array([11.        , 20.        ,  5.63466421]), 'targetState': array([18, 28], dtype=int32), 'currentDistance': 10.63014581273465}
done in step count: 23
reward sum = 0.608136115314243
running average episode reward sum: 0.445905727830818
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 28.]), 'previousTarget': array([18., 28.]), 'currentState': array([17.17627267, 27.13715892,  0.94312191]), 'targetState': array([18, 28], dtype=int32), 'currentDistance': 1.1929046289698373}
episode index:1195
target Thresh 31.99983464363886
target distance 20.0
model initialize at round 1195
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.71433445,  6.81837819]), 'previousTarget': array([20.60700849,  7.12283287]), 'currentState': array([14.12309047, 25.70105551,  5.0819348 ]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.3537433473685475
running average episode reward sum: 0.44582866898427764
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([20.2441632 ,  6.98201578,  5.15599658]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 1.2392111415871896}
episode index:1196
target Thresh 31.999836288962143
target distance 10.0
model initialize at round 1196
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([13.        , 13.        ,  3.67799276]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 10.049875621120892}
done in step count: 27
reward sum = 0.5927687125554795
running average episode reward sum: 0.4459514259129085
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([13.57447929, 22.21133118,  1.40003714]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 0.8961397126231192}
episode index:1197
target Thresh 31.99983791791419
target distance 10.0
model initialize at round 1197
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 22.]), 'previousTarget': array([25., 22.]), 'currentState': array([15.02006104, 22.97174928,  5.58236599]), 'targetState': array([25, 22], dtype=int32), 'currentDistance': 10.027137097793474}
done in step count: 22
reward sum = 0.6400275496680619
running average episode reward sum: 0.44611342601620996
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 22.]), 'previousTarget': array([25., 22.]), 'currentState': array([24.10524875, 21.86830872,  6.26561995]), 'targetState': array([25, 22], dtype=int32), 'currentDistance': 0.9043906235720807}
episode index:1198
target Thresh 31.999839530657887
target distance 12.0
model initialize at round 1198
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([3.99488348, 4.9976518 , 3.31936026]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 15.62593332754723}
done in step count: 43
reward sum = 0.4166548570099382
running average episode reward sum: 0.4460888567343031
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([15.34095128, 14.09631006,  0.6964234 ]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 1.1184814397815142}
episode index:1199
target Thresh 31.999841127354518
target distance 2.0
model initialize at round 1199
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 25.]), 'previousTarget': array([11., 25.]), 'currentState': array([13.        , 23.        ,  0.71971816]), 'targetState': array([11, 25], dtype=int32), 'currentDistance': 2.8284271247461903}
done in step count: 8
reward sum = 0.8675281691948643
running average episode reward sum: 0.44644005616135357
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 25.]), 'previousTarget': array([11., 25.]), 'currentState': array([11.87619474, 24.30025414,  2.42208743]), 'targetState': array([11, 25], dtype=int32), 'currentDistance': 1.1213213170652732}
episode index:1200
target Thresh 31.999842708163754
target distance 8.0
model initialize at round 1200
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([16.        , 15.        ,  2.23333907]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 8.246211251235321}
done in step count: 26
reward sum = 0.6270867075675557
running average episode reward sum: 0.44659046969291577
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.92339192,  7.8790008 ,  4.62451295]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.8823328171410221}
episode index:1201
target Thresh 31.999844273243674
target distance 9.0
model initialize at round 1201
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 17.]), 'previousTarget': array([19., 17.]), 'currentState': array([21.        , 26.        ,  2.16088432]), 'targetState': array([19, 17], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 24
reward sum = 0.6109233737121456
running average episode reward sum: 0.4467271859192213
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 17.]), 'previousTarget': array([19., 17.]), 'currentState': array([19.09407438, 17.8725958 ,  4.43340639]), 'targetState': array([19, 17], dtype=int32), 'currentDistance': 0.877652216422857}
episode index:1202
target Thresh 31.99984582275079
target distance 19.0
model initialize at round 1202
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 20.]), 'previousTarget': array([ 8., 20.]), 'currentState': array([27.        , 22.        ,  1.59934133]), 'targetState': array([ 8, 20], dtype=int32), 'currentDistance': 19.104973174542803}
done in step count: 51
reward sum = 0.35760246884835745
running average episode reward sum: 0.4466531005351224
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 20.]), 'previousTarget': array([ 8., 20.]), 'currentState': array([ 8.84755527, 20.43267206,  3.44385582]), 'targetState': array([ 8, 20], dtype=int32), 'currentDistance': 0.9516065595780248}
episode index:1203
target Thresh 31.999847356840053
target distance 10.0
model initialize at round 1203
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  6.]), 'previousTarget': array([17.,  6.]), 'currentState': array([26.80483057,  2.83754376,  3.61894172]), 'targetState': array([17,  6], dtype=int32), 'currentDistance': 10.302224609541263}
done in step count: 26
reward sum = 0.6248788178156086
running average episode reward sum: 0.44680112853950815
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  6.]), 'previousTarget': array([17.,  6.]), 'currentState': array([17.82152909,  5.63844327,  2.87322459]), 'targetState': array([17,  6], dtype=int32), 'currentDistance': 0.8975707846958708}
episode index:1204
target Thresh 31.99984887566487
target distance 19.0
model initialize at round 1204
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.40805363, 21.75186242]), 'previousTarget': array([24.4327075, 21.76114  ]), 'currentState': array([ 5.9630772 , 14.01992575,  2.40756074]), 'targetState': array([25, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.3089230761254673
running average episode reward sum: 0.4466867069192475
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 22.]), 'previousTarget': array([25., 22.]), 'currentState': array([24.1660606 , 21.5265063 ,  0.56192623]), 'targetState': array([25, 22], dtype=int32), 'currentDistance': 0.9589844630443711}
episode index:1205
target Thresh 31.99985037937713
target distance 7.0
model initialize at round 1205
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  2.]), 'previousTarget': array([10.,  2.]), 'currentState': array([8.        , 9.        , 1.43918338]), 'targetState': array([10,  2], dtype=int32), 'currentDistance': 7.280109889280518}
done in step count: 18
reward sum = 0.6710426815565504
running average episode reward sum: 0.4468727400657129
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  2.]), 'previousTarget': array([10.,  2.]), 'currentState': array([9.62975326, 2.76348935, 4.95116523]), 'targetState': array([10,  2], dtype=int32), 'currentDistance': 0.8485273360788224}
episode index:1206
target Thresh 31.9998518681272
target distance 11.0
model initialize at round 1206
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 17.]), 'previousTarget': array([22., 17.]), 'currentState': array([17.32190381, 27.62576536,  5.40508326]), 'targetState': array([22, 17], dtype=int32), 'currentDistance': 11.609973020114424}
done in step count: 28
reward sum = 0.5881734473790404
running average episode reward sum: 0.4469898077602559
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 17.]), 'previousTarget': array([22., 17.]), 'currentState': array([21.64667386, 17.8848605 ,  5.17652382]), 'targetState': array([22, 17], dtype=int32), 'currentDistance': 0.9527945582540485}
episode index:1207
target Thresh 31.999853342063965
target distance 9.0
model initialize at round 1207
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  5.]), 'previousTarget': array([13.,  5.]), 'currentState': array([11.96601073, 13.66371048,  4.74207604]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 8.725194153153288}
done in step count: 18
reward sum = 0.6961653959272689
running average episode reward sum: 0.4471960789425134
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  5.]), 'previousTarget': array([13.,  5.]), 'currentState': array([12.82207912,  5.80704166,  4.8243772 ]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 0.8264212536743641}
episode index:1208
target Thresh 31.99985480133481
target distance 10.0
model initialize at round 1208
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 18.]), 'previousTarget': array([21., 18.]), 'currentState': array([11.01714835, 15.97204956,  5.47502336]), 'targetState': array([21, 18], dtype=int32), 'currentDistance': 10.18675169294799}
done in step count: 23
reward sum = 0.6336204595348053
running average episode reward sum: 0.4473502761142191
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 18.]), 'previousTarget': array([21., 18.]), 'currentState': array([20.14772212, 17.52718297,  0.34332655]), 'targetState': array([21, 18], dtype=int32), 'currentDistance': 0.9746453363133109}
episode index:1209
target Thresh 31.999856246085667
target distance 16.0
model initialize at round 1209
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 29.]), 'previousTarget': array([16., 29.]), 'currentState': array([ 9.03825749, 12.99687449,  0.17098444]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 17.45181608679429}
done in step count: 45
reward sum = 0.4167360434012352
running average episode reward sum: 0.44732497509544805
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 29.]), 'previousTarget': array([16., 29.]), 'currentState': array([15.74685659, 28.04496327,  1.31733983]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 0.9880165726893081}
episode index:1210
target Thresh 31.999857676461012
target distance 10.0
model initialize at round 1210
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 13.]), 'previousTarget': array([12., 13.]), 'currentState': array([ 2.14930961, 14.17268002,  0.66931197]), 'targetState': array([12, 13], dtype=int32), 'currentDistance': 9.920245944840087}
done in step count: 29
reward sum = 0.6158731350984922
running average episode reward sum: 0.44746415606985185
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 13.]), 'previousTarget': array([12., 13.]), 'currentState': array([11.02835265, 12.85703614,  5.99450048]), 'targetState': array([12, 13], dtype=int32), 'currentDistance': 0.9821085671865496}
episode index:1211
target Thresh 31.999859092603888
target distance 19.0
model initialize at round 1211
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 27.]), 'previousTarget': array([20., 27.]), 'currentState': array([17.93674334,  8.12248839,  1.79500127]), 'targetState': array([20, 27], dtype=int32), 'currentDistance': 18.989930821361092}
done in step count: 49
reward sum = 0.3744151813832084
running average episode reward sum: 0.4474038846385923
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 27.]), 'previousTarget': array([20., 27.]), 'currentState': array([19.88816272, 26.10738403,  1.5585941 ]), 'targetState': array([20, 27], dtype=int32), 'currentDistance': 0.8995948287694372}
episode index:1212
target Thresh 31.999860494655906
target distance 3.0
model initialize at round 1212
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 3.]), 'previousTarget': array([9., 3.]), 'currentState': array([6.23042924, 3.21554177, 0.61494565]), 'targetState': array([9, 3], dtype=int32), 'currentDistance': 2.7779453637326506}
done in step count: 6
reward sum = 0.9028825519230524
running average episode reward sum: 0.4477793823032951
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 3.]), 'previousTarget': array([9., 3.]), 'currentState': array([8.14978085, 3.44194954, 6.04734951]), 'targetState': array([9, 3], dtype=int32), 'currentDistance': 0.9582233587258687}
episode index:1213
target Thresh 31.999861882757273
target distance 2.0
model initialize at round 1213
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([23.       , 17.       ,  4.8383604]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 2.0}
done in step count: 6
reward sum = 0.9002601154278381
running average episode reward sum: 0.4481521011938425
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([24.00649331, 16.84454199,  0.2266944 ]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 1.0055957155856325}
episode index:1214
target Thresh 31.999863257046798
target distance 9.0
model initialize at round 1214
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 13.]), 'previousTarget': array([12., 13.]), 'currentState': array([4.        , 4.        , 2.45021924]), 'targetState': array([12, 13], dtype=int32), 'currentDistance': 12.041594578792296}
done in step count: 37
reward sum = 0.5251970163720422
running average episode reward sum: 0.4482155126466641
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 13.]), 'previousTarget': array([12., 13.]), 'currentState': array([11.30856507, 12.22498835,  0.48099191]), 'targetState': array([12, 13], dtype=int32), 'currentDistance': 1.038617022958885}
episode index:1215
target Thresh 31.999864617661917
target distance 21.0
model initialize at round 1215
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.53421008, 26.40608773]), 'previousTarget': array([24.49442256, 26.23047895]), 'currentState': array([18.92423389,  7.20900026,  1.71259916]), 'targetState': array([25, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.30858012986023764
running average episode reward sum: 0.44810068091739896
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 28.]), 'previousTarget': array([25., 28.]), 'currentState': array([24.24401618, 27.00141192,  1.09297925]), 'targetState': array([25, 28], dtype=int32), 'currentDistance': 1.2524734251591432}
episode index:1216
target Thresh 31.99986596473869
target distance 11.0
model initialize at round 1216
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 17.]), 'previousTarget': array([15., 17.]), 'currentState': array([ 4.        , 28.        ,  0.54422635]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 15.556349186104047}
done in step count: 42
reward sum = 0.4537592025840512
running average episode reward sum: 0.44810533048327134
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 17.]), 'previousTarget': array([15., 17.]), 'currentState': array([14.22610861, 17.66840639,  5.44378362]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 1.022582503475066}
episode index:1217
target Thresh 31.99986729841182
target distance 4.0
model initialize at round 1217
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.81880506, 6.13249326, 2.29840711]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 3.8717489565935543}
done in step count: 8
reward sum = 0.8614310981905107
running average episode reward sum: 0.44844467840421326
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.59256672, 9.0730659 , 1.44297003]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.012525897446003}
episode index:1218
target Thresh 31.999868618814688
target distance 18.0
model initialize at round 1218
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.20085153,  6.63706463]), 'previousTarget': array([10.19631201,  6.63557441]), 'currentState': array([23.01586812, 21.99204265,  5.58635676]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.2554458642257606
running average episode reward sum: 0.4482863528798667
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 4.]), 'previousTarget': array([8., 4.]), 'currentState': array([8.64835459, 4.79627555, 3.84000499]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 1.0268487852013561}
episode index:1219
target Thresh 31.999869926079324
target distance 4.0
model initialize at round 1219
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 28.]), 'previousTarget': array([14., 28.]), 'currentState': array([14.82727562, 24.37023883,  1.8719129 ]), 'targetState': array([14, 28], dtype=int32), 'currentDistance': 3.7228417998096908}
done in step count: 7
reward sum = 0.881000221705205
running average episode reward sum: 0.44864103637890385
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 28.]), 'previousTarget': array([14., 28.]), 'currentState': array([14.36445657, 27.0096774 ,  1.78889066]), 'targetState': array([14, 28], dtype=int32), 'currentDistance': 1.0552570488550563}
episode index:1220
target Thresh 31.99987122033646
target distance 8.0
model initialize at round 1220
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 24.]), 'previousTarget': array([19., 24.]), 'currentState': array([26.81193936, 16.2385954 ,  2.32954194]), 'targetState': array([19, 24], dtype=int32), 'currentDistance': 11.01207509168552}
done in step count: 25
reward sum = 0.6254735767209298
running average episode reward sum: 0.44878586237426993
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 24.]), 'previousTarget': array([19., 24.]), 'currentState': array([19.84677326, 23.48628017,  2.38618378]), 'targetState': array([19, 24], dtype=int32), 'currentDistance': 0.9904206270783356}
episode index:1221
target Thresh 31.99987250171552
target distance 11.0
model initialize at round 1221
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 26.]), 'previousTarget': array([10., 26.]), 'currentState': array([21.        , 27.        ,  5.41376305]), 'targetState': array([10, 26], dtype=int32), 'currentDistance': 11.045361017187261}
done in step count: 31
reward sum = 0.5569458074865338
running average episode reward sum: 0.4488743729676515
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 26.]), 'previousTarget': array([10., 26.]), 'currentState': array([10.80010787, 25.69374345,  3.22820063]), 'targetState': array([10, 26], dtype=int32), 'currentDistance': 0.8567179684307528}
episode index:1222
target Thresh 31.999873770344646
target distance 11.0
model initialize at round 1222
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 23.]), 'previousTarget': array([10., 23.]), 'currentState': array([ 9.98931718, 11.98922339,  3.71785402]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 11.010781794425968}
done in step count: 30
reward sum = 0.5656648210771975
running average episode reward sum: 0.44896986801925376
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 23.]), 'previousTarget': array([10., 23.]), 'currentState': array([ 9.85739617, 22.09572592,  1.41119279]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 0.9154493251044653}
episode index:1223
target Thresh 31.999875026350704
target distance 6.0
model initialize at round 1223
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 14.]), 'previousTarget': array([ 6., 14.]), 'currentState': array([11.51277746, 14.88789959,  3.34357303]), 'targetState': array([ 6, 14], dtype=int32), 'currentDistance': 5.583823151343507}
done in step count: 10
reward sum = 0.8135862155467789
running average episode reward sum: 0.4492677571920704
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 14.]), 'previousTarget': array([ 6., 14.]), 'currentState': array([ 6.98645303, 14.16044447,  3.31333944]), 'targetState': array([ 6, 14], dtype=int32), 'currentDistance': 0.9994158368469556}
episode index:1224
target Thresh 31.999876269859293
target distance 11.0
model initialize at round 1224
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  4.]), 'previousTarget': array([10.,  4.]), 'currentState': array([20.90052525,  7.79034181,  4.12247425]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 11.540716689192939}
done in step count: 26
reward sum = 0.6058040001907714
running average episode reward sum: 0.4493955418802326
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  4.]), 'previousTarget': array([10.,  4.]), 'currentState': array([10.97884187,  4.81451624,  3.43220728]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 1.2734080699042571}
episode index:1225
target Thresh 31.99987750099476
target distance 21.0
model initialize at round 1225
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.13464492,  9.31708571]), 'previousTarget': array([15.09400392,  9.35899411]), 'currentState': array([ 4.07460718, 25.98068571,  5.80656353]), 'targetState': array([18,  5], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 76
reward sum = 0.22584599907903471
running average episode reward sum: 0.44921320130698533
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  5.]), 'previousTarget': array([18.,  5.]), 'currentState': array([17.40137911,  5.91382435,  5.2868344 ]), 'targetState': array([18,  5], dtype=int32), 'currentDistance': 1.0924385104841514}
episode index:1226
target Thresh 31.99987871988023
target distance 6.0
model initialize at round 1226
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 28.]), 'previousTarget': array([ 4., 28.]), 'currentState': array([ 9.99252722, 28.97053503,  4.21151018]), 'targetState': array([ 4, 28], dtype=int32), 'currentDistance': 6.0706112316993135}
done in step count: 14
reward sum = 0.7667111442361207
running average episode reward sum: 0.4494719608366749
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 28.]), 'previousTarget': array([ 4., 28.]), 'currentState': array([ 4.7876446 , 28.07099393,  3.07942499]), 'targetState': array([ 4, 28], dtype=int32), 'currentDistance': 0.7908376265811992}
episode index:1227
target Thresh 31.999879926637583
target distance 19.0
model initialize at round 1227
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 24.]), 'previousTarget': array([12., 24.]), 'currentState': array([8.94827043, 5.02655996, 2.41475201]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 19.21729638402267}
done in step count: 49
reward sum = 0.37004850219678115
running average episode reward sum: 0.44940728375309197
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 24.]), 'previousTarget': array([12., 24.]), 'currentState': array([11.73642239, 23.06009194,  1.66168891]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 0.9761661319701374}
episode index:1228
target Thresh 31.999881121387503
target distance 6.0
model initialize at round 1228
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([ 6.98582016, 12.01293928,  2.6544075 ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 7.811135733018701}
done in step count: 18
reward sum = 0.6956304629924812
running average episode reward sum: 0.4496076280811956
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.55565436, 6.83664241, 3.90216443]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.00435167252978}
episode index:1229
target Thresh 31.99988230424946
target distance 20.0
model initialize at round 1229
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 29.]), 'previousTarget': array([21.9007438 , 28.99007438]), 'currentState': array([ 2.21564367, 26.94695676,  6.04666074]), 'targetState': array([22, 29], dtype=int32), 'currentDistance': 19.890594308903168}
done in step count: 57
reward sum = 0.3623993896022994
running average episode reward sum: 0.44953672707430214
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 29.]), 'previousTarget': array([22., 29.]), 'currentState': array([21.02836372, 28.859444  ,  0.29517216]), 'targetState': array([22, 29], dtype=int32), 'currentDistance': 0.981749989924191}
episode index:1230
target Thresh 31.999883475341743
target distance 15.0
model initialize at round 1230
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3.35363499, 10.37889464]), 'previousTarget': array([ 3.35363499, 10.37889464]), 'currentState': array([17.        , 25.        ,  6.10921355]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.29352335573857213
running average episode reward sum: 0.4494099899732983
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.89411036, 10.2725873 ,  3.90045089]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.9347390927196298}
episode index:1231
target Thresh 31.999884634781466
target distance 17.0
model initialize at round 1231
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.86502556, 10.20859686]), 'previousTarget': array([12.86502556, 10.20859686]), 'currentState': array([ 2.       , 27.       ,  3.7595787]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.3246496960341368
running average episode reward sum: 0.4493087235009451
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([12.28048739, 10.77311739,  5.18455752]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 1.0561291994005584}
episode index:1232
target Thresh 31.999885782684572
target distance 5.0
model initialize at round 1232
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 22.]), 'previousTarget': array([19., 22.]), 'currentState': array([18.       , 27.       ,  0.4277643]), 'targetState': array([19, 22], dtype=int32), 'currentDistance': 5.0990195135927845}
done in step count: 14
reward sum = 0.7716249708735199
running average episode reward sum: 0.44957013164966575
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 22.]), 'previousTarget': array([19., 22.]), 'currentState': array([18.80154203, 22.90613111,  4.86489963]), 'targetState': array([19, 22], dtype=int32), 'currentDistance': 0.9276093737913933}
episode index:1233
target Thresh 31.99988691916585
target distance 9.0
model initialize at round 1233
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([24.        , 11.        ,  5.85239229]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 9.219544457292889}
done in step count: 24
reward sum = 0.6312335584396155
running average episode reward sum: 0.44971734674430913
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([25.89648885, 19.11950885,  1.62021985]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 0.8865546879143378}
episode index:1234
target Thresh 31.99988804433895
target distance 24.0
model initialize at round 1234
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.83014759, 21.98646924]), 'previousTarget': array([13.83261089, 21.98266146]), 'currentState': array([12.98450556,  2.004355  ,  2.63786295]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 76
reward sum = 0.24019417946244115
running average episode reward sum: 0.4495476923578461
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 26.]), 'previousTarget': array([14., 26.]), 'currentState': array([14.19759508, 25.01498013,  1.7075607 ]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 1.0046432008880446}
episode index:1235
target Thresh 31.99988915831639
target distance 23.0
model initialize at round 1235
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.14468739, 12.83083315]), 'previousTarget': array([21.13347761, 12.82323232]), 'currentState': array([2.00542642, 7.0266373 , 1.269868  ]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 64
reward sum = 0.25258167521036456
running average episode reward sum: 0.4493883347387947
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 14.]), 'previousTarget': array([25., 14.]), 'currentState': array([24.18455309, 13.60078214,  0.23114288]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 0.9079254190158613}
episode index:1236
target Thresh 31.99989026120957
target distance 15.0
model initialize at round 1236
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([12.        , 22.        ,  5.55076456]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 17.492855684535904}
done in step count: 45
reward sum = 0.4024352351088643
running average episode reward sum: 0.4493503775038473
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.30048578, 7.92945274, 4.17077471]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.976818353983079}
episode index:1237
target Thresh 31.999891353128778
target distance 13.0
model initialize at round 1237
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([27.       , 18.       ,  5.4136281]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 18.384776310850235}
done in step count: 44
reward sum = 0.3733365552697847
running average episode reward sum: 0.449288977001235
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.31864251,  5.91953122,  3.98524734]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.9731755839379609}
episode index:1238
target Thresh 31.99989243418321
target distance 17.0
model initialize at round 1238
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 29.]), 'previousTarget': array([23., 29.]), 'currentState': array([ 6.06124831, 28.96125945,  5.90073124]), 'targetState': array([23, 29], dtype=int32), 'currentDistance': 16.93879599256987}
done in step count: 40
reward sum = 0.44866637085576855
running average episode reward sum: 0.44928847449425724
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 29.]), 'previousTarget': array([23., 29.]), 'currentState': array([22.03968714, 28.95284151,  0.0870927 ]), 'targetState': array([23, 29], dtype=int32), 'currentDistance': 0.9614700774932491}
episode index:1239
target Thresh 31.99989350448097
target distance 17.0
model initialize at round 1239
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  2.]), 'previousTarget': array([22.,  2.]), 'currentState': array([5.        , 4.        , 3.70896864]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 17.117242768623687}
done in step count: 48
reward sum = 0.3749413106809572
running average episode reward sum: 0.44922851710408523
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  2.]), 'previousTarget': array([22.,  2.]), 'currentState': array([21.08681057,  2.10855509,  6.09188243]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 0.9196190184162649}
episode index:1240
target Thresh 31.999894564129086
target distance 21.0
model initialize at round 1240
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.18513205,  6.98417253]), 'previousTarget': array([17.18513205,  6.98417253]), 'currentState': array([ 2.        , 20.        ,  3.89189687]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 81
reward sum = 0.17124956968792326
running average episode reward sum: 0.44900452117546624
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  2.]), 'previousTarget': array([23.,  2.]), 'currentState': array([22.27289746,  2.8806667 ,  5.55264349]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 1.1420385079063227}
episode index:1241
target Thresh 31.99989561323353
target distance 20.0
model initialize at round 1241
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.4260262 , 20.56699653]), 'previousTarget': array([16.42781353, 20.56953382]), 'currentState': array([8.98958633, 2.00091558, 2.82268286]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.2984354251654261
running average episode reward sum: 0.4488832900192584
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([16.54710684, 21.13158719,  1.0696227 ]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 0.9794146289415326}
episode index:1242
target Thresh 31.999896651899213
target distance 21.0
model initialize at round 1242
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6.35322867, 28.74224216]), 'previousTarget': array([ 6.35322867, 28.74224216]), 'currentState': array([26.        , 25.        ,  4.04138142]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.28801778322084026
running average episode reward sum: 0.4487538728778277
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 29.]), 'previousTarget': array([ 5., 29.]), 'currentState': array([ 5.75574974, 28.73467932,  2.79814572]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 0.8009698711574533}
episode index:1243
target Thresh 31.999897680229996
target distance 26.0
model initialize at round 1243
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5.2967495 , 8.21346704]), 'previousTarget': array([5.29248216, 8.13182128]), 'currentState': array([ 3.0474772 , 28.08658391,  0.8167305 ]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 86
reward sum = 0.1602886688227917
running average episode reward sum: 0.44852198766556484
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 2.]), 'previousTarget': array([6., 2.]), 'currentState': array([5.75750742, 2.9758246 , 5.08739297]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 1.005503013690313}
episode index:1244
target Thresh 31.999898698328717
target distance 18.0
model initialize at round 1244
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 28.]), 'previousTarget': array([24., 28.]), 'currentState': array([ 5.99994533, 26.9920652 ,  4.95799971]), 'targetState': array([24, 28], dtype=int32), 'currentDistance': 18.02825284221001}
done in step count: 51
reward sum = 0.3826172929746526
running average episode reward sum: 0.44846905216782107
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 28.]), 'previousTarget': array([24., 28.]), 'currentState': array([23.00514263, 28.27657828,  0.21344536]), 'targetState': array([24, 28], dtype=int32), 'currentDistance': 1.0325874018824628}
episode index:1245
target Thresh 31.99989970629719
target distance 20.0
model initialize at round 1245
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.9223227 ,  9.38838649]), 'previousTarget': array([14.9223227 ,  9.38838649]), 'currentState': array([11.        , 29.        ,  0.09650943]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.31633421098083925
running average episode reward sum: 0.4483630049437545
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.09453942,  9.92177749,  5.01174981]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.9266128831284974}
episode index:1246
target Thresh 31.999900704236207
target distance 3.0
model initialize at round 1246
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 26.]), 'previousTarget': array([19., 26.]), 'currentState': array([15.95094567, 23.13395944,  1.74555159]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 4.184605218720506}
done in step count: 11
reward sum = 0.8322156624510151
running average episode reward sum: 0.4486708258399111
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 26.]), 'previousTarget': array([19., 26.]), 'currentState': array([18.23156533, 25.85492884,  0.42303839]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 0.782008617278764}
episode index:1247
target Thresh 31.999901692245565
target distance 23.0
model initialize at round 1247
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7.9626537 , 3.34036945]), 'previousTarget': array([8.11006407, 3.4295875 ]), 'currentState': array([26.90818712,  9.7487049 ,  4.1806269 ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 83
reward sum = 0.22764690711095303
running average episode reward sum: 0.4484937233409295
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.86014764, 2.37251344, 3.31887746]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.9373474409764287}
episode index:1248
target Thresh 31.999902670424063
target distance 10.0
model initialize at round 1248
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 21.]), 'previousTarget': array([25., 21.]), 'currentState': array([15.        , 13.        ,  2.30383745]), 'targetState': array([25, 21], dtype=int32), 'currentDistance': 12.806248474865697}
done in step count: 34
reward sum = 0.5260979825132288
running average episode reward sum: 0.44855585645475843
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 21.]), 'previousTarget': array([25., 21.]), 'currentState': array([24.07882776, 20.47278775,  0.57733486]), 'targetState': array([25, 21], dtype=int32), 'currentDistance': 1.0613722545447108}
episode index:1249
target Thresh 31.999903638869526
target distance 8.0
model initialize at round 1249
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'previousTarget': array([11., 21.]), 'currentState': array([11.23318508, 28.76623306,  5.29663783]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 7.7697330242166505}
done in step count: 18
reward sum = 0.7158327907898665
running average episode reward sum: 0.44876967800222645
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'previousTarget': array([11., 21.]), 'currentState': array([11.07818762, 21.79818191,  4.40792292]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 0.8020022822351682}
episode index:1250
target Thresh 31.999904597678796
target distance 16.0
model initialize at round 1250
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([ 9.        , 12.        ,  2.38744178]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 16.76305461424021}
done in step count: 47
reward sum = 0.39799746683266934
running average episode reward sum: 0.44872909270153133
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([24.12824851, 16.68187307,  0.12199128]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 0.9279845911854729}
episode index:1251
target Thresh 31.99990554694775
target distance 14.0
model initialize at round 1251
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 26.]), 'previousTarget': array([13., 26.]), 'currentState': array([14.03819426, 12.0091028 ,  0.46571894]), 'targetState': array([13, 26], dtype=int32), 'currentDistance': 14.029363914211961}
done in step count: 41
reward sum = 0.4841261629225526
running average episode reward sum: 0.4487573651218356
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 26.]), 'previousTarget': array([13., 26.]), 'currentState': array([12.77218851, 25.16745289,  1.47430006]), 'targetState': array([13, 26], dtype=int32), 'currentDistance': 0.8631528087027669}
episode index:1252
target Thresh 31.999906486771323
target distance 3.0
model initialize at round 1252
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([25.        , 14.        ,  3.87753171]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 3.6055512754639896}
done in step count: 10
reward sum = 0.848632324871147
running average episode reward sum: 0.44907649916792447
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([22.67411326, 15.18730375,  2.30672612]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 1.0558900851431783}
episode index:1253
target Thresh 31.999907417243495
target distance 14.0
model initialize at round 1253
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  2.]), 'previousTarget': array([12.,  2.]), 'currentState': array([16.        , 16.        ,  2.88114643]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 14.560219778561036}
done in step count: 42
reward sum = 0.4628193800384224
running average episode reward sum: 0.4490874584030684
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  2.]), 'previousTarget': array([12.,  2.]), 'currentState': array([12.1513551 ,  2.97337983,  4.1866892 ]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 0.9850769829873818}
episode index:1254
target Thresh 31.999908338457317
target distance 6.0
model initialize at round 1254
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 29.]), 'previousTarget': array([ 8., 29.]), 'currentState': array([ 8.1110136 , 23.40314232,  1.33199886]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 5.597958543463703}
done in step count: 12
reward sum = 0.8106461144434661
running average episode reward sum: 0.44937555294971415
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 29.]), 'previousTarget': array([ 8., 29.]), 'currentState': array([ 8.12355278, 28.01602861,  1.69441505]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 0.9916980359871141}
episode index:1255
target Thresh 31.999909250504903
target distance 5.0
model initialize at round 1255
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([18.31862502, 12.06036245,  6.22951928]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 6.893659392949178}
done in step count: 15
reward sum = 0.7568183295938413
running average episode reward sum: 0.4496203322304818
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([22.10382859,  7.8702798 ,  5.26184151]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 1.2492037957199909}
episode index:1256
target Thresh 31.99991015347747
target distance 8.0
model initialize at round 1256
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 26.]), 'previousTarget': array([15., 26.]), 'currentState': array([22.78090957, 22.02149379,  2.962347  ]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 8.739054031921805}
done in step count: 20
reward sum = 0.7019452041350722
running average episode reward sum: 0.4498210680076533
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 26.]), 'previousTarget': array([15., 26.]), 'currentState': array([15.86868762, 25.65097902,  2.73387109]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 0.9361804423503678}
episode index:1257
target Thresh 31.999911047465304
target distance 5.0
model initialize at round 1257
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([ 5.2650644 , 13.85170581,  5.56417373]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 4.907054065030642}
done in step count: 12
reward sum = 0.8145423571834435
running average episode reward sum: 0.4501109895411794
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.03458178, 9.92517191, 4.74694875]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.9258179957948112}
episode index:1258
target Thresh 31.99991193255781
target distance 14.0
model initialize at round 1258
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 19.]), 'previousTarget': array([12., 19.]), 'currentState': array([19.98238586,  5.14516191,  1.75143862]), 'targetState': array([12, 19], dtype=int32), 'currentDistance': 15.989841230425565}
done in step count: 41
reward sum = 0.4743328065152933
running average episode reward sum: 0.45013022847443923
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 19.]), 'previousTarget': array([12., 19.]), 'currentState': array([12.85424173, 18.20834772,  2.3927514 ]), 'targetState': array([12, 19], dtype=int32), 'currentDistance': 1.164664014632956}
episode index:1259
target Thresh 31.999912808843504
target distance 12.0
model initialize at round 1259
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 14.]), 'previousTarget': array([19., 14.]), 'currentState': array([ 7.34616775, 16.94091039,  6.21288514]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 12.019183002895621}
done in step count: 37
reward sum = 0.5411299326187118
running average episode reward sum: 0.4502024504618553
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 14.]), 'previousTarget': array([19., 14.]), 'currentState': array([18.0511846 , 14.04061136,  5.82336286]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 0.9496841244822626}
episode index:1260
target Thresh 31.999913676410007
target distance 17.0
model initialize at round 1260
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  6.]), 'previousTarget': array([25.,  6.]), 'currentState': array([21.      , 23.      ,  1.193156]), 'targetState': array([25,  6], dtype=int32), 'currentDistance': 17.46424919657298}
done in step count: 44
reward sum = 0.3745553522131123
running average episode reward sum: 0.4501424606932203
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  6.]), 'previousTarget': array([25.,  6.]), 'currentState': array([24.73063665,  6.9693368 ,  4.75414859]), 'targetState': array([25,  6], dtype=int32), 'currentDistance': 1.006066819487993}
episode index:1261
target Thresh 31.99991453534408
target distance 15.0
model initialize at round 1261
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 24.]), 'previousTarget': array([27., 24.]), 'currentState': array([17.0281491 ,  8.96972893,  5.70168576]), 'targetState': array([27, 24], dtype=int32), 'currentDistance': 18.037373947269046}
done in step count: 51
reward sum = 0.37433610501069126
running average episode reward sum: 0.4500823922655796
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 24.]), 'previousTarget': array([27., 24.]), 'currentState': array([26.82427427, 23.21880747,  1.15969865]), 'targetState': array([27, 24], dtype=int32), 'currentDistance': 0.8007129926961389}
episode index:1262
target Thresh 31.99991538573161
target distance 9.0
model initialize at round 1262
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 20.]), 'previousTarget': array([ 3., 20.]), 'currentState': array([12.01168678, 23.01108761,  1.01159549]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 9.501428689025998}
done in step count: 23
reward sum = 0.6120583374929549
running average episode reward sum: 0.45021063925309135
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 20.]), 'previousTarget': array([ 3., 20.]), 'currentState': array([ 3.87831839, 20.82501897,  3.59991003]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 1.2050309148475316}
episode index:1263
target Thresh 31.99991622765765
target distance 14.0
model initialize at round 1263
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([8.17506545, 4.22741938, 0.97229803]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 17.517510805141065}
done in step count: 41
reward sum = 0.4309116089671473
running average episode reward sum: 0.4501953710329284
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([18.21554508, 17.32558005,  0.52302825]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 1.0345104159501899}
episode index:1264
target Thresh 31.999917061206386
target distance 12.0
model initialize at round 1264
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 26.]), 'previousTarget': array([27., 26.]), 'currentState': array([26.913236  , 14.14157682,  1.9919022 ]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 11.858740587298712}
done in step count: 28
reward sum = 0.5947013886221075
running average episode reward sum: 0.4503096050389277
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 26.]), 'previousTarget': array([27., 26.]), 'currentState': array([26.92790071, 25.1366236 ,  1.62937322]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 0.8663816214578463}
episode index:1265
target Thresh 31.99991788646117
target distance 5.0
model initialize at round 1265
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([22.28364787,  7.10287297,  0.1239832 ]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 6.251203410317314}
done in step count: 12
reward sum = 0.7859821823429469
running average episode reward sum: 0.4505747492548077
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([26.00817418,  3.95520473,  5.43109628]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 1.3770020062424217}
episode index:1266
target Thresh 31.99991870350453
target distance 17.0
model initialize at round 1266
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 28.]), 'previousTarget': array([10., 28.]), 'currentState': array([27.        , 29.        ,  5.32338998]), 'targetState': array([10, 28], dtype=int32), 'currentDistance': 17.029386365926403}
done in step count: 51
reward sum = 0.3780104926593062
running average episode reward sum: 0.4505174767555216
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 28.]), 'previousTarget': array([10., 28.]), 'currentState': array([10.87110278, 27.73962032,  2.9270976 ]), 'targetState': array([10, 28], dtype=int32), 'currentDistance': 0.9091851504715138}
episode index:1267
target Thresh 31.999919512418177
target distance 7.0
model initialize at round 1267
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 18.]), 'previousTarget': array([24., 18.]), 'currentState': array([25.70503547, 11.11409687,  2.54419723]), 'targetState': array([24, 18], dtype=int32), 'currentDistance': 7.093857051030706}
done in step count: 16
reward sum = 0.7362086407381091
running average episode reward sum: 0.45074278524446687
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 18.]), 'previousTarget': array([24., 18.]), 'currentState': array([24.21285765, 17.22617716,  1.86872616]), 'targetState': array([24, 18], dtype=int32), 'currentDistance': 0.8025647455163593}
episode index:1268
target Thresh 31.999920313283
target distance 18.0
model initialize at round 1268
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 20.]), 'previousTarget': array([ 2., 20.]), 'currentState': array([19.66672754, 20.96265071,  3.30604888]), 'targetState': array([ 2, 20], dtype=int32), 'currentDistance': 17.6929352642226}
done in step count: 41
reward sum = 0.4226494052834133
running average episode reward sum: 0.4507206470411879
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 20.]), 'previousTarget': array([ 2., 20.]), 'currentState': array([ 2.85702269, 20.18900465,  2.9966681 ]), 'targetState': array([ 2, 20], dtype=int32), 'currentDistance': 0.8776164602287504}
episode index:1269
target Thresh 31.99992110617908
target distance 4.0
model initialize at round 1269
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([22.        , 20.        ,  4.80583143]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 4.123105625617661}
done in step count: 11
reward sum = 0.8165429322396108
running average episode reward sum: 0.4510086960846512
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([25.13036446, 20.56266095,  0.4435331 ]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 0.9734122494016788}
episode index:1270
target Thresh 31.999921891185714
target distance 15.0
model initialize at round 1270
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([ 5.        , 16.        ,  4.91861677]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 18.601075237738275}
done in step count: 53
reward sum = 0.38279205406926653
running average episode reward sum: 0.45095502445442665
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([19.10405345,  5.55238553,  5.45596142]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 1.0525445354270035}
episode index:1271
target Thresh 31.9999226683814
target distance 17.0
model initialize at round 1271
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 20.]), 'previousTarget': array([14., 20.]), 'currentState': array([24.02234516,  3.07582356,  1.48654592]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 19.669142092770134}
done in step count: 60
reward sum = 0.35463669329111236
running average episode reward sum: 0.4508793024959649
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 20.]), 'previousTarget': array([14., 20.]), 'currentState': array([14.56973532, 19.14055527,  1.85591289]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 1.031137031356379}
episode index:1272
target Thresh 31.999923437843865
target distance 10.0
model initialize at round 1272
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([12.75010419,  3.99156118,  2.92284918]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 13.294102863727462}
done in step count: 29
reward sum = 0.5450808751378896
running average episode reward sum: 0.4509533021602555
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([ 4.85095035, 13.17854821,  2.27894252]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 1.1827508385993464}
episode index:1273
target Thresh 31.999924199650046
target distance 8.0
model initialize at round 1273
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([19.87856072, 22.06221219,  2.79989931]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 7.949843629103421}
done in step count: 19
reward sum = 0.7196007597783229
running average episode reward sum: 0.45116417143625087
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([12.8285945 , 21.28240062,  3.22010988]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 0.8753964545694076}
episode index:1274
target Thresh 31.99992495387613
target distance 3.0
model initialize at round 1274
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 21.]), 'previousTarget': array([15., 21.]), 'currentState': array([17.76621243, 21.75531439,  3.81022719]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 2.867478172029097}
done in step count: 5
reward sum = 0.9098170495072305
running average episode reward sum: 0.45152389918375746
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 21.]), 'previousTarget': array([15., 21.]), 'currentState': array([15.91511089, 21.22254235,  3.16492556]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 0.9417818388905822}
episode index:1275
target Thresh 31.99992570059754
target distance 19.0
model initialize at round 1275
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.34049097, 14.07317786]), 'previousTarget': array([20.33589719, 14.09517373]), 'currentState': array([ 6.97401756, 28.95058988,  4.47146511]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 81
reward sum = 0.2110613115412509
running average episode reward sum: 0.4513354488799624
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([23.09775502, 10.70097227,  5.35790292]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 1.1425445830481331}
episode index:1276
target Thresh 31.999926439888945
target distance 9.0
model initialize at round 1276
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([16.9985616 , 14.97854094,  4.89327514]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 10.82976741927151}
done in step count: 28
reward sum = 0.58845791098982
running average episode reward sum: 0.451442827472061
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([25.047459  , 20.07757259,  0.66526344]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 1.32597385890693}
episode index:1277
target Thresh 31.99992717182428
target distance 12.0
model initialize at round 1277
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 20.]), 'previousTarget': array([19., 20.]), 'currentState': array([7.        , 9.        , 2.48822856]), 'targetState': array([19, 20], dtype=int32), 'currentDistance': 16.27882059609971}
done in step count: 41
reward sum = 0.43559652106311975
running average episode reward sum: 0.4514304281712715
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 20.]), 'previousTarget': array([19., 20.]), 'currentState': array([18.10613834, 19.09867469,  0.6176336 ]), 'targetState': array([19, 20], dtype=int32), 'currentDistance': 1.2693998503054031}
episode index:1278
target Thresh 31.999927896476734
target distance 8.0
model initialize at round 1278
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 16.]), 'previousTarget': array([18., 16.]), 'currentState': array([10.00288504, 18.01453119,  1.13482657]), 'targetState': array([18, 16], dtype=int32), 'currentDistance': 8.24694996113836}
done in step count: 20
reward sum = 0.6800461053042961
running average episode reward sum: 0.4516091738140651
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 16.]), 'previousTarget': array([18., 16.]), 'currentState': array([17.22240352, 16.21577341,  6.02320067]), 'targetState': array([18, 16], dtype=int32), 'currentDistance': 0.8069785962316003}
episode index:1279
target Thresh 31.99992861391878
target distance 8.0
model initialize at round 1279
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([11.27328803, 12.3264773 ,  0.91810804]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 9.57487235722629}
done in step count: 20
reward sum = 0.6713593163275722
running average episode reward sum: 0.45178085361290377
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([16.36238256, 19.20308695,  1.10637918]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 1.0206010009635547}
episode index:1280
target Thresh 31.999929324222155
target distance 7.0
model initialize at round 1280
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 28.]), 'previousTarget': array([10., 28.]), 'currentState': array([14.        , 21.        ,  0.31987637]), 'targetState': array([10, 28], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 18
reward sum = 0.6865023072731211
running average episode reward sum: 0.45196408659780635
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 28.]), 'previousTarget': array([10., 28.]), 'currentState': array([10.70706309, 27.0231409 ,  2.34495121]), 'targetState': array([10, 28], dtype=int32), 'currentDistance': 1.2058988035993088}
episode index:1281
target Thresh 31.999930027457896
target distance 10.0
model initialize at round 1281
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'previousTarget': array([22., 18.]), 'currentState': array([12.        , 18.        ,  2.52196184]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 10.0}
done in step count: 28
reward sum = 0.5807292701822093
running average episode reward sum: 0.4520645274586367
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'previousTarget': array([22., 18.]), 'currentState': array([21.28969754, 17.73205084,  0.03091332]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 0.7591616043746133}
episode index:1282
target Thresh 31.999930723696323
target distance 6.0
model initialize at round 1282
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 16.]), 'previousTarget': array([16., 16.]), 'currentState': array([10.01866842, 10.94436104,  5.25816053]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 7.831718383549937}
done in step count: 20
reward sum = 0.6935583109002015
running average episode reward sum: 0.4522527533225818
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 16.]), 'previousTarget': array([16., 16.]), 'currentState': array([15.17985787, 15.18940657,  0.77817562]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 1.1531239383991845}
episode index:1283
target Thresh 31.99993141300706
target distance 13.0
model initialize at round 1283
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 16.]), 'previousTarget': array([19., 16.]), 'currentState': array([19.04425213, 28.6330053 ,  4.77994208]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 12.633082803401825}
done in step count: 28
reward sum = 0.5769573639708738
running average episode reward sum: 0.4523498752934917
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 16.]), 'previousTarget': array([19., 16.]), 'currentState': array([19.30153558, 16.98822409,  4.53824109]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 1.0332040267786384}
episode index:1284
target Thresh 31.999932095459044
target distance 14.0
model initialize at round 1284
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 21.]), 'previousTarget': array([ 5., 21.]), 'currentState': array([18.93349993, 13.20409516,  1.97150095]), 'targetState': array([ 5, 21], dtype=int32), 'currentDistance': 15.966169004291288}
done in step count: 38
reward sum = 0.4690046435190408
running average episode reward sum: 0.45236283620261664
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 21.]), 'previousTarget': array([ 5., 21.]), 'currentState': array([ 5.94971405, 20.45983592,  2.61661185]), 'targetState': array([ 5, 21], dtype=int32), 'currentDistance': 1.0925813461313245}
episode index:1285
target Thresh 31.999932771120516
target distance 2.0
model initialize at round 1285
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 23.]), 'previousTarget': array([11., 23.]), 'currentState': array([12.01987485, 24.70428979,  4.70406315]), 'targetState': array([11, 23], dtype=int32), 'currentDistance': 1.986139064322446}
done in step count: 3
reward sum = 0.9554777991151403
running average episode reward sum: 0.45275406090161546
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 23.]), 'previousTarget': array([11., 23.]), 'currentState': array([11.86378446, 23.89374556,  4.39176206]), 'targetState': array([11, 23], dtype=int32), 'currentDistance': 1.2429419568733333}
episode index:1286
target Thresh 31.999933440059042
target distance 17.0
model initialize at round 1286
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 29.]), 'previousTarget': array([15., 29.]), 'currentState': array([25.        , 12.        ,  3.47056034]), 'targetState': array([15, 29], dtype=int32), 'currentDistance': 19.72308292331602}
done in step count: 51
reward sum = 0.3426695522466937
running average episode reward sum: 0.45266852515285483
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 29.]), 'previousTarget': array([15., 29.]), 'currentState': array([15.49016048, 28.14751444,  1.69795734]), 'targetState': array([15, 29], dtype=int32), 'currentDistance': 0.9833559512491136}
episode index:1287
target Thresh 31.99993410234152
target distance 17.0
model initialize at round 1287
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 14.]), 'previousTarget': array([ 5., 14.]), 'currentState': array([22.00305269, 16.03678926,  1.73375237]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 17.124611271389128}
done in step count: 53
reward sum = 0.38957208719775505
running average episode reward sum: 0.45261953723518783
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 14.]), 'previousTarget': array([ 5., 14.]), 'currentState': array([ 5.98022997, 14.45575796,  3.37678219]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 1.0810023630104706}
episode index:1288
target Thresh 31.99993475803418
target distance 16.0
model initialize at round 1288
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 21.]), 'previousTarget': array([21., 21.]), 'currentState': array([16.06531141,  5.2941565 ,  1.32752331]), 'targetState': array([21, 21], dtype=int32), 'currentDistance': 16.462826959603802}
done in step count: 41
reward sum = 0.4663892552376194
running average episode reward sum: 0.45263021971618267
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 21.]), 'previousTarget': array([21., 21.]), 'currentState': array([20.80503617, 20.01451505,  1.2029838 ]), 'targetState': array([21, 21], dtype=int32), 'currentDistance': 1.004585224121751}
episode index:1289
target Thresh 31.999935407202585
target distance 11.0
model initialize at round 1289
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([21.00317389, 17.00343125,  0.68644926]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 14.217335879521384}
done in step count: 44
reward sum = 0.44137706139806326
running average episode reward sum: 0.4526214963376415
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([12.48882224,  6.87450373,  4.04119452]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 1.0018502647303305}
episode index:1290
target Thresh 31.999936049911657
target distance 11.0
model initialize at round 1290
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 15.]), 'previousTarget': array([ 5., 15.]), 'currentState': array([ 2.94059531, 25.81599229,  4.48835178]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 11.01030593661183}
done in step count: 27
reward sum = 0.6217916348660056
running average episode reward sum: 0.4527525344000182
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 15.]), 'previousTarget': array([ 5., 15.]), 'currentState': array([ 4.7776294 , 15.94523744,  4.9268133 ]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 0.9710419634653875}
episode index:1291
target Thresh 31.99993668622567
target distance 11.0
model initialize at round 1291
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 15.]), 'previousTarget': array([ 6., 15.]), 'currentState': array([6.96917599, 4.00341522, 2.79195848]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 11.03921096983287}
done in step count: 27
reward sum = 0.5957589372129943
running average episode reward sum: 0.45286322047030686
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 15.]), 'previousTarget': array([ 6., 15.]), 'currentState': array([ 6.14727713, 14.18985888,  1.62120061]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 0.8234192092523794}
episode index:1292
target Thresh 31.99993731620825
target distance 12.0
model initialize at round 1292
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 15.]), 'previousTarget': array([ 3., 15.]), 'currentState': array([8.01394603, 3.30566557, 1.59175418]), 'targetState': array([ 3, 15], dtype=int32), 'currentDistance': 12.723879616726812}
done in step count: 30
reward sum = 0.563963151263349
running average episode reward sum: 0.45294914462405245
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 15.]), 'previousTarget': array([ 3., 15.]), 'currentState': array([ 3.2842415 , 14.00792513,  1.92526643]), 'targetState': array([ 3, 15], dtype=int32), 'currentDistance': 1.031991175098348}
episode index:1293
target Thresh 31.999937939922397
target distance 14.0
model initialize at round 1293
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 22.]), 'previousTarget': array([20., 22.]), 'currentState': array([ 6.        , 21.        ,  1.54993558]), 'targetState': array([20, 22], dtype=int32), 'currentDistance': 14.035668847618199}
done in step count: 39
reward sum = 0.493085174339994
running average episode reward sum: 0.4529801616485624
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 22.]), 'previousTarget': array([20., 22.]), 'currentState': array([19.03804131, 22.00540511,  6.17497533]), 'targetState': array([20, 22], dtype=int32), 'currentDistance': 0.9619738800797798}
episode index:1294
target Thresh 31.99993855743049
target distance 10.0
model initialize at round 1294
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([21.03953104, 13.88027915,  4.77881098]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 12.131566816129553}
done in step count: 31
reward sum = 0.570172118969094
running average episode reward sum: 0.4530706573685011
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.37566768,  4.86008459,  4.06069755]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.9385476581662724}
episode index:1295
target Thresh 31.99993916879427
target distance 3.0
model initialize at round 1295
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([21.        , 15.        ,  6.10175812]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 12
reward sum = 0.8223879233777877
running average episode reward sum: 0.4533556243947428
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([19.9524919 , 17.18750767,  2.19214312]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 1.251952323806216}
episode index:1296
target Thresh 31.99993977407488
target distance 7.0
model initialize at round 1296
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 25.]), 'previousTarget': array([20., 25.]), 'currentState': array([27.        , 24.        ,  1.24386489]), 'targetState': array([20, 25], dtype=int32), 'currentDistance': 7.0710678118654755}
done in step count: 21
reward sum = 0.7016987213464391
running average episode reward sum: 0.4535470994116678
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 25.]), 'previousTarget': array([20., 25.]), 'currentState': array([20.80563806, 25.02068773,  3.13341377]), 'targetState': array([20, 25], dtype=int32), 'currentDistance': 0.8059036366901834}
episode index:1297
target Thresh 31.999940373332848
target distance 13.0
model initialize at round 1297
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 24.]), 'previousTarget': array([22., 24.]), 'currentState': array([18.13247181, 11.35039258,  1.20474717]), 'targetState': array([22, 24], dtype=int32), 'currentDistance': 13.227635546341622}
done in step count: 28
reward sum = 0.5659850468331258
running average episode reward sum: 0.4536337234081404
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 24.]), 'previousTarget': array([22., 24.]), 'currentState': array([21.15993428, 23.00914329,  1.53541623]), 'targetState': array([22, 24], dtype=int32), 'currentDistance': 1.2990409712919657}
episode index:1298
target Thresh 31.9999409666281
target distance 17.0
model initialize at round 1298
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  4.]), 'previousTarget': array([27.,  4.]), 'currentState': array([9.9649642 , 7.99955605, 3.24214271]), 'targetState': array([27,  4], dtype=int32), 'currentDistance': 17.49825400480838}
done in step count: 51
reward sum = 0.3622574741264347
running average episode reward sum: 0.45356337987520606
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  4.]), 'previousTarget': array([27.,  4.]), 'currentState': array([26.24117575,  4.12319302,  5.95197958]), 'targetState': array([27,  4], dtype=int32), 'currentDistance': 0.7687592367306874}
episode index:1299
target Thresh 31.999941554019962
target distance 18.0
model initialize at round 1299
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 13.]), 'previousTarget': array([ 3., 13.]), 'currentState': array([20.67689603, 17.84906178,  3.62220904]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 18.32992234797036}
done in step count: 54
reward sum = 0.38947692769640163
running average episode reward sum: 0.45351408260429926
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 13.]), 'previousTarget': array([ 3., 13.]), 'currentState': array([ 3.76278952, 13.03240177,  3.29619314]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 0.763477386387473}
episode index:1300
target Thresh 31.99994213556718
target distance 3.0
model initialize at round 1300
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 23.]), 'previousTarget': array([18., 23.]), 'currentState': array([18.        , 20.        ,  2.71242923]), 'targetState': array([18, 23], dtype=int32), 'currentDistance': 2.9999999999999996}
done in step count: 8
reward sum = 0.8719977123485482
running average episode reward sum: 0.4538357456556015
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 23.]), 'previousTarget': array([18., 23.]), 'currentState': array([17.48848971, 22.22532206,  1.21935716]), 'targetState': array([18, 23], dtype=int32), 'currentDistance': 0.9283149774446711}
episode index:1301
target Thresh 31.999942711327908
target distance 13.0
model initialize at round 1301
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([17.9319822 ,  4.95973469,  3.44217867]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 14.27308546361301}
done in step count: 39
reward sum = 0.49729673077484693
running average episode reward sum: 0.4538691258285041
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.78308785, 10.83052877,  2.53956133]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.801216000171454}
episode index:1302
target Thresh 31.99994328135972
target distance 24.0
model initialize at round 1302
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7.66587866, 22.93980158]), 'previousTarget': array([ 7.6609096 , 22.93091516]), 'currentState': array([6.02558577, 3.0071791 , 0.47531441]), 'targetState': array([ 8, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 75
reward sum = 0.23571538217301052
running average episode reward sum: 0.45370170162001944
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 27.]), 'previousTarget': array([ 8., 27.]), 'currentState': array([ 8.41676247, 26.02062411,  1.7201521 ]), 'targetState': array([ 8, 27], dtype=int32), 'currentDistance': 1.0643627622541914}
episode index:1303
target Thresh 31.99994384571962
target distance 13.0
model initialize at round 1303
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([24.97603273, 12.14515785,  1.76401673]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 15.709412935148533}
done in step count: 43
reward sum = 0.44851559991650947
running average episode reward sum: 0.45369772454816093
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([12.83979633, 20.69866022,  2.6634642 ]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 0.8922239283950041}
episode index:1304
target Thresh 31.999944404464046
target distance 13.0
model initialize at round 1304
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([16.75606798,  2.93943114,  3.31687663]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 12.921424622071992}
done in step count: 32
reward sum = 0.5614839331524226
running average episode reward sum: 0.45378031934402624
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.81735391, 4.97850195, 3.13105968]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.8176365784985128}
episode index:1305
target Thresh 31.99994495764887
target distance 6.0
model initialize at round 1305
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([18.        ,  7.        ,  4.37445521]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 6.708203932499369}
done in step count: 20
reward sum = 0.7021499947597445
running average episode reward sum: 0.4539704952057535
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([23.15925823,  9.21452979,  0.59444181]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 1.1505694963829551}
episode index:1306
target Thresh 31.999945505329414
target distance 24.0
model initialize at round 1306
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.85058903, 21.40892203]), 'previousTarget': array([25.8507125, 21.40285  ]), 'currentState': array([20.99334819,  2.00770527,  2.24932826]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.07053244186347851
running average episode reward sum: 0.45356919226996983
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 26.]), 'previousTarget': array([27., 26.]), 'currentState': array([23.68569655, 14.09078655,  1.3940331 ]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 12.361794865523342}
episode index:1307
target Thresh 31.999946047560446
target distance 2.0
model initialize at round 1307
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([17.9931389 , 19.99446686,  4.03493941]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 2.234203963459405}
done in step count: 5
reward sum = 0.9160036230654941
running average episode reward sum: 0.4539227354127799
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([18.19383825, 18.783566  ,  5.28854654]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 1.1242208118817352}
episode index:1308
target Thresh 31.99994658439619
target distance 25.0
model initialize at round 1308
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3.57726968, 7.66762388]), 'previousTarget': array([3.60740149, 7.74071961]), 'currentState': array([ 8.93939281, 26.93541139,  4.00780539]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.03763041080866141
running average episode reward sum: 0.45354721734843956
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([ 9.65199688, 19.14411628,  4.48277952]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 18.774285051815035}
episode index:1309
target Thresh 31.99994711589033
target distance 7.0
model initialize at round 1309
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([ 6.        , 28.        ,  1.68773198]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 23
reward sum = 0.608584379837831
running average episode reward sum: 0.4536655663274391
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([11.66281228, 21.85222333,  5.06200405]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 0.9165043202395794}
episode index:1310
target Thresh 31.99994764209601
target distance 9.0
model initialize at round 1310
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 22.]), 'previousTarget': array([25., 22.]), 'currentState': array([26.00060619, 12.99935489,  5.71919346]), 'targetState': array([25, 22], dtype=int32), 'currentDistance': 9.056093265687052}
done in step count: 27
reward sum = 0.6150430126744897
running average episode reward sum: 0.4537886612521889
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 22.]), 'previousTarget': array([25., 22.]), 'currentState': array([25.2584202 , 21.15141976,  1.77212669]), 'targetState': array([25, 22], dtype=int32), 'currentDistance': 0.8870566118912044}
episode index:1311
target Thresh 31.999948163065863
target distance 20.0
model initialize at round 1311
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6.8434743 , 3.25304229]), 'previousTarget': array([6.8434743 , 3.25304229]), 'currentState': array([26.        ,  9.        ,  4.29413077]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.31566174154327187
running average episode reward sum: 0.45368338158777666
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 3.]), 'previousTarget': array([6., 3.]), 'currentState': array([6.85741697, 2.43287204, 3.17865358]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 1.0280068031952616}
episode index:1312
target Thresh 31.999948678851975
target distance 10.0
model initialize at round 1312
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 24.]), 'previousTarget': array([16., 24.]), 'currentState': array([ 6.        , 19.        ,  2.66432562]), 'targetState': array([16, 24], dtype=int32), 'currentDistance': 11.180339887498949}
done in step count: 30
reward sum = 0.5695994387144403
running average episode reward sum: 0.45377166495192495
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 24.]), 'previousTarget': array([16., 24.]), 'currentState': array([15.02167527, 23.41444646,  0.30933048]), 'targetState': array([16, 24], dtype=int32), 'currentDistance': 1.1401720126530528}
episode index:1313
target Thresh 31.99994918950593
target distance 14.0
model initialize at round 1313
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  5.]), 'previousTarget': array([18.,  5.]), 'currentState': array([13.98618089, 19.00689217,  2.89586118]), 'targetState': array([18,  5], dtype=int32), 'currentDistance': 14.57064761882251}
done in step count: 39
reward sum = 0.46243711362994944
running average episode reward sum: 0.4537782596617255
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  5.]), 'previousTarget': array([18.,  5.]), 'currentState': array([17.66831621,  5.98991106,  4.97661728]), 'targetState': array([18,  5], dtype=int32), 'currentDistance': 1.044000976352228}
episode index:1314
target Thresh 31.999949695078794
target distance 14.0
model initialize at round 1314
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 13.]), 'previousTarget': array([26., 13.]), 'currentState': array([20.93284909, 26.80522991,  4.53546539]), 'targetState': array([26, 13], dtype=int32), 'currentDistance': 14.705794473168472}
done in step count: 43
reward sum = 0.49837679031603677
running average episode reward sum: 0.45381217489416226
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 13.]), 'previousTarget': array([26., 13.]), 'currentState': array([25.08883849, 13.91469868,  5.33580382]), 'targetState': array([26, 13], dtype=int32), 'currentDistance': 1.291080546850951}
episode index:1315
target Thresh 31.99995019562112
target distance 25.0
model initialize at round 1315
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7.01598083, 3.79936077]), 'previousTarget': array([7.01598083, 3.79936077]), 'currentState': array([27.        ,  3.        ,  4.93395519]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 77
reward sum = 0.19540692087854056
running average episode reward sum: 0.45361581831816256
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.83591542, 4.05860304, 3.06942938]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.837967122227234}
episode index:1316
target Thresh 31.99995069118297
target distance 15.0
model initialize at round 1316
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 12.]), 'previousTarget': array([19., 12.]), 'currentState': array([4.06440394, 4.94920675, 5.85257506]), 'targetState': array([19, 12], dtype=int32), 'currentDistance': 16.516225816980754}
done in step count: 39
reward sum = 0.4486350221104396
running average episode reward sum: 0.4536120363924164
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 12.]), 'previousTarget': array([19., 12.]), 'currentState': array([18.05560948, 11.64719023,  0.56434048]), 'targetState': array([19, 12], dtype=int32), 'currentDistance': 1.008140952528445}
episode index:1317
target Thresh 31.9999511818139
target distance 5.0
model initialize at round 1317
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 17.]), 'previousTarget': array([21., 17.]), 'currentState': array([25.59783514, 15.98356331,  3.15581004]), 'targetState': array([21, 17], dtype=int32), 'currentDistance': 4.708846096390798}
done in step count: 11
reward sum = 0.8376871359884546
running average episode reward sum: 0.45390344390349074
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 17.]), 'previousTarget': array([21., 17.]), 'currentState': array([21.90836386, 16.48073608,  2.83963827]), 'targetState': array([21, 17], dtype=int32), 'currentDistance': 1.0463077606315037}
episode index:1318
target Thresh 31.999951667562968
target distance 4.0
model initialize at round 1318
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 28.]), 'previousTarget': array([18., 28.]), 'currentState': array([17.93754238, 24.35671537,  1.72637543]), 'targetState': array([18, 28], dtype=int32), 'currentDistance': 3.6438199539168714}
done in step count: 8
reward sum = 0.877079042331575
running average episode reward sum: 0.4542242745315636
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 28.]), 'previousTarget': array([18., 28.]), 'currentState': array([17.69720721, 27.16676553,  1.57791855]), 'targetState': array([18, 28], dtype=int32), 'currentDistance': 0.8865456294690652}
episode index:1319
target Thresh 31.99995214847875
target distance 6.0
model initialize at round 1319
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([6.03282009, 5.02569852, 0.91679618]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 6.700020585150649}
done in step count: 15
reward sum = 0.7491255544899407
running average episode reward sum: 0.4544476845921382
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.76311513, 10.09020233,  1.738901  ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.18746642007472}
episode index:1320
target Thresh 31.999952624609342
target distance 16.0
model initialize at round 1320
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 2.]), 'previousTarget': array([5., 2.]), 'currentState': array([ 2.01458978, 17.98133843,  5.2020393 ]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 16.257793581840904}
done in step count: 42
reward sum = 0.44681380066233545
running average episode reward sum: 0.4544419057246667
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 2.]), 'previousTarget': array([5., 2.]), 'currentState': array([4.48826418, 2.92608219, 4.68615075]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 1.0580651121182814}
episode index:1321
target Thresh 31.999953096002354
target distance 8.0
model initialize at round 1321
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 28.]), 'previousTarget': array([11., 28.]), 'currentState': array([11.00958665, 20.4322797 ,  1.54682989]), 'targetState': array([11, 28], dtype=int32), 'currentDistance': 7.567726368098428}
done in step count: 17
reward sum = 0.7343524733196698
running average episode reward sum: 0.45465363837791556
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 28.]), 'previousTarget': array([11., 28.]), 'currentState': array([10.72539766, 27.13391784,  1.62844026]), 'targetState': array([11, 28], dtype=int32), 'currentDistance': 0.9085729163454696}
episode index:1322
target Thresh 31.99995356270493
target distance 9.0
model initialize at round 1322
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([23.       , 15.       ,  0.8398369]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 29
reward sum = 0.597752611706428
running average episode reward sum: 0.45476180086720397
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([21.2056504 ,  6.88123123,  4.36876984]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 0.9049091498040468}
episode index:1323
target Thresh 31.999954024763735
target distance 21.0
model initialize at round 1323
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.47290771,  6.99469707]), 'previousTarget': array([10.47290771,  6.99469707]), 'currentState': array([21.        , 24.        ,  4.69409892]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 20.0}
done in step count: 78
reward sum = 0.23242671162070386
running average episode reward sum: 0.4545938740626371
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([8.483919  , 3.85452442, 3.93888258]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 0.9820333896992124}
episode index:1324
target Thresh 31.99995448222498
target distance 11.0
model initialize at round 1324
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 14.]), 'previousTarget': array([24., 14.]), 'currentState': array([13.03760196, 17.05993959,  0.75952879]), 'targetState': array([24, 14], dtype=int32), 'currentDistance': 11.381449864946156}
done in step count: 28
reward sum = 0.5781598357867903
running average episode reward sum: 0.45468713139224026
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 14.]), 'previousTarget': array([24., 14.]), 'currentState': array([23.56928977, 14.98264138,  5.54475672]), 'targetState': array([24, 14], dtype=int32), 'currentDistance': 1.0728911334524773}
episode index:1325
target Thresh 31.999954935134408
target distance 20.0
model initialize at round 1325
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.56953382, 13.57218647]), 'previousTarget': array([22.56953382, 13.57218647]), 'currentState': array([ 4.        , 21.        ,  1.63676429]), 'targetState': array([24, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.27114910510003426
running average episode reward sum: 0.4545487165911149
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 13.]), 'previousTarget': array([24., 13.]), 'currentState': array([23.17424496, 13.31906833,  6.28099412]), 'targetState': array([24, 13], dtype=int32), 'currentDistance': 0.8852547600706353}
episode index:1326
target Thresh 31.999955383537312
target distance 14.0
model initialize at round 1326
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([18.08741172,  7.31416922,  1.54548258]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 16.42811662672293}
done in step count: 41
reward sum = 0.45729388130341453
running average episode reward sum: 0.4545507852909734
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([ 9.89927075, 20.00182052,  2.39049699]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 1.3435215516295052}
episode index:1327
target Thresh 31.999955827478534
target distance 9.0
model initialize at round 1327
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 19.]), 'previousTarget': array([10., 19.]), 'currentState': array([ 8.91141036, 27.94330528,  3.96338463]), 'targetState': array([10, 19], dtype=int32), 'currentDistance': 9.009313887063296}
done in step count: 22
reward sum = 0.6722710695087334
running average episode reward sum: 0.45471473128812534
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 19.]), 'previousTarget': array([10., 19.]), 'currentState': array([ 9.54321766, 19.87380099,  4.82395962]), 'targetState': array([10, 19], dtype=int32), 'currentDistance': 0.9859910141986317}
episode index:1328
target Thresh 31.999956267002467
target distance 13.0
model initialize at round 1328
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 18.]), 'previousTarget': array([26., 18.]), 'currentState': array([18.        ,  5.        ,  2.58555293]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 15.264337522473747}
done in step count: 37
reward sum = 0.4469268152446688
running average episode reward sum: 0.4547088713061513
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 18.]), 'previousTarget': array([26., 18.]), 'currentState': array([26.00416992, 17.15768153,  0.94738491]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 0.8423287946737751}
episode index:1329
target Thresh 31.999956702153064
target distance 7.0
model initialize at round 1329
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 15.]), 'previousTarget': array([13., 15.]), 'currentState': array([20.        , 12.        ,  4.54503345]), 'targetState': array([13, 15], dtype=int32), 'currentDistance': 7.615773105863909}
done in step count: 19
reward sum = 0.6703415173940276
running average episode reward sum: 0.4548710011152399
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 15.]), 'previousTarget': array([13., 15.]), 'currentState': array([13.91539183, 15.0998233 ,  3.17458674]), 'targetState': array([13, 15], dtype=int32), 'currentDistance': 0.9208186035192724}
episode index:1330
target Thresh 31.99995713297384
target distance 7.0
model initialize at round 1330
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 29.]), 'previousTarget': array([24., 29.]), 'currentState': array([26.0220325 , 22.03877289,  1.22822922]), 'targetState': array([24, 29], dtype=int32), 'currentDistance': 7.248951533490364}
done in step count: 19
reward sum = 0.723289132418302
running average episode reward sum: 0.4550726676301183
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 29.]), 'previousTarget': array([24., 29.]), 'currentState': array([23.61013696, 28.02396586,  2.11983417]), 'targetState': array([24, 29], dtype=int32), 'currentDistance': 1.0510165731232677}
episode index:1331
target Thresh 31.999957559507877
target distance 22.0
model initialize at round 1331
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.73750984, 19.87322975]), 'previousTarget': array([19.73750984, 19.87322975]), 'currentState': array([9.        , 3.        , 1.62778464]), 'targetState': array([23, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 94
reward sum = 0.1648721738041392
running average episode reward sum: 0.45485479939151024
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 25.]), 'previousTarget': array([23., 25.]), 'currentState': array([23.13927161, 24.15481669,  1.45361513]), 'targetState': array([23, 25], dtype=int32), 'currentDistance': 0.8565812323149627}
episode index:1332
target Thresh 31.999957981797827
target distance 23.0
model initialize at round 1332
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.25799773, 13.11978447]), 'previousTarget': array([22.13347761, 13.17676768]), 'currentState': array([ 3.09750776, 18.85351137,  5.40069999]), 'targetState': array([26, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 75
reward sum = 0.26477683033777777
running average episode reward sum: 0.45471220526618855
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 12.]), 'previousTarget': array([26., 12.]), 'currentState': array([25.01101568, 12.51238901,  6.02836264]), 'targetState': array([26, 12], dtype=int32), 'currentDistance': 1.1138368324700514}
episode index:1333
target Thresh 31.999958399885926
target distance 15.0
model initialize at round 1333
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'previousTarget': array([ 2., 17.]), 'currentState': array([9.        , 2.        , 0.89415044]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 16.55294535724685}
done in step count: 45
reward sum = 0.43235887589497757
running average episode reward sum: 0.4546954486474695
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'previousTarget': array([ 2., 17.]), 'currentState': array([ 2.15924619, 16.11476476,  1.98544982]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 0.8994447020632108}
episode index:1334
target Thresh 31.999958813813976
target distance 7.0
model initialize at round 1334
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 28.]), 'previousTarget': array([21., 28.]), 'currentState': array([15.        , 21.        ,  4.26517817]), 'targetState': array([21, 28], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 23
reward sum = 0.610882872099117
running average episode reward sum: 0.4548124429721524
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 28.]), 'previousTarget': array([21., 28.]), 'currentState': array([20.68344584, 27.10543196,  0.9096987 ]), 'targetState': array([21, 28], dtype=int32), 'currentDistance': 0.9489249257860819}
episode index:1335
target Thresh 31.999959223623375
target distance 5.0
model initialize at round 1335
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([ 5.9209133 , 22.981596  ,  3.59257859]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 4.982223743658246}
done in step count: 13
reward sum = 0.8040930447064295
running average episode reward sum: 0.4550738805483008
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([ 5.43182608, 18.90475679,  4.81524273]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 1.0683662519217236}
episode index:1336
target Thresh 31.9999596293551
target distance 22.0
model initialize at round 1336
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7.37636887, 4.07384134]), 'previousTarget': array([7.42229124, 4.3226018 ]), 'currentState': array([10.94771029, 23.75239618,  4.52391082]), 'targetState': array([7, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 89
reward sum = 0.2322221513261588
running average episode reward sum: 0.454907200122555
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 2.]), 'previousTarget': array([7., 2.]), 'currentState': array([7.1334893 , 2.99921547, 4.79720714]), 'targetState': array([7, 2], dtype=int32), 'currentDistance': 1.0080927307516254}
episode index:1337
target Thresh 31.999960031049728
target distance 19.0
model initialize at round 1337
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.32014017,  5.09022194]), 'previousTarget': array([11.32014017,  5.09022194]), 'currentState': array([22.        , 22.        ,  2.13930082]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 72
reward sum = 0.24144898387910801
running average episode reward sum: 0.45474766483388274
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  3.]), 'previousTarget': array([10.,  3.]), 'currentState': array([10.02284935,  3.71321715,  4.71209033]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 0.7135830641404095}
episode index:1338
target Thresh 31.99996042874743
target distance 11.0
model initialize at round 1338
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 24.]), 'previousTarget': array([16., 24.]), 'currentState': array([ 7.07349868, 12.97921042,  6.26002979]), 'targetState': array([16, 24], dtype=int32), 'currentDistance': 14.18239150048039}
done in step count: 52
reward sum = 0.42844848999588336
running average episode reward sum: 0.4547280239266101
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 24.]), 'previousTarget': array([16., 24.]), 'currentState': array([15.01306755, 23.96067114,  0.83628082]), 'targetState': array([16, 24], dtype=int32), 'currentDistance': 0.9877157539713203}
episode index:1339
target Thresh 31.99996082248797
target distance 10.0
model initialize at round 1339
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 2.]), 'previousTarget': array([8., 2.]), 'currentState': array([17.88825197, 11.97203069,  3.52842608]), 'targetState': array([8, 2], dtype=int32), 'currentDistance': 14.0434654960236}
done in step count: 30
reward sum = 0.5391311478951785
running average episode reward sum: 0.4547910113325568
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 2.]), 'previousTarget': array([8., 2.]), 'currentState': array([8.63629898, 2.96691072, 3.91490117]), 'targetState': array([8, 2], dtype=int32), 'currentDistance': 1.1574941600849926}
episode index:1340
target Thresh 31.99996121231073
target distance 13.0
model initialize at round 1340
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([ 5.9905034 , 23.90304087,  4.8361125 ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 15.186852853391603}
done in step count: 38
reward sum = 0.4877715081522893
running average episode reward sum: 0.45481560528991677
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.06507001, 11.412215  ,  5.36861884]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.021770667940748}
episode index:1341
target Thresh 31.999961598254686
target distance 17.0
model initialize at round 1341
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 15.]), 'previousTarget': array([21., 15.]), 'currentState': array([ 3.99519146, 24.99396577,  4.27947497]), 'targetState': array([21, 15], dtype=int32), 'currentDistance': 19.724169576263236}
done in step count: 62
reward sum = 0.31471095026809937
running average episode reward sum: 0.45471120539794824
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 15.]), 'previousTarget': array([21., 15.]), 'currentState': array([20.80397366, 15.95247687,  5.7345002 ]), 'targetState': array([21, 15], dtype=int32), 'currentDistance': 0.9724394672850433}
episode index:1342
target Thresh 31.999961980358435
target distance 21.0
model initialize at round 1342
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.59774626,  6.94447788]), 'previousTarget': array([13.59867161,  6.94278962]), 'currentState': array([ 4.9984677 , 25.00139922,  2.65406179]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 86
reward sum = 0.1826067001350305
running average episode reward sum: 0.45450859593758863
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.00811835,  3.93804032,  5.41185146]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.9938149785522602}
episode index:1343
target Thresh 31.99996235866019
target distance 4.0
model initialize at round 1343
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  8.]), 'previousTarget': array([11.,  8.]), 'currentState': array([12.00923108,  4.0003403 ,  0.2831161 ]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 4.125024255844459}
done in step count: 11
reward sum = 0.8284366253077529
running average episode reward sum: 0.4547868161975367
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  8.]), 'previousTarget': array([11.,  8.]), 'currentState': array([11.99907857,  7.1335334 ,  1.90844453]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 1.3224682817804316}
episode index:1344
target Thresh 31.99996273319778
target distance 19.0
model initialize at round 1344
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.39052622, 11.31786599]), 'previousTarget': array([19.29367831, 11.49385478]), 'currentState': array([ 7.983353  , 27.74578122,  4.75190423]), 'targetState': array([21,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.30062906246946236
running average episode reward sum: 0.4546722007672556
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  9.]), 'previousTarget': array([21.,  9.]), 'currentState': array([20.00596237,  9.84557485,  5.2812112 ]), 'targetState': array([21,  9], dtype=int32), 'currentDistance': 1.3050316564630196}
episode index:1345
target Thresh 31.999963104008657
target distance 22.0
model initialize at round 1345
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.26673649, 11.47545314]), 'previousTarget': array([14.26673649, 11.47545314]), 'currentState': array([ 3.        , 28.        ,  3.54273134]), 'targetState': array([18,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 91
reward sum = 0.15703451753273162
running average episode reward sum: 0.45445107321656125
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  6.]), 'previousTarget': array([18.,  6.]), 'currentState': array([17.10898229,  5.49335595,  5.49609376]), 'targetState': array([18,  6], dtype=int32), 'currentDistance': 1.0249881691726968}
episode index:1346
target Thresh 31.999963471129906
target distance 10.0
model initialize at round 1346
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([12.38020232, 10.00985645,  6.1953829 ]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 9.827513949292772}
done in step count: 21
reward sum = 0.6583939078630351
running average episode reward sum: 0.4546024784390159
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([21.12792698,  7.8677723 ,  5.95094264]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 0.8820405465608839}
episode index:1347
target Thresh 31.999963834598237
target distance 4.0
model initialize at round 1347
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 12.]), 'previousTarget': array([12., 12.]), 'currentState': array([ 8.98740569, 15.89595769,  4.6633901 ]), 'targetState': array([12, 12], dtype=int32), 'currentDistance': 4.9248564243032655}
done in step count: 12
reward sum = 0.8285084123209159
running average episode reward sum: 0.4548798567282459
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 12.]), 'previousTarget': array([12., 12.]), 'currentState': array([11.0837066 , 12.57093144,  5.54319441]), 'targetState': array([12, 12], dtype=int32), 'currentDistance': 1.079609328947019}
episode index:1348
target Thresh 31.999964194449994
target distance 8.0
model initialize at round 1348
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 3.        , 18.        ,  3.29685855]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 10.0}
done in step count: 26
reward sum = 0.5951285130236197
running average episode reward sum: 0.4549838216328384
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 8.51232373, 10.84366703,  5.2173426 ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.9744753456173927}
episode index:1349
target Thresh 31.99996455072117
target distance 7.0
model initialize at round 1349
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 20.]), 'previousTarget': array([14., 20.]), 'currentState': array([14.        , 27.        ,  2.44132441]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 7.000000000000001}
done in step count: 21
reward sum = 0.6825910249683262
running average episode reward sum: 0.4551524195612351
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 20.]), 'previousTarget': array([14., 20.]), 'currentState': array([13.35766623, 20.76320027,  4.56533812]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 0.9975306150208761}
episode index:1350
target Thresh 31.99996490344739
target distance 14.0
model initialize at round 1350
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([23.876654  ,  9.91055408,  3.51693448]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 14.466689126450508}
done in step count: 38
reward sum = 0.4907919810669188
running average episode reward sum: 0.4551787996955842
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([10.83882201, 14.66476703,  3.02217306]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 1.0702978885155785}
episode index:1351
target Thresh 31.999965252663923
target distance 12.0
model initialize at round 1351
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 13.]), 'previousTarget': array([ 6., 13.]), 'currentState': array([14.97162674, 25.05218416,  2.2939662 ]), 'targetState': array([ 6, 13], dtype=int32), 'currentDistance': 15.024820446987231}
done in step count: 33
reward sum = 0.47392283969855464
running average episode reward sum: 0.4551926636304976
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 13.]), 'previousTarget': array([ 6., 13.]), 'currentState': array([ 6.93076461, 13.85438504,  4.33284025]), 'targetState': array([ 6, 13], dtype=int32), 'currentDistance': 1.2634463012898276}
episode index:1352
target Thresh 31.999965598405694
target distance 10.0
model initialize at round 1352
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 15.]), 'previousTarget': array([25., 15.]), 'currentState': array([15.07493753, 19.09201886,  0.69619125]), 'targetState': array([25, 15], dtype=int32), 'currentDistance': 10.73552436927536}
done in step count: 28
reward sum = 0.6091556573745746
running average episode reward sum: 0.45530645741744813
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 15.]), 'previousTarget': array([25., 15.]), 'currentState': array([24.04608482, 15.45280806,  6.07875177]), 'targetState': array([25, 15], dtype=int32), 'currentDistance': 1.0559305417661757}
episode index:1353
target Thresh 31.999965940707277
target distance 7.0
model initialize at round 1353
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 16.]), 'previousTarget': array([ 5., 16.]), 'currentState': array([4.00319344, 8.99560329, 5.59306574]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 7.074969722742055}
done in step count: 25
reward sum = 0.6705642898005164
running average episode reward sum: 0.455465436614186
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 16.]), 'previousTarget': array([ 5., 16.]), 'currentState': array([ 5.73920498, 15.10535823,  1.88206441]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 1.1605205350230126}
episode index:1354
target Thresh 31.9999662796029
target distance 15.0
model initialize at round 1354
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 8.]), 'previousTarget': array([6., 8.]), 'currentState': array([ 9.        , 23.        ,  2.79796457]), 'targetState': array([6, 8], dtype=int32), 'currentDistance': 15.297058540778355}
done in step count: 42
reward sum = 0.4411140387960223
running average episode reward sum: 0.45545484517668183
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 8.]), 'previousTarget': array([6., 8.]), 'currentState': array([5.84508644, 8.83873977, 4.68485225]), 'targetState': array([6, 8], dtype=int32), 'currentDistance': 0.8529259104501531}
episode index:1355
target Thresh 31.999966615126457
target distance 9.0
model initialize at round 1355
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([10.        , 19.        ,  0.54305768]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 27
reward sum = 0.6157839826566818
running average episode reward sum: 0.455573082003732
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 8.59240753, 10.93456786,  4.31041097]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 1.1065097175790757}
episode index:1356
target Thresh 31.9999669473115
target distance 10.0
model initialize at round 1356
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([20.91246391, 15.02566446,  2.99817064]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 10.698734281816561}
done in step count: 22
reward sum = 0.63060103350701
running average episode reward sum: 0.4557020635450019
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.76243567, 11.28080963,  3.61081863]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.8125036642965308}
episode index:1357
target Thresh 31.999967276191246
target distance 6.0
model initialize at round 1357
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([13.38282205, 19.07406044,  0.14020821]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 5.718941680613245}
done in step count: 12
reward sum = 0.806077276159059
running average episode reward sum: 0.4559600718017133
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([18.00682162, 18.21820898,  5.94140917]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 1.016866981510276}
episode index:1358
target Thresh 31.999967601798584
target distance 9.0
model initialize at round 1358
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 25.]), 'previousTarget': array([12., 25.]), 'currentState': array([20.99700838, 26.00420204,  2.39503375]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 9.052876971886292}
done in step count: 21
reward sum = 0.6709241683823075
running average episode reward sum: 0.4561182499448925
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 25.]), 'previousTarget': array([12., 25.]), 'currentState': array([12.893717  , 25.33189049,  3.35409937]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 0.9533527007243899}
episode index:1359
target Thresh 31.99996792416607
target distance 3.0
model initialize at round 1359
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([18.73308301, 18.14995191,  2.69885716]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 2.737193514084901}
done in step count: 6
reward sum = 0.9105825821286481
running average episode reward sum: 0.45645241489502764
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([16.85676391, 18.38259584,  3.28368852]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 0.9383091072985577}
episode index:1360
target Thresh 31.999968243325952
target distance 10.0
model initialize at round 1360
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 26.]), 'previousTarget': array([ 9., 26.]), 'currentState': array([19.        , 16.        ,  4.84324217]), 'targetState': array([ 9, 26], dtype=int32), 'currentDistance': 14.142135623730951}
done in step count: 37
reward sum = 0.46946573617808174
running average episode reward sum: 0.4564619764830387
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 26.]), 'previousTarget': array([ 9., 26.]), 'currentState': array([ 9.74259229, 25.07788379,  2.13762566]), 'targetState': array([ 9, 26], dtype=int32), 'currentDistance': 1.1839516967256989}
episode index:1361
target Thresh 31.99996855931014
target distance 20.0
model initialize at round 1361
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9.83743744, 12.83354859]), 'previousTarget': array([ 9.85786438, 12.85786438]), 'currentState': array([23.98428454, 26.97097117,  4.11546168]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 87
reward sum = 0.18735546040002274
running average episode reward sum: 0.45626439460632573
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.74490019, 7.85954158, 3.96424525]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.137404066265499}
episode index:1362
target Thresh 31.99996887215023
target distance 10.0
model initialize at round 1362
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([18.       , 20.       ,  0.2451261]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 14.142135623730951}
done in step count: 36
reward sum = 0.4571537072837781
running average episode reward sum: 0.4562650470734405
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 8.34730407, 10.82998622,  4.10640857]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.8997206415250867}
episode index:1363
target Thresh 31.99996918187751
target distance 12.0
model initialize at round 1363
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([ 7.        , 15.        ,  1.43574008]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 13.0}
done in step count: 38
reward sum = 0.499045089214013
running average episode reward sum: 0.45629641074069904
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([18.29731575, 10.75412017,  5.45659592]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 1.03075816376234}
episode index:1364
target Thresh 31.999969488522954
target distance 5.0
model initialize at round 1364
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  9.]), 'previousTarget': array([21.,  9.]), 'currentState': array([15.99001954, 14.01353214,  2.07677948]), 'targetState': array([21,  9], dtype=int32), 'currentDistance': 7.087694176029491}
done in step count: 21
reward sum = 0.6821785047622693
running average episode reward sum: 0.45646189212826066
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  9.]), 'previousTarget': array([21.,  9.]), 'currentState': array([20.32785356,  9.93197415,  5.41432657]), 'targetState': array([21,  9], dtype=int32), 'currentDistance': 1.1490677302514642}
episode index:1365
target Thresh 31.999969792117223
target distance 21.0
model initialize at round 1365
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8.25775784, 4.35322867]), 'previousTarget': array([8.25775784, 4.35322867]), 'currentState': array([12.        , 24.        ,  0.39725175]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 20.0}
done in step count: 87
reward sum = 0.2372942910731798
running average episode reward sum: 0.45630144732514566
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([8.09133773, 3.95304369, 4.7410269 ]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 0.9574104917295129}
episode index:1366
target Thresh 31.999970092690678
target distance 21.0
model initialize at round 1366
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3.35322867, 4.25775784]), 'previousTarget': array([3.35322867, 4.25775784]), 'currentState': array([23.        ,  8.        ,  4.98446846]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.2841565039653894
running average episode reward sum: 0.4561755183248825
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.68534177, 4.33567275, 3.3898224 ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.7631314073099088}
episode index:1367
target Thresh 31.99997039027338
target distance 16.0
model initialize at round 1367
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([ 5.98797387, 10.88875627,  4.8572011 ]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 16.311493438241797}
done in step count: 40
reward sum = 0.42256948787647064
running average episode reward sum: 0.4561509525131512
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([21.27920522, 13.85458573,  0.22901627]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 0.7353165486611987}
episode index:1368
target Thresh 31.99997068489508
target distance 16.0
model initialize at round 1368
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3.44241851, 27.65095633]), 'previousTarget': array([ 3.40925592, 27.67882258]), 'currentState': array([18.04944103, 13.98951816,  0.04358643]), 'targetState': array([ 2, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.07369443602088707
running average episode reward sum: 0.45576392155001455
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 29.]), 'previousTarget': array([ 2., 29.]), 'currentState': array([17.65792303, 17.18788582,  1.36480146]), 'targetState': array([ 2, 29], dtype=int32), 'currentDistance': 19.613683871982925}
episode index:1369
target Thresh 31.99997097658525
target distance 10.0
model initialize at round 1369
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([15.        , 23.        ,  2.48508584]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 10.04987562112089}
done in step count: 29
reward sum = 0.5846882552718865
running average episode reward sum: 0.45585802690309624
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([14.20665491, 13.86689566,  4.62464489]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 0.8911870408288777}
episode index:1370
target Thresh 31.99997126537305
target distance 14.0
model initialize at round 1370
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([ 7.12001857, 23.96039935,  6.05497986]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 14.02903615255086}
done in step count: 30
reward sum = 0.5307305882185512
running average episode reward sum: 0.4559126385451936
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([20.00859859, 25.85084381,  6.24762475]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 1.0025588869245492}
episode index:1371
target Thresh 31.999971551287366
target distance 11.0
model initialize at round 1371
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 24.]), 'previousTarget': array([17., 24.]), 'currentState': array([ 5.97337063, 22.0296201 ,  2.05057549]), 'targetState': array([17, 24], dtype=int32), 'currentDistance': 11.201292436405575}
done in step count: 29
reward sum = 0.5668215539854592
running average episode reward sum: 0.45599347594711803
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 24.]), 'previousTarget': array([17., 24.]), 'currentState': array([16.11461705, 23.61295693,  6.22278377]), 'targetState': array([17, 24], dtype=int32), 'currentDistance': 0.966284277178009}
episode index:1372
target Thresh 31.999971834356785
target distance 13.0
model initialize at round 1372
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 16.]), 'previousTarget': array([ 4., 16.]), 'currentState': array([16.97903827,  3.02137606,  2.31883445]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 18.354838971921335}
done in step count: 67
reward sum = 0.34057446871853886
running average episode reward sum: 0.4559094125769588
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 16.]), 'previousTarget': array([ 4., 16.]), 'currentState': array([ 4.95136734, 15.48296645,  2.30924089]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 1.0827850732464352}
episode index:1373
target Thresh 31.99997211460962
target distance 6.0
model initialize at round 1373
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 28.]), 'previousTarget': array([ 7., 28.]), 'currentState': array([13.        , 22.        ,  4.73804873]), 'targetState': array([ 7, 28], dtype=int32), 'currentDistance': 8.485281374238571}
done in step count: 23
reward sum = 0.6433749902165136
running average episode reward sum: 0.45604585040639084
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 28.]), 'previousTarget': array([ 7., 28.]), 'currentState': array([ 7.51376981, 27.14706655,  2.17752377]), 'targetState': array([ 7, 28], dtype=int32), 'currentDistance': 0.9957182758917457}
episode index:1374
target Thresh 31.99997239207389
target distance 22.0
model initialize at round 1374
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.33524419, 13.52085402]), 'previousTarget': array([13.33524419, 13.52085402]), 'currentState': array([26.       , 29.       ,  5.9037296]), 'targetState': array([8, 7], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.04980487696499598
running average episode reward sum: 0.4556779589683025
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.29249354, 11.08700458]), 'previousTarget': array([11.31233746, 11.11497167]), 'currentState': array([23.83951021, 26.66173966,  4.33128795]), 'targetState': array([8, 7], dtype=int32), 'currentDistance': 20.0}
episode index:1375
target Thresh 31.999972666777342
target distance 7.0
model initialize at round 1375
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  5.]), 'previousTarget': array([27.,  5.]), 'currentState': array([20.25165073,  8.99276175,  6.19500941]), 'targetState': array([27,  5], dtype=int32), 'currentDistance': 7.841069079069981}
done in step count: 19
reward sum = 0.7234883576521167
running average episode reward sum: 0.4558725886185088
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  5.]), 'previousTarget': array([27.,  5.]), 'currentState': array([26.12284467,  5.47847928,  5.81479156]), 'targetState': array([27,  5], dtype=int32), 'currentDistance': 0.9991716053739331}
episode index:1376
target Thresh 31.99997293874745
target distance 15.0
model initialize at round 1376
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([17.        ,  2.        ,  3.90614036]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 16.15549442140351}
done in step count: 45
reward sum = 0.42238093098053475
running average episode reward sum: 0.4558482664270506
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.50381261, 7.05048839, 2.26849404]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.0748950834330229}
episode index:1377
target Thresh 31.999973208011415
target distance 13.0
model initialize at round 1377
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 26.]), 'previousTarget': array([16., 26.]), 'currentState': array([ 4.23785701, 13.14230164,  0.67958629]), 'targetState': array([16, 26], dtype=int32), 'currentDistance': 17.426084321691718}
done in step count: 40
reward sum = 0.4210961186328266
running average episode reward sum: 0.4558230471615976
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 26.]), 'previousTarget': array([16., 26.]), 'currentState': array([15.8904048 , 25.11572526,  0.97633469]), 'targetState': array([16, 26], dtype=int32), 'currentDistance': 0.8910403579202409}
episode index:1378
target Thresh 31.999973474596157
target distance 10.0
model initialize at round 1378
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 6.]), 'previousTarget': array([5., 6.]), 'currentState': array([14.92547249, 16.01691605,  3.17089701]), 'targetState': array([5, 6], dtype=int32), 'currentDistance': 14.101546414349961}
done in step count: 36
reward sum = 0.5149887045466611
running average episode reward sum: 0.4558659519167717
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 6.]), 'previousTarget': array([5., 6.]), 'currentState': array([5.59472842, 6.86332186, 3.97529104]), 'targetState': array([5, 6], dtype=int32), 'currentDistance': 1.0483446602921422}
episode index:1379
target Thresh 31.999973738528332
target distance 14.0
model initialize at round 1379
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([ 2.        , 10.        ,  4.15581876]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 14.560219778561036}
done in step count: 39
reward sum = 0.463082281493686
running average episode reward sum: 0.45587118114110287
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.10762559,  6.45039622,  5.91478468]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.9995943379820598}
episode index:1380
target Thresh 31.999973999834342
target distance 14.0
model initialize at round 1380
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([21.95077343, 20.93433149,  4.01857895]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 17.725227350498486}
done in step count: 49
reward sum = 0.4137698820250576
running average episode reward sum: 0.45584069504471175
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 8.83581694, 10.28870214,  3.45060555]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.8842730820908767}
episode index:1381
target Thresh 31.999974258540313
target distance 11.0
model initialize at round 1381
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 13.]), 'previousTarget': array([ 8., 13.]), 'currentState': array([11.01394259, 23.78215432,  4.73898212]), 'targetState': array([ 8, 13], dtype=int32), 'currentDistance': 11.195476842268766}
done in step count: 27
reward sum = 0.6203130410329447
running average episode reward sum: 0.4559597054253111
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 13.]), 'previousTarget': array([ 8., 13.]), 'currentState': array([ 8.33979105, 13.99624784,  4.22016223]), 'targetState': array([ 8, 13], dtype=int32), 'currentDistance': 1.0526004508252298}
episode index:1382
target Thresh 31.999974514672118
target distance 5.0
model initialize at round 1382
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 12.]), 'previousTarget': array([14., 12.]), 'currentState': array([19.        ,  8.        ,  1.23492119]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 6.403124237432849}
done in step count: 17
reward sum = 0.7491449452784676
running average episode reward sum: 0.4561716976450169
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 12.]), 'previousTarget': array([14., 12.]), 'currentState': array([14.85427122, 11.42983155,  2.32701269]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 1.027069319561876}
episode index:1383
target Thresh 31.999974768255367
target distance 4.0
model initialize at round 1383
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 14.]), 'previousTarget': array([ 6., 14.]), 'currentState': array([ 6.        , 10.        ,  2.97218561]), 'targetState': array([ 6, 14], dtype=int32), 'currentDistance': 3.9999999999999996}
done in step count: 10
reward sum = 0.8298635667759093
running average episode reward sum: 0.4564417062209785
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 14.]), 'previousTarget': array([ 6., 14.]), 'currentState': array([ 5.90593538, 13.05597366,  1.24795312]), 'targetState': array([ 6, 14], dtype=int32), 'currentDistance': 0.9487011541438782}
episode index:1384
target Thresh 31.99997501931542
target distance 3.0
model initialize at round 1384
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([3.96768912, 7.97349601, 3.75888164]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 3.6455471364394634}
done in step count: 14
reward sum = 0.7823214283361251
running average episode reward sum: 0.4566769984391122
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.12695861, 11.05222582,  0.58114375]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.8746020819590832}
episode index:1385
target Thresh 31.999975267877385
target distance 20.0
model initialize at round 1385
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.00120956,  5.02407155]), 'previousTarget': array([15.00124766,  5.02495322]), 'currentState': array([16.00491329, 24.99887014,  5.80465538]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.3235132869267409
running average episode reward sum: 0.4565809207251783
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.30787896,  5.88151966,  4.68662973]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.933737845584211}
episode index:1386
target Thresh 31.999975513966117
target distance 15.0
model initialize at round 1386
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([25.02313359, 17.75126651,  4.85028203]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 14.883140273589483}
done in step count: 35
reward sum = 0.4981630481182866
running average episode reward sum: 0.45661090062957127
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([26.68974232,  3.79982111,  4.80865572]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 0.8578890571269473}
episode index:1387
target Thresh 31.999975757606226
target distance 12.0
model initialize at round 1387
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 26.]), 'previousTarget': array([16., 26.]), 'currentState': array([12.12832809, 14.13095268,  0.88346966]), 'targetState': array([16, 26], dtype=int32), 'currentDistance': 12.484555567625302}
done in step count: 30
reward sum = 0.5781880756129819
running average episode reward sum: 0.45669849225419906
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 26.]), 'previousTarget': array([16., 26.]), 'currentState': array([15.69159238, 25.09117701,  1.37006296]), 'targetState': array([16, 26], dtype=int32), 'currentDistance': 0.9597262531202115}
episode index:1388
target Thresh 31.999975998822073
target distance 18.0
model initialize at round 1388
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.63023523,  4.91341687]), 'previousTarget': array([24.64100589,  4.90599608]), 'currentState': array([ 7.9905771 , 16.00944216,  2.14182246]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.265004429487186
running average episode reward sum: 0.4565604835697016
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  4.]), 'previousTarget': array([26.,  4.]), 'currentState': array([25.08239523,  4.93751685,  5.45844261]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 1.3118446397694334}
episode index:1389
target Thresh 31.99997623763778
target distance 11.0
model initialize at round 1389
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 13.]), 'previousTarget': array([18., 13.]), 'currentState': array([ 6.98798667, 18.02877653,  1.74006161]), 'targetState': array([18, 13], dtype=int32), 'currentDistance': 12.10590892721174}
done in step count: 32
reward sum = 0.5349926541327873
running average episode reward sum: 0.4566169095916894
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 13.]), 'previousTarget': array([18., 13.]), 'currentState': array([17.20520572, 13.49019761,  5.94882406]), 'targetState': array([18, 13], dtype=int32), 'currentDistance': 0.9338049274512016}
episode index:1390
target Thresh 31.999976474077236
target distance 8.0
model initialize at round 1390
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 13.]), 'previousTarget': array([21., 13.]), 'currentState': array([13.07308864, 12.93763042,  5.76399663]), 'targetState': array([21, 13], dtype=int32), 'currentDistance': 7.927156721407714}
done in step count: 19
reward sum = 0.7088397237457866
running average episode reward sum: 0.4567982344041654
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 13.]), 'previousTarget': array([21., 13.]), 'currentState': array([20.17454145, 12.83828182,  6.20941177]), 'targetState': array([21, 13], dtype=int32), 'currentDistance': 0.8411507505960258}
episode index:1391
target Thresh 31.99997670816408
target distance 13.0
model initialize at round 1391
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  2.]), 'previousTarget': array([27.,  2.]), 'currentState': array([14.17669392,  3.21304045,  0.63607857]), 'targetState': array([27,  2], dtype=int32), 'currentDistance': 12.880553014706205}
done in step count: 31
reward sum = 0.5455632445393108
running average episode reward sum: 0.45686200237121644
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  2.]), 'previousTarget': array([27.,  2.]), 'currentState': array([26.01432116,  2.02240565,  0.17741094]), 'targetState': array([27,  2], dtype=int32), 'currentDistance': 0.9859334604892022}
episode index:1392
target Thresh 31.99997693992172
target distance 15.0
model initialize at round 1392
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 13.]), 'previousTarget': array([ 6., 13.]), 'currentState': array([ 3.        , 28.        ,  3.71841812]), 'targetState': array([ 6, 13], dtype=int32), 'currentDistance': 15.297058540778355}
done in step count: 37
reward sum = 0.45727223242368836
running average episode reward sum: 0.4568622968651521
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 13.]), 'previousTarget': array([ 6., 13.]), 'currentState': array([ 5.83694486, 13.92330899,  4.84978496]), 'targetState': array([ 6, 13], dtype=int32), 'currentDistance': 0.9375961113410435}
episode index:1393
target Thresh 31.999977169373334
target distance 14.0
model initialize at round 1393
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([19.        ,  6.        ,  5.81437111]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 16.1245154965971}
done in step count: 41
reward sum = 0.43338129080180693
running average episode reward sum: 0.45684545252794745
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([26.58741881, 19.19038359,  1.12950381]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 0.9086814445066199}
episode index:1394
target Thresh 31.999977396541865
target distance 3.0
model initialize at round 1394
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 14.]), 'previousTarget': array([13., 14.]), 'currentState': array([15.75377314, 14.69940341,  3.91035622]), 'targetState': array([13, 14], dtype=int32), 'currentDistance': 2.841202503109313}
done in step count: 5
reward sum = 0.9040341077110131
running average episode reward sum: 0.4571660178721647
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 14.]), 'previousTarget': array([13., 14.]), 'currentState': array([13.876949  , 14.16846421,  3.11757379]), 'targetState': array([13, 14], dtype=int32), 'currentDistance': 0.8929836179522965}
episode index:1395
target Thresh 31.99997762145003
target distance 20.0
model initialize at round 1395
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9.53956192, 15.49042639]), 'previousTarget': array([ 9.61536159, 15.46924689]), 'currentState': array([25.85486689,  3.92276354,  3.37814903]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 77
reward sum = 0.23722579212670192
running average episode reward sum: 0.4570084675671894
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([ 6.84306905, 17.42741459,  2.57180825]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 1.0191268229252803}
episode index:1396
target Thresh 31.999977844120323
target distance 19.0
model initialize at round 1396
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  7.]), 'previousTarget': array([20.,  7.]), 'currentState': array([18.        , 26.        ,  0.18677333]), 'targetState': array([20,  7], dtype=int32), 'currentDistance': 19.104973174542803}
done in step count: 57
reward sum = 0.33929729130332
running average episode reward sum: 0.45692420759849656
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  7.]), 'previousTarget': array([20.,  7.]), 'currentState': array([20.21139503,  7.9507265 ,  4.94395789]), 'targetState': array([20,  7], dtype=int32), 'currentDistance': 0.9739449330070372}
episode index:1397
target Thresh 31.99997806457501
target distance 18.0
model initialize at round 1397
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.42900019,  8.93436333]), 'previousTarget': array([24.42900019,  8.93436333]), 'currentState': array([14.        , 26.        ,  1.59057209]), 'targetState': array([25,  8], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 65
reward sum = 0.28352423355007916
running average episode reward sum: 0.456800173282296
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  8.]), 'previousTarget': array([25.,  8.]), 'currentState': array([24.40046779,  8.83579415,  5.43132003]), 'targetState': array([25,  8], dtype=int32), 'currentDistance': 1.028586759833751}
episode index:1398
target Thresh 31.999978282836135
target distance 15.0
model initialize at round 1398
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 19.]), 'previousTarget': array([19., 19.]), 'currentState': array([ 4.04517767, 14.22665036,  1.13175035]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 15.698139312046086}
done in step count: 39
reward sum = 0.46833883147747574
running average episode reward sum: 0.45680842107228536
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 19.]), 'previousTarget': array([19., 19.]), 'currentState': array([18.11465214, 18.43149056,  0.31576276]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 1.0521614947009124}
episode index:1399
target Thresh 31.999978498925525
target distance 18.0
model initialize at round 1399
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.10157472,  8.34657481]), 'previousTarget': array([17.09400392,  8.35899411]), 'currentState': array([ 6.00150938, 24.98353816,  4.91790928]), 'targetState': array([18,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.325632417106571
running average episode reward sum: 0.4567147239265956
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  7.]), 'previousTarget': array([18.,  7.]), 'currentState': array([17.6200718 ,  7.88939152,  5.23570585]), 'targetState': array([18,  7], dtype=int32), 'currentDistance': 0.9671415221602151}
episode index:1400
target Thresh 31.99997871286479
target distance 16.0
model initialize at round 1400
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([11.9703962 , 23.03863529,  2.47710848]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 16.159216666540512}
done in step count: 45
reward sum = 0.41193529403875967
running average episode reward sum: 0.4566827614498734
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([9.84614045, 7.87477851, 4.5919626 ]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 0.8882061750249789}
episode index:1401
target Thresh 31.999978924675325
target distance 3.0
model initialize at round 1401
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([19.0553831 ,  9.03866309,  0.82899866]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 3.6307311054624574}
done in step count: 11
reward sum = 0.8273840936491507
running average episode reward sum: 0.4569471703886746
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.89641788, 10.80039695,  2.85499031]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.9183716023759565}
episode index:1402
target Thresh 31.99997913437831
target distance 9.0
model initialize at round 1402
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 12.]), 'previousTarget': array([15., 12.]), 'currentState': array([ 6.2954689 , 13.83013559,  5.87220781]), 'targetState': array([15, 12], dtype=int32), 'currentDistance': 8.894844456663684}
done in step count: 19
reward sum = 0.6921426337123429
running average episode reward sum: 0.4571148079248996
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 12.]), 'previousTarget': array([15., 12.]), 'currentState': array([14.11914405, 11.99774291,  6.15802477]), 'targetState': array([15, 12], dtype=int32), 'currentDistance': 0.8808588446332248}
episode index:1403
target Thresh 31.999979341994713
target distance 7.0
model initialize at round 1403
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 18.]), 'previousTarget': array([18., 18.]), 'currentState': array([17.93919411, 24.55596205,  4.61664494]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 6.5562440258979}
done in step count: 13
reward sum = 0.7686671965489916
running average episode reward sum: 0.45733671133560055
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 18.]), 'previousTarget': array([18., 18.]), 'currentState': array([17.80676824, 18.69354403,  4.83311977]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 0.7199596081964547}
episode index:1404
target Thresh 31.9999795475453
target distance 14.0
model initialize at round 1404
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([ 2.92750962, 11.09172302,  1.98710585]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 17.172513822009062}
done in step count: 41
reward sum = 0.42559499101184217
running average episode reward sum: 0.45731411936383987
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([12.31707626, 24.11309538,  0.95797347]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 1.1193679601806077}
episode index:1405
target Thresh 31.999979751050624
target distance 9.0
model initialize at round 1405
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 16.]), 'previousTarget': array([25., 16.]), 'currentState': array([16.31470653, 24.90872919,  5.86583459]), 'targetState': array([25, 16], dtype=int32), 'currentDistance': 12.441855909914604}
done in step count: 28
reward sum = 0.5800379662752999
running average episode reward sum: 0.45740140517245403
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 16.]), 'previousTarget': array([25., 16.]), 'currentState': array([24.10494869, 16.67414974,  5.33749094]), 'targetState': array([25, 16], dtype=int32), 'currentDistance': 1.1205332309567255}
episode index:1406
target Thresh 31.99997995253104
target distance 5.0
model initialize at round 1406
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 13.]), 'previousTarget': array([ 6., 13.]), 'currentState': array([10.02017833, 18.01982582,  0.5550023 ]), 'targetState': array([ 6, 13], dtype=int32), 'currentDistance': 6.431211783664776}
done in step count: 18
reward sum = 0.6999418704744236
running average episode reward sum: 0.45757378645554
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 13.]), 'previousTarget': array([ 6., 13.]), 'currentState': array([ 6.81467778, 13.60597673,  4.01354958]), 'targetState': array([ 6, 13], dtype=int32), 'currentDistance': 1.0153362356260824}
episode index:1407
target Thresh 31.99998015200669
target distance 3.0
model initialize at round 1407
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([15.94875474, 23.9854957 ,  3.66674519]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 3.1651700078588836}
done in step count: 9
reward sum = 0.8683638120791933
running average episode reward sum: 0.4578655407351022
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([16.10558707, 21.9508538 ,  5.14466191]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 1.3054108267356606}
episode index:1408
target Thresh 31.99998034949752
target distance 18.0
model initialize at round 1408
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.85786438, 22.14213562]), 'previousTarget': array([10.85786438, 22.14213562]), 'currentState': array([25.        ,  8.        ,  0.65337172]), 'targetState': array([ 7, 26], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 89
reward sum = 0.20204880815107798
running average episode reward sum: 0.45768398166300567
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 26.]), 'previousTarget': array([ 7., 26.]), 'currentState': array([ 7.98108432, 25.28728242,  2.35530946]), 'targetState': array([ 7, 26], dtype=int32), 'currentDistance': 1.212638769356172}
episode index:1409
target Thresh 31.999980545023288
target distance 6.0
model initialize at round 1409
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([26.        ,  3.        ,  5.63891029]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 6.082762530298219}
done in step count: 15
reward sum = 0.7357057495335827
running average episode reward sum: 0.45788116022177916
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([26.85412578,  8.03155676,  1.46225441]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 0.9793679609072088}
episode index:1410
target Thresh 31.99998073860354
target distance 8.0
model initialize at round 1410
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  8.]), 'previousTarget': array([25.,  8.]), 'currentState': array([16.99077496, 15.99211548,  4.08395633]), 'targetState': array([25,  8], dtype=int32), 'currentDistance': 11.314662861557625}
done in step count: 24
reward sum = 0.5948536304901553
running average episode reward sum: 0.4579782349703748
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  8.]), 'previousTarget': array([25.,  8.]), 'currentState': array([24.01639305,  8.6660362 ,  5.358167  ]), 'targetState': array([25,  8], dtype=int32), 'currentDistance': 1.187891771242797}
episode index:1411
target Thresh 31.99998093025764
target distance 12.0
model initialize at round 1411
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 17.]), 'previousTarget': array([18., 17.]), 'currentState': array([21.84618431, 28.60100183,  4.34376802]), 'targetState': array([18, 17], dtype=int32), 'currentDistance': 12.221962902574242}
done in step count: 26
reward sum = 0.5739281912328115
running average episode reward sum: 0.45806035250313853
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 17.]), 'previousTarget': array([18., 17.]), 'currentState': array([17.89442216, 17.8126863 ,  4.63300641]), 'targetState': array([18, 17], dtype=int32), 'currentDistance': 0.8195155261050876}
episode index:1412
target Thresh 31.999981120004744
target distance 7.0
model initialize at round 1412
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 13.]), 'previousTarget': array([13., 13.]), 'currentState': array([19.03527805, 19.95764616,  5.17500141]), 'targetState': array([13, 13], dtype=int32), 'currentDistance': 9.21050602475384}
done in step count: 20
reward sum = 0.6637247925329001
running average episode reward sum: 0.45820590412382484
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 13.]), 'previousTarget': array([13., 13.]), 'currentState': array([13.93955795, 13.5553123 ,  3.95448129]), 'targetState': array([13, 13], dtype=int32), 'currentDistance': 1.091394015306796}
episode index:1413
target Thresh 31.999981307863834
target distance 14.0
model initialize at round 1413
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 24.]), 'previousTarget': array([ 2., 24.]), 'currentState': array([15.59553308, 27.01128061,  3.26626384]), 'targetState': array([ 2, 24], dtype=int32), 'currentDistance': 13.925025340182998}
done in step count: 32
reward sum = 0.5174070826573544
running average episode reward sum: 0.45824777200114697
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 24.]), 'previousTarget': array([ 2., 24.]), 'currentState': array([ 2.91457493, 23.83013434,  2.86296359]), 'targetState': array([ 2, 24], dtype=int32), 'currentDistance': 0.9302159137773667}
episode index:1414
target Thresh 31.9999814938537
target distance 4.0
model initialize at round 1414
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 29.]), 'previousTarget': array([11., 29.]), 'currentState': array([15.02453785, 28.95802854,  4.98893166]), 'targetState': array([11, 29], dtype=int32), 'currentDistance': 4.024756703079006}
done in step count: 11
reward sum = 0.809353315498346
running average episode reward sum: 0.4584959031272934
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 29.]), 'previousTarget': array([11., 29.]), 'currentState': array([11.84716198, 28.61568026,  2.81386405]), 'targetState': array([11, 29], dtype=int32), 'currentDistance': 0.9302607564577934}
episode index:1415
target Thresh 31.99998167799293
target distance 10.0
model initialize at round 1415
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([11.0123315 , 14.88749122,  5.02539152]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 11.119401622983155}
done in step count: 28
reward sum = 0.6031766709716818
running average episode reward sum: 0.45859807881079934
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([20.11431596, 10.65023837,  5.79464695]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 1.0987475421367563}
episode index:1416
target Thresh 31.999981860299947
target distance 15.0
model initialize at round 1416
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 6.]), 'previousTarget': array([5., 6.]), 'currentState': array([17.98354885, 20.8903696 ,  4.3908143 ]), 'targetState': array([5, 6], dtype=int32), 'currentDistance': 19.75590159002503}
done in step count: 53
reward sum = 0.36264358493257454
running average episode reward sum: 0.45853036216021487
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 6.]), 'previousTarget': array([5., 6.]), 'currentState': array([5.62675923, 6.8773524 , 4.09612934]), 'targetState': array([5, 6], dtype=int32), 'currentDistance': 1.0782274183315987}
episode index:1417
target Thresh 31.99998204079298
target distance 13.0
model initialize at round 1417
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 16.]), 'previousTarget': array([21., 16.]), 'currentState': array([24.96705086,  2.98387246,  3.35569656]), 'targetState': array([21, 16], dtype=int32), 'currentDistance': 13.607243242332755}
done in step count: 32
reward sum = 0.5084888501993966
running average episode reward sum: 0.4585655938160958
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 16.]), 'previousTarget': array([21., 16.]), 'currentState': array([21.42908057, 15.08796338,  1.83174635]), 'targetState': array([21, 16], dtype=int32), 'currentDistance': 1.007929030641373}
episode index:1418
target Thresh 31.999982219490075
target distance 13.0
model initialize at round 1418
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 27.]), 'previousTarget': array([ 2., 27.]), 'currentState': array([ 6.05037726, 14.164664  ,  1.37280865]), 'targetState': array([ 2, 27], dtype=int32), 'currentDistance': 13.459249840574566}
done in step count: 28
reward sum = 0.5464346599247925
running average episode reward sum: 0.4586275170480258
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 27.]), 'previousTarget': array([ 2., 27.]), 'currentState': array([ 2.52432401, 26.19122328,  1.85209808]), 'targetState': array([ 2, 27], dtype=int32), 'currentDistance': 0.9638648448948955}
episode index:1419
target Thresh 31.999982396409102
target distance 5.0
model initialize at round 1419
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 28.]), 'previousTarget': array([16., 28.]), 'currentState': array([13.        , 23.        ,  2.32004565]), 'targetState': array([16, 28], dtype=int32), 'currentDistance': 5.830951894845301}
done in step count: 12
reward sum = 0.7851115135744309
running average episode reward sum: 0.4588574353554387
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 28.]), 'previousTarget': array([16., 28.]), 'currentState': array([15.12366624, 27.04884482,  0.91321435]), 'targetState': array([16, 28], dtype=int32), 'currentDistance': 1.2933124262631643}
episode index:1420
target Thresh 31.99998257156776
target distance 17.0
model initialize at round 1420
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 28.]), 'previousTarget': array([23., 28.]), 'currentState': array([25.        , 11.        ,  4.43433219]), 'targetState': array([23, 28], dtype=int32), 'currentDistance': 17.117242768623687}
done in step count: 44
reward sum = 0.3746296299994353
running average episode reward sum: 0.4587981617415358
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 28.]), 'previousTarget': array([23., 28.]), 'currentState': array([23.17908976, 27.01876173,  1.5779329 ]), 'targetState': array([23, 28], dtype=int32), 'currentDistance': 0.9974475862332494}
episode index:1421
target Thresh 31.99998274498356
target distance 18.0
model initialize at round 1421
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 24.]), 'previousTarget': array([14., 24.]), 'currentState': array([6.99980225, 5.99047168, 4.90031505]), 'targetState': array([14, 24], dtype=int32), 'currentDistance': 19.322160308209426}
done in step count: 52
reward sum = 0.325483526768101
running average episode reward sum: 0.4587044102401481
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 24.]), 'previousTarget': array([14., 24.]), 'currentState': array([13.6086288 , 23.09505642,  1.05035975]), 'targetState': array([14, 24], dtype=int32), 'currentDistance': 0.9859484267771249}
episode index:1422
target Thresh 31.99998291667384
target distance 17.0
model initialize at round 1422
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.56079123, 21.84181548]), 'previousTarget': array([24.43860471, 21.71414506]), 'currentState': array([8.97948395, 9.30296133, 1.38591146]), 'targetState': array([26, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.29351925596346373
running average episode reward sum: 0.458588327911071
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 23.]), 'previousTarget': array([26., 23.]), 'currentState': array([25.11763649, 22.54149975,  0.53810041]), 'targetState': array([26, 23], dtype=int32), 'currentDistance': 0.9943781210371951}
episode index:1423
target Thresh 31.999983086655774
target distance 19.0
model initialize at round 1423
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 29.]), 'previousTarget': array([18., 29.]), 'currentState': array([17.        , 10.        ,  6.26626658]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 19.026297590440446}
done in step count: 54
reward sum = 0.34967407651454085
running average episode reward sum: 0.4585118431839667
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 29.]), 'previousTarget': array([18., 29.]), 'currentState': array([17.93099594, 28.10734064,  1.4206404 ]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 0.8953224562572476}
episode index:1424
target Thresh 31.99998325494636
target distance 3.0
model initialize at round 1424
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([ 9.        , 10.        ,  1.81455928]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 9
reward sum = 0.8522285435108242
running average episode reward sum: 0.4587881356052487
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.52689727, 9.94856358, 3.79577203]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 1.0850776985619786}
episode index:1425
target Thresh 31.99998342156243
target distance 6.0
model initialize at round 1425
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([19.07411389, 24.99036592,  5.90142   ]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 7.244507431748213}
done in step count: 15
reward sum = 0.7169267992482362
running average episode reward sum: 0.4589691585110292
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([15.97072583, 19.97498011,  4.08758932]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 1.3758251501197614}
episode index:1426
target Thresh 31.99998358652064
target distance 5.0
model initialize at round 1426
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  5.]), 'previousTarget': array([27.,  5.]), 'currentState': array([22.        , 10.        ,  2.46831496]), 'targetState': array([27,  5], dtype=int32), 'currentDistance': 7.0710678118654755}
done in step count: 19
reward sum = 0.6814516628457399
running average episode reward sum: 0.459125067764242
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  5.]), 'previousTarget': array([27.,  5.]), 'currentState': array([26.27332526,  5.82526805,  5.75896736]), 'targetState': array([27,  5], dtype=int32), 'currentDistance': 1.0996015347227097}
episode index:1427
target Thresh 31.999983749837487
target distance 11.0
model initialize at round 1427
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([11.06409237, 14.1939485 ,  1.44316369]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 11.93380830242255}
done in step count: 27
reward sum = 0.5777633578550804
running average episode reward sum: 0.4592081477993196
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([ 5.93223248, 24.10552968,  2.14289727]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 0.8970337771094634}
episode index:1428
target Thresh 31.999983911529306
target distance 13.0
model initialize at round 1428
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 22.]), 'previousTarget': array([ 9., 22.]), 'currentState': array([22.02094725, 26.00945021,  0.53710374]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 13.624270923286659}
done in step count: 36
reward sum = 0.4607633054249417
running average episode reward sum: 0.45920923608317243
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 22.]), 'previousTarget': array([ 9., 22.]), 'currentState': array([ 9.8125914 , 22.07038954,  3.31316011]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 0.8156343980626012}
episode index:1429
target Thresh 31.999984071612264
target distance 16.0
model initialize at round 1429
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 12.]), 'previousTarget': array([15., 12.]), 'currentState': array([27.03688469, 27.91505917,  4.86955976]), 'targetState': array([15, 12], dtype=int32), 'currentDistance': 19.954340418406098}
done in step count: 56
reward sum = 0.34548036128115023
running average episode reward sum: 0.45912970540149267
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 12.]), 'previousTarget': array([15., 12.]), 'currentState': array([15.5722528 , 12.84429791,  4.10166769]), 'targetState': array([15, 12], dtype=int32), 'currentDistance': 1.019956973339343}
episode index:1430
target Thresh 31.99998423010237
target distance 4.0
model initialize at round 1430
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 27.]), 'previousTarget': array([13., 27.]), 'currentState': array([13.16119918, 23.08419275,  0.66010317]), 'targetState': array([13, 27], dtype=int32), 'currentDistance': 3.919123833150371}
done in step count: 11
reward sum = 0.8456981884778566
running average episode reward sum: 0.45939984410385215
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 27.]), 'previousTarget': array([13., 27.]), 'currentState': array([13.15831847, 26.01998784,  1.9286879 ]), 'targetState': array([13, 27], dtype=int32), 'currentDistance': 0.9927177668869699}
episode index:1431
target Thresh 31.999984387015473
target distance 14.0
model initialize at round 1431
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([25.96202967,  9.05614066,  2.2480097 ]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 19.03895393735094}
done in step count: 55
reward sum = 0.3725546942456469
running average episode reward sum: 0.45933919804948187
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([12.31026295, 21.13352861,  2.28922993]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 0.9203454601161954}
episode index:1432
target Thresh 31.999984542367265
target distance 6.0
model initialize at round 1432
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([14.        ,  9.        ,  0.32761544]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 6.324555320336759}
done in step count: 15
reward sum = 0.71494696147424
running average episode reward sum: 0.45951757052919207
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.94426645, 10.42710484,  2.86110198]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.1044672891451162}
episode index:1433
target Thresh 31.999984696173282
target distance 6.0
model initialize at round 1433
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([10.95092991, 12.06739616,  2.25218549]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 7.2943995520517015}
done in step count: 21
reward sum = 0.665630343945342
running average episode reward sum: 0.45966130328610716
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.42766969,  6.90018559,  5.46367825]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 1.0667221172340597}
episode index:1434
target Thresh 31.9999848484489
target distance 24.0
model initialize at round 1434
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.07931069,  4.50909488]), 'previousTarget': array([21.84555753,  4.48069469]), 'currentState': array([2.23426376, 2.02431877, 0.03001368]), 'targetState': array([26,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 81
reward sum = 0.25403159737685194
running average episode reward sum: 0.45951800732380105
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  5.]), 'previousTarget': array([26.,  5.]), 'currentState': array([25.20727084,  5.15593389,  0.05181011]), 'targetState': array([26,  5], dtype=int32), 'currentDistance': 0.8079201065064733}
episode index:1435
target Thresh 31.999984999209353
target distance 17.0
model initialize at round 1435
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([26.21907179, 26.72015282,  5.13183963]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 17.51577062191936}
done in step count: 46
reward sum = 0.4027690289389358
running average episode reward sum: 0.45947848853662493
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([21.37524001, 10.85800843,  4.30659578]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 0.9364739914868205}
episode index:1436
target Thresh 31.999985148469715
target distance 13.0
model initialize at round 1436
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([ 8.97267406, 21.98595478,  3.86886835]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 14.33676657574047}
done in step count: 37
reward sum = 0.4720219373693776
running average episode reward sum: 0.4594872174502177
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([21.26210379, 16.41968606,  5.79471238]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 0.8488976437849103}
episode index:1437
target Thresh 31.999985296244912
target distance 9.0
model initialize at round 1437
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 17.]), 'previousTarget': array([15., 17.]), 'currentState': array([12.9017771 , 25.88571818,  4.17969579]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 9.130089094471614}
done in step count: 22
reward sum = 0.6754231645893384
running average episode reward sum: 0.4596373815302866
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 17.]), 'previousTarget': array([15., 17.]), 'currentState': array([14.63361999, 17.96685214,  4.92152783]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 1.0339426317589646}
episode index:1438
target Thresh 31.99998544254972
target distance 6.0
model initialize at round 1438
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  7.]), 'previousTarget': array([11.,  7.]), 'currentState': array([16.89026544,  9.78580626,  4.02600646]), 'targetState': array([11,  7], dtype=int32), 'currentDistance': 6.5158225459275565}
done in step count: 18
reward sum = 0.7534519934820981
running average episode reward sum: 0.45984156124672293
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  7.]), 'previousTarget': array([11.,  7.]), 'currentState': array([11.99786086,  7.06740086,  3.59135198]), 'targetState': array([11,  7], dtype=int32), 'currentDistance': 1.0001345796653434}
episode index:1439
target Thresh 31.99998558739877
target distance 14.0
model initialize at round 1439
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([6.        , 7.        , 1.45760512]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 14.142135623730951}
done in step count: 33
reward sum = 0.4944355332244401
running average episode reward sum: 0.45986558483837414
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([19.00722451,  5.38478133,  6.0457615 ]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 1.0647346369278365}
episode index:1440
target Thresh 31.999985730806547
target distance 12.0
model initialize at round 1440
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 23.]), 'previousTarget': array([ 6., 23.]), 'currentState': array([13.00625367, 11.10367368,  1.62525135]), 'targetState': array([ 6, 23], dtype=int32), 'currentDistance': 13.806164216123898}
done in step count: 32
reward sum = 0.5344546008179627
running average episode reward sum: 0.45991734682031693
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 23.]), 'previousTarget': array([ 6., 23.]), 'currentState': array([ 6.41892256, 22.19487656,  2.17402729]), 'targetState': array([ 6, 23], dtype=int32), 'currentDistance': 0.9075901454555053}
episode index:1441
target Thresh 31.999985872787395
target distance 12.0
model initialize at round 1441
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 14.]), 'previousTarget': array([15., 14.]), 'currentState': array([ 3.        , 22.        ,  2.23391253]), 'targetState': array([15, 14], dtype=int32), 'currentDistance': 14.422205101855956}
done in step count: 39
reward sum = 0.46340632446721575
running average episode reward sum: 0.45991976636098747
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 14.]), 'previousTarget': array([15., 14.]), 'currentState': array([14.00641215, 14.6560911 ,  5.58808921]), 'targetState': array([15, 14], dtype=int32), 'currentDistance': 1.190660467992713}
episode index:1442
target Thresh 31.999986013355507
target distance 4.0
model initialize at round 1442
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([9.97972515, 2.96794208, 4.38027233]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 4.135156799611185}
done in step count: 12
reward sum = 0.8112150810818272
running average episode reward sum: 0.4601632142575369
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.06392665,  1.95820466,  0.10665683]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.9370059628728603}
episode index:1443
target Thresh 31.999986152524947
target distance 6.0
model initialize at round 1443
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([23.        , 11.        ,  5.63000154]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 6.708203932499369}
done in step count: 16
reward sum = 0.7298069485093057
running average episode reward sum: 0.46034994814552294
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([20.23834633,  5.86282388,  4.34764624]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 0.8951391103047349}
episode index:1444
target Thresh 31.999986290309625
target distance 12.0
model initialize at round 1444
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 16.]), 'previousTarget': array([ 3., 16.]), 'currentState': array([3.89918901, 3.99842018, 2.92936015]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 12.035217452218951}
done in step count: 30
reward sum = 0.5544513956041814
running average episode reward sum: 0.46041507025449085
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 16.]), 'previousTarget': array([ 3., 16.]), 'currentState': array([ 2.8130573 , 15.1927737 ,  1.57416278]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 0.8285902901618118}
episode index:1445
target Thresh 31.999986426723325
target distance 9.0
model initialize at round 1445
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 15.]), 'previousTarget': array([ 3., 15.]), 'currentState': array([12.00468598, 20.99818682,  5.70682314]), 'targetState': array([ 3, 15], dtype=int32), 'currentDistance': 10.819547804440408}
done in step count: 27
reward sum = 0.5905471942822078
running average episode reward sum: 0.4605050648077603
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 15.]), 'previousTarget': array([ 3., 15.]), 'currentState': array([ 3.98077018, 15.64002487,  3.69728203]), 'targetState': array([ 3, 15], dtype=int32), 'currentDistance': 1.1711285038267492}
episode index:1446
target Thresh 31.999986561779682
target distance 8.0
model initialize at round 1446
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'previousTarget': array([25.,  3.]), 'currentState': array([24.03665245, 11.05917577,  0.76374662]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 8.116548067080501}
done in step count: 20
reward sum = 0.6527481677144782
running average episode reward sum: 0.46063792113319685
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'previousTarget': array([25.,  3.]), 'currentState': array([25.16649382,  3.77585333,  4.53860765]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 0.7935165951308732}
episode index:1447
target Thresh 31.999986695492208
target distance 2.0
model initialize at round 1447
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 14.]), 'previousTarget': array([11., 14.]), 'currentState': array([12.61891   , 13.30180277,  2.56451476]), 'targetState': array([11, 14], dtype=int32), 'currentDistance': 1.763051036489901}
done in step count: 2
reward sum = 0.9554686736087247
running average episode reward sum: 0.46097965507827665
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 14.]), 'previousTarget': array([11., 14.]), 'currentState': array([11.9055719 , 13.65799725,  2.78659868]), 'targetState': array([11, 14], dtype=int32), 'currentDistance': 0.9680012123342618}
episode index:1448
target Thresh 31.999986827874274
target distance 19.0
model initialize at round 1448
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.51234961, 17.74024214]), 'previousTarget': array([21.51905368, 17.75489296]), 'currentState': array([7.00669682, 3.97121759, 5.19349241]), 'targetState': array([26, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 85
reward sum = 0.18931591570228337
running average episode reward sum: 0.4607921714762228
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 22.]), 'previousTarget': array([26., 22.]), 'currentState': array([25.08883338, 21.03774321,  0.90800495]), 'targetState': array([26, 22], dtype=int32), 'currentDistance': 1.3252029032553732}
episode index:1449
target Thresh 31.999986958939115
target distance 7.0
model initialize at round 1449
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 13.]), 'previousTarget': array([ 8., 13.]), 'currentState': array([14.57293899, 11.16776626,  2.82073779]), 'targetState': array([ 8, 13], dtype=int32), 'currentDistance': 6.823533355799243}
done in step count: 15
reward sum = 0.7549416879903327
running average episode reward sum: 0.46099503321174984
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 13.]), 'previousTarget': array([ 8., 13.]), 'currentState': array([ 8.83504433, 13.00177606,  2.97004367]), 'targetState': array([ 8, 13], dtype=int32), 'currentDistance': 0.8350462229255936}
episode index:1450
target Thresh 31.99998708869984
target distance 4.0
model initialize at round 1450
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 25.]), 'previousTarget': array([27., 25.]), 'currentState': array([23.01604078, 20.99141988,  5.99084789]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 5.651605582769964}
done in step count: 13
reward sum = 0.7915826125634425
running average episode reward sum: 0.46122286751867725
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 25.]), 'previousTarget': array([27., 25.]), 'currentState': array([26.38628133, 24.11799142,  0.90753838]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 1.0745183760599366}
episode index:1451
target Thresh 31.999987217169423
target distance 12.0
model initialize at round 1451
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([13.01324852, 22.99869444,  5.93245861]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 13.421170813418144}
done in step count: 34
reward sum = 0.5041347691895599
running average episode reward sum: 0.46125242116996573
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.05808853, 11.83455298,  4.14018141]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.8365721413816753}
episode index:1452
target Thresh 31.99998734436071
target distance 9.0
model initialize at round 1452
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 28.]), 'previousTarget': array([24., 28.]), 'currentState': array([20.84466613, 19.33099995,  1.7919881 ]), 'targetState': array([24, 28], dtype=int32), 'currentDistance': 9.225383122571039}
done in step count: 19
reward sum = 0.6763462556946815
running average episode reward sum: 0.4614004554676427
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 28.]), 'previousTarget': array([24., 28.]), 'currentState': array([23.65810602, 27.0339282 ,  1.03273934]), 'targetState': array([24, 28], dtype=int32), 'currentDistance': 1.0247859355834525}
episode index:1453
target Thresh 31.999987470286424
target distance 11.0
model initialize at round 1453
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([ 7.06430031, 11.10269658,  0.75889254]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 11.367332747135313}
done in step count: 24
reward sum = 0.593570832606782
running average episode reward sum: 0.4614913566898842
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([17.01981093,  8.29411979,  5.81184941]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 1.0233655595484499}
episode index:1454
target Thresh 31.99998759495916
target distance 3.0
model initialize at round 1454
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([19.94855094, 10.10178281,  1.79571152]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 3.5507895083361234}
done in step count: 8
reward sum = 0.8694689989753238
running average episode reward sum: 0.46177175369489143
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([21.31376085, 12.17637545,  0.78489081]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 1.072045507390645}
episode index:1455
target Thresh 31.999987718391377
target distance 5.0
model initialize at round 1455
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  5.]), 'previousTarget': array([13.,  5.]), 'currentState': array([10.        , 10.        ,  0.80230272]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 5.830951894845301}
done in step count: 14
reward sum = 0.7534039898060122
running average episode reward sum: 0.46197205056035234
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  5.]), 'previousTarget': array([13.,  5.]), 'currentState': array([12.34228107,  5.78715996,  5.08510322]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 1.0257753119228732}
episode index:1456
target Thresh 31.999987840595423
target distance 16.0
model initialize at round 1456
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.1343156 , 23.01498401]), 'previousTarget': array([20.17009216, 23.05153389]), 'currentState': array([6.93147024, 7.99217222, 3.00282431]), 'targetState': array([21, 24], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.27238791898119485
running average episode reward sum: 0.4618419310465711
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 24.]), 'previousTarget': array([21., 24.]), 'currentState': array([20.33390921, 23.00539344,  0.85643161]), 'targetState': array([21, 24], dtype=int32), 'currentDistance': 1.1970460088536619}
episode index:1457
target Thresh 31.99998796158352
target distance 20.0
model initialize at round 1457
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.38262381,  5.50609905]), 'previousTarget': array([11.38262381,  5.50609905]), 'currentState': array([27.        , 18.        ,  5.95911555]), 'targetState': array([7, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 87
reward sum = 0.17569889984309559
running average episode reward sum: 0.46164567382352345
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 2.]), 'previousTarget': array([7., 2.]), 'currentState': array([7.86606348, 1.97079977, 3.71811987]), 'targetState': array([7, 2], dtype=int32), 'currentDistance': 0.8665555967340678}
episode index:1458
target Thresh 31.999988081367768
target distance 13.0
model initialize at round 1458
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([ 7.22843043, 16.19163795,  0.51934138]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 12.827041367265885}
done in step count: 30
reward sum = 0.5512697223491612
running average episode reward sum: 0.4617071022323827
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([19.02022147, 15.46899331,  5.93746919]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 1.0862415434556765}
episode index:1459
target Thresh 31.99998819996014
target distance 13.0
model initialize at round 1459
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 29.]), 'previousTarget': array([14., 29.]), 'currentState': array([26.6467762 , 28.90674541,  3.406808  ]), 'targetState': array([14, 29], dtype=int32), 'currentDistance': 12.647120013022734}
done in step count: 30
reward sum = 0.5616782250092102
running average episode reward sum: 0.46177557560414767
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 29.]), 'previousTarget': array([14., 29.]), 'currentState': array([14.76492375, 28.73841543,  3.03772136]), 'targetState': array([14, 29], dtype=int32), 'currentDistance': 0.8084150078485227}
episode index:1460
target Thresh 31.999988317372498
target distance 4.0
model initialize at round 1460
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([21.        , 22.        ,  6.04663228]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 4.123105625617661}
done in step count: 11
reward sum = 0.8210422551981987
running average episode reward sum: 0.4620214802445269
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([20.50745842, 18.8412287 ,  4.22853807]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 0.9824356330653491}
episode index:1461
target Thresh 31.999988433616583
target distance 5.0
model initialize at round 1461
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 17.]), 'previousTarget': array([ 3., 17.]), 'currentState': array([ 2.10876585, 21.96763721,  5.76399279]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 5.0469513283539955}
done in step count: 11
reward sum = 0.8087255292734672
running average episode reward sum: 0.46225862391691336
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 17.]), 'previousTarget': array([ 3., 17.]), 'currentState': array([ 3.01379948, 17.79284399,  4.75684491]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 0.7929640744306156}
episode index:1462
target Thresh 31.99998854870402
target distance 12.0
model initialize at round 1462
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 16.]), 'previousTarget': array([ 2., 16.]), 'currentState': array([11.00080815, 27.99920735,  5.2549696 ]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 14.99985081129898}
done in step count: 33
reward sum = 0.48595657980823326
running average episode reward sum: 0.46227482210959364
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 16.]), 'previousTarget': array([ 2., 16.]), 'currentState': array([ 2.62756121, 16.93379424,  3.94400238]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 1.125079888558127}
episode index:1463
target Thresh 31.99998866264632
target distance 3.0
model initialize at round 1463
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 27.]), 'previousTarget': array([13., 27.]), 'currentState': array([16.00116084, 26.10542095,  1.76041842]), 'targetState': array([13, 27], dtype=int32), 'currentDistance': 3.1316510149848464}
done in step count: 8
reward sum = 0.867955320622448
running average episode reward sum: 0.4625519262752445
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 27.]), 'previousTarget': array([13., 27.]), 'currentState': array([13.85848613, 27.5097372 ,  3.11224456]), 'targetState': array([13, 27], dtype=int32), 'currentDistance': 0.9984139675662246}
episode index:1464
target Thresh 31.999988775454874
target distance 1.0
model initialize at round 1464
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  2.]), 'previousTarget': array([12.,  2.]), 'currentState': array([12.90898154,  2.80590404,  4.18156691]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 1.2147957708707584}
done in step count: 0
reward sum = 0.9968193666307138
running average episode reward sum: 0.46291661394784206
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  2.]), 'previousTarget': array([12.,  2.]), 'currentState': array([12.90898154,  2.80590404,  4.18156691]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 1.2147957708707584}
episode index:1465
target Thresh 31.999988887140965
target distance 4.0
model initialize at round 1465
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([6.16886741, 7.28134575, 1.16118062]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 3.898030334031882}
done in step count: 9
reward sum = 0.866674105996654
running average episode reward sum: 0.4631920283353242
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.84841303, 10.0441985 ,  2.11417693]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.2780301967846546}
episode index:1466
target Thresh 31.99998899771576
target distance 11.0
model initialize at round 1466
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 29.]), 'previousTarget': array([14., 29.]), 'currentState': array([ 5.        , 18.        ,  5.09773731]), 'targetState': array([14, 29], dtype=int32), 'currentDistance': 14.212670403551895}
done in step count: 43
reward sum = 0.4527723313168892
running average episode reward sum: 0.46318492561070357
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 29.]), 'previousTarget': array([14., 29.]), 'currentState': array([14.17004514, 28.19669888,  1.03281964]), 'targetState': array([14, 29], dtype=int32), 'currentDistance': 0.8211017229455446}
episode index:1467
target Thresh 31.999989107190316
target distance 5.0
model initialize at round 1467
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 17.]), 'previousTarget': array([20., 17.]), 'currentState': array([25.04089233, 21.96732947,  5.37441018]), 'targetState': array([20, 17], dtype=int32), 'currentDistance': 7.0770726626977964}
done in step count: 16
reward sum = 0.7264390029607078
running average episode reward sum: 0.46336425400126896
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 17.]), 'previousTarget': array([20., 17.]), 'currentState': array([20.80806735, 17.64942823,  3.7716882 ]), 'targetState': array([20, 17], dtype=int32), 'currentDistance': 1.0366917901700075}
episode index:1468
target Thresh 31.999989215575585
target distance 12.0
model initialize at round 1468
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 24.]), 'previousTarget': array([22., 24.]), 'currentState': array([10.24717222, 19.88175371,  6.0095211 ]), 'targetState': array([22, 24], dtype=int32), 'currentDistance': 12.453469930709463}
done in step count: 28
reward sum = 0.5717461927898941
running average episode reward sum: 0.46343803340139733
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 24.]), 'previousTarget': array([22., 24.]), 'currentState': array([21.00129394, 23.65430447,  0.20056475]), 'targetState': array([22, 24], dtype=int32), 'currentDistance': 1.056843980914676}
episode index:1469
target Thresh 31.9999893228824
target distance 21.0
model initialize at round 1469
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9.6805654 , 5.03856732]), 'previousTarget': array([9.67544468, 5.02633404]), 'currentState': array([16.01385517, 24.00931956,  0.33962739]), 'targetState': array([9, 3], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.26031993347156995
running average episode reward sum: 0.4632998578232138
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 3.]), 'previousTarget': array([9., 3.]), 'currentState': array([9.1054127 , 3.95136511, 4.35320999]), 'targetState': array([9, 3], dtype=int32), 'currentDistance': 0.9571872390590477}
episode index:1470
target Thresh 31.999989429121495
target distance 10.0
model initialize at round 1470
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 20.]), 'previousTarget': array([15., 20.]), 'currentState': array([24.6195853 , 19.83793693,  3.49200985]), 'targetState': array([15, 20], dtype=int32), 'currentDistance': 9.620950353368817}
done in step count: 22
reward sum = 0.6639433007850462
running average episode reward sum: 0.46343625717261
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 20.]), 'previousTarget': array([15., 20.]), 'currentState': array([15.90589297, 19.66772976,  2.96576909]), 'targetState': array([15, 20], dtype=int32), 'currentDistance': 0.9649070306231771}
episode index:1471
target Thresh 31.999989534303495
target distance 4.0
model initialize at round 1471
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 6.]), 'previousTarget': array([6., 6.]), 'currentState': array([1.94425543, 5.01225946, 2.72817004]), 'targetState': array([6, 6], dtype=int32), 'currentDistance': 4.174289805876209}
done in step count: 12
reward sum = 0.7886454594431457
running average episode reward sum: 0.46365718733719596
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 6.]), 'previousTarget': array([6., 6.]), 'currentState': array([5.08790865, 6.00097473, 0.03681277]), 'targetState': array([6, 6], dtype=int32), 'currentDistance': 0.9120918734566829}
episode index:1472
target Thresh 31.999989638438915
target distance 20.0
model initialize at round 1472
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8.35538263, 4.48381247]), 'previousTarget': array([8.361625  , 4.47568183]), 'currentState': array([17.93556191, 22.04001265,  2.838413  ]), 'targetState': array([7, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.281599615307469
running average episode reward sum: 0.463533590886395
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 2.]), 'previousTarget': array([7., 2.]), 'currentState': array([7.73335938, 2.95755356, 4.23779228]), 'targetState': array([7, 2], dtype=int32), 'currentDistance': 1.2061197281622316}
episode index:1473
target Thresh 31.99998974153817
target distance 22.0
model initialize at round 1473
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.429867  , 25.49896705]), 'previousTarget': array([15.42734309, 25.51093912]), 'currentState': array([22.9917687 ,  6.98363171,  4.05033213]), 'targetState': array([14, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.23815772040518557
running average episode reward sum: 0.46338069002446747
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 29.]), 'previousTarget': array([14., 29.]), 'currentState': array([14.49831916, 28.13946225,  1.97148123]), 'targetState': array([14, 29], dtype=int32), 'currentDistance': 0.9944079687897373}
episode index:1474
target Thresh 31.99998984361157
target distance 13.0
model initialize at round 1474
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 14.]), 'previousTarget': array([ 7., 14.]), 'currentState': array([18.86196625, 26.9576411 ,  3.58784908]), 'targetState': array([ 7, 14], dtype=int32), 'currentDistance': 17.567205423030718}
done in step count: 43
reward sum = 0.4364512506673542
running average episode reward sum: 0.4633624327774457
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 14.]), 'previousTarget': array([ 7., 14.]), 'currentState': array([ 7.70880036, 14.97095786,  3.89982401]), 'targetState': array([ 7, 14], dtype=int32), 'currentDistance': 1.2021468764529961}
episode index:1475
target Thresh 31.999989944669323
target distance 12.0
model initialize at round 1475
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 8.]), 'previousTarget': array([6., 8.]), 'currentState': array([18.00353712, 12.97832239,  4.62576678]), 'targetState': array([6, 8], dtype=int32), 'currentDistance': 12.99494505910338}
done in step count: 30
reward sum = 0.5353250836670649
running average episode reward sum: 0.46341118796097525
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 8.]), 'previousTarget': array([6., 8.]), 'currentState': array([6.86813836, 8.33524322, 3.33676201]), 'targetState': array([6, 8], dtype=int32), 'currentDistance': 0.9306192752554772}
episode index:1476
target Thresh 31.999990044721535
target distance 13.0
model initialize at round 1476
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([ 7.        , 16.        ,  1.63444757]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 13.341664064126334}
done in step count: 37
reward sum = 0.4993559928062449
running average episode reward sum: 0.4634355243217371
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([19.11502671, 13.3335404 ,  5.82484719]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 0.9457414720383074}
episode index:1477
target Thresh 31.999990143778213
target distance 5.0
model initialize at round 1477
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 14.]), 'previousTarget': array([ 9., 14.]), 'currentState': array([ 4.        , 17.        ,  0.40535241]), 'targetState': array([ 9, 14], dtype=int32), 'currentDistance': 5.830951894845301}
done in step count: 13
reward sum = 0.7764826748790252
running average episode reward sum: 0.4636473288890966
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 14.]), 'previousTarget': array([ 9., 14.]), 'currentState': array([ 8.01061681, 14.04976501,  5.60798473]), 'targetState': array([ 9, 14], dtype=int32), 'currentDistance': 0.9906339644362618}
episode index:1478
target Thresh 31.999990241849257
target distance 18.0
model initialize at round 1478
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 20.]), 'previousTarget': array([22., 20.]), 'currentState': array([ 3.98058895, 13.01726497,  2.17402279]), 'targetState': array([22, 20], dtype=int32), 'currentDistance': 19.32505532112055}
done in step count: 51
reward sum = 0.3358480237954636
running average episode reward sum: 0.4635609196226371
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 20.]), 'previousTarget': array([22., 20.]), 'currentState': array([21.13786024, 19.94034207,  0.12926141]), 'targetState': array([22, 20], dtype=int32), 'currentDistance': 0.8642013843897447}
episode index:1479
target Thresh 31.999990338944478
target distance 11.0
model initialize at round 1479
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([12.84573993, 15.55601736,  4.32119998]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 14.434960936606423}
done in step count: 28
reward sum = 0.5205791284285428
running average episode reward sum: 0.46359944543939785
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.7572211 , 5.7993324 , 3.83749204]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.1010522555841509}
episode index:1480
target Thresh 31.999990435073588
target distance 10.0
model initialize at round 1480
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 2.]), 'previousTarget': array([6., 2.]), 'currentState': array([14.01034926, 12.04104724,  1.57631361]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 12.84477812146119}
done in step count: 32
reward sum = 0.5074052659879841
running average episode reward sum: 0.4636290239812943
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 2.]), 'previousTarget': array([6., 2.]), 'currentState': array([6.43639933, 2.95724885, 4.08438243]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 1.0520312424289069}
episode index:1481
target Thresh 31.999990530246194
target distance 16.0
model initialize at round 1481
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6.17238886, 24.78855083]), 'previousTarget': array([ 6.3881475, 24.52228  ]), 'currentState': array([18.81011837,  9.28733176,  2.19515891]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.36364106203777935
running average episode reward sum: 0.4635615557208736
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([ 6.32787191, 24.12692111,  2.11856491]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 0.9326128589624864}
episode index:1482
target Thresh 31.99999062447182
target distance 7.0
model initialize at round 1482
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 14.]), 'previousTarget': array([24., 14.]), 'currentState': array([16.90036457, 21.01401406,  3.25435591]), 'targetState': array([24, 14], dtype=int32), 'currentDistance': 9.980040907509172}
done in step count: 24
reward sum = 0.5898229194434692
running average episode reward sum: 0.46364669487375465
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 14.]), 'previousTarget': array([24., 14.]), 'currentState': array([23.23828209, 14.35908652,  5.74871847]), 'targetState': array([24, 14], dtype=int32), 'currentDistance': 0.8421147840290015}
episode index:1483
target Thresh 31.999990717759886
target distance 5.0
model initialize at round 1483
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 13.]), 'previousTarget': array([18., 13.]), 'currentState': array([12.91903364, 17.83010742,  4.49841011]), 'targetState': array([18, 13], dtype=int32), 'currentDistance': 7.0104320029198375}
done in step count: 14
reward sum = 0.7473102622962466
running average episode reward sum: 0.46383784283023877
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 13.]), 'previousTarget': array([18., 13.]), 'currentState': array([17.10332209, 13.6938776 ,  5.64699593]), 'targetState': array([18, 13], dtype=int32), 'currentDistance': 1.133797778520728}
episode index:1484
target Thresh 31.999990810119716
target distance 22.0
model initialize at round 1484
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.9624485 , 21.89556084]), 'previousTarget': array([14.94427191, 21.88854382]), 'currentState': array([6.06942905, 3.98148244, 6.27504063]), 'targetState': array([17, 26], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 67
reward sum = 0.22462541376232692
running average episode reward sum: 0.4636767570194186
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 26.]), 'previousTarget': array([17., 26.]), 'currentState': array([16.78386529, 25.17878162,  0.90620219]), 'targetState': array([17, 26], dtype=int32), 'currentDistance': 0.849184219038122}
episode index:1485
target Thresh 31.999990901560555
target distance 7.0
model initialize at round 1485
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([21.00131797, 17.02087065,  1.25523067]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 7.6344429098339885}
done in step count: 21
reward sum = 0.6704395088413603
running average episode reward sum: 0.4638158974984374
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([23.55979241, 10.94445601,  5.04442134]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 1.0420076189897653}
episode index:1486
target Thresh 31.99999099209154
target distance 18.0
model initialize at round 1486
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'previousTarget': array([27.,  8.]), 'currentState': array([9.        , 4.        , 4.74474764]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 18.439088914585774}
done in step count: 54
reward sum = 0.3571759397634975
running average episode reward sum: 0.4637441826647219
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'previousTarget': array([27.,  8.]), 'currentState': array([26.16261005,  7.59824841,  0.23110898]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 0.928776755843358}
episode index:1487
target Thresh 31.999991081721724
target distance 18.0
model initialize at round 1487
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([ 8.98817944, 26.00408597,  3.06128335]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 18.445644903728585}
done in step count: 49
reward sum = 0.3625334912236948
running average episode reward sum: 0.46367616472692547
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([12.57941928,  8.7545792 ,  4.93713702]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 0.8638737857074609}
episode index:1488
target Thresh 31.999991170460078
target distance 25.0
model initialize at round 1488
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.01072024, 20.86002549]), 'previousTarget': array([20.81774824, 20.77438937]), 'currentState': array([ 2.17682445, 14.13065927,  0.47957724]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 68
reward sum = 0.20757578483904898
running average episode reward sum: 0.4635041698445293
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 23.]), 'previousTarget': array([27., 23.]), 'currentState': array([26.23287951, 22.41896117,  0.46791062]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 0.962330493049774}
episode index:1489
target Thresh 31.999991258315468
target distance 4.0
model initialize at round 1489
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 22.]), 'previousTarget': array([ 8., 22.]), 'currentState': array([ 3.99983632, 20.99958512,  4.5891118 ]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 4.123365057438423}
done in step count: 11
reward sum = 0.8089711270662884
running average episode reward sum: 0.46373602686279897
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 22.]), 'previousTarget': array([ 8., 22.]), 'currentState': array([ 7.31067225, 21.17883457,  0.4448545 ]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 1.0721405686481984}
episode index:1490
target Thresh 31.99999134529668
target distance 14.0
model initialize at round 1490
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 6.]), 'previousTarget': array([6., 6.]), 'currentState': array([13.05903177, 20.02678746,  0.17561698]), 'targetState': array([6, 6], dtype=int32), 'currentDistance': 15.702888139919429}
done in step count: 42
reward sum = 0.4275444628307098
running average episode reward sum: 0.46371175351334754
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 6.]), 'previousTarget': array([6., 6.]), 'currentState': array([6.09516193, 6.86367615, 4.03347854]), 'targetState': array([6, 6], dtype=int32), 'currentDistance': 0.8689029177170429}
episode index:1491
target Thresh 31.99999143141242
target distance 7.0
model initialize at round 1491
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 21.]), 'previousTarget': array([18., 21.]), 'currentState': array([11.1298764 , 15.91445597,  5.87687892]), 'targetState': array([18, 21], dtype=int32), 'currentDistance': 8.547593600726437}
done in step count: 19
reward sum = 0.6878583255173474
running average episode reward sum: 0.4638619858002135
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 21.]), 'previousTarget': array([18., 21.]), 'currentState': array([17.62196519, 20.04641745,  0.8277505 ]), 'targetState': array([18, 21], dtype=int32), 'currentDistance': 1.0257826253239577}
episode index:1492
target Thresh 31.99999151667129
target distance 12.0
model initialize at round 1492
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 12.]), 'previousTarget': array([12., 12.]), 'currentState': array([19.0865387 , 23.77281978,  5.01517442]), 'targetState': array([12, 12], dtype=int32), 'currentDistance': 13.74111772611956}
done in step count: 33
reward sum = 0.5350576074433174
running average episode reward sum: 0.4639096720839664
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 12.]), 'previousTarget': array([12., 12.]), 'currentState': array([12.51146   , 12.93087058,  4.06707968]), 'targetState': array([12, 12], dtype=int32), 'currentDistance': 1.0621258735529406}
episode index:1493
target Thresh 31.99999160108182
target distance 11.0
model initialize at round 1493
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 25.]), 'previousTarget': array([11., 25.]), 'currentState': array([13.77502061, 14.28178396,  2.23575266]), 'targetState': array([11, 25], dtype=int32), 'currentDistance': 11.071625652537856}
done in step count: 23
reward sum = 0.6208453357717933
running average episode reward sum: 0.46401471603556466
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 25.]), 'previousTarget': array([11., 25.]), 'currentState': array([11.60283086, 24.25063409,  1.81121516]), 'targetState': array([11, 25], dtype=int32), 'currentDistance': 0.96174545280386}
episode index:1494
target Thresh 31.999991684652453
target distance 3.0
model initialize at round 1494
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([13.97502746, 11.06582086,  2.08691478]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 3.232619102113176}
done in step count: 11
reward sum = 0.8067203358479431
running average episode reward sum: 0.46424395056386725
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.01096572,  8.65441069,  5.25860557]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.1859351366403705}
episode index:1495
target Thresh 31.999991767391542
target distance 8.0
model initialize at round 1495
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 27.]), 'previousTarget': array([ 6., 27.]), 'currentState': array([13.70086365, 23.23954431,  2.52346327]), 'targetState': array([ 6, 27], dtype=int32), 'currentDistance': 8.569966624462893}
done in step count: 17
reward sum = 0.706076538340023
running average episode reward sum: 0.4644056033631828
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 27.]), 'previousTarget': array([ 6., 27.]), 'currentState': array([ 6.8252375 , 26.58341422,  2.85034756]), 'targetState': array([ 6, 27], dtype=int32), 'currentDistance': 0.9244244895447097}
episode index:1496
target Thresh 31.999991849307367
target distance 8.0
model initialize at round 1496
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 24.]), 'previousTarget': array([17., 24.]), 'currentState': array([25.        , 23.        ,  1.83669019]), 'targetState': array([17, 24], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 17
reward sum = 0.6986726998179287
running average episode reward sum: 0.4645620944095788
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 24.]), 'previousTarget': array([17., 24.]), 'currentState': array([17.93530675, 24.15168646,  2.97907525]), 'targetState': array([17, 24], dtype=int32), 'currentDistance': 0.9475270473607605}
episode index:1497
target Thresh 31.99999193040811
target distance 19.0
model initialize at round 1497
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 29.]), 'previousTarget': array([ 7.08589282, 28.76686234]), 'currentState': array([13.92308054, 10.31105387,  1.85723656]), 'targetState': array([ 7, 29], dtype=int32), 'currentDistance': 19.930021361167135}
done in step count: 58
reward sum = 0.350894296091677
running average episode reward sum: 0.4644862147044267
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 29.]), 'previousTarget': array([ 7., 29.]), 'currentState': array([ 7.44585635, 28.1028471 ,  1.6861481 ]), 'targetState': array([ 7, 29], dtype=int32), 'currentDistance': 1.0018339214909562}
episode index:1498
target Thresh 31.99999201070189
target distance 5.0
model initialize at round 1498
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 18.]), 'previousTarget': array([23., 18.]), 'currentState': array([18.        , 21.        ,  2.43974783]), 'targetState': array([23, 18], dtype=int32), 'currentDistance': 5.830951894845301}
done in step count: 20
reward sum = 0.6921432023845355
running average episode reward sum: 0.4646380872779291
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 18.]), 'previousTarget': array([23., 18.]), 'currentState': array([22.20532414, 17.98917786,  5.80471598]), 'targetState': array([23, 18], dtype=int32), 'currentDistance': 0.7947495426265156}
episode index:1499
target Thresh 31.999992090196738
target distance 20.0
model initialize at round 1499
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.07614104, 28.61784535]), 'previousTarget': array([23.0776773 , 28.61161351]), 'currentState': array([26.98415614,  9.00337574,  2.88076972]), 'targetState': array([23, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.34347940990356396
running average episode reward sum: 0.46455731482634616
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 29.]), 'previousTarget': array([23., 29.]), 'currentState': array([23.53484821, 28.00123444,  1.7243547 ]), 'targetState': array([23, 29], dtype=int32), 'currentDistance': 1.1329586330276415}
episode index:1500
target Thresh 31.999992168900594
target distance 8.0
model initialize at round 1500
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 13.]), 'previousTarget': array([ 2., 13.]), 'currentState': array([ 2.97097624, 21.05300435,  2.30290672]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 8.11132997109231}
done in step count: 20
reward sum = 0.6520707037368656
running average episode reward sum: 0.4646822404685251
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 13.]), 'previousTarget': array([ 2., 13.]), 'currentState': array([ 1.77873543, 13.83936326,  4.69674993]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 0.8680372604880531}
episode index:1501
target Thresh 31.999992246821336
target distance 8.0
model initialize at round 1501
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 25.]), 'previousTarget': array([15., 25.]), 'currentState': array([ 8.05885596, 16.99672221,  0.16768014]), 'targetState': array([15, 25], dtype=int32), 'currentDistance': 10.593957517509267}
done in step count: 27
reward sum = 0.6153932956467657
running average episode reward sum: 0.46478258071831086
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 25.]), 'previousTarget': array([15., 25.]), 'currentState': array([14.70734113, 24.10284532,  1.09038307]), 'targetState': array([15, 25], dtype=int32), 'currentDistance': 0.9436820095678121}
episode index:1502
target Thresh 31.999992323966755
target distance 22.0
model initialize at round 1502
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.77104998,  8.17429997]), 'previousTarget': array([13.77104998,  8.17429997]), 'currentState': array([26.        , 24.        ,  3.33278537]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 83
reward sum = 0.1762872462189089
running average episode reward sum: 0.4645906343879719
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 2.]), 'previousTarget': array([9., 2.]), 'currentState': array([9.39797125, 2.95106399, 3.96462118]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 1.030972274403269}
episode index:1503
target Thresh 31.99999240034456
target distance 19.0
model initialize at round 1503
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 21.]), 'previousTarget': array([23., 21.]), 'currentState': array([17.        ,  2.        ,  4.94937897]), 'targetState': array([23, 21], dtype=int32), 'currentDistance': 19.924858845171272}
done in step count: 68
reward sum = 0.27023457780575744
running average episode reward sum: 0.464461408286521
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 21.]), 'previousTarget': array([23., 21.]), 'currentState': array([22.12921956, 20.00820323,  0.91352263]), 'targetState': array([23, 21], dtype=int32), 'currentDistance': 1.3198179475784049}
episode index:1504
target Thresh 31.999992475962394
target distance 19.0
model initialize at round 1504
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.33589719, 14.09517373]), 'previousTarget': array([18.33589719, 14.09517373]), 'currentState': array([ 5.        , 29.        ,  1.58515763]), 'targetState': array([22, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 92
reward sum = 0.1731670874605925
running average episode reward sum: 0.4642678572427828
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 10.]), 'previousTarget': array([22., 10.]), 'currentState': array([21.11489844, 10.62643722,  5.49295081]), 'targetState': array([22, 10], dtype=int32), 'currentDistance': 1.0843561978294876}
episode index:1505
target Thresh 31.99999255082782
target distance 12.0
model initialize at round 1505
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 14.]), 'previousTarget': array([18., 14.]), 'currentState': array([25.00384936,  2.02073611,  1.57427034]), 'targetState': array([18, 14], dtype=int32), 'currentDistance': 13.876478989699887}
done in step count: 34
reward sum = 0.5415201125855695
running average episode reward sum: 0.4643191535610715
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 14.]), 'previousTarget': array([18., 14.]), 'currentState': array([18.83809523, 13.00387722,  2.05375368]), 'targetState': array([18, 14], dtype=int32), 'currentDistance': 1.3017926835309455}
episode index:1506
target Thresh 31.999992624948323
target distance 13.0
model initialize at round 1506
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([15.14126974, 26.94467757,  5.83858888]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 12.97312973187037}
done in step count: 47
reward sum = 0.48850446937223235
running average episode reward sum: 0.4643352022112448
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([16.17262274, 14.87897082,  4.28408517]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 0.8957613011279145}
episode index:1507
target Thresh 31.99999269833131
target distance 19.0
model initialize at round 1507
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7.68506666, 22.5553939 ]), 'previousTarget': array([ 7.70632169, 22.50614522]), 'currentState': array([19.03576046,  6.08840418,  1.42123011]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 80
reward sum = 0.26787423483109934
running average episode reward sum: 0.4642049230551572
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([ 6.89711334, 24.40789027,  2.14850054]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 1.0748982634854114}
episode index:1508
target Thresh 31.99999277098413
target distance 16.0
model initialize at round 1508
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([25.01994451, 25.93528308,  4.86533964]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 19.960268354949758}
done in step count: 56
reward sum = 0.33221296834933867
running average episode reward sum: 0.46411745323759207
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13.93335554, 10.18305054,  3.59019112]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.9511361941709415}
episode index:1509
target Thresh 31.99999284291404
target distance 13.0
model initialize at round 1509
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([17.0028573 ,  3.97248382,  4.61762875]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 15.835643893016094}
done in step count: 50
reward sum = 0.3910414378740107
running average episode reward sum: 0.46406905852543073
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([ 8.93549559, 16.63661011,  2.27494284]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 1.0035956435343438}
episode index:1510
target Thresh 31.999992914128235
target distance 24.0
model initialize at round 1510
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.81967452,  7.901753  ]), 'previousTarget': array([13.83261089,  8.01733854]), 'currentState': array([12.89632953, 27.88042748,  4.09215432]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 69
reward sum = 0.265177701083274
running average episode reward sum: 0.46393742956617057
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.03345457,  4.89593313,  4.61157156]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.8965575141468735}
episode index:1511
target Thresh 31.999992984633835
target distance 14.0
model initialize at round 1511
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([10.        , 25.        ,  4.20665932]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 19.1049731745428}
done in step count: 55
reward sum = 0.3620500126247349
running average episode reward sum: 0.4638700437084051
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([23.04372781, 12.60439904,  5.34937924]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 1.1312624362803627}
episode index:1512
target Thresh 31.999993054437894
target distance 14.0
model initialize at round 1512
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([17.        ,  8.        ,  0.06642231]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 19.1049731745428}
done in step count: 53
reward sum = 0.3497531063432972
running average episode reward sum: 0.4637946194272649
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([ 4.93661944, 21.1324364 ,  2.58916508]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 1.276684212394487}
episode index:1513
target Thresh 31.999993123547394
target distance 8.0
model initialize at round 1513
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([23.02132934, 21.0136074 ,  0.82036904]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 8.270207925564742}
done in step count: 21
reward sum = 0.6384777582547412
running average episode reward sum: 0.46390999798659616
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([15.87734854, 19.16030041,  3.30337288]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 0.8918725677623781}
episode index:1514
target Thresh 31.99999319196924
target distance 24.0
model initialize at round 1514
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.88278574, 25.94698988]), 'previousTarget': array([20.88854382, 25.94427191]), 'currentState': array([ 2.98769739, 17.01581901,  1.9906213 ]), 'targetState': array([27, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 97
reward sum = 0.16958223573237768
running average episode reward sum: 0.4637157222359333
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 29.]), 'previousTarget': array([27., 29.]), 'currentState': array([26.11247769, 28.56240676,  0.53020209]), 'targetState': array([27, 29], dtype=int32), 'currentDistance': 0.9895371058548842}
episode index:1515
target Thresh 31.999993259710276
target distance 18.0
model initialize at round 1515
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 24.]), 'previousTarget': array([ 5., 24.]), 'currentState': array([7.        , 6.        , 3.90758905]), 'targetState': array([ 5, 24], dtype=int32), 'currentDistance': 18.110770276274835}
done in step count: 53
reward sum = 0.3518990487003856
running average episode reward sum: 0.463641964535712
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 24.]), 'previousTarget': array([ 5., 24.]), 'currentState': array([ 5.39973768, 23.03902667,  1.60953288]), 'targetState': array([ 5, 24], dtype=int32), 'currentDistance': 1.0407977466374407}
episode index:1516
target Thresh 31.999993326777282
target distance 23.0
model initialize at round 1516
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.03055064, 10.38117166]), 'previousTarget': array([21.04268443, 10.37089005]), 'currentState': array([ 2.99419779, 19.0235079 ,  1.56027961]), 'targetState': array([26,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 90
reward sum = 0.1808557204568856
running average episode reward sum: 0.4634555530366488
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  8.]), 'previousTarget': array([26.,  8.]), 'currentState': array([25.11249219,  8.06496248,  5.65976299]), 'targetState': array([26,  8], dtype=int32), 'currentDistance': 0.8898821519186756}
episode index:1517
target Thresh 31.999993393176958
target distance 19.0
model initialize at round 1517
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.39845035,  2.25281174]), 'previousTarget': array([26.4327075,  2.23886  ]), 'currentState': array([ 7.96056933, 10.00165318,  3.29883945]), 'targetState': array([27,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 68
reward sum = 0.2878302800397293
running average episode reward sum: 0.4633398578633966
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  2.]), 'previousTarget': array([27.,  2.]), 'currentState': array([26.01088136,  1.99642406,  5.70382138]), 'targetState': array([27,  2], dtype=int32), 'currentDistance': 0.9891251039087696}
episode index:1518
target Thresh 31.999993458915945
target distance 16.0
model initialize at round 1518
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([12.07285563,  3.0165728 ,  0.3172864 ]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 18.81536978784897}
done in step count: 59
reward sum = 0.3711293109332633
running average episode reward sum: 0.4632791530925407
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([22.22947312, 18.10545256,  1.23793163]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 0.9235112565631448}
episode index:1519
target Thresh 31.999993524000818
target distance 6.0
model initialize at round 1519
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 28.]), 'previousTarget': array([ 2., 28.]), 'currentState': array([ 8.06260532, 22.02637493,  0.62415332]), 'targetState': array([ 2, 28], dtype=int32), 'currentDistance': 8.511132695538883}
done in step count: 22
reward sum = 0.6626729040357682
running average episode reward sum: 0.4634103331918454
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 28.]), 'previousTarget': array([ 2., 28.]), 'currentState': array([ 2.92705096, 27.44967576,  2.32732133]), 'targetState': array([ 2, 28], dtype=int32), 'currentDistance': 1.0780910217977888}
episode index:1520
target Thresh 31.999993588438087
target distance 14.0
model initialize at round 1520
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([20.        , 28.        ,  5.31444499]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 17.204650534085257}
done in step count: 55
reward sum = 0.3964990748799563
running average episode reward sum: 0.4633663415690237
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([10.77251197, 14.82805037,  4.09062452]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 1.1324496284845091}
episode index:1521
target Thresh 31.999993652234195
target distance 5.0
model initialize at round 1521
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 16.]), 'previousTarget': array([ 9., 16.]), 'currentState': array([ 8.96763122, 20.53096833,  4.66404045]), 'targetState': array([ 9, 16], dtype=int32), 'currentDistance': 4.531083950531264}
done in step count: 9
reward sum = 0.8468104451986528
running average episode reward sum: 0.46361827593408916
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 16.]), 'previousTarget': array([ 9., 16.]), 'currentState': array([ 8.63349286, 16.87533372,  4.69130153]), 'targetState': array([ 9, 16], dtype=int32), 'currentDistance': 0.9489660757846047}
episode index:1522
target Thresh 31.99999371539552
target distance 9.0
model initialize at round 1522
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([21.98312077,  5.92411657,  4.2910701 ]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 9.219856334046316}
done in step count: 23
reward sum = 0.6512606159524243
running average episode reward sum: 0.46374148167277485
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([13.83405369,  7.13328054,  2.68864661]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 1.2028500284174597}
episode index:1523
target Thresh 31.999993777928378
target distance 11.0
model initialize at round 1523
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 18.]), 'previousTarget': array([13., 18.]), 'currentState': array([ 1.99100496, 23.99100007,  4.1797626 ]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 12.533557099779916}
done in step count: 36
reward sum = 0.5278659500022128
running average episode reward sum: 0.46378355809556326
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 18.]), 'previousTarget': array([13., 18.]), 'currentState': array([12.06574567, 18.17930477,  5.80554267]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 0.9513050785502525}
episode index:1524
target Thresh 31.999993839839025
target distance 16.0
model initialize at round 1524
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([10.03915616,  6.00940938,  0.46372886]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 17.06548117493181}
done in step count: 43
reward sum = 0.4305238161590812
running average episode reward sum: 0.46376174842871964
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([15.86852027, 21.17722015,  1.39175825]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 0.8332188227533868}
episode index:1525
target Thresh 31.999993901133653
target distance 20.0
model initialize at round 1525
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  6.]), 'previousTarget': array([23.00124766,  6.02495322]), 'currentState': array([24.06604609, 25.59790656,  4.82637263]), 'targetState': array([23,  6], dtype=int32), 'currentDistance': 19.626879421079238}
done in step count: 59
reward sum = 0.36515506799567343
running average episode reward sum: 0.46369713068269536
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  6.]), 'previousTarget': array([23.,  6.]), 'currentState': array([23.1875088 ,  6.93865368,  4.86511055]), 'targetState': array([23,  6], dtype=int32), 'currentDistance': 0.9571991868086627}
episode index:1526
target Thresh 31.999993961818387
target distance 13.0
model initialize at round 1526
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 10.]), 'previousTarget': array([18., 10.]), 'currentState': array([ 4.9843062 , 10.97224509,  4.45026731]), 'targetState': array([18, 10], dtype=int32), 'currentDistance': 13.051955622119788}
done in step count: 36
reward sum = 0.5071515890812321
running average episode reward sum: 0.46372558808832637
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 10.]), 'previousTarget': array([18., 10.]), 'currentState': array([17.18045342,  9.86177518,  6.09936636]), 'targetState': array([18, 10], dtype=int32), 'currentDistance': 0.83112134945796}
episode index:1527
target Thresh 31.999994021899298
target distance 18.0
model initialize at round 1527
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 4.]), 'previousTarget': array([5., 4.]), 'currentState': array([11.89796033, 21.74927662,  4.33733034]), 'targetState': array([5, 4], dtype=int32), 'currentDistance': 19.04254912999214}
done in step count: 54
reward sum = 0.3861866586579331
running average episode reward sum: 0.4636748427156625
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 4.]), 'previousTarget': array([5., 4.]), 'currentState': array([5.06710062, 4.87922716, 4.18990779]), 'targetState': array([5, 4], dtype=int32), 'currentDistance': 0.8817839239032461}
episode index:1528
target Thresh 31.999994081382393
target distance 11.0
model initialize at round 1528
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 16.]), 'previousTarget': array([19., 16.]), 'currentState': array([18.        , 27.        ,  5.95122216]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 11.045361017187261}
done in step count: 28
reward sum = 0.5952036953547084
running average episode reward sum: 0.46376086551006346
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 16.]), 'previousTarget': array([19., 16.]), 'currentState': array([19.18454167, 16.82964388,  4.64689971]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 0.8499203485512998}
episode index:1529
target Thresh 31.99999414027362
target distance 12.0
model initialize at round 1529
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 5.]), 'previousTarget': array([8., 5.]), 'currentState': array([ 7.02122488, 17.00524024,  6.27273587]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 12.045073426055012}
done in step count: 31
reward sum = 0.5585179137918344
running average episode reward sum: 0.46382279822135875
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 5.]), 'previousTarget': array([8., 5.]), 'currentState': array([7.39324487, 5.95444506, 4.83105419]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 1.1309806168025998}
episode index:1530
target Thresh 31.99999419857887
target distance 16.0
model initialize at round 1530
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  8.]), 'previousTarget': array([12.,  8.]), 'currentState': array([ 5.87769914, 23.86098881,  4.17334199]), 'targetState': array([12,  8], dtype=int32), 'currentDistance': 17.001574450506787}
done in step count: 46
reward sum = 0.4279758594743437
running average episode reward sum: 0.4637993841529414
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  8.]), 'previousTarget': array([12.,  8.]), 'currentState': array([11.65444497,  8.87920397,  4.95868043]), 'targetState': array([12,  8], dtype=int32), 'currentDistance': 0.9446734344712515}
episode index:1531
target Thresh 31.999994256303978
target distance 12.0
model initialize at round 1531
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 20.]), 'previousTarget': array([14., 20.]), 'currentState': array([18.90896525,  8.27050723,  1.94146675]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 12.715303393747094}
done in step count: 28
reward sum = 0.5729846533115897
running average episode reward sum: 0.46387065391087784
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 20.]), 'previousTarget': array([14., 20.]), 'currentState': array([14.45785251, 19.21528868,  2.18392235]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 0.9085157018625641}
episode index:1532
target Thresh 31.999994313454707
target distance 12.0
model initialize at round 1532
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 15.]), 'previousTarget': array([13., 15.]), 'currentState': array([25.        , 17.        ,  1.30940866]), 'targetState': array([13, 15], dtype=int32), 'currentDistance': 12.16552506059644}
done in step count: 33
reward sum = 0.5235243818786185
running average episode reward sum: 0.463909566975436
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 15.]), 'previousTarget': array([13., 15.]), 'currentState': array([13.91480416, 14.93099033,  3.36370118]), 'targetState': array([13, 15], dtype=int32), 'currentDistance': 0.9174033947458963}
episode index:1533
target Thresh 31.999994370036777
target distance 7.0
model initialize at round 1533
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 17.]), 'previousTarget': array([26., 17.]), 'currentState': array([21.20483956, 10.3225826 ,  1.01476185]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 8.220794777751951}
done in step count: 17
reward sum = 0.7128426152150273
running average episode reward sum: 0.46407184406033797
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 17.]), 'previousTarget': array([26., 17.]), 'currentState': array([25.93677931, 16.17531089,  1.02121434]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 0.8271088121829544}
episode index:1534
target Thresh 31.99999442605585
target distance 5.0
model initialize at round 1534
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([5.94470946, 9.36105704, 1.57517818]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 6.161588524522791}
done in step count: 13
reward sum = 0.7737732791721212
running average episode reward sum: 0.46427360395291895
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([ 9.64961274, 13.10495434,  0.85709118]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 0.9611857138738307}
episode index:1535
target Thresh 31.99999448151752
target distance 2.0
model initialize at round 1535
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.03293626,  5.02423084,  0.55052944]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 2.243373077098942}
done in step count: 10
reward sum = 0.8742195124904133
running average episode reward sum: 0.46454049582045637
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.49379847,  3.98716894,  5.18286165]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.10378414817205}
episode index:1536
target Thresh 31.999994536427337
target distance 15.0
model initialize at round 1536
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  8.]), 'previousTarget': array([10.,  8.]), 'currentState': array([13.        , 23.        ,  2.21097243]), 'targetState': array([10,  8], dtype=int32), 'currentDistance': 15.297058540778357}
done in step count: 43
reward sum = 0.4339564824358758
running average episode reward sum: 0.4645205973081697
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  8.]), 'previousTarget': array([10.,  8.]), 'currentState': array([10.19113729,  8.92338112,  4.43977371]), 'targetState': array([10,  8], dtype=int32), 'currentDistance': 0.9429560758111742}
episode index:1537
target Thresh 31.999994590790795
target distance 20.0
model initialize at round 1537
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8.84070294, 20.15505762]), 'previousTarget': array([ 8.85786438, 20.14213562]), 'currentState': array([22.97770701,  6.0077923 ,  2.62271035]), 'targetState': array([ 3, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 82
reward sum = 0.1895134941260471
running average episode reward sum: 0.46434178904862344
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 26.]), 'previousTarget': array([ 3., 26.]), 'currentState': array([ 3.90545816, 25.08979249,  2.19511399]), 'targetState': array([ 3, 26], dtype=int32), 'currentDistance': 1.2838738936441636}
episode index:1538
target Thresh 31.999994644613324
target distance 5.0
model initialize at round 1538
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([6.        , 9.        , 0.13540402]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 5.830951894845301}
done in step count: 15
reward sum = 0.7695934395106668
running average episode reward sum: 0.46454013320097043
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([8.60934778, 4.93572236, 5.04702431]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 1.0139948210783871}
episode index:1539
target Thresh 31.999994697900313
target distance 11.0
model initialize at round 1539
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 16.]), 'previousTarget': array([14., 16.]), 'currentState': array([7.        , 5.        , 5.57467556]), 'targetState': array([14, 16], dtype=int32), 'currentDistance': 13.038404810405298}
done in step count: 36
reward sum = 0.5073848507596472
running average episode reward sum: 0.4645679544461384
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 16.]), 'previousTarget': array([14., 16.]), 'currentState': array([13.99551841, 15.08775593,  1.49178705]), 'targetState': array([14, 16], dtype=int32), 'currentDistance': 0.9122550753052695}
episode index:1540
target Thresh 31.999994750657088
target distance 11.0
model initialize at round 1540
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'previousTarget': array([25.,  3.]), 'currentState': array([14.        , 14.        ,  0.66395116]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 15.556349186104047}
done in step count: 42
reward sum = 0.4574780388294055
running average episode reward sum: 0.46456335359239626
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'previousTarget': array([25.,  3.]), 'currentState': array([24.16231506,  3.80311291,  5.37892311]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 1.160476803304859}
episode index:1541
target Thresh 31.99999480288892
target distance 11.0
model initialize at round 1541
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 13.]), 'previousTarget': array([27., 13.]), 'currentState': array([23.96869651, 23.88104464,  4.57147563]), 'targetState': array([27, 13], dtype=int32), 'currentDistance': 11.295394345675698}
done in step count: 26
reward sum = 0.6182503107552646
running average episode reward sum: 0.46466302087979106
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 13.]), 'previousTarget': array([27., 13.]), 'currentState': array([26.53934287, 13.96764131,  4.80126112]), 'targetState': array([27, 13], dtype=int32), 'currentDistance': 1.0716971099733255}
episode index:1542
target Thresh 31.99999485460104
target distance 15.0
model initialize at round 1542
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8.85786438, 13.85786438]), 'previousTarget': array([ 8.85786438, 13.85786438]), 'currentState': array([23.        , 28.        ,  5.48805475]), 'targetState': array([ 8, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.2945019520122908
running average episode reward sum: 0.46455274150917053
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 13.]), 'previousTarget': array([ 8., 13.]), 'currentState': array([ 8.55072333, 13.85980686,  3.84530729]), 'targetState': array([ 8, 13], dtype=int32), 'currentDistance': 1.021060243584149}
episode index:1543
target Thresh 31.999994905798616
target distance 16.0
model initialize at round 1543
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 27.]), 'previousTarget': array([24., 27.]), 'currentState': array([ 8.00106243, 27.01103393,  1.22230458]), 'targetState': array([24, 27], dtype=int32), 'currentDistance': 15.998941373225632}
done in step count: 39
reward sum = 0.43683733139588204
running average episode reward sum: 0.4645347911140194
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 27.]), 'previousTarget': array([24., 27.]), 'currentState': array([23.19230453, 26.96203484,  5.93360447]), 'targetState': array([24, 27], dtype=int32), 'currentDistance': 0.8085872389500389}
episode index:1544
target Thresh 31.999994956486766
target distance 7.0
model initialize at round 1544
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 17.]), 'previousTarget': array([ 9., 17.]), 'currentState': array([ 2.        , 20.        ,  1.67715178]), 'targetState': array([ 9, 17], dtype=int32), 'currentDistance': 7.615773105863909}
done in step count: 21
reward sum = 0.6784936623554955
running average episode reward sum: 0.4646732758203246
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 17.]), 'previousTarget': array([ 9., 17.]), 'currentState': array([ 8.11495166, 17.45135554,  5.68930864]), 'targetState': array([ 9, 17], dtype=int32), 'currentDistance': 0.9934950387807473}
episode index:1545
target Thresh 31.999995006670563
target distance 16.0
model initialize at round 1545
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 26.]), 'previousTarget': array([ 2., 26.]), 'currentState': array([18.        , 23.        ,  0.51060079]), 'targetState': array([ 2, 26], dtype=int32), 'currentDistance': 16.278820596099706}
done in step count: 42
reward sum = 0.4127309871277607
running average episode reward sum: 0.46463967796217936
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 26.]), 'previousTarget': array([ 2., 26.]), 'currentState': array([ 2.91666377, 25.92634799,  2.82151184]), 'targetState': array([ 2, 26], dtype=int32), 'currentDistance': 0.9196179048227725}
episode index:1546
target Thresh 31.99999505635502
target distance 13.0
model initialize at round 1546
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 16.]), 'previousTarget': array([24., 16.]), 'currentState': array([16.02056261,  2.89822419,  5.16112676]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 15.340402555419718}
done in step count: 43
reward sum = 0.4341958603795547
running average episode reward sum: 0.4646199987006521
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 16.]), 'previousTarget': array([24., 16.]), 'currentState': array([24.02315528, 15.01592496,  1.38118432]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 0.9843474195151634}
episode index:1547
target Thresh 31.999995105545107
target distance 5.0
model initialize at round 1547
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 29.]), 'previousTarget': array([ 4., 29.]), 'currentState': array([ 8.83947912, 29.13122631,  2.65141481]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 4.841257948899563}
done in step count: 11
reward sum = 0.8256043741016239
running average episode reward sum: 0.46485319274160886
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 29.]), 'previousTarget': array([ 4., 29.]), 'currentState': array([ 4.87689505, 29.02686614,  3.37638078]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 0.8773065165470325}
episode index:1548
target Thresh 31.99999515424575
target distance 9.0
model initialize at round 1548
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([13.01615101,  2.04180666,  1.45463306]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 11.388917133515431}
done in step count: 26
reward sum = 0.5994439664080067
running average episode reward sum: 0.46494008155611266
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.99653931, 8.27634245, 2.32242159]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.2315725109489162}
episode index:1549
target Thresh 31.999995202461808
target distance 8.0
model initialize at round 1549
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 23.]), 'previousTarget': array([13., 23.]), 'currentState': array([13.92085686, 14.99386743,  2.99055853]), 'targetState': array([13, 23], dtype=int32), 'currentDistance': 8.058916558522764}
done in step count: 19
reward sum = 0.6862424168084453
running average episode reward sum: 0.46508285725627546
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 23.]), 'previousTarget': array([13., 23.]), 'currentState': array([13.13300146, 22.15293734,  1.70132059]), 'targetState': array([13, 23], dtype=int32), 'currentDistance': 0.8574406934022727}
episode index:1550
target Thresh 31.99999525019811
target distance 11.0
model initialize at round 1550
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 17.]), 'previousTarget': array([18., 17.]), 'currentState': array([10.99190611, 28.00441808,  2.86852092]), 'targetState': array([18, 17], dtype=int32), 'currentDistance': 13.046478346558178}
done in step count: 37
reward sum = 0.4920069940299834
running average episode reward sum: 0.4651002164676061
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 17.]), 'previousTarget': array([18., 17.]), 'currentState': array([17.79619596, 17.8154394 ,  5.25026347]), 'targetState': array([18, 17], dtype=int32), 'currentDistance': 0.8405221642092682}
episode index:1551
target Thresh 31.99999529745943
target distance 12.0
model initialize at round 1551
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 13.]), 'previousTarget': array([21., 13.]), 'currentState': array([ 8.97163889, 10.97296689,  4.15552187]), 'targetState': array([21, 13], dtype=int32), 'currentDistance': 12.197964348755509}
done in step count: 29
reward sum = 0.518230702008386
running average episode reward sum: 0.4651344500278772
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 13.]), 'previousTarget': array([21., 13.]), 'currentState': array([20.03559842, 12.63846989,  0.40325185]), 'targetState': array([21, 13], dtype=int32), 'currentDistance': 1.0299390418176144}
episode index:1552
target Thresh 31.99999534425049
target distance 4.0
model initialize at round 1552
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 22.]), 'previousTarget': array([23., 22.]), 'currentState': array([26.77362174, 22.76066342,  3.83996525]), 'targetState': array([23, 22], dtype=int32), 'currentDistance': 3.849523334367317}
done in step count: 9
reward sum = 0.8614578449569525
running average episode reward sum: 0.4653896486079989
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 22.]), 'previousTarget': array([23., 22.]), 'currentState': array([23.77501956, 22.11060451,  3.16747139]), 'targetState': array([23, 22], dtype=int32), 'currentDistance': 0.782872064996357}
episode index:1553
target Thresh 31.99999539057597
target distance 16.0
model initialize at round 1553
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 17.]), 'previousTarget': array([ 5., 17.]), 'currentState': array([21.01478183, 22.9327869 ,  4.70670605]), 'targetState': array([ 5, 17], dtype=int32), 'currentDistance': 17.078383920739054}
done in step count: 45
reward sum = 0.417959946669746
running average episode reward sum: 0.4653591275642806
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 17.]), 'previousTarget': array([ 5., 17.]), 'currentState': array([ 5.7915651 , 17.49619703,  3.24217156]), 'targetState': array([ 5, 17], dtype=int32), 'currentDistance': 0.934230586355409}
episode index:1554
target Thresh 31.999995436440507
target distance 14.0
model initialize at round 1554
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 16.]), 'previousTarget': array([23., 16.]), 'currentState': array([8.98134847, 3.98667436, 3.69920188]), 'targetState': array([23, 16], dtype=int32), 'currentDistance': 18.461922533770323}
done in step count: 58
reward sum = 0.3401553278547301
running average episode reward sum: 0.4652786106512841
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 16.]), 'previousTarget': array([23., 16.]), 'currentState': array([22.20807675, 15.04074134,  0.58769044]), 'targetState': array([23, 16], dtype=int32), 'currentDistance': 1.2439130227952129}
episode index:1555
target Thresh 31.999995481848682
target distance 14.0
model initialize at round 1555
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 14.]), 'previousTarget': array([27., 14.]), 'currentState': array([13.42591889, 16.79963879,  5.9194755 ]), 'targetState': array([27, 14], dtype=int32), 'currentDistance': 13.859785540171218}
done in step count: 32
reward sum = 0.5304756287063814
running average episode reward sum: 0.46532051104849176
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 14.]), 'previousTarget': array([27., 14.]), 'currentState': array([26.06467355, 14.25085008,  0.10054585]), 'targetState': array([27, 14], dtype=int32), 'currentDistance': 0.9683807744315186}
episode index:1556
target Thresh 31.99999552680504
target distance 25.0
model initialize at round 1556
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.31390548, 20.23068683]), 'previousTarget': array([15.31390548, 20.23068683]), 'currentState': array([27.        ,  4.        ,  5.91728428]), 'targetState': array([ 9, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 93
reward sum = 0.0804251603537734
running average episode reward sum: 0.46507330786885487
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 29.]), 'previousTarget': array([ 9., 29.]), 'currentState': array([ 9.96722683, 28.42225026,  2.29551274]), 'targetState': array([ 9, 29], dtype=int32), 'currentDistance': 1.1266421339667956}
episode index:1557
target Thresh 31.999995571314074
target distance 20.0
model initialize at round 1557
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  7.]), 'previousTarget': array([10.99007438,  7.0992562 ]), 'currentState': array([ 9.06782913, 26.77901355,  4.91476883]), 'targetState': array([11,  7], dtype=int32), 'currentDistance': 19.873164348396053}
done in step count: 70
reward sum = 0.3368900750432213
running average episode reward sum: 0.4649910336500964
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  7.]), 'previousTarget': array([11.,  7.]), 'currentState': array([10.85848991,  7.87766161,  5.47880374]), 'targetState': array([11,  7], dtype=int32), 'currentDistance': 0.8889966302299223}
episode index:1558
target Thresh 31.999995615380236
target distance 14.0
model initialize at round 1558
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  9.]), 'previousTarget': array([22.,  9.]), 'currentState': array([8.        , 5.        , 1.63328996]), 'targetState': array([22,  9], dtype=int32), 'currentDistance': 14.560219778561036}
done in step count: 46
reward sum = 0.447961948422147
running average episode reward sum: 0.4649801105678463
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  9.]), 'previousTarget': array([22.,  9.]), 'currentState': array([21.22034213,  8.72286106,  0.27532447]), 'targetState': array([22,  9], dtype=int32), 'currentDistance': 0.827449324574302}
episode index:1559
target Thresh 31.99999565900793
target distance 5.0
model initialize at round 1559
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 12.]), 'previousTarget': array([19., 12.]), 'currentState': array([13.93634806,  9.92631048,  4.21783808]), 'targetState': array([19, 12], dtype=int32), 'currentDistance': 5.471814981921455}
done in step count: 14
reward sum = 0.7536867226077132
running average episode reward sum: 0.4651651789088976
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 12.]), 'previousTarget': array([19., 12.]), 'currentState': array([18.02383321, 11.31085512,  0.60431054]), 'targetState': array([19, 12], dtype=int32), 'currentDistance': 1.1949151668240616}
episode index:1560
target Thresh 31.999995702201524
target distance 20.0
model initialize at round 1560
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.48050538, 21.4474533 ]), 'previousTarget': array([14.53075311, 21.38463841]), 'currentState': array([25.93029755,  5.04921379,  2.33356687]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 70
reward sum = 0.2403217018967237
running average episode reward sum: 0.465021140807032
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 25.]), 'previousTarget': array([12., 25.]), 'currentState': array([12.34412339, 24.0527514 ,  2.0692584 ]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 1.0078198331376509}
episode index:1561
target Thresh 31.999995744965332
target distance 17.0
model initialize at round 1561
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 21.]), 'previousTarget': array([ 6., 21.]), 'currentState': array([22.5824828 , 15.04766716,  3.00873694]), 'targetState': array([ 6, 21], dtype=int32), 'currentDistance': 17.618427914900625}
done in step count: 41
reward sum = 0.42396942513458774
running average episode reward sum: 0.46499485929891904
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 21.]), 'previousTarget': array([ 6., 21.]), 'currentState': array([ 6.75120472, 20.69923379,  2.90257232]), 'targetState': array([ 6, 21], dtype=int32), 'currentDistance': 0.8091778834412721}
episode index:1562
target Thresh 31.999995787303636
target distance 25.0
model initialize at round 1562
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9.59266453, 10.43828145]), 'previousTarget': array([ 9.57218647, 10.43046618]), 'currentState': array([17.06356197, 28.99052367,  5.90614537]), 'targetState': array([7, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 71
reward sum = 0.15226351570593022
running average episode reward sum: 0.4647947752659101
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 4.]), 'previousTarget': array([7., 4.]), 'currentState': array([7.32283401, 4.95831053, 3.84325566]), 'targetState': array([7, 4], dtype=int32), 'currentDistance': 1.0112274116360709}
episode index:1563
target Thresh 31.999995829220666
target distance 22.0
model initialize at round 1563
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.9793708 , 28.09184678]), 'previousTarget': array([23.9793708 , 28.09184678]), 'currentState': array([ 4.        , 29.        ,  5.41325855]), 'targetState': array([26, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.2911102223858085
running average episode reward sum: 0.464683723761511
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 28.]), 'previousTarget': array([26., 28.]), 'currentState': array([2.51219348e+01, 2.85515034e+01, 1.03153493e-02]), 'targetState': array([26, 28], dtype=int32), 'currentDistance': 1.0368965398381034}
episode index:1564
target Thresh 31.99999587072061
target distance 10.0
model initialize at round 1564
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([23.65533507,  7.27360818,  2.49291767]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 12.37767557943223}
done in step count: 25
reward sum = 0.5853713056401022
running average episode reward sum: 0.4647608404272482
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([16.69159585, 16.15517518,  2.140364  ]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 1.0918030004091535}
episode index:1565
target Thresh 31.999995911807627
target distance 13.0
model initialize at round 1565
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  6.]), 'previousTarget': array([17.,  6.]), 'currentState': array([17.        , 19.        ,  3.41388887]), 'targetState': array([17,  6], dtype=int32), 'currentDistance': 13.0}
done in step count: 30
reward sum = 0.5347227883594146
running average episode reward sum: 0.46480551600064035
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  6.]), 'previousTarget': array([17.,  6.]), 'currentState': array([16.27255596,  6.93108865,  4.95133227]), 'targetState': array([17,  6], dtype=int32), 'currentDistance': 1.1815671380952795}
episode index:1566
target Thresh 31.999995952485822
target distance 4.0
model initialize at round 1566
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 16.]), 'previousTarget': array([ 9., 16.]), 'currentState': array([ 9.99939241, 20.00104309,  2.30529773]), 'targetState': array([ 9, 16], dtype=int32), 'currentDistance': 4.123970293100237}
done in step count: 13
reward sum = 0.7908692242124503
running average episode reward sum: 0.46501359749918014
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 16.]), 'previousTarget': array([ 9., 16.]), 'currentState': array([ 8.31094276, 16.72319822,  4.60353123]), 'targetState': array([ 9, 16], dtype=int32), 'currentDistance': 0.998907179253318}
episode index:1567
target Thresh 31.99999599275926
target distance 12.0
model initialize at round 1567
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([25.97309201, 21.00184193,  3.30314374]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 12.001872091766701}
done in step count: 28
reward sum = 0.5576700289539537
running average episode reward sum: 0.4650726896110773
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([25.69024978,  9.74151378,  4.83352836]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 0.8036092816813436}
episode index:1568
target Thresh 31.999996032631973
target distance 14.0
model initialize at round 1568
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 2.]), 'previousTarget': array([7., 2.]), 'currentState': array([12.67701373, 15.97733851,  3.41840461]), 'targetState': array([7, 2], dtype=int32), 'currentDistance': 15.086234673379407}
done in step count: 40
reward sum = 0.4765256209232015
running average episode reward sum: 0.4650799891211551
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 2.]), 'previousTarget': array([7., 2.]), 'currentState': array([7.26538373, 2.88078705, 4.31442108]), 'targetState': array([7, 2], dtype=int32), 'currentDistance': 0.9198990929765245}
episode index:1569
target Thresh 31.999996072107944
target distance 23.0
model initialize at round 1569
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.68947512, 21.29662129]), 'previousTarget': array([10.8764257 , 21.09211044]), 'currentState': array([23.80287172,  6.1956667 ,  2.34696435]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.15945617964849307
running average episode reward sum: 0.4646821953830853
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 29.]), 'previousTarget': array([ 4., 29.]), 'currentState': array([ 7.73833815, 24.10014098,  2.07102474]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 6.163099099344584}
episode index:1570
target Thresh 31.999996111191123
target distance 3.0
model initialize at round 1570
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 29.]), 'previousTarget': array([ 8., 29.]), 'currentState': array([ 4.99331617, 27.98754423,  4.44738886]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 3.172572193642855}
done in step count: 9
reward sum = 0.8486411591882947
running average episode reward sum: 0.46492659956119176
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 29.]), 'previousTarget': array([ 8., 29.]), 'currentState': array([ 7.02653149, 28.08953553,  0.44834271]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 1.332886523255857}
episode index:1571
target Thresh 31.99999614988542
target distance 17.0
model initialize at round 1571
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([20.00349442,  2.97269131,  4.58856648]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 18.398420856517404}
done in step count: 55
reward sum = 0.33711423761548664
running average episode reward sum: 0.46484529398743496
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.92190452, 10.27037727,  2.99145143]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.9607350416538749}
episode index:1572
target Thresh 31.999996188194697
target distance 10.0
model initialize at round 1572
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([1.96625891, 6.94126045, 4.43050197]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 10.242771537370972}
done in step count: 32
reward sum = 0.5460382966664981
running average episode reward sum: 0.4648969106452093
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([11.9681563 ,  8.04533624,  0.47614582]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.9551946961723057}
episode index:1573
target Thresh 31.999996226122793
target distance 4.0
model initialize at round 1573
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 21.]), 'previousTarget': array([13., 21.]), 'currentState': array([17.        , 17.        ,  0.04019692]), 'targetState': array([13, 21], dtype=int32), 'currentDistance': 5.656854249492381}
done in step count: 19
reward sum = 0.7198625933300332
running average episode reward sum: 0.4650588964664831
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 21.]), 'previousTarget': array([13., 21.]), 'currentState': array([13.77882188, 21.01031516,  2.78718066]), 'targetState': array([13, 21], dtype=int32), 'currentDistance': 0.7788901913415663}
episode index:1574
target Thresh 31.9999962636735
target distance 19.0
model initialize at round 1574
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  3.]), 'previousTarget': array([22.,  3.]), 'currentState': array([3.01448518, 6.04670264, 1.10112926]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 19.228420886538647}
done in step count: 62
reward sum = 0.33340262179296887
running average episode reward sum: 0.4649753051809761
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  3.]), 'previousTarget': array([22.,  3.]), 'currentState': array([21.05756921,  3.89133749,  6.00790879]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 1.2971731963426048}
episode index:1575
target Thresh 31.99999630085057
target distance 20.0
model initialize at round 1575
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.49290798, 23.44490202]), 'previousTarget': array([11.53075311, 23.38463841]), 'currentState': array([22.97556566,  7.06965936,  2.05192658]), 'targetState': array([ 9, 27], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 86
reward sum = 0.23239559769383658
running average episode reward sum: 0.46482772922444876
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 27.]), 'previousTarget': array([ 9., 27.]), 'currentState': array([ 9.06925049, 26.17721593,  2.43918512]), 'targetState': array([ 9, 27], dtype=int32), 'currentDistance': 0.8256931964853716}
episode index:1576
target Thresh 31.999996337657723
target distance 11.0
model initialize at round 1576
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 29.]), 'previousTarget': array([18., 29.]), 'currentState': array([16.01664754, 18.39959647,  1.56576177]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 10.784351717822636}
done in step count: 24
reward sum = 0.6233650389246518
running average episode reward sum: 0.46492826017543176
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 29.]), 'previousTarget': array([18., 29.]), 'currentState': array([18.13429728, 28.10602114,  1.18071518]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 0.9040099382466469}
episode index:1577
target Thresh 31.999996374098636
target distance 16.0
model initialize at round 1577
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 27.]), 'previousTarget': array([ 6., 27.]), 'currentState': array([ 9.92471201, 11.25799757,  1.82911309]), 'targetState': array([ 6, 27], dtype=int32), 'currentDistance': 16.223871454105417}
done in step count: 49
reward sum = 0.43928136973984133
running average episode reward sum: 0.46491200739315325
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 27.]), 'previousTarget': array([ 6., 27.]), 'currentState': array([ 5.93135007, 26.17342391,  1.5569616 ]), 'targetState': array([ 6, 27], dtype=int32), 'currentDistance': 0.8294219989572504}
episode index:1578
target Thresh 31.99999641017696
target distance 17.0
model initialize at round 1578
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4.56139529, 12.28585494]), 'previousTarget': array([ 4.56139529, 12.28585494]), 'currentState': array([20.        , 25.        ,  5.08667862]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.022329544290073756
running average episode reward sum: 0.46460343136295484
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3.28863845, 11.21971541]), 'previousTarget': array([ 3.31237119, 11.23815962]), 'currentState': array([19.20257638, 23.33361566,  4.27045501]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 20.0}
episode index:1579
target Thresh 31.999996445896294
target distance 11.0
model initialize at round 1579
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 10.]), 'previousTarget': array([27., 10.]), 'currentState': array([15.95696256, 15.99252002,  3.56338656]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 12.564194044150042}
done in step count: 31
reward sum = 0.5274441841034955
running average episode reward sum: 0.4646432039912717
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 10.]), 'previousTarget': array([27., 10.]), 'currentState': array([26.07750942, 10.85396903,  5.99409372]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 1.2570807351352564}
episode index:1580
target Thresh 31.999996481260215
target distance 8.0
model initialize at round 1580
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  3.]), 'previousTarget': array([24.,  3.]), 'currentState': array([24.82136416, 10.86321631,  3.98628047]), 'targetState': array([24,  3], dtype=int32), 'currentDistance': 7.905998341397238}
done in step count: 15
reward sum = 0.7228461782461157
running average episode reward sum: 0.464806520230522
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  3.]), 'previousTarget': array([24.,  3.]), 'currentState': array([23.45308467,  3.95757735,  4.56907781]), 'targetState': array([24,  3], dtype=int32), 'currentDistance': 1.1027559857767864}
episode index:1581
target Thresh 31.99999651627226
target distance 18.0
model initialize at round 1581
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.74274983,  9.76120699]), 'previousTarget': array([23.70981108,  9.78641543]), 'currentState': array([12.12260594, 26.03917542,  0.10697412]), 'targetState': array([25,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 94
reward sum = 0.21558352236885794
running average episode reward sum: 0.4646489835694211
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  8.]), 'previousTarget': array([25.,  8.]), 'currentState': array([24.12004935,  8.22734596,  5.34768527]), 'targetState': array([25,  8], dtype=int32), 'currentDistance': 0.9088450499753985}
episode index:1582
target Thresh 31.999996550935933
target distance 14.0
model initialize at round 1582
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 22.]), 'previousTarget': array([ 9., 22.]), 'currentState': array([22.71268625, 28.82184263,  3.66461398]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 15.315851299270058}
done in step count: 38
reward sum = 0.48812561641081664
running average episode reward sum: 0.4646638140386829
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 22.]), 'previousTarget': array([ 9., 22.]), 'currentState': array([ 9.39477315, 22.88096343,  3.68785869]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 0.9653716447115084}
episode index:1583
target Thresh 31.999996585254692
target distance 13.0
model initialize at round 1583
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([18.97547088, 24.05617181,  2.19801831]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 13.648003190069685}
done in step count: 43
reward sum = 0.4272126409198096
running average episode reward sum: 0.46464017062130986
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.9417936 , 11.71981774,  4.86638567]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.7221672721457412}
episode index:1584
target Thresh 31.999996619231975
target distance 12.0
model initialize at round 1584
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 17.]), 'previousTarget': array([26., 17.]), 'currentState': array([23.21113717, 28.62587715,  5.1926597 ]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 11.95570053724063}
done in step count: 28
reward sum = 0.5923778584218229
running average episode reward sum: 0.46472076222244585
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 17.]), 'previousTarget': array([26., 17.]), 'currentState': array([25.58971905, 17.8773191 ,  4.97917434]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 0.9685139454164666}
episode index:1585
target Thresh 31.999996652871182
target distance 14.0
model initialize at round 1585
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 26.]), 'previousTarget': array([ 9., 26.]), 'currentState': array([22.86384065, 23.1703882 ,  2.37753361]), 'targetState': array([ 9, 26], dtype=int32), 'currentDistance': 14.149656546187666}
done in step count: 33
reward sum = 0.5226743593529746
running average episode reward sum: 0.4647573029520364
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 26.]), 'previousTarget': array([ 9., 26.]), 'currentState': array([ 9.88256246, 26.16532016,  2.89593723]), 'targetState': array([ 9, 26], dtype=int32), 'currentDistance': 0.8979127150349125}
episode index:1586
target Thresh 31.99999668617567
target distance 7.0
model initialize at round 1586
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 14.]), 'previousTarget': array([23., 14.]), 'currentState': array([15.9303225 ,  7.04284774,  2.33775759]), 'targetState': array([23, 14], dtype=int32), 'currentDistance': 9.918785591851481}
done in step count: 21
reward sum = 0.6211997615072335
running average episode reward sum: 0.4648558804306471
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 14.]), 'previousTarget': array([23., 14.]), 'currentState': array([22.43438502, 13.13009461,  0.62067699]), 'targetState': array([23, 14], dtype=int32), 'currentDistance': 1.0376202040854787}
episode index:1587
target Thresh 31.999996719148772
target distance 5.0
model initialize at round 1587
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([20.        , 16.        ,  2.45390892]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 5.0990195135927845}
done in step count: 14
reward sum = 0.7636832041083722
running average episode reward sum: 0.46504405884606126
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([20.12138383, 11.96636487,  5.13150345]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 1.3060732849675736}
episode index:1588
target Thresh 31.999996751793788
target distance 20.0
model initialize at round 1588
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  6.]), 'previousTarget': array([25.,  6.]), 'currentState': array([25.04701227, 25.98730289,  5.84630901]), 'targetState': array([25,  6], dtype=int32), 'currentDistance': 19.987358176176812}
done in step count: 53
reward sum = 0.3409224230582567
running average episode reward sum: 0.464965945796478
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  6.]), 'previousTarget': array([25.,  6.]), 'currentState': array([24.4265478 ,  6.95763132,  4.7117826 ]), 'targetState': array([25,  6], dtype=int32), 'currentDistance': 1.1162012203765759}
episode index:1589
target Thresh 31.99999678411398
target distance 11.0
model initialize at round 1589
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 28.]), 'previousTarget': array([23., 28.]), 'currentState': array([12.00468136, 17.99725252,  5.98114336]), 'targetState': array([23, 28], dtype=int32), 'currentDistance': 14.864453883382707}
done in step count: 40
reward sum = 0.48251367975722637
running average episode reward sum: 0.4649769821071451
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 28.]), 'previousTarget': array([23., 28.]), 'currentState': array([22.36650039, 27.02563505,  0.7127519 ]), 'targetState': array([23, 28], dtype=int32), 'currentDistance': 1.1621999814063735}
episode index:1590
target Thresh 31.999996816112578
target distance 14.0
model initialize at round 1590
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 14.]), 'previousTarget': array([13., 14.]), 'currentState': array([27.        , 11.        ,  4.69781017]), 'targetState': array([13, 14], dtype=int32), 'currentDistance': 14.317821063276352}
done in step count: 37
reward sum = 0.47953787440718737
running average episode reward sum: 0.46498613414504586
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 14.]), 'previousTarget': array([13., 14.]), 'currentState': array([13.90933115, 13.7540343 ,  3.13025596]), 'targetState': array([13, 14], dtype=int32), 'currentDistance': 0.9420096980971497}
episode index:1591
target Thresh 31.99999684779279
target distance 16.0
model initialize at round 1591
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 25.]), 'previousTarget': array([20., 25.]), 'currentState': array([ 3.99972233, 15.98699782,  4.94353676]), 'targetState': array([20, 25], dtype=int32), 'currentDistance': 18.364179637573393}
done in step count: 47
reward sum = 0.35940907122902277
running average episode reward sum: 0.4649198168944705
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 25.]), 'previousTarget': array([20., 25.]), 'currentState': array([19.16767702, 24.98891207,  0.56885518]), 'targetState': array([20, 25], dtype=int32), 'currentDistance': 0.8323968365106053}
episode index:1592
target Thresh 31.999996879157774
target distance 19.0
model initialize at round 1592
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([9.8000865 , 2.25413806, 2.11313908]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 19.210585966207397}
done in step count: 54
reward sum = 0.364374883893987
running average episode reward sum: 0.4648567001757006
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([13.83692563, 20.0494791 ,  1.03328636]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 0.9644082279949667}
episode index:1593
target Thresh 31.999996910210672
target distance 12.0
model initialize at round 1593
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([8.        , 9.        , 2.15306503]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 12.649110640673518}
done in step count: 31
reward sum = 0.525133810990914
running average episode reward sum: 0.464894515176212
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([19.17714154, 12.67384923,  0.07734801]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 0.8851386176453796}
episode index:1594
target Thresh 31.999996940954592
target distance 14.0
model initialize at round 1594
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 20.]), 'previousTarget': array([16., 20.]), 'currentState': array([14.67824473,  6.36936381,  2.13003758]), 'targetState': array([16, 20], dtype=int32), 'currentDistance': 13.694571184486453}
done in step count: 29
reward sum = 0.5229360185223018
running average episode reward sum: 0.46493090483348226
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 20.]), 'previousTarget': array([16., 20.]), 'currentState': array([15.78509604, 19.0273491 ,  1.48098072]), 'targetState': array([16, 20], dtype=int32), 'currentDistance': 0.9961091741409264}
episode index:1595
target Thresh 31.9999969713926
target distance 6.0
model initialize at round 1595
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 22.]), 'previousTarget': array([ 9., 22.]), 'currentState': array([15.00097422, 20.99839037,  5.20798352]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 6.083988280367393}
done in step count: 16
reward sum = 0.7388228675767542
running average episode reward sum: 0.46510251633896055
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 22.]), 'previousTarget': array([ 9., 22.]), 'currentState': array([ 9.87565228, 21.52500566,  2.63199994]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 0.9961859968914801}
episode index:1596
target Thresh 31.99999700152775
target distance 14.0
model initialize at round 1596
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 13.]), 'previousTarget': array([10., 13.]), 'currentState': array([24.00216365, 21.99535376,  4.95574701]), 'targetState': array([10, 13], dtype=int32), 'currentDistance': 16.64262527532656}
done in step count: 38
reward sum = 0.43688503577401694
running average episode reward sum: 0.46508484728412963
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 13.]), 'previousTarget': array([10., 13.]), 'currentState': array([10.91101387, 13.5749104 ,  3.60454274]), 'targetState': array([10, 13], dtype=int32), 'currentDistance': 1.0772503161802436}
episode index:1597
target Thresh 31.999997031363044
target distance 17.0
model initialize at round 1597
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([24.        ,  3.        ,  2.60050246]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 17.11724276862369}
done in step count: 40
reward sum = 0.4219127651771066
running average episode reward sum: 0.4650578309624106
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([26.10605815, 19.24352415,  1.26106715]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 0.76387435946983}
episode index:1598
target Thresh 31.999997060901478
target distance 17.0
model initialize at round 1598
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 22.]), 'previousTarget': array([ 9., 22.]), 'currentState': array([25.69951318, 22.63343951,  3.81827837]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 16.71152255773514}
done in step count: 42
reward sum = 0.4290589816527538
running average episode reward sum: 0.4650353176107473
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 22.]), 'previousTarget': array([ 9., 22.]), 'currentState': array([ 9.7861712 , 21.85070978,  2.92766903]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 0.8002204268582885}
episode index:1599
target Thresh 31.999997090145996
target distance 20.0
model initialize at round 1599
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.28991511, 23.14985851]), 'previousTarget': array([19.28991511, 23.14985851]), 'currentState': array([9.       , 6.       , 2.8656106]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.23611465540780718
running average episode reward sum: 0.4648922421968705
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([20.31004542, 25.20532498,  1.23678678]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 1.052399975854758}
episode index:1600
target Thresh 31.99999711909953
target distance 14.0
model initialize at round 1600
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  2.]), 'previousTarget': array([23.,  2.]), 'currentState': array([23.92744529, 15.96973369,  3.78929687]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 14.00048620927369}
done in step count: 33
reward sum = 0.5041917444675148
running average episode reward sum: 0.4649167890440102
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  2.]), 'previousTarget': array([23.,  2.]), 'currentState': array([23.3053749,  2.8778305,  4.941737 ]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 0.9294300465637211}
episode index:1601
target Thresh 31.999997147764965
target distance 6.0
model initialize at round 1601
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  6.]), 'previousTarget': array([18.,  6.]), 'currentState': array([24.        ,  5.        ,  6.16333172]), 'targetState': array([18,  6], dtype=int32), 'currentDistance': 6.082762530298219}
done in step count: 17
reward sum = 0.7023613559932356
running average episode reward sum: 0.4650650066263755
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  6.]), 'previousTarget': array([18.,  6.]), 'currentState': array([18.97736101,  5.60664792,  3.31266773]), 'targetState': array([18,  6], dtype=int32), 'currentDistance': 1.0535465831933652}
episode index:1602
target Thresh 31.99999717614518
target distance 7.0
model initialize at round 1602
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'previousTarget': array([10., 12.]), 'currentState': array([11.96020241, 19.01930416,  2.9424696 ]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 7.28786829820771}
done in step count: 16
reward sum = 0.7116175713080557
running average episode reward sum: 0.4652188135912424
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'previousTarget': array([10., 12.]), 'currentState': array([10.04971194, 12.95811842,  4.42836853]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 0.9594072057979961}
episode index:1603
target Thresh 31.999997204243005
target distance 3.0
model initialize at round 1603
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 28.]), 'previousTarget': array([10., 28.]), 'currentState': array([ 7.00164426, 24.99830329,  5.71401933]), 'targetState': array([10, 28], dtype=int32), 'currentDistance': 4.242678434336591}
done in step count: 11
reward sum = 0.8203239849613191
running average episode reward sum: 0.46544020085518883
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 28.]), 'previousTarget': array([10., 28.]), 'currentState': array([ 9.78796848, 27.13612731,  0.99453983]), 'targetState': array([10, 28], dtype=int32), 'currentDistance': 0.8895130087600909}
episode index:1604
target Thresh 31.99999723206125
target distance 9.0
model initialize at round 1604
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([22.67737457, 11.98732393,  3.32554406]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 10.13162496212977}
done in step count: 22
reward sum = 0.6384148722315843
running average episode reward sum: 0.4655479732361087
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([18.27788416,  3.92822848,  4.51045083]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 0.9689312290913811}
episode index:1605
target Thresh 31.9999972596027
target distance 10.0
model initialize at round 1605
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([12.        , 18.        ,  5.45643309]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 12.806248474865699}
done in step count: 31
reward sum = 0.5230116449603117
running average episode reward sum: 0.46558375385362066
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.08216652, 8.88443331, 4.29838538]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.8882418726325391}
episode index:1606
target Thresh 31.99999728687011
target distance 12.0
model initialize at round 1606
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([ 9.        , 14.        ,  0.93760675]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 14.422205101855956}
done in step count: 33
reward sum = 0.4892611801612645
running average episode reward sum: 0.46559848778411705
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([20.26142597,  6.51501493,  5.86981019]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 0.9004065581305172}
episode index:1607
target Thresh 31.999997313866203
target distance 7.0
model initialize at round 1607
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([18.04766337,  2.02430437,  0.72404973]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 7.3193532682992135}
done in step count: 18
reward sum = 0.6832982170698388
running average episode reward sum: 0.46573387318790166
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([11.83569964,  4.087411  ,  2.72147301]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 0.8402586332494615}
episode index:1608
target Thresh 31.99999734059368
target distance 15.0
model initialize at round 1608
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([18.5318599 ,  8.96645011,  3.18935269]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 14.563961617981386}
done in step count: 30
reward sum = 0.5074761452521688
running average episode reward sum: 0.46575981617861906
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.82190894, 8.54999814, 3.01390891]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.9889551345381579}
episode index:1609
target Thresh 31.999997367055215
target distance 26.0
model initialize at round 1609
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.06096494, 11.13883146]), 'previousTarget': array([10.05572809, 11.11145618]), 'currentState': array([18.99036823, 29.03480187,  2.06684801]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.25408669931809374
running average episode reward sum: 0.46531270654166457
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 3.]), 'previousTarget': array([6., 3.]), 'currentState': array([4.72468489, 8.93449326, 5.01179359]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 6.069978489164757}
episode index:1610
target Thresh 31.999997393253455
target distance 19.0
model initialize at round 1610
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.5821301 ,  3.88293602]), 'previousTarget': array([13.56172689,  3.92524322]), 'currentState': array([ 5.02647313, 21.96056678,  5.33912902]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.31683404924857544
running average episode reward sum: 0.4652205410188259
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.11200572,  3.45625952,  5.24411428]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.9983519403486021}
episode index:1611
target Thresh 31.999997419191015
target distance 10.0
model initialize at round 1611
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 12.]), 'previousTarget': array([ 2., 12.]), 'currentState': array([10.00392976, 21.99811557,  5.62988225]), 'targetState': array([ 2, 12], dtype=int32), 'currentDistance': 12.807232586311219}
done in step count: 32
reward sum = 0.529883626924641
running average episode reward sum: 0.46526065459569055
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 12.]), 'previousTarget': array([ 2., 12.]), 'currentState': array([ 2.77860208, 12.33308074,  3.83835868]), 'targetState': array([ 2, 12], dtype=int32), 'currentDistance': 0.8468553452031536}
episode index:1612
target Thresh 31.999997444870495
target distance 7.0
model initialize at round 1612
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 17.]), 'previousTarget': array([ 5., 17.]), 'currentState': array([11.9890133 ,  9.95036277,  4.49403926]), 'targetState': array([ 5, 17], dtype=int32), 'currentDistance': 9.926917545427147}
done in step count: 26
reward sum = 0.6038123997966642
running average episode reward sum: 0.4653465515238994
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 17.]), 'previousTarget': array([ 5., 17.]), 'currentState': array([ 5.89635577, 16.390353  ,  2.16628987]), 'targetState': array([ 5, 17], dtype=int32), 'currentDistance': 1.0840309647509394}
episode index:1613
target Thresh 31.999997470294456
target distance 2.0
model initialize at round 1613
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 13.]), 'previousTarget': array([17., 13.]), 'currentState': array([15.39016545, 12.15321275,  0.369239  ]), 'targetState': array([17, 13], dtype=int32), 'currentDistance': 1.818960119572884}
done in step count: 2
reward sum = 0.9619346958221889
running average episode reward sum: 0.4656542269540718
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 13.]), 'previousTarget': array([17., 13.]), 'currentState': array([16.09431784, 12.40637928,  0.32097327]), 'targetState': array([17, 13], dtype=int32), 'currentDistance': 1.082887683015019}
episode index:1614
target Thresh 31.999997495465447
target distance 9.0
model initialize at round 1614
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([21.63052485, 21.88168808,  3.51475088]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 11.709367733790849}
done in step count: 24
reward sum = 0.5947845798564347
running average episode reward sum: 0.465734183828934
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([14.27325399, 13.81552529,  3.9394556 ]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 0.8600867599492458}
episode index:1615
target Thresh 31.999997520385985
target distance 8.0
model initialize at round 1615
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 20.]), 'previousTarget': array([ 3., 20.]), 'currentState': array([ 2.00410333, 27.50008246,  4.74894052]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 7.56591349872188}
done in step count: 16
reward sum = 0.7344342269911612
running average episode reward sum: 0.4659004586081186
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 20.]), 'previousTarget': array([ 3., 20.]), 'currentState': array([ 2.29119477, 20.90722386,  4.69294302]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 1.1512862266557637}
episode index:1616
target Thresh 31.999997545058555
target distance 15.0
model initialize at round 1616
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([23.        , 13.        ,  0.95572174]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 15.033296378372908}
done in step count: 38
reward sum = 0.43289090485399206
running average episode reward sum: 0.46588004453653287
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([ 8.92217178, 14.33058346,  2.96563112]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 0.9796357549508792}
episode index:1617
target Thresh 31.999997569485632
target distance 7.0
model initialize at round 1617
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([23.        ,  4.        ,  0.33499565]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 7.280109889280518}
done in step count: 20
reward sum = 0.6654213293145498
running average episode reward sum: 0.46600337042329304
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.88201492,  2.43888863,  3.51988893]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.9851769091202247}
episode index:1618
target Thresh 31.999997593669654
target distance 13.0
model initialize at round 1618
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 26.]), 'previousTarget': array([ 4., 26.]), 'currentState': array([10.98569796, 12.98381601,  4.00428546]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 14.772305907228544}
done in step count: 53
reward sum = 0.40894691483729817
running average episode reward sum: 0.4659681286347903
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 26.]), 'previousTarget': array([ 4., 26.]), 'currentState': array([ 4.77683792, 25.0741306 ,  1.88999408]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 1.2085989000110458}
episode index:1619
target Thresh 31.99999761761304
target distance 15.0
model initialize at round 1619
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 11.]), 'previousTarget': array([17., 11.]), 'currentState': array([22.        , 26.        ,  5.74653622]), 'targetState': array([17, 11], dtype=int32), 'currentDistance': 15.811388300841898}
done in step count: 40
reward sum = 0.45016486838080505
running average episode reward sum: 0.4659583735358681
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 11.]), 'previousTarget': array([17., 11.]), 'currentState': array([17.08561139, 11.95303362,  4.12679269]), 'targetState': array([17, 11], dtype=int32), 'currentDistance': 0.9568711452415424}
episode index:1620
target Thresh 31.999997641318185
target distance 20.0
model initialize at round 1620
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 29.]), 'previousTarget': array([23., 29.]), 'currentState': array([23.        ,  9.        ,  3.38350374]), 'targetState': array([23, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.3663183312390477
running average episode reward sum: 0.46544493941817844
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 29.]), 'previousTarget': array([23., 29.]), 'currentState': array([23.216759  , 27.10469036,  1.58841795]), 'targetState': array([23, 29], dtype=int32), 'currentDistance': 1.9076643051264692}
episode index:1621
target Thresh 31.99999766478746
target distance 6.0
model initialize at round 1621
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([19.07405963,  9.92100483,  5.71805143]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 6.7465034872802}
done in step count: 15
reward sum = 0.7199301676520067
running average episode reward sum: 0.4656018353665347
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([21.56710785, 15.2093246 ,  1.44770154]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 0.9014228779925554}
episode index:1622
target Thresh 31.999997688023214
target distance 12.0
model initialize at round 1622
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([ 6.16696914, 20.66362593,  5.10872293]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 11.664820980309823}
done in step count: 24
reward sum = 0.5953179967946616
running average episode reward sum: 0.4656817590642723
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([5.5800416 , 9.95014279, 4.42406749]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 1.0388148893526226}
episode index:1623
target Thresh 31.99999771102777
target distance 9.0
model initialize at round 1623
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 16.]), 'previousTarget': array([ 6., 16.]), 'currentState': array([15.        , 23.        ,  5.39976203]), 'targetState': array([ 6, 16], dtype=int32), 'currentDistance': 11.40175425099138}
done in step count: 25
reward sum = 0.5756164383526571
running average episode reward sum: 0.465749452832307
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 16.]), 'previousTarget': array([ 6., 16.]), 'currentState': array([ 6.698755  , 16.45989342,  3.62075899]), 'targetState': array([ 6, 16], dtype=int32), 'currentDistance': 0.836516892842447}
episode index:1624
target Thresh 31.999997733803422
target distance 19.0
model initialize at round 1624
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.33317421, 13.31130219]), 'previousTarget': array([20.07475678, 13.43827311]), 'currentState': array([ 2.2107282 , 21.77161842,  5.44635682]), 'targetState': array([21, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.3299784020585522
running average episode reward sum: 0.46566590141644626
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 13.]), 'previousTarget': array([21., 13.]), 'currentState': array([20.19340997, 12.83115089,  5.87259308]), 'targetState': array([21, 13], dtype=int32), 'currentDistance': 0.8240737169598464}
episode index:1625
target Thresh 31.999997756352457
target distance 15.0
model initialize at round 1625
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.62110536, 25.64636501]), 'previousTarget': array([23.62110536, 25.64636501]), 'currentState': array([ 9.        , 12.        ,  5.28279901]), 'targetState': array([24, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.17100536467000702
running average episode reward sum: 0.4652743446722356
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 26.]), 'previousTarget': array([24., 26.]), 'currentState': array([17.88004493, 17.685063  ,  1.0105181 ]), 'targetState': array([24, 26], dtype=int32), 'currentDistance': 10.324341498587659}
episode index:1626
target Thresh 31.999997778677123
target distance 12.0
model initialize at round 1626
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  5.]), 'previousTarget': array([19.,  5.]), 'currentState': array([6.94107233, 1.9395249 , 4.19244981]), 'targetState': array([19,  5], dtype=int32), 'currentDistance': 12.441231624946903}
done in step count: 32
reward sum = 0.5134491110284756
running average episode reward sum: 0.46530395423975635
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  5.]), 'previousTarget': array([19.,  5.]), 'currentState': array([18.12584927,  4.8154022 ,  0.4665964 ]), 'targetState': array([19,  5], dtype=int32), 'currentDistance': 0.8934292587857681}
episode index:1627
target Thresh 31.999997800779653
target distance 24.0
model initialize at round 1627
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.28664033, 24.7229086 ]), 'previousTarget': array([11.28797975, 24.72787848]), 'currentState': array([7.99636691, 4.99541254, 3.80609477]), 'targetState': array([12, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 95
reward sum = 0.16660650937184868
running average episode reward sum: 0.46512047915077115
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 29.]), 'previousTarget': array([12., 29.]), 'currentState': array([11.82636658, 28.20054199,  1.49417626]), 'targetState': array([12, 29], dtype=int32), 'currentDistance': 0.8180963705493269}
episode index:1628
target Thresh 31.999997822662262
target distance 6.0
model initialize at round 1628
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([20.5637909, 16.2419191,  2.7563881]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 5.83494795783607}
done in step count: 11
reward sum = 0.7963571262633321
running average episode reward sum: 0.46532381656459104
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([15.91220132, 17.82978142,  2.67872781]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 0.9279469908770933}
episode index:1629
target Thresh 31.999997844327133
target distance 5.0
model initialize at round 1629
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([14.0666018 , 14.34124902,  1.23237777]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 6.142058001459721}
done in step count: 12
reward sum = 0.7877513407476028
running average episode reward sum: 0.46552162486163584
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([18.07428627, 17.42467243,  0.73377337]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 1.0899301480292778}
episode index:1630
target Thresh 31.99999786577644
target distance 26.0
model initialize at round 1630
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.4756956 , 20.62198534]), 'previousTarget': array([12.48199646, 20.609422  ]), 'currentState': array([2.97243031, 3.02403237, 2.5133003 ]), 'targetState': array([17, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.005856188027374392
running average episode reward sum: 0.46523261332706256
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.17380394, 20.90098603]), 'previousTarget': array([12.17757606, 20.89378121]), 'currentState': array([1.93574096, 3.72012266, 2.39896344]), 'targetState': array([17, 29], dtype=int32), 'currentDistance': 20.0}
episode index:1631
target Thresh 31.999997887012317
target distance 8.0
model initialize at round 1631
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([17.03017141,  9.63334704,  4.62758142]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 8.212790378701346}
done in step count: 17
reward sum = 0.7044504262344103
running average episode reward sum: 0.4653791928692852
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.94297318,  2.90649145,  4.21735049]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.908283444327303}
episode index:1632
target Thresh 31.999997908036896
target distance 20.0
model initialize at round 1632
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.89976701, 22.76887233]), 'previousTarget': array([20.89976701, 22.76887233]), 'currentState': array([10.        ,  6.        ,  5.24965039]), 'targetState': array([23, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 90
reward sum = 0.19859513781680999
running average episode reward sum: 0.46521582235180053
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 26.]), 'previousTarget': array([23., 26.]), 'currentState': array([22.96563466, 25.0648827 ,  1.19765607]), 'targetState': array([23, 26], dtype=int32), 'currentDistance': 0.9357485471034588}
episode index:1633
target Thresh 31.999997928852277
target distance 9.0
model initialize at round 1633
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 13.]), 'previousTarget': array([19., 13.]), 'currentState': array([10.33820875, 13.14336958,  0.29477029]), 'targetState': array([19, 13], dtype=int32), 'currentDistance': 8.66297768770972}
done in step count: 18
reward sum = 0.6959134762924115
running average episode reward sum: 0.4653570081865255
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 13.]), 'previousTarget': array([19., 13.]), 'currentState': array([18.17894537, 13.0250722 ,  0.06654521]), 'targetState': array([19, 13], dtype=int32), 'currentDistance': 0.8214373502709434}
episode index:1634
target Thresh 31.99999794946054
target distance 20.0
model initialize at round 1634
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.78000914, 14.89506974]), 'previousTarget': array([18.76887233, 14.89976701]), 'currentState': array([2.03954923, 3.9517155 , 5.65116072]), 'targetState': array([22, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.2557856206303005
running average episode reward sum: 0.46522882996783665
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 17.]), 'previousTarget': array([22., 17.]), 'currentState': array([21.09877829, 16.21192639,  0.4950305 ]), 'targetState': array([22, 17], dtype=int32), 'currentDistance': 1.1971886165502175}
episode index:1635
target Thresh 31.99999796986375
target distance 3.0
model initialize at round 1635
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([12.36805275,  4.31954918,  0.7044193 ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 3.7565892621090127}
done in step count: 8
reward sum = 0.8708949230468451
running average episode reward sum: 0.46547679212742044
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.64937089,  6.05143015,  0.75911976]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.0112989318704906}
episode index:1636
target Thresh 31.999997990063942
target distance 11.0
model initialize at round 1636
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([14.67526834, 16.07183026,  3.15929234]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 14.676618105534084}
done in step count: 40
reward sum = 0.4889314406442072
running average episode reward sum: 0.46549111995180453
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.54908607, 6.8512507 , 4.00360365]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.012977423193259}
episode index:1637
target Thresh 31.99999801006314
target distance 16.0
model initialize at round 1637
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 29.]), 'previousTarget': array([15., 29.]), 'currentState': array([17.95641553, 12.9742679 ,  3.48119229]), 'targetState': array([15, 29], dtype=int32), 'currentDistance': 16.296149298918397}
done in step count: 41
reward sum = 0.41350081971756314
running average episode reward sum: 0.46545937984177144
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 29.]), 'previousTarget': array([15., 29.]), 'currentState': array([15.3948874 , 28.29735621,  1.71486621]), 'targetState': array([15, 29], dtype=int32), 'currentDistance': 0.8060051796592683}
episode index:1638
target Thresh 31.99999802986334
target distance 11.0
model initialize at round 1638
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([11.01626091, 10.98117981,  5.6667088 ]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 13.599411812620342}
done in step count: 29
reward sum = 0.5184036602311403
running average episode reward sum: 0.46549168263639584
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([21.34030373, 18.07316807,  0.7788923 ]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 1.1376364044370308}
episode index:1639
target Thresh 31.99999804946653
target distance 10.0
model initialize at round 1639
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([10.72525149,  8.19402193,  2.40214498]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 10.88509106248293}
done in step count: 21
reward sum = 0.6347991149532637
running average episode reward sum: 0.4655949188756134
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([ 6.9074712 , 17.04142356,  1.80447736]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 1.319989690221081}
episode index:1640
target Thresh 31.999998068874664
target distance 11.0
model initialize at round 1640
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  5.]), 'previousTarget': array([13.,  5.]), 'currentState': array([20.99668872, 16.01906112,  1.99283215]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 13.614945405117423}
done in step count: 34
reward sum = 0.4960836782034176
running average episode reward sum: 0.4656134982536315
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  5.]), 'previousTarget': array([13.,  5.]), 'currentState': array([13.53529229,  5.94768068,  4.20579164]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 1.088410082901594}
episode index:1641
target Thresh 31.99999808808968
target distance 11.0
model initialize at round 1641
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 28.]), 'previousTarget': array([ 9., 28.]), 'currentState': array([13.70043783, 17.40032561,  2.21805057]), 'targetState': array([ 9, 28], dtype=int32), 'currentDistance': 11.595137475552654}
done in step count: 23
reward sum = 0.6032807762450566
running average episode reward sum: 0.46569733947043507
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 28.]), 'previousTarget': array([ 9., 28.]), 'currentState': array([ 9.3636631 , 27.12790404,  1.79110108]), 'targetState': array([ 9, 28], dtype=int32), 'currentDistance': 0.9448821192882892}
episode index:1642
target Thresh 31.999998107113505
target distance 5.0
model initialize at round 1642
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([ 6.99937656, 11.99899639,  4.39926931]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 5.3853710495669125}
done in step count: 14
reward sum = 0.7728574976358984
running average episode reward sum: 0.46588429026664047
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([11.13426182, 10.16831426,  6.06265641]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.8819480058181738}
episode index:1643
target Thresh 31.999998125948043
target distance 17.0
model initialize at round 1643
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 23.]), 'previousTarget': array([10., 23.]), 'currentState': array([16.71968431,  6.00452231,  3.13987113]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 18.27567834564515}
done in step count: 50
reward sum = 0.3531061258711663
running average episode reward sum: 0.4658156904099522
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 23.]), 'previousTarget': array([10., 23.]), 'currentState': array([10.14327847, 22.18920871,  1.76111256]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 0.8233536506467714}
episode index:1644
target Thresh 31.99999814459517
target distance 6.0
model initialize at round 1644
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 11.]), 'previousTarget': array([24., 11.]), 'currentState': array([25.00657648,  5.4991838 ,  1.65491761]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 5.59215298899225}
done in step count: 11
reward sum = 0.8003233404266427
running average episode reward sum: 0.46601903852546384
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 11.]), 'previousTarget': array([24., 11.]), 'currentState': array([24.13373481, 10.19728799,  1.56610256]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 0.8137761180001793}
episode index:1645
target Thresh 31.999998163056755
target distance 5.0
model initialize at round 1645
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 22.]), 'previousTarget': array([25., 22.]), 'currentState': array([21.9057573 , 17.16546211,  1.90549579]), 'targetState': array([25, 22], dtype=int32), 'currentDistance': 5.739955969736676}
done in step count: 11
reward sum = 0.8056784443418215
running average episode reward sum: 0.46622539296399135
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 22.]), 'previousTarget': array([25., 22.]), 'currentState': array([24.06429545, 21.04276438,  0.80228244]), 'targetState': array([25, 22], dtype=int32), 'currentDistance': 1.3385974161191831}
episode index:1646
target Thresh 31.999998181334647
target distance 8.0
model initialize at round 1646
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([14.34973434, 22.92278992,  5.9004086 ]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 9.731192232638435}
done in step count: 21
reward sum = 0.6608554421634041
running average episode reward sum: 0.46634356542859334
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([19.36526349, 15.7713411 ,  5.17429767]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 0.9989281915287193}
episode index:1647
target Thresh 31.99999819943067
target distance 6.0
model initialize at round 1647
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  5.]), 'previousTarget': array([19.,  5.]), 'currentState': array([25.        ,  7.        ,  1.24518621]), 'targetState': array([19,  5], dtype=int32), 'currentDistance': 6.324555320336759}
done in step count: 16
reward sum = 0.7286360711553164
running average episode reward sum: 0.46650272350245664
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  5.]), 'previousTarget': array([19.,  5.]), 'currentState': array([19.93536627,  5.58214959,  3.64978988]), 'targetState': array([19,  5], dtype=int32), 'currentDistance': 1.10172964423567}
episode index:1648
target Thresh 31.999998217346633
target distance 22.0
model initialize at round 1648
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.11214363,  9.77701066]), 'previousTarget': array([23.06407315,  9.9414844 ]), 'currentState': array([17.02152684, 28.82706019,  4.80454513]), 'targetState': array([24,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.3019362160961019
running average episode reward sum: 0.4664029257417493
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  7.]), 'previousTarget': array([24.,  7.]), 'currentState': array([23.16846423,  7.94031309,  5.20870383]), 'targetState': array([24,  7], dtype=int32), 'currentDistance': 1.2552451702419076}
episode index:1649
target Thresh 31.99999823508433
target distance 2.0
model initialize at round 1649
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([6.29545188, 9.07817118, 0.42725892]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 2.5688344248083297}
done in step count: 5
reward sum = 0.9196047804728343
running average episode reward sum: 0.46667759353249544
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.44360022, 10.17736788,  0.92720007]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.9931285530079024}
episode index:1650
target Thresh 31.999998252645536
target distance 17.0
model initialize at round 1650
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.43924472, 12.38808837]), 'previousTarget': array([20.33935727, 12.46633605]), 'currentState': array([ 3.99366035, 23.76977343,  4.92857817]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.327042960878417
running average episode reward sum: 0.4665930177404578
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([20.02914285, 12.17441392,  5.61554333]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 0.9863994236669372}
episode index:1651
target Thresh 31.999998270032002
target distance 4.0
model initialize at round 1651
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'previousTarget': array([10., 17.]), 'currentState': array([ 5.98851646, 14.01454573,  2.01223928]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 5.000493713018921}
done in step count: 11
reward sum = 0.8079255917663262
running average episode reward sum: 0.4667996355213452
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'previousTarget': array([10., 17.]), 'currentState': array([ 9.10461296, 16.10876809,  0.35675825]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 1.2633337905446194}
episode index:1652
target Thresh 31.99999828724547
target distance 4.0
model initialize at round 1652
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 28.]), 'previousTarget': array([ 2., 28.]), 'currentState': array([ 5.00128675, 24.00083914,  0.81009603]), 'targetState': array([ 2, 28], dtype=int32), 'currentDistance': 5.0001009721815874}
done in step count: 11
reward sum = 0.8085102639644766
running average episode reward sum: 0.46700635701465615
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 28.]), 'previousTarget': array([ 2., 28.]), 'currentState': array([ 2.9935316 , 27.25987559,  2.44727634]), 'targetState': array([ 2, 28], dtype=int32), 'currentDistance': 1.2389064458441865}
episode index:1653
target Thresh 31.999998304287665
target distance 15.0
model initialize at round 1653
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  3.]), 'previousTarget': array([11.,  3.]), 'currentState': array([ 4.93563685, 17.79743054,  4.63897318]), 'targetState': array([11,  3], dtype=int32), 'currentDistance': 15.99188704032116}
done in step count: 36
reward sum = 0.4676277713261075
running average episode reward sum: 0.46700673271859294
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  3.]), 'previousTarget': array([11.,  3.]), 'currentState': array([10.49739568,  3.92676623,  5.0860447 ]), 'targetState': array([11,  3], dtype=int32), 'currentDistance': 1.05428020328281}
episode index:1654
target Thresh 31.999998321160284
target distance 3.0
model initialize at round 1654
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'previousTarget': array([26., 11.]), 'currentState': array([25.97049658,  8.00518596,  2.82284239]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 2.9949593652621247}
done in step count: 8
reward sum = 0.8822158897535778
running average episode reward sum: 0.46725761438447505
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'previousTarget': array([26., 11.]), 'currentState': array([25.24326272, 10.07942356,  1.5962206 ]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 1.1916846459212478}
episode index:1655
target Thresh 31.99999833786502
target distance 13.0
model initialize at round 1655
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([3.32007279, 5.08256583, 0.23566344]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 13.011225007065692}
done in step count: 29
reward sum = 0.5585573635919776
running average episode reward sum: 0.46731274708327186
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.00232701,  8.11274084,  0.12787964]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.0040228496306522}
episode index:1656
target Thresh 31.999998354403537
target distance 17.0
model initialize at round 1656
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 25.]), 'previousTarget': array([22., 25.]), 'currentState': array([20.87421989,  8.42332097,  1.70706356]), 'targetState': array([22, 25], dtype=int32), 'currentDistance': 16.614862877655636}
done in step count: 41
reward sum = 0.44397821507391255
running average episode reward sum: 0.467298664686163
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 25.]), 'previousTarget': array([22., 25.]), 'currentState': array([21.69505818, 24.1421317 ,  1.66745924]), 'targetState': array([22, 25], dtype=int32), 'currentDistance': 0.910454575647279}
episode index:1657
target Thresh 31.999998370777497
target distance 18.0
model initialize at round 1657
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7.84573718, 8.84927025]), 'previousTarget': array([7.85786438, 8.85786438]), 'currentState': array([21.98137814, 22.99789756,  3.4524934 ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 20.0}
done in step count: 71
reward sum = 0.22833418527851373
running average episode reward sum: 0.46715453653211736
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.37841022, 5.92392375, 4.22183357]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.9984134342415754}
episode index:1658
target Thresh 31.99999838698853
target distance 22.0
model initialize at round 1658
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.84207203, 21.06649696]), 'previousTarget': array([17.73750984, 20.87322975]), 'currentState': array([7.05982086, 4.22182244, 1.09377193]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 79
reward sum = 0.20442975156817753
running average episode reward sum: 0.4669961731897642
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([21.39890718, 25.04770335,  1.43582592]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 1.0324707491905545}
episode index:1659
target Thresh 31.999998403038262
target distance 20.0
model initialize at round 1659
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.59865276,  6.14225521]), 'previousTarget': array([22.60700849,  6.12283287]), 'currentState': array([15.96872215, 25.01138396,  3.04411608]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.288327903631302
running average episode reward sum: 0.4668885417020784
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  5.]), 'previousTarget': array([23.,  5.]), 'currentState': array([22.07687713,  5.72990541,  5.15488767]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 1.1768252803258152}
episode index:1660
target Thresh 31.9999984189283
target distance 18.0
model initialize at round 1660
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6.52479606, 2.29314193]), 'previousTarget': array([6.51685448, 2.28714138]), 'currentState': array([23.98545712, 12.04636268,  2.10680926]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.3189659965783651
running average episode reward sum: 0.46679948538352106
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 2.]), 'previousTarget': array([6., 2.]), 'currentState': array([6.7917705 , 2.96149594, 3.71731926]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 1.2455420390131033}
episode index:1661
target Thresh 31.999998434660224
target distance 17.0
model initialize at round 1661
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  7.]), 'previousTarget': array([22.,  7.]), 'currentState': array([ 5.        , 10.        ,  3.99105871]), 'targetState': array([22,  7], dtype=int32), 'currentDistance': 17.26267650163207}
done in step count: 50
reward sum = 0.3750706997384825
running average episode reward sum: 0.46674429357507036
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  7.]), 'previousTarget': array([22.,  7.]), 'currentState': array([21.13778012,  6.85493332,  6.15869327]), 'targetState': array([22,  7], dtype=int32), 'currentDistance': 0.8743382989895858}
episode index:1662
target Thresh 31.999998450235616
target distance 12.0
model initialize at round 1662
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 16.]), 'previousTarget': array([ 5., 16.]), 'currentState': array([16.75588088, 24.62488318,  4.08583806]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 14.580443932153496}
done in step count: 33
reward sum = 0.4961449921875909
running average episode reward sum: 0.46676197288872795
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 16.]), 'previousTarget': array([ 5., 16.]), 'currentState': array([ 5.73760026, 16.09537648,  3.47437855]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 0.7437410952999665}
episode index:1663
target Thresh 31.999998465656027
target distance 7.0
model initialize at round 1663
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'previousTarget': array([10., 12.]), 'currentState': array([9.        , 5.        , 2.80094337]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 7.0710678118654755}
done in step count: 16
reward sum = 0.7205910554368378
running average episode reward sum: 0.46691451440468235
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'previousTarget': array([10., 12.]), 'currentState': array([10.12570216, 11.09721006,  1.21150255]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 0.9114991592768954}
episode index:1664
target Thresh 31.999998480923008
target distance 16.0
model initialize at round 1664
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 24.]), 'previousTarget': array([25., 24.]), 'currentState': array([ 9.32127444, 28.0464975 ,  6.2487624 ]), 'targetState': array([25, 24], dtype=int32), 'currentDistance': 16.192485205437066}
done in step count: 38
reward sum = 0.46044115931558516
running average episode reward sum: 0.4669106265037279
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 24.]), 'previousTarget': array([25., 24.]), 'currentState': array([24.03300438, 24.42745251,  5.8219405 ]), 'targetState': array([25, 24], dtype=int32), 'currentDistance': 1.0572587988811106}
episode index:1665
target Thresh 31.999998496038074
target distance 13.0
model initialize at round 1665
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  5.]), 'previousTarget': array([24.,  5.]), 'currentState': array([26.79874771, 17.9391864 ,  3.65856254]), 'targetState': array([24,  5], dtype=int32), 'currentDistance': 13.238411287203935}
done in step count: 33
reward sum = 0.5369650977221617
running average episode reward sum: 0.46695267600625995
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  5.]), 'previousTarget': array([24.,  5.]), 'currentState': array([24.04050192,  5.98876075,  4.53385858]), 'targetState': array([24,  5], dtype=int32), 'currentDistance': 0.9895899275851845}
episode index:1666
target Thresh 31.999998511002747
target distance 11.0
model initialize at round 1666
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 29.]), 'previousTarget': array([ 9., 29.]), 'currentState': array([18.        , 18.        ,  0.32149666]), 'targetState': array([ 9, 29], dtype=int32), 'currentDistance': 14.212670403551895}
done in step count: 33
reward sum = 0.48210868911056043
running average episode reward sum: 0.46696176779576465
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 29.]), 'previousTarget': array([ 9., 29.]), 'currentState': array([ 9.47075702, 28.02193863,  2.45404757]), 'targetState': array([ 9, 29], dtype=int32), 'currentDistance': 1.0854566816982802}
episode index:1667
target Thresh 31.999998525818516
target distance 7.0
model initialize at round 1667
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([ 9.        , 22.        ,  1.93183759]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 6.999999999999999}
done in step count: 19
reward sum = 0.7069514314111168
running average episode reward sum: 0.46710564649097763
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([15.24693419, 22.20077907,  0.0230837 ]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 0.7793717630661717}
episode index:1668
target Thresh 31.999998540486867
target distance 14.0
model initialize at round 1668
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 29.]), 'previousTarget': array([17., 29.]), 'currentState': array([10.05027271, 14.98422481,  6.20899707]), 'targetState': array([17, 29], dtype=int32), 'currentDistance': 15.644189454080571}
done in step count: 39
reward sum = 0.4577842617186663
running average episode reward sum: 0.4671000614791308
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 29.]), 'previousTarget': array([17., 29.]), 'currentState': array([16.61668577, 28.13877706,  0.82941577]), 'targetState': array([17, 29], dtype=int32), 'currentDistance': 0.9426742541856465}
episode index:1669
target Thresh 31.999998555009267
target distance 11.0
model initialize at round 1669
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([2.98289224, 9.7153602 , 4.88662502]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 11.983792601662111}
done in step count: 27
reward sum = 0.568601936447483
running average episode reward sum: 0.4671608410449802
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.15992143,  5.14269419,  5.78578533]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.8521112834166487}
episode index:1670
target Thresh 31.999998569387163
target distance 10.0
model initialize at round 1670
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 12.]), 'previousTarget': array([12., 12.]), 'currentState': array([11.01857604, 22.00590996,  0.05552351]), 'targetState': array([12, 12], dtype=int32), 'currentDistance': 10.053925954685608}
done in step count: 25
reward sum = 0.6105465632953883
running average episode reward sum: 0.46724664937666804
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 12.]), 'previousTarget': array([12., 12.]), 'currentState': array([11.36982444, 12.7773287 ,  4.5837678 ]), 'targetState': array([12, 12], dtype=int32), 'currentDistance': 1.0006803396127013}
episode index:1671
target Thresh 31.999998583622
target distance 18.0
model initialize at round 1671
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 23.]), 'previousTarget': array([ 7., 23.]), 'currentState': array([25.0323957 , 23.04028002,  1.10584798]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 18.03244068816568}
done in step count: 49
reward sum = 0.36664908100823995
running average episode reward sum: 0.46718648336687835
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 23.]), 'previousTarget': array([ 7., 23.]), 'currentState': array([ 7.93957755, 23.2015486 ,  2.9092243 ]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 0.9609515134893938}
episode index:1672
target Thresh 31.999998597715194
target distance 8.0
model initialize at round 1672
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 16.]), 'previousTarget': array([24., 16.]), 'currentState': array([16.05250737, 20.04914713,  0.51400718]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 8.919542114553604}
done in step count: 21
reward sum = 0.6751270260795574
running average episode reward sum: 0.46731077538284527
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 16.]), 'previousTarget': array([24., 16.]), 'currentState': array([23.42437863, 16.9605672 ,  5.53530623]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 1.1198344989455697}
episode index:1673
target Thresh 31.99999861166816
target distance 5.0
model initialize at round 1673
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([8.        , 4.        , 4.19887039]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 5.099019513592785}
done in step count: 23
reward sum = 0.7083304204189034
running average episode reward sum: 0.4674547536654235
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([8.0117099 , 8.63069002, 1.25870962]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 1.0550389497983268}
episode index:1674
target Thresh 31.999998625482295
target distance 16.0
model initialize at round 1674
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9.85786438, 24.14213562]), 'previousTarget': array([ 9.85786438, 24.14213562]), 'currentState': array([24.        , 10.        ,  1.28825861]), 'targetState': array([ 8, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.15055453696167198
running average episode reward sum: 0.4670857928948999
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 26.]), 'previousTarget': array([ 8., 26.]), 'currentState': array([10.49280095, 23.4670888 ,  2.36170815]), 'targetState': array([ 8, 26], dtype=int32), 'currentDistance': 3.5538283194202034}
episode index:1675
target Thresh 31.999998639158974
target distance 24.0
model initialize at round 1675
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.45691698,  9.09171686]), 'previousTarget': array([21.48069469,  9.15444247]), 'currentState': array([18.82544615, 28.91784511,  3.83399105]), 'targetState': array([22,  5], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 99
reward sum = -0.14356776702009025
running average episode reward sum: 0.46672144112884084
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  5.]), 'previousTarget': array([22.,  5.]), 'currentState': array([20.87887119,  9.24847736,  5.05746589]), 'targetState': array([22,  5], dtype=int32), 'currentDistance': 4.393915078687466}
episode index:1676
target Thresh 31.99999865269957
target distance 18.0
model initialize at round 1676
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 28.]), 'previousTarget': array([19., 28.]), 'currentState': array([26.        , 10.        ,  0.79596183]), 'targetState': array([19, 28], dtype=int32), 'currentDistance': 19.31320791582797}
done in step count: 58
reward sum = 0.35040328350977445
running average episode reward sum: 0.4666520802715844
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 28.]), 'previousTarget': array([19., 28.]), 'currentState': array([19.88604548, 27.01872595,  1.65572303]), 'targetState': array([19, 28], dtype=int32), 'currentDistance': 1.3221101855594624}
episode index:1677
target Thresh 31.999998666105434
target distance 7.0
model initialize at round 1677
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 25.]), 'previousTarget': array([18., 25.]), 'currentState': array([10.98378813, 21.99761387,  3.03522778]), 'targetState': array([18, 25], dtype=int32), 'currentDistance': 7.6316152619384985}
done in step count: 23
reward sum = 0.6582491636563592
running average episode reward sum: 0.4667662620852821
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 25.]), 'previousTarget': array([18., 25.]), 'currentState': array([17.04650067, 24.07060235,  0.07517818]), 'targetState': array([18, 25], dtype=int32), 'currentDistance': 1.3315182939179069}
episode index:1678
target Thresh 31.999998679377907
target distance 6.0
model initialize at round 1678
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  5.]), 'previousTarget': array([13.,  5.]), 'currentState': array([19.        ,  9.        ,  1.61149901]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 7.211102550927978}
done in step count: 21
reward sum = 0.6772264515883895
running average episode reward sum: 0.4668916106198283
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  5.]), 'previousTarget': array([13.,  5.]), 'currentState': array([13.33788633,  5.98547144,  3.66137058]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 1.0417874713346273}
episode index:1679
target Thresh 31.999998692518314
target distance 14.0
model initialize at round 1679
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([3.        , 9.        , 1.59789133]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 14.866068747318504}
done in step count: 38
reward sum = 0.44513300690060104
running average episode reward sum: 0.4668786590699955
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([16.10234186,  3.91767772,  5.84817855]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 0.901425039155377}
episode index:1680
target Thresh 31.999998705527975
target distance 9.0
model initialize at round 1680
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 15.]), 'previousTarget': array([10., 15.]), 'currentState': array([9.        , 6.        , 3.61756086]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 9.055385138137417}
done in step count: 24
reward sum = 0.6313518067302186
running average episode reward sum: 0.4669765015135768
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 15.]), 'previousTarget': array([10., 15.]), 'currentState': array([ 9.26542867, 14.15608789,  1.49670971]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 1.1188309450193956}
episode index:1681
target Thresh 31.999998718408186
target distance 13.0
model initialize at round 1681
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'previousTarget': array([16., 23.]), 'currentState': array([ 2.92050817, 17.98724529,  3.11594987]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 14.007170171009921}
done in step count: 39
reward sum = 0.4538883162801365
running average episode reward sum: 0.46696872019060803
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'previousTarget': array([16., 23.]), 'currentState': array([15.0387479 , 22.69339171,  6.27814204]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 1.0089669170068212}
episode index:1682
target Thresh 31.99999873116024
target distance 12.0
model initialize at round 1682
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([26.0215015 , 13.01013902,  0.39700085]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 12.392636329723071}
done in step count: 37
reward sum = 0.4919163412642229
running average episode reward sum: 0.46698354349487037
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.86630476, 10.63902275,  3.0401785 ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.0764915264977977}
episode index:1683
target Thresh 31.999998743785405
target distance 15.0
model initialize at round 1683
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 29.]), 'previousTarget': array([ 6., 29.]), 'currentState': array([ 9.73332587, 14.19700174,  2.3799206 ]), 'targetState': array([ 6, 29], dtype=int32), 'currentDistance': 15.266514974909748}
done in step count: 33
reward sum = 0.4695129112148651
running average episode reward sum: 0.46698504549470415
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 29.]), 'previousTarget': array([ 6., 29.]), 'currentState': array([ 5.81879404, 28.0346727 ,  1.71709198]), 'targetState': array([ 6, 29], dtype=int32), 'currentDistance': 0.982187557385296}
episode index:1684
target Thresh 31.99999875628495
target distance 11.0
model initialize at round 1684
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([19.        , 10.        ,  5.21384001]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 13.038404810405298}
done in step count: 45
reward sum = 0.45647699654003193
running average episode reward sum: 0.4669788092638705
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([26.25354855, 20.00163519,  0.73062906]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 1.0300578414631334}
episode index:1685
target Thresh 31.99999876866012
target distance 11.0
model initialize at round 1685
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  4.]), 'previousTarget': array([12.,  4.]), 'currentState': array([23.        , 14.        ,  5.46870375]), 'targetState': array([12,  4], dtype=int32), 'currentDistance': 14.866068747318506}
done in step count: 36
reward sum = 0.4647407107161192
running average episode reward sum: 0.46697748180328463
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  4.]), 'previousTarget': array([12.,  4.]), 'currentState': array([12.15393392,  4.86993712,  3.8050741 ]), 'targetState': array([12,  4], dtype=int32), 'currentDistance': 0.8834513222515734}
episode index:1686
target Thresh 31.999998780912158
target distance 5.0
model initialize at round 1686
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 20.]), 'previousTarget': array([ 5., 20.]), 'currentState': array([ 5.15042153, 15.0278596 ,  0.41034162]), 'targetState': array([ 5, 20], dtype=int32), 'currentDistance': 4.974415225333744}
done in step count: 11
reward sum = 0.7988803887908569
running average episode reward sum: 0.46717422330120256
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 20.]), 'previousTarget': array([ 5., 20.]), 'currentState': array([ 5.16504866, 19.25361943,  1.83274775]), 'targetState': array([ 5, 20], dtype=int32), 'currentDistance': 0.7644115532607547}
episode index:1687
target Thresh 31.999998793042284
target distance 9.0
model initialize at round 1687
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 25.]), 'previousTarget': array([15., 25.]), 'currentState': array([23.89195316, 27.14531909,  2.46263373]), 'targetState': array([15, 25], dtype=int32), 'currentDistance': 9.147088334260518}
done in step count: 23
reward sum = 0.6577039402255305
running average episode reward sum: 0.4672870963562525
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 25.]), 'previousTarget': array([15., 25.]), 'currentState': array([15.94425586, 25.2367293 ,  3.63323409]), 'targetState': array([15, 25], dtype=int32), 'currentDistance': 0.9734782394890902}
episode index:1688
target Thresh 31.999998805051714
target distance 18.0
model initialize at round 1688
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.54026305, 23.73247066]), 'previousTarget': array([18.54026305, 23.73247066]), 'currentState': array([ 4.        , 10.        ,  2.10895017]), 'targetState': array([22, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 78
reward sum = 0.2188603014957597
running average episode reward sum: 0.4671400112201598
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 27.]), 'previousTarget': array([22., 27.]), 'currentState': array([21.08570097, 26.56416328,  0.93088983]), 'targetState': array([22, 27], dtype=int32), 'currentDistance': 1.0128654246796176}
episode index:1689
target Thresh 31.99999881694165
target distance 11.0
model initialize at round 1689
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 15.]), 'previousTarget': array([ 7., 15.]), 'currentState': array([13.        ,  4.        ,  3.91060278]), 'targetState': array([ 7, 15], dtype=int32), 'currentDistance': 12.52996408614167}
done in step count: 32
reward sum = 0.5271426793880898
running average episode reward sum: 0.46717551575753735
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 15.]), 'previousTarget': array([ 7., 15.]), 'currentState': array([ 7.57331355, 14.19884185,  1.88904152]), 'targetState': array([ 7, 15], dtype=int32), 'currentDistance': 0.9851613098743878}
episode index:1690
target Thresh 31.999998828713274
target distance 8.0
model initialize at round 1690
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 18.]), 'previousTarget': array([ 7., 18.]), 'currentState': array([15.        , 13.        ,  0.79950264]), 'targetState': array([ 7, 18], dtype=int32), 'currentDistance': 9.433981132056603}
done in step count: 23
reward sum = 0.6284455156682032
running average episode reward sum: 0.46727088536126926
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 18.]), 'previousTarget': array([ 7., 18.]), 'currentState': array([ 7.78872385, 17.57158811,  2.63072451]), 'targetState': array([ 7, 18], dtype=int32), 'currentDistance': 0.8975645159704795}
episode index:1691
target Thresh 31.999998840367773
target distance 9.0
model initialize at round 1691
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 10.]), 'previousTarget': array([27., 10.]), 'currentState': array([18.35707314,  4.22889514,  0.560447  ]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 10.392585629975198}
done in step count: 23
reward sum = 0.6320210914265589
running average episode reward sum: 0.46736825545941657
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 10.]), 'previousTarget': array([27., 10.]), 'currentState': array([26.77658045,  9.06529895,  0.60383821]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 0.9610319211055726}
episode index:1692
target Thresh 31.999998851906305
target distance 24.0
model initialize at round 1692
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8.04783105, 12.32279762]), 'previousTarget': array([ 8.02633404, 12.32455532]), 'currentState': array([27.02758836,  6.01654574,  0.79272562]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.04355018487049844
running average episode reward sum: 0.46706647256495115
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5.01036251, 13.45948369]), 'previousTarget': array([ 5.1045882 , 13.43271182]), 'currentState': array([24.32444954,  8.26659986,  2.76220045]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 20.0}
episode index:1693
target Thresh 31.999998863330028
target distance 19.0
model initialize at round 1693
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.70632169, 24.50614522]), 'previousTarget': array([12.70632169, 24.50614522]), 'currentState': array([24.        ,  8.        ,  4.88778663]), 'targetState': array([11, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 87
reward sum = 0.1677821413463385
running average episode reward sum: 0.4668897994060264
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 27.]), 'previousTarget': array([11., 27.]), 'currentState': array([11.57697297, 26.05866471,  2.06518903]), 'targetState': array([11, 27], dtype=int32), 'currentDistance': 1.1040878277574757}
episode index:1694
target Thresh 31.999998874640085
target distance 5.0
model initialize at round 1694
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 24.]), 'previousTarget': array([13., 24.]), 'currentState': array([ 7.99982343, 23.00062713,  1.63182446]), 'targetState': array([13, 24], dtype=int32), 'currentDistance': 5.099069704233627}
done in step count: 30
reward sum = 0.631739743322183
running average episode reward sum: 0.46698705601010676
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 24.]), 'previousTarget': array([13., 24.]), 'currentState': array([13.81759212, 24.95588495,  5.11141849]), 'targetState': array([13, 24], dtype=int32), 'currentDistance': 1.2578445497395865}
episode index:1695
target Thresh 31.999998885837602
target distance 14.0
model initialize at round 1695
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 26.]), 'previousTarget': array([12., 26.]), 'currentState': array([25.95054072, 12.96380738,  3.62880617]), 'targetState': array([12, 26], dtype=int32), 'currentDistance': 19.093451871853592}
done in step count: 50
reward sum = 0.3586474280750451
running average episode reward sum: 0.46692317651250353
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 26.]), 'previousTarget': array([12., 26.]), 'currentState': array([12.76263603, 25.35145544,  2.71817191]), 'targetState': array([12, 26], dtype=int32), 'currentDistance': 1.0011112691964288}
episode index:1696
target Thresh 31.999998896923703
target distance 9.0
model initialize at round 1696
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 25.]), 'previousTarget': array([11., 25.]), 'currentState': array([10.01353103, 16.01479778,  1.02456665]), 'targetState': array([11, 25], dtype=int32), 'currentDistance': 9.03919133337849}
done in step count: 19
reward sum = 0.6878876458544656
running average episode reward sum: 0.4670533853924929
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 25.]), 'previousTarget': array([11., 25.]), 'currentState': array([10.33966252, 24.04540209,  1.36414049]), 'targetState': array([11, 25], dtype=int32), 'currentDistance': 1.1607337150491592}
episode index:1697
target Thresh 31.999998907899496
target distance 23.0
model initialize at round 1697
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5.00847935, 26.36007791]), 'previousTarget': array([ 5.16799178, 26.41321632]), 'currentState': array([24.86674888, 28.73686807,  4.12390178]), 'targetState': array([ 2, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.08785291788957512
running average episode reward sum: 0.46672658544945284
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 26.]), 'previousTarget': array([ 2., 26.]), 'currentState': array([10.77069975, 26.59082293,  3.22744315]), 'targetState': array([ 2, 26], dtype=int32), 'currentDistance': 8.790577109836613}
episode index:1698
target Thresh 31.999998918766078
target distance 15.0
model initialize at round 1698
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 24.]), 'previousTarget': array([23., 24.]), 'currentState': array([12.99645401,  8.99966428,  2.98445874]), 'targetState': array([23, 24], dtype=int32), 'currentDistance': 18.030002884756897}
done in step count: 56
reward sum = 0.35203258325008224
running average episode reward sum: 0.4666590786794709
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 24.]), 'previousTarget': array([23., 24.]), 'currentState': array([22.43835349, 23.02835856,  0.60868186]), 'targetState': array([23, 24], dtype=int32), 'currentDistance': 1.1222895718541552}
episode index:1699
target Thresh 31.999998929524534
target distance 9.0
model initialize at round 1699
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([17.09745039, 10.03676505,  0.57238735]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 9.205372287416012}
done in step count: 25
reward sum = 0.6444148585621365
running average episode reward sum: 0.4667636409029313
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([15.7961734 , 18.1434761 ,  1.83824975]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 1.1694123659080298}
episode index:1700
target Thresh 31.999998940175946
target distance 12.0
model initialize at round 1700
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 15.]), 'previousTarget': array([ 3., 15.]), 'currentState': array([12.        , 27.        ,  1.53617352]), 'targetState': array([ 3, 15], dtype=int32), 'currentDistance': 15.000000000000002}
done in step count: 41
reward sum = 0.4431050392904843
running average episode reward sum: 0.46674973226000804
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 15.]), 'previousTarget': array([ 3., 15.]), 'currentState': array([ 3.37721754, 15.92285715,  4.22182451]), 'targetState': array([ 3, 15], dtype=int32), 'currentDistance': 0.996974617202611}
episode index:1701
target Thresh 31.99999895072137
target distance 9.0
model initialize at round 1701
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([ 6.12092268, 12.90924743,  5.87789562]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 10.234929204487281}
done in step count: 24
reward sum = 0.6326475049115838
running average episode reward sum: 0.46684720451185974
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([14.11400176, 17.45074972,  0.47119723]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 1.042434055783231}
episode index:1702
target Thresh 31.999998961161868
target distance 4.0
model initialize at round 1702
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  7.]), 'previousTarget': array([13.,  7.]), 'currentState': array([9.20426115, 2.90399031, 6.05308709]), 'targetState': array([13,  7], dtype=int32), 'currentDistance': 5.584346763317156}
done in step count: 14
reward sum = 0.7818999965140944
running average episode reward sum: 0.4670322032153255
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  7.]), 'previousTarget': array([13.,  7.]), 'currentState': array([12.63940335,  6.04738914,  0.84333009]), 'targetState': array([13,  7], dtype=int32), 'currentDistance': 1.0185761642701137}
episode index:1703
target Thresh 31.999998971498478
target distance 2.0
model initialize at round 1703
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  9.]), 'previousTarget': array([19.,  9.]), 'currentState': array([21.00203614,  7.99768608,  5.34798945]), 'targetState': array([19,  9], dtype=int32), 'currentDistance': 2.2389242725650744}
done in step count: 15
reward sum = 0.8165726352755095
running average episode reward sum: 0.4672373325768632
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  9.]), 'previousTarget': array([19.,  9.]), 'currentState': array([19.08512331,  8.12260544,  2.31619966]), 'targetState': array([19,  9], dtype=int32), 'currentDistance': 0.8815141520668304}
episode index:1704
target Thresh 31.99999898173224
target distance 4.0
model initialize at round 1704
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 2.]), 'previousTarget': array([6., 2.]), 'currentState': array([2.14156065, 1.89680439, 5.74809045]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 3.859819108880972}
done in step count: 9
reward sum = 0.8659623822345167
running average episode reward sum: 0.4674711889109733
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 2.]), 'previousTarget': array([6., 2.]), 'currentState': array([5.20790466, 1.64792296, 0.18039034]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 0.866817902301282}
episode index:1705
target Thresh 31.999998991864175
target distance 5.0
model initialize at round 1705
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([10.87816697,  9.83507236,  3.89115375]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 5.015333425199149}
done in step count: 11
reward sum = 0.8095755520116142
running average episode reward sum: 0.46767171901830074
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.95475203, 10.83439363,  2.52720411]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.9690082076598611}
episode index:1706
target Thresh 31.999999001895294
target distance 18.0
model initialize at round 1706
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.58781834,  2.96021108]), 'previousTarget': array([11.57099981,  2.93436333]), 'currentState': array([22.0300428 , 20.01775922,  0.28137231]), 'targetState': array([11,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.2790948584786502
running average episode reward sum: 0.4675612463407731
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  2.]), 'previousTarget': array([11.,  2.]), 'currentState': array([11.31209678,  2.92735661,  3.75090147]), 'targetState': array([11,  2], dtype=int32), 'currentDistance': 0.9784654730637763}
episode index:1707
target Thresh 31.9999990118266
target distance 18.0
model initialize at round 1707
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  2.]), 'previousTarget': array([22.,  2.]), 'currentState': array([16.03646325, 20.00007276,  6.23503872]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 18.962235886323427}
done in step count: 56
reward sum = 0.3736380809758885
running average episode reward sum: 0.46750625619711683
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  2.]), 'previousTarget': array([22.,  2.]), 'currentState': array([21.80386945,  2.8340027 ,  4.85428988]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 0.8567541663111746}
episode index:1708
target Thresh 31.99999902165909
target distance 22.0
model initialize at round 1708
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.70463318, 22.86455523]), 'previousTarget': array([20.70226409, 22.81660336]), 'currentState': array([17.96439532,  3.05316718,  2.11161322]), 'targetState': array([21, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.3083118736452043
running average episode reward sum: 0.467413105592932
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 25.]), 'previousTarget': array([21., 25.]), 'currentState': array([21.31086285, 24.18598705,  1.24279582]), 'targetState': array([21, 25], dtype=int32), 'currentDistance': 0.8713511261121833}
episode index:1709
target Thresh 31.999999031393745
target distance 9.0
model initialize at round 1709
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([ 3.87017752, 16.59157528,  4.39973013]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 8.792765764401803}
done in step count: 23
reward sum = 0.6828471133855198
running average episode reward sum: 0.46753909039281066
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.7805576 , 8.84800196, 4.29897817]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.8759350988604834}
episode index:1710
target Thresh 31.999999041031536
target distance 9.0
model initialize at round 1710
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 14.]), 'previousTarget': array([ 7., 14.]), 'currentState': array([12.01721058,  5.01177254,  0.82676446]), 'targetState': array([ 7, 14], dtype=int32), 'currentDistance': 10.293718223619544}
done in step count: 27
reward sum = 0.6185263132302387
running average episode reward sum: 0.46762733540908036
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 14.]), 'previousTarget': array([ 7., 14.]), 'currentState': array([ 7.79697248, 13.24510599,  2.18476865]), 'targetState': array([ 7, 14], dtype=int32), 'currentDistance': 1.0977386313162274}
episode index:1711
target Thresh 31.999999050573432
target distance 21.0
model initialize at round 1711
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5.35322867, 7.25775784]), 'previousTarget': array([5.35322867, 7.25775784]), 'currentState': array([25.        , 11.        ,  2.31938586]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.30836330371714094
running average episode reward sum: 0.4675343073531855
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.86799284, 7.19894581, 3.24251587]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.8905004271908841}
episode index:1712
target Thresh 31.999999060020386
target distance 15.0
model initialize at round 1712
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  9.]), 'previousTarget': array([22.,  9.]), 'currentState': array([13.        , 24.        ,  0.43821219]), 'targetState': array([22,  9], dtype=int32), 'currentDistance': 17.4928556845359}
done in step count: 49
reward sum = 0.3942469601827151
running average episode reward sum: 0.4674915243133896
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  9.]), 'previousTarget': array([22.,  9.]), 'currentState': array([21.30900756,  9.82217024,  5.06936232]), 'targetState': array([22,  9], dtype=int32), 'currentDistance': 1.0739806518844313}
episode index:1713
target Thresh 31.999999069373338
target distance 13.0
model initialize at round 1713
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 29.]), 'previousTarget': array([ 7., 29.]), 'currentState': array([20.        , 19.        ,  0.52645875]), 'targetState': array([ 7, 29], dtype=int32), 'currentDistance': 16.401219466856723}
done in step count: 40
reward sum = 0.4292307507043703
running average episode reward sum: 0.4674692018083668
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 29.]), 'previousTarget': array([ 7., 29.]), 'currentState': array([ 7.94882112, 28.15946692,  2.41713608]), 'targetState': array([ 7, 29], dtype=int32), 'currentDistance': 1.2675793372262536}
episode index:1714
target Thresh 31.99999907863323
target distance 8.0
model initialize at round 1714
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 17.]), 'previousTarget': array([12., 17.]), 'currentState': array([2.00601079e+01, 2.10162310e+01, 1.92691684e-02]), 'targetState': array([12, 17], dtype=int32), 'currentDistance': 9.00530123958741}
done in step count: 26
reward sum = 0.6123580532084456
running average episode reward sum: 0.46755368510364387
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 17.]), 'previousTarget': array([12., 17.]), 'currentState': array([12.85196404, 17.64151531,  3.24134144]), 'targetState': array([12, 17], dtype=int32), 'currentDistance': 1.0664823535212107}
episode index:1715
target Thresh 31.99999908780098
target distance 7.0
model initialize at round 1715
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([12.02870316, 28.98543425,  5.56108618]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 6.985493215792847}
done in step count: 16
reward sum = 0.7344384160928766
running average episode reward sum: 0.4677092123361551
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([11.83850608, 22.73863345,  4.63180608]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 0.7560817786315976}
episode index:1716
target Thresh 31.999999096877513
target distance 12.0
model initialize at round 1716
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([20.9513349 , 15.09718268,  2.16049208]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 14.902837389493754}
done in step count: 37
reward sum = 0.5009354943559348
running average episode reward sum: 0.4677285636943495
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([ 9.935174  , 23.62509134,  2.39603162]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 1.0075251387252513}
episode index:1717
target Thresh 31.999999105863733
target distance 7.0
model initialize at round 1717
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([10.6357552 , 24.00525117,  3.2151942 ]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 6.9321193939640215}
done in step count: 15
reward sum = 0.7501569009824046
running average episode reward sum: 0.46789295737146713
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([ 4.88609913, 21.61697862,  3.65743215]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 0.9653377835837608}
episode index:1718
target Thresh 31.999999114760538
target distance 1.0
model initialize at round 1718
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([25.01439396,  2.99156762,  5.64424758]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 1.4185208742122206}
done in step count: 12
reward sum = 0.868649119624276
running average episode reward sum: 0.46812609068284167
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([24.91590476,  1.72164684,  3.97140835]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 0.9572679924899803}
episode index:1719
target Thresh 31.999999123568816
target distance 8.0
model initialize at round 1719
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 15.]), 'previousTarget': array([18., 15.]), 'currentState': array([ 9.99928363, 14.97836983,  4.93178225]), 'targetState': array([18, 15], dtype=int32), 'currentDistance': 8.000745604944123}
done in step count: 20
reward sum = 0.6826560026643949
running average episode reward sum: 0.4682508173758542
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 15.]), 'previousTarget': array([18., 15.]), 'currentState': array([1.72635972e+01, 1.47776581e+01, 1.67897611e-02]), 'targetState': array([18, 15], dtype=int32), 'currentDistance': 0.7692366211635121}
episode index:1720
target Thresh 31.999999132289453
target distance 13.0
model initialize at round 1720
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 15.]), 'previousTarget': array([18., 15.]), 'currentState': array([11.2912461 ,  2.26229933,  0.77631172]), 'targetState': array([18, 15], dtype=int32), 'currentDistance': 14.396402230080872}
done in step count: 31
reward sum = 0.5219932011415098
running average episode reward sum: 0.46828204479233626
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 15.]), 'previousTarget': array([18., 15.]), 'currentState': array([17.37005428, 14.16402478,  1.0783591 ]), 'targetState': array([18, 15], dtype=int32), 'currentDistance': 1.0467502972293143}
episode index:1721
target Thresh 31.999999140923318
target distance 7.0
model initialize at round 1721
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 13.]), 'previousTarget': array([26., 13.]), 'currentState': array([22.06054425,  6.02275252,  0.55246162]), 'targetState': array([26, 13], dtype=int32), 'currentDistance': 8.012570994687993}
done in step count: 19
reward sum = 0.7113003130073319
running average episode reward sum: 0.4684231703836342
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 13.]), 'previousTarget': array([26., 13.]), 'currentState': array([25.87312945, 12.2529179 ,  1.09420645]), 'targetState': array([26, 13], dtype=int32), 'currentDistance': 0.7577781957791045}
episode index:1722
target Thresh 31.99999914947127
target distance 23.0
model initialize at round 1722
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.12137663,  9.42672931]), 'previousTarget': array([23.04843794,  9.64765455]), 'currentState': array([18.15400318, 28.80004088,  5.30923449]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.258828172397154
running average episode reward sum: 0.46830152499884803
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  6.]), 'previousTarget': array([24.,  6.]), 'currentState': array([23.82480296,  6.81196925,  4.46731099]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 0.8306551990231393}
episode index:1723
target Thresh 31.999999157934173
target distance 22.0
model initialize at round 1723
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.06407315,  7.9414844 ]), 'previousTarget': array([18.06407315,  7.9414844 ]), 'currentState': array([12.        , 27.        ,  1.24509084]), 'targetState': array([19,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.22616804267372664
running average episode reward sum: 0.468161076343207
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  5.]), 'previousTarget': array([19.,  5.]), 'currentState': array([18.56522629,  5.74386026,  4.45435884]), 'targetState': array([19,  5], dtype=int32), 'currentDistance': 0.861600992257145}
episode index:1724
target Thresh 31.99999916631287
target distance 6.0
model initialize at round 1724
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 22.]), 'previousTarget': array([21., 22.]), 'currentState': array([23.04819559, 27.96479706,  5.41400608]), 'targetState': array([21, 22], dtype=int32), 'currentDistance': 6.3066559360413335}
done in step count: 16
reward sum = 0.7554840690569173
running average episode reward sum: 0.4683276403969541
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 22.]), 'previousTarget': array([21., 22.]), 'currentState': array([21.07437107, 22.86180792,  4.17508249]), 'targetState': array([21, 22], dtype=int32), 'currentDistance': 0.8650109498761671}
episode index:1725
target Thresh 31.999999174608195
target distance 17.0
model initialize at round 1725
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2.24685237, 25.83972634]), 'previousTarget': array([ 2.20859686, 25.86502556]), 'currentState': array([19.02132362, 14.94857789,  4.85298443]), 'targetState': array([ 2, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.30593369684236027
running average episode reward sum: 0.46823355352351576
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 26.]), 'previousTarget': array([ 2., 26.]), 'currentState': array([ 2.97276019, 25.37162628,  2.13700636]), 'targetState': array([ 2, 26], dtype=int32), 'currentDistance': 1.1580655953163121}
episode index:1726
target Thresh 31.999999182820982
target distance 19.0
model initialize at round 1726
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([14.        , 28.        ,  6.01315847]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 19.1049731745428}
done in step count: 49
reward sum = 0.36996989410170966
running average episode reward sum: 0.46817665505251294
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.99880244,  9.84403875,  4.82765391]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.8440396041545567}
episode index:1727
target Thresh 31.99999919095205
target distance 8.0
model initialize at round 1727
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([26.8060423 , 18.03497593,  2.81483158]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 9.856558002507603}
done in step count: 24
reward sum = 0.6397993141488202
running average episode reward sum: 0.46827597372097146
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([20.55139724, 25.03795428,  2.17145698]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 1.0614972486311602}
episode index:1728
target Thresh 31.99999919900221
target distance 12.0
model initialize at round 1728
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([26.66361255, 12.75568567,  3.80817799]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 12.595888375256143}
done in step count: 27
reward sum = 0.5764583527940372
running average episode reward sum: 0.468338543055311
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.99346704,  8.68833434,  3.47386906]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.2086277051825902}
episode index:1729
target Thresh 31.99999920697227
target distance 12.0
model initialize at round 1729
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 23.]), 'previousTarget': array([15., 23.]), 'currentState': array([26.79790878, 25.82591374,  3.75071289]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 12.131629739194622}
done in step count: 28
reward sum = 0.5852980427797858
running average episode reward sum: 0.46840614970255057
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 23.]), 'previousTarget': array([15., 23.]), 'currentState': array([15.74775292, 23.41171344,  3.20044799]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 0.8536055250245611}
episode index:1730
target Thresh 31.999999214863028
target distance 14.0
model initialize at round 1730
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 26.]), 'previousTarget': array([25., 26.]), 'currentState': array([17.21489642, 12.28426645,  0.92400522]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 15.771150390347888}
done in step count: 39
reward sum = 0.4751244798930212
running average episode reward sum: 0.4684100308869471
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 26.]), 'previousTarget': array([25., 26.]), 'currentState': array([24.90664058, 25.0383322 ,  0.9520675 ]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 0.9661888745645285}
episode index:1731
target Thresh 31.99999922267527
target distance 20.0
model initialize at round 1731
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 24.]), 'previousTarget': array([16., 24.]), 'currentState': array([15.95688219,  4.24628373,  1.72717012]), 'targetState': array([16, 24], dtype=int32), 'currentDistance': 19.753763330111045}
done in step count: 48
reward sum = 0.37246136095299515
running average episode reward sum: 0.46835463327151183
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 24.]), 'previousTarget': array([16., 24.]), 'currentState': array([15.91679442, 23.27066494,  1.34144896]), 'targetState': array([16, 24], dtype=int32), 'currentDistance': 0.7340659352646914}
episode index:1732
target Thresh 31.99999923040978
target distance 17.0
model initialize at round 1732
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  5.]), 'previousTarget': array([23.,  5.]), 'currentState': array([ 6.05917881, 12.07654471,  0.77536249]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 18.359436469146903}
done in step count: 44
reward sum = 0.3870674724069975
running average episode reward sum: 0.4683077278122709
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  5.]), 'previousTarget': array([23.,  5.]), 'currentState': array([22.13143996,  5.07921881,  5.71884862]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 0.8721652119060606}
episode index:1733
target Thresh 31.999999238067332
target distance 2.0
model initialize at round 1733
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'previousTarget': array([7., 3.]), 'currentState': array([7.78151173, 4.65370412, 4.13302156]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 1.8290702338686453}
done in step count: 3
reward sum = 0.9486438239445081
running average episode reward sum: 0.46858473824833335
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'previousTarget': array([7., 3.]), 'currentState': array([7.2134344 , 3.75434568, 4.18545033]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 0.7839589543102045}
episode index:1734
target Thresh 31.999999245648688
target distance 15.0
model initialize at round 1734
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 17.]), 'previousTarget': array([ 6., 17.]), 'currentState': array([20.90600802, 24.84883958,  4.04051487]), 'targetState': array([ 6, 17], dtype=int32), 'currentDistance': 16.84616745284004}
done in step count: 38
reward sum = 0.44985306871773184
running average episode reward sum: 0.4685739418970189
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 17.]), 'previousTarget': array([ 6., 17.]), 'currentState': array([ 6.80733616, 17.65868667,  3.60919807]), 'targetState': array([ 6, 17], dtype=int32), 'currentDistance': 1.041949995760244}
episode index:1735
target Thresh 31.99999925315461
target distance 3.0
model initialize at round 1735
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([19.86637475,  8.27691551,  1.94490409]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 2.949626305826638}
done in step count: 9
reward sum = 0.8758435460711549
running average episode reward sum: 0.46880854420357077
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([20.02406703, 10.77827833,  1.18521955]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 1.0008025087779155}
episode index:1736
target Thresh 31.999999260585845
target distance 10.0
model initialize at round 1736
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([17.05702369,  4.14845306,  1.41889957]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 11.63547708970055}
done in step count: 28
reward sum = 0.567975549732527
running average episode reward sum: 0.4688656351681816
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 7.93722666, 10.48000959,  2.91341096]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 1.0529971623007481}
episode index:1737
target Thresh 31.99999926794314
target distance 11.0
model initialize at round 1737
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 14.]), 'previousTarget': array([ 3., 14.]), 'currentState': array([11.79447723,  3.26592571,  2.22395328]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 13.876713609231441}
done in step count: 34
reward sum = 0.5424643819867943
running average episode reward sum: 0.4689079819730254
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 14.]), 'previousTarget': array([ 3., 14.]), 'currentState': array([ 3.92518967, 13.31352925,  2.22663488]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 1.1520494888382529}
episode index:1738
target Thresh 31.99999927522723
target distance 12.0
model initialize at round 1738
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'previousTarget': array([11., 21.]), 'currentState': array([20.97967976,  8.97881338,  3.70797479]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 15.623793898483461}
done in step count: 38
reward sum = 0.46000285889297265
running average episode reward sum: 0.46890286114319213
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'previousTarget': array([11., 21.]), 'currentState': array([11.66010107, 20.32586541,  2.23599594]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 0.9434992666951701}
episode index:1739
target Thresh 31.999999282438836
target distance 8.0
model initialize at round 1739
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  2.]), 'previousTarget': array([18.,  2.]), 'currentState': array([10.49634958,  2.94014437,  6.18897934]), 'targetState': array([18,  2], dtype=int32), 'currentDistance': 7.562317170443974}
done in step count: 15
reward sum = 0.7333588264382274
running average episode reward sum: 0.4690548473301433
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  2.]), 'previousTarget': array([18.,  2.]), 'currentState': array([17.19469218,  2.13965937,  5.91382871]), 'targetState': array([18,  2], dtype=int32), 'currentDistance': 0.8173282224563277}
episode index:1740
target Thresh 31.99999928957869
target distance 10.0
model initialize at round 1740
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 13.]), 'previousTarget': array([ 5., 13.]), 'currentState': array([7.98642519, 2.9931598 , 3.35584664]), 'targetState': array([ 5, 13], dtype=int32), 'currentDistance': 10.442968264526712}
done in step count: 25
reward sum = 0.6061579717836889
running average episode reward sum: 0.4691335969708403
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 13.]), 'previousTarget': array([ 5., 13.]), 'currentState': array([ 5.2194518 , 12.22721381,  1.68986038]), 'targetState': array([ 5, 13], dtype=int32), 'currentDistance': 0.803341516152585}
episode index:1741
target Thresh 31.9999992966475
target distance 11.0
model initialize at round 1741
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 22.]), 'previousTarget': array([25., 22.]), 'currentState': array([13.95751377, 16.96560836,  3.87275302]), 'targetState': array([25, 22], dtype=int32), 'currentDistance': 12.13596313647992}
done in step count: 30
reward sum = 0.5206939419441673
running average episode reward sum: 0.4691631953319042
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 22.]), 'previousTarget': array([25., 22.]), 'currentState': array([24.1796344 , 21.54642933,  0.47492211]), 'targetState': array([25, 22], dtype=int32), 'currentDistance': 0.9374039050979134}
episode index:1742
target Thresh 31.999999303645975
target distance 12.0
model initialize at round 1742
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 21.]), 'previousTarget': array([18., 21.]), 'currentState': array([ 5.96020129, 24.96818811,  4.06432965]), 'targetState': array([18, 21], dtype=int32), 'currentDistance': 12.676879345132377}
done in step count: 33
reward sum = 0.517773884899296
running average episode reward sum: 0.46919108442517293
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 21.]), 'previousTarget': array([18., 21.]), 'currentState': array([17.23465506, 21.07700151,  5.8716421 ]), 'targetState': array([18, 21], dtype=int32), 'currentDistance': 0.7692087539429723}
episode index:1743
target Thresh 31.99999931057481
target distance 16.0
model initialize at round 1743
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 21.]), 'previousTarget': array([ 6., 21.]), 'currentState': array([21.84424131, 26.82622064,  3.78948158]), 'targetState': array([ 6, 21], dtype=int32), 'currentDistance': 16.881493698827505}
done in step count: 37
reward sum = 0.44740650871824084
running average episode reward sum: 0.46917859326937766
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 21.]), 'previousTarget': array([ 6., 21.]), 'currentState': array([ 6.88448461, 21.10634876,  3.6703895 ]), 'targetState': array([ 6, 21], dtype=int32), 'currentDistance': 0.8908552546281348}
episode index:1744
target Thresh 31.999999317434707
target distance 18.0
model initialize at round 1744
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.26153547, 26.78323899]), 'previousTarget': array([25.27881227, 26.78704435]), 'currentState': array([12.91949083, 11.04557925,  2.51496446]), 'targetState': array([27, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 78
reward sum = 0.2536013839628015
running average episode reward sum: 0.46905505332135095
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 29.]), 'previousTarget': array([27., 29.]), 'currentState': array([26.40671086, 28.05393898,  0.92373212]), 'targetState': array([27, 29], dtype=int32), 'currentDistance': 1.1167020460391852}
episode index:1745
target Thresh 31.999999324226348
target distance 9.0
model initialize at round 1745
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 27.]), 'previousTarget': array([21., 27.]), 'currentState': array([12.        , 27.        ,  1.61842287]), 'targetState': array([21, 27], dtype=int32), 'currentDistance': 9.0}
done in step count: 23
reward sum = 0.64157696330566
running average episode reward sum: 0.46915386312088375
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 27.]), 'previousTarget': array([21., 27.]), 'currentState': array([20.33175968, 26.91587091,  6.18475536]), 'targetState': array([21, 27], dtype=int32), 'currentDistance': 0.6735152813346595}
episode index:1746
target Thresh 31.999999330950406
target distance 17.0
model initialize at round 1746
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 10.]), 'previousTarget': array([20., 10.]), 'currentState': array([ 2.99688412, 15.9993344 ,  3.58301845]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 18.030473167955886}
done in step count: 53
reward sum = 0.3535202055292156
running average episode reward sum: 0.46908767327681294
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 10.]), 'previousTarget': array([20., 10.]), 'currentState': array([19.27394466, 10.38284179,  6.05775863]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 0.8208070348197097}
episode index:1747
target Thresh 31.999999337607562
target distance 17.0
model initialize at round 1747
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 20.]), 'previousTarget': array([21., 20.]), 'currentState': array([ 3.98707899, 22.0041545 ,  2.59287944]), 'targetState': array([21, 20], dtype=int32), 'currentDistance': 17.130560898204436}
done in step count: 51
reward sum = 0.3694613829183841
running average episode reward sum: 0.4690306788315278
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 20.]), 'previousTarget': array([21., 20.]), 'currentState': array([20.17040655, 20.11525074,  5.74974884]), 'targetState': array([21, 20], dtype=int32), 'currentDistance': 0.8375607607681554}
episode index:1748
target Thresh 31.999999344198475
target distance 17.0
model initialize at round 1748
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([2.93790809, 7.27575579, 1.53977251]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 17.789021971203287}
done in step count: 45
reward sum = 0.4121019564327906
running average episode reward sum: 0.46899812953341535
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([ 8.74266306, 23.14786975,  1.33208155]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 0.8901394616997048}
episode index:1749
target Thresh 31.99999935072381
target distance 19.0
model initialize at round 1749
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.76564973,  8.55444607]), 'previousTarget': array([25.76114  ,  8.5672925]), 'currentState': array([17.97913678, 26.97644965,  4.23991966]), 'targetState': array([26,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.32967431474864745
running average episode reward sum: 0.4689185159249669
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  8.]), 'previousTarget': array([26.,  8.]), 'currentState': array([25.79970111,  8.87009705,  5.07760773]), 'targetState': array([26,  8], dtype=int32), 'currentDistance': 0.8928541432036452}
episode index:1750
target Thresh 31.999999357184215
target distance 6.0
model initialize at round 1750
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([24.       ,  9.       ,  4.2941885]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 7.211102550927978}
done in step count: 19
reward sum = 0.6867051966606924
running average episode reward sum: 0.46904289438341107
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([20.10360752, 14.18332886,  2.0314546 ]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 0.8232170194367748}
episode index:1751
target Thresh 31.99999936358034
target distance 19.0
model initialize at round 1751
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([11.99679759,  9.98998936,  4.15027618]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 19.114594034520746}
done in step count: 53
reward sum = 0.31499254668142285
running average episode reward sum: 0.4689549661027593
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([ 9.89267234, 28.17423891,  1.89141575]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 0.8327067911014815}
episode index:1752
target Thresh 31.99999936991282
target distance 10.0
model initialize at round 1752
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([14.04096759, 14.99756101,  0.17973119]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 12.232089372778923}
done in step count: 28
reward sum = 0.5462143821817278
running average episode reward sum: 0.46899903878734506
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([ 7.85200497, 24.03794873,  2.24259044]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 1.2850895327535545}
episode index:1753
target Thresh 31.999999376182295
target distance 8.0
model initialize at round 1753
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([2.        , 2.        , 2.40755361]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 8.246211251235321}
done in step count: 21
reward sum = 0.6847740341491155
running average episode reward sum: 0.4691220575988398
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.18209728, 9.2054678 , 1.06492284]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.815132405826217}
episode index:1754
target Thresh 31.999999382389383
target distance 5.0
model initialize at round 1754
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'previousTarget': array([11., 21.]), 'currentState': array([ 6.97608883, 15.98108742,  3.61264938]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 6.432833325182162}
done in step count: 18
reward sum = 0.7106838651816741
running average episode reward sum: 0.46925969965444253
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'previousTarget': array([11., 21.]), 'currentState': array([10.07035819, 20.26001021,  0.86907221]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 1.1881998063550945}
episode index:1755
target Thresh 31.99999938853471
target distance 11.0
model initialize at round 1755
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 3.]), 'previousTarget': array([6., 3.]), 'currentState': array([13.00432515, 14.0391354 ,  1.71322548]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 13.073755438622102}
done in step count: 35
reward sum = 0.49414731775887155
running average episode reward sum: 0.4692738725576911
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 3.]), 'previousTarget': array([6., 3.]), 'currentState': array([6.5850147 , 3.68705189, 4.30966975]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 0.9023760322718024}
episode index:1756
target Thresh 31.999999394618893
target distance 2.0
model initialize at round 1756
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 15.]), 'previousTarget': array([24., 15.]), 'currentState': array([22.39850852, 15.91607366,  6.03478378]), 'targetState': array([24, 15], dtype=int32), 'currentDistance': 1.8449839845717741}
done in step count: 2
reward sum = 0.9631560582078743
running average episode reward sum: 0.46955496657342827
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 15.]), 'previousTarget': array([24., 15.]), 'currentState': array([23.06865222, 15.70062928,  5.9312514 ]), 'targetState': array([24, 15], dtype=int32), 'currentDistance': 1.165457021859941}
episode index:1757
target Thresh 31.999999400642537
target distance 3.0
model initialize at round 1757
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 12.]), 'previousTarget': array([22., 12.]), 'currentState': array([22.76548044,  9.15926889,  2.38025504]), 'targetState': array([22, 12], dtype=int32), 'currentDistance': 2.942059410009137}
done in step count: 6
reward sum = 0.8980721598043516
running average episode reward sum: 0.46979871924307043
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 12.]), 'previousTarget': array([22., 12.]), 'currentState': array([21.94466179, 11.17827767,  1.77156347]), 'targetState': array([22, 12], dtype=int32), 'currentDistance': 0.8235835789489224}
episode index:1758
target Thresh 31.999999406606243
target distance 19.0
model initialize at round 1758
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3.57010347, 17.76115089]), 'previousTarget': array([ 3.5672925, 17.76114  ]), 'currentState': array([22.01660419, 10.03285153,  1.35531306]), 'targetState': array([ 3, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.3141859873800452
running average episode reward sum: 0.4697102526530403
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 18.]), 'previousTarget': array([ 3., 18.]), 'currentState': array([ 3.72105795, 17.69951995,  2.82863631]), 'targetState': array([ 3, 18], dtype=int32), 'currentDistance': 0.7811612060792031}
episode index:1759
target Thresh 31.999999412510608
target distance 22.0
model initialize at round 1759
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.98671341, 24.98665611]), 'previousTarget': array([10.94427191, 24.88854382]), 'currentState': array([2.01887825, 7.10991333, 1.2348595 ]), 'targetState': array([13, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.2696153971109328
running average episode reward sum: 0.4695965623942095
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 29.]), 'previousTarget': array([13., 29.]), 'currentState': array([12.6415157 , 28.06788869,  1.1597456 ]), 'targetState': array([13, 29], dtype=int32), 'currentDistance': 0.99867036252425}
episode index:1760
target Thresh 31.999999418356225
target distance 13.0
model initialize at round 1760
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 22.]), 'previousTarget': array([26., 22.]), 'currentState': array([12.9704589 ,  8.99676265,  2.99824524]), 'targetState': array([26, 22], dtype=int32), 'currentDistance': 18.407963577244914}
done in step count: 53
reward sum = 0.3535200031353266
running average episode reward sum: 0.46953064725550486
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 22.]), 'previousTarget': array([26., 22.]), 'currentState': array([25.17795448, 21.58072399,  0.71298861]), 'targetState': array([26, 22], dtype=int32), 'currentDistance': 0.9227953217244728}
episode index:1761
target Thresh 31.999999424143677
target distance 7.0
model initialize at round 1761
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 12.]), 'previousTarget': array([ 9., 12.]), 'currentState': array([16.        ,  5.        ,  0.33108878]), 'targetState': array([ 9, 12], dtype=int32), 'currentDistance': 9.899494936611665}
done in step count: 23
reward sum = 0.6183947522743033
running average episode reward sum: 0.4696151331266847
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 12.]), 'previousTarget': array([ 9., 12.]), 'currentState': array([ 9.91057809, 11.137506  ,  2.22083746]), 'targetState': array([ 9, 12], dtype=int32), 'currentDistance': 1.2542122470261}
episode index:1762
target Thresh 31.999999429873544
target distance 1.0
model initialize at round 1762
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 25.]), 'previousTarget': array([16., 25.]), 'currentState': array([15.28613275, 24.03281947,  0.1922879 ]), 'targetState': array([16, 25], dtype=int32), 'currentDistance': 1.202100090650406}
done in step count: 0
reward sum = 0.9957190339915627
running average episode reward sum: 0.46991354713738515
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 25.]), 'previousTarget': array([16., 25.]), 'currentState': array([15.28613275, 24.03281947,  0.1922879 ]), 'targetState': array([16, 25], dtype=int32), 'currentDistance': 1.202100090650406}
episode index:1763
target Thresh 31.999999435546396
target distance 15.0
model initialize at round 1763
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([ 9.06073332, 24.00405305,  6.20071516]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 15.512557170004087}
done in step count: 38
reward sum = 0.4668623858044034
running average episode reward sum: 0.4699118174540898
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([12.62818901,  9.99639569,  5.13727844]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 1.063507300858067}
episode index:1764
target Thresh 31.999999441162803
target distance 19.0
model initialize at round 1764
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 21.]), 'previousTarget': array([25., 21.]), 'currentState': array([ 6.        , 17.        ,  4.71625203]), 'targetState': array([25, 21], dtype=int32), 'currentDistance': 19.416487838947596}
done in step count: 59
reward sum = 0.3395926110033888
running average episode reward sum: 0.46983798220964174
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 21.]), 'previousTarget': array([25., 21.]), 'currentState': array([24.08883897, 20.74780754,  0.18213063]), 'targetState': array([25, 21], dtype=int32), 'currentDistance': 0.9454181418216417}
episode index:1765
target Thresh 31.999999446723326
target distance 9.0
model initialize at round 1765
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 26.]), 'previousTarget': array([22., 26.]), 'currentState': array([13.23199443, 25.7351856 ,  5.67008683]), 'targetState': array([22, 26], dtype=int32), 'currentDistance': 8.772003669938957}
done in step count: 20
reward sum = 0.6785999467447834
running average episode reward sum: 0.4699561939675892
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 26.]), 'previousTarget': array([22., 26.]), 'currentState': array([21.14876008, 25.89352674,  0.07752577]), 'targetState': array([22, 26], dtype=int32), 'currentDistance': 0.8578729225387173}
episode index:1766
target Thresh 31.999999452228522
target distance 22.0
model initialize at round 1766
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.45730569,  5.40538103]), 'previousTarget': array([25.43242207,  5.49734288]), 'currentState': array([21.05560932, 24.91499581,  5.08617496]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.2985570508592929
running average episode reward sum: 0.46985919388659975
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  3.]), 'previousTarget': array([26.,  3.]), 'currentState': array([26.20518143,  3.90670606,  5.27074314]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 0.9296318055584981}
episode index:1767
target Thresh 31.99999945767894
target distance 6.0
model initialize at round 1767
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.83629387, 16.68595014,  4.41290778]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 5.688306313306738}
done in step count: 11
reward sum = 0.7962469782769719
running average episode reward sum: 0.47004380236193366
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.07028777, 11.7323825 ,  4.69517121]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.7357475801228957}
episode index:1768
target Thresh 31.999999463075124
target distance 21.0
model initialize at round 1768
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.11229412,  9.14119127]), 'previousTarget': array([13.10381815,  9.09009055]), 'currentState': array([15.07085072, 29.04506165,  0.31397629]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.2950945114857472
running average episode reward sum: 0.4699449050804887
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([12.74170541,  8.85495434,  4.44989354]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 0.8931198215055505}
episode index:1769
target Thresh 31.999999468417617
target distance 16.0
model initialize at round 1769
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.99245765,  4.00567746]), 'previousTarget': array([23.,  4.]), 'currentState': array([ 7.01352335, 16.03371382,  0.93682313]), 'targetState': array([23,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.3195318866711807
running average episode reward sum: 0.46985992597404275
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  4.]), 'previousTarget': array([23.,  4.]), 'currentState': array([22.19830569,  3.9591005 ,  5.43411251]), 'targetState': array([23,  4], dtype=int32), 'currentDistance': 0.8027369008418711}
episode index:1770
target Thresh 31.99999947370695
target distance 6.0
model initialize at round 1770
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  7.]), 'previousTarget': array([13.,  7.]), 'currentState': array([18.6252899 , 10.08146685,  3.05074292]), 'targetState': array([13,  7], dtype=int32), 'currentDistance': 6.413994417229317}
done in step count: 12
reward sum = 0.7752933945629978
running average episode reward sum: 0.4700323898185311
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  7.]), 'previousTarget': array([13.,  7.]), 'currentState': array([13.65029661,  7.8425936 ,  3.90310796]), 'targetState': array([13,  7], dtype=int32), 'currentDistance': 1.064354101279247}
episode index:1771
target Thresh 31.999999478943653
target distance 3.0
model initialize at round 1771
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 8.]), 'previousTarget': array([8., 8.]), 'currentState': array([ 8.96294646, 11.04435247,  2.5192728 ]), 'targetState': array([8, 8], dtype=int32), 'currentDistance': 3.193015476362753}
done in step count: 11
reward sum = 0.8259148989207883
running average episode reward sum: 0.47023322644895005
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 8.]), 'previousTarget': array([8., 8.]), 'currentState': array([7.72596148, 8.84903349, 4.7271171 ]), 'targetState': array([8, 8], dtype=int32), 'currentDistance': 0.8921630856219384}
episode index:1772
target Thresh 31.99999948412825
target distance 13.0
model initialize at round 1772
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([21.86050768, 20.86711025,  4.12968409]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 12.867866343891889}
done in step count: 32
reward sum = 0.550662151997951
running average episode reward sum: 0.47027858963312885
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([22.07369937,  8.86839785,  4.82289215]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 0.8715196065060754}
episode index:1773
target Thresh 31.99999948926126
target distance 23.0
model initialize at round 1773
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.22789528, 14.99966803]), 'previousTarget': array([24., 15.]), 'currentState': array([ 4.22789543, 14.99727297,  0.06914989]), 'targetState': array([27, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.29426553553027424
running average episode reward sum: 0.47017937145156014
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 15.]), 'previousTarget': array([27., 15.]), 'currentState': array([26.17302722, 15.14307764,  6.24282065]), 'targetState': array([27, 15], dtype=int32), 'currentDistance': 0.8392587171106247}
episode index:1774
target Thresh 31.999999494343196
target distance 24.0
model initialize at round 1774
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.94427191, 10.11145618]), 'previousTarget': array([22.94427191, 10.11145618]), 'currentState': array([14.        , 28.        ,  2.55157137]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 76
reward sum = 0.1580588501567931
running average episode reward sum: 0.47000352890435176
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  4.]), 'previousTarget': array([26.,  4.]), 'currentState': array([25.41046618,  4.71813729,  4.96560405]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 0.9291239430207242}
episode index:1775
target Thresh 31.999999499374564
target distance 24.0
model initialize at round 1775
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.62675679, 24.53416829]), 'previousTarget': array([20.58583933, 24.52566297]), 'currentState': array([ 3.05671068, 14.97940614,  6.11360574]), 'targetState': array([27, 28], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 76
reward sum = 0.20135614413881625
running average episode reward sum: 0.4698522634850018
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 28.]), 'previousTarget': array([27., 28.]), 'currentState': array([26.06440076, 27.35916764,  0.68614056]), 'targetState': array([27, 28], dtype=int32), 'currentDistance': 1.134024714558244}
episode index:1776
target Thresh 31.99999950435587
target distance 7.0
model initialize at round 1776
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 18.]), 'previousTarget': array([17., 18.]), 'currentState': array([24.        , 21.        ,  5.19610643]), 'targetState': array([17, 18], dtype=int32), 'currentDistance': 7.615773105863908}
done in step count: 21
reward sum = 0.692537664079136
running average episode reward sum: 0.4699775788483075
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 18.]), 'previousTarget': array([17., 18.]), 'currentState': array([17.99383962, 18.59633484,  3.38686221]), 'targetState': array([17, 18], dtype=int32), 'currentDistance': 1.1590221917321921}
episode index:1777
target Thresh 31.999999509287612
target distance 19.0
model initialize at round 1777
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2.50857472, 9.78528323]), 'previousTarget': array([2.5672925, 9.76114  ]), 'currentState': array([20.93375775,  2.00629678,  2.87242663]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.35041597876507324
running average episode reward sum: 0.469910333853885
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.86299777, 9.79435773, 2.63997603]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.8871605799546054}
episode index:1778
target Thresh 31.99999951417028
target distance 13.0
model initialize at round 1778
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([13.      , 23.      ,  3.969603]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 13.341664064126334}
done in step count: 34
reward sum = 0.48983552263367924
running average episode reward sum: 0.4699215340724234
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([25.24519553, 19.69765284,  6.06843039]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 0.8131073718506122}
episode index:1779
target Thresh 31.99999951900437
target distance 9.0
model initialize at round 1779
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([24.98452214, 14.17220074,  1.85869756]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 8.986172215565595}
done in step count: 21
reward sum = 0.6589452726597527
running average episode reward sum: 0.4700277271839893
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([16.70072708, 14.61301548,  3.33018867]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 0.9310243906225083}
episode index:1780
target Thresh 31.999999523790354
target distance 9.0
model initialize at round 1780
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 17.]), 'previousTarget': array([14., 17.]), 'currentState': array([9.9913302 , 8.25510175, 1.49980618]), 'targetState': array([14, 17], dtype=int32), 'currentDistance': 9.619910544919497}
done in step count: 26
reward sum = 0.6546660529159706
running average episode reward sum: 0.47013139833824646
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 17.]), 'previousTarget': array([14., 17.]), 'currentState': array([13.73812468, 16.07900327,  0.93695347]), 'targetState': array([14, 17], dtype=int32), 'currentDistance': 0.9575038670082505}
episode index:1781
target Thresh 31.99999952852872
target distance 5.0
model initialize at round 1781
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 18.]), 'previousTarget': array([10., 18.]), 'currentState': array([ 5.        , 16.        ,  1.06513155]), 'targetState': array([10, 18], dtype=int32), 'currentDistance': 5.385164807134504}
done in step count: 12
reward sum = 0.8103728948293633
running average episode reward sum: 0.4703223307156264
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 18.]), 'previousTarget': array([10., 18.]), 'currentState': array([ 9.12322475, 17.65493577,  0.53192867]), 'targetState': array([10, 18], dtype=int32), 'currentDistance': 0.9422335998540713}
episode index:1782
target Thresh 31.99999953321994
target distance 23.0
model initialize at round 1782
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5.54707779, 5.14842362]), 'previousTarget': array([5.5731765, 5.2957649]), 'currentState': array([ 8.97102088, 24.85315944,  4.55889973]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.2974367481110962
running average episode reward sum: 0.47022536740513593
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 2.]), 'previousTarget': array([5., 2.]), 'currentState': array([5.34207186, 2.98295047, 4.44971561]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 1.0407712457197622}
episode index:1783
target Thresh 31.999999537864475
target distance 13.0
model initialize at round 1783
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 16.]), 'previousTarget': array([26., 16.]), 'currentState': array([20.07834709, 28.76289103,  4.9989955 ]), 'targetState': array([26, 16], dtype=int32), 'currentDistance': 14.069732072244152}
done in step count: 34
reward sum = 0.5317780531849261
running average episode reward sum: 0.47025987003169406
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 16.]), 'previousTarget': array([26., 16.]), 'currentState': array([25.59536483, 16.77980615,  5.12639018]), 'targetState': array([26, 16], dtype=int32), 'currentDistance': 0.878536996770235}
episode index:1784
target Thresh 31.999999542462803
target distance 8.0
model initialize at round 1784
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 22.]), 'previousTarget': array([27., 22.]), 'currentState': array([19.        , 29.        ,  0.61633676]), 'targetState': array([27, 22], dtype=int32), 'currentDistance': 10.63014581273465}
done in step count: 29
reward sum = 0.6015993292107144
running average episode reward sum: 0.4703334495606459
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 22.]), 'previousTarget': array([27., 22.]), 'currentState': array([26.12610296, 22.53771345,  5.76646031]), 'targetState': array([27, 22], dtype=int32), 'currentDistance': 1.026075923888756}
episode index:1785
target Thresh 31.999999547015374
target distance 8.0
model initialize at round 1785
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([ 8.35252631, 17.88642962,  5.99428696]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 8.17406446121545}
done in step count: 18
reward sum = 0.7187810722940651
running average episode reward sum: 0.4704725579720308
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([15.25741285, 15.3689936 ,  5.93393413]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 0.8292116408091584}
episode index:1786
target Thresh 31.999999551522645
target distance 9.0
model initialize at round 1786
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.02823229, 20.0355939 ,  0.64772904]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 9.03563800248518}
done in step count: 27
reward sum = 0.6112010127046076
running average episode reward sum: 0.4705513092057927
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.83302273, 11.93637126,  4.63539694]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.9511427560607707}
episode index:1787
target Thresh 31.99999955598507
target distance 13.0
model initialize at round 1787
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 16.]), 'previousTarget': array([ 2., 16.]), 'currentState': array([15.00056534, 13.01569895,  1.7516765 ]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 13.338693789549025}
done in step count: 32
reward sum = 0.5299528051131701
running average episode reward sum: 0.47058453151894003
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 16.]), 'previousTarget': array([ 2., 16.]), 'currentState': array([ 2.86156658, 15.99019781,  2.8997582 ]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 0.8616223428386808}
episode index:1788
target Thresh 31.999999560403094
target distance 4.0
model initialize at round 1788
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  4.]), 'previousTarget': array([18.,  4.]), 'currentState': array([21.80984838,  4.20435589,  2.50387156]), 'targetState': array([18,  4], dtype=int32), 'currentDistance': 3.8153251482655737}
done in step count: 8
reward sum = 0.8661707989530494
running average episode reward sum: 0.4708056529652419
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  4.]), 'previousTarget': array([18.,  4.]), 'currentState': array([18.8852711 ,  4.53691092,  3.29153619]), 'targetState': array([18,  4], dtype=int32), 'currentDistance': 1.035363833692477}
episode index:1789
target Thresh 31.999999564777156
target distance 16.0
model initialize at round 1789
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 27.]), 'previousTarget': array([ 8., 27.]), 'currentState': array([ 4.90131261, 11.0478885 ,  2.53266513]), 'targetState': array([ 8, 27], dtype=int32), 'currentDistance': 16.25028384204583}
done in step count: 40
reward sum = 0.45293536277375157
running average episode reward sum: 0.47079566956290025
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 27.]), 'previousTarget': array([ 8., 27.]), 'currentState': array([ 7.02548692, 26.18746518,  1.56229684]), 'targetState': array([ 8, 27], dtype=int32), 'currentDistance': 1.2688138522611243}
episode index:1790
target Thresh 31.999999569107693
target distance 8.0
model initialize at round 1790
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 22.]), 'previousTarget': array([23., 22.]), 'currentState': array([15.25119966, 22.17366319,  0.58490705]), 'targetState': array([23, 22], dtype=int32), 'currentDistance': 7.750746137125708}
done in step count: 26
reward sum = 0.6628716616458161
running average episode reward sum: 0.470902914672941
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 22.]), 'previousTarget': array([23., 22.]), 'currentState': array([23.18068837, 22.96484039,  5.59343012]), 'targetState': array([23, 22], dtype=int32), 'currentDistance': 0.9816136025355112}
episode index:1791
target Thresh 31.999999573395144
target distance 11.0
model initialize at round 1791
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 24.]), 'previousTarget': array([ 2., 24.]), 'currentState': array([ 6.92817133, 12.91700071,  3.80599084]), 'targetState': array([ 2, 24], dtype=int32), 'currentDistance': 12.129292890596139}
done in step count: 33
reward sum = 0.5443228591330438
running average episode reward sum: 0.47094388562409056
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 24.]), 'previousTarget': array([ 2., 24.]), 'currentState': array([ 2.57558345, 23.05901453,  2.00262657]), 'targetState': array([ 2, 24], dtype=int32), 'currentDistance': 1.1030638965577395}
episode index:1792
target Thresh 31.999999577639933
target distance 13.0
model initialize at round 1792
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 28.]), 'previousTarget': array([ 2., 28.]), 'currentState': array([15.00178404, 27.00951108,  1.63787627]), 'targetState': array([ 2, 28], dtype=int32), 'currentDistance': 13.039457675398877}
done in step count: 32
reward sum = 0.5300900265828956
running average episode reward sum: 0.4709768728750436
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 28.]), 'previousTarget': array([ 2., 28.]), 'currentState': array([ 2.97477833, 28.44459951,  3.27691879]), 'targetState': array([ 2, 28], dtype=int32), 'currentDistance': 1.071382995321664}
episode index:1793
target Thresh 31.999999581842488
target distance 12.0
model initialize at round 1793
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([19.87245898, 14.67279901,  4.36101643]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 11.822027716014555}
done in step count: 25
reward sum = 0.5953904640029724
running average episode reward sum: 0.4710462227028741
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([18.46633887,  3.72396191,  4.65167073]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 0.8611578178440336}
episode index:1794
target Thresh 31.999999586003224
target distance 17.0
model initialize at round 1794
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 12.]), 'previousTarget': array([ 9., 12.]), 'currentState': array([25.99822007,  4.93058735,  4.45973182]), 'targetState': array([ 9, 12], dtype=int32), 'currentDistance': 18.409673565989745}
done in step count: 49
reward sum = 0.3714403691289476
running average episode reward sum: 0.4709907319766491
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 12.]), 'previousTarget': array([ 9., 12.]), 'currentState': array([ 9.80126412, 11.70369374,  2.96656975]), 'targetState': array([ 9, 12], dtype=int32), 'currentDistance': 0.8542959647966388}
episode index:1795
target Thresh 31.99999959012256
target distance 8.0
model initialize at round 1795
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  9.]), 'previousTarget': array([18.,  9.]), 'currentState': array([21.01375382, 17.00079803,  6.12414402]), 'targetState': array([18,  9], dtype=int32), 'currentDistance': 8.549589536706858}
done in step count: 24
reward sum = 0.64985097377892
running average episode reward sum: 0.4710903200845568
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  9.]), 'previousTarget': array([18.,  9.]), 'currentState': array([18.38199475,  9.91816721,  4.3087948 ]), 'targetState': array([18,  9], dtype=int32), 'currentDistance': 0.9944601639711512}
episode index:1796
target Thresh 31.999999594200908
target distance 15.0
model initialize at round 1796
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([13.81848954, 17.93791724,  3.62299699]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 17.875796718412126}
done in step count: 47
reward sum = 0.4105845091969438
running average episode reward sum: 0.4710566496277468
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.11601213, 3.9138689 , 4.17947634]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.9212031175377907}
episode index:1797
target Thresh 31.999999598238677
target distance 19.0
model initialize at round 1797
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.97927459,  5.69147429]), 'previousTarget': array([12.97927459,  5.69147429]), 'currentState': array([23.        , 23.        ,  5.43824449]), 'targetState': array([12,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.29025732108480173
running average episode reward sum: 0.4709560938276673
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  4.]), 'previousTarget': array([12.,  4.]), 'currentState': array([12.24989963,  4.80979383,  3.88814402]), 'targetState': array([12,  4], dtype=int32), 'currentDistance': 0.8474761817267135}
episode index:1798
target Thresh 31.99999960223627
target distance 22.0
model initialize at round 1798
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.35547081, 23.64773603]), 'previousTarget': array([16.26234812, 23.29527642]), 'currentState': array([11.07020871,  4.3587235 ,  1.31526121]), 'targetState': array([17, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.30418439450611945
running average episode reward sum: 0.4708633913822412
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 26.]), 'previousTarget': array([17., 26.]), 'currentState': array([16.88069315, 25.06202626,  1.1695192 ]), 'targetState': array([17, 26], dtype=int32), 'currentDistance': 0.9455309939293901}
episode index:1799
target Thresh 31.999999606194084
target distance 3.0
model initialize at round 1799
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 17.]), 'previousTarget': array([ 3., 17.]), 'currentState': array([ 2.        , 14.        ,  3.77944708]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 10
reward sum = 0.8315360654576303
running average episode reward sum: 0.47106376509006087
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 17.]), 'previousTarget': array([ 3., 17.]), 'currentState': array([ 2.02920297, 16.09434975,  1.2082881 ]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 1.3276480115336435}
episode index:1800
target Thresh 31.999999610112518
target distance 11.0
model initialize at round 1800
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 26.]), 'previousTarget': array([23., 26.]), 'currentState': array([12.41761203, 23.95841482,  6.2711574 ]), 'targetState': array([23, 26], dtype=int32), 'currentDistance': 10.777523144153273}
done in step count: 21
reward sum = 0.6291864594866862
running average episode reward sum: 0.4711515622551895
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 26.]), 'previousTarget': array([23., 26.]), 'currentState': array([22.12604435, 25.77288379,  0.03120513]), 'targetState': array([23, 26], dtype=int32), 'currentDistance': 0.9029840789440606}
episode index:1801
target Thresh 31.999999613991964
target distance 11.0
model initialize at round 1801
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 22.]), 'previousTarget': array([11., 22.]), 'currentState': array([ 3.99605807, 11.00292747,  2.32929713]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 13.038052262371389}
done in step count: 33
reward sum = 0.542987363171233
running average episode reward sum: 0.4711914267396046
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 22.]), 'previousTarget': array([11., 22.]), 'currentState': array([10.33095091, 21.02343828,  0.88412467]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 1.1837649595528716}
episode index:1802
target Thresh 31.99999961783281
target distance 7.0
model initialize at round 1802
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([26.01126462, 12.99844488,  0.09364712]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 7.071023388313112}
done in step count: 18
reward sum = 0.7208158280124908
running average episode reward sum: 0.47132987621341105
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([27.24610708, 19.09746913,  1.41426452]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 0.9354841859015895}
episode index:1803
target Thresh 31.999999621635435
target distance 11.0
model initialize at round 1803
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 13.]), 'previousTarget': array([16., 13.]), 'currentState': array([ 5.13679265, 10.14108507,  0.68278041]), 'targetState': array([16, 13], dtype=int32), 'currentDistance': 11.233105914657454}
done in step count: 28
reward sum = 0.6168862474301382
running average episode reward sum: 0.4714105615633094
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 13.]), 'previousTarget': array([16., 13.]), 'currentState': array([15.02977595, 12.61299099,  0.35196468]), 'targetState': array([16, 13], dtype=int32), 'currentDistance': 1.0445624332308105}
episode index:1804
target Thresh 31.999999625400225
target distance 2.0
model initialize at round 1804
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  9.]), 'previousTarget': array([21.,  9.]), 'currentState': array([19.        ,  7.        ,  2.96992916]), 'targetState': array([21,  9], dtype=int32), 'currentDistance': 2.82842712474619}
done in step count: 17
reward sum = 0.794388263759859
running average episode reward sum: 0.47158949657837673
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  9.]), 'previousTarget': array([21.,  9.]), 'currentState': array([20.01078914,  9.38196151,  0.53372668]), 'targetState': array([21,  9], dtype=int32), 'currentDistance': 1.0603927218405163}
episode index:1805
target Thresh 31.999999629127554
target distance 14.0
model initialize at round 1805
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 26.]), 'previousTarget': array([19., 26.]), 'currentState': array([14.        , 12.        ,  6.14502844]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 14.866068747318504}
done in step count: 42
reward sum = 0.46563691354247827
running average episode reward sum: 0.47158620057448086
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 26.]), 'previousTarget': array([19., 26.]), 'currentState': array([18.86378113, 25.12689858,  1.50138967]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 0.8836637787326289}
episode index:1806
target Thresh 31.999999632817797
target distance 18.0
model initialize at round 1806
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.94818637, 15.28727678]), 'previousTarget': array([21.94818637, 15.28727678]), 'currentState': array([7.        , 2.        , 2.57664698]), 'targetState': array([25, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 70
reward sum = 0.2372764179535179
running average episode reward sum: 0.4714565327368378
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 18.]), 'previousTarget': array([25., 18.]), 'currentState': array([24.02293824, 17.13203153,  0.57238504]), 'targetState': array([25, 18], dtype=int32), 'currentDistance': 1.3069119895279853}
episode index:1807
target Thresh 31.999999636471323
target distance 2.0
model initialize at round 1807
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 20.]), 'previousTarget': array([ 8., 20.]), 'currentState': array([ 9.00822299, 18.00172858,  0.42125782]), 'targetState': array([ 8, 20], dtype=int32), 'currentDistance': 2.238214080137582}
done in step count: 6
reward sum = 0.9007126589243056
running average episode reward sum: 0.471693953160614
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 20.]), 'previousTarget': array([ 8., 20.]), 'currentState': array([ 8.97877179, 19.23035893,  2.05777945]), 'targetState': array([ 8, 20], dtype=int32), 'currentDistance': 1.245127137092068}
episode index:1808
target Thresh 31.999999640088493
target distance 18.0
model initialize at round 1808
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3.90297944, 5.35308124]), 'previousTarget': array([3.90599608, 5.35899411]), 'currentState': array([15.00485787, 21.98883476,  4.98795745]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.3048057358459817
running average episode reward sum: 0.47160169875634944
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.92809758, 4.63655442, 3.600765  ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.1254184351405365}
episode index:1809
target Thresh 31.99999964366967
target distance 20.0
model initialize at round 1809
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.52431817, 10.361625  ]), 'previousTarget': array([22.52431817, 10.361625  ]), 'currentState': array([ 5.        , 20.        ,  1.35490084]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.2568101100801662
running average episode reward sum: 0.47148302937034053
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([24.11363786,  9.09197004,  5.78250585]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 0.8911208330727405}
episode index:1810
target Thresh 31.99999964721522
target distance 5.0
model initialize at round 1810
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([17.07012979, 16.08359051,  0.78427558]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 4.930578832484316}
done in step count: 12
reward sum = 0.8185685353628909
running average episode reward sum: 0.4716746834321807
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([21.24043446, 16.29528621,  6.02018316]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 0.8149440216623216}
episode index:1811
target Thresh 31.999999650725485
target distance 13.0
model initialize at round 1811
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([26.        , 28.        ,  2.33226061]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 16.401219466856727}
done in step count: 39
reward sum = 0.4183548566542689
running average episode reward sum: 0.47164525747921276
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([16.1791889 , 15.82763045,  3.89104065]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 0.8468062521624816}
episode index:1812
target Thresh 31.999999654200824
target distance 5.0
model initialize at round 1812
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([14.02002957, 13.03416603,  1.28050405]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 5.4165051798038775}
done in step count: 14
reward sum = 0.7543649029580731
running average episode reward sum: 0.471801197713895
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.66886392, 11.94871315,  3.64186574]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.1607909301338026}
episode index:1813
target Thresh 31.999999657641585
target distance 17.0
model initialize at round 1813
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4.84971196, 11.11015467]), 'previousTarget': array([ 4.85099785, 11.11284334]), 'currentState': array([17.00566517, 26.9919934 ,  5.14707226]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 60
reward sum = 0.30254572492881404
running average episode reward sum: 0.47170789260210605
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.49932006, 10.91342227,  3.99032907]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.0409902841947534}
episode index:1814
target Thresh 31.999999661048108
target distance 4.0
model initialize at round 1814
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 26.]), 'previousTarget': array([15., 26.]), 'currentState': array([18.95205342, 21.97862488,  3.48512515]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 5.638278472674141}
done in step count: 14
reward sum = 0.782681957074961
running average episode reward sum: 0.47187922817481837
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 26.]), 'previousTarget': array([15., 26.]), 'currentState': array([15.84870181, 25.32327776,  2.06374914]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 1.0854712153954764}
episode index:1815
target Thresh 31.999999664420734
target distance 11.0
model initialize at round 1815
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 13.]), 'previousTarget': array([25., 13.]), 'currentState': array([14.00766912,  4.98802829,  5.52836937]), 'targetState': array([25, 13], dtype=int32), 'currentDistance': 13.602317039583397}
done in step count: 31
reward sum = 0.5073378005013842
running average episode reward sum: 0.47189875382037266
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 13.]), 'previousTarget': array([25., 13.]), 'currentState': array([24.55818409, 12.11460872,  0.87592003]), 'targetState': array([25, 13], dtype=int32), 'currentDistance': 0.9895044282242677}
episode index:1816
target Thresh 31.999999667759806
target distance 9.0
model initialize at round 1816
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 26.]), 'previousTarget': array([13., 26.]), 'currentState': array([ 9.        , 17.        ,  3.60370731]), 'targetState': array([13, 26], dtype=int32), 'currentDistance': 9.848857801796106}
done in step count: 31
reward sum = 0.5712493640647978
running average episode reward sum: 0.47195343219695185
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 26.]), 'previousTarget': array([13., 26.]), 'currentState': array([12.95293955, 25.0044451 ,  1.02665116]), 'targetState': array([13, 26], dtype=int32), 'currentDistance': 0.9966665656409475}
episode index:1817
target Thresh 31.99999967106565
target distance 7.0
model initialize at round 1817
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 23.]), 'previousTarget': array([25., 23.]), 'currentState': array([18.        , 24.        ,  4.35981885]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 7.071067811865475}
done in step count: 17
reward sum = 0.7026613387078775
running average episode reward sum: 0.47208033423573675
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 23.]), 'previousTarget': array([25., 23.]), 'currentState': array([24.27749397, 22.58467923,  6.23932491]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 0.8333704462031255}
episode index:1818
target Thresh 31.9999996743386
target distance 12.0
model initialize at round 1818
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 6.]), 'previousTarget': array([5., 6.]), 'currentState': array([16.79241435,  9.15644348,  2.70237097]), 'targetState': array([5, 6], dtype=int32), 'currentDistance': 12.20754568697995}
done in step count: 26
reward sum = 0.5747081270776787
running average episode reward sum: 0.4721367541328461
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 6.]), 'previousTarget': array([5., 6.]), 'currentState': array([5.82627048, 6.3348541 , 3.64652214]), 'targetState': array([5, 6], dtype=int32), 'currentDistance': 0.8915437069130113}
episode index:1819
target Thresh 31.999999677578987
target distance 11.0
model initialize at round 1819
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([20.99448222, 25.01101218,  2.28779316]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 11.056828439137197}
done in step count: 28
reward sum = 0.5562676980236513
running average episode reward sum: 0.47218297992619274
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([21.5946229 , 14.99126057,  4.73732624]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 1.0709472953599777}
episode index:1820
target Thresh 31.999999680787127
target distance 15.0
model initialize at round 1820
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  5.]), 'previousTarget': array([17.,  5.]), 'currentState': array([2.15769699, 7.83355938, 5.68962038]), 'targetState': array([17,  5], dtype=int32), 'currentDistance': 15.110361262966293}
done in step count: 32
reward sum = 0.48454667943785956
running average episode reward sum: 0.4721897694371821
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  5.]), 'previousTarget': array([17.,  5.]), 'currentState': array([16.14881454,  4.78977501,  6.2309718 ]), 'targetState': array([17,  5], dtype=int32), 'currentDistance': 0.8767617856803361}
episode index:1821
target Thresh 31.99999968396335
target distance 8.0
model initialize at round 1821
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 28.]), 'previousTarget': array([27., 28.]), 'currentState': array([21.        , 20.        ,  2.33182985]), 'targetState': array([27, 28], dtype=int32), 'currentDistance': 10.0}
done in step count: 26
reward sum = 0.6172129544108559
running average episode reward sum: 0.4722693650381556
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 28.]), 'previousTarget': array([27., 28.]), 'currentState': array([26.99154499, 27.1381974 ,  0.90228743]), 'targetState': array([27, 28], dtype=int32), 'currentDistance': 0.8618440785805013}
episode index:1822
target Thresh 31.999999687107966
target distance 16.0
model initialize at round 1822
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 27.]), 'previousTarget': array([ 5., 27.]), 'currentState': array([ 2.97255981, 10.99039478,  3.36722961]), 'targetState': array([ 5, 27], dtype=int32), 'currentDistance': 16.13747108366556}
done in step count: 43
reward sum = 0.4161275825757952
running average episode reward sum: 0.47223856866818176
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 27.]), 'previousTarget': array([ 5., 27.]), 'currentState': array([ 5.18177051, 26.13167644,  1.43876408]), 'targetState': array([ 5, 27], dtype=int32), 'currentDistance': 0.8871450425892017}
episode index:1823
target Thresh 31.999999690221294
target distance 19.0
model initialize at round 1823
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 29.]), 'previousTarget': array([11., 29.]), 'currentState': array([ 6.        , 10.        ,  5.74880582]), 'targetState': array([11, 29], dtype=int32), 'currentDistance': 19.646882704388503}
done in step count: 50
reward sum = 0.3406459110072688
running average episode reward sum: 0.4721664235707799
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 29.]), 'previousTarget': array([11., 29.]), 'currentState': array([10.78645403, 28.04273027,  1.385642  ]), 'targetState': array([11, 29], dtype=int32), 'currentDistance': 0.9807992708545609}
episode index:1824
target Thresh 31.999999693303643
target distance 15.0
model initialize at round 1824
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 26.]), 'previousTarget': array([25., 26.]), 'currentState': array([11.96800834, 10.98428179,  3.36333904]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 19.882268482162367}
done in step count: 68
reward sum = 0.30341372351376705
running average episode reward sum: 0.472073956337872
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 26.]), 'previousTarget': array([25., 26.]), 'currentState': array([24.20947163, 25.21801446,  0.73137752]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 1.1119516544271921}
episode index:1825
target Thresh 31.999999696355324
target distance 18.0
model initialize at round 1825
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7.13903609, 23.69700476]), 'previousTarget': array([ 7.26752934, 23.54026305]), 'currentState': array([20.91677186,  9.19962579,  2.08684695]), 'targetState': array([ 4, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 74
reward sum = 0.2648126691235754
running average episode reward sum: 0.47196045070412923
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 27.]), 'previousTarget': array([ 4., 27.]), 'currentState': array([ 4.71756919, 26.08512852,  2.30578387]), 'targetState': array([ 4, 27], dtype=int32), 'currentDistance': 1.1627103533080065}
episode index:1826
target Thresh 31.99999969937664
target distance 7.0
model initialize at round 1826
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 26.]), 'previousTarget': array([15., 26.]), 'currentState': array([22.04403466, 20.23548727,  1.63819483]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 9.10208939515064}
done in step count: 18
reward sum = 0.6781843265458282
running average episode reward sum: 0.4720733263887717
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 26.]), 'previousTarget': array([15., 26.]), 'currentState': array([15.97528829, 25.18304478,  2.47469137]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 1.2722433277041765}
episode index:1827
target Thresh 31.99999970236789
target distance 18.0
model initialize at round 1827
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.27992582, 24.78812091]), 'previousTarget': array([14.27881227, 24.78704435]), 'currentState': array([2.00234208, 9.00012112, 0.29627499]), 'targetState': array([16, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.2899436783183723
running average episode reward sum: 0.4719736931020811
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 27.]), 'previousTarget': array([16., 27.]), 'currentState': array([15.09175752, 26.35092019,  0.69851438]), 'targetState': array([16, 27], dtype=int32), 'currentDistance': 1.1163373159008685}
episode index:1828
target Thresh 31.99999970532938
target distance 27.0
model initialize at round 1828
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5.02305623, 20.78437852]), 'previousTarget': array([ 5.05371288, 20.75497521]), 'currentState': array([11.92961599,  2.01473728,  2.80305594]), 'targetState': array([ 2, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 91
reward sum = 0.16852376777058486
running average episode reward sum: 0.4718077828093903
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 29.]), 'previousTarget': array([ 2., 29.]), 'currentState': array([ 2.56590739, 28.20585317,  1.87006104]), 'targetState': array([ 2, 29], dtype=int32), 'currentDistance': 0.9751514533893882}
episode index:1829
target Thresh 31.999999708261402
target distance 20.0
model initialize at round 1829
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6.76121364, 23.9529684 ]), 'previousTarget': array([ 6.76121364, 23.9529684 ]), 'currentState': array([22.        , 11.        ,  1.38643932]), 'targetState': array([ 2, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.1918930086493999
running average episode reward sum: 0.4716548239164067
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 28.]), 'previousTarget': array([ 2., 28.]), 'currentState': array([ 2.84160273, 27.38882559,  2.44776208]), 'targetState': array([ 2, 28], dtype=int32), 'currentDistance': 1.0401102391259518}
episode index:1830
target Thresh 31.99999971116425
target distance 8.0
model initialize at round 1830
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([10.97326864,  2.04438227,  2.02418905]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 9.410625995623958}
done in step count: 21
reward sum = 0.6679783894384956
running average episode reward sum: 0.47176204596202226
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.06888821,  9.44754264,  0.81841744]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 1.0826718312663393}
episode index:1831
target Thresh 31.999999714038214
target distance 3.0
model initialize at round 1831
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 15.]), 'previousTarget': array([12., 15.]), 'currentState': array([10.        , 12.        ,  4.15444119]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 16
reward sum = 0.7777798876064329
running average episode reward sum: 0.4719290862685967
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 15.]), 'previousTarget': array([12., 15.]), 'currentState': array([11.17420128, 14.42150476,  0.62462057]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 1.0082659683969755}
episode index:1832
target Thresh 31.99999971688358
target distance 12.0
model initialize at round 1832
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 7.]), 'previousTarget': array([5., 7.]), 'currentState': array([ 6.8389572 , 18.8356064 ,  4.16359803]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 11.977618393685827}
done in step count: 26
reward sum = 0.5910885855195054
running average episode reward sum: 0.47199409417871724
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 7.]), 'previousTarget': array([5., 7.]), 'currentState': array([5.38473739, 7.9019781 , 4.56467175]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 0.9806056100745124}
episode index:1833
target Thresh 31.999999719700636
target distance 12.0
model initialize at round 1833
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 17.]), 'previousTarget': array([12., 17.]), 'currentState': array([6.91388777, 5.26024395, 1.82555343]), 'targetState': array([12, 17], dtype=int32), 'currentDistance': 12.79415529730703}
done in step count: 30
reward sum = 0.547690175703671
running average episode reward sum: 0.472035367941817
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 17.]), 'previousTarget': array([12., 17.]), 'currentState': array([11.98372909, 16.13893123,  0.95082559]), 'targetState': array([12, 17], dtype=int32), 'currentDistance': 0.8612224899785845}
episode index:1834
target Thresh 31.99999972248966
target distance 18.0
model initialize at round 1834
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 10.]), 'previousTarget': array([22., 10.]), 'currentState': array([4.        , 7.        , 1.43743753]), 'targetState': array([22, 10], dtype=int32), 'currentDistance': 18.24828759089466}
done in step count: 54
reward sum = 0.3910213762434293
running average episode reward sum: 0.47199121862753995
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 10.]), 'previousTarget': array([22., 10.]), 'currentState': array([21.02523867,  9.78429243,  0.29787959]), 'targetState': array([22, 10], dtype=int32), 'currentDistance': 0.9983433290386823}
episode index:1835
target Thresh 31.999999725250934
target distance 8.0
model initialize at round 1835
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([21.04000908, 29.02559357,  0.34787655]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 9.476911087907157}
done in step count: 47
reward sum = 0.497192682610494
running average episode reward sum: 0.47200494491511236
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([16.42763461, 21.91099725,  3.9836176 ]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 1.0063733640059276}
episode index:1836
target Thresh 31.999999727984733
target distance 13.0
model initialize at round 1836
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'previousTarget': array([7., 3.]), 'currentState': array([ 4.00429574, 15.5960221 ,  4.76765458]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 12.947355586201594}
done in step count: 28
reward sum = 0.5652805937839102
running average episode reward sum: 0.47205572098961907
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'previousTarget': array([7., 3.]), 'currentState': array([6.45613527, 3.86988232, 4.80875979]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 1.0259064737460255}
episode index:1837
target Thresh 31.999999730691332
target distance 15.0
model initialize at round 1837
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'previousTarget': array([10., 17.]), 'currentState': array([24.73222153,  5.95988685,  3.12555164]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 18.409846537808356}
done in step count: 44
reward sum = 0.398656280053945
running average episode reward sum: 0.4720157865821459
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'previousTarget': array([10., 17.]), 'currentState': array([10.9312106 , 16.12741372,  2.32051539]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 1.2761504589273398}
episode index:1838
target Thresh 31.999999733371
target distance 21.0
model initialize at round 1838
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.41603543, 20.54489741]), 'previousTarget': array([12.41603543, 20.54489741]), 'currentState': array([25.       ,  5.       ,  0.9049997]), 'targetState': array([ 8, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 72
reward sum = 0.1815716963894815
running average episode reward sum: 0.4718578506984087
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 26.]), 'previousTarget': array([ 8., 26.]), 'currentState': array([ 8.44722147, 25.0086872 ,  2.00716438]), 'targetState': array([ 8, 26], dtype=int32), 'currentDistance': 1.087523842135582}
episode index:1839
target Thresh 31.999999736024
target distance 8.0
model initialize at round 1839
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([14.93876649, 26.60952737,  4.64125191]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 7.609773734147203}
done in step count: 17
reward sum = 0.7367581571705384
running average episode reward sum: 0.472001818256274
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([14.92684894, 19.95484524,  4.73797989]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 0.9576432089839815}
episode index:1840
target Thresh 31.999999738650605
target distance 15.0
model initialize at round 1840
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7.31292785, 20.66355904]), 'previousTarget': array([ 7.35363499, 20.62110536]), 'currentState': array([20.9340619 ,  6.01894544,  2.74017212]), 'targetState': array([ 7, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.35824759634103454
running average episode reward sum: 0.47194002889075787
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 21.]), 'previousTarget': array([ 7., 21.]), 'currentState': array([ 7.70634041, 20.05727217,  2.26742592]), 'targetState': array([ 7, 21], dtype=int32), 'currentDistance': 1.1779866461849098}
episode index:1841
target Thresh 31.999999741251074
target distance 9.0
model initialize at round 1841
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'previousTarget': array([ 2., 17.]), 'currentState': array([11.        , 14.        ,  0.71926259]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 9.48683298050514}
done in step count: 27
reward sum = 0.6037535281181832
running average episode reward sum: 0.4720115888794807
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'previousTarget': array([ 2., 17.]), 'currentState': array([ 2.88940617, 16.87981261,  2.76331363]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 0.8974900297141175}
episode index:1842
target Thresh 31.99999974382567
target distance 13.0
model initialize at round 1842
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  6.]), 'previousTarget': array([24.,  6.]), 'currentState': array([11.01661258,  9.98556087,  5.76738074]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 13.581349134304668}
done in step count: 37
reward sum = 0.531207876536931
running average episode reward sum: 0.4720437084061532
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  6.]), 'previousTarget': array([24.,  6.]), 'currentState': array([23.2382139 ,  6.12549676,  5.98080473]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 0.7720540833373749}
episode index:1843
target Thresh 31.999999746374648
target distance 11.0
model initialize at round 1843
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 18.]), 'previousTarget': array([13., 18.]), 'currentState': array([2.        , 8.        , 2.82395104]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 14.866068747318506}
done in step count: 41
reward sum = 0.4564363416288344
running average episode reward sum: 0.4720352445413065
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 18.]), 'previousTarget': array([13., 18.]), 'currentState': array([12.2299679 , 17.30134919,  0.41709413]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 1.0397415022395802}
episode index:1844
target Thresh 31.99999974889826
target distance 17.0
model initialize at round 1844
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.78720009,  9.1378073 ]), 'previousTarget': array([26.79140314,  9.13497444]), 'currentState': array([ 9.99987245, 20.00912877,  1.33226776]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 61
reward sum = 0.3032113889365221
running average episode reward sum: 0.4719437410965343
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([26.01049163,  9.44170767,  5.47687687]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 1.083620084316446}
episode index:1845
target Thresh 31.999999751396764
target distance 8.0
model initialize at round 1845
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 16.]), 'previousTarget': array([11., 16.]), 'currentState': array([18.9594606 , 16.13805809,  2.08195835]), 'targetState': array([11, 16], dtype=int32), 'currentDistance': 7.960657828592614}
done in step count: 20
reward sum = 0.695059003924008
running average episode reward sum: 0.47206460526924693
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 16.]), 'previousTarget': array([11., 16.]), 'currentState': array([11.86256949, 16.27301549,  2.97616818]), 'targetState': array([11, 16], dtype=int32), 'currentDistance': 0.9047450407131219}
episode index:1846
target Thresh 31.99999975387041
target distance 15.0
model initialize at round 1846
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 17.]), 'previousTarget': array([12., 17.]), 'currentState': array([4.00888994, 2.20636724, 1.43411103]), 'targetState': array([12, 17], dtype=int32), 'currentDistance': 16.81396474207506}
done in step count: 40
reward sum = 0.441686262471524
running average episode reward sum: 0.47204815787195525
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 17.]), 'previousTarget': array([12., 17.]), 'currentState': array([11.93850231, 16.14045355,  0.99740633]), 'targetState': array([12, 17], dtype=int32), 'currentDistance': 0.8617436227121125}
episode index:1847
target Thresh 31.99999975631944
target distance 22.0
model initialize at round 1847
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.91786413, 12.81071492]), 'previousTarget': array([22.91786413, 12.81071492]), 'currentState': array([ 3.        , 11.        ,  5.15741765]), 'targetState': array([25, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.0735936249310966
running average episode reward sum: 0.47175289716697527
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.94238515, 12.59475993]), 'previousTarget': array([22.91395596, 12.59550943]), 'currentState': array([3.31933344, 8.7300683 , 5.30930656]), 'targetState': array([25, 13], dtype=int32), 'currentDistance': 20.0}
episode index:1848
target Thresh 31.999999758744103
target distance 6.0
model initialize at round 1848
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 17.]), 'previousTarget': array([ 6., 17.]), 'currentState': array([ 4.94612274, 22.6992723 ,  4.78761339]), 'targetState': array([ 6, 17], dtype=int32), 'currentDistance': 5.795891821187245}
done in step count: 11
reward sum = 0.7974334332884955
running average episode reward sum: 0.47192903591014534
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 17.]), 'previousTarget': array([ 6., 17.]), 'currentState': array([ 6.14904673, 17.99942368,  4.91104243]), 'targetState': array([ 6, 17], dtype=int32), 'currentDistance': 1.010476429400518}
episode index:1849
target Thresh 31.99999976114464
target distance 17.0
model initialize at round 1849
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 21.]), 'previousTarget': array([ 4., 21.]), 'currentState': array([21.01185011, 28.97352877,  5.12744889]), 'targetState': array([ 4, 21], dtype=int32), 'currentDistance': 18.78776743387478}
done in step count: 73
reward sum = 0.3067230816063135
running average episode reward sum: 0.4718397353943054
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 21.]), 'previousTarget': array([ 4., 21.]), 'currentState': array([ 4.94821329, 20.96223678,  3.13199931]), 'targetState': array([ 4, 21], dtype=int32), 'currentDistance': 0.9489649673192505}
episode index:1850
target Thresh 31.999999763521288
target distance 3.0
model initialize at round 1850
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 22.]), 'previousTarget': array([ 7., 22.]), 'currentState': array([ 9.97346132, 19.19320126,  1.93352696]), 'targetState': array([ 7, 22], dtype=int32), 'currentDistance': 4.088959694661374}
done in step count: 7
reward sum = 0.8697233294549634
running average episode reward sum: 0.4720546914148676
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 22.]), 'previousTarget': array([ 7., 22.]), 'currentState': array([ 7.91990703, 21.24216835,  2.40638144]), 'targetState': array([ 7, 22], dtype=int32), 'currentDistance': 1.1918631456307491}
episode index:1851
target Thresh 31.999999765874293
target distance 10.0
model initialize at round 1851
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 26.]), 'previousTarget': array([13., 26.]), 'currentState': array([ 3.26130329, 17.10322729,  0.54379897]), 'targetState': array([13, 26], dtype=int32), 'currentDistance': 13.190708027802724}
done in step count: 27
reward sum = 0.5658844633421927
running average episode reward sum: 0.47210535543858645
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 26.]), 'previousTarget': array([13., 26.]), 'currentState': array([12.00066968, 25.00324861,  0.69737797]), 'targetState': array([13, 26], dtype=int32), 'currentDistance': 1.4114440926576972}
episode index:1852
target Thresh 31.99999976820388
target distance 15.0
model initialize at round 1852
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 5.]), 'previousTarget': array([7., 5.]), 'currentState': array([22.03332424, 10.98607743,  5.66220865]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 16.18128427280187}
done in step count: 59
reward sum = 0.3749026505696415
running average episode reward sum: 0.47205289850125837
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 5.]), 'previousTarget': array([7., 5.]), 'currentState': array([7.97949803, 5.53547368, 3.26348673]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 1.1163101932839103}
episode index:1853
target Thresh 31.99999977051029
target distance 13.0
model initialize at round 1853
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 26.]), 'previousTarget': array([22., 26.]), 'currentState': array([19.89984432, 13.13342445,  2.12261657]), 'targetState': array([22, 26], dtype=int32), 'currentDistance': 13.03684855204114}
done in step count: 35
reward sum = 0.539580605772177
running average episode reward sum: 0.4720893212128392
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 26.]), 'previousTarget': array([22., 26.]), 'currentState': array([21.95158431, 25.21163516,  1.4027182 ]), 'targetState': array([22, 26], dtype=int32), 'currentDistance': 0.7898501105601633}
episode index:1854
target Thresh 31.999999772793753
target distance 13.0
model initialize at round 1854
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'previousTarget': array([26., 11.]), 'currentState': array([13.        , 19.        ,  0.78447899]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 15.264337522473749}
done in step count: 52
reward sum = 0.4300446446725458
running average episode reward sum: 0.47206665561901695
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'previousTarget': array([26., 11.]), 'currentState': array([25.08428384, 11.23248189,  5.38851283]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 0.9447665877361368}
episode index:1855
target Thresh 31.99999977505449
target distance 8.0
model initialize at round 1855
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([7.10514477, 3.9890617 , 0.14884203]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 10.558503438881067}
done in step count: 22
reward sum = 0.63179064138521
running average episode reward sum: 0.47215271380100304
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.12863803, 10.43727078,  0.60201753]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.0372732840068961}
episode index:1856
target Thresh 31.999999777292736
target distance 7.0
model initialize at round 1856
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 17.]), 'previousTarget': array([15., 17.]), 'currentState': array([21.87514075, 17.1212582 ,  2.62332487]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 6.87620999036004}
done in step count: 15
reward sum = 0.7436866678479198
running average episode reward sum: 0.4722989356394774
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 17.]), 'previousTarget': array([15., 17.]), 'currentState': array([15.87145381, 17.37783855,  2.89261727]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 0.9498387847006494}
episode index:1857
target Thresh 31.999999779508713
target distance 17.0
model initialize at round 1857
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([26.        , 21.        ,  1.20085666]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 19.235384061671343}
done in step count: 62
reward sum = 0.3103589945080192
running average episode reward sum: 0.4722117774365003
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([17.11608081,  4.79240728,  4.50763988]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 0.8008645675078246}
episode index:1858
target Thresh 31.999999781702638
target distance 18.0
model initialize at round 1858
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.90599608,  7.35899411]), 'previousTarget': array([13.90599608,  7.35899411]), 'currentState': array([25.        , 24.        ,  0.82634374]), 'targetState': array([13,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.19437048165876064
running average episode reward sum: 0.47206232004232185
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  6.]), 'previousTarget': array([13.,  6.]), 'currentState': array([13.687142  ,  6.49313056,  3.4560409 ]), 'targetState': array([13,  6], dtype=int32), 'currentDistance': 0.8457788613726418}
episode index:1859
target Thresh 31.99999978387473
target distance 6.0
model initialize at round 1859
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 28.]), 'previousTarget': array([12., 28.]), 'currentState': array([11.        , 22.        ,  3.21690741]), 'targetState': array([12, 28], dtype=int32), 'currentDistance': 6.08276253029822}
done in step count: 17
reward sum = 0.7326811507853517
running average episode reward sum: 0.4722024376932589
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 28.]), 'previousTarget': array([12., 28.]), 'currentState': array([12.09681261, 27.13671545,  1.26347099]), 'targetState': array([12, 28], dtype=int32), 'currentDistance': 0.8686960868824658}
episode index:1860
target Thresh 31.999999786025214
target distance 15.0
model initialize at round 1860
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 19.]), 'previousTarget': array([19., 19.]), 'currentState': array([ 4.00871534, 27.93109434,  5.04655552]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 17.45001609733412}
done in step count: 52
reward sum = 0.40571917971280813
running average episode reward sum: 0.47216671321288256
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 19.]), 'previousTarget': array([19., 19.]), 'currentState': array([18.07341871, 19.03898373,  5.7560033 ]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 0.9274010060258309}
episode index:1861
target Thresh 31.999999788154298
target distance 19.0
model initialize at round 1861
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([15.13524205,  2.19587299,  1.04111467]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 18.824000605778664}
done in step count: 59
reward sum = 0.35745085957321115
running average episode reward sum: 0.47210510426893
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([15.83267244, 20.21953711,  1.24055124]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 0.7981984908342723}
episode index:1862
target Thresh 31.999999790262198
target distance 9.0
model initialize at round 1862
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([23.99135101, 12.88536967,  4.49341744]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 9.44297363719628}
done in step count: 21
reward sum = 0.6550014493988011
running average episode reward sum: 0.47220327729369105
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.71946532, 10.45262622,  3.29839574]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.8500004994047814}
episode index:1863
target Thresh 31.999999792349126
target distance 22.0
model initialize at round 1863
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.58616133,  5.30819378]), 'previousTarget': array([18.57770876,  5.3226018 ]), 'currentState': array([15.05661914, 24.99428866,  5.9315504 ]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 90
reward sum = 0.21608359624117654
running average episode reward sum: 0.4720658740313239
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'previousTarget': array([19.,  3.]), 'currentState': array([19.09922263,  3.72395598,  4.16996993]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 0.7307238819502144}
episode index:1864
target Thresh 31.999999794415285
target distance 23.0
model initialize at round 1864
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.03231343, 10.40108266]), 'previousTarget': array([21.88993593, 10.4295875 ]), 'currentState': array([ 3.17358507, 17.06053944,  0.121595  ]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.12480712908990793
running average episode reward sum: 0.4717458348875591
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([22.48299657, 12.69720314,  5.42785174]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 5.102805521802422}
episode index:1865
target Thresh 31.999999796460887
target distance 1.0
model initialize at round 1865
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([13.68543893,  7.2843366 ,  2.36081439]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 0.9909594485242276}
done in step count: 0
reward sum = 0.9924741602236719
running average episode reward sum: 0.47202489615515614
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([13.68543893,  7.2843366 ,  2.36081439]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 0.9909594485242276}
episode index:1866
target Thresh 31.999999798486137
target distance 2.0
model initialize at round 1866
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'previousTarget': array([7., 3.]), 'currentState': array([5.44186439, 3.00829988, 6.25072854]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 1.5581577184320587}
done in step count: 2
reward sum = 0.9601651047785639
running average episode reward sum: 0.4722863531495982
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'previousTarget': array([7., 3.]), 'currentState': array([6.19767182, 2.93719981, 6.12132044]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 0.8047821857743004}
episode index:1867
target Thresh 31.99999980049123
target distance 10.0
model initialize at round 1867
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([13.73005102, 11.61253745,  4.09663821]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 12.99421771773631}
done in step count: 27
reward sum = 0.5671683316699574
running average episode reward sum: 0.4723371464999838
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.8974556 , 3.74912615, 3.86026604]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.1690237544751365}
episode index:1868
target Thresh 31.999999802476378
target distance 10.0
model initialize at round 1868
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 15.]), 'previousTarget': array([ 5., 15.]), 'currentState': array([15.       , 15.       ,  1.1139611]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 10.0}
done in step count: 28
reward sum = 0.5941568522211373
running average episode reward sum: 0.4724023255827667
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 15.]), 'previousTarget': array([ 5., 15.]), 'currentState': array([ 5.94789175, 15.06125479,  3.11283994]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 0.9498688916118788}
episode index:1869
target Thresh 31.99999980444177
target distance 13.0
model initialize at round 1869
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 26.]), 'previousTarget': array([10., 26.]), 'currentState': array([19.       , 13.       ,  0.3848353]), 'targetState': array([10, 26], dtype=int32), 'currentDistance': 15.811388300841896}
done in step count: 39
reward sum = 0.44053142525688305
running average episode reward sum: 0.47238528232056026
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 26.]), 'previousTarget': array([10., 26.]), 'currentState': array([10.79633395, 25.18659221,  2.01080151]), 'targetState': array([10, 26], dtype=int32), 'currentDistance': 1.1383233213520896}
episode index:1870
target Thresh 31.999999806387606
target distance 16.0
model initialize at round 1870
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.57388637, 10.33913459]), 'previousTarget': array([20.59074408, 10.32117742]), 'currentState': array([ 5.99412993, 24.0296679 ,  1.51363277]), 'targetState': array([22,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.252497317933423
running average episode reward sum: 0.47226775802104815
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  9.]), 'previousTarget': array([22.,  9.]), 'currentState': array([21.12346725,  9.64112043,  5.27392044]), 'targetState': array([22,  9], dtype=int32), 'currentDistance': 1.085976545283941}
episode index:1871
target Thresh 31.999999808314083
target distance 10.0
model initialize at round 1871
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([24.78686619,  1.88702182,  3.38443616]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 11.041978797162033}
done in step count: 22
reward sum = 0.6069089028186672
running average episode reward sum: 0.4723396817095084
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.75124696,  6.67048758,  2.48379891]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.820335558918284}
episode index:1872
target Thresh 31.99999981022139
target distance 22.0
model initialize at round 1872
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8.82543095, 10.24593376]), 'previousTarget': array([ 8.83486126, 10.20413153]), 'currentState': array([ 2.02010709, 29.05251684,  0.96284631]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.22358394589148406
running average episode reward sum: 0.4722068703182548
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([9.84337306, 7.83289429, 4.72176033]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 0.8474933013031113}
episode index:1873
target Thresh 31.999999812109717
target distance 15.0
model initialize at round 1873
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'previousTarget': array([ 2., 17.]), 'currentState': array([16.98109035, 17.9393149 ,  4.15782166]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 15.010509005539872}
done in step count: 37
reward sum = 0.47678346264430593
running average episode reward sum: 0.4722093124699763
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'previousTarget': array([ 2., 17.]), 'currentState': array([ 2.8586369 , 16.96981403,  3.23630613]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 0.8591673384478348}
episode index:1874
target Thresh 31.999999813979258
target distance 12.0
model initialize at round 1874
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 22.]), 'previousTarget': array([ 7., 22.]), 'currentState': array([19.        , 17.        ,  1.69511747]), 'targetState': array([ 7, 22], dtype=int32), 'currentDistance': 13.0}
done in step count: 30
reward sum = 0.537695111300833
running average episode reward sum: 0.4722442382293528
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 22.]), 'previousTarget': array([ 7., 22.]), 'currentState': array([ 7.89958263, 21.20983107,  2.60945027]), 'targetState': array([ 7, 22], dtype=int32), 'currentDistance': 1.1973369820789987}
episode index:1875
target Thresh 31.999999815830193
target distance 4.0
model initialize at round 1875
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([19.99793681,  5.09623513,  1.84473205]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 4.098815648101306}
done in step count: 9
reward sum = 0.8372060639822357
running average episode reward sum: 0.47243878078039375
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.93581209,  5.73836484,  2.94915343]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.971698115116811}
episode index:1876
target Thresh 31.999999817662715
target distance 4.0
model initialize at round 1876
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([7.        , 5.        , 6.23978874]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 5.0}
done in step count: 13
reward sum = 0.769132733831043
running average episode reward sum: 0.47259684894930726
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.64199789, 8.0014467 , 2.19046548]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.1871267790496514}
episode index:1877
target Thresh 31.999999819477
target distance 17.0
model initialize at round 1877
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  3.]), 'previousTarget': array([22.,  3.]), 'currentState': array([25.00344658, 20.00137484,  0.12705898]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 17.2646296785807}
done in step count: 45
reward sum = 0.38830333192257505
running average episode reward sum: 0.47255196422245593
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  3.]), 'previousTarget': array([22.,  3.]), 'currentState': array([22.21333277,  3.88511744,  4.21207302]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 0.9104634795288294}
episode index:1878
target Thresh 31.999999821273235
target distance 13.0
model initialize at round 1878
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([11.00388759,  4.00357916,  0.99086332]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 13.3390515139437}
done in step count: 31
reward sum = 0.5294772801839941
running average episode reward sum: 0.4725822597604876
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([ 8.75358543, 16.14922858,  1.41226301]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 1.136531127218728}
episode index:1879
target Thresh 31.999999823051596
target distance 6.0
model initialize at round 1879
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 17.]), 'previousTarget': array([ 7., 17.]), 'currentState': array([ 7.94615911, 23.06274491,  2.48103055]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 6.13613012515661}
done in step count: 16
reward sum = 0.7251462953230376
running average episode reward sum: 0.4727166023325953
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 17.]), 'previousTarget': array([ 7., 17.]), 'currentState': array([ 6.86148632, 17.83049606,  4.61414047]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 0.8419677774028591}
episode index:1880
target Thresh 31.99999982481226
target distance 3.0
model initialize at round 1880
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 23.]), 'previousTarget': array([ 6., 23.]), 'currentState': array([ 5.        , 20.        ,  4.58666151]), 'targetState': array([ 6, 23], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 17
reward sum = 0.7651265501666773
running average episode reward sum: 0.4728720568503168
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 23.]), 'previousTarget': array([ 6., 23.]), 'currentState': array([ 5.18984659, 22.16462491,  0.84094391]), 'targetState': array([ 6, 23], dtype=int32), 'currentDistance': 1.1637010301623631}
episode index:1881
target Thresh 31.99999982655541
target distance 20.0
model initialize at round 1881
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6.86364966, 15.0628849 ]), 'previousTarget': array([ 7.11145618, 14.94427191]), 'currentState': array([24.73185872,  6.07805853,  2.93115894]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.30596671496751343
running average episode reward sum: 0.472783371758987
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 16.]), 'previousTarget': array([ 5., 16.]), 'currentState': array([ 5.96670557, 15.89790169,  2.37996896]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 0.9720821591671014}
episode index:1882
target Thresh 31.99999982828121
target distance 4.0
model initialize at round 1882
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  9.]), 'previousTarget': array([22.,  9.]), 'currentState': array([24.61772808,  5.22027828,  2.44516528]), 'targetState': array([22,  9], dtype=int32), 'currentDistance': 4.597694707952979}
done in step count: 9
reward sum = 0.8420442408314219
running average episode reward sum: 0.4729794741854726
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  9.]), 'previousTarget': array([22.,  9.]), 'currentState': array([22.59733854,  8.15403194,  2.08813973]), 'targetState': array([22,  9], dtype=int32), 'currentDistance': 1.035603825086206}
episode index:1883
target Thresh 31.999999829989843
target distance 9.0
model initialize at round 1883
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([19.95142723, 16.65868078,  4.34703499]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 10.590029159356689}
done in step count: 21
reward sum = 0.6319513324148966
running average episode reward sum: 0.4730638541526857
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.96704526, 11.4919223 ,  3.71510009]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.0849719240740883}
episode index:1884
target Thresh 31.99999983168147
target distance 3.0
model initialize at round 1884
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([ 8.8645476 , 26.4803585 ,  1.74011159]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 2.763665215705578}
done in step count: 4
reward sum = 0.9145425319760876
running average episode reward sum: 0.47329806034781746
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([ 9.00981225, 28.23809795,  1.20157029]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 1.2493864510863102}
episode index:1885
target Thresh 31.99999983335627
target distance 3.0
model initialize at round 1885
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([12.65641424, 11.64812069,  3.94843999]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 3.750877204366389}
done in step count: 6
reward sum = 0.8819194783721535
running average episode reward sum: 0.4735147206967169
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.86621415,  9.54405202,  3.98057714]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 1.0228976290254583}
episode index:1886
target Thresh 31.9999998350144
target distance 7.0
model initialize at round 1886
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  8.]), 'previousTarget': array([24.,  8.]), 'currentState': array([17.        , 13.        ,  2.04893619]), 'targetState': array([24,  8], dtype=int32), 'currentDistance': 8.602325267042627}
done in step count: 21
reward sum = 0.6474124801637274
running average episode reward sum: 0.4736068763721102
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  8.]), 'previousTarget': array([24.,  8.]), 'currentState': array([23.14142159,  8.97859936,  5.64742508]), 'targetState': array([24,  8], dtype=int32), 'currentDistance': 1.3018500621897895}
episode index:1887
target Thresh 31.999999836656034
target distance 16.0
model initialize at round 1887
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5.3040568 , 25.75488146]), 'previousTarget': array([ 5.47772  , 25.6118525]), 'currentState': array([20.8745333 , 13.20258041,  2.30147424]), 'targetState': array([ 5, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.33086656082021215
running average episode reward sum: 0.47353127239141535
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 26.]), 'previousTarget': array([ 5., 26.]), 'currentState': array([ 5.93988624, 25.55204004,  2.08433936]), 'targetState': array([ 5, 26], dtype=int32), 'currentDistance': 1.041179271372867}
episode index:1888
target Thresh 31.999999838281337
target distance 10.0
model initialize at round 1888
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([ 3.        , 19.        ,  1.02571952]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 12.806248474865697}
done in step count: 32
reward sum = 0.5282725226534651
running average episode reward sum: 0.4735602513486742
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.15554075, 11.63358404,  5.49362507]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.0557178390954904}
episode index:1889
target Thresh 31.999999839890464
target distance 14.0
model initialize at round 1889
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 16.]), 'previousTarget': array([11., 16.]), 'currentState': array([25.        , 25.        ,  0.34988037]), 'targetState': array([11, 16], dtype=int32), 'currentDistance': 16.643316977093235}
done in step count: 42
reward sum = 0.39105318774078324
running average episode reward sum: 0.4735165968176647
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 16.]), 'previousTarget': array([11., 16.]), 'currentState': array([11.89735726, 16.3356743 ,  3.6592427 ]), 'targetState': array([11, 16], dtype=int32), 'currentDistance': 0.9580852174435236}
episode index:1890
target Thresh 31.99999984148358
target distance 9.0
model initialize at round 1890
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 14.]), 'previousTarget': array([25., 14.]), 'currentState': array([16.02736317, 19.02671555,  0.58609056]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 10.284749918441786}
done in step count: 24
reward sum = 0.6324457559136982
running average episode reward sum: 0.4736006418515601
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 14.]), 'previousTarget': array([25., 14.]), 'currentState': array([24.06842777, 14.49159273,  5.47622491]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 1.0533234273614456}
episode index:1891
target Thresh 31.999999843060845
target distance 6.0
model initialize at round 1891
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  2.]), 'previousTarget': array([17.,  2.]), 'currentState': array([19.87271357,  8.04024269,  3.08787823]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 6.688573467084108}
done in step count: 15
reward sum = 0.7315049738269174
running average episode reward sum: 0.47373695492342865
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  2.]), 'previousTarget': array([17.,  2.]), 'currentState': array([16.8755343 ,  2.76328002,  4.04610847]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 0.7733615560948853}
episode index:1892
target Thresh 31.999999844622415
target distance 12.0
model initialize at round 1892
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([26.00111562, 17.93539019,  4.47715425]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 13.026014301124398}
done in step count: 30
reward sum = 0.5090704986656749
running average episode reward sum: 0.47375562029254764
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([14.73046049, 22.73795544,  2.73037371]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 0.776041160661718}
episode index:1893
target Thresh 31.999999846168446
target distance 3.0
model initialize at round 1893
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([4.        , 7.        , 5.64128685]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 9
reward sum = 0.8354368187394341
running average episode reward sum: 0.4739465818545576
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([5.45380399, 9.23975732, 1.09053413]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.9361084386853374}
episode index:1894
target Thresh 31.999999847699097
target distance 23.0
model initialize at round 1894
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5.3536948 , 26.40672412]), 'previousTarget': array([ 5.2957649, 26.4268235]), 'currentState': array([25.04790962, 22.92277482,  5.01516533]), 'targetState': array([ 2, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.22447910540359667
running average episode reward sum: 0.47381493674825104
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 27.]), 'previousTarget': array([ 2., 27.]), 'currentState': array([ 2.78510144, 27.0218272 ,  2.46128935]), 'targetState': array([ 2, 27], dtype=int32), 'currentDistance': 0.7854047962885236}
episode index:1895
target Thresh 31.999999849214518
target distance 6.0
model initialize at round 1895
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 21.]), 'previousTarget': array([21., 21.]), 'currentState': array([17.83528051, 15.20832139,  2.00774738]), 'targetState': array([21, 21], dtype=int32), 'currentDistance': 6.59992353267111}
done in step count: 13
reward sum = 0.7447391660241586
running average episode reward sum: 0.47395782927424046
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 21.]), 'previousTarget': array([21., 21.]), 'currentState': array([20.75117747, 20.27511662,  0.94990544]), 'targetState': array([21, 21], dtype=int32), 'currentDistance': 0.7663997447450519}
episode index:1896
target Thresh 31.999999850714858
target distance 17.0
model initialize at round 1896
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.02065746, 17.20737294]), 'previousTarget': array([10.00324289, 17.23243274]), 'currentState': array([24.98175198,  3.93463204,  4.19797692]), 'targetState': array([ 8, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.26122876381770915
running average episode reward sum: 0.47384568954548106
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 19.]), 'previousTarget': array([ 8., 19.]), 'currentState': array([ 8.86986261, 18.41737234,  2.30049248]), 'targetState': array([ 8, 19], dtype=int32), 'currentDistance': 1.0469555583121788}
episode index:1897
target Thresh 31.99999985220027
target distance 8.0
model initialize at round 1897
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([ 7.96472086, 16.9868949 ,  3.74976397]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 8.277285923186508}
done in step count: 23
reward sum = 0.6360291003282096
running average episode reward sum: 0.47393113918235286
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([15.24971279, 14.87429985,  5.99003287]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 0.7607439965308141}
episode index:1898
target Thresh 31.9999998536709
target distance 11.0
model initialize at round 1898
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 21.]), 'previousTarget': array([22., 21.]), 'currentState': array([11.95816253, 10.03078997,  2.29307559]), 'targetState': array([22, 21], dtype=int32), 'currentDistance': 14.871518705966004}
done in step count: 34
reward sum = 0.4853244109536635
running average episode reward sum: 0.4739371387988728
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 21.]), 'previousTarget': array([22., 21.]), 'currentState': array([21.0453835 , 20.06589127,  0.71609245]), 'targetState': array([22, 21], dtype=int32), 'currentDistance': 1.33560914405394}
episode index:1899
target Thresh 31.999999855126898
target distance 16.0
model initialize at round 1899
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 24.]), 'previousTarget': array([13., 24.]), 'currentState': array([4.05579431, 8.31559141, 1.21401429]), 'targetState': array([13, 24], dtype=int32), 'currentDistance': 18.055455911274816}
done in step count: 42
reward sum = 0.4138001864233255
running average episode reward sum: 0.47390548777130675
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 24.]), 'previousTarget': array([13., 24.]), 'currentState': array([12.44187474, 23.11271829,  0.99729132]), 'targetState': array([13, 24], dtype=int32), 'currentDistance': 1.048223558337355}
episode index:1900
target Thresh 31.99999985656841
target distance 5.0
model initialize at round 1900
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([8.44449729, 4.22711056, 0.57401475]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 5.9150063125953825}
done in step count: 11
reward sum = 0.8011072653937578
running average episode reward sum: 0.4740776086432807
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([12.16527025,  7.28545316,  0.66595551]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 1.0987952184225565}
episode index:1901
target Thresh 31.99999985799558
target distance 8.0
model initialize at round 1901
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 13.]), 'previousTarget': array([11., 13.]), 'currentState': array([19.        , 18.        ,  1.45653075]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 9.433981132056603}
done in step count: 24
reward sum = 0.6175704371697638
running average episode reward sum: 0.474153051770792
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 13.]), 'previousTarget': array([11., 13.]), 'currentState': array([11.95309896, 13.89056559,  3.53950939]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 1.3044173823489524}
episode index:1902
target Thresh 31.999999859408547
target distance 20.0
model initialize at round 1902
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.60700849,  9.12283287]), 'previousTarget': array([19.60700849,  9.12283287]), 'currentState': array([13.        , 28.        ,  3.67714846]), 'targetState': array([20,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.2920929520520364
running average episode reward sum: 0.47405738172364603
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  8.]), 'previousTarget': array([20.,  8.]), 'currentState': array([19.76029701,  8.79145629,  4.86852441]), 'targetState': array([20,  8], dtype=int32), 'currentDistance': 0.8269586320001133}
episode index:1903
target Thresh 31.999999860807456
target distance 11.0
model initialize at round 1903
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 13.]), 'previousTarget': array([ 9., 13.]), 'currentState': array([20.        , 11.        ,  5.25680849]), 'targetState': array([ 9, 13], dtype=int32), 'currentDistance': 11.180339887498949}
done in step count: 28
reward sum = 0.5534076537798465
running average episode reward sum: 0.4740990572867007
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 13.]), 'previousTarget': array([ 9., 13.]), 'currentState': array([ 9.62873471, 12.71155709,  2.856019  ]), 'targetState': array([ 9, 13], dtype=int32), 'currentDistance': 0.691741749802754}
episode index:1904
target Thresh 31.999999862192443
target distance 15.0
model initialize at round 1904
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.76971688,  9.24843199]), 'previousTarget': array([26.64636501,  9.37889464]), 'currentState': array([13.17349989, 23.91618177,  5.60370734]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.3550594674277044
running average episode reward sum: 0.47403656931302146
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([26.3305338 ,  9.97540706,  5.18244909]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 1.1830485730042462}
episode index:1905
target Thresh 31.99999986356365
target distance 8.0
model initialize at round 1905
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 22.]), 'previousTarget': array([20., 22.]), 'currentState': array([16.13449469, 14.08505684,  0.74068722]), 'targetState': array([20, 22], dtype=int32), 'currentDistance': 8.808430990831475}
done in step count: 19
reward sum = 0.6897899536351164
running average episode reward sum: 0.4741497662617739
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 22.]), 'previousTarget': array([20., 22.]), 'currentState': array([19.88095929, 21.20902019,  0.89613305]), 'targetState': array([20, 22], dtype=int32), 'currentDistance': 0.7998873385066}
episode index:1906
target Thresh 31.999999864921218
target distance 4.0
model initialize at round 1906
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 14.]), 'previousTarget': array([23., 14.]), 'currentState': array([18.9589862 , 11.15386124,  1.60154194]), 'targetState': array([23, 14], dtype=int32), 'currentDistance': 4.942701529521126}
done in step count: 11
reward sum = 0.8118979565188638
running average episode reward sum: 0.47432687595776607
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 14.]), 'previousTarget': array([23., 14.]), 'currentState': array([22.1442167 , 13.63307186,  0.38153458]), 'targetState': array([23, 14], dtype=int32), 'currentDistance': 0.9311290595434913}
episode index:1907
target Thresh 31.999999866265274
target distance 3.0
model initialize at round 1907
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 18.]), 'previousTarget': array([27., 18.]), 'currentState': array([24.00623159, 21.01254077,  0.9377816 ]), 'targetState': array([27, 18], dtype=int32), 'currentDistance': 4.247122689361345}
done in step count: 9
reward sum = 0.8257491881111904
running average episode reward sum: 0.47451105955952366
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 18.]), 'previousTarget': array([27., 18.]), 'currentState': array([26.24985752, 18.8904343 ,  5.16253322]), 'targetState': array([27, 18], dtype=int32), 'currentDistance': 1.164296773750128}
episode index:1908
target Thresh 31.999999867595957
target distance 11.0
model initialize at round 1908
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 27.]), 'previousTarget': array([10., 27.]), 'currentState': array([ 2.98530301, 15.98764719,  3.58820513]), 'targetState': array([10, 27], dtype=int32), 'currentDistance': 13.056718126664393}
done in step count: 28
reward sum = 0.4952490176009053
running average episode reward sum: 0.4745219228167481
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 27.]), 'previousTarget': array([10., 27.]), 'currentState': array([ 9.1148832 , 26.10095054,  1.00703251]), 'targetState': array([10, 27], dtype=int32), 'currentDistance': 1.261634529997342}
episode index:1909
target Thresh 31.9999998689134
target distance 10.0
model initialize at round 1909
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 23.]), 'previousTarget': array([17., 23.]), 'currentState': array([ 7.33347564, 23.97488793,  6.26521596]), 'targetState': array([17, 23], dtype=int32), 'currentDistance': 9.715559664191105}
done in step count: 25
reward sum = 0.6537090963214656
running average episode reward sum: 0.4746157380908343
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 23.]), 'previousTarget': array([17., 23.]), 'currentState': array([16.17659374, 22.62461439,  6.02979904]), 'targetState': array([17, 23], dtype=int32), 'currentDistance': 0.9049376927524134}
episode index:1910
target Thresh 31.99999987021773
target distance 7.0
model initialize at round 1910
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  8.]), 'previousTarget': array([17.,  8.]), 'currentState': array([15.2097931 , 14.66786885,  5.14577767]), 'targetState': array([17,  8], dtype=int32), 'currentDistance': 6.904007226700396}
done in step count: 15
reward sum = 0.7504509726817615
running average episode reward sum: 0.47476007887293314
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  8.]), 'previousTarget': array([17.,  8.]), 'currentState': array([17.01802101,  8.69435293,  4.88128235]), 'targetState': array([17,  8], dtype=int32), 'currentDistance': 0.6945867466781291}
episode index:1911
target Thresh 31.999999871509086
target distance 8.0
model initialize at round 1911
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 17.]), 'previousTarget': array([12., 17.]), 'currentState': array([16.99937226, 25.0104437 ,  1.8833307 ]), 'targetState': array([12, 17], dtype=int32), 'currentDistance': 9.44250661951351}
done in step count: 22
reward sum = 0.6080163205489706
running average episode reward sum: 0.4748297735600022
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 17.]), 'previousTarget': array([12., 17.]), 'currentState': array([12.01826791, 17.83151829,  4.22658013]), 'targetState': array([12, 17], dtype=int32), 'currentDistance': 0.8317189349294961}
episode index:1912
target Thresh 31.999999872787594
target distance 4.0
model initialize at round 1912
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'previousTarget': array([10., 12.]), 'currentState': array([13.56628201,  9.24700938,  2.5203131 ]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 4.505255237331931}
done in step count: 8
reward sum = 0.8518970595951177
running average episode reward sum: 0.4750268813937895
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'previousTarget': array([10., 12.]), 'currentState': array([10.93547444, 11.49591424,  2.3489115 ]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 1.0626452330749978}
episode index:1913
target Thresh 31.999999874053376
target distance 9.0
model initialize at round 1913
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([ 2.18511445, 22.68548229,  5.02985427]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 8.873072304787842}
done in step count: 17
reward sum = 0.6901361695057117
running average episode reward sum: 0.4751392686916536
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([ 3.77635132, 14.89382165,  5.08566592]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 0.9213771651426494}
episode index:1914
target Thresh 31.999999875306568
target distance 18.0
model initialize at round 1914
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 16.]), 'previousTarget': array([26., 16.]), 'currentState': array([8.36006848e+00, 1.09630342e+01, 1.73247954e-03]), 'targetState': array([26, 16], dtype=int32), 'currentDistance': 18.344977755077135}
done in step count: 44
reward sum = 0.4049512139690333
running average episode reward sum: 0.47510261696594996
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 16.]), 'previousTarget': array([26., 16.]), 'currentState': array([25.00969484, 15.46901387,  0.12241208]), 'targetState': array([26, 16], dtype=int32), 'currentDistance': 1.1236772589977355}
episode index:1915
target Thresh 31.99999987654729
target distance 14.0
model initialize at round 1915
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([11.14861991, 23.56325562,  5.15384927]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 18.68474974609971}
done in step count: 45
reward sum = 0.3992415442427817
running average episode reward sum: 0.4750630235041946
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([23.11998021, 10.58835323,  5.54223951]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 1.0585812892709003}
episode index:1916
target Thresh 31.99999987777566
target distance 7.0
model initialize at round 1916
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 20.]), 'previousTarget': array([13., 20.]), 'currentState': array([16.60029428, 13.12320575,  2.62565365]), 'targetState': array([13, 20], dtype=int32), 'currentDistance': 7.7622431108109895}
done in step count: 17
reward sum = 0.7186169720024553
running average episode reward sum: 0.4751900730339277
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 20.]), 'previousTarget': array([13., 20.]), 'currentState': array([13.43495251, 19.12460233,  1.74236976]), 'targetState': array([13, 20], dtype=int32), 'currentDistance': 0.9774992399584325}
episode index:1917
target Thresh 31.999999878991815
target distance 12.0
model initialize at round 1917
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 17.]), 'previousTarget': array([20., 17.]), 'currentState': array([25.62603326,  5.21925354,  2.37502101]), 'targetState': array([20, 17], dtype=int32), 'currentDistance': 13.055199636066869}
done in step count: 30
reward sum = 0.5516514573391587
running average episode reward sum: 0.475229938197799
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 17.]), 'previousTarget': array([20., 17.]), 'currentState': array([20.88770618, 16.19313577,  1.98111575]), 'targetState': array([20, 17], dtype=int32), 'currentDistance': 1.1996049962138067}
episode index:1918
target Thresh 31.999999880195865
target distance 10.0
model initialize at round 1918
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 20.]), 'previousTarget': array([ 3., 20.]), 'currentState': array([12.93335656, 18.07044435,  2.5342631 ]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 10.119029474207329}
done in step count: 22
reward sum = 0.6560510929352311
running average episode reward sum: 0.47532416495899626
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 20.]), 'previousTarget': array([ 3., 20.]), 'currentState': array([ 3.97729679, 19.61449516,  3.08699931]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 1.0505822169920003}
episode index:1919
target Thresh 31.999999881387936
target distance 14.0
model initialize at round 1919
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 18.]), 'previousTarget': array([13., 18.]), 'currentState': array([27.04674967, 25.94870693,  5.21105367]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 16.139799193102174}
done in step count: 34
reward sum = 0.4238061999547878
running average episode reward sum: 0.4752973326855566
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 18.]), 'previousTarget': array([13., 18.]), 'currentState': array([13.96361171, 18.44637463,  3.31178786]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 1.061978267227931}
episode index:1920
target Thresh 31.999999882568147
target distance 7.0
model initialize at round 1920
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 20.]), 'previousTarget': array([10., 20.]), 'currentState': array([16.61872679, 13.11956336,  2.62219459]), 'targetState': array([10, 20], dtype=int32), 'currentDistance': 9.547143690960644}
done in step count: 21
reward sum = 0.6598650361068771
running average episode reward sum: 0.4753934116566244
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 20.]), 'previousTarget': array([10., 20.]), 'currentState': array([10.96306009, 19.70199917,  2.29482334]), 'targetState': array([10, 20], dtype=int32), 'currentDistance': 1.0081117163569324}
episode index:1921
target Thresh 31.999999883736614
target distance 17.0
model initialize at round 1921
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 12.]), 'previousTarget': array([17., 12.]), 'currentState': array([23.98577475, 28.9993966 ,  3.42662126]), 'targetState': array([17, 12], dtype=int32), 'currentDistance': 18.378806644971394}
done in step count: 44
reward sum = 0.37717547704108045
running average episode reward sum: 0.4753423097135362
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 12.]), 'previousTarget': array([17., 12.]), 'currentState': array([17.38831278, 12.76512005,  4.59609933]), 'targetState': array([17, 12], dtype=int32), 'currentDistance': 0.858018365250353}
episode index:1922
target Thresh 31.999999884893455
target distance 22.0
model initialize at round 1922
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.17650985, 24.219782  ]), 'previousTarget': array([21.17458624, 24.21853056]), 'currentState': array([11.00508549,  6.99938344,  0.13184994]), 'targetState': array([24, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.20822865104376265
running average episode reward sum: 0.4752034050548416
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 29.]), 'previousTarget': array([24., 29.]), 'currentState': array([23.82626999, 28.10101579,  0.88063792]), 'targetState': array([24, 29], dtype=int32), 'currentDistance': 0.9156171321346194}
episode index:1923
target Thresh 31.999999886038783
target distance 4.0
model initialize at round 1923
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 24.]), 'previousTarget': array([15., 24.]), 'currentState': array([17.        , 20.        ,  4.47790492]), 'targetState': array([15, 24], dtype=int32), 'currentDistance': 4.47213595499958}
done in step count: 12
reward sum = 0.7826832703842277
running average episode reward sum: 0.47536321787465935
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 24.]), 'previousTarget': array([15., 24.]), 'currentState': array([15.25772899, 23.02230953,  1.82547064]), 'targetState': array([15, 24], dtype=int32), 'currentDistance': 1.0110899542928258}
episode index:1924
target Thresh 31.999999887172716
target distance 13.0
model initialize at round 1924
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 21.]), 'previousTarget': array([25., 21.]), 'currentState': array([12.04712478,  8.95045387,  5.725245  ]), 'targetState': array([25, 21], dtype=int32), 'currentDistance': 17.690916835535372}
done in step count: 45
reward sum = 0.40141093369386105
running average episode reward sum: 0.4753248011036564
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 21.]), 'previousTarget': array([25., 21.]), 'currentState': array([24.47439993, 20.04341617,  0.7383107 ]), 'targetState': array([25, 21], dtype=int32), 'currentDistance': 1.0914705968349205}
episode index:1925
target Thresh 31.999999888295367
target distance 4.0
model initialize at round 1925
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 13.]), 'previousTarget': array([13., 13.]), 'currentState': array([17.02042689, 14.97489643,  5.20011705]), 'targetState': array([13, 13], dtype=int32), 'currentDistance': 4.479291049322351}
done in step count: 11
reward sum = 0.8095403735671725
running average episode reward sum: 0.4754983294382688
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 13.]), 'previousTarget': array([13., 13.]), 'currentState': array([13.9830983 , 13.13964299,  3.14758953]), 'targetState': array([13, 13], dtype=int32), 'currentDistance': 0.992966485000861}
episode index:1926
target Thresh 31.999999889406848
target distance 7.0
model initialize at round 1926
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'previousTarget': array([10., 12.]), 'currentState': array([ 5.        , 19.        ,  0.35590225]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 8.602325267042627}
done in step count: 18
reward sum = 0.6764605324627553
running average episode reward sum: 0.47560261703713985
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'previousTarget': array([10., 12.]), 'currentState': array([ 9.57708718, 12.81910072,  5.0855592 ]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 0.9218358013960557}
episode index:1927
target Thresh 31.999999890507265
target distance 20.0
model initialize at round 1927
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9.35109624, 26.55625457]), 'previousTarget': array([ 9.361625  , 26.52431817]), 'currentState': array([19.02813966,  9.05325966,  1.22973067]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 68
reward sum = 0.27771828968230217
running average episode reward sum: 0.47549997993788945
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 29.]), 'previousTarget': array([ 8., 29.]), 'currentState': array([ 8.35273687, 28.0749681 ,  1.84990218]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 0.9900036959473794}
episode index:1928
target Thresh 31.999999891596737
target distance 11.0
model initialize at round 1928
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 26.]), 'previousTarget': array([16., 26.]), 'currentState': array([ 7.99439497, 15.00954041,  1.87595153]), 'targetState': array([16, 26], dtype=int32), 'currentDistance': 13.597055343263127}
done in step count: 30
reward sum = 0.5378215212924605
running average episode reward sum: 0.47553228763169686
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 26.]), 'previousTarget': array([16., 26.]), 'currentState': array([15.11209793, 25.04222592,  0.79129332]), 'targetState': array([16, 26], dtype=int32), 'currentDistance': 1.3060249895865423}
episode index:1929
target Thresh 31.99999989267537
target distance 10.0
model initialize at round 1929
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 13.]), 'previousTarget': array([23., 13.]), 'currentState': array([13.        , 21.        ,  4.06820083]), 'targetState': array([23, 13], dtype=int32), 'currentDistance': 12.806248474865697}
done in step count: 30
reward sum = 0.5260191380686012
running average episode reward sum: 0.47555844662156055
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 13.]), 'previousTarget': array([23., 13.]), 'currentState': array([22.09242985, 13.43297025,  5.57643669]), 'targetState': array([23, 13], dtype=int32), 'currentDistance': 1.0055579615879995}
episode index:1930
target Thresh 31.999999893743265
target distance 19.0
model initialize at round 1930
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9.43827311, 8.92524322]), 'previousTarget': array([9.43827311, 8.92524322]), 'currentState': array([18.        , 27.        ,  1.94706714]), 'targetState': array([9, 8], dtype=int32), 'currentDistance': 20.0}
done in step count: 92
reward sum = 0.1961558044184785
running average episode reward sum: 0.4754137533837547
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 8.]), 'previousTarget': array([9., 8.]), 'currentState': array([9.21252691, 8.71534825, 4.34640985]), 'targetState': array([9, 8], dtype=int32), 'currentDistance': 0.7462511691202531}
episode index:1931
target Thresh 31.99999989480054
target distance 5.0
model initialize at round 1931
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([24.10906442, 17.90150587,  5.32014337]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 5.336001543021165}
done in step count: 10
reward sum = 0.8031388594093033
running average episode reward sum: 0.4755833833558176
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([22.62715195, 13.82842406,  4.05332036]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 1.0390408996976772}
episode index:1932
target Thresh 31.99999989584729
target distance 14.0
model initialize at round 1932
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 17.]), 'previousTarget': array([12., 17.]), 'currentState': array([12.98757616,  2.9963839 ,  3.17334235]), 'targetState': array([12, 17], dtype=int32), 'currentDistance': 14.038396295276046}
done in step count: 35
reward sum = 0.4816625603499384
running average episode reward sum: 0.47558652829994286
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 17.]), 'previousTarget': array([12., 17.]), 'currentState': array([12.22033756, 16.19000874,  1.70765774]), 'targetState': array([12, 17], dtype=int32), 'currentDistance': 0.8394250921354726}
episode index:1933
target Thresh 31.999999896883626
target distance 3.0
model initialize at round 1933
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([12.        , 20.        ,  4.85078508]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 9
reward sum = 0.8426494386800539
running average episode reward sum: 0.4757763229795603
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([ 9.60840151, 20.08396783,  2.69490276]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 1.099666919298204}
episode index:1934
target Thresh 31.999999897909653
target distance 9.0
model initialize at round 1934
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 11.]), 'previousTarget': array([20., 11.]), 'currentState': array([10.98980692,  6.00238552,  2.6685093 ]), 'targetState': array([20, 11], dtype=int32), 'currentDistance': 10.303384383699392}
done in step count: 25
reward sum = 0.5862868707576541
running average episode reward sum: 0.4758334343737609
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 11.]), 'previousTarget': array([20., 11.]), 'currentState': array([19.25258406, 10.80368846,  0.58142374]), 'targetState': array([20, 11], dtype=int32), 'currentDistance': 0.772766985954666}
episode index:1935
target Thresh 31.999999898925466
target distance 8.0
model initialize at round 1935
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 28.]), 'previousTarget': array([ 6., 28.]), 'currentState': array([13.50484196, 20.03463162,  2.86065906]), 'targetState': array([ 6, 28], dtype=int32), 'currentDistance': 10.943936502408762}
done in step count: 28
reward sum = 0.5927707319255203
running average episode reward sum: 0.47589383587043016
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 28.]), 'previousTarget': array([ 6., 28.]), 'currentState': array([ 6.96145802, 27.97850731,  2.09437183]), 'targetState': array([ 6, 28], dtype=int32), 'currentDistance': 0.9616982140270635}
episode index:1936
target Thresh 31.999999899931176
target distance 17.0
model initialize at round 1936
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  3.]), 'previousTarget': array([17.,  3.]), 'currentState': array([21.9633052 , 19.6546674 ,  4.42207307]), 'targetState': array([17,  3], dtype=int32), 'currentDistance': 17.378502376388422}
done in step count: 38
reward sum = 0.41456223044836
running average episode reward sum: 0.4758621726771302
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  3.]), 'previousTarget': array([17.,  3.]), 'currentState': array([17.03576547,  3.73669267,  4.58569496]), 'targetState': array([17,  3], dtype=int32), 'currentDistance': 0.7375603400662981}
episode index:1937
target Thresh 31.999999900926877
target distance 10.0
model initialize at round 1937
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([22.00398852, 10.95038525,  4.5401063 ]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 11.706521040600741}
done in step count: 28
reward sum = 0.5379661906566059
running average episode reward sum: 0.47589421809404425
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([16.67597735, 20.27966841,  2.03323998]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 0.9878375274806894}
episode index:1938
target Thresh 31.999999901912673
target distance 10.0
model initialize at round 1938
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  7.]), 'previousTarget': array([19.,  7.]), 'currentState': array([ 9.1188208 , 10.12043092,  0.56670612]), 'targetState': array([19,  7], dtype=int32), 'currentDistance': 10.36218082778698}
done in step count: 23
reward sum = 0.6318051234931411
running average episode reward sum: 0.475974625987494
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  7.]), 'previousTarget': array([19.,  7.]), 'currentState': array([18.20063674,  7.4953269 ,  6.13022141]), 'targetState': array([19,  7], dtype=int32), 'currentDistance': 0.9403884072899988}
episode index:1939
target Thresh 31.999999902888657
target distance 11.0
model initialize at round 1939
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 17.]), 'previousTarget': array([ 4., 17.]), 'currentState': array([13.84188666, 27.68637648,  4.16763936]), 'targetState': array([ 4, 17], dtype=int32), 'currentDistance': 14.527951519103622}
done in step count: 29
reward sum = 0.5153323727030363
running average episode reward sum: 0.475994913485801
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 17.]), 'previousTarget': array([ 4., 17.]), 'currentState': array([ 4.83009539, 17.76137649,  3.76882173]), 'targetState': array([ 4, 17], dtype=int32), 'currentDistance': 1.1263891479277828}
episode index:1940
target Thresh 31.99999990385493
target distance 23.0
model initialize at round 1940
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4.92486667, 22.58861079]), 'previousTarget': array([ 5.16799178, 22.58678368]), 'currentState': array([24.7299237 , 19.80298408,  3.52921611]), 'targetState': array([ 2, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.2619395385988752
running average episode reward sum: 0.47588463250955837
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 23.]), 'previousTarget': array([ 2., 23.]), 'currentState': array([ 2.86187768, 22.65611938,  2.81810449]), 'targetState': array([ 2, 23], dtype=int32), 'currentDistance': 0.9279477422232981}
episode index:1941
target Thresh 31.99999990481159
target distance 19.0
model initialize at round 1941
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([22.02600883,  3.04077778,  1.24973902]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 19.05017373074715}
done in step count: 58
reward sum = 0.3350698086767664
running average episode reward sum: 0.4758121223016115
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.7330779 , 3.23060976, 3.15330237]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.0627156500781547}
episode index:1942
target Thresh 31.99999990575873
target distance 5.0
model initialize at round 1942
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([14.57855918,  9.74292671,  3.54886761]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 4.747992991209051}
done in step count: 8
reward sum = 0.8320661089436768
running average episode reward sum: 0.4759954748423434
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.99822647, 10.59776719,  2.50063284]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.0762189961714972}
episode index:1943
target Thresh 31.99999990669645
target distance 19.0
model initialize at round 1943
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.75313304, 28.90013591]), 'previousTarget': array([25.4327075, 28.76114  ]), 'currentState': array([ 7.21267568, 21.40003985,  0.84403527]), 'targetState': array([26, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.3417384609446117
running average episode reward sum: 0.475926412592396
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 29.]), 'previousTarget': array([26., 29.]), 'currentState': array([25.11424085, 28.23396445,  0.61690017]), 'targetState': array([26, 29], dtype=int32), 'currentDistance': 1.1710592377969855}
episode index:1944
target Thresh 31.999999907624833
target distance 17.0
model initialize at round 1944
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  7.]), 'previousTarget': array([18.,  7.]), 'currentState': array([18.01791223, 24.00811814,  0.20854732]), 'targetState': array([18,  7], dtype=int32), 'currentDistance': 17.00812756869341}
done in step count: 47
reward sum = 0.4039445401141014
running average episode reward sum: 0.4758894039176
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  7.]), 'previousTarget': array([18.,  7.]), 'currentState': array([18.03003275,  7.90233958,  4.60625997]), 'targetState': array([18,  7], dtype=int32), 'currentDistance': 0.9028392363036946}
episode index:1945
target Thresh 31.99999990854398
target distance 10.0
model initialize at round 1945
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([22.01109103, 10.99838718,  0.10809582]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 12.814438670538316}
done in step count: 32
reward sum = 0.5096133382236145
running average episode reward sum: 0.47590673379134407
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([14.23123156, 20.13463024,  2.37897127]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 0.8957303430502417}
episode index:1946
target Thresh 31.999999909453983
target distance 2.0
model initialize at round 1946
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 14.]), 'previousTarget': array([ 3., 14.]), 'currentState': array([ 3.04380275, 16.06866028,  0.78688377]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 2.0691239747448056}
done in step count: 8
reward sum = 0.866160574626143
running average episode reward sum: 0.4761071723331185
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 14.]), 'previousTarget': array([ 3., 14.]), 'currentState': array([ 3.89143098, 14.81455069,  4.69293078]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 1.2075355142646447}
episode index:1947
target Thresh 31.99999991035493
target distance 14.0
model initialize at round 1947
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  5.]), 'previousTarget': array([11.,  5.]), 'currentState': array([24.95595786, 19.06496052,  2.38321403]), 'targetState': array([11,  5], dtype=int32), 'currentDistance': 19.813931318983965}
done in step count: 99
reward sum = -0.057211273295300034
running average episode reward sum: 0.4758333948969643
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  5.]), 'previousTarget': array([11.,  5.]), 'currentState': array([18.94191266, 13.32610467,  3.96256779]), 'targetState': array([11,  5], dtype=int32), 'currentDistance': 11.50643279575014}
episode index:1948
target Thresh 31.999999911246913
target distance 16.0
model initialize at round 1948
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([20.        , 27.        ,  5.29696417]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 16.76305461424021}
done in step count: 44
reward sum = 0.40404124736792757
running average episode reward sum: 0.47579655952111566
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([ 4.69890023, 22.12059322,  3.34821655]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 0.7092279300190307}
episode index:1949
target Thresh 31.999999912130022
target distance 19.0
model initialize at round 1949
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 15.]), 'previousTarget': array([25., 15.]), 'currentState': array([ 6.        , 15.        ,  2.68484187]), 'targetState': array([25, 15], dtype=int32), 'currentDistance': 19.0}
done in step count: 61
reward sum = 0.3112385894655725
running average episode reward sum: 0.4757121708185231
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 15.]), 'previousTarget': array([25., 15.]), 'currentState': array([24.27202096, 15.09551346,  0.07957619]), 'targetState': array([25, 15], dtype=int32), 'currentDistance': 0.7342181541480466}
episode index:1950
target Thresh 31.999999913004345
target distance 20.0
model initialize at round 1950
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5.70182087, 8.84585351]), 'previousTarget': array([5.71008489, 8.85014149]), 'currentState': array([15.96648429, 26.01083778,  2.93932731]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.07453490041169743
running average episode reward sum: 0.4754301374657654
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([11.37433879, 14.2646254 ,  4.03768526]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 11.07632184878562}
episode index:1951
target Thresh 31.999999913869964
target distance 10.0
model initialize at round 1951
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([14.75627255, 20.75209289,  3.9928294 ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 12.460460650897664}
done in step count: 34
reward sum = 0.5656810180900629
running average episode reward sum: 0.4754763725480525
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.16408316, 11.9680432 ,  4.08427392]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.9818507660064641}
episode index:1952
target Thresh 31.999999914726974
target distance 21.0
model initialize at round 1952
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6.97203801, 8.80525593]), 'previousTarget': array([6.98417253, 8.81486795]), 'currentState': array([19.98198829, 23.9954236 ,  3.39006743]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.11518677895915184
running average episode reward sum: 0.47517393365839183
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([11.43736336,  8.21388368,  3.66868491]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 10.781855604630833}
episode index:1953
target Thresh 31.999999915575454
target distance 17.0
model initialize at round 1953
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.74524418, 18.01305876]), 'previousTarget': array([10.85099785, 17.88715666]), 'currentState': array([22.7973267 ,  2.05225343,  2.86726098]), 'targetState': array([10, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.3120987055929042
running average episode reward sum: 0.47509047653041564
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 19.]), 'previousTarget': array([10., 19.]), 'currentState': array([10.67847214, 18.06341167,  2.06489045]), 'targetState': array([10, 19], dtype=int32), 'currentDistance': 1.1565129226015027}
episode index:1954
target Thresh 31.999999916415494
target distance 2.0
model initialize at round 1954
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 5.]), 'previousTarget': array([8., 5.]), 'currentState': array([6.90057323, 3.36099866, 1.77021696]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 1.9735918036092694}
done in step count: 6
reward sum = 0.9193338542835099
running average episode reward sum: 0.4753177109947395
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 5.]), 'previousTarget': array([8., 5.]), 'currentState': array([7.0080679 , 4.69535987, 1.16160771]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 1.0376583764131169}
episode index:1955
target Thresh 31.999999917247173
target distance 5.0
model initialize at round 1955
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([20.9622342 , 20.07157854,  2.27098492]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 5.04833979503633}
done in step count: 13
reward sum = 0.8057041602303431
running average episode reward sum: 0.47548662022236504
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([16.83489386, 21.51629551,  3.09990428]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 0.9816357879550817}
episode index:1956
target Thresh 31.999999918070575
target distance 16.0
model initialize at round 1956
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  7.]), 'previousTarget': array([19.,  7.]), 'currentState': array([10.98645367, 22.97052946,  4.48566625]), 'targetState': array([19,  7], dtype=int32), 'currentDistance': 17.868260577088265}
done in step count: 50
reward sum = 0.39694111669818954
running average episode reward sum: 0.47544648455372723
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  7.]), 'previousTarget': array([19.,  7.]), 'currentState': array([18.65559868,  7.9227931 ,  5.12001262]), 'targetState': array([19,  7], dtype=int32), 'currentDistance': 0.9849666863127345}
episode index:1957
target Thresh 31.999999918885788
target distance 4.0
model initialize at round 1957
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([15.        , 22.        ,  1.26215702]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 4.123105625617661}
done in step count: 15
reward sum = 0.7780055208648344
running average episode reward sum: 0.47560100908708325
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([16.90711005, 18.99911025,  5.08349255]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 1.349470245076504}
episode index:1958
target Thresh 31.999999919692886
target distance 14.0
model initialize at round 1958
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 17.]), 'previousTarget': array([ 4., 17.]), 'currentState': array([18.00503443, 18.00439685,  0.97039747]), 'targetState': array([ 4, 17], dtype=int32), 'currentDistance': 14.041004324939427}
done in step count: 36
reward sum = 0.4672052615086715
running average episode reward sum: 0.4755967233558028
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 17.]), 'previousTarget': array([ 4., 17.]), 'currentState': array([ 4.84135457, 17.49330726,  3.2011534 ]), 'targetState': array([ 4, 17], dtype=int32), 'currentDistance': 0.9753099831484486}
episode index:1959
target Thresh 31.999999920491955
target distance 16.0
model initialize at round 1959
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([13.        , 27.        ,  2.08292539]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 18.35755975068582}
done in step count: 62
reward sum = 0.33129651303930824
running average episode reward sum: 0.47552310079951887
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([21.56315568, 11.76050962,  5.0997855 ]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 0.8770449500562474}
episode index:1960
target Thresh 31.999999921283074
target distance 10.0
model initialize at round 1960
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 15.]), 'previousTarget': array([12., 15.]), 'currentState': array([ 1.96951976, 13.94563634,  4.45388842]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 10.085743236312334}
done in step count: 23
reward sum = 0.6015582791847193
running average episode reward sum: 0.4755873716706995
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 15.]), 'previousTarget': array([12., 15.]), 'currentState': array([11.11909248, 14.64938042,  0.15673297]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 0.9481203235873271}
episode index:1961
target Thresh 31.999999922066323
target distance 8.0
model initialize at round 1961
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 10.]), 'previousTarget': array([20., 10.]), 'currentState': array([1.31131870e+01, 1.80185081e+01, 4.76270914e-04]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 10.569988952048405}
done in step count: 25
reward sum = 0.6289297815969044
running average episode reward sum: 0.47566552784293503
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 10.]), 'previousTarget': array([20., 10.]), 'currentState': array([19.36924001, 10.79901064,  5.21424663]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 1.0179765060404173}
episode index:1962
target Thresh 31.999999922841774
target distance 9.0
model initialize at round 1962
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([19.89184209,  3.20274301,  1.9514574 ]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 8.866777572241027}
done in step count: 19
reward sum = 0.6876839519268975
running average episode reward sum: 0.47577353519091464
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([21.25078844, 11.1843008 ,  1.5367902 ]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 0.8533815201234827}
episode index:1963
target Thresh 31.99999992360951
target distance 15.0
model initialize at round 1963
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.64677391, 14.37885309]), 'previousTarget': array([24.64636501, 14.37889464]), 'currentState': array([11.0080486 , 29.0070851 ,  0.71255982]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.06752945150872168
running average episode reward sum: 0.4754969043422896
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 14.]), 'previousTarget': array([25., 14.]), 'currentState': array([19.86098721, 22.4113846 ,  5.29888663]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 9.857020005926879}
episode index:1964
target Thresh 31.99999992436961
target distance 26.0
model initialize at round 1964
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.11346765,  9.88223901]), 'previousTarget': array([21.11828302,  9.88441983]), 'currentState': array([26.98466647, 29.00105238,  3.26751593]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 92
reward sum = 0.1696555310181169
running average episode reward sum: 0.47534125987749354
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'previousTarget': array([19.,  3.]), 'currentState': array([19.96819293,  3.97557456,  4.37875727]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 1.3744610796290628}
episode index:1965
target Thresh 31.999999925122143
target distance 15.0
model initialize at round 1965
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([19.98263767,  6.15908572,  1.60599238]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 16.014411819397694}
done in step count: 41
reward sum = 0.4667239173174377
running average episode reward sum: 0.47533687669206115
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([25.9160287 , 20.03762975,  1.32921224]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 0.9660267504746748}
episode index:1966
target Thresh 31.999999925867193
target distance 9.0
model initialize at round 1966
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([13.        , 26.        ,  0.98747599]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 11.40175425099138}
done in step count: 28
reward sum = 0.5754589424223352
running average episode reward sum: 0.47538777758973794
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([21.66900557, 19.86067911,  5.61131229]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 0.9221311394804896}
episode index:1967
target Thresh 31.999999926604826
target distance 8.0
model initialize at round 1967
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 7.73687567, 12.59063487,  4.22156096]), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.068966862002984}
done in step count: 17
reward sum = 0.7155041717055807
running average episode reward sum: 0.4755097879526017
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.42293259, 5.75854453, 4.53248097]), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.8684824613580032}
episode index:1968
target Thresh 31.99999992733512
target distance 15.0
model initialize at round 1968
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([26.9811687 , 27.88725988,  4.43504024]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 16.096435771475424}
done in step count: 53
reward sum = 0.42722743529233786
running average episode reward sum: 0.47548526669680674
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([12.98213141, 22.09935726,  3.49983985]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 0.9871443539709988}
episode index:1969
target Thresh 31.99999992805815
target distance 18.0
model initialize at round 1969
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 20.]), 'previousTarget': array([22., 20.]), 'currentState': array([ 4.37700964, 24.96257948,  6.27644709]), 'targetState': array([22, 20], dtype=int32), 'currentDistance': 18.308385632742905}
done in step count: 54
reward sum = 0.38257353753978307
running average episode reward sum: 0.4754381033825138
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 20.]), 'previousTarget': array([22., 20.]), 'currentState': array([21.12257316, 20.12734357,  5.89992099]), 'targetState': array([22, 20], dtype=int32), 'currentDistance': 0.8866195630606778}
episode index:1970
target Thresh 31.99999992877398
target distance 4.0
model initialize at round 1970
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([23.80497101, 17.5969334 ,  4.27864354]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 4.024406817387833}
done in step count: 8
reward sum = 0.8667514406936292
running average episode reward sum: 0.47563663881493956
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([22.50218733, 14.76075685,  4.43372563]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 0.9115608035844991}
episode index:1971
target Thresh 31.99999992948269
target distance 19.0
model initialize at round 1971
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8.5672925, 17.76114  ]), 'previousTarget': array([ 8.5672925, 17.76114  ]), 'currentState': array([27.        , 10.        ,  4.93450126]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.2973040487748829
running average episode reward sum: 0.47554620646704904
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 18.]), 'previousTarget': array([ 8., 18.]), 'currentState': array([ 8.8786071 , 17.84041627,  2.67272805]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 0.8929823050581985}
episode index:1972
target Thresh 31.99999993018435
target distance 14.0
model initialize at round 1972
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 25.]), 'previousTarget': array([11., 25.]), 'currentState': array([25.       , 20.       ,  5.5764125]), 'targetState': array([11, 25], dtype=int32), 'currentDistance': 14.866068747318504}
done in step count: 48
reward sum = 0.4097214000947035
running average episode reward sum: 0.4755128436660494
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 25.]), 'previousTarget': array([11., 25.]), 'currentState': array([11.96548835, 25.19701834,  3.00362061]), 'targetState': array([11, 25], dtype=int32), 'currentDistance': 0.9853851911816773}
episode index:1973
target Thresh 31.999999930879028
target distance 6.0
model initialize at round 1973
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 26.]), 'previousTarget': array([ 4., 26.]), 'currentState': array([ 9.9879893 , 25.0810615 ,  1.95736852]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 6.058090772511386}
done in step count: 14
reward sum = 0.7639557347272079
running average episode reward sum: 0.47565896468482405
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 26.]), 'previousTarget': array([ 4., 26.]), 'currentState': array([ 4.75534555, 25.94770641,  3.13593041]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 0.75715356396882}
episode index:1974
target Thresh 31.999999931566794
target distance 17.0
model initialize at round 1974
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  5.]), 'previousTarget': array([18.,  5.]), 'currentState': array([26.81536812, 21.88898413,  3.76631247]), 'targetState': array([18,  5], dtype=int32), 'currentDistance': 19.051207316418797}
done in step count: 50
reward sum = 0.38753007572389286
running average episode reward sum: 0.47561434246256534
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  5.]), 'previousTarget': array([18.,  5.]), 'currentState': array([18.21140398,  5.8432511 ,  4.38116475]), 'targetState': array([18,  5], dtype=int32), 'currentDistance': 0.8693469154909791}
episode index:1975
target Thresh 31.999999932247714
target distance 19.0
model initialize at round 1975
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.71809795,  4.04878783]), 'previousTarget': array([20.67985983,  4.09022194]), 'currentState': array([10.10973169, 21.00350886,  6.23014047]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 68
reward sum = 0.283822304037206
running average episode reward sum: 0.4755172817143744
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  2.]), 'previousTarget': array([22.,  2.]), 'currentState': array([21.51635389,  2.82780579,  5.03333347]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 0.9587366584912399}
episode index:1976
target Thresh 31.99999993292186
target distance 15.0
model initialize at round 1976
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 22.]), 'previousTarget': array([11., 22.]), 'currentState': array([12.05238955,  7.09981891,  1.19599909]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 14.937299626616138}
done in step count: 35
reward sum = 0.502332179879375
running average episode reward sum: 0.4755308451428848
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 22.]), 'previousTarget': array([11., 22.]), 'currentState': array([11.66766468, 21.09047929,  1.78813819]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 1.1282748122142239}
episode index:1977
target Thresh 31.9999999335893
target distance 22.0
model initialize at round 1977
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6.29854999, 6.20425485]), 'previousTarget': array([6.29773591, 6.18339664]), 'currentState': array([ 8.982892  , 26.02329388,  2.45676577]), 'targetState': array([6, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.2507219206035181
running average episode reward sum: 0.47541719047931585
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 4.]), 'previousTarget': array([6., 4.]), 'currentState': array([5.72901674, 4.69961105, 4.79313089]), 'targetState': array([6, 4], dtype=int32), 'currentDistance': 0.7502583166545044}
episode index:1978
target Thresh 31.9999999342501
target distance 17.0
model initialize at round 1978
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 5.]), 'previousTarget': array([6., 5.]), 'currentState': array([15.99053607, 22.01644931,  2.33283132]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 19.732469639093207}
done in step count: 50
reward sum = 0.32670997345450853
running average episode reward sum: 0.47534204787344175
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 5.]), 'previousTarget': array([6., 5.]), 'currentState': array([6.17136179, 5.87249725, 4.01069847]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 0.8891660824269958}
episode index:1979
target Thresh 31.99999993490432
target distance 23.0
model initialize at round 1979
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5.94771459, 4.52496392]), 'previousTarget': array([6.2957649, 4.5731765]), 'currentState': array([25.63789814,  8.03162499,  3.0726531 ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.29237386611520133
running average episode reward sum: 0.4752496397008366
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.95443155, 4.32928538, 3.1221392 ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.009637773611492}
episode index:1980
target Thresh 31.999999935552033
target distance 23.0
model initialize at round 1980
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.10953392, 11.27785917]), 'previousTarget': array([13.07464439, 11.24778806]), 'currentState': array([24.06344467, 28.01141347,  6.20867795]), 'targetState': array([9, 5], dtype=int32), 'currentDistance': 20.0}
done in step count: 76
reward sum = 0.15743888706045156
running average episode reward sum: 0.475089210244683
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 5.]), 'previousTarget': array([9., 5.]), 'currentState': array([9.7119993 , 5.66973149, 3.86668269]), 'targetState': array([9, 5], dtype=int32), 'currentDistance': 0.9774882408531569}
episode index:1981
target Thresh 31.9999999361933
target distance 2.0
model initialize at round 1981
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 15.]), 'previousTarget': array([ 9., 15.]), 'currentState': array([ 6.97502878, 13.02482274,  2.19551602]), 'targetState': array([ 9, 15], dtype=int32), 'currentDistance': 2.8287512560709898}
done in step count: 11
reward sum = 0.8419790833279649
running average episode reward sum: 0.4752743211796392
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 15.]), 'previousTarget': array([ 9., 15.]), 'currentState': array([ 8.11407997, 15.68926092,  0.50173666]), 'targetState': array([ 9, 15], dtype=int32), 'currentDistance': 1.1224682181821155}
episode index:1982
target Thresh 31.99999993682819
target distance 21.0
model initialize at round 1982
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7.19796494, 8.17303412]), 'previousTarget': array([7.20101013, 8.17157288]), 'currentState': array([26.99254437, 11.03216422,  2.0362688 ]), 'targetState': array([6, 8], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.29276448786775416
running average episode reward sum: 0.4751822839465016
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 8.]), 'previousTarget': array([6., 8.]), 'currentState': array([6.87712923, 8.16780495, 3.40216098]), 'targetState': array([6, 8], dtype=int32), 'currentDistance': 0.8930365035688911}
episode index:1983
target Thresh 31.999999937456757
target distance 7.0
model initialize at round 1983
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([13.        , 16.        ,  4.27521583]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 7.280109889280519}
done in step count: 19
reward sum = 0.7064565739515669
running average episode reward sum: 0.4752988536491251
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([ 6.75418415, 17.76087359,  2.53666158]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 0.7911859299989684}
episode index:1984
target Thresh 31.999999938079075
target distance 10.0
model initialize at round 1984
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 17.]), 'previousTarget': array([20., 17.]), 'currentState': array([10.30094653, 21.99798448,  6.23164136]), 'targetState': array([20, 17], dtype=int32), 'currentDistance': 10.911071768216699}
done in step count: 24
reward sum = 0.6226584716107428
running average episode reward sum: 0.4753730902324811
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 17.]), 'previousTarget': array([20., 17.]), 'currentState': array([19.0096155 , 17.16240376,  5.52615463]), 'targetState': array([20, 17], dtype=int32), 'currentDistance': 1.0036116975235434}
episode index:1985
target Thresh 31.999999938695197
target distance 8.0
model initialize at round 1985
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 22.]), 'previousTarget': array([15., 22.]), 'currentState': array([23.       , 28.       ,  6.0514407]), 'targetState': array([15, 22], dtype=int32), 'currentDistance': 10.0}
done in step count: 24
reward sum = 0.604855334971308
running average episode reward sum: 0.4754382877373848
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 22.]), 'previousTarget': array([15., 22.]), 'currentState': array([15.94731663, 22.62062497,  3.74236986]), 'targetState': array([15, 22], dtype=int32), 'currentDistance': 1.1325123199417857}
episode index:1986
target Thresh 31.99999993930519
target distance 18.0
model initialize at round 1986
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3.936621  , 4.57220302]), 'previousTarget': array([3.93436333, 4.57099981]), 'currentState': array([21.00368831, 14.99886181,  5.77032894]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 70
reward sum = 0.28768950926374015
running average episode reward sum: 0.47534379917247604
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.80482913, 4.97409368, 3.38872155]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.2635697164793562}
episode index:1987
target Thresh 31.999999939909113
target distance 11.0
model initialize at round 1987
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([17.89904637, 19.80924486,  4.17000781]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 14.013993247793275}
done in step count: 33
reward sum = 0.5415069941616418
running average episode reward sum: 0.4753770804576819
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.93548174, 11.89478755,  3.81496089]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.294515682455394}
episode index:1988
target Thresh 31.999999940507028
target distance 12.0
model initialize at round 1988
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([ 9.01010648, 14.98757786,  5.24320704]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 13.886814462776613}
done in step count: 38
reward sum = 0.5108435861758499
running average episode reward sum: 0.4753949117828293
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.07327497, 3.92808525, 3.88084427]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.9309733949414368}
episode index:1989
target Thresh 31.99999994109899
target distance 12.0
model initialize at round 1989
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 12.]), 'previousTarget': array([16., 12.]), 'currentState': array([ 8.97800123, 23.98463616,  3.97813946]), 'targetState': array([16, 12], dtype=int32), 'currentDistance': 13.890283317715786}
done in step count: 33
reward sum = 0.5149600999016968
running average episode reward sum: 0.4754147937869091
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 12.]), 'previousTarget': array([16., 12.]), 'currentState': array([15.17106367, 12.9878989 ,  5.27747844]), 'targetState': array([16, 12], dtype=int32), 'currentDistance': 1.289604461843022}
episode index:1990
target Thresh 31.99999994168507
target distance 7.0
model initialize at round 1990
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([20.68956476, 12.74807955,  3.88846521]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 8.819903358883426}
done in step count: 19
reward sum = 0.694755230894108
running average episode reward sum: 0.47552495975230696
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.3658342 ,  7.90516328,  3.6759329 ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.9762966852951306}
episode index:1991
target Thresh 31.999999942265312
target distance 7.0
model initialize at round 1991
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  9.]), 'previousTarget': array([20.,  9.]), 'currentState': array([16.98185309, 15.52826836,  4.72326975]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 7.1921831553818025}
done in step count: 13
reward sum = 0.7525584530732347
running average episode reward sum: 0.4756640327911227
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  9.]), 'previousTarget': array([20.,  9.]), 'currentState': array([19.33850562,  9.81715765,  5.12222125]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 1.051342682755466}
episode index:1992
target Thresh 31.999999942839782
target distance 15.0
model initialize at round 1992
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([26.79213472, 24.88833682,  3.77174845]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 15.147890597718625}
done in step count: 36
reward sum = 0.4788565731192327
running average episode reward sum: 0.47566563466785533
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([23.53890039, 10.86625931,  4.37268809]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 0.9813348332097453}
episode index:1993
target Thresh 31.999999943408536
target distance 18.0
model initialize at round 1993
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 14.]), 'previousTarget': array([26., 14.]), 'currentState': array([ 8.        , 13.        ,  1.47232592]), 'targetState': array([26, 14], dtype=int32), 'currentDistance': 18.027756377319946}
done in step count: 58
reward sum = 0.36312648709541456
running average episode reward sum: 0.47560919577739774
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 14.]), 'previousTarget': array([26., 14.]), 'currentState': array([25.2357745 , 13.88707355,  5.85294423]), 'targetState': array([26, 14], dtype=int32), 'currentDistance': 0.7725237805263305}
episode index:1994
target Thresh 31.99999994397163
target distance 25.0
model initialize at round 1994
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5.53822007, 21.66656512]), 'previousTarget': array([ 5.60740149, 21.25928039]), 'currentState': array([11.08053246,  2.44983352,  1.41171499]), 'targetState': array([ 4, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 72
reward sum = 0.20292624840536022
running average episode reward sum: 0.4754725125957576
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 27.]), 'previousTarget': array([ 4., 27.]), 'currentState': array([ 4.84307383, 26.25135565,  2.09931021]), 'targetState': array([ 4, 27], dtype=int32), 'currentDistance': 1.1274936123451142}
episode index:1995
target Thresh 31.99999994452912
target distance 8.0
model initialize at round 1995
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 15.]), 'previousTarget': array([ 5., 15.]), 'currentState': array([10.8197828 , 22.57613025,  4.27357046]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 9.553408888395492}
done in step count: 18
reward sum = 0.6831667893319722
running average episode reward sum: 0.4755765678446235
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 15.]), 'previousTarget': array([ 5., 15.]), 'currentState': array([ 5.88953973, 15.88603755,  3.97800309]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 1.2555251841294905}
episode index:1996
target Thresh 31.999999945081065
target distance 10.0
model initialize at round 1996
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 10.]), 'previousTarget': array([27., 10.]), 'currentState': array([23.03009715, 20.06216914,  0.87632447]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 10.816994795131757}
done in step count: 25
reward sum = 0.581458967232366
running average episode reward sum: 0.47562958857541354
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 10.]), 'previousTarget': array([27., 10.]), 'currentState': array([26.38635844, 10.88331208,  5.02053653]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 1.0755446024759472}
episode index:1997
target Thresh 31.99999994562752
target distance 18.0
model initialize at round 1997
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6.72118773, 17.78704435]), 'previousTarget': array([ 6.72118773, 17.78704435]), 'currentState': array([19.        ,  2.        ,  4.63061953]), 'targetState': array([ 5, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 72
reward sum = 0.22849405757657493
running average episode reward sum: 0.47550589711845714
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 20.]), 'previousTarget': array([ 5., 20.]), 'currentState': array([ 5.39570167, 19.0476177 ,  2.00412776]), 'targetState': array([ 5, 20], dtype=int32), 'currentDistance': 1.0313155955449633}
episode index:1998
target Thresh 31.999999946168533
target distance 17.0
model initialize at round 1998
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8.65855334, 25.53585188]), 'previousTarget': array([ 8.66064273, 25.53366395]), 'currentState': array([25.00624581, 14.01400521,  1.27800313]), 'targetState': array([ 8, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.29648049613640454
running average episode reward sum: 0.47541633963922647
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 26.]), 'previousTarget': array([ 8., 26.]), 'currentState': array([ 8.97247119, 25.87459877,  2.89723524]), 'targetState': array([ 8, 26], dtype=int32), 'currentDistance': 0.9805231672616911}
episode index:1999
target Thresh 31.999999946704165
target distance 4.0
model initialize at round 1999
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([18.45661261, 10.20272904,  0.4883523 ]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 5.193732850676503}
done in step count: 9
reward sum = 0.828628507799203
running average episode reward sum: 0.47559294572330646
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([21.10709143, 13.24740716,  0.94841363]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 1.16776782161348}
episode index:2000
target Thresh 31.999999947234468
target distance 11.0
model initialize at round 2000
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 19.]), 'previousTarget': array([ 4., 19.]), 'currentState': array([14.85767436, 28.0059346 ,  3.29996321]), 'targetState': array([ 4, 19], dtype=int32), 'currentDistance': 14.106592449316807}
done in step count: 34
reward sum = 0.5174154771550556
running average episode reward sum: 0.47561384653861466
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 19.]), 'previousTarget': array([ 4., 19.]), 'currentState': array([ 4.88569904, 19.58657935,  3.61060731]), 'targetState': array([ 4, 19], dtype=int32), 'currentDistance': 1.062326744455819}
episode index:2001
target Thresh 31.999999947759495
target distance 10.0
model initialize at round 2001
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 14.]), 'previousTarget': array([25., 14.]), 'currentState': array([26.07163007, 23.59860982,  4.84920029]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 9.658245256455887}
done in step count: 24
reward sum = 0.6511999143912428
running average episode reward sum: 0.47570155186721236
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 14.]), 'previousTarget': array([25., 14.]), 'currentState': array([24.41343715, 14.95291071,  4.31866133]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 1.118970423105698}
episode index:2002
target Thresh 31.999999948279296
target distance 15.0
model initialize at round 2002
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 15.]), 'previousTarget': array([10., 15.]), 'currentState': array([25.        ,  2.        ,  6.09736004]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 19.849433241279208}
done in step count: 71
reward sum = 0.27273462970423806
running average episode reward sum: 0.4756002204033267
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 15.]), 'previousTarget': array([10., 15.]), 'currentState': array([10.90906316, 14.91034284,  2.25697984]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 0.9134737211364641}
episode index:2003
target Thresh 31.999999948793924
target distance 22.0
model initialize at round 2003
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.82988368,  8.20621355]), 'previousTarget': array([16.83486126,  8.20413153]), 'currentState': array([ 9.97318402, 26.99412642,  3.57474762]), 'targetState': array([18,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 98
reward sum = 0.19718726253497026
running average episode reward sum: 0.47546129178163593
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  5.]), 'previousTarget': array([18.,  5.]), 'currentState': array([17.58773036,  5.88642129,  4.82480782]), 'targetState': array([18,  5], dtype=int32), 'currentDistance': 0.977603684695796}
episode index:2004
target Thresh 31.999999949303433
target distance 19.0
model initialize at round 2004
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9.30163555, 11.31492866]), 'previousTarget': array([ 9.30163555, 11.31492866]), 'currentState': array([27.        ,  2.        ,  0.85892534]), 'targetState': array([ 8, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 89
reward sum = 0.22054588671065548
running average episode reward sum: 0.4753341519287327
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 12.]), 'previousTarget': array([ 8., 12.]), 'currentState': array([ 8.99163705, 11.76389525,  2.54909163]), 'targetState': array([ 8, 12], dtype=int32), 'currentDistance': 1.019357392023351}
episode index:2005
target Thresh 31.999999949807872
target distance 12.0
model initialize at round 2005
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 25.]), 'previousTarget': array([25., 25.]), 'currentState': array([23.00429056, 12.9771794 ,  5.128618  ]), 'targetState': array([25, 25], dtype=int32), 'currentDistance': 12.187332416643127}
done in step count: 30
reward sum = 0.5161435054206945
running average episode reward sum: 0.47535449557454124
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 25.]), 'previousTarget': array([25., 25.]), 'currentState': array([24.66203316, 24.20163306,  1.53375116]), 'targetState': array([25, 25], dtype=int32), 'currentDistance': 0.866955222342651}
episode index:2006
target Thresh 31.99999995030729
target distance 3.0
model initialize at round 2006
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'previousTarget': array([10., 17.]), 'currentState': array([10.24956382, 19.56705757,  5.17687194]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 2.5791600702477284}
done in step count: 4
reward sum = 0.9177891421698162
running average episode reward sum: 0.4755749413376679
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'previousTarget': array([10., 17.]), 'currentState': array([10.86707808, 17.88536311,  4.96050548]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 1.2392304968573744}
episode index:2007
target Thresh 31.999999950801744
target distance 16.0
model initialize at round 2007
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 21.]), 'previousTarget': array([19., 21.]), 'currentState': array([ 3.11062803, 25.10495229,  0.60352707]), 'targetState': array([19, 21], dtype=int32), 'currentDistance': 16.411056482813038}
done in step count: 37
reward sum = 0.44912077925279925
running average episode reward sum: 0.4755617669541595
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 21.]), 'previousTarget': array([19., 21.]), 'currentState': array([18.28989713, 21.56726883,  5.83043105]), 'targetState': array([19, 21], dtype=int32), 'currentDistance': 0.908867433525432}
episode index:2008
target Thresh 31.999999951291276
target distance 4.0
model initialize at round 2008
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 25.]), 'previousTarget': array([14., 25.]), 'currentState': array([10.08554914, 25.82156586,  5.31723565]), 'targetState': array([14, 25], dtype=int32), 'currentDistance': 3.999736995234183}
done in step count: 8
reward sum = 0.8587113999254041
running average episode reward sum: 0.47575248354598193
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 25.]), 'previousTarget': array([14., 25.]), 'currentState': array([13.06313015, 25.28485286,  0.23909986]), 'targetState': array([14, 25], dtype=int32), 'currentDistance': 0.9792171728183048}
episode index:2009
target Thresh 31.999999951775933
target distance 1.0
model initialize at round 2009
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 13.]), 'previousTarget': array([23., 13.]), 'currentState': array([24.        , 14.        ,  6.28073599]), 'targetState': array([23, 13], dtype=int32), 'currentDistance': 1.4142135623730951}
done in step count: 23
reward sum = 0.7661052582397545
running average episode reward sum: 0.475896937662745
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 13.]), 'previousTarget': array([23., 13.]), 'currentState': array([23.99750988, 12.66035236,  3.8155355 ]), 'targetState': array([23, 13], dtype=int32), 'currentDistance': 1.0537487739902904}
episode index:2010
target Thresh 31.99999995225577
target distance 10.0
model initialize at round 2010
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 3.]), 'previousTarget': array([5., 3.]), 'currentState': array([ 8.98968901, 13.07023394,  1.94872227]), 'targetState': array([5, 3], dtype=int32), 'currentDistance': 10.83176947780716}
done in step count: 26
reward sum = 0.5583926170828566
running average episode reward sum: 0.4759379598802587
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 3.]), 'previousTarget': array([5., 3.]), 'currentState': array([5.28701398, 3.77452775, 4.47972508]), 'targetState': array([5, 3], dtype=int32), 'currentDistance': 0.8259965227894186}
episode index:2011
target Thresh 31.999999952730835
target distance 16.0
model initialize at round 2011
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.02896858, 10.03856534]), 'previousTarget': array([14., 10.]), 'currentState': array([26.04079441, 26.02968914,  0.4150503 ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.2930331520967503
running average episode reward sum: 0.4758470529181397
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.79638745, 10.85588636,  4.01774303]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.8797724310292123}
episode index:2012
target Thresh 31.999999953201172
target distance 10.0
model initialize at round 2012
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 17.]), 'previousTarget': array([17., 17.]), 'currentState': array([24.58235579, 26.72564134,  3.78349863]), 'targetState': array([17, 17], dtype=int32), 'currentDistance': 12.33208087507989}
done in step count: 29
reward sum = 0.5766882384873351
running average episode reward sum: 0.4758971478935839
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 17.]), 'previousTarget': array([17., 17.]), 'currentState': array([17.59562266, 17.93924481,  3.9218706 ]), 'targetState': array([17, 17], dtype=int32), 'currentDistance': 1.1121812637452722}
episode index:2013
target Thresh 31.999999953666826
target distance 16.0
model initialize at round 2013
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  3.]), 'previousTarget': array([13.,  3.]), 'currentState': array([10.13223087, 18.88266149,  5.41679674]), 'targetState': array([13,  3], dtype=int32), 'currentDistance': 16.13948684681357}
done in step count: 46
reward sum = 0.4569526095394631
running average episode reward sum: 0.47588774146937635
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  3.]), 'previousTarget': array([13.,  3.]), 'currentState': array([12.45507695,  3.77162529,  4.7804616 ]), 'targetState': array([13,  3], dtype=int32), 'currentDistance': 0.9446410506568677}
episode index:2014
target Thresh 31.99999995412785
target distance 26.0
model initialize at round 2014
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5.78107031, 9.67455616]), 'previousTarget': array([5.80053053, 9.68768483]), 'currentState': array([10.93753616, 28.99839791,  3.31432599]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 20.0}
done in step count: 90
reward sum = 0.18340041729940237
running average episode reward sum: 0.47574258646978823
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.3875331 , 3.95080259, 4.16684953]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.0267460618524917}
episode index:2015
target Thresh 31.999999954584286
target distance 12.0
model initialize at round 2015
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 18.]), 'previousTarget': array([ 7., 18.]), 'currentState': array([18.9981547 ,  8.99696368,  3.97330427]), 'targetState': array([ 7, 18], dtype=int32), 'currentDistance': 15.000345971047345}
done in step count: 39
reward sum = 0.45357132029855224
running average episode reward sum: 0.4757315888179176
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 18.]), 'previousTarget': array([ 7., 18.]), 'currentState': array([ 6.79422216, 17.08693735,  2.4410914 ]), 'targetState': array([ 7, 18], dtype=int32), 'currentDistance': 0.935963632533884}
episode index:2016
target Thresh 31.99999995503618
target distance 5.0
model initialize at round 2016
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  3.]), 'previousTarget': array([12.,  3.]), 'currentState': array([6.99255995, 4.01752181, 1.77602962]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 5.1097755820905855}
done in step count: 15
reward sum = 0.7699706227771463
running average episode reward sum: 0.4758774683587997
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  3.]), 'previousTarget': array([12.,  3.]), 'currentState': array([11.13796474,  3.97469647,  5.51348006]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 1.3012063654018162}
episode index:2017
target Thresh 31.999999955483577
target distance 14.0
model initialize at round 2017
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([ 2.01337749, 28.02672368,  0.85418046]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 17.81050938275486}
done in step count: 48
reward sum = 0.39593095801549033
running average episode reward sum: 0.47583785165397147
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([15.09963302, 17.66586624,  5.66696554]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 1.119838627737747}
episode index:2018
target Thresh 31.999999955926523
target distance 13.0
model initialize at round 2018
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([11.        ,  4.        ,  1.23878285]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 13.152946437965907}
done in step count: 30
reward sum = 0.5238426894032048
running average episode reward sum: 0.47586162819569966
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([23.13093262,  2.71110801,  0.4354726 ]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 1.1229215073770464}
episode index:2019
target Thresh 31.99999995636506
target distance 6.0
model initialize at round 2019
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  4.]), 'previousTarget': array([13.,  4.]), 'currentState': array([6.9856122 , 5.96843346, 4.53525385]), 'targetState': array([13,  4], dtype=int32), 'currentDistance': 6.32831659185904}
done in step count: 14
reward sum = 0.7483648130785093
running average episode reward sum: 0.47599653076247334
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  4.]), 'previousTarget': array([13.,  4.]), 'currentState': array([12.12052105,  4.73227581,  0.12594125]), 'targetState': array([13,  4], dtype=int32), 'currentDistance': 1.1444260922101486}
episode index:2020
target Thresh 31.999999956799236
target distance 20.0
model initialize at round 2020
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8.31194184, 14.70535022]), 'previousTarget': array([ 8.47568183, 14.638375  ]), 'currentState': array([25.76218333,  4.93349922,  3.30824721]), 'targetState': array([ 6, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.2931743676588484
running average episode reward sum: 0.4759060695239263
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 16.]), 'previousTarget': array([ 6., 16.]), 'currentState': array([ 6.96023127, 15.28007601,  2.55672887]), 'targetState': array([ 6, 16], dtype=int32), 'currentDistance': 1.2001394288974752}
episode index:2021
target Thresh 31.99999995722909
target distance 20.0
model initialize at round 2021
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8.50001133, 20.77498924]), 'previousTarget': array([ 8.50001133, 20.77498924]), 'currentState': array([23.        ,  7.        ,  4.89964589]), 'targetState': array([ 3, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 91
reward sum = 0.13308137885970672
running average episode reward sum: 0.4757365221991665
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 26.]), 'previousTarget': array([ 3., 26.]), 'currentState': array([ 3.80532174, 25.19704011,  2.10325311]), 'targetState': array([ 3, 26], dtype=int32), 'currentDistance': 1.137228077182542}
episode index:2022
target Thresh 31.99999995765467
target distance 8.0
model initialize at round 2022
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([4.85862421, 3.13386627, 2.3031103 ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 7.948509199753849}
done in step count: 18
reward sum = 0.712133629964744
running average episode reward sum: 0.47585337692371704
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.48833257, 10.29059301,  1.41711728]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.8746781380629723}
episode index:2023
target Thresh 31.99999995807601
target distance 18.0
model initialize at round 2023
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([24.99338977, 26.0032101 ,  2.9420104 ]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 18.113232087715925}
done in step count: 51
reward sum = 0.3725088712140667
running average episode reward sum: 0.4758023173853229
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([22.70917898,  8.94357076,  4.45102346]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 0.9873715850059224}
episode index:2024
target Thresh 31.999999958493163
target distance 12.0
model initialize at round 2024
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 11.]), 'previousTarget': array([19., 11.]), 'currentState': array([ 7.        , 19.        ,  1.50919318]), 'targetState': array([19, 11], dtype=int32), 'currentDistance': 14.422205101855958}
done in step count: 37
reward sum = 0.4722791303223116
running average episode reward sum: 0.47580057753985977
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 11.]), 'previousTarget': array([19., 11.]), 'currentState': array([18.39363514, 11.81733858,  5.90191052]), 'targetState': array([19, 11], dtype=int32), 'currentDistance': 1.0177036422680275}
episode index:2025
target Thresh 31.999999958906162
target distance 17.0
model initialize at round 2025
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 20.]), 'previousTarget': array([11., 20.]), 'currentState': array([13.99869048,  3.44333529,  1.67164055]), 'targetState': array([11, 20], dtype=int32), 'currentDistance': 16.826030159221414}
done in step count: 41
reward sum = 0.4377403627017477
running average episode reward sum: 0.4757817916490216
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 20.]), 'previousTarget': array([11., 20.]), 'currentState': array([11.34761347, 19.20942026,  1.35481026]), 'targetState': array([11, 20], dtype=int32), 'currentDistance': 0.8636269185562325}
episode index:2026
target Thresh 31.99999995931505
target distance 16.0
model initialize at round 2026
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([ 9.90946092, 24.01947055,  3.13337827]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 16.808837703364965}
done in step count: 41
reward sum = 0.41270201822062946
running average episode reward sum: 0.4757506718792
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.43900062,  8.82193889,  4.99714285]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.9951401121000074}
episode index:2027
target Thresh 31.999999959719876
target distance 27.0
model initialize at round 2027
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4.68110908, 20.93850829]), 'previousTarget': array([ 4.67544468, 20.97366596]), 'currentState': array([10.99283442,  1.96057049,  4.28011942]), 'targetState': array([ 2, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 84
reward sum = 0.1012332611744009
running average episode reward sum: 0.47556599859975984
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 29.]), 'previousTarget': array([ 2., 29.]), 'currentState': array([ 2.22371383, 28.00193005,  1.20636915]), 'targetState': array([ 2, 29], dtype=int32), 'currentDistance': 1.0228350344359967}
episode index:2028
target Thresh 31.999999960120668
target distance 4.0
model initialize at round 2028
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 23.]), 'previousTarget': array([ 5., 23.]), 'currentState': array([ 9.        , 23.        ,  5.17539167]), 'targetState': array([ 5, 23], dtype=int32), 'currentDistance': 3.9999999999999996}
done in step count: 11
reward sum = 0.8077473976944177
running average episode reward sum: 0.47572971540562214
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 23.]), 'previousTarget': array([ 5., 23.]), 'currentState': array([ 5.87889745, 22.03947382,  2.94951557]), 'targetState': array([ 5, 23], dtype=int32), 'currentDistance': 1.3019490298176701}
episode index:2029
target Thresh 31.999999960517474
target distance 10.0
model initialize at round 2029
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 15.]), 'previousTarget': array([ 5., 15.]), 'currentState': array([7.14541617, 5.17920638, 1.0440968 ]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 10.052402594904265}
done in step count: 22
reward sum = 0.6373193276952674
running average episode reward sum: 0.47580931619985356
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 15.]), 'previousTarget': array([ 5., 15.]), 'currentState': array([ 4.72029616, 14.0531405 ,  2.1838073 ]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 0.9873080312714279}
episode index:2030
target Thresh 31.999999960910333
target distance 19.0
model initialize at round 2030
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5.32014017, 8.09022194]), 'previousTarget': array([5.32014017, 8.09022194]), 'currentState': array([16.        , 25.        ,  3.26408291]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.2554926158832422
running average episode reward sum: 0.4757008392425337
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.97046846, 5.68005152, 3.75102127]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.0218493333403305}
episode index:2031
target Thresh 31.99999996129928
target distance 6.0
model initialize at round 2031
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 13.]), 'previousTarget': array([ 9., 13.]), 'currentState': array([12.14612007,  7.11376543,  0.86983016]), 'targetState': array([ 9, 13], dtype=int32), 'currentDistance': 6.674266165864609}
done in step count: 13
reward sum = 0.750629301463989
running average episode reward sum: 0.47583613868260327
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 13.]), 'previousTarget': array([ 9., 13.]), 'currentState': array([ 9.56733539, 12.14974569,  2.19954775]), 'targetState': array([ 9, 13], dtype=int32), 'currentDistance': 1.0221554866523614}
episode index:2032
target Thresh 31.99999996168436
target distance 10.0
model initialize at round 2032
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  6.]), 'previousTarget': array([18.,  6.]), 'currentState': array([ 7.98911946, 10.00874912,  2.23059797]), 'targetState': array([18,  6], dtype=int32), 'currentDistance': 10.783682051543545}
done in step count: 30
reward sum = 0.5617874410732122
running average episode reward sum: 0.47587841674575654
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  6.]), 'previousTarget': array([18.,  6.]), 'currentState': array([17.1793562 ,  6.3924    ,  5.84712706]), 'targetState': array([18,  6], dtype=int32), 'currentDistance': 0.9096339967410053}
episode index:2033
target Thresh 31.999999962065605
target distance 4.0
model initialize at round 2033
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 15.]), 'previousTarget': array([ 7., 15.]), 'currentState': array([ 8.        , 19.        ,  3.29737681]), 'targetState': array([ 7, 15], dtype=int32), 'currentDistance': 4.123105625617661}
done in step count: 9
reward sum = 0.8331839489640429
running average episode reward sum: 0.476054083182442
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 15.]), 'previousTarget': array([ 7., 15.]), 'currentState': array([ 7.50555091, 15.82239714,  4.72691396]), 'targetState': array([ 7, 15], dtype=int32), 'currentDistance': 0.9653594034536342}
episode index:2034
target Thresh 31.99999996244306
target distance 1.0
model initialize at round 2034
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 22.]), 'previousTarget': array([24., 22.]), 'currentState': array([24.67554878, 21.98562918,  3.17179778]), 'targetState': array([24, 22], dtype=int32), 'currentDistance': 0.6757016201481085}
done in step count: 0
reward sum = 0.9957497230893406
running average episode reward sum: 0.47630946187527096
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 22.]), 'previousTarget': array([24., 22.]), 'currentState': array([24.67554878, 21.98562918,  3.17179778]), 'targetState': array([24, 22], dtype=int32), 'currentDistance': 0.6757016201481085}
episode index:2035
target Thresh 31.99999996281676
target distance 3.0
model initialize at round 2035
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 14.]), 'previousTarget': array([21., 14.]), 'currentState': array([23.96848569, 13.91874011,  4.24731435]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 2.969597696201202}
done in step count: 23
reward sum = 0.7537337548267032
running average episode reward sum: 0.47644572135118035
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 14.]), 'previousTarget': array([21., 14.]), 'currentState': array([20.62811032, 13.01261411,  2.51416873]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 1.055098492619822}
episode index:2036
target Thresh 31.99999996318674
target distance 19.0
model initialize at round 2036
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 23.]), 'previousTarget': array([ 2., 23.]), 'currentState': array([20.95989703, 22.09915454,  2.10208654]), 'targetState': array([ 2, 23], dtype=int32), 'currentDistance': 18.981285988618037}
done in step count: 57
reward sum = 0.35962259333450636
running average episode reward sum: 0.4763883707728707
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 23.]), 'previousTarget': array([ 2., 23.]), 'currentState': array([ 2.89137404, 23.33255636,  3.15251375]), 'targetState': array([ 2, 23], dtype=int32), 'currentDistance': 0.9513891969191677}
episode index:2037
target Thresh 31.999999963553034
target distance 19.0
model initialize at round 2037
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.266971  , 23.21617134]), 'previousTarget': array([21.14213562, 23.14213562]), 'currentState': array([7.20052453, 8.99874951, 0.05117613]), 'targetState': array([26, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 85
reward sum = 0.1863546826138391
running average episode reward sum: 0.47624605787387214
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 28.]), 'previousTarget': array([26., 28.]), 'currentState': array([25.69426487, 27.09771774,  0.81912476]), 'targetState': array([26, 28], dtype=int32), 'currentDistance': 0.9526737359743864}
episode index:2038
target Thresh 31.999999963915688
target distance 8.0
model initialize at round 2038
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  2.]), 'previousTarget': array([17.,  2.]), 'currentState': array([9.14461651, 7.99904924, 6.15662817]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 9.884110556273907}
done in step count: 19
reward sum = 0.6747419412166433
running average episode reward sum: 0.4763434074978755
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  2.]), 'previousTarget': array([17.,  2.]), 'currentState': array([16.06358643,  2.87203172,  5.63375746]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 1.279574025607192}
episode index:2039
target Thresh 31.999999964274732
target distance 14.0
model initialize at round 2039
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([18.99853019, 19.00498411,  2.1097655 ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 16.125712943696318}
done in step count: 40
reward sum = 0.42874449217613153
running average episode reward sum: 0.4763200746962472
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.74319993, 11.56515427,  3.59520584]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.9336731188398532}
episode index:2040
target Thresh 31.999999964630206
target distance 18.0
model initialize at round 2040
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 16.]), 'previousTarget': array([ 3., 16.]), 'currentState': array([20.98685426, 22.95402102,  4.26671544]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 19.284328731354915}
done in step count: 47
reward sum = 0.36777369647596725
running average episode reward sum: 0.4762668917573838
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 16.]), 'previousTarget': array([ 3., 16.]), 'currentState': array([ 3.8061752 , 15.89775632,  3.4910307 ]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 0.8126328989655058}
episode index:2041
target Thresh 31.99999996498214
target distance 7.0
model initialize at round 2041
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 19.]), 'previousTarget': array([26., 19.]), 'currentState': array([24.00273706, 26.02645168,  1.21518922]), 'targetState': array([26, 19], dtype=int32), 'currentDistance': 7.304798590901944}
done in step count: 19
reward sum = 0.6756062363393919
running average episode reward sum: 0.47636451141682645
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 19.]), 'previousTarget': array([26., 19.]), 'currentState': array([25.95230139, 19.94635287,  4.7947448 ]), 'targetState': array([26, 19], dtype=int32), 'currentDistance': 0.9475541708968652}
episode index:2042
target Thresh 31.999999965330574
target distance 22.0
model initialize at round 2042
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.26739954, 22.81234081]), 'previousTarget': array([16.42295739, 22.55791146]), 'currentState': array([25.79008848,  5.22489087,  2.31177855]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.20725498897540703
running average episode reward sum: 0.47602989590023703
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 27.]), 'previousTarget': array([14., 27.]), 'currentState': array([11.59061085, 25.37428077,  1.84270636]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 2.906564830631699}
episode index:2043
target Thresh 31.999999965675542
target distance 13.0
model initialize at round 2043
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 23.]), 'previousTarget': array([15., 23.]), 'currentState': array([26.06092384, 10.15621306,  1.35083264]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 16.950129768800796}
done in step count: 45
reward sum = 0.4180633646659496
running average episode reward sum: 0.4760015365405334
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 23.]), 'previousTarget': array([15., 23.]), 'currentState': array([15.87886306, 22.65004519,  2.15520292]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 0.9459749716725654}
episode index:2044
target Thresh 31.999999966017075
target distance 5.0
model initialize at round 2044
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 11.]), 'previousTarget': array([20., 11.]), 'currentState': array([18.1171507 , 15.99139886,  6.10852728]), 'targetState': array([20, 11], dtype=int32), 'currentDistance': 5.334714992009429}
done in step count: 13
reward sum = 0.790160461241806
running average episode reward sum: 0.4761551594865976
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 11.]), 'previousTarget': array([20., 11.]), 'currentState': array([20.16995212, 11.64371827,  4.75394307]), 'targetState': array([20, 11], dtype=int32), 'currentDistance': 0.6657754350204269}
episode index:2045
target Thresh 31.99999996635521
target distance 16.0
model initialize at round 2045
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 16.]), 'previousTarget': array([ 4., 16.]), 'currentState': array([19.83845417, 13.15836524,  2.49452329]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 16.091349181588832}
done in step count: 37
reward sum = 0.46617096123413604
running average episode reward sum: 0.4761502796243041
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 16.]), 'previousTarget': array([ 4., 16.]), 'currentState': array([ 4.83591237, 15.24630979,  2.95557126]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 1.1255213974344878}
episode index:2046
target Thresh 31.99999996668998
target distance 6.0
model initialize at round 2046
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 27.]), 'previousTarget': array([20., 27.]), 'currentState': array([14.06566885, 26.18389755,  1.15983012]), 'targetState': array([20, 27], dtype=int32), 'currentDistance': 5.990184423232493}
done in step count: 13
reward sum = 0.7741636860716059
running average episode reward sum: 0.4762958650695642
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 27.]), 'previousTarget': array([20., 27.]), 'currentState': array([19.22469241, 27.7756063 ,  6.17774217]), 'targetState': array([20, 27], dtype=int32), 'currentDistance': 1.0966617521584747}
episode index:2047
target Thresh 31.999999967021424
target distance 15.0
model initialize at round 2047
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 25.]), 'previousTarget': array([24., 25.]), 'currentState': array([ 9.02853234, 24.06403547,  1.03143317]), 'targetState': array([24, 25], dtype=int32), 'currentDistance': 15.000695766589423}
done in step count: 32
reward sum = 0.4938774683996304
running average episode reward sum: 0.4763044498368152
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 25.]), 'previousTarget': array([24., 25.]), 'currentState': array([23.02285249, 25.30252769,  6.22469393]), 'targetState': array([24, 25], dtype=int32), 'currentDistance': 1.0229077434611769}
episode index:2048
target Thresh 31.999999967349563
target distance 7.0
model initialize at round 2048
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 17.]), 'previousTarget': array([20., 17.]), 'currentState': array([13.        , 19.        ,  2.61539683]), 'targetState': array([20, 17], dtype=int32), 'currentDistance': 7.280109889280517}
done in step count: 22
reward sum = 0.6578846346938036
running average episode reward sum: 0.47639306876549115
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 17.]), 'previousTarget': array([20., 17.]), 'currentState': array([19.4704866 , 17.97195004,  5.65163754]), 'targetState': array([20, 17], dtype=int32), 'currentDistance': 1.106829403386243}
episode index:2049
target Thresh 31.99999996767444
target distance 6.0
model initialize at round 2049
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 23.]), 'previousTarget': array([13., 23.]), 'currentState': array([ 9.       , 29.       ,  1.2526027]), 'targetState': array([13, 23], dtype=int32), 'currentDistance': 7.211102550927979}
done in step count: 20
reward sum = 0.683902164583269
running average episode reward sum: 0.47649429271467053
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 23.]), 'previousTarget': array([13., 23.]), 'currentState': array([12.94610239, 23.97614178,  5.49847999]), 'targetState': array([13, 23], dtype=int32), 'currentDistance': 0.9776286291992545}
episode index:2050
target Thresh 31.999999967996086
target distance 26.0
model initialize at round 2050
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.05572809, 20.88854382]), 'previousTarget': array([14.05572809, 20.88854382]), 'currentState': array([23.        ,  3.        ,  4.83903399]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 88
reward sum = 0.11930927309190625
running average episode reward sum: 0.4763201410717535
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([10.72174827, 28.22474389,  1.81821031]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 1.059217920364538}
episode index:2051
target Thresh 31.99999996831453
target distance 10.0
model initialize at round 2051
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 5.]), 'previousTarget': array([6., 5.]), 'currentState': array([14.06345008, 14.78027229,  4.86102828]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 12.675683537476827}
done in step count: 26
reward sum = 0.5732632758030023
running average episode reward sum: 0.4763673843147999
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 5.]), 'previousTarget': array([6., 5.]), 'currentState': array([6.84443252, 5.94725135, 3.76235002]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 1.2689962159587524}
episode index:2052
target Thresh 31.999999968629808
target distance 10.0
model initialize at round 2052
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  2.]), 'previousTarget': array([21.,  2.]), 'currentState': array([10.9523482 , 11.98855617,  3.60858458]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 14.167800147203481}
done in step count: 52
reward sum = 0.35942054218309827
running average episode reward sum: 0.4763104204365088
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  2.]), 'previousTarget': array([21.,  2.]), 'currentState': array([21.82115705,  2.96845452,  4.93401893]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 1.2697255834425343}
episode index:2053
target Thresh 31.999999968941946
target distance 9.0
model initialize at round 2053
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([19.        , 19.        ,  1.63153064]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 10.295630140987}
done in step count: 29
reward sum = 0.5738663409554866
running average episode reward sum: 0.4763579160161188
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([23.30715783, 10.76129277,  5.14107891]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 1.029367260562872}
episode index:2054
target Thresh 31.99999996925098
target distance 10.0
model initialize at round 2054
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 22.]), 'previousTarget': array([11., 22.]), 'currentState': array([20.67138762, 25.89517636,  3.50698976]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 10.426319456897334}
done in step count: 23
reward sum = 0.6455556489519717
running average episode reward sum: 0.476440250679348
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 22.]), 'previousTarget': array([11., 22.]), 'currentState': array([11.80470227, 22.38361918,  3.44833157]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 0.8914647551006731}
episode index:2055
target Thresh 31.999999969556935
target distance 23.0
model initialize at round 2055
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.7042351,  4.5731765]), 'previousTarget': array([21.7042351,  4.5731765]), 'currentState': array([2.       , 8.       , 1.3199625]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.24548167604132365
running average episode reward sum: 0.47632791674226727
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  4.]), 'previousTarget': array([25.,  4.]), 'currentState': array([24.32603562,  4.43878921,  6.1601068 ]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 0.8042163614667158}
episode index:2056
target Thresh 31.99999996985985
target distance 4.0
model initialize at round 2056
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([16.83384072, 12.12119083,  2.75853568]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 4.3815277195489495}
done in step count: 14
reward sum = 0.784877923678168
running average episode reward sum: 0.4764779167456391
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13.9359859 ,  9.57491759,  3.93550756]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 1.0279905935751343}
episode index:2057
target Thresh 31.99999997015975
target distance 11.0
model initialize at round 2057
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([11.95195237, 14.5727956 ,  4.54686127]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 10.977159453080144}
done in step count: 24
reward sum = 0.6190643722336073
running average episode reward sum: 0.4765472007376158
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([8.92868477, 4.80967647, 4.56914651]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 0.8128110800728179}
episode index:2058
target Thresh 31.999999970456663
target distance 17.0
model initialize at round 2058
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 18.]), 'previousTarget': array([27., 18.]), 'currentState': array([9.99981252, 7.99997632, 3.01476049]), 'targetState': array([27, 18], dtype=int32), 'currentDistance': 19.72325652616349}
done in step count: 52
reward sum = 0.3136160883338598
running average episode reward sum: 0.4764680695514071
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 18.]), 'previousTarget': array([27., 18.]), 'currentState': array([26.0960252 , 17.57000928,  0.50740477]), 'targetState': array([27, 18], dtype=int32), 'currentDistance': 1.0010306932473299}
episode index:2059
target Thresh 31.999999970750626
target distance 11.0
model initialize at round 2059
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([17.        ,  6.        ,  5.84530294]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 11.401754250991381}
done in step count: 32
reward sum = 0.5175107136172994
running average episode reward sum: 0.4764879931650313
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.20989485, 8.05871875, 2.73079279]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.9643994186241616}
episode index:2060
target Thresh 31.99999997104166
target distance 13.0
model initialize at round 2060
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([ 8.00136582, 18.97278531,  5.01503372]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 15.248927706306949}
done in step count: 41
reward sum = 0.46959817798194897
running average episode reward sum: 0.47648465021734426
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([20.05564553, 11.22036944,  5.63356347]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 0.9697257678596992}
episode index:2061
target Thresh 31.999999971329803
target distance 16.0
model initialize at round 2061
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 28.]), 'previousTarget': array([21., 28.]), 'currentState': array([ 5.        , 21.        ,  1.54277134]), 'targetState': array([21, 28], dtype=int32), 'currentDistance': 17.46424919657298}
done in step count: 43
reward sum = 0.394201419380036
running average episode reward sum: 0.4764447456437083
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 28.]), 'previousTarget': array([21., 28.]), 'currentState': array([20.02829257, 28.82942803,  0.52045873]), 'targetState': array([21, 28], dtype=int32), 'currentDistance': 1.277562594039068}
episode index:2062
target Thresh 31.999999971615075
target distance 25.0
model initialize at round 2062
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.22561063, 21.81774824]), 'previousTarget': array([16.22561063, 21.81774824]), 'currentState': array([23.        ,  3.        ,  3.89606571]), 'targetState': array([14, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 75
reward sum = 0.18036959752342807
running average episode reward sum: 0.4763012288486912
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 28.]), 'previousTarget': array([14., 28.]), 'currentState': array([14.35384902, 27.23585715,  1.53856936]), 'targetState': array([14, 28], dtype=int32), 'currentDistance': 0.8420946639775369}
episode index:2063
target Thresh 31.99999997189751
target distance 7.0
model initialize at round 2063
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 16.]), 'previousTarget': array([24., 16.]), 'currentState': array([26.07972148,  9.22083288,  1.36743259]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 7.091004742199486}
done in step count: 15
reward sum = 0.7459780318118001
running average episode reward sum: 0.47643188621446786
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 16.]), 'previousTarget': array([24., 16.]), 'currentState': array([24.30114578, 15.28930142,  2.08766628]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 0.7718686711602721}
episode index:2064
target Thresh 31.999999972177136
target distance 14.0
model initialize at round 2064
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 26.]), 'previousTarget': array([13., 26.]), 'currentState': array([27.0177829 , 28.03506222,  1.35267255]), 'targetState': array([13, 26], dtype=int32), 'currentDistance': 14.164734931106462}
done in step count: 37
reward sum = 0.4713146842308937
running average episode reward sum: 0.47642940815055335
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 26.]), 'previousTarget': array([13., 26.]), 'currentState': array([13.99832514, 26.30705906,  3.34637618]), 'targetState': array([13, 26], dtype=int32), 'currentDistance': 1.0444799437408314}
episode index:2065
target Thresh 31.999999972453978
target distance 21.0
model initialize at round 2065
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.09254435,  8.95024189]), 'previousTarget': array([22.10381815,  9.09009055]), 'currentState': array([24.03117836, 28.8560626 ,  4.81154309]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.3122992234919958
running average episode reward sum: 0.4763499646923449
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([22.34046018,  8.67900783,  4.70129557]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 0.7595819674069134}
episode index:2066
target Thresh 31.999999972728062
target distance 11.0
model initialize at round 2066
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([16.03017901, 20.00326058,  6.14233715]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 11.05138055311807}
done in step count: 28
reward sum = 0.5742436245474536
running average episode reward sum: 0.476397324953523
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.81512951,  9.73173627,  4.31541572]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.7547284744180028}
episode index:2067
target Thresh 31.999999972999426
target distance 14.0
model initialize at round 2067
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 19.]), 'previousTarget': array([ 9., 19.]), 'currentState': array([23.        , 22.        ,  1.62185243]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 14.317821063276353}
done in step count: 35
reward sum = 0.48433288773830707
running average episode reward sum: 0.47640116226628165
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 19.]), 'previousTarget': array([ 9., 19.]), 'currentState': array([ 9.90021847, 19.51963343,  3.51941371]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 1.0394287855287085}
episode index:2068
target Thresh 31.999999973268086
target distance 19.0
model initialize at round 2068
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 13.]), 'previousTarget': array([21., 13.]), 'currentState': array([ 2.        , 10.        ,  2.38160181]), 'targetState': array([21, 13], dtype=int32), 'currentDistance': 19.235384061671343}
done in step count: 66
reward sum = 0.2926439120474965
running average episode reward sum: 0.4763123477422513
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 13.]), 'previousTarget': array([21., 13.]), 'currentState': array([20.26396731, 12.8187821 ,  0.13253736]), 'targetState': array([21, 13], dtype=int32), 'currentDistance': 0.7580132282868611}
episode index:2069
target Thresh 31.99999997353407
target distance 18.0
model initialize at round 2069
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.70689454, 20.20417984]), 'previousTarget': array([24.70981108, 20.21358457]), 'currentState': array([13.02011585,  3.97398564,  5.62311077]), 'targetState': array([26, 22], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 79
reward sum = 0.23642220949544582
running average episode reward sum: 0.4761964587865765
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 22.]), 'previousTarget': array([26., 22.]), 'currentState': array([25.35237637, 21.04432291,  1.27752928]), 'targetState': array([26, 22], dtype=int32), 'currentDistance': 1.1544414538102266}
episode index:2070
target Thresh 31.99999997379741
target distance 8.0
model initialize at round 2070
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 13.]), 'previousTarget': array([13., 13.]), 'currentState': array([ 5.       , 20.       ,  2.0838981]), 'targetState': array([13, 13], dtype=int32), 'currentDistance': 10.630145812734648}
done in step count: 27
reward sum = 0.5770017822415278
running average episode reward sum: 0.47624513349611536
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 13.]), 'previousTarget': array([13., 13.]), 'currentState': array([12.02594936, 13.96218564,  5.55783253]), 'targetState': array([13, 13], dtype=int32), 'currentDistance': 1.3691515040778228}
episode index:2071
target Thresh 31.99999997405813
target distance 8.0
model initialize at round 2071
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'previousTarget': array([19.,  3.]), 'currentState': array([27.02096262,  6.98692386,  5.49513707]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 8.957198402367581}
done in step count: 20
reward sum = 0.6462734466114002
running average episode reward sum: 0.4763271934927926
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'previousTarget': array([19.,  3.]), 'currentState': array([19.8611926 ,  3.43706505,  3.42990742]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 0.9657528449494769}
episode index:2072
target Thresh 31.999999974316257
target distance 22.0
model initialize at round 2072
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.09983599,  8.25597584]), 'previousTarget': array([10.12677025,  8.26249016]), 'currentState': array([26.95713871, 19.01847299,  2.97140732]), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 73
reward sum = 0.2157890103862573
running average episode reward sum: 0.476201511783624
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.8452487 , 5.82779021, 3.62267665]), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.1830815676107584}
episode index:2073
target Thresh 31.999999974571814
target distance 17.0
model initialize at round 2073
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 10.]), 'previousTarget': array([17., 10.]), 'currentState': array([15.05032704, 26.73647352,  4.92828181]), 'targetState': array([17, 10], dtype=int32), 'currentDistance': 16.849651945734557}
done in step count: 39
reward sum = 0.4523722821996421
running average episode reward sum: 0.47619002228044954
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 10.]), 'previousTarget': array([17., 10.]), 'currentState': array([16.98146169, 10.82515183,  4.83870787]), 'targetState': array([17, 10], dtype=int32), 'currentDistance': 0.8253600525127819}
episode index:2074
target Thresh 31.99999997482483
target distance 8.0
model initialize at round 2074
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 27.]), 'previousTarget': array([13., 27.]), 'currentState': array([ 9.32183913, 19.00147936,  0.23730117]), 'targetState': array([13, 27], dtype=int32), 'currentDistance': 8.803703758893032}
done in step count: 19
reward sum = 0.6688425850535151
running average episode reward sum: 0.47628286688901483
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 27.]), 'previousTarget': array([13., 27.]), 'currentState': array([12.93513936, 26.2636287 ,  1.19834637]), 'targetState': array([13, 27], dtype=int32), 'currentDistance': 0.7392222863303279}
episode index:2075
target Thresh 31.999999975075326
target distance 21.0
model initialize at round 2075
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.24830374, 11.96374756]), 'previousTarget': array([19.18513205, 11.98417253]), 'currentState': array([ 4.1071221 , 25.03067599,  0.17263538]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.07085421818587725
running average episode reward sum: 0.4760193133798265
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.77979549,  7.3988297 ]), 'previousTarget': array([24.72796617,  7.486577  ]), 'currentState': array([15.11284603, 24.90740155,  6.03939797]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 20.0}
episode index:2076
target Thresh 31.99999997532333
target distance 20.0
model initialize at round 2076
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9.03290182, 6.0379725 ]), 'previousTarget': array([9., 6.]), 'currentState': array([25.00758607, 18.07165288,  1.70413938]), 'targetState': array([5, 3], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 87
reward sum = 0.15859058962289352
running average episode reward sum: 0.47586648298803214
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 3.]), 'previousTarget': array([5., 3.]), 'currentState': array([5.60553326, 3.72794948, 4.22269764]), 'targetState': array([5, 3], dtype=int32), 'currentDistance': 0.94687959642928}
episode index:2077
target Thresh 31.99999997556887
target distance 10.0
model initialize at round 2077
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 6.]), 'previousTarget': array([9., 6.]), 'currentState': array([ 6.9839794 , 16.00194847,  3.27306414]), 'targetState': array([9, 6], dtype=int32), 'currentDistance': 10.203103071535395}
done in step count: 23
reward sum = 0.6038345265502947
running average episode reward sum: 0.47592806529965975
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 6.]), 'previousTarget': array([9., 6.]), 'currentState': array([8.56767532, 6.79785566, 4.86282672]), 'targetState': array([9, 6], dtype=int32), 'currentDistance': 0.9074570432194558}
episode index:2078
target Thresh 31.999999975811964
target distance 8.0
model initialize at round 2078
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 14.]), 'previousTarget': array([ 3., 14.]), 'currentState': array([ 4.78457948, 21.5999172 ,  4.34871349]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 7.8066295725208255}
done in step count: 16
reward sum = 0.7256508636029325
running average episode reward sum: 0.47604818208576044
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 14.]), 'previousTarget': array([ 3., 14.]), 'currentState': array([ 2.93871002, 14.99523251,  4.39611606]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 0.9971179517772325}
episode index:2079
target Thresh 31.99999997605264
target distance 13.0
model initialize at round 2079
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  3.]), 'previousTarget': array([22.,  3.]), 'currentState': array([9.        , 5.        , 4.17944956]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 13.152946437965905}
done in step count: 36
reward sum = 0.5068447165695542
running average episode reward sum: 0.4760629881119545
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  3.]), 'previousTarget': array([22.,  3.]), 'currentState': array([21.1144693 ,  2.94830399,  6.12947868]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 0.8870383815915245}
episode index:2080
target Thresh 31.99999997629092
target distance 6.0
model initialize at round 2080
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 19.]), 'previousTarget': array([23., 19.]), 'currentState': array([17.48576238, 21.98882132,  6.08465235]), 'targetState': array([23, 19], dtype=int32), 'currentDistance': 6.2721503044742315}
done in step count: 11
reward sum = 0.7803794822119439
running average episode reward sum: 0.4762092238131078
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 19.]), 'previousTarget': array([23., 19.]), 'currentState': array([22.01989867, 19.33418641,  5.85535667]), 'targetState': array([23, 19], dtype=int32), 'currentDistance': 1.035509136993447}
episode index:2081
target Thresh 31.999999976526826
target distance 6.0
model initialize at round 2081
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 15.]), 'previousTarget': array([24., 15.]), 'currentState': array([21.03197627, 20.57525091,  4.96989453]), 'targetState': array([24, 15], dtype=int32), 'currentDistance': 6.316057914351936}
done in step count: 11
reward sum = 0.78857851415571
running average episode reward sum: 0.4763592570937719
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 15.]), 'previousTarget': array([24., 15.]), 'currentState': array([23.31348068, 15.95419917,  5.17737522]), 'targetState': array([24, 15], dtype=int32), 'currentDistance': 1.175501947286682}
episode index:2082
target Thresh 31.99999997676039
target distance 7.0
model initialize at round 2082
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([11.78639306, 14.39152543,  1.99042608]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 7.171884156819519}
done in step count: 15
reward sum = 0.7506945410512467
running average episode reward sum: 0.4764909591023929
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([ 9.52092027, 20.20108204,  1.93679754]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 0.9537441184560412}
episode index:2083
target Thresh 31.999999976991628
target distance 17.0
model initialize at round 2083
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  7.]), 'previousTarget': array([21.,  7.]), 'currentState': array([27.        , 24.        ,  2.85756397]), 'targetState': array([21,  7], dtype=int32), 'currentDistance': 18.027756377319946}
done in step count: 54
reward sum = 0.3782210568707728
running average episode reward sum: 0.4764438046387501
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  7.]), 'previousTarget': array([21.,  7.]), 'currentState': array([21.3969398 ,  7.9579355 ,  4.48795473]), 'targetState': array([21,  7], dtype=int32), 'currentDistance': 1.0369192963775682}
episode index:2084
target Thresh 31.999999977220565
target distance 6.0
model initialize at round 2084
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 11.]), 'previousTarget': array([20., 11.]), 'currentState': array([25.63572775,  7.21911745,  2.6284159 ]), 'targetState': array([20, 11], dtype=int32), 'currentDistance': 6.786493948751857}
done in step count: 15
reward sum = 0.7536387338093103
running average episode reward sum: 0.4765767518469854
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 11.]), 'previousTarget': array([20., 11.]), 'currentState': array([20.79789643, 10.83763317,  2.61674875]), 'targetState': array([20, 11], dtype=int32), 'currentDistance': 0.814249161027699}
episode index:2085
target Thresh 31.999999977447224
target distance 25.0
model initialize at round 2085
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.68609452, 18.23068683]), 'previousTarget': array([17.68609452, 18.23068683]), 'currentState': array([6.        , 2.        , 3.21309349]), 'targetState': array([24, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.2252595901189085
running average episode reward sum: 0.47624030105984927
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 27.]), 'previousTarget': array([24., 27.]), 'currentState': array([19.24075125, 22.5657074 ,  0.66185453]), 'targetState': array([24, 27], dtype=int32), 'currentDistance': 6.504875061083304}
episode index:2086
target Thresh 31.999999977671628
target distance 5.0
model initialize at round 2086
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 13.]), 'previousTarget': array([21., 13.]), 'currentState': array([22.69064071,  8.24340179,  2.29665503]), 'targetState': array([21, 13], dtype=int32), 'currentDistance': 5.048117719475984}
done in step count: 9
reward sum = 0.8255674743643561
running average episode reward sum: 0.4764076835099233
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 13.]), 'previousTarget': array([21., 13.]), 'currentState': array([21.31124639, 12.12764551,  1.86118678]), 'targetState': array([21, 13], dtype=int32), 'currentDistance': 0.9262163198395647}
episode index:2087
target Thresh 31.999999977893797
target distance 18.0
model initialize at round 2087
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7.05209562, 5.1043344 ]), 'previousTarget': array([7.05572809, 5.11145618]), 'currentState': array([15.98654523, 22.997786  ,  3.55451205]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.35082281740431187
running average episode reward sum: 0.476347537501252
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 5.]), 'previousTarget': array([7., 5.]), 'currentState': array([7.26915393, 5.98973171, 4.16805345]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 1.0256766961760395}
episode index:2088
target Thresh 31.99999997811376
target distance 16.0
model initialize at round 2088
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.14213562, 27.14213562]), 'previousTarget': array([19.14213562, 27.14213562]), 'currentState': array([ 5.        , 13.        ,  1.92439085]), 'targetState': array([21, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 75
reward sum = 0.255732469417157
running average episode reward sum: 0.47624192952227445
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 29.]), 'previousTarget': array([21., 29.]), 'currentState': array([20.30612528, 28.64091014,  0.52689232]), 'targetState': array([21, 29], dtype=int32), 'currentDistance': 0.781285892546039}
episode index:2089
target Thresh 31.99999997833153
target distance 17.0
model initialize at round 2089
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 16.]), 'previousTarget': array([10., 16.]), 'currentState': array([26.72765825, 18.88580445,  3.4390498 ]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 16.974758260701282}
done in step count: 41
reward sum = 0.4494580364467245
running average episode reward sum: 0.4762291142624297
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 16.]), 'previousTarget': array([10., 16.]), 'currentState': array([10.96795655, 16.05508776,  3.52301758]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 0.969522846906411}
episode index:2090
target Thresh 31.999999978547134
target distance 5.0
model initialize at round 2090
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 15.]), 'previousTarget': array([ 5., 15.]), 'currentState': array([10.        , 17.        ,  1.84569168]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 5.385164807134504}
done in step count: 12
reward sum = 0.7717891273935813
running average episode reward sum: 0.4763704629057253
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 15.]), 'previousTarget': array([ 5., 15.]), 'currentState': array([ 5.95429245, 15.53311413,  3.49816295]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 1.0931078459086085}
episode index:2091
target Thresh 31.999999978760595
target distance 7.0
model initialize at round 2091
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([ 7.        , 15.        ,  5.55942228]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 9.219544457292889}
done in step count: 21
reward sum = 0.6407735397331333
running average episode reward sum: 0.4764490494625262
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([13.24969002, 20.69455622,  0.71967501]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 0.8100993571906459}
episode index:2092
target Thresh 31.999999978971932
target distance 22.0
model initialize at round 2092
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7.23099415, 10.50855393]), 'previousTarget': array([ 7.49734288, 10.56757793]), 'currentState': array([26.73079537, 14.95352324,  3.44189325]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.3067848153783085
running average episode reward sum: 0.4763679867611004
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 5.85357615, 10.05904979,  3.4648568 ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.855616219272242}
episode index:2093
target Thresh 31.999999979181162
target distance 20.0
model initialize at round 2093
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.21065549,  7.80856954]), 'previousTarget': array([25.2384301 ,  7.79270645]), 'currentState': array([ 6.98507207, 16.04435177,  1.69312617]), 'targetState': array([27,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.28128714820421075
running average episode reward sum: 0.4762748249470809
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  7.]), 'previousTarget': array([27.,  7.]), 'currentState': array([26.12579994,  7.5178495 ,  5.81053246]), 'targetState': array([27,  7], dtype=int32), 'currentDistance': 1.0160678397771472}
episode index:2094
target Thresh 31.999999979388313
target distance 15.0
model initialize at round 2094
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 21.]), 'previousTarget': array([ 2., 21.]), 'currentState': array([16.96247567, 13.09590436,  2.07898521]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 16.921891326781864}
done in step count: 43
reward sum = 0.44410125588609084
running average episode reward sum: 0.4762594676348799
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 21.]), 'previousTarget': array([ 2., 21.]), 'currentState': array([ 2.8123778 , 20.47424644,  2.37876041]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 0.9676644513837811}
episode index:2095
target Thresh 31.999999979593404
target distance 4.0
model initialize at round 2095
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 26.]), 'previousTarget': array([25., 26.]), 'currentState': array([21.1348866 , 24.90499177,  5.89957619]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 4.0172309621092435}
done in step count: 8
reward sum = 0.8488378249560206
running average episode reward sum: 0.47643722448474685
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 26.]), 'previousTarget': array([25., 26.]), 'currentState': array([24.30577325, 25.79811834,  0.45825379]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 0.7229847720058494}
episode index:2096
target Thresh 31.999999979796453
target distance 13.0
model initialize at round 2096
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([21.06675852, 17.01487402,  6.24990794]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 17.748059245768108}
done in step count: 52
reward sum = 0.3651034294131136
running average episode reward sum: 0.4763841325462292
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([9.90624683, 4.34645064, 3.97412734]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 0.9702120166990412}
episode index:2097
target Thresh 31.999999979997483
target distance 22.0
model initialize at round 2097
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8.44274887, 11.57570914]), 'previousTarget': array([ 8.44208854, 11.57704261]), 'currentState': array([25.99904323,  1.99570244,  4.2408309 ]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.18837049473515674
running average episode reward sum: 0.47624685245194365
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([ 4.92620233, 13.48065563,  2.94428308]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 1.061870673886908}
episode index:2098
target Thresh 31.99999998019651
target distance 15.0
model initialize at round 2098
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 16.]), 'previousTarget': array([21., 16.]), 'currentState': array([ 6.      , 12.      ,  5.063694]), 'targetState': array([21, 16], dtype=int32), 'currentDistance': 15.524174696260022}
done in step count: 43
reward sum = 0.4450247044890172
running average episode reward sum: 0.4762319776792124
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 16.]), 'previousTarget': array([21., 16.]), 'currentState': array([20.22333877, 15.49083713,  0.18977757]), 'targetState': array([21, 16], dtype=int32), 'currentDistance': 0.9286815960103252}
episode index:2099
target Thresh 31.999999980393557
target distance 6.0
model initialize at round 2099
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  8.]), 'previousTarget': array([12.,  8.]), 'currentState': array([17.75162837,  2.33659952,  2.21545755]), 'targetState': array([12,  8], dtype=int32), 'currentDistance': 8.07188539641451}
done in step count: 15
reward sum = 0.7325750312112674
running average episode reward sum: 0.4763540457999419
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  8.]), 'previousTarget': array([12.,  8.]), 'currentState': array([12.90399871,  7.27689217,  2.60017647]), 'targetState': array([12,  8], dtype=int32), 'currentDistance': 1.1576262783022644}
episode index:2100
target Thresh 31.999999980588644
target distance 11.0
model initialize at round 2100
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 18.]), 'previousTarget': array([13., 18.]), 'currentState': array([ 4.04588842, 29.02115411,  0.2078433 ]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 14.20006873500581}
done in step count: 32
reward sum = 0.5125255392015357
running average episode reward sum: 0.4763712621223606
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 18.]), 'previousTarget': array([13., 18.]), 'currentState': array([12.47381333, 18.98250119,  5.64124593]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 1.1145317389233775}
episode index:2101
target Thresh 31.99999998078179
target distance 25.0
model initialize at round 2101
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8.12395776, 20.95624294]), 'previousTarget': array([ 8.21892607, 20.84542801]), 'currentState': array([18.86923295,  4.08795728,  2.517058  ]), 'targetState': array([ 3, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 95
reward sum = 0.15936585753013618
running average episode reward sum: 0.4762204507976259
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 29.]), 'previousTarget': array([ 3., 29.]), 'currentState': array([ 3.04652629, 28.31416077,  1.74680257]), 'targetState': array([ 3, 29], dtype=int32), 'currentDistance': 0.6874155510825701}
episode index:2102
target Thresh 31.999999980973016
target distance 17.0
model initialize at round 2102
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 10.]), 'previousTarget': array([23., 10.]), 'currentState': array([ 6.        , 20.        ,  0.94458109]), 'targetState': array([23, 10], dtype=int32), 'currentDistance': 19.72308292331602}
done in step count: 72
reward sum = 0.3120635022896428
running average episode reward sum: 0.4761423923342365
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 10.]), 'previousTarget': array([23., 10.]), 'currentState': array([22.29555487, 10.82968051,  5.51570498]), 'targetState': array([23, 10], dtype=int32), 'currentDistance': 1.0883991376898094}
episode index:2103
target Thresh 31.999999981162336
target distance 15.0
model initialize at round 2103
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([17.09711463, 24.76819366,  5.07483482]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 16.30182107294237}
done in step count: 41
reward sum = 0.46168584487685227
running average episode reward sum: 0.47613552135160464
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([23.69263368, 10.71225014,  5.19552228]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 0.7757411402009741}
episode index:2104
target Thresh 31.999999981349774
target distance 3.0
model initialize at round 2104
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 13.]), 'previousTarget': array([10., 13.]), 'currentState': array([10.88397705, 15.94675448,  3.77492708]), 'targetState': array([10, 13], dtype=int32), 'currentDistance': 3.0764878347288143}
done in step count: 7
reward sum = 0.8898026493938255
running average episode reward sum: 0.47633203780198097
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 13.]), 'previousTarget': array([10., 13.]), 'currentState': array([10.06146968, 13.88958372,  4.53261469]), 'targetState': array([10, 13], dtype=int32), 'currentDistance': 0.8917049517713923}
episode index:2105
target Thresh 31.99999998153535
target distance 7.0
model initialize at round 2105
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([24.00764888, 16.02337617,  1.47695553]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 9.21016694748003}
done in step count: 21
reward sum = 0.6599647062857894
running average episode reward sum: 0.47641923280126103
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([17.40958714, 21.16360499,  2.42223793]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 0.9312992205802331}
episode index:2106
target Thresh 31.999999981719075
target distance 13.0
model initialize at round 2106
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 22.]), 'previousTarget': array([19., 22.]), 'currentState': array([ 6.01283325, 17.04698055,  1.14119673]), 'targetState': array([19, 22], dtype=int32), 'currentDistance': 13.899600784948479}
done in step count: 32
reward sum = 0.5196938698521263
running average episode reward sum: 0.47643977130959086
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 22.]), 'previousTarget': array([19., 22.]), 'currentState': array([18.29247378, 22.14731303,  0.27159436]), 'targetState': array([19, 22], dtype=int32), 'currentDistance': 0.7226994423543299}
episode index:2107
target Thresh 31.999999981900974
target distance 8.0
model initialize at round 2107
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 29.]), 'previousTarget': array([ 4., 29.]), 'currentState': array([ 2.85554742, 21.46748317,  1.80356821]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 7.618961972522054}
done in step count: 15
reward sum = 0.7275268776547861
running average episode reward sum: 0.4765588828401151
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 29.]), 'previousTarget': array([ 4., 29.]), 'currentState': array([ 3.62665091, 28.31721712,  1.37897991]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 0.7781914977842457}
episode index:2108
target Thresh 31.99999998208106
target distance 2.0
model initialize at round 2108
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 29.]), 'previousTarget': array([ 5., 29.]), 'currentState': array([ 3.27437353, 27.01472638,  0.26465043]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 2.6304178491426793}
done in step count: 4
reward sum = 0.9143052108562663
running average episode reward sum: 0.4767664439249971
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 29.]), 'previousTarget': array([ 5., 29.]), 'currentState': array([ 4.52417609, 28.17174016,  0.97084115]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 0.9552082287287994}
episode index:2109
target Thresh 31.999999982259357
target distance 8.0
model initialize at round 2109
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 12.]), 'previousTarget': array([25., 12.]), 'currentState': array([16.90346237, 12.02030371,  2.6900543 ]), 'targetState': array([25, 12], dtype=int32), 'currentDistance': 8.096563085787363}
done in step count: 20
reward sum = 0.6377369081301528
running average episode reward sum: 0.47684273324452564
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 12.]), 'previousTarget': array([25., 12.]), 'currentState': array([24.09529313, 11.97651344,  6.11733267]), 'targetState': array([25, 12], dtype=int32), 'currentDistance': 0.9050116830679398}
episode index:2110
target Thresh 31.99999998243588
target distance 5.0
model initialize at round 2110
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 20.]), 'previousTarget': array([ 8., 20.]), 'currentState': array([ 4.92588092, 15.06309922,  2.18383145]), 'targetState': array([ 8, 20], dtype=int32), 'currentDistance': 5.815771436685034}
done in step count: 11
reward sum = 0.7776328807218259
running average episode reward sum: 0.47698522028738555
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 20.]), 'previousTarget': array([ 8., 20.]), 'currentState': array([ 7.43608633, 19.17815458,  0.90631743]), 'targetState': array([ 8, 20], dtype=int32), 'currentDistance': 0.9967088437528363}
episode index:2111
target Thresh 31.999999982610646
target distance 4.0
model initialize at round 2111
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 27.]), 'previousTarget': array([10., 27.]), 'currentState': array([ 6.4385138 , 26.23840313,  0.39471292]), 'targetState': array([10, 27], dtype=int32), 'currentDistance': 3.6420068268735006}
done in step count: 6
reward sum = 0.8822658426844538
running average episode reward sum: 0.4771771145214751
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 27.]), 'previousTarget': array([10., 27.]), 'currentState': array([ 9.05146326, 26.71994408,  0.20546462]), 'targetState': array([10, 27], dtype=int32), 'currentDistance': 0.9890163083416299}
episode index:2112
target Thresh 31.999999982783674
target distance 20.0
model initialize at round 2112
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.86588292, 18.37929463]), 'previousTarget': array([19.86588292, 18.37929463]), 'currentState': array([5.        , 5.        , 1.92760431]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 87
reward sum = 0.1871908049306142
running average episode reward sum: 0.4770398753782707
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 23.]), 'previousTarget': array([25., 23.]), 'currentState': array([24.19425653, 22.69032121,  0.5915534 ]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 0.863205353636305}
episode index:2113
target Thresh 31.99999998295498
target distance 14.0
model initialize at round 2113
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  8.]), 'previousTarget': array([21.,  8.]), 'currentState': array([7.30007196, 8.95984247, 6.14934197]), 'targetState': array([21,  8], dtype=int32), 'currentDistance': 13.733511058640056}
done in step count: 33
reward sum = 0.5364904486966048
running average episode reward sum: 0.4770679976929908
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  8.]), 'previousTarget': array([21.,  8.]), 'currentState': array([20.28100101,  8.17210458,  0.0709354 ]), 'targetState': array([21,  8], dtype=int32), 'currentDistance': 0.7393101782064087}
episode index:2114
target Thresh 31.999999983124578
target distance 21.0
model initialize at round 2114
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2.88866812, 16.08915939]), 'previousTarget': array([ 3.09009055, 16.10381815]), 'currentState': array([22.78876216, 18.08572065,  2.81381507]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 61
reward sum = 0.33481369270293276
running average episode reward sum: 0.4770007379743194
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 16.]), 'previousTarget': array([ 2., 16.]), 'currentState': array([ 2.70098791, 15.92604023,  3.35764449]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 0.7048787813954933}
episode index:2115
target Thresh 31.999999983292494
target distance 15.0
model initialize at round 2115
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'previousTarget': array([ 2., 17.]), 'currentState': array([2.00484251, 2.36631222, 1.51682547]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 14.633688584369365}
done in step count: 35
reward sum = 0.5081215206772068
running average episode reward sum: 0.4770154453385457
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'previousTarget': array([ 2., 17.]), 'currentState': array([ 2.17546806, 16.27129888,  1.59664181]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 0.7495294271882322}
episode index:2116
target Thresh 31.999999983458736
target distance 17.0
model initialize at round 2116
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 11.]), 'previousTarget': array([24., 11.]), 'currentState': array([ 7.18958433, 13.0308887 ,  0.04353058]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 16.93264846428755}
done in step count: 46
reward sum = 0.44177082077774166
running average episode reward sum: 0.4769987969566086
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 11.]), 'previousTarget': array([24., 11.]), 'currentState': array([23.08720322, 11.16064558,  0.05176129]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 0.9268252039002264}
episode index:2117
target Thresh 31.999999983623322
target distance 8.0
model initialize at round 2117
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 25.]), 'previousTarget': array([19., 25.]), 'currentState': array([26.62812766, 26.06984114,  3.08574897]), 'targetState': array([19, 25], dtype=int32), 'currentDistance': 7.702784667791323}
done in step count: 15
reward sum = 0.7301927246777096
running average episode reward sum: 0.477118340831831
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 25.]), 'previousTarget': array([19., 25.]), 'currentState': array([19.8278272 , 24.93661292,  3.45758586]), 'targetState': array([19, 25], dtype=int32), 'currentDistance': 0.8302504433709794}
episode index:2118
target Thresh 31.999999983786275
target distance 7.0
model initialize at round 2118
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 28.]), 'previousTarget': array([16., 28.]), 'currentState': array([19.98833046, 20.98485425,  3.81632885]), 'targetState': array([16, 28], dtype=int32), 'currentDistance': 8.069637517498055}
done in step count: 19
reward sum = 0.675121193876945
running average episode reward sum: 0.4772117824802714
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 28.]), 'previousTarget': array([16., 28.]), 'currentState': array([16.33324485, 27.14936087,  2.21666508]), 'targetState': array([16, 28], dtype=int32), 'currentDistance': 0.9135858274001422}
episode index:2119
target Thresh 31.999999983947603
target distance 18.0
model initialize at round 2119
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.73247066, 13.45973695]), 'previousTarget': array([17.73247066, 13.45973695]), 'currentState': array([4.00000000e+00, 2.80000000e+01, 2.26689875e-02]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 80
reward sum = 0.24654950991047886
running average episode reward sum: 0.47710297952151204
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([20.46347713, 10.87371574,  5.49144961]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 1.0252979968642522}
episode index:2120
target Thresh 31.999999984107326
target distance 13.0
model initialize at round 2120
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 14.]), 'previousTarget': array([19., 14.]), 'currentState': array([ 6.18230678, 25.91847594,  5.77305104]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 17.50266631611691}
done in step count: 45
reward sum = 0.43595552435916696
running average episode reward sum: 0.47708357949550434
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 14.]), 'previousTarget': array([19., 14.]), 'currentState': array([18.17667293, 14.91487458,  5.8353007 ]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 1.2307976889441816}
episode index:2121
target Thresh 31.99999998426546
target distance 2.0
model initialize at round 2121
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([16.76781867, 20.10460176,  2.94699225]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 2.748550783968324}
done in step count: 6
reward sum = 0.8880984250281514
running average episode reward sum: 0.4772772716941531
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([15.56125816, 18.8362229 ,  4.47124074]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 1.0071144211676906}
episode index:2122
target Thresh 31.999999984422022
target distance 11.0
model initialize at round 2122
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([3.27568322, 2.14674159, 0.62670264]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 15.25792215812587}
done in step count: 39
reward sum = 0.4952096510165158
running average episode reward sum: 0.4772857184107439
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([13.19358835, 12.2266524 ,  0.94806199]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 1.1173031178520607}
episode index:2123
target Thresh 31.999999984577027
target distance 19.0
model initialize at round 2123
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 24.]), 'previousTarget': array([13., 24.]), 'currentState': array([16.        ,  5.        ,  3.60601836]), 'targetState': array([13, 24], dtype=int32), 'currentDistance': 19.235384061671343}
done in step count: 99
reward sum = -0.04502410268359192
running average episode reward sum: 0.4770398098320743
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 24.]), 'previousTarget': array([13., 24.]), 'currentState': array([14.14652096,  9.05015198,  1.61797012]), 'targetState': array([13, 24], dtype=int32), 'currentDistance': 14.993747565274743}
episode index:2124
target Thresh 31.999999984730486
target distance 4.0
model initialize at round 2124
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 12.]), 'previousTarget': array([20., 12.]), 'currentState': array([24.01380724, 16.03113584,  1.25798132]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 5.688646999748163}
done in step count: 16
reward sum = 0.7410525598553044
running average episode reward sum: 0.4771640511262029
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 12.]), 'previousTarget': array([20., 12.]), 'currentState': array([20.79215046, 12.91271989,  4.29637316]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 1.208536284811043}
episode index:2125
target Thresh 31.99999998488242
target distance 7.0
model initialize at round 2125
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 16.]), 'previousTarget': array([ 9., 16.]), 'currentState': array([10.91898154, 22.93708961,  4.0157336 ]), 'targetState': array([ 9, 16], dtype=int32), 'currentDistance': 7.197617827457693}
done in step count: 14
reward sum = 0.7519501872157995
running average episode reward sum: 0.47729330142539844
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 16.]), 'previousTarget': array([ 9., 16.]), 'currentState': array([ 9.14727058, 16.98957813,  4.5783686 ]), 'targetState': array([ 9, 16], dtype=int32), 'currentDistance': 1.0004766367565285}
episode index:2126
target Thresh 31.999999985032844
target distance 8.0
model initialize at round 2126
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 24.]), 'previousTarget': array([18., 24.]), 'currentState': array([ 9.9921331 , 17.00317536,  2.50576249]), 'targetState': array([18, 24], dtype=int32), 'currentDistance': 10.633977960258726}
done in step count: 23
reward sum = 0.5972068115508556
running average episode reward sum: 0.47734967825197366
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 24.]), 'previousTarget': array([18., 24.]), 'currentState': array([17.06916706, 23.50040038,  0.85154994]), 'targetState': array([18, 24], dtype=int32), 'currentDistance': 1.0564325564759836}
episode index:2127
target Thresh 31.99999998518177
target distance 16.0
model initialize at round 2127
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 28.]), 'previousTarget': array([ 5., 28.]), 'currentState': array([ 5.97575847, 11.99395951,  3.17284897]), 'targetState': array([ 5, 28], dtype=int32), 'currentDistance': 16.03575494449069}
done in step count: 38
reward sum = 0.4401022788423671
running average episode reward sum: 0.4773321747748075
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 28.]), 'previousTarget': array([ 5., 28.]), 'currentState': array([ 4.06858067, 27.02524845,  1.13701125]), 'targetState': array([ 5, 28], dtype=int32), 'currentDistance': 1.3482145756515123}
episode index:2128
target Thresh 31.999999985329215
target distance 23.0
model initialize at round 2128
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.4157244 , 16.92135594]), 'previousTarget': array([21.35234545, 16.95156206]), 'currentState': array([ 2.04545531, 21.90058034,  5.25845604]), 'targetState': array([25, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.2691696624062352
running average episode reward sum: 0.47723439999210737
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 16.]), 'previousTarget': array([25., 16.]), 'currentState': array([24.1551073 , 15.80510926,  6.14608838]), 'targetState': array([25, 16], dtype=int32), 'currentDistance': 0.8670790462574084}
episode index:2129
target Thresh 31.999999985475192
target distance 23.0
model initialize at round 2129
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7.45647272, 20.75140711]), 'previousTarget': array([ 7.45647272, 20.75140711]), 'currentState': array([27.        , 25.        ,  4.69707918]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.25803304153096995
running average episode reward sum: 0.4771314885562101
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 20.]), 'previousTarget': array([ 4., 20.]), 'currentState': array([ 4.8951285 , 20.32091188,  3.62892419]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 0.9509150721153912}
episode index:2130
target Thresh 31.999999985619716
target distance 11.0
model initialize at round 2130
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([24.94502963, 16.81666589,  4.27673143]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 12.894208295150104}
done in step count: 34
reward sum = 0.5487383419337842
running average episode reward sum: 0.47716509102142723
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.457703  , 10.98381318,  3.78545176]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.0850716163700724}
episode index:2131
target Thresh 31.999999985762802
target distance 12.0
model initialize at round 2131
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 15.]), 'previousTarget': array([ 9., 15.]), 'currentState': array([20.        , 27.        ,  3.23916344]), 'targetState': array([ 9, 15], dtype=int32), 'currentDistance': 16.278820596099706}
done in step count: 42
reward sum = 0.457971266281996
running average episode reward sum: 0.4771560882893731
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 15.]), 'previousTarget': array([ 9., 15.]), 'currentState': array([ 9.33693189, 15.6864605 ,  4.12303429]), 'targetState': array([ 9, 15], dtype=int32), 'currentDistance': 0.7646902103491617}
episode index:2132
target Thresh 31.999999985904463
target distance 13.0
model initialize at round 2132
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 15.]), 'previousTarget': array([ 2., 15.]), 'currentState': array([ 1.8933024 , 27.85451377,  4.22778571]), 'targetState': array([ 2, 15], dtype=int32), 'currentDistance': 12.854956580824354}
done in step count: 30
reward sum = 0.5617898673424587
running average episode reward sum: 0.47719576657303603
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 15.]), 'previousTarget': array([ 2., 15.]), 'currentState': array([ 2.45284957, 15.84056461,  4.82891901]), 'targetState': array([ 2, 15], dtype=int32), 'currentDistance': 0.9547887719645284}
episode index:2133
target Thresh 31.999999986044717
target distance 7.0
model initialize at round 2133
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 23.]), 'previousTarget': array([15., 23.]), 'currentState': array([18.69071295, 16.22224578,  2.48768455]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 7.717468134042602}
done in step count: 16
reward sum = 0.7301285581467855
running average episode reward sum: 0.4773142917799591
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 23.]), 'previousTarget': array([15., 23.]), 'currentState': array([15.40098036, 22.22218906,  2.0081652 ]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 0.8750857754451149}
episode index:2134
target Thresh 31.999999986183575
target distance 21.0
model initialize at round 2134
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  7.]), 'previousTarget': array([24.,  7.]), 'currentState': array([4.        , 7.        , 1.20519575]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.32464036323941087
running average episode reward sum: 0.4772427817431719
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([24.17998381,  6.95009012,  6.25965522]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 0.8215336542984034}
episode index:2135
target Thresh 31.99999998632105
target distance 24.0
model initialize at round 2135
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.46153846, 18.69230769]), 'previousTarget': array([20.46153846, 18.69230769]), 'currentState': array([ 2.        , 11.        ,  4.57159758]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 97
reward sum = 0.15890010536830484
running average episode reward sum: 0.4770937449096631
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([25.24676902, 20.4067765 ,  0.64219813]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 0.9587862251965181}
episode index:2136
target Thresh 31.99999998645716
target distance 8.0
model initialize at round 2136
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 17.]), 'previousTarget': array([24., 17.]), 'currentState': array([19.93228239,  8.98279386,  3.15036753]), 'targetState': array([24, 17], dtype=int32), 'currentDistance': 8.990101273819166}
done in step count: 21
reward sum = 0.6398255079493771
running average episode reward sum: 0.4771698945414084
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 17.]), 'previousTarget': array([24., 17.]), 'currentState': array([23.42563162, 16.10741866,  1.15725462]), 'targetState': array([24, 17], dtype=int32), 'currentDistance': 1.0614143855247011}
episode index:2137
target Thresh 31.99999998659191
target distance 9.0
model initialize at round 2137
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([26.00405261, 17.99494252,  5.18589193]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 9.492277589048568}
done in step count: 25
reward sum = 0.6017386216266047
running average episode reward sum: 0.47722815867942764
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([17.65987157, 20.85226091,  2.7639652 ]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 0.6762080471620324}
episode index:2138
target Thresh 31.999999986725324
target distance 15.0
model initialize at round 2138
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 26.]), 'previousTarget': array([13., 26.]), 'currentState': array([17.8642552 , 11.35195967,  1.96405004]), 'targetState': array([13, 26], dtype=int32), 'currentDistance': 15.434573663092987}
done in step count: 35
reward sum = 0.48838745109738435
running average episode reward sum: 0.47723337573993163
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 26.]), 'previousTarget': array([13., 26.]), 'currentState': array([13.43328231, 25.35306082,  1.89502745]), 'targetState': array([13, 26], dtype=int32), 'currentDistance': 0.778629475315107}
episode index:2139
target Thresh 31.999999986857407
target distance 14.0
model initialize at round 2139
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([18.04787667, 21.95365903,  5.42340989]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 14.103134371016825}
done in step count: 40
reward sum = 0.49756008996150114
running average episode reward sum: 0.47724287420452116
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.08633384,  8.7862969 ,  4.38768433]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.7910223431643516}
episode index:2140
target Thresh 31.99999998698818
target distance 6.0
model initialize at round 2140
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([12.1624874 , 23.56202566,  5.04845138]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 6.24400569717184}
done in step count: 11
reward sum = 0.7991155792829276
running average episode reward sum: 0.47739321175943866
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([14.09379996, 18.95267689,  5.24023374]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 1.3148352636462186}
episode index:2141
target Thresh 31.99999998711765
target distance 14.0
model initialize at round 2141
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 13.]), 'previousTarget': array([23., 13.]), 'currentState': array([8.97705277, 2.00781539, 2.56083345]), 'targetState': array([23, 13], dtype=int32), 'currentDistance': 17.81772071697212}
done in step count: 55
reward sum = 0.3574140745763072
running average episode reward sum: 0.47733719909035227
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 13.]), 'previousTarget': array([23., 13.]), 'currentState': array([22.34513699, 12.4062222 ,  0.63965417]), 'targetState': array([23, 13], dtype=int32), 'currentDistance': 0.8839783026711393}
episode index:2142
target Thresh 31.999999987245833
target distance 4.0
model initialize at round 2142
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 13.]), 'previousTarget': array([27., 13.]), 'currentState': array([27.1682937 ,  9.1997455 ,  1.05658793]), 'targetState': array([27, 13], dtype=int32), 'currentDistance': 3.8039791020351426}
done in step count: 8
reward sum = 0.8623237286076151
running average episode reward sum: 0.4775168474942334
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 13.]), 'previousTarget': array([27., 13.]), 'currentState': array([27.37935585, 12.31015022,  1.64619423]), 'targetState': array([27, 13], dtype=int32), 'currentDistance': 0.7872760520690463}
episode index:2143
target Thresh 31.99999998737274
target distance 7.0
model initialize at round 2143
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([15.0049201 , 25.00876935,  0.80700421]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 9.221521128331295}
done in step count: 24
reward sum = 0.6422550463019017
running average episode reward sum: 0.4775936843406922
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([21.22584141, 19.513311  ,  5.45324587]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 0.928875506373315}
episode index:2144
target Thresh 31.99999998749838
target distance 8.0
model initialize at round 2144
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([23.        ,  4.        ,  1.21948531]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 24
reward sum = 0.6541871515918127
running average episode reward sum: 0.47767601229745266
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.73381436,  3.01358043,  3.45844891]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.7339400147458628}
episode index:2145
target Thresh 31.999999987622775
target distance 12.0
model initialize at round 2145
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 14.]), 'previousTarget': array([ 9., 14.]), 'currentState': array([21.02158491, 21.97254505,  5.16318813]), 'targetState': array([ 9, 14], dtype=int32), 'currentDistance': 14.424977590427636}
done in step count: 37
reward sum = 0.48997995415345963
running average episode reward sum: 0.4776817457279541
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 14.]), 'previousTarget': array([ 9., 14.]), 'currentState': array([ 9.63019754, 14.89211371,  3.93232512]), 'targetState': array([ 9, 14], dtype=int32), 'currentDistance': 1.0922526362166174}
episode index:2146
target Thresh 31.99999998774593
target distance 18.0
model initialize at round 2146
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([19.013743  ,  3.99212644,  6.0073688 ]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 18.12011785472954}
done in step count: 46
reward sum = 0.368690308213516
running average episode reward sum: 0.4776309812018644
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([17.07538739, 21.29646432,  1.90038585]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 0.7075632221252399}
episode index:2147
target Thresh 31.99999998786786
target distance 14.0
model initialize at round 2147
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 20.]), 'previousTarget': array([10., 20.]), 'currentState': array([24.02372448, 22.99976573,  6.03162966]), 'targetState': array([10, 20], dtype=int32), 'currentDistance': 14.34097077408868}
done in step count: 43
reward sum = 0.44835381111828637
running average episode reward sum: 0.47761735123441396
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 20.]), 'previousTarget': array([10., 20.]), 'currentState': array([10.80294453, 19.96859571,  3.20583361]), 'targetState': array([10, 20], dtype=int32), 'currentDistance': 0.803558427008999}
episode index:2148
target Thresh 31.999999987988577
target distance 19.0
model initialize at round 2148
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5.57698398, 18.09892409]), 'previousTarget': array([ 5.69147429, 18.02072541]), 'currentState': array([22.94212092,  8.17662501,  1.92314989]), 'targetState': array([ 4, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.32328761100665154
running average episode reward sum: 0.47754553655771426
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 19.]), 'previousTarget': array([ 4., 19.]), 'currentState': array([ 4.96388344, 18.55408001,  2.73475761]), 'targetState': array([ 4, 19], dtype=int32), 'currentDistance': 1.0620338593401981}
episode index:2149
target Thresh 31.99999998810809
target distance 10.0
model initialize at round 2149
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([12.00886285, 15.97435235,  5.27516013]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 10.439222446847436}
done in step count: 25
reward sum = 0.6140537507645578
running average episode reward sum: 0.47760902875036854
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([21.11431683, 18.73042315,  0.52547643]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 0.9258003842423752}
episode index:2150
target Thresh 31.999999988226417
target distance 4.0
model initialize at round 2150
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 15.]), 'previousTarget': array([17., 15.]), 'currentState': array([20.71200252, 16.06088757,  3.08204612]), 'targetState': array([17, 15], dtype=int32), 'currentDistance': 3.8606275533754277}
done in step count: 7
reward sum = 0.8744296910676976
running average episode reward sum: 0.47779351069472803
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 15.]), 'previousTarget': array([17., 15.]), 'currentState': array([17.8995974 , 15.32177346,  3.46179633]), 'targetState': array([17, 15], dtype=int32), 'currentDistance': 0.9554128151641592}
episode index:2151
target Thresh 31.999999988343568
target distance 13.0
model initialize at round 2151
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  8.]), 'previousTarget': array([17.,  8.]), 'currentState': array([4.23005758, 6.95145623, 6.11861476]), 'targetState': array([17,  8], dtype=int32), 'currentDistance': 12.812918223014247}
done in step count: 28
reward sum = 0.5737921891847013
running average episode reward sum: 0.47783811974607104
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  8.]), 'previousTarget': array([17.,  8.]), 'currentState': array([16.05112112,  8.13672751,  0.18215064]), 'targetState': array([17,  8], dtype=int32), 'currentDistance': 0.9586790603193764}
episode index:2152
target Thresh 31.99999998845955
target distance 13.0
model initialize at round 2152
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  6.]), 'previousTarget': array([18.,  6.]), 'currentState': array([4.99066992, 4.95500141, 4.76044416]), 'targetState': array([18,  6], dtype=int32), 'currentDistance': 13.051233323737943}
done in step count: 34
reward sum = 0.524407417440304
running average episode reward sum: 0.4778597497031979
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  6.]), 'previousTarget': array([18.,  6.]), 'currentState': array([17.05556165,  5.65778076,  0.32624734]), 'targetState': array([18,  6], dtype=int32), 'currentDistance': 1.0045286501215853}
episode index:2153
target Thresh 31.99999998857438
target distance 3.0
model initialize at round 2153
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([14.       , 23.       ,  2.3259403]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 4.242640687119284}
done in step count: 17
reward sum = 0.7551854160232195
running average episode reward sum: 0.4779884988519073
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([17.01707821, 20.97672864,  5.36397724]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 0.9768779362842648}
episode index:2154
target Thresh 31.999999988688067
target distance 11.0
model initialize at round 2154
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 27.]), 'previousTarget': array([26., 27.]), 'currentState': array([15.05449722, 16.98405161,  6.25098896]), 'targetState': array([26, 27], dtype=int32), 'currentDistance': 14.83655125886318}
done in step count: 36
reward sum = 0.4930683698644471
running average episode reward sum: 0.47799549647186673
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 27.]), 'previousTarget': array([26., 27.]), 'currentState': array([25.39722343, 26.06817213,  0.64319734]), 'targetState': array([26, 27], dtype=int32), 'currentDistance': 1.109794024979988}
episode index:2155
target Thresh 31.999999988800624
target distance 19.0
model initialize at round 2155
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 28.]), 'previousTarget': array([ 9., 28.]), 'currentState': array([13.83370469,  9.11563305,  2.42232209]), 'targetState': array([ 9, 28], dtype=int32), 'currentDistance': 19.493178707403246}
done in step count: 55
reward sum = 0.3701859786905469
running average episode reward sum: 0.47794549205731146
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 28.]), 'previousTarget': array([ 9., 28.]), 'currentState': array([ 9.35231713, 27.04440427,  1.60110026]), 'targetState': array([ 9, 28], dtype=int32), 'currentDistance': 1.0184746249337215}
episode index:2156
target Thresh 31.99999998891206
target distance 4.0
model initialize at round 2156
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 5.]), 'previousTarget': array([8., 5.]), 'currentState': array([8.99886559, 8.58068025, 4.57738537]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 3.7173920285919526}
done in step count: 7
reward sum = 0.8759118780221355
running average episode reward sum: 0.47812999200444395
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 5.]), 'previousTarget': array([8., 5.]), 'currentState': array([8.16905661, 5.86006536, 4.47137963]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 0.8765229906795607}
episode index:2157
target Thresh 31.999999989022385
target distance 23.0
model initialize at round 2157
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4.96944871, 18.72750061]), 'previousTarget': array([ 5.07518824, 18.73259233]), 'currentState': array([24.88576384, 16.89982677,  3.73222175]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.2896992608597252
running average episode reward sum: 0.4780426747054891
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 19.]), 'previousTarget': array([ 2., 19.]), 'currentState': array([ 2.69815458, 18.7980662 ,  3.06547972]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 0.7267716803799948}
episode index:2158
target Thresh 31.999999989131613
target distance 11.0
model initialize at round 2158
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 5.]), 'previousTarget': array([8., 5.]), 'currentState': array([15.0156131 , 15.80450781,  4.66844103]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 12.88239947180242}
done in step count: 32
reward sum = 0.5618482933387503
running average episode reward sum: 0.47808149157377683
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 5.]), 'previousTarget': array([8., 5.]), 'currentState': array([8.34052547, 5.95310294, 4.0206205 ]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 1.0121080993664926}
episode index:2159
target Thresh 31.999999989239758
target distance 11.0
model initialize at round 2159
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 28.]), 'previousTarget': array([17., 28.]), 'currentState': array([23.8872153 , 17.30563289,  1.92542487]), 'targetState': array([17, 28], dtype=int32), 'currentDistance': 12.72018956311865}
done in step count: 29
reward sum = 0.5798915543375605
running average episode reward sum: 0.4781286258620934
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 28.]), 'previousTarget': array([17., 28.]), 'currentState': array([17.55034473, 27.03001207,  2.28598737]), 'targetState': array([17, 28], dtype=int32), 'currentDistance': 1.115238051312282}
episode index:2160
target Thresh 31.999999989346822
target distance 10.0
model initialize at round 2160
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([17.89248584, 21.92987728,  3.95004541]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 11.657795331272732}
done in step count: 28
reward sum = 0.5659830797679752
running average episode reward sum: 0.4781692803988383
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([24.0034933 , 12.77979041,  5.15105757]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 0.779798230752113}
episode index:2161
target Thresh 31.999999989452824
target distance 4.0
model initialize at round 2161
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([ 5.        , 21.        ,  2.29172772]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 4.123105625617661}
done in step count: 9
reward sum = 0.8427983113428483
running average episode reward sum: 0.4783379339746681
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([ 5.63424826, 24.21023877,  1.03140876]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 0.8703431133294981}
episode index:2162
target Thresh 31.999999989557768
target distance 9.0
model initialize at round 2162
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'previousTarget': array([26., 11.]), 'currentState': array([16.99224254, 19.0100528 ,  1.97552347]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 12.05407152761259}
done in step count: 34
reward sum = 0.5104182407756899
running average episode reward sum: 0.47835276536939814
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'previousTarget': array([26., 11.]), 'currentState': array([25.23641647, 11.33805773,  5.33417634]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 0.8350705598080784}
episode index:2163
target Thresh 31.99999998966167
target distance 5.0
model initialize at round 2163
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  4.]), 'previousTarget': array([13.,  4.]), 'currentState': array([7.97950912, 6.08664323, 1.55052662]), 'targetState': array([13,  4], dtype=int32), 'currentDistance': 5.436856504025816}
done in step count: 12
reward sum = 0.7668723235250516
running average episode reward sum: 0.4784860923371226
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  4.]), 'previousTarget': array([13.,  4.]), 'currentState': array([12.04648519,  4.51466099,  5.56661469]), 'targetState': array([13,  4], dtype=int32), 'currentDistance': 1.0835434576047436}
episode index:2164
target Thresh 31.99999998976454
target distance 19.0
model initialize at round 2164
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9.39548895, 25.37061688]), 'previousTarget': array([ 9.49385478, 25.29367831]), 'currentState': array([25.9325723 , 14.12228926,  2.21762699]), 'targetState': array([ 7, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.29530276203314193
running average episode reward sum: 0.47840148109910685
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 27.]), 'previousTarget': array([ 7., 27.]), 'currentState': array([ 7.85710169, 26.79129675,  2.53856177]), 'targetState': array([ 7, 27], dtype=int32), 'currentDistance': 0.8821453084999916}
episode index:2165
target Thresh 31.999999989866385
target distance 5.0
model initialize at round 2165
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 22.]), 'previousTarget': array([23., 22.]), 'currentState': array([21.97268338, 27.00126912,  3.3476665 ]), 'targetState': array([23, 22], dtype=int32), 'currentDistance': 5.105690181676787}
done in step count: 13
reward sum = 0.7732091118744705
running average episode reward sum: 0.478537588038523
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 22.]), 'previousTarget': array([23., 22.]), 'currentState': array([22.65791076, 22.68773462,  5.13348353]), 'targetState': array([23, 22], dtype=int32), 'currentDistance': 0.768117150816778}
episode index:2166
target Thresh 31.999999989967215
target distance 16.0
model initialize at round 2166
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.05153389, 20.17009216]), 'previousTarget': array([25.05153389, 20.17009216]), 'currentState': array([10.       ,  7.       ,  3.1617527]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.28288233952361147
running average episode reward sum: 0.47844729950667486
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([25.61442309, 20.06888957,  0.59921934]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 1.007787765336968}
episode index:2167
target Thresh 31.999999990067042
target distance 5.0
model initialize at round 2167
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([10.03026204, 15.01166321,  0.11536348]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 5.3848524362176855}
done in step count: 12
reward sum = 0.7750640938427757
running average episode reward sum: 0.47858411537122103
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([11.8827789 , 10.58025223,  4.79078796]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.5919741866293391}
episode index:2168
target Thresh 31.99999999016588
target distance 17.0
model initialize at round 2168
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([24.01882002, 26.98785166,  5.64743355]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 17.253996009074527}
done in step count: 42
reward sum = 0.4254971058263121
running average episode reward sum: 0.47855964003256496
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([21.02215082, 10.62928511,  4.49588288]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 0.6296748447760329}
episode index:2169
target Thresh 31.999999990263728
target distance 6.0
model initialize at round 2169
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 13.]), 'previousTarget': array([21., 13.]), 'currentState': array([27.01422134, 16.96959172,  4.89734793]), 'targetState': array([21, 13], dtype=int32), 'currentDistance': 7.2061443781102446}
done in step count: 14
reward sum = 0.7391479418537512
running average episode reward sum: 0.4786797268075978
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 13.]), 'previousTarget': array([21., 13.]), 'currentState': array([21.92118917, 13.96408932,  3.60024486]), 'targetState': array([21, 13], dtype=int32), 'currentDistance': 1.3334383042414535}
episode index:2170
target Thresh 31.999999990360607
target distance 18.0
model initialize at round 2170
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.36219535, 12.20093515]), 'previousTarget': array([20.36442559, 12.19631201]), 'currentState': array([ 5.00568813, 25.01411923,  0.93532324]), 'targetState': array([23, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.24827123215646363
running average episode reward sum: 0.47857359668569494
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 10.]), 'previousTarget': array([23., 10.]), 'currentState': array([22.1214566 , 10.39381318,  5.6261658 ]), 'targetState': array([23, 10], dtype=int32), 'currentDistance': 0.9627706500117238}
episode index:2171
target Thresh 31.99999999045652
target distance 19.0
model initialize at round 2171
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  6.]), 'previousTarget': array([20.,  6.]), 'currentState': array([16.10240301, 25.03026509,  0.03486818]), 'targetState': array([20,  6], dtype=int32), 'currentDistance': 19.425299270003308}
done in step count: 55
reward sum = 0.34067695938788756
running average episode reward sum: 0.47851010836281377
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  6.]), 'previousTarget': array([20.,  6.]), 'currentState': array([20.04070008,  6.72087021,  5.07230568]), 'targetState': array([20,  6], dtype=int32), 'currentDistance': 0.7220182508798644}
episode index:2172
target Thresh 31.99999999055148
target distance 11.0
model initialize at round 2172
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  5.]), 'previousTarget': array([27.,  5.]), 'currentState': array([22.99514475, 15.99495719,  4.18186387]), 'targetState': array([27,  5], dtype=int32), 'currentDistance': 11.701621652681276}
done in step count: 28
reward sum = 0.5829285073283782
running average episode reward sum: 0.4785581610084491
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  5.]), 'previousTarget': array([27.,  5.]), 'currentState': array([26.56518883,  5.72951137,  5.09050661]), 'targetState': array([27,  5], dtype=int32), 'currentDistance': 0.8492629721589072}
episode index:2173
target Thresh 31.999999990645495
target distance 14.0
model initialize at round 2173
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 20.]), 'previousTarget': array([13., 20.]), 'currentState': array([26.86369038, 10.27475711,  2.15843418]), 'targetState': array([13, 20], dtype=int32), 'currentDistance': 16.934646737919053}
done in step count: 41
reward sum = 0.4363137400096492
running average episode reward sum: 0.4785387293520559
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 20.]), 'previousTarget': array([13., 20.]), 'currentState': array([13.77885857, 19.9433726 ,  2.38720405]), 'targetState': array([13, 20], dtype=int32), 'currentDistance': 0.7809144187994148}
episode index:2174
target Thresh 31.999999990738573
target distance 12.0
model initialize at round 2174
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([17.91921043,  2.18151498,  2.12187842]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 13.73169018319236}
done in step count: 29
reward sum = 0.5327427243588198
running average episode reward sum: 0.4785636507290705
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.89149882, 9.14735192, 2.81418658]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.903594337752506}
episode index:2175
target Thresh 31.999999990830727
target distance 21.0
model initialize at round 2175
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.36734329, 27.44026363]), 'previousTarget': array([13.36758945, 27.45612429]), 'currentState': array([17.95222764,  7.97288488,  3.40534043]), 'targetState': array([13, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.2992598566761672
running average episode reward sum: 0.47848125008842113
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 29.]), 'previousTarget': array([13., 29.]), 'currentState': array([13.13108997, 28.04734204,  1.71348665]), 'targetState': array([13, 29], dtype=int32), 'currentDistance': 0.9616349480190172}
episode index:2176
target Thresh 31.99999999092196
target distance 14.0
model initialize at round 2176
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([26.99629448, 19.97323177,  4.343539  ]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 14.87160576765895}
done in step count: 34
reward sum = 0.47646538845133646
running average episode reward sum: 0.47848032410696173
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([13.98411534, 24.98932065,  2.85796746]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 0.9841732848751115}
episode index:2177
target Thresh 31.999999991012288
target distance 16.0
model initialize at round 2177
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7.19710024, 16.84028244]), 'previousTarget': array([ 7.47772  , 16.6118525]), 'currentState': array([22.73581832,  4.2486884 ,  2.42264315]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.3576178158673151
running average episode reward sum: 0.4784248316789362
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 17.]), 'previousTarget': array([ 7., 17.]), 'currentState': array([ 7.83343743, 16.72841474,  2.45223841]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 0.8765708737487012}
episode index:2178
target Thresh 31.999999991101717
target distance 3.0
model initialize at round 2178
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 21.]), 'previousTarget': array([ 8., 21.]), 'currentState': array([10.60753418, 23.83623187,  3.61607684]), 'targetState': array([ 8, 21], dtype=int32), 'currentDistance': 3.852719267521487}
done in step count: 7
reward sum = 0.8625789186731441
running average episode reward sum: 0.4786011300208335
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 21.]), 'previousTarget': array([ 8., 21.]), 'currentState': array([ 8.04401393, 21.76554481,  3.86373492]), 'targetState': array([ 8, 21], dtype=int32), 'currentDistance': 0.7668090279001414}
episode index:2179
target Thresh 31.999999991190258
target distance 16.0
model initialize at round 2179
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 28.]), 'previousTarget': array([27., 28.]), 'currentState': array([10.99538067, 24.00515896,  2.048563  ]), 'targetState': array([27, 28], dtype=int32), 'currentDistance': 16.495653811548202}
done in step count: 46
reward sum = 0.42065573119660843
running average episode reward sum: 0.4785745495626572
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 28.]), 'previousTarget': array([27., 28.]), 'currentState': array([26.03697119, 27.61760655,  0.10619491]), 'targetState': array([27, 28], dtype=int32), 'currentDistance': 1.036170470849984}
episode index:2180
target Thresh 31.999999991277917
target distance 11.0
model initialize at round 2180
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 23.]), 'previousTarget': array([13., 23.]), 'currentState': array([ 6.        , 12.        ,  1.91471806]), 'targetState': array([13, 23], dtype=int32), 'currentDistance': 13.038404810405298}
done in step count: 32
reward sum = 0.549614321633955
running average episode reward sum: 0.4786071216727312
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 23.]), 'previousTarget': array([13., 23.]), 'currentState': array([12.54597918, 22.09256538,  0.96363158]), 'targetState': array([13, 23], dtype=int32), 'currentDistance': 1.01467852136791}
episode index:2181
target Thresh 31.999999991364703
target distance 7.0
model initialize at round 2181
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 2.]), 'previousTarget': array([9., 2.]), 'currentState': array([10.99098935,  9.00804736,  2.64465526]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 7.28538031971528}
done in step count: 18
reward sum = 0.7044129571523435
running average episode reward sum: 0.47871060739018284
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 2.]), 'previousTarget': array([9., 2.]), 'currentState': array([9.19862727, 2.93010949, 4.65122976]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 0.9510817270363285}
episode index:2182
target Thresh 31.999999991450625
target distance 8.0
model initialize at round 2182
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([14.03199926, 21.0242829 ,  0.76455109]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 8.582499586403983}
done in step count: 24
reward sum = 0.6360659891277376
running average episode reward sum: 0.47878268956230263
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([ 6.97948059, 18.43670388,  3.53102885]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 1.0724236550381885}
episode index:2183
target Thresh 31.99999999153569
target distance 14.0
model initialize at round 2183
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 22.]), 'previousTarget': array([27., 22.]), 'currentState': array([13.        , 15.        ,  3.36765748]), 'targetState': array([27, 22], dtype=int32), 'currentDistance': 15.652475842498529}
done in step count: 47
reward sum = 0.39960114681933834
running average episode reward sum: 0.4787464342771639
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 22.]), 'previousTarget': array([27., 22.]), 'currentState': array([26.28695539, 21.82665418,  0.35464981]), 'targetState': array([27, 22], dtype=int32), 'currentDistance': 0.7338129088718061}
episode index:2184
target Thresh 31.999999991619912
target distance 14.0
model initialize at round 2184
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 27.]), 'previousTarget': array([15., 27.]), 'currentState': array([ 8.        , 13.        ,  3.38928294]), 'targetState': array([15, 27], dtype=int32), 'currentDistance': 15.65247584249853}
done in step count: 45
reward sum = 0.4237451741956984
running average episode reward sum: 0.47872126207575366
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 27.]), 'previousTarget': array([15., 27.]), 'currentState': array([14.54070032, 26.30752258,  1.32268522]), 'targetState': array([15, 27], dtype=int32), 'currentDistance': 0.8309519706252152}
episode index:2185
target Thresh 31.999999991703298
target distance 6.0
model initialize at round 2185
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  7.]), 'previousTarget': array([18.,  7.]), 'currentState': array([23.99200173,  8.96215548,  4.39107528]), 'targetState': array([18,  7], dtype=int32), 'currentDistance': 6.305088335069512}
done in step count: 14
reward sum = 0.7689069729035466
running average episode reward sum: 0.478854009427459
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  7.]), 'previousTarget': array([18.,  7.]), 'currentState': array([18.88344296,  6.96507556,  3.33973076]), 'targetState': array([18,  7], dtype=int32), 'currentDistance': 0.8841330153727055}
episode index:2186
target Thresh 31.999999991785852
target distance 17.0
model initialize at round 2186
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.43860471, 27.71414506]), 'previousTarget': array([25.43860471, 27.71414506]), 'currentState': array([10.        , 15.        ,  2.59796295]), 'targetState': array([27, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.28490604269059033
running average episode reward sum: 0.4787653272295912
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 29.]), 'previousTarget': array([27., 29.]), 'currentState': array([26.13206746, 28.33451593,  0.56730299]), 'targetState': array([27, 29], dtype=int32), 'currentDistance': 1.0936982810588274}
episode index:2187
target Thresh 31.999999991867583
target distance 20.0
model initialize at round 2187
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.00124766,  8.02495322]), 'previousTarget': array([19.00124766,  8.02495322]), 'currentState': array([20.        , 28.        ,  0.71647656]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.3124893588071294
running average episode reward sum: 0.4786893327284841
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  8.]), 'previousTarget': array([19.,  8.]), 'currentState': array([18.70370717,  8.97079607,  4.59767917]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 1.0150046563244097}
episode index:2188
target Thresh 31.999999991948503
target distance 9.0
model initialize at round 2188
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  5.]), 'previousTarget': array([22.,  5.]), 'currentState': array([13.        ,  6.        ,  4.29345292]), 'targetState': array([22,  5], dtype=int32), 'currentDistance': 9.055385138137417}
done in step count: 22
reward sum = 0.6408467770516659
running average episode reward sum: 0.47876341104932607
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  5.]), 'previousTarget': array([22.,  5.]), 'currentState': array([21.07201536,  4.66080855,  0.06834535]), 'targetState': array([22,  5], dtype=int32), 'currentDistance': 0.9880315431696567}
episode index:2189
target Thresh 31.999999992028616
target distance 22.0
model initialize at round 2189
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.94657813, 10.7552047 ]), 'previousTarget': array([16.73750984, 11.12677025]), 'currentState': array([ 6.14018107, 27.58439906,  5.07808796]), 'targetState': array([20,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.22419733484132176
running average episode reward sum: 0.4786471708318795
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  6.]), 'previousTarget': array([20.,  6.]), 'currentState': array([19.72110995,  6.78550215,  5.12899275]), 'targetState': array([20,  6], dtype=int32), 'currentDistance': 0.8335426131310523}
episode index:2190
target Thresh 31.99999999210793
target distance 10.0
model initialize at round 2190
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 29.]), 'previousTarget': array([23., 29.]), 'currentState': array([13.        , 22.        ,  3.09775996]), 'targetState': array([23, 29], dtype=int32), 'currentDistance': 12.206555615733702}
done in step count: 35
reward sum = 0.5187582316296461
running average episode reward sum: 0.4786654780253061
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 29.]), 'previousTarget': array([23., 29.]), 'currentState': array([22.27125762, 28.54492985,  0.48924455]), 'targetState': array([23, 29], dtype=int32), 'currentDistance': 0.859159064128156}
episode index:2191
target Thresh 31.99999999218646
target distance 12.0
model initialize at round 2191
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 28.]), 'previousTarget': array([ 8., 28.]), 'currentState': array([ 5.91708206, 16.24037918,  1.8204305 ]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 11.942664228803082}
done in step count: 27
reward sum = 0.5910154046227744
running average episode reward sum: 0.4787167325538634
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 28.]), 'previousTarget': array([ 8., 28.]), 'currentState': array([ 7.6239867 , 27.31077026,  1.45780498]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 0.7851265096209166}
episode index:2192
target Thresh 31.999999992264204
target distance 17.0
model initialize at round 2192
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 3.97164416, 26.65320892,  4.73644026]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 16.776280715569012}
done in step count: 39
reward sum = 0.4440041483352034
running average episode reward sum: 0.4787009037420901
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 5.99104543, 10.6591384 ,  5.01412562]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.6591992242619598}
episode index:2193
target Thresh 31.999999992341177
target distance 20.0
model initialize at round 2193
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7.10843334, 5.01109421]), 'previousTarget': array([7.0992562 , 5.00992562]), 'currentState': array([27.00456733,  7.04673968,  1.72588718]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.3331118477411666
running average episode reward sum: 0.4786345459225819
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 5.]), 'previousTarget': array([7., 5.]), 'currentState': array([7.9671289 , 5.0909176 , 3.32748106]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 0.9713929762129295}
episode index:2194
target Thresh 31.999999992417383
target distance 11.0
model initialize at round 2194
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 20.]), 'previousTarget': array([14., 20.]), 'currentState': array([ 3.03447973, 23.03767488,  0.65631926]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 11.37849302306282}
done in step count: 25
reward sum = 0.6037864951933802
running average episode reward sum: 0.4786915627559627
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 20.]), 'previousTarget': array([14., 20.]), 'currentState': array([13.04999337, 20.23989471,  6.0089571 ]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 0.97982757458501}
episode index:2195
target Thresh 31.999999992492832
target distance 11.0
model initialize at round 2195
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  2.]), 'previousTarget': array([21.,  2.]), 'currentState': array([23.        , 13.        ,  1.02331439]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 11.180339887498947}
done in step count: 29
reward sum = 0.5461485535308324
running average episode reward sum: 0.4787222808756234
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  2.]), 'previousTarget': array([21.,  2.]), 'currentState': array([21.17521175,  2.83728488,  4.3789274 ]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 0.8554210203127317}
episode index:2196
target Thresh 31.99999999256753
target distance 19.0
model initialize at round 2196
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.47690067, 27.77706223]), 'previousTarget': array([22.4327075, 27.76114  ]), 'currentState': array([ 4.07814872, 19.93576601,  5.66490068]), 'targetState': array([23, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.34658191759946827
running average episode reward sum: 0.478662135057109
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 28.]), 'previousTarget': array([23., 28.]), 'currentState': array([22.00742068, 27.31155343,  0.56573952]), 'targetState': array([23, 28], dtype=int32), 'currentDistance': 1.2079620751609677}
episode index:2197
target Thresh 31.999999992641484
target distance 12.0
model initialize at round 2197
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 27.]), 'previousTarget': array([12., 27.]), 'currentState': array([24.00864315, 26.00742502,  0.94983676]), 'targetState': array([12, 27], dtype=int32), 'currentDistance': 12.049593989294852}
done in step count: 31
reward sum = 0.5307351149484556
running average episode reward sum: 0.4786858261307629
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 27.]), 'previousTarget': array([12., 27.]), 'currentState': array([12.69532728, 27.00327387,  3.35620729]), 'targetState': array([12, 27], dtype=int32), 'currentDistance': 0.6953349903969432}
episode index:2198
target Thresh 31.999999992714702
target distance 7.0
model initialize at round 2198
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  3.]), 'previousTarget': array([17.,  3.]), 'currentState': array([23.70380337,  9.90684166,  3.54123701]), 'targetState': array([17,  3], dtype=int32), 'currentDistance': 9.625250193576875}
done in step count: 23
reward sum = 0.6746651880470087
running average episode reward sum: 0.4787749481689239
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  3.]), 'previousTarget': array([17.,  3.]), 'currentState': array([17.84239437,  3.92383688,  4.18949978]), 'targetState': array([17,  3], dtype=int32), 'currentDistance': 1.2502411193404364}
episode index:2199
target Thresh 31.999999992787192
target distance 13.0
model initialize at round 2199
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([ 2.26891382, 15.95282623,  6.17753629]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 13.090676956816946}
done in step count: 30
reward sum = 0.5561791126844391
running average episode reward sum: 0.4788101318800674
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([14.07555988, 19.05511979,  0.54250008]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 0.9260819252550294}
episode index:2200
target Thresh 31.99999999285896
target distance 10.0
model initialize at round 2200
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 27.]), 'previousTarget': array([ 5., 27.]), 'currentState': array([15.        , 26.        ,  4.17665949]), 'targetState': array([ 5, 27], dtype=int32), 'currentDistance': 10.04987562112089}
done in step count: 24
reward sum = 0.6205949698940629
running average episode reward sum: 0.4788745502526316
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 27.]), 'previousTarget': array([ 5., 27.]), 'currentState': array([ 5.61588549, 27.30811199,  3.02387781]), 'targetState': array([ 5, 27], dtype=int32), 'currentDistance': 0.688656618472042}
episode index:2201
target Thresh 31.999999992930015
target distance 21.0
model initialize at round 2201
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.62348233,  9.60031083]), 'previousTarget': array([10.63241055,  9.54387571]), 'currentState': array([ 6.04299605, 29.06872488,  0.75924718]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.27113838820368796
running average episode reward sum: 0.47878021048785013
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  8.]), 'previousTarget': array([11.,  8.]), 'currentState': array([10.92001906,  8.89137211,  4.88554608]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 0.8949531772789451}
episode index:2202
target Thresh 31.999999993000365
target distance 16.0
model initialize at round 2202
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([26.01162787, 18.99919925,  5.98057034]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 16.280180561152363}
done in step count: 42
reward sum = 0.4329099342303856
running average episode reward sum: 0.47875938875554985
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([23.33148384,  3.95637082,  4.63806121]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 1.0121890517117358}
episode index:2203
target Thresh 31.999999993070013
target distance 16.0
model initialize at round 2203
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.05153389, 15.82990784]), 'previousTarget': array([19.05153389, 15.82990784]), 'currentState': array([ 4.        , 29.        ,  3.48255944]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.28202939110342595
running average episode reward sum: 0.4786701283210434
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([19.2076765 , 15.83763033,  5.22119741]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 1.152996577705866}
episode index:2204
target Thresh 31.999999993138967
target distance 1.0
model initialize at round 2204
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 22.]), 'previousTarget': array([23., 22.]), 'currentState': array([21.99788399, 22.99939921,  3.53294884]), 'targetState': array([23, 22], dtype=int32), 'currentDistance': 1.4152862867497877}
done in step count: 8
reward sum = 0.8763727596949575
running average episode reward sum: 0.47885049232620164
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 22.]), 'previousTarget': array([23., 22.]), 'currentState': array([22.07117019, 21.24675692,  5.67271246]), 'targetState': array([23, 22], dtype=int32), 'currentDistance': 1.1958678666796942}
episode index:2205
target Thresh 31.999999993207233
target distance 10.0
model initialize at round 2205
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 26.]), 'previousTarget': array([12., 26.]), 'currentState': array([ 5.      , 16.      ,  1.720153]), 'targetState': array([12, 26], dtype=int32), 'currentDistance': 12.206555615733704}
done in step count: 29
reward sum = 0.582191812358185
running average episode reward sum: 0.47889733789285255
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 26.]), 'previousTarget': array([12., 26.]), 'currentState': array([11.05770906, 25.03638297,  1.03671611]), 'targetState': array([12, 26], dtype=int32), 'currentDistance': 1.3477648204266202}
episode index:2206
target Thresh 31.999999993274823
target distance 19.0
model initialize at round 2206
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4.23313766, 28.91410718]), 'previousTarget': array([ 4.23313766, 28.91410718]), 'currentState': array([23.        , 22.        ,  5.08008906]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.301768504936269
running average episode reward sum: 0.4788170801525007
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 29.]), 'previousTarget': array([ 4., 29.]), 'currentState': array([ 4.77839665, 29.21667365,  2.6429144 ]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 0.8079905995671683}
episode index:2207
target Thresh 31.99999999334174
target distance 3.0
model initialize at round 2207
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([13.98938302, 17.96041911,  4.19782305]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 3.6327258119317793}
done in step count: 12
reward sum = 0.7992637668538766
running average episode reward sum: 0.4789622099924923
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([11.53057927, 20.03359171,  1.91268329]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 1.0743839205256522}
episode index:2208
target Thresh 31.99999999340799
target distance 17.0
model initialize at round 2208
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  2.]), 'previousTarget': array([13.,  2.]), 'currentState': array([18.        , 19.        ,  2.62988329]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 17.72004514666935}
done in step count: 44
reward sum = 0.3874861020305895
running average episode reward sum: 0.4789207993505901
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  2.]), 'previousTarget': array([13.,  2.]), 'currentState': array([13.35350287,  2.9132629 ,  4.500816  ]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 0.9792922971076878}
episode index:2209
target Thresh 31.999999993473583
target distance 10.0
model initialize at round 2209
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 24.]), 'previousTarget': array([ 7., 24.]), 'currentState': array([16.65119668, 24.06396825,  3.00697164]), 'targetState': array([ 7, 24], dtype=int32), 'currentDistance': 9.651408669518029}
done in step count: 22
reward sum = 0.6653709640906653
running average episode reward sum: 0.47900516594097026
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 24.]), 'previousTarget': array([ 7., 24.]), 'currentState': array([ 7.94850443, 23.67664711,  3.4762063 ]), 'targetState': array([ 7, 24], dtype=int32), 'currentDistance': 1.002106657019214}
episode index:2210
target Thresh 31.99999999353852
target distance 16.0
model initialize at round 2210
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 21.]), 'previousTarget': array([25., 21.]), 'currentState': array([ 9.        , 21.        ,  4.96082291]), 'targetState': array([25, 21], dtype=int32), 'currentDistance': 16.0}
done in step count: 38
reward sum = 0.4445094633118803
running average episode reward sum: 0.4789895640854167
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 21.]), 'previousTarget': array([25., 21.]), 'currentState': array([24.36107636, 21.1517364 ,  0.22923853]), 'targetState': array([25, 21], dtype=int32), 'currentDistance': 0.6566942627194593}
episode index:2211
target Thresh 31.999999993602813
target distance 14.0
model initialize at round 2211
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([17.68439204, 23.77526044,  3.84373562]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 16.284239788553595}
done in step count: 35
reward sum = 0.4784643825312649
running average episode reward sum: 0.4789893266615676
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 9.88073198, 10.99203809,  4.40316096]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 1.3265852366228181}
episode index:2212
target Thresh 31.999999993666467
target distance 9.0
model initialize at round 2212
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 17.]), 'previousTarget': array([17., 17.]), 'currentState': array([13.01920581,  7.92233203,  5.20730686]), 'targetState': array([17, 17], dtype=int32), 'currentDistance': 9.912153053575127}
done in step count: 25
reward sum = 0.5977083516353225
running average episode reward sum: 0.4790429728545064
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 17.]), 'previousTarget': array([17., 17.]), 'currentState': array([16.21491314, 16.24834048,  1.58782691]), 'targetState': array([17, 17], dtype=int32), 'currentDistance': 1.0869008279496755}
episode index:2213
target Thresh 31.999999993729485
target distance 14.0
model initialize at round 2213
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 29.]), 'previousTarget': array([21., 29.]), 'currentState': array([21.04176261, 14.99913945,  0.23189705]), 'targetState': array([21, 29], dtype=int32), 'currentDistance': 14.000922840514063}
done in step count: 33
reward sum = 0.5049796972257751
running average episode reward sum: 0.47905468772549614
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 29.]), 'previousTarget': array([21., 29.]), 'currentState': array([21.1170671 , 28.11300576,  1.73481175]), 'targetState': array([21, 29], dtype=int32), 'currentDistance': 0.894686247985912}
episode index:2214
target Thresh 31.999999993791878
target distance 13.0
model initialize at round 2214
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([26.97208199,  3.97280579,  3.66135836]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 13.91213832639195}
done in step count: 30
reward sum = 0.5286153008203134
running average episode reward sum: 0.47907706272012135
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.90244931,  8.12720787,  3.00812139]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.2554604157479796}
episode index:2215
target Thresh 31.999999993853653
target distance 24.0
model initialize at round 2215
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7.83274634, 22.37373409]), 'previousTarget': array([ 7.8507125, 22.40285  ]), 'currentState': array([2.93988224, 2.98147066, 3.22906953]), 'targetState': array([ 9, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.2101638773503698
running average episode reward sum: 0.47895571200470183
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 27.]), 'previousTarget': array([ 9., 27.]), 'currentState': array([ 8.92761018, 26.33618012,  1.40323086]), 'targetState': array([ 9, 27], dtype=int32), 'currentDistance': 0.6677552829035063}
episode index:2216
target Thresh 31.99999999391481
target distance 9.0
model initialize at round 2216
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  6.]), 'previousTarget': array([22.,  6.]), 'currentState': array([12.96216339, 13.98455309,  3.7638838 ]), 'targetState': array([22,  6], dtype=int32), 'currentDistance': 12.059667437537367}
done in step count: 33
reward sum = 0.5485468913046504
running average episode reward sum: 0.478987101801409
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  6.]), 'previousTarget': array([22.,  6.]), 'currentState': array([21.11685921,  6.62647884,  5.6923482 ]), 'targetState': array([22,  6], dtype=int32), 'currentDistance': 1.0827803926044557}
episode index:2217
target Thresh 31.999999993975358
target distance 20.0
model initialize at round 2217
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.99875234,  9.02495322]), 'previousTarget': array([20.99875234,  9.02495322]), 'currentState': array([20.        , 29.        ,  5.62431926]), 'targetState': array([21,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.33717246775917975
running average episode reward sum: 0.47892316373376154
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  9.]), 'previousTarget': array([21.,  9.]), 'currentState': array([20.8202071 ,  9.63294457,  4.94333573]), 'targetState': array([21,  9], dtype=int32), 'currentDistance': 0.6579850394980973}
episode index:2218
target Thresh 31.999999994035303
target distance 3.0
model initialize at round 2218
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 21.]), 'previousTarget': array([10., 21.]), 'currentState': array([13.00894839, 19.35214373,  1.75254777]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 3.430626863721328}
done in step count: 7
reward sum = 0.8671848210817191
running average episode reward sum: 0.479098135188177
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 21.]), 'previousTarget': array([10., 21.]), 'currentState': array([10.84751298, 20.71487685,  2.91751336]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 0.8941887153174631}
episode index:2219
target Thresh 31.999999994094654
target distance 25.0
model initialize at round 2219
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.11977998,  9.9155048 ]), 'previousTarget': array([15.09551454,  9.95151706]), 'currentState': array([ 9.06151807, 28.97586846,  5.70123574]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 68
reward sum = 0.20841850450136457
running average episode reward sum: 0.4789762074266064
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([17.15693357,  4.99422757,  5.18765797]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 1.0065369398271573}
episode index:2220
target Thresh 31.999999994153413
target distance 25.0
model initialize at round 2220
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.22793262, 11.54987968]), 'previousTarget': array([15.22793262, 11.54987968]), 'currentState': array([25.        , 29.        ,  2.98112941]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 85
reward sum = 0.15962529544357723
running average episode reward sum: 0.4788324204333678
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([11.49552663,  4.78773945,  4.22158669]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 0.9306342389802786}
episode index:2221
target Thresh 31.99999999421159
target distance 14.0
model initialize at round 2221
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 25.]), 'previousTarget': array([18., 25.]), 'currentState': array([ 5.        , 11.        ,  1.73587537]), 'targetState': array([18, 25], dtype=int32), 'currentDistance': 19.104973174542796}
done in step count: 48
reward sum = 0.3806853422246157
running average episode reward sum: 0.4787882498311136
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 25.]), 'previousTarget': array([18., 25.]), 'currentState': array([17.30855514, 24.07481811,  0.93455086]), 'targetState': array([18, 25], dtype=int32), 'currentDistance': 1.1550140770511617}
episode index:2222
target Thresh 31.99999999426918
target distance 23.0
model initialize at round 2222
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.71778957, 28.42335651]), 'previousTarget': array([23.7042351, 28.4268235]), 'currentState': array([ 4.01948428, 24.96260967,  5.35588041]), 'targetState': array([27, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.26043596904055916
running average episode reward sum: 0.4786900256832096
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 29.]), 'previousTarget': array([27., 29.]), 'currentState': array([26.14958118, 29.28836935,  0.32510576]), 'targetState': array([27, 29], dtype=int32), 'currentDistance': 0.8979805419523373}
episode index:2223
target Thresh 31.999999994326206
target distance 14.0
model initialize at round 2223
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 13.]), 'previousTarget': array([27., 13.]), 'currentState': array([23.03951918, 26.73116687,  4.94584695]), 'targetState': array([27, 13], dtype=int32), 'currentDistance': 14.290918515057449}
done in step count: 32
reward sum = 0.5246484281900474
running average episode reward sum: 0.4787106904325382
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 13.]), 'previousTarget': array([27., 13.]), 'currentState': array([26.82987754, 13.99678295,  4.98788407]), 'targetState': array([27, 13], dtype=int32), 'currentDistance': 1.0111962705062743}
episode index:2224
target Thresh 31.99999999438266
target distance 16.0
model initialize at round 2224
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 28.]), 'previousTarget': array([26., 28.]), 'currentState': array([26.       , 12.       ,  3.8844234]), 'targetState': array([26, 28], dtype=int32), 'currentDistance': 16.0}
done in step count: 46
reward sum = 0.40881619162096205
running average episode reward sum: 0.4786792771746453
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 28.]), 'previousTarget': array([26., 28.]), 'currentState': array([25.82334752, 27.14481281,  1.74805663]), 'targetState': array([26, 28], dtype=int32), 'currentDistance': 0.8732417936295745}
episode index:2225
target Thresh 31.999999994438554
target distance 13.0
model initialize at round 2225
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 3.]), 'previousTarget': array([6., 3.]), 'currentState': array([19.07511108,  7.9384054 ,  5.34383965]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 13.976636848956495}
done in step count: 34
reward sum = 0.47795656661172725
running average episode reward sum: 0.4786789525068273
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 3.]), 'previousTarget': array([6., 3.]), 'currentState': array([6.87150521, 3.11789992, 3.61367327]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 0.8794439888331689}
episode index:2226
target Thresh 31.99999999449389
target distance 16.0
model initialize at round 2226
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  7.]), 'previousTarget': array([20.,  7.]), 'currentState': array([ 4.      , 14.      ,  1.863004]), 'targetState': array([20,  7], dtype=int32), 'currentDistance': 17.46424919657298}
done in step count: 46
reward sum = 0.38208678938869967
running average episode reward sum: 0.47863557928584927
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  7.]), 'previousTarget': array([20.,  7.]), 'currentState': array([19.05341723,  6.85291612,  5.82019037]), 'targetState': array([20,  7], dtype=int32), 'currentDistance': 0.9579418623691165}
episode index:2227
target Thresh 31.999999994548677
target distance 8.0
model initialize at round 2227
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([21.94545896, 24.88545181,  4.36259228]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 8.456392331044885}
done in step count: 17
reward sum = 0.704894522827828
running average episode reward sum: 0.47873713177397403
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([24.65565586, 17.86965292,  5.3328888 ]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 0.9353443715283569}
episode index:2228
target Thresh 31.99999999460292
target distance 13.0
model initialize at round 2228
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 16.]), 'previousTarget': array([ 6., 16.]), 'currentState': array([19.04979352, 14.96925557,  5.47825679]), 'targetState': array([ 6, 16], dtype=int32), 'currentDistance': 13.09043715884616}
done in step count: 32
reward sum = 0.49477097173313395
running average episode reward sum: 0.4787443250624258
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 16.]), 'previousTarget': array([ 6., 16.]), 'currentState': array([ 6.86774149, 16.09484743,  3.25361691]), 'targetState': array([ 6, 16], dtype=int32), 'currentDistance': 0.8729096894700396}
episode index:2229
target Thresh 31.999999994656623
target distance 6.0
model initialize at round 2229
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([9.29124173, 2.2193529 , 0.51192051]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 5.761886053325054}
done in step count: 13
reward sum = 0.794220551554503
running average episode reward sum: 0.47888579422228766
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.12395902,  3.20976781,  0.17195052]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.9008053847714327}
episode index:2230
target Thresh 31.99999999470979
target distance 14.0
model initialize at round 2230
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([ 3.        , 19.        ,  0.07585046]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 14.0356688476182}
done in step count: 35
reward sum = 0.49217645229673057
running average episode reward sum: 0.4788917514872247
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.52753592, 5.93500028, 4.66214079]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 1.0735546915159475}
episode index:2231
target Thresh 31.999999994762426
target distance 20.0
model initialize at round 2231
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6.24436131, 28.94913127]), 'previousTarget': array([ 6.38838649, 28.9223227 ]), 'currentState': array([25.8246074 , 24.87310833,  3.66595098]), 'targetState': array([ 6, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.33060396781894846
running average episode reward sum: 0.4788253143081618
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 29.]), 'previousTarget': array([ 6., 29.]), 'currentState': array([ 6.86176659, 28.44925753,  2.97250035]), 'targetState': array([ 6, 29], dtype=int32), 'currentDistance': 1.0227213336498202}
episode index:2232
target Thresh 31.99999999481454
target distance 19.0
model initialize at round 2232
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8.5672925, 19.23886  ]), 'previousTarget': array([ 8.5672925, 19.23886  ]), 'currentState': array([27.        , 27.        ,  4.78912094]), 'targetState': array([ 8, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.3269133139947878
running average episode reward sum: 0.47875728385571514
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 19.]), 'previousTarget': array([ 8., 19.]), 'currentState': array([ 8.97601937, 19.35707435,  3.54720959]), 'targetState': array([ 8, 19], dtype=int32), 'currentDistance': 1.0392862491167199}
episode index:2233
target Thresh 31.999999994866137
target distance 19.0
model initialize at round 2233
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.76686234, 26.91410718]), 'previousTarget': array([20.76686234, 26.91410718]), 'currentState': array([ 2.        , 20.        ,  5.70544729]), 'targetState': array([21, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.35583047388297057
running average episode reward sum: 0.4787022584260048
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 27.]), 'previousTarget': array([21., 27.]), 'currentState': array([20.07634193, 26.44478915,  0.21464745]), 'targetState': array([21, 27], dtype=int32), 'currentDistance': 1.077684232761822}
episode index:2234
target Thresh 31.99999999491722
target distance 12.0
model initialize at round 2234
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 26.]), 'previousTarget': array([ 3., 26.]), 'currentState': array([ 9.        , 14.        ,  2.84232521]), 'targetState': array([ 3, 26], dtype=int32), 'currentDistance': 13.416407864998739}
done in step count: 32
reward sum = 0.5302432029043838
running average episode reward sum: 0.47872531925127476
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 26.]), 'previousTarget': array([ 3., 26.]), 'currentState': array([ 2.80408551, 25.04179495,  1.88686373]), 'targetState': array([ 3, 26], dtype=int32), 'currentDistance': 0.9780283209881108}
episode index:2235
target Thresh 31.999999994967794
target distance 22.0
model initialize at round 2235
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.3213874, 23.624799 ]), 'previousTarget': array([13.26673649, 23.52454686]), 'currentState': array([2.02594501, 7.11986092, 1.31754762]), 'targetState': array([17, 29], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 66
reward sum = 0.2117558179167976
running average episode reward sum: 0.47860592323099993
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 29.]), 'previousTarget': array([17., 29.]), 'currentState': array([16.00316175, 28.27348108,  1.30829293]), 'targetState': array([17, 29], dtype=int32), 'currentDistance': 1.2334975656345881}
episode index:2236
target Thresh 31.999999995017866
target distance 5.0
model initialize at round 2236
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([ 7.93965126, 10.98568508,  3.57716435]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 6.3543613178205565}
done in step count: 14
reward sum = 0.7751069943600394
running average episode reward sum: 0.4787384672949825
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.98374132, 6.53769017, 4.3652484 ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.1210966491218817}
episode index:2237
target Thresh 31.99999999506744
target distance 12.0
model initialize at round 2237
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 12.]), 'previousTarget': array([18., 12.]), 'currentState': array([20.        , 24.        ,  0.59516323]), 'targetState': array([18, 12], dtype=int32), 'currentDistance': 12.165525060596439}
done in step count: 33
reward sum = 0.5280640544330599
running average episode reward sum: 0.47876050732498165
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 12.]), 'previousTarget': array([18., 12.]), 'currentState': array([18.36089697, 12.94040656,  4.73163421]), 'targetState': array([18, 12], dtype=int32), 'currentDistance': 1.007279070879816}
episode index:2238
target Thresh 31.999999995116518
target distance 20.0
model initialize at round 2238
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.34109383,  9.41559356]), 'previousTarget': array([11.361625  ,  9.47568183]), 'currentState': array([21.04895934, 26.9015121 ,  5.02635357]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.29316491437181513
running average episode reward sum: 0.4786776151441182
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([10.46397847,  7.82133227,  4.49267414]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 0.9433253471166702}
episode index:2239
target Thresh 31.999999995165112
target distance 1.0
model initialize at round 2239
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  2.]), 'previousTarget': array([22.,  2.]), 'currentState': array([21.        ,  2.        ,  2.58734998]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 1.0}
done in step count: 4
reward sum = 0.9364905471810592
running average episode reward sum: 0.47888199591734903
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  2.]), 'previousTarget': array([22.,  2.]), 'currentState': array([21.00013702,  2.00046621,  1.07354909]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 0.9998630915719569}
episode index:2240
target Thresh 31.99999999521322
target distance 13.0
model initialize at round 2240
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 22.]), 'previousTarget': array([ 9., 22.]), 'currentState': array([22.        , 24.        ,  0.87656239]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 13.152946437965907}
done in step count: 39
reward sum = 0.47639967262910976
running average episode reward sum: 0.4788808882318122
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 22.]), 'previousTarget': array([ 9., 22.]), 'currentState': array([ 9.83295873, 21.64616827,  3.44503677]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 0.904995656260045}
episode index:2241
target Thresh 31.999999995260847
target distance 7.0
model initialize at round 2241
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([17.05222769,  6.01727076,  0.5718625 ]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 7.620669361229202}
done in step count: 18
reward sum = 0.6959041014732492
running average episode reward sum: 0.4789776871672454
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([14.00032476, 12.11165622,  2.26969884]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 0.8883438389865773}
episode index:2242
target Thresh 31.999999995308002
target distance 20.0
model initialize at round 2242
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 4.]), 'previousTarget': array([6.0992562 , 4.00992562]), 'currentState': array([25.87051742,  5.96569129,  3.37485858]), 'targetState': array([6, 4], dtype=int32), 'currentDistance': 19.96750872897393}
done in step count: 47
reward sum = 0.36804945576869075
running average episode reward sum: 0.47892823187014394
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 4.]), 'previousTarget': array([6., 4.]), 'currentState': array([6.85853783, 3.93639838, 3.29617014]), 'targetState': array([6, 4], dtype=int32), 'currentDistance': 0.8608904486683334}
episode index:2243
target Thresh 31.99999999535469
target distance 13.0
model initialize at round 2243
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  4.]), 'previousTarget': array([22.,  4.]), 'currentState': array([17.99030678, 16.99464057,  3.82698685]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 13.599203037852362}
done in step count: 36
reward sum = 0.5247272921566637
running average episode reward sum: 0.47894864143355154
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  4.]), 'previousTarget': array([22.,  4.]), 'currentState': array([21.3560675 ,  4.86584006,  5.10349674]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 1.0790403515279827}
episode index:2244
target Thresh 31.999999995400913
target distance 23.0
model initialize at round 2244
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.10348051,  6.50193128]), 'previousTarget': array([10.04843794,  6.64765455]), 'currentState': array([ 5.14330207, 25.87708624,  5.43220839]), 'targetState': array([11,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.2697397649950408
running average episode reward sum: 0.47885545262444745
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  3.]), 'previousTarget': array([11.,  3.]), 'currentState': array([11.2921296 ,  3.74862358,  4.99923701]), 'targetState': array([11,  3], dtype=int32), 'currentDistance': 0.803602496126273}
episode index:2245
target Thresh 31.99999999544667
target distance 19.0
model initialize at round 2245
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([10.97034218, 25.89247759,  4.54964837]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 19.550528565043347}
done in step count: 47
reward sum = 0.3782783430913324
running average episode reward sum: 0.4788106720770151
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.79962353,  7.83863667,  5.30656888]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.8622425360477087}
episode index:2246
target Thresh 31.99999999549198
target distance 12.0
model initialize at round 2246
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 6.]), 'previousTarget': array([6., 6.]), 'currentState': array([17.91285969, 12.99736521,  3.22128222]), 'targetState': array([6, 6], dtype=int32), 'currentDistance': 13.815909154027583}
done in step count: 35
reward sum = 0.5271590748270322
running average episode reward sum: 0.47883218894517265
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 6.]), 'previousTarget': array([6., 6.]), 'currentState': array([6.87206407, 6.12267442, 3.794606  ]), 'targetState': array([6, 6], dtype=int32), 'currentDistance': 0.8806501839225706}
episode index:2247
target Thresh 31.999999995536836
target distance 6.0
model initialize at round 2247
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 23.]), 'previousTarget': array([11., 23.]), 'currentState': array([14.99524213, 16.99999402,  2.8903501 ]), 'targetState': array([11, 23], dtype=int32), 'currentDistance': 7.208469427736928}
done in step count: 13
reward sum = 0.7476607920623669
running average episode reward sum: 0.4789517746227159
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 23.]), 'previousTarget': array([11., 23.]), 'currentState': array([11.51538031, 22.07931421,  2.11046405]), 'targetState': array([11, 23], dtype=int32), 'currentDistance': 1.0551204645317058}
episode index:2248
target Thresh 31.999999995581245
target distance 14.0
model initialize at round 2248
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([ 7.25041888, 16.97813881,  6.08210534]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 17.594616013966633}
done in step count: 40
reward sum = 0.4381485341764053
running average episode reward sum: 0.4789336317857011
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([20.03021791,  6.93205243,  5.85769197]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 1.3450646924334304}
episode index:2249
target Thresh 31.99999999562521
target distance 7.0
model initialize at round 2249
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([22.99275633,  7.97172393,  4.2188279 ]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 7.638930229126239}
done in step count: 21
reward sum = 0.6688387023022931
running average episode reward sum: 0.479018034039264
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([19.83371005, 14.12071005,  1.97064142]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 0.8948760621970222}
episode index:2250
target Thresh 31.99999999566874
target distance 13.0
model initialize at round 2250
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 20.]), 'previousTarget': array([13., 20.]), 'currentState': array([26.00066741, 16.98835334,  4.52349454]), 'targetState': array([13, 20], dtype=int32), 'currentDistance': 13.344937944723183}
done in step count: 35
reward sum = 0.510624243200806
running average episode reward sum: 0.4790320750029075
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 20.]), 'previousTarget': array([13., 20.]), 'currentState': array([13.88375585, 19.58615154,  3.16378095]), 'targetState': array([13, 20], dtype=int32), 'currentDistance': 0.9758560109013611}
episode index:2251
target Thresh 31.99999999571184
target distance 20.0
model initialize at round 2251
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.42781353, 24.56953382]), 'previousTarget': array([15.42781353, 24.56953382]), 'currentState': array([8.        , 6.        , 5.75751877]), 'targetState': array([16, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.28740984513477835
running average episode reward sum: 0.47894698520278844
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 26.]), 'previousTarget': array([16., 26.]), 'currentState': array([15.53229797, 25.09033795,  1.25327135]), 'targetState': array([16, 26], dtype=int32), 'currentDistance': 1.0228539614509211}
episode index:2252
target Thresh 31.999999995754507
target distance 6.0
model initialize at round 2252
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([11.62994028, 14.66412521,  3.93326172]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 7.3156449797862235}
done in step count: 15
reward sum = 0.7386722285081426
running average episode reward sum: 0.47906226493794396
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([7.93341258, 9.32701053, 4.30383627]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.9890373751312672}
episode index:2253
target Thresh 31.99999999579675
target distance 17.0
model initialize at round 2253
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5.14148489, 2.21630556]), 'previousTarget': array([5.13497444, 2.20859686]), 'currentState': array([16.08941721, 18.95377185,  5.57938167]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 68
reward sum = 0.2810672776219443
running average episode reward sum: 0.4789744233286645
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 2.]), 'previousTarget': array([5., 2.]), 'currentState': array([5.94662276, 2.4940752 , 4.08585548]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 1.067803796797213}
episode index:2254
target Thresh 31.99999999583857
target distance 9.0
model initialize at round 2254
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 25.]), 'previousTarget': array([21., 25.]), 'currentState': array([20.13231291, 16.17341647,  1.0101162 ]), 'targetState': array([21, 25], dtype=int32), 'currentDistance': 8.869129475779927}
done in step count: 20
reward sum = 0.6917553969710648
running average episode reward sum: 0.4790687829622088
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 25.]), 'previousTarget': array([21., 25.]), 'currentState': array([20.4591647 , 24.11333186,  1.76812588]), 'targetState': array([21, 25], dtype=int32), 'currentDistance': 1.0385967560636957}
episode index:2255
target Thresh 31.99999999587998
target distance 17.0
model initialize at round 2255
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.30320878, 16.49036518]), 'previousTarget': array([23.33935727, 16.46633605]), 'currentState': array([ 6.94741862, 28.00071394,  3.37845269]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.3000895906988823
running average episode reward sum: 0.47898944821386513
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 16.]), 'previousTarget': array([24., 16.]), 'currentState': array([23.11512399, 16.73768891,  5.80685279]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 1.152037534270806}
episode index:2256
target Thresh 31.999999995920973
target distance 21.0
model initialize at round 2256
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.59480715, 25.04713595]), 'previousTarget': array([17.59867161, 25.05721038]), 'currentState': array([9.00078665, 6.98771149, 5.02881718]), 'targetState': array([19, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 79
reward sum = 0.2277412092822246
running average episode reward sum: 0.4788781286574045
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 28.]), 'previousTarget': array([19., 28.]), 'currentState': array([18.5098707 , 27.25908672,  1.47112642]), 'targetState': array([19, 28], dtype=int32), 'currentDistance': 0.8883575989531528}
episode index:2257
target Thresh 31.999999995961563
target distance 14.0
model initialize at round 2257
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 19.]), 'previousTarget': array([18., 19.]), 'currentState': array([19.       ,  5.       ,  0.4109652]), 'targetState': array([18, 19], dtype=int32), 'currentDistance': 14.035668847618199}
done in step count: 42
reward sum = 0.4707082326099298
running average episode reward sum: 0.4788745104572063
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 19.]), 'previousTarget': array([18., 19.]), 'currentState': array([17.83448463, 18.26259539,  1.85286137]), 'targetState': array([18, 19], dtype=int32), 'currentDistance': 0.7557518771871288}
episode index:2258
target Thresh 31.999999996001744
target distance 19.0
model initialize at round 2258
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.8570147 , 11.85006263]), 'previousTarget': array([12.85786438, 11.85786438]), 'currentState': array([27.00927508, 25.98206624,  5.04560426]), 'targetState': array([8, 7], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 76
reward sum = 0.18310842840302805
running average episode reward sum: 0.4787435825767043
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 7.]), 'previousTarget': array([8., 7.]), 'currentState': array([8.83585601, 7.13766145, 3.66522481]), 'targetState': array([8, 7], dtype=int32), 'currentDistance': 0.8471162479303543}
episode index:2259
target Thresh 31.999999996041527
target distance 10.0
model initialize at round 2259
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([ 4.03157002, 14.25166541,  1.38348994]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 10.941449801429462}
done in step count: 22
reward sum = 0.6352862156675891
running average episode reward sum: 0.47881284922851436
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([ 8.17763856, 23.08493111,  1.53722526]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 1.2302964699362309}
episode index:2260
target Thresh 31.999999996080916
target distance 10.0
model initialize at round 2260
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'previousTarget': array([11., 19.]), 'currentState': array([18.        , 29.        ,  0.40894422]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 12.206555615733702}
done in step count: 30
reward sum = 0.5307970471409382
running average episode reward sum: 0.4788358409126862
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'previousTarget': array([11., 19.]), 'currentState': array([11.73959118, 19.93412851,  4.10718569]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 1.191465980835947}
episode index:2261
target Thresh 31.99999999611991
target distance 19.0
model initialize at round 2261
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3.43827311, 9.92524322]), 'previousTarget': array([3.43827311, 9.92524322]), 'currentState': array([12.        , 28.        ,  3.04879215]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 58
reward sum = 0.3208758199052786
running average episode reward sum: 0.4787660088963257
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.77034687, 9.79530924, 4.34861549]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.107226750563205}
episode index:2262
target Thresh 31.999999996158518
target distance 9.0
model initialize at round 2262
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 23.]), 'previousTarget': array([ 4., 23.]), 'currentState': array([ 5.90048088, 14.42138728,  1.76661551]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 8.786604787643968}
done in step count: 16
reward sum = 0.7004117790177272
running average episode reward sum: 0.47886395223265865
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 23.]), 'previousTarget': array([ 4., 23.]), 'currentState': array([ 4.22410584, 22.06576325,  1.98401683]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 0.9607402009658855}
episode index:2263
target Thresh 31.999999996196742
target distance 6.0
model initialize at round 2263
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'previousTarget': array([11., 21.]), 'currentState': array([11.54159937, 26.8696942 ,  3.6632441 ]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 5.894628056134278}
done in step count: 12
reward sum = 0.7668131719931848
running average episode reward sum: 0.4789911382837896
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'previousTarget': array([11., 21.]), 'currentState': array([11.33900097, 21.90443348,  5.1292022 ]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 0.9658786549646543}
episode index:2264
target Thresh 31.999999996234585
target distance 8.0
model initialize at round 2264
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 27.]), 'previousTarget': array([21., 27.]), 'currentState': array([19.0048131 , 18.98080437,  5.21056318]), 'targetState': array([21, 27], dtype=int32), 'currentDistance': 8.2636716651218}
done in step count: 20
reward sum = 0.6407452453102949
running average episode reward sum: 0.4790625529005783
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 27.]), 'previousTarget': array([21., 27.]), 'currentState': array([20.95661573, 26.27681018,  1.57450002]), 'targetState': array([21, 27], dtype=int32), 'currentDistance': 0.7244899685270333}
episode index:2265
target Thresh 31.999999996272052
target distance 8.0
model initialize at round 2265
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([18.35269102, 13.79405071,  5.61090392]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 8.23137116709727}
done in step count: 16
reward sum = 0.7139176857634418
running average episode reward sum: 0.4791661959424418
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([20.53349244,  6.97279253,  5.04570117]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 1.078867286292374}
episode index:2266
target Thresh 31.999999996309146
target distance 2.0
model initialize at round 2266
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 16.]), 'previousTarget': array([25., 16.]), 'currentState': array([23.05948397, 16.65880941,  5.0694432 ]), 'targetState': array([25, 16], dtype=int32), 'currentDistance': 2.0493004363454954}
done in step count: 4
reward sum = 0.9209187948246159
running average episode reward sum: 0.47936105813868457
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 16.]), 'previousTarget': array([25., 16.]), 'currentState': array([24.12990195, 15.66881125,  5.90058881]), 'targetState': array([25, 16], dtype=int32), 'currentDistance': 0.930997639373213}
episode index:2267
target Thresh 31.99999999634587
target distance 9.0
model initialize at round 2267
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([20.00453526, 11.00072955,  6.19426719]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 9.84768353234137}
done in step count: 23
reward sum = 0.6334076457865007
running average episode reward sum: 0.4794289799145434
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([24.13958909,  2.96641905,  5.32038583]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 0.9764481042427207}
episode index:2268
target Thresh 31.99999999638223
target distance 10.0
model initialize at round 2268
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([25.        , 20.        ,  2.42611361]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 11.661903789690601}
done in step count: 29
reward sum = 0.5508274217326576
running average episode reward sum: 0.47946044683469247
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([19.22256731, 10.73578227,  4.4813132 ]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 0.768707852539868}
episode index:2269
target Thresh 31.999999996418225
target distance 10.0
model initialize at round 2269
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([26.        ,  9.        ,  6.08677012]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 10.77032961426901}
done in step count: 28
reward sum = 0.5713961489579107
running average episode reward sum: 0.4795009471439979
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([22.14711164, 18.12045852,  2.32221864]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 0.8917595212256189}
episode index:2270
target Thresh 31.999999996453866
target distance 16.0
model initialize at round 2270
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.14213562,  6.85786438]), 'previousTarget': array([18.14213562,  6.85786438]), 'currentState': array([ 4.        , 21.        ,  3.65827799]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.2551798110567951
running average episode reward sum: 0.47940217077407843
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([19.27721394,  5.35398535,  5.6364234 ]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 0.8048138415581821}
episode index:2271
target Thresh 31.99999999648915
target distance 6.0
model initialize at round 2271
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 12.]), 'previousTarget': array([17., 12.]), 'currentState': array([19.00828324,  5.98860905,  5.53261584]), 'targetState': array([17, 12], dtype=int32), 'currentDistance': 6.337982538322821}
done in step count: 17
reward sum = 0.7060285860655366
running average episode reward sum: 0.47950191831602007
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 12.]), 'previousTarget': array([17., 12.]), 'currentState': array([17.59878054, 11.17206647,  2.17129172]), 'targetState': array([17, 12], dtype=int32), 'currentDistance': 1.0217690881352708}
episode index:2272
target Thresh 31.999999996524082
target distance 12.0
model initialize at round 2272
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 11.]), 'previousTarget': array([17., 11.]), 'currentState': array([5.        , 5.        , 1.16676188]), 'targetState': array([17, 11], dtype=int32), 'currentDistance': 13.416407864998739}
done in step count: 31
reward sum = 0.5437443795459896
running average episode reward sum: 0.4795301816073663
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 11.]), 'previousTarget': array([17., 11.]), 'currentState': array([16.14763218, 10.26204289,  0.56637155]), 'targetState': array([17, 11], dtype=int32), 'currentDistance': 1.1274358461064982}
episode index:2273
target Thresh 31.99999999655867
target distance 18.0
model initialize at round 2273
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 12.]), 'previousTarget': array([23., 12.]), 'currentState': array([ 4.99354911, 16.93629541,  4.85927767]), 'targetState': array([23, 12], dtype=int32), 'currentDistance': 18.670813746535156}
done in step count: 55
reward sum = 0.3727080133531707
running average episode reward sum: 0.4794832061595852
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 12.]), 'previousTarget': array([23., 12.]), 'currentState': array([22.27610871, 12.12386026,  0.0264674 ]), 'targetState': array([23, 12], dtype=int32), 'currentDistance': 0.7344113071626126}
episode index:2274
target Thresh 31.999999996592912
target distance 6.0
model initialize at round 2274
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([20.02075961, 19.96554817,  5.48208901]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 6.113388636938323}
done in step count: 15
reward sum = 0.7201886393917228
running average episode reward sum: 0.4795890107456213
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([20.75369247, 25.17056309,  1.96704883]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 0.8652357966142799}
episode index:2275
target Thresh 31.999999996626812
target distance 15.0
model initialize at round 2275
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 7.]), 'previousTarget': array([8., 7.]), 'currentState': array([23.06430127, 17.96146325,  5.53536674]), 'targetState': array([8, 7], dtype=int32), 'currentDistance': 18.63026702165834}
done in step count: 57
reward sum = 0.3537671991695747
running average episode reward sum: 0.479533728754595
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 7.]), 'previousTarget': array([8., 7.]), 'currentState': array([8.57052498, 7.78047051, 3.80959021]), 'targetState': array([8, 7], dtype=int32), 'currentDistance': 0.9667641769127779}
episode index:2276
target Thresh 31.999999996660375
target distance 19.0
model initialize at round 2276
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 10.]), 'previousTarget': array([17., 10.]), 'currentState': array([19.93664548, 29.0318039 ,  2.91219547]), 'targetState': array([17, 10], dtype=int32), 'currentDistance': 19.257036282099758}
done in step count: 54
reward sum = 0.33333649433157597
running average episode reward sum: 0.4794695226788712
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 10.]), 'previousTarget': array([17., 10.]), 'currentState': array([16.74727346, 10.86833018,  5.19783078]), 'targetState': array([17, 10], dtype=int32), 'currentDistance': 0.9043605473099139}
episode index:2277
target Thresh 31.999999996693607
target distance 9.0
model initialize at round 2277
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 16.]), 'previousTarget': array([ 6., 16.]), 'currentState': array([8.03382819, 7.07643346, 1.3243348 ]), 'targetState': array([ 6, 16], dtype=int32), 'currentDistance': 9.152403885633854}
done in step count: 20
reward sum = 0.6761160520455489
running average episode reward sum: 0.47955584687964675
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 16.]), 'previousTarget': array([ 6., 16.]), 'currentState': array([ 5.91967736, 15.16204776,  2.10756813]), 'targetState': array([ 6, 16], dtype=int32), 'currentDistance': 0.84179313508142}
episode index:2278
target Thresh 31.999999996726505
target distance 3.0
model initialize at round 2278
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([18.70735484,  2.1871086 ,  2.72641605]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 2.8267583344595804}
done in step count: 6
reward sum = 0.90751390100159
running average episode reward sum: 0.4797436301416572
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.85439043,  2.46273521,  3.16760543]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.009275215254593}
episode index:2279
target Thresh 31.999999996759076
target distance 5.0
model initialize at round 2279
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 22.]), 'previousTarget': array([22., 22.]), 'currentState': array([26.        , 17.        ,  4.85859302]), 'targetState': array([22, 22], dtype=int32), 'currentDistance': 6.4031242374328485}
done in step count: 19
reward sum = 0.7029715118297774
running average episode reward sum: 0.47984153710730987
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 22.]), 'previousTarget': array([22., 22.]), 'currentState': array([22.31137835, 21.15720175,  2.16628701]), 'targetState': array([22, 22], dtype=int32), 'currentDistance': 0.8984794724014589}
episode index:2280
target Thresh 31.999999996791324
target distance 14.0
model initialize at round 2280
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  6.]), 'previousTarget': array([17.,  6.]), 'currentState': array([27.        , 20.        ,  2.08113897]), 'targetState': array([17,  6], dtype=int32), 'currentDistance': 17.204650534085257}
done in step count: 43
reward sum = 0.38974511803199935
running average episode reward sum: 0.4798020384580002
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  6.]), 'previousTarget': array([17.,  6.]), 'currentState': array([17.3085984 ,  6.68622903,  4.44665653]), 'targetState': array([17,  6], dtype=int32), 'currentDistance': 0.7524249180605365}
episode index:2281
target Thresh 31.999999996823252
target distance 3.0
model initialize at round 2281
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 23.]), 'previousTarget': array([ 3., 23.]), 'currentState': array([ 3.15792854, 25.54760151,  4.98682129]), 'targetState': array([ 3, 23], dtype=int32), 'currentDistance': 2.5524918961662495}
done in step count: 5
reward sum = 0.9176661026016406
running average episode reward sum: 0.47999391578672224
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 23.]), 'previousTarget': array([ 3., 23.]), 'currentState': array([ 3.328208  , 23.89499321,  4.71263751]), 'targetState': array([ 3, 23], dtype=int32), 'currentDistance': 0.9532750585477097}
episode index:2282
target Thresh 31.99999999685486
target distance 6.0
model initialize at round 2282
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 19.]), 'previousTarget': array([24., 19.]), 'currentState': array([25.16306241, 24.65929677,  5.0316053 ]), 'targetState': array([24, 19], dtype=int32), 'currentDistance': 5.777573373105553}
done in step count: 13
reward sum = 0.7957971153071525
running average episode reward sum: 0.4801322439512077
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 19.]), 'previousTarget': array([24., 19.]), 'currentState': array([24.33436956, 19.97359699,  4.71318801]), 'targetState': array([24, 19], dtype=int32), 'currentDistance': 1.0294144446043745}
episode index:2283
target Thresh 31.999999996886157
target distance 8.0
model initialize at round 2283
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 13.]), 'previousTarget': array([19., 13.]), 'currentState': array([27.        , 20.        ,  0.09773418]), 'targetState': array([19, 13], dtype=int32), 'currentDistance': 10.63014581273465}
done in step count: 27
reward sum = 0.5754078923505872
running average episode reward sum: 0.4801739583331689
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 13.]), 'previousTarget': array([19., 13.]), 'currentState': array([19.74110775, 13.82837987,  4.13031594]), 'targetState': array([19, 13], dtype=int32), 'currentDistance': 1.1115097364930318}
episode index:2284
target Thresh 31.99999999691714
target distance 10.0
model initialize at round 2284
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 28.]), 'previousTarget': array([19., 28.]), 'currentState': array([20.05140489, 18.24978742,  1.45584981]), 'targetState': array([19, 28], dtype=int32), 'currentDistance': 9.806737357406625}
done in step count: 23
reward sum = 0.6578168575459258
running average episode reward sum: 0.48025170139628165
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 28.]), 'previousTarget': array([19., 28.]), 'currentState': array([19.09042333, 27.18662847,  1.91720713]), 'targetState': array([19, 28], dtype=int32), 'currentDistance': 0.8183823253709337}
episode index:2285
target Thresh 31.999999996947814
target distance 9.0
model initialize at round 2285
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 29.]), 'previousTarget': array([22., 29.]), 'currentState': array([25.97130026, 20.20049379,  1.81287028]), 'targetState': array([22, 29], dtype=int32), 'currentDistance': 9.654146017446823}
done in step count: 25
reward sum = 0.6586038163946344
running average episode reward sum: 0.4803297206941812
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 29.]), 'previousTarget': array([22., 29.]), 'currentState': array([22.10820978, 28.06877331,  2.25456485]), 'targetState': array([22, 29], dtype=int32), 'currentDistance': 0.9374926650253506}
episode index:2286
target Thresh 31.999999996978183
target distance 20.0
model initialize at round 2286
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.09356069, 17.9495029 ]), 'previousTarget': array([20.88854382, 18.05572809]), 'currentState': array([ 3.19107774, 26.86584207,  5.71859744]), 'targetState': array([23, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.3215631274954043
running average episode reward sum: 0.4802602993591577
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 17.]), 'previousTarget': array([23., 17.]), 'currentState': array([22.12303954, 17.69441217,  6.21657344]), 'targetState': array([23, 17], dtype=int32), 'currentDistance': 1.1186008741432225}
episode index:2287
target Thresh 31.999999997008253
target distance 14.0
model initialize at round 2287
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([ 8.088763  , 12.92613292,  5.72928125]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 14.757686142125563}
done in step count: 35
reward sum = 0.506129726000449
running average episode reward sum: 0.4802716059267456
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([2.12781157e+01, 8.24051564e+00, 1.30217813e-02]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 0.7608972806794182}
episode index:2288
target Thresh 31.99999999703802
target distance 19.0
model initialize at round 2288
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8.55615755, 7.25072827]), 'previousTarget': array([8.60711423, 7.30234469]), 'currentState': array([20.91860984, 22.9723622 ,  3.54391411]), 'targetState': array([6, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.272277616764865
running average episode reward sum: 0.4801807391774394
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 4.]), 'previousTarget': array([6., 4.]), 'currentState': array([6.5809838 , 4.9376137 , 4.33141939]), 'targetState': array([6, 4], dtype=int32), 'currentDistance': 1.1030238514934094}
episode index:2289
target Thresh 31.99999999706749
target distance 10.0
model initialize at round 2289
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 5.]), 'previousTarget': array([7., 5.]), 'currentState': array([17.00483245, 15.00977115,  1.36400819]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 14.15246235839961}
done in step count: 42
reward sum = 0.46046290673006895
running average episode reward sum: 0.48017212877025717
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 5.]), 'previousTarget': array([7., 5.]), 'currentState': array([7.98507093, 5.99853583, 4.13000987]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 1.4026541030230104}
episode index:2290
target Thresh 31.99999999709667
target distance 9.0
model initialize at round 2290
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 16.]), 'previousTarget': array([25., 16.]), 'currentState': array([16.1608603 ,  8.97952986,  0.08586821]), 'targetState': array([25, 16], dtype=int32), 'currentDistance': 11.287931235242805}
done in step count: 25
reward sum = 0.6108543410262839
running average episode reward sum: 0.48022917032951334
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 16.]), 'previousTarget': array([25., 16.]), 'currentState': array([24.11060203, 15.4585021 ,  0.70199838]), 'targetState': array([25, 16], dtype=int32), 'currentDistance': 1.0412726479884908}
episode index:2291
target Thresh 31.99999999712556
target distance 14.0
model initialize at round 2291
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  8.]), 'previousTarget': array([12.,  8.]), 'currentState': array([26.        ,  5.        ,  1.31390414]), 'targetState': array([12,  8], dtype=int32), 'currentDistance': 14.317821063276353}
done in step count: 34
reward sum = 0.4831719113988092
running average episode reward sum: 0.48023045424795546
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  8.]), 'previousTarget': array([12.,  8.]), 'currentState': array([12.88439383,  8.02764369,  3.32537619]), 'targetState': array([12,  8], dtype=int32), 'currentDistance': 0.8848257581867024}
episode index:2292
target Thresh 31.99999999715416
target distance 14.0
model initialize at round 2292
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([ 7.        , 18.        ,  0.71898901]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 15.65247584249853}
done in step count: 40
reward sum = 0.45632988754666615
running average episode reward sum: 0.4802200309742087
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([20.31636926, 11.45066153,  6.05932049]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 0.8188081615211888}
episode index:2293
target Thresh 31.999999997182478
target distance 8.0
model initialize at round 2293
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([17.9908652 , 16.00778034,  2.68365902]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 8.955317297313261}
done in step count: 21
reward sum = 0.6280228847609671
running average episode reward sum: 0.48028446116330487
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([21.14469471,  8.68899299,  5.39130968]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 1.098297988435047}
episode index:2294
target Thresh 31.999999997210512
target distance 17.0
model initialize at round 2294
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.30200734, 10.5862937 ]), 'previousTarget': array([10.28585494, 10.56139529]), 'currentState': array([22.9908615 , 26.04569149,  1.99143296]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.2776906173947329
running average episode reward sum: 0.48019618497865624
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([9.49701865, 9.81732718, 4.32668634]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.9565831179242352}
episode index:2295
target Thresh 31.999999997238266
target distance 12.0
model initialize at round 2295
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([20.76767765,  8.1145194 ,  2.72883474]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 11.800945439072905}
done in step count: 28
reward sum = 0.5917001584556596
running average episode reward sum: 0.48024474942703477
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([9.75607874, 9.19834237, 3.32906347]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.781661537475998}
episode index:2296
target Thresh 31.999999997265746
target distance 5.0
model initialize at round 2296
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 14.]), 'previousTarget': array([ 2., 14.]), 'currentState': array([ 6.76811574, 17.07382776,  3.03774995]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 5.673036647641889}
done in step count: 10
reward sum = 0.8109560550438413
running average episode reward sum: 0.4803887247451091
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 14.]), 'previousTarget': array([ 2., 14.]), 'currentState': array([ 2.88578343, 14.89201199,  3.8680793 ]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 1.2570989099472223}
episode index:2297
target Thresh 31.999999997292953
target distance 25.0
model initialize at round 2297
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.46376987,  9.57935406]), 'previousTarget': array([17.39259851,  9.74071961]), 'currentState': array([12.15451074, 28.86177526,  5.3838    ]), 'targetState': array([19,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.22133583075954572
running average episode reward sum: 0.48027599502622936
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  4.]), 'previousTarget': array([19.,  4.]), 'currentState': array([18.35877828,  4.72813555,  5.09234615]), 'targetState': array([19,  4], dtype=int32), 'currentDistance': 0.9702302160513337}
episode index:2298
target Thresh 31.99999999731989
target distance 7.0
model initialize at round 2298
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 28.]), 'previousTarget': array([21., 28.]), 'currentState': array([14.03085412, 24.94078803,  5.43462026]), 'targetState': array([21, 28], dtype=int32), 'currentDistance': 7.611029640142959}
done in step count: 16
reward sum = 0.7208419649320037
running average episode reward sum: 0.48038063442157775
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 28.]), 'previousTarget': array([21., 28.]), 'currentState': array([20.10872409, 27.02024161,  0.70914118]), 'targetState': array([21, 28], dtype=int32), 'currentDistance': 1.3244996271180032}
episode index:2299
target Thresh 31.999999997346556
target distance 17.0
model initialize at round 2299
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'previousTarget': array([11., 19.]), 'currentState': array([20.96244472,  1.98825291,  3.22041023]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 19.714204112143836}
done in step count: 47
reward sum = 0.3563367774667638
running average episode reward sum: 0.48032670230985824
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'previousTarget': array([11., 19.]), 'currentState': array([11.84961852, 18.37854309,  2.31954267]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 1.0526444384502827}
episode index:2300
target Thresh 31.99999999737296
target distance 9.0
model initialize at round 2300
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 12.]), 'previousTarget': array([ 3., 12.]), 'currentState': array([12.0003776 ,  6.96187268,  4.46979237]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 10.314529745084254}
done in step count: 26
reward sum = 0.5962762735910715
running average episode reward sum: 0.4803770932578292
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 12.]), 'previousTarget': array([ 3., 12.]), 'currentState': array([ 3.67507959, 11.47448191,  2.71239634]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 0.8555125514913637}
episode index:2301
target Thresh 31.999999997399097
target distance 8.0
model initialize at round 2301
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([26.01741821,  3.39147949,  1.75498113]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 9.247564874734502}
done in step count: 23
reward sum = 0.6550967407473801
running average episode reward sum: 0.4804529923227682
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([18.96269965,  7.82755056,  2.92332163]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 0.9780232267355787}
episode index:2302
target Thresh 31.99999999742498
target distance 3.0
model initialize at round 2302
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 19.]), 'previousTarget': array([13., 19.]), 'currentState': array([10.25513902, 16.08008687,  0.46063212]), 'targetState': array([13, 19], dtype=int32), 'currentDistance': 4.0075122560459615}
done in step count: 9
reward sum = 0.86239101860316
running average episode reward sum: 0.4806188360163333
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 19.]), 'previousTarget': array([13., 19.]), 'currentState': array([12.55971597, 18.16000866,  1.00927547]), 'targetState': array([13, 19], dtype=int32), 'currentDistance': 0.9483857206972688}
episode index:2303
target Thresh 31.9999999974506
target distance 11.0
model initialize at round 2303
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([ 5.16837641, 12.25070686,  0.9117668 ]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 12.262888781825653}
done in step count: 25
reward sum = 0.5833891396747294
running average episode reward sum: 0.48066344118285165
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([15.23820353, 17.42447037,  0.69205697]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 0.9547608162860307}
episode index:2304
target Thresh 31.999999997475967
target distance 5.0
model initialize at round 2304
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 20.]), 'previousTarget': array([19., 20.]), 'currentState': array([15.99422834, 14.97022879,  4.74760765]), 'targetState': array([19, 20], dtype=int32), 'currentDistance': 5.859459161830465}
done in step count: 17
reward sum = 0.718306398700488
running average episode reward sum: 0.4807665400798224
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 20.]), 'previousTarget': array([19., 20.]), 'currentState': array([18.95274885, 19.08300086,  1.44579989]), 'targetState': array([19, 20], dtype=int32), 'currentDistance': 0.9182157071606889}
episode index:2305
target Thresh 31.99999999750108
target distance 21.0
model initialize at round 2305
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.79898987,  8.82842712]), 'previousTarget': array([21.79898987,  8.82842712]), 'currentState': array([2.       , 6.       , 2.4663682]), 'targetState': array([23,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.28698649343941285
running average episode reward sum: 0.48068250710209454
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  9.]), 'previousTarget': array([23.,  9.]), 'currentState': array([22.00908042,  8.55749637,  0.25375698]), 'targetState': array([23,  9], dtype=int32), 'currentDistance': 1.0852331910688895}
episode index:2306
target Thresh 31.999999997525947
target distance 17.0
model initialize at round 2306
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([6.        , 6.        , 2.21155828]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 18.78829422805594}
done in step count: 58
reward sum = 0.36633371657401315
running average episode reward sum: 0.48063294108972865
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([13.78502029, 22.20498785,  1.5110284 ]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 0.823565781060651}
episode index:2307
target Thresh 31.999999997550564
target distance 17.0
model initialize at round 2307
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 28.]), 'previousTarget': array([23., 28.]), 'currentState': array([22.        , 11.        ,  2.79150128]), 'targetState': array([23, 28], dtype=int32), 'currentDistance': 17.0293863659264}
done in step count: 40
reward sum = 0.4153875312862321
running average episode reward sum: 0.48060467184804606
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 28.]), 'previousTarget': array([23., 28.]), 'currentState': array([23.15860887, 27.14717452,  1.93309139]), 'targetState': array([23, 28], dtype=int32), 'currentDistance': 0.8674491799090379}
episode index:2308
target Thresh 31.999999997574935
target distance 10.0
model initialize at round 2308
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  7.]), 'previousTarget': array([19.,  7.]), 'currentState': array([18.77794699, 16.86620709,  3.92984149]), 'targetState': array([19,  7], dtype=int32), 'currentDistance': 9.868705581251298}
done in step count: 22
reward sum = 0.6431829019369973
running average episode reward sum: 0.48067508251504004
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  7.]), 'previousTarget': array([19.,  7.]), 'currentState': array([18.48839781,  7.93934809,  4.88107446]), 'targetState': array([19,  7], dtype=int32), 'currentDistance': 1.0696315390645879}
episode index:2309
target Thresh 31.999999997599065
target distance 13.0
model initialize at round 2309
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 17.]), 'previousTarget': array([17., 17.]), 'currentState': array([7.05988001, 4.27703933, 1.2667354 ]), 'targetState': array([17, 17], dtype=int32), 'currentDistance': 16.14557876737244}
done in step count: 37
reward sum = 0.4669589499998038
running average episode reward sum: 0.4806691447953364
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 17.]), 'previousTarget': array([17., 17.]), 'currentState': array([16.45666924, 16.25550477,  1.24850402]), 'targetState': array([17, 17], dtype=int32), 'currentDistance': 0.9216731850599984}
episode index:2310
target Thresh 31.999999997622954
target distance 5.0
model initialize at round 2310
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([18.22194361,  3.42561254,  1.04803801]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 6.614744404889244}
done in step count: 13
reward sum = 0.7691742731382899
running average episode reward sum: 0.48079398474702095
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([22.6541986 ,  7.02126074,  0.6967997 ]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 1.0380313767836427}
episode index:2311
target Thresh 31.999999997646608
target distance 3.0
model initialize at round 2311
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 20.]), 'previousTarget': array([ 9., 20.]), 'currentState': array([ 6.23395955, 19.30411845,  0.76468551]), 'targetState': array([ 9, 20], dtype=int32), 'currentDistance': 2.8522326147160797}
done in step count: 5
reward sum = 0.9052631847981237
running average episode reward sum: 0.48097757869167973
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 20.]), 'previousTarget': array([ 9., 20.]), 'currentState': array([8.06812180e+00, 1.98439810e+01, 1.79035962e-02]), 'targetState': array([ 9, 20], dtype=int32), 'currentDistance': 0.9448486160087598}
episode index:2312
target Thresh 31.999999997670024
target distance 14.0
model initialize at round 2312
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 13.]), 'previousTarget': array([ 5., 13.]), 'currentState': array([18.97715546,  6.98354536,  3.55173475]), 'targetState': array([ 5, 13], dtype=int32), 'currentDistance': 15.2170496883417}
done in step count: 35
reward sum = 0.4744919330904701
running average episode reward sum: 0.4809747746944462
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 13.]), 'previousTarget': array([ 5., 13.]), 'currentState': array([ 5.91589411, 12.81817243,  3.19982631]), 'targetState': array([ 5, 13], dtype=int32), 'currentDistance': 0.9337683226466446}
episode index:2313
target Thresh 31.99999999769321
target distance 13.0
model initialize at round 2313
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 12.]), 'previousTarget': array([ 8., 12.]), 'currentState': array([ 5.13202951, 24.86880713,  5.38180806]), 'targetState': array([ 8, 12], dtype=int32), 'currentDistance': 13.184515599345302}
done in step count: 31
reward sum = 0.5483242235163104
running average episode reward sum: 0.4810038799013701
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 12.]), 'previousTarget': array([ 8., 12.]), 'currentState': array([ 7.54698357, 12.69287073,  5.24823416]), 'targetState': array([ 8, 12], dtype=int32), 'currentDistance': 0.8278247055339072}
episode index:2314
target Thresh 31.99999999771616
target distance 6.0
model initialize at round 2314
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 23.]), 'previousTarget': array([ 5., 23.]), 'currentState': array([ 6.96760237, 17.00833291,  2.72443378]), 'targetState': array([ 5, 23], dtype=int32), 'currentDistance': 6.306467598963858}
done in step count: 14
reward sum = 0.7697746004118893
running average episode reward sum: 0.4811286188735129
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 23.]), 'previousTarget': array([ 5., 23.]), 'currentState': array([ 5.22956055, 22.12313135,  1.79092529]), 'targetState': array([ 5, 23], dtype=int32), 'currentDistance': 0.9064196993198809}
episode index:2315
target Thresh 31.999999997738886
target distance 3.0
model initialize at round 2315
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([16.94214664, 11.24622603,  1.96666911]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 4.029826042004723}
done in step count: 8
reward sum = 0.8627282839202156
running average episode reward sum: 0.48129338556826534
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([14.83361798, 13.62611105,  2.65192538]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 0.9136256774764915}
episode index:2316
target Thresh 31.999999997761382
target distance 23.0
model initialize at round 2316
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.17554708, 17.45283714]), 'previousTarget': array([12.17554708, 17.45283714]), 'currentState': array([26.        ,  3.        ,  5.97486544]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.26427818302818523
running average episode reward sum: 0.4809716024139294
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 26.]), 'previousTarget': array([ 4., 26.]), 'currentState': array([ 7.27318147, 23.79070243,  2.37444743]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 3.9490141423168024}
episode index:2317
target Thresh 31.999999997783657
target distance 26.0
model initialize at round 2317
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.53392998, 21.94108971]), 'previousTarget': array([12.53392998, 21.94108971]), 'currentState': array([11.        ,  2.        ,  6.14159238]), 'targetState': array([13, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 70
reward sum = 0.1932781661611594
running average episode reward sum: 0.48084748962866075
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 28.]), 'previousTarget': array([13., 28.]), 'currentState': array([12.90594402, 27.12502932,  1.81702322]), 'targetState': array([13, 28], dtype=int32), 'currentDistance': 0.8800114891647094}
episode index:2318
target Thresh 31.99999999780571
target distance 22.0
model initialize at round 2318
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.94662591, 18.82074749]), 'previousTarget': array([19.87322975, 18.73750984]), 'currentState': array([3.01819176, 8.17048377, 1.34205206]), 'targetState': array([25, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 70
reward sum = 0.2203145397123577
running average episode reward sum: 0.4807351425178732
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 22.]), 'previousTarget': array([25., 22.]), 'currentState': array([24.06190633, 21.31609307,  0.98542498]), 'targetState': array([25, 22], dtype=int32), 'currentDistance': 1.1609256749076162}
episode index:2319
target Thresh 31.999999997827544
target distance 11.0
model initialize at round 2319
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 24.]), 'previousTarget': array([24., 24.]), 'currentState': array([21.        , 13.        ,  2.64878651]), 'targetState': array([24, 24], dtype=int32), 'currentDistance': 11.401754250991381}
done in step count: 29
reward sum = 0.5739998859713296
running average episode reward sum: 0.48077534283832724
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 24.]), 'previousTarget': array([24., 24.]), 'currentState': array([23.84230634, 23.22330626,  1.53316638]), 'targetState': array([24, 24], dtype=int32), 'currentDistance': 0.7925405057407512}
episode index:2320
target Thresh 31.999999997849162
target distance 14.0
model initialize at round 2320
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  5.]), 'previousTarget': array([17.,  5.]), 'currentState': array([3.24802157, 4.76118997, 5.72261441]), 'targetState': array([17,  5], dtype=int32), 'currentDistance': 13.754051796869467}
done in step count: 31
reward sum = 0.5210872018434568
running average episode reward sum: 0.48079271115327993
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  5.]), 'previousTarget': array([17.,  5.]), 'currentState': array([16.1180599 ,  5.10801688,  0.4510966 ]), 'targetState': array([17,  5], dtype=int32), 'currentDistance': 0.8885302373051049}
episode index:2321
target Thresh 31.99999999787056
target distance 16.0
model initialize at round 2321
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 23.]), 'previousTarget': array([24., 23.]), 'currentState': array([16.93791011,  7.01487309,  2.65398145]), 'targetState': array([24, 23], dtype=int32), 'currentDistance': 17.475622903411264}
done in step count: 42
reward sum = 0.3964698325201754
running average episode reward sum: 0.4807563963907333
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 23.]), 'previousTarget': array([24., 23.]), 'currentState': array([23.63280245, 22.19270138,  1.49829875]), 'targetState': array([24, 23], dtype=int32), 'currentDistance': 0.8868850553103004}
episode index:2322
target Thresh 31.99999999789175
target distance 17.0
model initialize at round 2322
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([12.85572726, 22.65028261,  4.2802642 ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 18.858839208483246}
done in step count: 53
reward sum = 0.38744955620702376
running average episode reward sum: 0.4807162298646103
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.41988166, 6.93484096, 4.55961126]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.0248064291138343}
episode index:2323
target Thresh 31.999999997912727
target distance 15.0
model initialize at round 2323
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 29.]), 'previousTarget': array([ 8., 29.]), 'currentState': array([18.01464609, 13.99429032,  6.13804114]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 18.040633561053912}
done in step count: 47
reward sum = 0.37506290661320096
running average episode reward sum: 0.480670768021559
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 29.]), 'previousTarget': array([ 8., 29.]), 'currentState': array([ 8.9496813 , 28.07584199,  2.39555168]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 1.3251273949085252}
episode index:2324
target Thresh 31.999999997933497
target distance 3.0
model initialize at round 2324
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([24.99769036,  8.01258103,  1.9509955 ]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 2.9977167576989645}
done in step count: 7
reward sum = 0.8769602987102402
running average episode reward sum: 0.48084121513153255
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([22.8297816 ,  8.44854196,  3.49720495]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 0.9432536214693473}
episode index:2325
target Thresh 31.99999999795406
target distance 19.0
model initialize at round 2325
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.13601924, 10.89888325]), 'previousTarget': array([15.13601924, 10.89888325]), 'currentState': array([27.        , 27.        ,  2.51581466]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.24638215102143934
running average episode reward sum: 0.4807404158778309
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([12.87309802,  8.85603379,  4.24757398]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 0.865388904643769}
episode index:2326
target Thresh 31.999999997974417
target distance 15.0
model initialize at round 2326
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 17.]), 'previousTarget': array([26., 17.]), 'currentState': array([11.30363463,  8.15269098,  0.43048046]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 17.153950911833725}
done in step count: 44
reward sum = 0.44405047432626
running average episode reward sum: 0.4807246488208685
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 17.]), 'previousTarget': array([26., 17.]), 'currentState': array([25.33546754, 16.17344658,  0.75244059]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 1.0605630323443769}
episode index:2327
target Thresh 31.99999999799457
target distance 11.0
model initialize at round 2327
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 28.]), 'previousTarget': array([ 7., 28.]), 'currentState': array([ 3.        , 17.        ,  5.23490655]), 'targetState': array([ 7, 28], dtype=int32), 'currentDistance': 11.704699910719626}
done in step count: 31
reward sum = 0.5419439201184135
running average episode reward sum: 0.4807509457587111
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 28.]), 'previousTarget': array([ 7., 28.]), 'currentState': array([ 6.93759041, 27.14172252,  1.56687647]), 'targetState': array([ 7, 28], dtype=int32), 'currentDistance': 0.8605435410352086}
episode index:2328
target Thresh 31.999999998014523
target distance 9.0
model initialize at round 2328
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  5.]), 'previousTarget': array([11.,  5.]), 'currentState': array([19.98513876, 11.7562624 ,  4.52059752]), 'targetState': array([11,  5], dtype=int32), 'currentDistance': 11.241877078667333}
done in step count: 25
reward sum = 0.6120987835559496
running average episode reward sum: 0.48080734242586315
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  5.]), 'previousTarget': array([11.,  5.]), 'currentState': array([11.51958878,  5.93523821,  3.80225347]), 'targetState': array([11,  5], dtype=int32), 'currentDistance': 1.0698799033375233}
episode index:2329
target Thresh 31.99999999803428
target distance 21.0
model initialize at round 2329
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.4686336 ,  5.61086196]), 'previousTarget': array([16.50557744,  5.76952105]), 'currentState': array([22.05543554, 24.81470647,  4.79647106]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.3174618367647833
running average episode reward sum: 0.48073723705862664
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.1481002 ,  4.87364151,  4.73708808]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.8861056078279694}
episode index:2330
target Thresh 31.99999999805384
target distance 11.0
model initialize at round 2330
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'previousTarget': array([ 5., 22.]), 'currentState': array([14.00125689, 10.97245212,  4.50548315]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 14.234796730506346}
done in step count: 38
reward sum = 0.4709054955320624
running average episode reward sum: 0.4807330192372939
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'previousTarget': array([ 5., 22.]), 'currentState': array([ 5.71414276, 21.09844645,  2.3840646 ]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 1.1501298570038938}
episode index:2331
target Thresh 31.999999998073203
target distance 16.0
model initialize at round 2331
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 11.]), 'previousTarget': array([23., 11.]), 'currentState': array([ 7.        , 13.        ,  2.85279224]), 'targetState': array([23, 11], dtype=int32), 'currentDistance': 16.1245154965971}
done in step count: 47
reward sum = 0.4051633985957528
running average episode reward sum: 0.48070061373959166
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 11.]), 'previousTarget': array([23., 11.]), 'currentState': array([22.00140719, 11.40231471,  0.18433032]), 'targetState': array([23, 11], dtype=int32), 'currentDistance': 1.076589389833879}
episode index:2332
target Thresh 31.999999998092377
target distance 13.0
model initialize at round 2332
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([16.99773061, 17.75528385,  4.78311399]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 13.103850107814361}
done in step count: 31
reward sum = 0.5528982527006021
running average episode reward sum: 0.4807315600057558
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([19.92058461,  5.8180419 ,  4.95783676]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 0.821887678943222}
episode index:2333
target Thresh 31.999999998111356
target distance 21.0
model initialize at round 2333
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7.40132839, 23.05721038]), 'previousTarget': array([ 7.40132839, 23.05721038]), 'currentState': array([16.        ,  5.        ,  3.10455352]), 'targetState': array([ 6, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.2780919923307074
running average episode reward sum: 0.4806447392826731
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 26.]), 'previousTarget': array([ 6., 26.]), 'currentState': array([ 6.64774112, 25.0766205 ,  2.14949732]), 'targetState': array([ 6, 26], dtype=int32), 'currentDistance': 1.1279176627483976}
episode index:2334
target Thresh 31.99999999813015
target distance 12.0
model initialize at round 2334
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 27.]), 'previousTarget': array([26., 27.]), 'currentState': array([19.19860776, 15.0555481 ,  0.41484864]), 'targetState': array([26, 27], dtype=int32), 'currentDistance': 13.74513977954551}
done in step count: 36
reward sum = 0.5219653167669236
running average episode reward sum: 0.4806624354614672
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 27.]), 'previousTarget': array([26., 27.]), 'currentState': array([25.49102051, 26.1794301 ,  1.37038037]), 'targetState': array([26, 27], dtype=int32), 'currentDistance': 0.9656060675368896}
episode index:2335
target Thresh 31.999999998148756
target distance 3.0
model initialize at round 2335
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 18.]), 'previousTarget': array([24., 18.]), 'currentState': array([22.        , 15.        ,  2.92739025]), 'targetState': array([24, 18], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 9
reward sum = 0.831631170625055
running average episode reward sum: 0.4808126789268626
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 18.]), 'previousTarget': array([24., 18.]), 'currentState': array([23.05909554, 17.44389425,  0.84445073]), 'targetState': array([24, 18], dtype=int32), 'currentDistance': 1.0929569114691875}
episode index:2336
target Thresh 31.999999998167176
target distance 24.0
model initialize at round 2336
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3.51875901, 6.19499601]), 'previousTarget': array([3.51930531, 6.15444247]), 'currentState': array([ 5.97329038, 26.04380645,  2.34267274]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 95
reward sum = 0.006063242610023889
running average episode reward sum: 0.48060953411029567
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.78077964, 2.89782466, 5.22619604]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.9242005670283724}
episode index:2337
target Thresh 31.999999998185412
target distance 17.0
model initialize at round 2337
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'previousTarget': array([19.,  3.]), 'currentState': array([23.81480502, 20.01023208,  3.27654588]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 17.67852773542258}
done in step count: 44
reward sum = 0.41192386003129655
running average episode reward sum: 0.4805801561487563
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'previousTarget': array([19.,  3.]), 'currentState': array([19.03595059,  3.95232953,  4.85296722]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 0.9530078576173102}
episode index:2338
target Thresh 31.999999998203467
target distance 12.0
model initialize at round 2338
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 12.]), 'previousTarget': array([ 6., 12.]), 'currentState': array([ 9.98270096, 24.0776114 ,  2.04260445]), 'targetState': array([ 6, 12], dtype=int32), 'currentDistance': 12.717334784102865}
done in step count: 32
reward sum = 0.49290897921886123
running average episode reward sum: 0.4805854271291197
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 12.]), 'previousTarget': array([ 6., 12.]), 'currentState': array([ 6.17555142, 12.78958885,  4.39623572]), 'targetState': array([ 6, 12], dtype=int32), 'currentDistance': 0.8088688753205568}
episode index:2339
target Thresh 31.999999998221345
target distance 7.0
model initialize at round 2339
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.1148056 , 12.55552582,  4.89250311]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 6.61501987027599}
done in step count: 13
reward sum = 0.767292351312987
running average episode reward sum: 0.48070795145569406
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.65086207,  6.82425042,  5.13712244]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.8951458252946041}
episode index:2340
target Thresh 31.99999999823904
target distance 24.0
model initialize at round 2340
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.71606109, 21.87886575]), 'previousTarget': array([19.71202025, 21.72787848]), 'currentState': array([23.13983119,  2.17409987,  1.09886688]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.24771904202743633
running average episode reward sum: 0.4806084260778947
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 26.]), 'previousTarget': array([19., 26.]), 'currentState': array([19.31294015, 25.02419426,  2.00943902]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 1.024757717364637}
episode index:2341
target Thresh 31.999999998256563
target distance 7.0
model initialize at round 2341
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 13.]), 'previousTarget': array([27., 13.]), 'currentState': array([20.        , 12.        ,  4.41029027]), 'targetState': array([27, 13], dtype=int32), 'currentDistance': 7.0710678118654755}
done in step count: 18
reward sum = 0.7057062959474396
running average episode reward sum: 0.48070453960046927
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 13.]), 'previousTarget': array([27., 13.]), 'currentState': array([26.01962815, 12.46576073,  0.51283364]), 'targetState': array([27, 13], dtype=int32), 'currentDistance': 1.1164858102192259}
episode index:2342
target Thresh 31.99999999827391
target distance 23.0
model initialize at round 2342
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5.43474237, 6.08423428]), 'previousTarget': array([5.4295875 , 6.11006407]), 'currentState': array([12.0633977 , 24.95381108,  5.41504779]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.2386018922191676
running average episode reward sum: 0.4806012094052574
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.41474088, 2.94228605, 4.87849927]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 1.029520761283511}
episode index:2343
target Thresh 31.999999998291084
target distance 9.0
model initialize at round 2343
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 21.]), 'previousTarget': array([27., 21.]), 'currentState': array([18.        , 17.        ,  4.85064173]), 'targetState': array([27, 21], dtype=int32), 'currentDistance': 9.848857801796104}
done in step count: 27
reward sum = 0.6016474457320014
running average episode reward sum: 0.4806528502910623
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 21.]), 'previousTarget': array([27., 21.]), 'currentState': array([26.57146709, 20.10563346,  0.82882874]), 'targetState': array([27, 21], dtype=int32), 'currentDistance': 0.9917318000113255}
episode index:2344
target Thresh 31.99999999830809
target distance 7.0
model initialize at round 2344
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 12.]), 'previousTarget': array([20., 12.]), 'currentState': array([24.13360414,  5.15240536,  1.07540351]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 7.998514580436368}
done in step count: 20
reward sum = 0.6854607471789789
running average episode reward sum: 0.48074018841340255
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 12.]), 'previousTarget': array([20., 12.]), 'currentState': array([20.16585861, 11.03815295,  2.63761746]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 0.976042431796789}
episode index:2345
target Thresh 31.999999998324924
target distance 10.0
model initialize at round 2345
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([14.13419954, 11.87424027,  5.62677786]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 10.04225051276201}
done in step count: 24
reward sum = 0.6492206148954797
running average episode reward sum: 0.48081200445197125
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([23.15092945, 10.10381802,  6.27349178]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 0.8553940470835498}
episode index:2346
target Thresh 31.999999998341593
target distance 12.0
model initialize at round 2346
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 28.]), 'previousTarget': array([21., 28.]), 'currentState': array([ 9.        , 24.        ,  5.28718182]), 'targetState': array([21, 28], dtype=int32), 'currentDistance': 12.649110640673516}
done in step count: 38
reward sum = 0.5319963589834683
running average episode reward sum: 0.48083381286889987
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 28.]), 'previousTarget': array([21., 28.]), 'currentState': array([20.16710961, 27.52812138,  0.69700965]), 'targetState': array([21, 28], dtype=int32), 'currentDistance': 0.9572752144335838}
episode index:2347
target Thresh 31.999999998358092
target distance 6.0
model initialize at round 2347
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  3.]), 'previousTarget': array([13.,  3.]), 'currentState': array([18.94162912,  3.06890124,  2.47563249]), 'targetState': array([13,  3], dtype=int32), 'currentDistance': 5.94202860887243}
done in step count: 15
reward sum = 0.7726215416142025
running average episode reward sum: 0.4809580836221985
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  3.]), 'previousTarget': array([13.,  3.]), 'currentState': array([13.96642052,  2.57904985,  3.43300532]), 'targetState': array([13,  3], dtype=int32), 'currentDistance': 1.0541193695982252}
episode index:2348
target Thresh 31.99999999837443
target distance 17.0
model initialize at round 2348
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.67141219,  8.46882734]), 'previousTarget': array([20.53366395,  8.66064273]), 'currentState': array([ 9.19258568, 24.84675584,  5.56967473]), 'targetState': array([21,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.31740325257432433
running average episode reward sum: 0.4808884561930594
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  8.]), 'previousTarget': array([21.,  8.]), 'currentState': array([20.74187223,  8.99510683,  5.65191214]), 'targetState': array([21,  8], dtype=int32), 'currentDistance': 1.028040637300634}
episode index:2349
target Thresh 31.999999998390606
target distance 16.0
model initialize at round 2349
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  6.]), 'previousTarget': array([20.,  6.]), 'currentState': array([4.        , 4.        , 1.61351198]), 'targetState': array([20,  6], dtype=int32), 'currentDistance': 16.1245154965971}
done in step count: 45
reward sum = 0.4373228114024237
running average episode reward sum: 0.4808699176208081
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  6.]), 'previousTarget': array([20.,  6.]), 'currentState': array([19.09715014,  5.92327366,  0.37012035]), 'targetState': array([20,  6], dtype=int32), 'currentDistance': 0.906104193062574}
episode index:2350
target Thresh 31.99999999840662
target distance 4.0
model initialize at round 2350
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 14.]), 'previousTarget': array([15., 14.]), 'currentState': array([19.        , 14.        ,  5.30543232]), 'targetState': array([15, 14], dtype=int32), 'currentDistance': 4.0}
done in step count: 11
reward sum = 0.7987523109272956
running average episode reward sum: 0.4810051291875059
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 14.]), 'previousTarget': array([15., 14.]), 'currentState': array([15.73775556, 13.51948423,  2.94453884]), 'targetState': array([15, 14], dtype=int32), 'currentDistance': 0.8804423144770711}
episode index:2351
target Thresh 31.999999998422474
target distance 17.0
model initialize at round 2351
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 28.]), 'previousTarget': array([15., 28.]), 'currentState': array([15.25570108, 11.17739854,  0.70503075]), 'targetState': array([15, 28], dtype=int32), 'currentDistance': 16.824544656474252}
done in step count: 43
reward sum = 0.4373592776884817
running average episode reward sum: 0.48098657227785496
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 28.]), 'previousTarget': array([15., 28.]), 'currentState': array([14.67758441, 27.05138491,  1.34091031]), 'targetState': array([15, 28], dtype=int32), 'currentDistance': 1.001909373203142}
episode index:2352
target Thresh 31.99999999843817
target distance 8.0
model initialize at round 2352
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  6.]), 'previousTarget': array([17.,  6.]), 'currentState': array([ 9.        , 13.        ,  3.36713699]), 'targetState': array([17,  6], dtype=int32), 'currentDistance': 10.630145812734648}
done in step count: 27
reward sum = 0.5795338044132857
running average episode reward sum: 0.4810284538044744
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  6.]), 'previousTarget': array([17.,  6.]), 'currentState': array([16.34810382,  6.54526427,  5.6251651 ]), 'targetState': array([17,  6], dtype=int32), 'currentDistance': 0.8498716092381153}
episode index:2353
target Thresh 31.99999999845371
target distance 13.0
model initialize at round 2353
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([ 3.93603053, 12.94976812,  4.05146921]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 14.844967758960477}
done in step count: 41
reward sum = 0.431953298567063
running average episode reward sum: 0.4810076062449003
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([16.66077366, 19.04788847,  0.72814369]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 1.0107377858902171}
episode index:2354
target Thresh 31.999999998469097
target distance 8.0
model initialize at round 2354
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 18.]), 'previousTarget': array([21., 18.]), 'currentState': array([21.0263783 , 25.56878272,  4.71629107]), 'targetState': array([21, 18], dtype=int32), 'currentDistance': 7.56882868462589}
done in step count: 16
reward sum = 0.7367242605119134
running average episode reward sum: 0.48111619081146806
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 18.]), 'previousTarget': array([21., 18.]), 'currentState': array([20.85518638, 18.86265584,  4.8919224 ]), 'targetState': array([21, 18], dtype=int32), 'currentDistance': 0.8747262865426145}
episode index:2355
target Thresh 31.99999999848433
target distance 13.0
model initialize at round 2355
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 25.]), 'previousTarget': array([ 2., 25.]), 'currentState': array([ 9.68940147, 12.23057367,  2.41880424]), 'targetState': array([ 2, 25], dtype=int32), 'currentDistance': 14.905876144644855}
done in step count: 36
reward sum = 0.49762422329349054
running average episode reward sum: 0.48112319761642647
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 25.]), 'previousTarget': array([ 2., 25.]), 'currentState': array([ 2.01912763, 24.20936813,  2.26225177]), 'targetState': array([ 2, 25], dtype=int32), 'currentDistance': 0.7908632138101185}
episode index:2356
target Thresh 31.999999998499412
target distance 18.0
model initialize at round 2356
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  3.]), 'previousTarget': array([26.,  3.]), 'currentState': array([25.07239566, 21.00930312,  6.17526558]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 18.033176334906205}
done in step count: 43
reward sum = 0.3904114992175558
running average episode reward sum: 0.48108471153310073
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  3.]), 'previousTarget': array([26.,  3.]), 'currentState': array([26.02326513,  3.84795941,  4.90645848]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 0.8482785041745308}
episode index:2357
target Thresh 31.99999999851434
target distance 4.0
model initialize at round 2357
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 22.]), 'previousTarget': array([ 8., 22.]), 'currentState': array([ 4.39608771, 19.23640934,  0.56751136]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 4.541543476102648}
done in step count: 11
reward sum = 0.8306035705413477
running average episode reward sum: 0.48123293836050035
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 22.]), 'previousTarget': array([ 8., 22.]), 'currentState': array([ 7.00664399, 21.86143754,  0.93852072]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 1.0029734405079738}
episode index:2358
target Thresh 31.999999998529123
target distance 12.0
model initialize at round 2358
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([19.1563921 , 24.87329471,  5.54016423]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 11.903226514862784}
done in step count: 30
reward sum = 0.577608072457001
running average episode reward sum: 0.4812737925928431
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([19.53175158, 13.77980079,  4.71368922]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 0.9095855440105751}
episode index:2359
target Thresh 31.99999999854376
target distance 1.0
model initialize at round 2359
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([17.        ,  9.        ,  0.39889646]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 1.4142135623730951}
done in step count: 2
reward sum = 0.9639864902365946
running average episode reward sum: 0.4814783318715056
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([17.1138393 ,  8.99457829,  6.1483689 ]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 1.3320911288520665}
episode index:2360
target Thresh 31.99999999855825
target distance 22.0
model initialize at round 2360
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.67564042, 18.42242262]), 'previousTarget': array([24.6773982 , 18.42229124]), 'currentState': array([ 4.99796184, 21.99858896,  3.99963903]), 'targetState': array([27, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.2536443641694081
running average episode reward sum: 0.4813818329440587
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 18.]), 'previousTarget': array([27., 18.]), 'currentState': array([2.61240704e+01, 1.73185934e+01, 3.77104919e-03]), 'targetState': array([27, 18], dtype=int32), 'currentDistance': 1.1097601526676952}
episode index:2361
target Thresh 31.999999998572594
target distance 25.0
model initialize at round 2361
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.86860529, 20.53279477]), 'previousTarget': array([13.77206738, 20.45012032]), 'currentState': array([4.18630035, 3.03270988, 0.31090784]), 'targetState': array([18, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 86
reward sum = 0.16179674245843517
running average episode reward sum: 0.4812465301961816
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 28.]), 'previousTarget': array([18., 28.]), 'currentState': array([17.84314198, 27.01380728,  1.2616174 ]), 'targetState': array([18, 28], dtype=int32), 'currentDistance': 0.9985892655551727}
episode index:2362
target Thresh 31.999999998586798
target distance 20.0
model initialize at round 2362
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6.20107272, 16.08544743]), 'previousTarget': array([ 6.23112767, 16.10023299]), 'currentState': array([22.95857319, 27.00268968,  3.17974393]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.27707169506756346
running average episode reward sum: 0.48116012527230156
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 14.]), 'previousTarget': array([ 3., 14.]), 'currentState': array([ 3.58450223, 14.80772356,  3.86079088]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 0.997025678423461}
episode index:2363
target Thresh 31.99999999860086
target distance 18.0
model initialize at round 2363
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9.92192837, 19.70180774]), 'previousTarget': array([10.19631201, 19.36442559]), 'currentState': array([22.75226221,  4.35962623,  2.18469029]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.295359330832294
running average episode reward sum: 0.4810815293355672
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 22.]), 'previousTarget': array([ 8., 22.]), 'currentState': array([ 8.54340837, 21.04063861,  2.47433735]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 1.1025728714581182}
episode index:2364
target Thresh 31.999999998614783
target distance 25.0
model initialize at round 2364
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.68335397, 20.22366736]), 'previousTarget': array([20.68609452, 20.23068683]), 'currentState': array([9.00007773, 3.99095174, 4.95380253]), 'targetState': array([27, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 90
reward sum = 0.08872358786385165
running average episode reward sum: 0.48091562745756644
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 29.]), 'previousTarget': array([27., 29.]), 'currentState': array([26.87011593, 28.01433301,  1.16065423]), 'targetState': array([27, 29], dtype=int32), 'currentDistance': 0.9941877543801296}
episode index:2365
target Thresh 31.999999998628564
target distance 5.0
model initialize at round 2365
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  6.]), 'previousTarget': array([23.,  6.]), 'currentState': array([18.07328099,  5.81980033,  5.32590783]), 'targetState': array([23,  6], dtype=int32), 'currentDistance': 4.930013401234661}
done in step count: 10
reward sum = 0.8164075021019463
running average episode reward sum: 0.4810574245305354
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  6.]), 'previousTarget': array([23.,  6.]), 'currentState': array([22.03600081,  5.90037776,  0.08176333]), 'targetState': array([23,  6], dtype=int32), 'currentDistance': 0.9691331358604063}
episode index:2366
target Thresh 31.99999999864221
target distance 15.0
model initialize at round 2366
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 21.]), 'previousTarget': array([ 6., 21.]), 'currentState': array([21.01074788, 10.96871451,  4.79079914]), 'targetState': array([ 6, 21], dtype=int32), 'currentDistance': 18.05406437619299}
done in step count: 43
reward sum = 0.3612531778920739
running average episode reward sum: 0.4810068101466577
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 21.]), 'previousTarget': array([ 6., 21.]), 'currentState': array([ 6.83058393, 20.56381801,  2.85475003]), 'targetState': array([ 6, 21], dtype=int32), 'currentDistance': 0.9381494481616376}
episode index:2367
target Thresh 31.99999999865572
target distance 19.0
model initialize at round 2367
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'previousTarget': array([27.,  8.]), 'currentState': array([27.        , 27.        ,  2.66068056]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 19.0}
done in step count: 53
reward sum = 0.34232129245499066
running average episode reward sum: 0.48094824362736216
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'previousTarget': array([27.,  8.]), 'currentState': array([26.84124992,  8.80678626,  4.91619979]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 0.8222564394323358}
episode index:2368
target Thresh 31.999999998669097
target distance 10.0
model initialize at round 2368
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 15.]), 'previousTarget': array([13., 15.]), 'currentState': array([20.04467718,  4.97390598,  5.99878854]), 'targetState': array([13, 15], dtype=int32), 'currentDistance': 12.253572457050394}
done in step count: 30
reward sum = 0.5184835384634378
running average episode reward sum: 0.4809640879898932
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 15.]), 'previousTarget': array([13., 15.]), 'currentState': array([13.30796313, 14.10771576,  2.46816491]), 'targetState': array([13, 15], dtype=int32), 'currentDistance': 0.9439345654315419}
episode index:2369
target Thresh 31.999999998682338
target distance 7.0
model initialize at round 2369
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 16.]), 'previousTarget': array([14., 16.]), 'currentState': array([8.00422339, 8.96880838, 5.09947205]), 'targetState': array([14, 16], dtype=int32), 'currentDistance': 9.240508251632287}
done in step count: 24
reward sum = 0.6188720010430617
running average episode reward sum: 0.48102227698274264
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 16.]), 'previousTarget': array([14., 16.]), 'currentState': array([13.01137995, 15.62814355,  1.01440845]), 'targetState': array([14, 16], dtype=int32), 'currentDistance': 1.0562418430839404}
episode index:2370
target Thresh 31.99999999869545
target distance 24.0
model initialize at round 2370
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.32879871, 23.02954034]), 'previousTarget': array([16.32455532, 22.97366596]), 'currentState': array([9.95489969, 4.07239363, 1.99441314]), 'targetState': array([18, 28], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 66
reward sum = 0.24661916379734478
running average episode reward sum: 0.48092341442973313
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 28.]), 'previousTarget': array([18., 28.]), 'currentState': array([17.37172148, 27.07871771,  1.405689  ]), 'targetState': array([18, 28], dtype=int32), 'currentDistance': 1.115121050568516}
episode index:2371
target Thresh 31.99999999870843
target distance 7.0
model initialize at round 2371
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([15.0297686 , 12.03472547,  0.61124825]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 8.613394366836973}
done in step count: 19
reward sum = 0.6682014656846972
running average episode reward sum: 0.48100236807697383
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([19.26573237,  5.89188164,  5.44193881]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 1.1552496748605943}
episode index:2372
target Thresh 31.999999998721282
target distance 11.0
model initialize at round 2372
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 15.]), 'previousTarget': array([10., 15.]), 'currentState': array([ 4.       , 26.       ,  3.3011016]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 12.529964086141668}
done in step count: 32
reward sum = 0.5260107436729083
running average episode reward sum: 0.48102133494406024
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 15.]), 'previousTarget': array([10., 15.]), 'currentState': array([ 9.79329485, 15.83406422,  5.1851599 ]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 0.8592963052189316}
episode index:2373
target Thresh 31.999999998734005
target distance 8.0
model initialize at round 2373
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 16.]), 'previousTarget': array([11., 16.]), 'currentState': array([ 3.26044609, 15.76296309,  5.67968038]), 'targetState': array([11, 16], dtype=int32), 'currentDistance': 7.743182887102374}
done in step count: 15
reward sum = 0.7303865213381926
running average episode reward sum: 0.48112637503942424
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 16.]), 'previousTarget': array([11., 16.]), 'currentState': array([10.0797116 , 15.63991235,  0.1665528 ]), 'targetState': array([11, 16], dtype=int32), 'currentDistance': 0.9882276300873555}
episode index:2374
target Thresh 31.999999998746603
target distance 17.0
model initialize at round 2374
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([4.17885812, 9.25915719, 0.8247436 ]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 16.910983050067443}
done in step count: 42
reward sum = 0.4395426871147885
running average episode reward sum: 0.48110886611819276
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([20.18846137, 10.88980614,  0.20473749]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 0.8189857314375267}
episode index:2375
target Thresh 31.999999998759073
target distance 12.0
model initialize at round 2375
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 29.]), 'previousTarget': array([ 4., 29.]), 'currentState': array([15.92304552, 27.30496943,  1.96310565]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 12.042929173028933}
done in step count: 26
reward sum = 0.5742162764458779
running average episode reward sum: 0.48114805273870104
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 29.]), 'previousTarget': array([ 4., 29.]), 'currentState': array([ 4.94145412, 29.4733333 ,  3.20490872]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 1.0537458262652655}
episode index:2376
target Thresh 31.999999998771422
target distance 8.0
model initialize at round 2376
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 10.]), 'previousTarget': array([18., 10.]), 'currentState': array([9.99475866, 4.01735789, 1.6264742 ]), 'targetState': array([18, 10], dtype=int32), 'currentDistance': 9.993792851469266}
done in step count: 25
reward sum = 0.6321712154165623
running average episode reward sum: 0.4812115879354524
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 10.]), 'previousTarget': array([18., 10.]), 'currentState': array([17.10544248,  9.95742804,  0.68287227]), 'targetState': array([18, 10], dtype=int32), 'currentDistance': 0.8955699447253141}
episode index:2377
target Thresh 31.999999998783647
target distance 8.0
model initialize at round 2377
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([15.9970743 , 12.00723511,  1.74250811]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 8.002928968455068}
done in step count: 20
reward sum = 0.675991458105104
running average episode reward sum: 0.48129349704822344
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([23.29100654, 12.59365618,  6.18098376]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 0.9247158412157271}
episode index:2378
target Thresh 31.999999998795747
target distance 4.0
model initialize at round 2378
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([ 9.91797856, 14.67784478,  4.70117253]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 5.49449189705685}
done in step count: 11
reward sum = 0.803674274667692
running average episode reward sum: 0.481429008093881
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.1572402 , 11.77801351,  5.69380406]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.146973890434644}
episode index:2379
target Thresh 31.99999999880773
target distance 10.0
model initialize at round 2379
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 24.]), 'previousTarget': array([14., 24.]), 'currentState': array([22.70780686, 14.0774651 ,  2.7880144 ]), 'targetState': array([14, 24], dtype=int32), 'currentDistance': 13.201613509063069}
done in step count: 26
reward sum = 0.563761068041474
running average episode reward sum: 0.48146360139638
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 24.]), 'previousTarget': array([14., 24.]), 'currentState': array([14.92829827, 23.11352958,  2.48409896]), 'targetState': array([14, 24], dtype=int32), 'currentDistance': 1.2835760583535127}
episode index:2380
target Thresh 31.999999998819593
target distance 15.0
model initialize at round 2380
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  8.]), 'previousTarget': array([12.,  8.]), 'currentState': array([27.        ,  4.        ,  3.84447712]), 'targetState': array([12,  8], dtype=int32), 'currentDistance': 15.524174696260024}
done in step count: 42
reward sum = 0.46434187084675094
running average episode reward sum: 0.4814564104133688
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  8.]), 'previousTarget': array([12.,  8.]), 'currentState': array([12.75677987,  7.40949104,  3.03493795]), 'targetState': array([12,  8], dtype=int32), 'currentDistance': 0.9599044766267438}
episode index:2381
target Thresh 31.99999999883134
target distance 18.0
model initialize at round 2381
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 27.]), 'previousTarget': array([27., 27.]), 'currentState': array([ 9.       , 22.       ,  2.6574457]), 'targetState': array([27, 27], dtype=int32), 'currentDistance': 18.681541692269406}
done in step count: 52
reward sum = 0.3453741906769684
running average episode reward sum: 0.48139928101801344
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 27.]), 'previousTarget': array([27., 27.]), 'currentState': array([26.1935172 , 26.56606412,  0.22405109]), 'targetState': array([27, 27], dtype=int32), 'currentDistance': 0.9158137648349547}
episode index:2382
target Thresh 31.999999998842966
target distance 19.0
model initialize at round 2382
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.69836445,  7.68507134]), 'previousTarget': array([21.69836445,  7.68507134]), 'currentState': array([ 4.       , 17.       ,  1.7482121]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.28781604163015745
running average episode reward sum: 0.4813180459196551
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([22.06477219,  7.43811827,  5.801834  ]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 1.0327626418716847}
episode index:2383
target Thresh 31.99999999885448
target distance 17.0
model initialize at round 2383
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([7.21402403, 7.86167084, 5.86043048]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 16.921626425308936}
done in step count: 44
reward sum = 0.4304070018926225
running average episode reward sum: 0.48129669061595254
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([23.21325314, 10.57559255,  0.04219021]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 0.9748217295097752}
episode index:2384
target Thresh 31.999999998865878
target distance 14.0
model initialize at round 2384
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 4.]), 'previousTarget': array([6., 4.]), 'currentState': array([19.94458499,  2.93710841,  3.79399747]), 'targetState': array([6, 4], dtype=int32), 'currentDistance': 13.985034471159434}
done in step count: 33
reward sum = 0.5228911766015782
running average episode reward sum: 0.48131413065200523
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 4.]), 'previousTarget': array([6., 4.]), 'currentState': array([6.73940789, 3.61507646, 3.1495154 ]), 'targetState': array([6, 4], dtype=int32), 'currentDistance': 0.8336007189635111}
episode index:2385
target Thresh 31.999999998877165
target distance 15.0
model initialize at round 2385
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'previousTarget': array([11., 21.]), 'currentState': array([25.68769907, 27.85377609,  3.55863645]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 16.20810756155356}
done in step count: 44
reward sum = 0.46852063217090884
running average episode reward sum: 0.48130876874987577
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'previousTarget': array([11., 21.]), 'currentState': array([11.91281706, 21.59027995,  3.65635661]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 1.0870443404499137}
episode index:2386
target Thresh 31.999999998888335
target distance 16.0
model initialize at round 2386
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 29.]), 'previousTarget': array([ 7., 29.]), 'currentState': array([ 3.11173468, 13.20512915,  1.12068635]), 'targetState': array([ 7, 29], dtype=int32), 'currentDistance': 16.26642407744611}
done in step count: 40
reward sum = 0.46610922268847294
running average episode reward sum: 0.4813024011143242
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 29.]), 'previousTarget': array([ 7., 29.]), 'currentState': array([ 7.52816357, 28.10682026,  1.34996423]), 'targetState': array([ 7, 29], dtype=int32), 'currentDistance': 1.037654473015102}
episode index:2387
target Thresh 31.999999998899398
target distance 13.0
model initialize at round 2387
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 29.]), 'previousTarget': array([12., 29.]), 'currentState': array([17.9836654 , 16.00078187,  2.84934258]), 'targetState': array([12, 29], dtype=int32), 'currentDistance': 14.31027335461014}
done in step count: 33
reward sum = 0.5131741654784764
running average episode reward sum: 0.48131574774931757
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 29.]), 'previousTarget': array([12., 29.]), 'currentState': array([12.52061283, 28.09498372,  1.98514856]), 'targetState': array([12, 29], dtype=int32), 'currentDistance': 1.0440747961823948}
episode index:2388
target Thresh 31.999999998910347
target distance 9.0
model initialize at round 2388
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([14.19466771, 26.6096006 ,  5.10136043]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 8.79684302306707}
done in step count: 21
reward sum = 0.6860425378758972
running average episode reward sum: 0.48140144335004026
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([15.96427418, 18.84436735,  5.14406741]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 0.8451228054631658}
episode index:2389
target Thresh 31.99999999892119
target distance 8.0
model initialize at round 2389
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'previousTarget': array([11., 21.]), 'currentState': array([ 3.        , 27.        ,  3.97018242]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 10.0}
done in step count: 25
reward sum = 0.617484942747326
running average episode reward sum: 0.48145838205271696
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'previousTarget': array([11., 21.]), 'currentState': array([10.18953483, 21.51129674,  5.80540245]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 0.9582683037547284}
episode index:2390
target Thresh 31.999999998931926
target distance 3.0
model initialize at round 2390
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 26.]), 'previousTarget': array([16., 26.]), 'currentState': array([18.        , 23.        ,  0.72773957]), 'targetState': array([16, 26], dtype=int32), 'currentDistance': 3.6055512754639896}
done in step count: 9
reward sum = 0.8478283022288757
running average episode reward sum: 0.48161161079390313
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 26.]), 'previousTarget': array([16., 26.]), 'currentState': array([16.77999542, 25.34775716,  2.38359985]), 'targetState': array([16, 26], dtype=int32), 'currentDistance': 1.0167662326042004}
episode index:2391
target Thresh 31.999999998942553
target distance 19.0
model initialize at round 2391
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8.11728508, 11.70177604]), 'previousTarget': array([ 8.11728508, 11.70177604]), 'currentState': array([21.        , 27.        ,  1.40306151]), 'targetState': array([5, 8], dtype=int32), 'currentDistance': 20.0}
done in step count: 71
reward sum = 0.20010669152523386
running average episode reward sum: 0.4814939247908644
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 8.]), 'previousTarget': array([5., 8.]), 'currentState': array([4.86605381, 8.90433335, 4.15963436]), 'targetState': array([5, 8], dtype=int32), 'currentDistance': 0.9141993150634726}
episode index:2392
target Thresh 31.999999998953076
target distance 7.0
model initialize at round 2392
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  2.]), 'previousTarget': array([17.,  2.]), 'currentState': array([16.22870531,  8.58330478,  5.11700979]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 6.628332925284776}
done in step count: 13
reward sum = 0.7691759910299433
running average episode reward sum: 0.4816141429547754
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  2.]), 'previousTarget': array([17.,  2.]), 'currentState': array([17.09469215,  2.91377944,  4.97489843]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 0.9186726679008834}
episode index:2393
target Thresh 31.999999998963492
target distance 22.0
model initialize at round 2393
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.02430127,  5.98519465]), 'previousTarget': array([15.05572809,  6.11145618]), 'currentState': array([24.08187865, 23.81663576,  5.07672249]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.2652295827184532
running average episode reward sum: 0.48152375675584624
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  2.]), 'previousTarget': array([13.,  2.]), 'currentState': array([13.66848321,  2.8810358 ,  4.2894593 ]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 1.1059357527084144}
episode index:2394
target Thresh 31.999999998973806
target distance 9.0
model initialize at round 2394
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 16.]), 'previousTarget': array([ 5., 16.]), 'currentState': array([7.02845638, 7.28926028, 1.55846579]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 8.943803541893402}
done in step count: 18
reward sum = 0.6937427235809482
running average episode reward sum: 0.48161236592779827
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 16.]), 'previousTarget': array([ 5., 16.]), 'currentState': array([ 5.3717522 , 15.2712349 ,  1.80083214]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 0.8181065135110963}
episode index:2395
target Thresh 31.999999998984016
target distance 3.0
model initialize at round 2395
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 16.]), 'previousTarget': array([25., 16.]), 'currentState': array([21.89545595, 18.72070272,  4.56196883]), 'targetState': array([25, 16], dtype=int32), 'currentDistance': 4.128004000238198}
done in step count: 9
reward sum = 0.8401024740096115
running average episode reward sum: 0.4817619861732414
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 16.]), 'previousTarget': array([25., 16.]), 'currentState': array([24.05938397, 16.19824559,  5.81562142]), 'targetState': array([25, 16], dtype=int32), 'currentDistance': 0.9612803103693164}
episode index:2396
target Thresh 31.999999998994124
target distance 19.0
model initialize at round 2396
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 18.]), 'previousTarget': array([ 3., 18.]), 'currentState': array([22.        , 18.        ,  4.28172019]), 'targetState': array([ 3, 18], dtype=int32), 'currentDistance': 19.0}
done in step count: 54
reward sum = 0.3646320510734587
running average episode reward sum: 0.48171312095209007
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 18.]), 'previousTarget': array([ 3., 18.]), 'currentState': array([ 3.76450412, 18.74298556,  3.18741622]), 'targetState': array([ 3, 18], dtype=int32), 'currentDistance': 1.0660647652876223}
episode index:2397
target Thresh 31.99999999900413
target distance 3.0
model initialize at round 2397
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  7.]), 'previousTarget': array([21.,  7.]), 'currentState': array([18.        ,  5.        ,  2.49045253]), 'targetState': array([21,  7], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 9
reward sum = 0.8267271481246493
running average episode reward sum: 0.48185699669319626
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  7.]), 'previousTarget': array([21.,  7.]), 'currentState': array([20.13081044,  6.81360907,  0.4234901 ]), 'targetState': array([21,  7], dtype=int32), 'currentDistance': 0.8889499767924816}
episode index:2398
target Thresh 31.999999999014044
target distance 16.0
model initialize at round 2398
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([ 8.02031736, 10.05562977,  0.98434263]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 16.097541202223468}
done in step count: 38
reward sum = 0.4588540143085692
running average episode reward sum: 0.48184740812196464
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([23.14519173, 11.93425574,  0.17188926]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 0.8573327764094713}
episode index:2399
target Thresh 31.999999999023853
target distance 23.0
model initialize at round 2399
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.44214014, 22.76277578]), 'previousTarget': array([10.4268235, 22.7042351]), 'currentState': array([7.04567064, 3.05328581, 1.02239347]), 'targetState': array([11, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.2802600548573941
running average episode reward sum: 0.4817634133914377
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 26.]), 'previousTarget': array([11., 26.]), 'currentState': array([10.94347807, 25.09451238,  1.45972835]), 'targetState': array([11, 26], dtype=int32), 'currentDistance': 0.9072500009822256}
episode index:2400
target Thresh 31.999999999033566
target distance 4.0
model initialize at round 2400
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([22.05591741, 14.37206993,  1.54100999]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 4.743470015332255}
done in step count: 9
reward sum = 0.8374867879844479
running average episode reward sum: 0.4819115697323761
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([19.91167563, 17.30846466,  2.53383514]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 1.144278628031421}
episode index:2401
target Thresh 31.999999999043183
target distance 23.0
model initialize at round 2401
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.62910995, 20.04268443]), 'previousTarget': array([23.62910995, 20.04268443]), 'currentState': array([15.        ,  2.        ,  2.10487843]), 'targetState': array([26, 25], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 68
reward sum = 0.22436509629802087
running average episode reward sum: 0.4818043480531778
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 25.]), 'previousTarget': array([26., 25.]), 'currentState': array([25.30119684, 24.035198  ,  0.79603818]), 'targetState': array([26, 25], dtype=int32), 'currentDistance': 1.1912886922786459}
episode index:2402
target Thresh 31.9999999990527
target distance 27.0
model initialize at round 2402
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.25221604,  9.00522476]), 'previousTarget': array([15.25976679,  9.01370332]), 'currentState': array([15.97182957, 28.99227448,  3.46610191]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.13230220779327825
running average episode reward sum: 0.4815487897694297
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([16.63427571,  9.50364311,  4.43302728]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 7.679551876213103}
episode index:2403
target Thresh 31.99999999906213
target distance 18.0
model initialize at round 2403
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 20.]), 'previousTarget': array([15., 20.]), 'currentState': array([22.0422659 ,  2.00616443,  0.3973276 ]), 'targetState': array([15, 20], dtype=int32), 'currentDistance': 19.322826593762784}
done in step count: 53
reward sum = 0.3345283924598317
running average episode reward sum: 0.4814876331981695
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 20.]), 'previousTarget': array([15., 20.]), 'currentState': array([15.88714007, 19.99754296,  2.19050776]), 'targetState': array([15, 20], dtype=int32), 'currentDistance': 0.8871434765135303}
episode index:2404
target Thresh 31.99999999907146
target distance 10.0
model initialize at round 2404
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 23.]), 'previousTarget': array([10., 23.]), 'currentState': array([19.63079093, 15.94101215,  3.15560532]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 11.940747185659369}
done in step count: 25
reward sum = 0.5694632442728336
running average episode reward sum: 0.4815242134938346
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 23.]), 'previousTarget': array([10., 23.]), 'currentState': array([10.96202242, 23.29597028,  2.688623  ]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 1.0065215096416438}
episode index:2405
target Thresh 31.9999999990807
target distance 6.0
model initialize at round 2405
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 15.]), 'previousTarget': array([24., 15.]), 'currentState': array([22.        ,  9.        ,  4.65538543]), 'targetState': array([24, 15], dtype=int32), 'currentDistance': 6.324555320336759}
done in step count: 17
reward sum = 0.6976291080904576
running average episode reward sum: 0.4816140326520211
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 15.]), 'previousTarget': array([24., 15.]), 'currentState': array([24.10628763, 14.22349296,  1.43293769]), 'targetState': array([24, 15], dtype=int32), 'currentDistance': 0.7837475599481668}
episode index:2406
target Thresh 31.999999999089848
target distance 13.0
model initialize at round 2406
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 15.]), 'previousTarget': array([ 5., 15.]), 'currentState': array([18.        ,  9.        ,  1.56325138]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 14.317821063276353}
done in step count: 33
reward sum = 0.4952949364910456
running average episode reward sum: 0.48161971645087404
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 15.]), 'previousTarget': array([ 5., 15.]), 'currentState': array([ 5.93090203, 15.35906775,  2.62101391]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 0.9977515881189436}
episode index:2407
target Thresh 31.999999999098904
target distance 8.0
model initialize at round 2407
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  3.]), 'previousTarget': array([21.,  3.]), 'currentState': array([12.99723642,  7.01219419,  1.54116249]), 'targetState': array([21,  3], dtype=int32), 'currentDistance': 8.95220236173428}
done in step count: 20
reward sum = 0.627333179025072
running average episode reward sum: 0.48168022868616234
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  3.]), 'previousTarget': array([21.,  3.]), 'currentState': array([20.1194466 ,  2.87399992,  5.94998553]), 'targetState': array([21,  3], dtype=int32), 'currentDistance': 0.8895225164672178}
episode index:2408
target Thresh 31.999999999107867
target distance 10.0
model initialize at round 2408
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([17.17276163, 16.68615487,  5.25394777]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 12.453403415715854}
done in step count: 27
reward sum = 0.5797555694411853
running average episode reward sum: 0.48172094074127025
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([24.79408803,  7.96829859,  5.67308044]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 0.9899504550913931}
episode index:2409
target Thresh 31.999999999116746
target distance 13.0
model initialize at round 2409
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 11.]), 'previousTarget': array([20., 11.]), 'currentState': array([ 6.97714182, 22.96320848,  4.40896249]), 'targetState': array([20, 11], dtype=int32), 'currentDistance': 17.68369848823531}
done in step count: 44
reward sum = 0.3923024158435969
running average episode reward sum: 0.48168383761890615
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 11.]), 'previousTarget': array([20., 11.]), 'currentState': array([19.02420884, 11.76759915,  5.23847034]), 'targetState': array([20, 11], dtype=int32), 'currentDistance': 1.2415219862458027}
episode index:2410
target Thresh 31.999999999125535
target distance 11.0
model initialize at round 2410
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 10.]), 'previousTarget': array([20., 10.]), 'currentState': array([ 9.14008225, 14.06965707,  0.24766168]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 11.597410154670268}
done in step count: 27
reward sum = 0.5941801805642435
running average episode reward sum: 0.4817304972385434
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 10.]), 'previousTarget': array([20., 10.]), 'currentState': array([19.16901405, 10.13194871,  6.08921637]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 0.8413965258012577}
episode index:2411
target Thresh 31.999999999134236
target distance 12.0
model initialize at round 2411
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  2.]), 'previousTarget': array([19.,  2.]), 'currentState': array([7.00087496, 5.00312854, 1.06446874]), 'targetState': array([19,  2], dtype=int32), 'currentDistance': 12.36922725429305}
done in step count: 30
reward sum = 0.5529190041969552
running average episode reward sum: 0.48176001154491094
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  2.]), 'previousTarget': array([19.,  2.]), 'currentState': array([18.17953506,  2.14064906,  6.08831961]), 'targetState': array([19,  2], dtype=int32), 'currentDistance': 0.8324331065364777}
episode index:2412
target Thresh 31.99999999914285
target distance 2.0
model initialize at round 2412
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([13.29013756,  2.71579271,  5.55724665]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 1.8536420251128127}
done in step count: 3
reward sum = 0.9485843786380214
running average episode reward sum: 0.4819534737774402
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.09140565,  2.06676808,  5.64004165]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.9110442726723518}
episode index:2413
target Thresh 31.999999999151377
target distance 18.0
model initialize at round 2413
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 21.]), 'previousTarget': array([ 5., 21.]), 'currentState': array([23.        , 28.        ,  2.13949346]), 'targetState': array([ 5, 21], dtype=int32), 'currentDistance': 19.313207915827967}
done in step count: 86
reward sum = 0.2719733627105486
running average episode reward sum: 0.4818664894729386
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 21.]), 'previousTarget': array([ 5., 21.]), 'currentState': array([ 5.91754979, 21.150214  ,  3.52468888]), 'targetState': array([ 5, 21], dtype=int32), 'currentDistance': 0.9297644153573207}
episode index:2414
target Thresh 31.999999999159822
target distance 1.0
model initialize at round 2414
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 18.]), 'previousTarget': array([ 7., 18.]), 'currentState': array([ 6.9983305 , 16.97771362,  4.64878185]), 'targetState': array([ 7, 18], dtype=int32), 'currentDistance': 1.0222877444097656}
done in step count: 13
reward sum = 0.8460998435789899
running average episode reward sum: 0.48201731073757875
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 18.]), 'previousTarget': array([ 7., 18.]), 'currentState': array([ 7.18723607, 17.11894926,  1.02538309]), 'targetState': array([ 7, 18], dtype=int32), 'currentDistance': 0.900726240542607}
episode index:2415
target Thresh 31.999999999168182
target distance 16.0
model initialize at round 2415
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.67920493, 18.3238709 ]), 'previousTarget': array([12.85786438, 18.14213562]), 'currentState': array([26.83429865,  4.19470525,  2.27908432]), 'targetState': array([11, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.025523399101951633
running average episode reward sum: 0.4818072359404597
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 20.]), 'previousTarget': array([11., 20.]), 'currentState': array([25.10119898,  8.11084665,  1.84339302]), 'targetState': array([11, 20], dtype=int32), 'currentDistance': 18.444396981177796}
episode index:2416
target Thresh 31.99999999917646
target distance 22.0
model initialize at round 2416
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.94427191, 11.11145618]), 'previousTarget': array([12.94427191, 11.11145618]), 'currentState': array([ 4.        , 29.        ,  4.51026238]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.0062273707598529
running average episode reward sum: 0.48160531843665316
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.52796861,  9.54435065]), 'previousTarget': array([13.52696805,  9.54643217]), 'currentState': array([ 3.51240943, 26.85586631,  4.70252026]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:2417
target Thresh 31.999999999184652
target distance 10.0
model initialize at round 2417
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 24.]), 'previousTarget': array([13., 24.]), 'currentState': array([10.880451  , 14.29354127,  1.79665482]), 'targetState': array([13, 24], dtype=int32), 'currentDistance': 9.935181377698761}
done in step count: 20
reward sum = 0.6558077477404238
running average episode reward sum: 0.48167736245208076
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 24.]), 'previousTarget': array([13., 24.]), 'currentState': array([13.03770341, 23.07519091,  1.39055513]), 'targetState': array([13, 24], dtype=int32), 'currentDistance': 0.9255773357559108}
episode index:2418
target Thresh 31.999999999192767
target distance 11.0
model initialize at round 2418
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 12.]), 'previousTarget': array([27., 12.]), 'currentState': array([16.14120822,  4.3401777 ,  1.02560604]), 'targetState': array([27, 12], dtype=int32), 'currentDistance': 13.288575413054298}
done in step count: 27
reward sum = 0.5512816601478295
running average episode reward sum: 0.48170613644864785
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 12.]), 'previousTarget': array([27., 12.]), 'currentState': array([26.10562379, 11.41274237,  0.75185035]), 'targetState': array([27, 12], dtype=int32), 'currentDistance': 1.0699440792094563}
episode index:2419
target Thresh 31.9999999992008
target distance 18.0
model initialize at round 2419
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.48314552, 13.28714138]), 'previousTarget': array([19.48314552, 13.28714138]), 'currentState': array([ 2.        , 23.        ,  0.98244211]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 76
reward sum = 0.2688206718736331
running average episode reward sum: 0.48161816724841017
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([19.10955242, 13.13406605,  5.77744245]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 0.9004835404488023}
episode index:2420
target Thresh 31.99999999920875
target distance 2.0
model initialize at round 2420
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.        , 8.        , 2.93314567]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 1.9999999999999998}
done in step count: 6
reward sum = 0.8973819501278084
running average episode reward sum: 0.48178989950073536
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.44912678, 6.93900585, 4.64313926]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 1.0886658329400096}
episode index:2421
target Thresh 31.999999999216623
target distance 11.0
model initialize at round 2421
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([11.02754223, 18.98348657,  5.4905839 ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 13.039299334243257}
done in step count: 32
reward sum = 0.5287321008760304
running average episode reward sum: 0.48180928108676985
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.38871423, 8.72657528, 4.32511362]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.8240208644455068}
episode index:2422
target Thresh 31.999999999224418
target distance 2.0
model initialize at round 2422
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 13.]), 'previousTarget': array([ 3., 13.]), 'currentState': array([ 3.98047441, 11.39760565,  1.77539036]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 1.878562677905234}
done in step count: 2
reward sum = 0.9552259385417573
running average episode reward sum: 0.48200466559252925
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 13.]), 'previousTarget': array([ 3., 13.]), 'currentState': array([ 3.68444632, 12.09852504,  2.17066726]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 1.131867428464085}
episode index:2423
target Thresh 31.999999999232134
target distance 14.0
model initialize at round 2423
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 18.]), 'previousTarget': array([10., 18.]), 'currentState': array([14.        ,  4.        ,  3.01629344]), 'targetState': array([10, 18], dtype=int32), 'currentDistance': 14.560219778561038}
done in step count: 39
reward sum = 0.487792564520534
running average episode reward sum: 0.48200705333961175
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 18.]), 'previousTarget': array([10., 18.]), 'currentState': array([10.28358831, 17.22165203,  2.01086908]), 'targetState': array([10, 18], dtype=int32), 'currentDistance': 0.8284008007574857}
episode index:2424
target Thresh 31.999999999239776
target distance 17.0
model initialize at round 2424
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 17.]), 'previousTarget': array([19., 17.]), 'currentState': array([ 2.38239702, 23.85864891,  5.93176675]), 'targetState': array([19, 17], dtype=int32), 'currentDistance': 17.977368931211448}
done in step count: 44
reward sum = 0.41458620095458687
running average episode reward sum: 0.4819792509262571
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 17.]), 'previousTarget': array([19., 17.]), 'currentState': array([18.29189439, 17.3395795 ,  6.25352377]), 'targetState': array([19, 17], dtype=int32), 'currentDistance': 0.7853201857986383}
episode index:2425
target Thresh 31.99999999924734
target distance 16.0
model initialize at round 2425
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 19.]), 'previousTarget': array([27., 19.]), 'currentState': array([11.        , 12.        ,  5.41837549]), 'targetState': array([27, 19], dtype=int32), 'currentDistance': 17.46424919657298}
done in step count: 43
reward sum = 0.4101617610461442
running average episode reward sum: 0.48194964767403936
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 19.]), 'previousTarget': array([27., 19.]), 'currentState': array([26.10842871, 18.51152759,  0.77686676]), 'targetState': array([27, 19], dtype=int32), 'currentDistance': 1.0166143183829868}
episode index:2426
target Thresh 31.99999999925483
target distance 6.0
model initialize at round 2426
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 18.]), 'previousTarget': array([26., 18.]), 'currentState': array([25.92554648, 12.49423517,  1.67229894]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 5.50626822263649}
done in step count: 10
reward sum = 0.807800466512789
running average episode reward sum: 0.4820839084152173
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 18.]), 'previousTarget': array([26., 18.]), 'currentState': array([26.0147107 , 17.20820402,  1.65296845]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 0.791932619336816}
episode index:2427
target Thresh 31.999999999262243
target distance 7.0
model initialize at round 2427
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 16.]), 'previousTarget': array([10., 16.]), 'currentState': array([16.80576295, 14.25694922,  2.34414642]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 7.0254277714598485}
done in step count: 14
reward sum = 0.752346536784207
running average episode reward sum: 0.48219521921767566
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 16.]), 'previousTarget': array([10., 16.]), 'currentState': array([10.8657003 , 16.07360681,  3.00402936]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 0.8688239001775678}
episode index:2428
target Thresh 31.999999999269583
target distance 20.0
model initialize at round 2428
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.85617884, 27.43150312]), 'previousTarget': array([24.8507125, 27.40285  ]), 'currentState': array([19.95101392,  8.04234746,  2.3272098 ]), 'targetState': array([25, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.335842064866781
running average episode reward sum: 0.4821349667869013
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 28.]), 'previousTarget': array([25., 28.]), 'currentState': array([24.70925958, 27.18070073,  1.25116423]), 'targetState': array([25, 28], dtype=int32), 'currentDistance': 0.8693568229511637}
episode index:2429
target Thresh 31.999999999276852
target distance 12.0
model initialize at round 2429
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 20.]), 'previousTarget': array([13., 20.]), 'currentState': array([24.99978409, 24.99686115,  4.45509914]), 'targetState': array([13, 20], dtype=int32), 'currentDistance': 12.998593755260028}
done in step count: 30
reward sum = 0.558400612205548
running average episode reward sum: 0.48216635182616824
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 20.]), 'previousTarget': array([13., 20.]), 'currentState': array([13.98081563, 20.73540639,  3.47024304]), 'targetState': array([13, 20], dtype=int32), 'currentDistance': 1.2258963475704943}
episode index:2430
target Thresh 31.999999999284046
target distance 14.0
model initialize at round 2430
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 17.]), 'previousTarget': array([24., 17.]), 'currentState': array([9.97698969, 3.9882155 , 3.36240506]), 'targetState': array([24, 17], dtype=int32), 'currentDistance': 19.12985504924624}
done in step count: 50
reward sum = 0.33800050225786804
running average episode reward sum: 0.48210704872062804
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 17.]), 'previousTarget': array([24., 17.]), 'currentState': array([23.0098531 , 16.19719746,  0.67087956]), 'targetState': array([24, 17], dtype=int32), 'currentDistance': 1.2747089095134503}
episode index:2431
target Thresh 31.999999999291173
target distance 8.0
model initialize at round 2431
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 24.]), 'previousTarget': array([22., 24.]), 'currentState': array([24.70097506, 16.35692528,  2.20201991]), 'targetState': array([22, 24], dtype=int32), 'currentDistance': 8.10628505895737}
done in step count: 16
reward sum = 0.7180259354410875
running average episode reward sum: 0.48220405484181245
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 24.]), 'previousTarget': array([22., 24.]), 'currentState': array([22.10884278, 23.03390607,  1.873525  ]), 'targetState': array([22, 24], dtype=int32), 'currentDistance': 0.9722058551709933}
episode index:2432
target Thresh 31.999999999298225
target distance 12.0
model initialize at round 2432
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 25.]), 'previousTarget': array([ 5., 25.]), 'currentState': array([16.80119314, 25.19121182,  2.52928543]), 'targetState': array([ 5, 25], dtype=int32), 'currentDistance': 11.802742118908887}
done in step count: 25
reward sum = 0.591562322800725
running average episode reward sum: 0.48224900275301624
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 25.]), 'previousTarget': array([ 5., 25.]), 'currentState': array([ 5.81946033, 24.92218148,  3.27153258]), 'targetState': array([ 5, 25], dtype=int32), 'currentDistance': 0.8231469792415509}
episode index:2433
target Thresh 31.999999999305206
target distance 6.0
model initialize at round 2433
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 12.]), 'previousTarget': array([13., 12.]), 'currentState': array([13.03188593, 18.02110783,  0.3322506 ]), 'targetState': array([13, 12], dtype=int32), 'currentDistance': 6.021192257944202}
done in step count: 14
reward sum = 0.7407415257388634
running average episode reward sum: 0.4823552034608987
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 12.]), 'previousTarget': array([13., 12.]), 'currentState': array([13.50090388, 12.82331159,  4.63636562]), 'targetState': array([13, 12], dtype=int32), 'currentDistance': 0.9637150340359366}
episode index:2434
target Thresh 31.99999999931212
target distance 4.0
model initialize at round 2434
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([ 3.20939581, 28.98046894,  6.03918109]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 4.861234896103379}
done in step count: 10
reward sum = 0.823387145509697
running average episode reward sum: 0.48249525764654505
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([ 5.85514202, 25.85031487,  5.14117071]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 0.862565485131363}
episode index:2435
target Thresh 31.999999999318966
target distance 9.0
model initialize at round 2435
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 16.]), 'previousTarget': array([14., 16.]), 'currentState': array([7.01755343e+00, 6.99553192e+00, 3.25125059e-03]), 'targetState': array([14, 16], dtype=int32), 'currentDistance': 11.394516469675608}
done in step count: 27
reward sum = 0.5911717041663853
running average episode reward sum: 0.48253987030932
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 16.]), 'previousTarget': array([14., 16.]), 'currentState': array([13.76432797, 15.16795332,  1.10832412]), 'targetState': array([14, 16], dtype=int32), 'currentDistance': 0.8647791548253864}
episode index:2436
target Thresh 31.99999999932574
target distance 12.0
model initialize at round 2436
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  7.]), 'previousTarget': array([13.,  7.]), 'currentState': array([25.       ,  3.       ,  3.6217345]), 'targetState': array([13,  7], dtype=int32), 'currentDistance': 12.649110640673518}
done in step count: 31
reward sum = 0.5566291726302741
running average episode reward sum: 0.482570272156805
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  7.]), 'previousTarget': array([13.,  7.]), 'currentState': array([13.76215096,  6.64300559,  2.9950372 ]), 'targetState': array([13,  7], dtype=int32), 'currentDistance': 0.8416169479803155}
episode index:2437
target Thresh 31.999999999332452
target distance 12.0
model initialize at round 2437
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 15.]), 'previousTarget': array([ 2., 15.]), 'currentState': array([12.80684698, 26.79149437,  4.04822046]), 'targetState': array([ 2, 15], dtype=int32), 'currentDistance': 15.994601621535395}
done in step count: 38
reward sum = 0.4764440465330106
running average episode reward sum: 0.48256775934891993
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 15.]), 'previousTarget': array([ 2., 15.]), 'currentState': array([ 2.8757105 , 15.52038368,  3.72167837]), 'targetState': array([ 2, 15], dtype=int32), 'currentDistance': 1.0186599340444598}
episode index:2438
target Thresh 31.999999999339092
target distance 6.0
model initialize at round 2438
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([5.92551062, 6.98632732, 3.10204738]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 6.399554585466915}
done in step count: 17
reward sum = 0.6938142532403652
running average episode reward sum: 0.48265437127753474
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([11.1693716 ,  8.99698735,  0.29220675]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.8306338683255781}
episode index:2439
target Thresh 31.99999999934567
target distance 12.0
model initialize at round 2439
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 14.]), 'previousTarget': array([19., 14.]), 'currentState': array([27.        ,  2.        ,  4.74393266]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 14.422205101855958}
done in step count: 34
reward sum = 0.4638675893052935
running average episode reward sum: 0.4826466717767264
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 14.]), 'previousTarget': array([19., 14.]), 'currentState': array([19.65927287, 13.05599258,  2.03288146]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 1.1514298604211997}
episode index:2440
target Thresh 31.99999999935218
target distance 13.0
model initialize at round 2440
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([13.98946118, 16.01228247,  2.53243804]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 13.616310684150706}
done in step count: 34
reward sum = 0.46734545090497426
running average episode reward sum: 0.4826404033535917
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([17.54643834,  3.84985686,  5.51600215]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 0.963314523316378}
episode index:2441
target Thresh 31.999999999358625
target distance 6.0
model initialize at round 2441
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 22.]), 'previousTarget': array([27., 22.]), 'currentState': array([26.92531736, 16.05879748,  2.23358446]), 'targetState': array([27, 22], dtype=int32), 'currentDistance': 5.941671895694598}
done in step count: 12
reward sum = 0.7842168369288496
running average episode reward sum: 0.48276389902663647
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 22.]), 'previousTarget': array([27., 22.]), 'currentState': array([26.66235325, 21.16476513,  1.48499362]), 'targetState': array([27, 22], dtype=int32), 'currentDistance': 0.9009010075494793}
episode index:2442
target Thresh 31.999999999365006
target distance 12.0
model initialize at round 2442
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 29.]), 'previousTarget': array([12., 29.]), 'currentState': array([24.        , 26.        ,  5.01026773]), 'targetState': array([12, 29], dtype=int32), 'currentDistance': 12.369316876852983}
done in step count: 32
reward sum = 0.525692754812638
running average episode reward sum: 0.48278147121484194
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 29.]), 'previousTarget': array([12., 29.]), 'currentState': array([12.80420793, 28.89172727,  3.00340691]), 'targetState': array([12, 29], dtype=int32), 'currentDistance': 0.8114637266275572}
episode index:2443
target Thresh 31.999999999371326
target distance 18.0
model initialize at round 2443
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.82921668, 27.30288226]), 'previousTarget': array([19.78704435, 27.27881227]), 'currentState': array([ 4.07293205, 14.98462368,  6.17526063]), 'targetState': array([22, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 97
reward sum = 0.2064000891789994
running average episode reward sum: 0.4826683855429778
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 29.]), 'previousTarget': array([22., 29.]), 'currentState': array([21.37689738, 28.13478002,  0.9794619 ]), 'targetState': array([22, 29], dtype=int32), 'currentDistance': 1.0662375429259685}
episode index:2444
target Thresh 31.999999999377582
target distance 21.0
model initialize at round 2444
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.82842712,  5.20101013]), 'previousTarget': array([13.82842712,  5.20101013]), 'currentState': array([11.        , 25.        ,  2.43631569]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 85
reward sum = 0.21660471072558704
running average episode reward sum: 0.48255956604407496
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.33916305,  4.8639452 ,  4.84754247]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.9281340887703073}
episode index:2445
target Thresh 31.999999999383775
target distance 9.0
model initialize at round 2445
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 16.]), 'previousTarget': array([15., 16.]), 'currentState': array([22.        , 25.        ,  2.05326664]), 'targetState': array([15, 16], dtype=int32), 'currentDistance': 11.40175425099138}
done in step count: 25
reward sum = 0.5747124868389164
running average episode reward sum: 0.48259724099125195
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 16.]), 'previousTarget': array([15., 16.]), 'currentState': array([15.85699711, 16.90146517,  4.23477397]), 'targetState': array([15, 16], dtype=int32), 'currentDistance': 1.2438181172324916}
episode index:2446
target Thresh 31.999999999389907
target distance 2.0
model initialize at round 2446
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([13.99376692,  6.04992631,  1.45656011]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 2.006854206212469}
done in step count: 5
reward sum = 0.9073378944526366
running average episode reward sum: 0.4827708170654086
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.05290464,  6.39595274,  0.10124491]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.0265321195730863}
episode index:2447
target Thresh 31.999999999395975
target distance 7.0
model initialize at round 2447
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([9.54223231, 7.9546336 , 3.16374536]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 6.625224110041932}
done in step count: 13
reward sum = 0.7710428932370367
running average episode reward sum: 0.4828885752664591
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.8664598 , 8.41730541, 3.05722353]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.0441673998384216}
episode index:2448
target Thresh 31.999999999401986
target distance 6.0
model initialize at round 2448
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  3.]), 'previousTarget': array([22.,  3.]), 'currentState': array([18.        ,  9.        ,  1.69106936]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 7.211102550927977}
done in step count: 19
reward sum = 0.6650310285942843
running average episode reward sum: 0.48296294948178287
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  3.]), 'previousTarget': array([22.,  3.]), 'currentState': array([22.03595779,  3.76625589,  5.38421311]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 0.7670991191281894}
episode index:2449
target Thresh 31.999999999407937
target distance 21.0
model initialize at round 2449
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6.58107741, 22.9734288 ]), 'previousTarget': array([ 7.02263725, 22.95130299]), 'currentState': array([26.56020012, 22.05983395,  3.04748935]), 'targetState': array([ 6, 23], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 56
reward sum = 0.3334342296133291
running average episode reward sum: 0.48290191735122434
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 23.]), 'previousTarget': array([ 6., 23.]), 'currentState': array([ 6.81712414, 23.23961317,  2.90273718]), 'targetState': array([ 6, 23], dtype=int32), 'currentDistance': 0.8515317583075033}
episode index:2450
target Thresh 31.999999999413827
target distance 13.0
model initialize at round 2450
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 22.]), 'previousTarget': array([23., 22.]), 'currentState': array([10.01160947, 27.03688575,  1.01337087]), 'targetState': array([23, 22], dtype=int32), 'currentDistance': 13.930847305402594}
done in step count: 30
reward sum = 0.5055227936662996
running average episode reward sum: 0.482911146594927
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 22.]), 'previousTarget': array([23., 22.]), 'currentState': array([22.2634611 , 22.68058138,  5.74729328]), 'targetState': array([23, 22], dtype=int32), 'currentDistance': 1.0028362595525515}
episode index:2451
target Thresh 31.99999999941966
target distance 13.0
model initialize at round 2451
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 17.]), 'previousTarget': array([14., 17.]), 'currentState': array([27.        , 22.        ,  0.35107294]), 'targetState': array([14, 17], dtype=int32), 'currentDistance': 13.928388277184121}
done in step count: 35
reward sum = 0.46351716216977323
running average episode reward sum: 0.48290323713961497
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 17.]), 'previousTarget': array([14., 17.]), 'currentState': array([14.98811084, 17.30759144,  3.55050013]), 'targetState': array([14, 17], dtype=int32), 'currentDistance': 1.034879476553277}
episode index:2452
target Thresh 31.999999999425434
target distance 5.0
model initialize at round 2452
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 6.]), 'previousTarget': array([7., 6.]), 'currentState': array([11.89199147, 10.71826984,  4.22608288]), 'targetState': array([7, 6], dtype=int32), 'currentDistance': 6.796591119533417}
done in step count: 12
reward sum = 0.7827765424620349
running average episode reward sum: 0.48302548471618345
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 6.]), 'previousTarget': array([7., 6.]), 'currentState': array([7.94814434, 6.95403665, 4.08072785]), 'targetState': array([7, 6], dtype=int32), 'currentDistance': 1.3450515319568521}
episode index:2453
target Thresh 31.99999999943115
target distance 6.0
model initialize at round 2453
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  3.]), 'previousTarget': array([26.,  3.]), 'currentState': array([25.8588412 ,  8.79084366,  4.25324616]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 5.792563862933266}
done in step count: 11
reward sum = 0.7991115661011402
running average episode reward sum: 0.4831542891503256
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  3.]), 'previousTarget': array([26.,  3.]), 'currentState': array([26.30702568,  3.91273116,  4.92172171]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 0.9629864683565899}
episode index:2454
target Thresh 31.999999999436813
target distance 19.0
model initialize at round 2454
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  6.]), 'previousTarget': array([13.,  6.]), 'currentState': array([10.03914616, 24.9943375 ,  5.90945671]), 'targetState': array([13,  6], dtype=int32), 'currentDistance': 19.223722647436524}
done in step count: 51
reward sum = 0.3685632364820616
running average episode reward sum: 0.48310761255046075
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  6.]), 'previousTarget': array([13.,  6.]), 'currentState': array([13.43117642,  6.84863392,  4.91976493]), 'targetState': array([13,  6], dtype=int32), 'currentDistance': 0.9518889809399497}
episode index:2455
target Thresh 31.999999999442416
target distance 9.0
model initialize at round 2455
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([23.00297571, 16.96038686,  4.55283257]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 9.867727500581458}
done in step count: 23
reward sum = 0.6235959831068114
running average episode reward sum: 0.4831648146557361
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([14.99585261, 20.40040525,  2.87238446]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 1.1624268923661307}
episode index:2456
target Thresh 31.999999999447965
target distance 3.0
model initialize at round 2456
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  9.]), 'previousTarget': array([20.,  9.]), 'currentState': array([22.07531073,  5.97112766,  6.16959143]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 3.671645719031694}
done in step count: 10
reward sum = 0.8196122968714568
running average episode reward sum: 0.4833017489179321
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  9.]), 'previousTarget': array([20.,  9.]), 'currentState': array([20.89077131,  8.05764303,  2.55224427]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 1.296730572890623}
episode index:2457
target Thresh 31.999999999453458
target distance 20.0
model initialize at round 2457
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.679134 , 15.4036941]), 'previousTarget': array([20.61737619, 15.50609905]), 'currentState': array([ 4.96819666, 27.77973741,  4.81578985]), 'targetState': array([25, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 71
reward sum = 0.23008974635652085
running average episode reward sum: 0.4831987334571667
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 12.]), 'previousTarget': array([25., 12.]), 'currentState': array([24.04180163, 12.46474742,  5.64277711]), 'targetState': array([25, 12], dtype=int32), 'currentDistance': 1.0649574085815048}
episode index:2458
target Thresh 31.999999999458893
target distance 19.0
model initialize at round 2458
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 22.]), 'previousTarget': array([23., 22.]), 'currentState': array([22.02379325,  2.96825323,  5.60290694]), 'targetState': array([23, 22], dtype=int32), 'currentDistance': 19.056766900563964}
done in step count: 54
reward sum = 0.3383054499705577
running average episode reward sum: 0.48313980979572435
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 22.]), 'previousTarget': array([23., 22.]), 'currentState': array([23.41717436, 21.05077931,  1.88021032]), 'targetState': array([23, 22], dtype=int32), 'currentDistance': 1.0368482812373114}
episode index:2459
target Thresh 31.99999999946428
target distance 10.0
model initialize at round 2459
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 17.]), 'previousTarget': array([15., 17.]), 'currentState': array([ 5.        , 13.        ,  1.97935653]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 10.770329614269007}
done in step count: 23
reward sum = 0.5953090195755188
running average episode reward sum: 0.48318540703547225
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 17.]), 'previousTarget': array([15., 17.]), 'currentState': array([14.07251663, 16.41039038,  0.6149488 ]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 1.0990290694936955}
episode index:2460
target Thresh 31.99999999946961
target distance 8.0
model initialize at round 2460
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([10.00126389,  9.00158886,  1.15132588]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 8.946112965079845}
done in step count: 21
reward sum = 0.6259084545837355
running average episode reward sum: 0.4832434009597096
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.84815173, 5.63288488, 3.93608549]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 1.058255460684427}
episode index:2461
target Thresh 31.999999999474888
target distance 14.0
model initialize at round 2461
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  3.]), 'previousTarget': array([10.,  3.]), 'currentState': array([17.05446589, 16.81900825,  4.81881458]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 15.515491546483712}
done in step count: 39
reward sum = 0.47623644427718154
running average episode reward sum: 0.4832405549171903
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  3.]), 'previousTarget': array([10.,  3.]), 'currentState': array([10.71797901,  3.78794548,  4.45361471]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 1.0659980920488556}
episode index:2462
target Thresh 31.999999999480114
target distance 16.0
model initialize at round 2462
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 9.        , 26.        ,  6.27470246]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 16.0312195418814}
done in step count: 44
reward sum = 0.42637570448955353
running average episode reward sum: 0.4832174672799887
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 8.14210247, 10.80458736,  5.05169829]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.8170397332448891}
episode index:2463
target Thresh 31.999999999485286
target distance 17.0
model initialize at round 2463
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([18.99739311, 13.02692917,  1.919801  ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 19.245696214457144}
done in step count: 50
reward sum = 0.34705844360264715
running average episode reward sum: 0.4831622079359638
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.75890285, 4.56078495, 3.91007503]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.9436171373513027}
episode index:2464
target Thresh 31.999999999490406
target distance 13.0
model initialize at round 2464
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([12.00077899, 12.98809139,  5.03021002]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 13.604233074509153}
done in step count: 36
reward sum = 0.4976391732779827
running average episode reward sum: 0.48316808094421615
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([24.10554753, 16.90968781,  0.74870213]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 0.8990002879717611}
episode index:2465
target Thresh 31.999999999495476
target distance 17.0
model initialize at round 2465
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'previousTarget': array([16., 23.]), 'currentState': array([11.        ,  6.        ,  2.92664513]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 17.720045146669353}
done in step count: 49
reward sum = 0.3751169307467507
running average episode reward sum: 0.4831242645816057
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'previousTarget': array([16., 23.]), 'currentState': array([16.25015244, 22.02310943,  1.1828654 ]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 1.0084103503579853}
episode index:2466
target Thresh 31.999999999500496
target distance 20.0
model initialize at round 2466
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.13893185, 28.74304585]), 'previousTarget': array([24.1565257 , 28.74695771]), 'currentState': array([ 4.97405681, 23.02399308,  2.14272666]), 'targetState': array([25, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.4065677408106418
running average episode reward sum: 0.48276362736823225
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 29.]), 'previousTarget': array([25., 29.]), 'currentState': array([ 5.96142228, 29.38307287,  0.92981815]), 'targetState': array([25, 29], dtype=int32), 'currentDistance': 19.042431212214705}
episode index:2467
target Thresh 31.999999999505466
target distance 15.0
model initialize at round 2467
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([ 8.        , 24.        ,  1.37170613]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 17.0}
done in step count: 64
reward sum = 0.3214134214679539
running average episode reward sum: 0.4826982504614655
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.76639257,  9.9123693 ,  5.08909986]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.9418015547558235}
episode index:2468
target Thresh 31.999999999510386
target distance 17.0
model initialize at round 2468
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.98094527, 17.21089953]), 'previousTarget': array([23.99675711, 17.23243274]), 'currentState': array([9.01210691, 3.94689267, 5.13308591]), 'targetState': array([26, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.2463695764825219
running average episode reward sum: 0.48260253208399323
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 19.]), 'previousTarget': array([26., 19.]), 'currentState': array([25.511719  , 18.03322753,  1.23604756]), 'targetState': array([26, 19], dtype=int32), 'currentDistance': 1.0830823348838035}
episode index:2469
target Thresh 31.99999999951526
target distance 12.0
model initialize at round 2469
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 29.]), 'previousTarget': array([ 7., 29.]), 'currentState': array([15.05582113, 17.32080213,  1.54319954]), 'targetState': array([ 7, 29], dtype=int32), 'currentDistance': 14.188020194950196}
done in step count: 36
reward sum = 0.5025690056503043
running average episode reward sum: 0.4826106156765302
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 29.]), 'previousTarget': array([ 7., 29.]), 'currentState': array([ 7.48656057, 28.03549779,  2.47894641]), 'targetState': array([ 7, 29], dtype=int32), 'currentDistance': 1.0802803827365466}
episode index:2470
target Thresh 31.99999999952008
target distance 17.0
model initialize at round 2470
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 20.]), 'previousTarget': array([18., 20.]), 'currentState': array([24.        ,  3.        ,  3.15801668]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 18.02775637731995}
done in step count: 47
reward sum = 0.3842476738450786
running average episode reward sum: 0.482570808739326
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 20.]), 'previousTarget': array([18., 20.]), 'currentState': array([18.18635235, 19.00863473,  2.23288208]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 1.0087280613895964}
episode index:2471
target Thresh 31.999999999524857
target distance 11.0
model initialize at round 2471
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 3.]), 'previousTarget': array([5., 3.]), 'currentState': array([ 7.        , 14.        ,  0.96155223]), 'targetState': array([5, 3], dtype=int32), 'currentDistance': 11.180339887498949}
done in step count: 28
reward sum = 0.53895950497634
running average episode reward sum: 0.482593619700587
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 3.]), 'previousTarget': array([5., 3.]), 'currentState': array([5.53010925, 3.84547354, 4.78239189]), 'targetState': array([5, 3], dtype=int32), 'currentDistance': 0.9979184956050734}
episode index:2472
target Thresh 31.999999999529585
target distance 22.0
model initialize at round 2472
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9.82541376, 21.21853056]), 'previousTarget': array([ 9.82541376, 21.21853056]), 'currentState': array([20.        ,  4.        ,  0.31726425]), 'targetState': array([ 7, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 81
reward sum = 0.16714140473448552
running average episode reward sum: 0.48246606118260643
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 26.]), 'previousTarget': array([ 7., 26.]), 'currentState': array([ 7.74037799, 25.10546673,  2.67594497]), 'targetState': array([ 7, 26], dtype=int32), 'currentDistance': 1.1611844575092019}
episode index:2473
target Thresh 31.999999999534268
target distance 20.0
model initialize at round 2473
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.64909937, 17.0714863 ]), 'previousTarget': array([21.61161351, 17.0776773 ]), 'currentState': array([ 2.05163984, 21.06392665,  0.85460226]), 'targetState': array([22, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.33507425048339995
running average episode reward sum: 0.4824064848646196
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 17.]), 'previousTarget': array([22., 17.]), 'currentState': array([21.09746093, 17.44814954,  6.2663014 ]), 'targetState': array([22, 17], dtype=int32), 'currentDistance': 1.007677919415625}
episode index:2474
target Thresh 31.9999999995389
target distance 2.0
model initialize at round 2474
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  5.]), 'previousTarget': array([11.,  5.]), 'currentState': array([13.        ,  3.        ,  5.23932153]), 'targetState': array([11,  5], dtype=int32), 'currentDistance': 2.82842712474619}
done in step count: 10
reward sum = 0.8201878064798969
running average episode reward sum: 0.4825429621662824
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  5.]), 'previousTarget': array([11.,  5.]), 'currentState': array([11.22167043,  4.07275225,  2.30991734]), 'targetState': array([11,  5], dtype=int32), 'currentDistance': 0.9533761979803635}
episode index:2475
target Thresh 31.999999999543487
target distance 9.0
model initialize at round 2475
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([18.65170925, 23.19033789,  2.60932083]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 10.421336131476952}
done in step count: 22
reward sum = 0.6517445441599066
running average episode reward sum: 0.48261129883106174
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([10.79295504, 28.13233332,  2.78546885]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 1.175424675109085}
episode index:2476
target Thresh 31.99999999954803
target distance 15.0
model initialize at round 2476
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 12.]), 'previousTarget': array([11., 12.]), 'currentState': array([18.67622611, 26.85370928,  3.75604573]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 16.71996192348809}
done in step count: 44
reward sum = 0.440267350257576
running average episode reward sum: 0.4825942039789933
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 12.]), 'previousTarget': array([11., 12.]), 'currentState': array([11.32054682, 12.98224448,  4.59308409]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 1.0332252782638291}
episode index:2477
target Thresh 31.99999999955253
target distance 10.0
model initialize at round 2477
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 20.]), 'previousTarget': array([15., 20.]), 'currentState': array([ 5.19628644, 24.63875568,  5.35256729]), 'targetState': array([15, 20], dtype=int32), 'currentDistance': 10.845775846871502}
done in step count: 22
reward sum = 0.6150191383984133
running average episode reward sum: 0.482647644226943
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 20.]), 'previousTarget': array([15., 20.]), 'currentState': array([14.0863512 , 19.73415796,  6.17992415]), 'targetState': array([15, 20], dtype=int32), 'currentDistance': 0.9515388161884809}
episode index:2478
target Thresh 31.99999999955698
target distance 10.0
model initialize at round 2478
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 14.]), 'previousTarget': array([12., 14.]), 'currentState': array([ 6.92187937, 23.65287115,  4.63730189]), 'targetState': array([12, 14], dtype=int32), 'currentDistance': 10.907118347690846}
done in step count: 22
reward sum = 0.6229247245974342
running average episode reward sum: 0.48270423038280036
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 14.]), 'previousTarget': array([12., 14.]), 'currentState': array([11.03634199, 14.655177  ,  5.12124134]), 'targetState': array([12, 14], dtype=int32), 'currentDistance': 1.1652869448086065}
episode index:2479
target Thresh 31.99999999956139
target distance 19.0
model initialize at round 2479
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 19.]), 'previousTarget': array([ 3., 19.]), 'currentState': array([21.75281793, 16.97991262,  3.11119479]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 18.86130784154381}
done in step count: 47
reward sum = 0.38410182082067224
running average episode reward sum: 0.4826644713466866
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 19.]), 'previousTarget': array([ 3., 19.]), 'currentState': array([ 3.7941046 , 19.288649  ,  3.29041477]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 0.8449380834595988}
episode index:2480
target Thresh 31.999999999565752
target distance 8.0
model initialize at round 2480
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 20.]), 'previousTarget': array([22., 20.]), 'currentState': array([26.77800541, 28.03238244,  3.16775265]), 'targetState': array([22, 20], dtype=int32), 'currentDistance': 9.346042126092804}
done in step count: 21
reward sum = 0.6502692809266184
running average episode reward sum: 0.48273202669113635
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 20.]), 'previousTarget': array([22., 20.]), 'currentState': array([22.00367016, 20.74068658,  4.59462488]), 'targetState': array([22, 20], dtype=int32), 'currentDistance': 0.7406956777395716}
episode index:2481
target Thresh 31.999999999570075
target distance 8.0
model initialize at round 2481
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 26.]), 'previousTarget': array([20., 26.]), 'currentState': array([15.97892933, 18.03360513,  1.89713457]), 'targetState': array([20, 26], dtype=int32), 'currentDistance': 8.92370195495393}
done in step count: 17
reward sum = 0.685797442543969
running average episode reward sum: 0.482813841927177
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 26.]), 'previousTarget': array([20., 26.]), 'currentState': array([19.57451959, 25.03921064,  1.05540999]), 'targetState': array([20, 26], dtype=int32), 'currentDistance': 1.0507853177072703}
episode index:2482
target Thresh 31.999999999574353
target distance 20.0
model initialize at round 2482
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.96728002, 22.78259482]), 'previousTarget': array([11.96680906, 22.77872706]), 'currentState': array([8.99075531, 3.00532755, 2.36629534]), 'targetState': array([12, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.34229048846993243
running average episode reward sum: 0.4827572477453577
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 23.]), 'previousTarget': array([12., 23.]), 'currentState': array([11.64400905, 22.02372331,  1.5280916 ]), 'targetState': array([12, 23], dtype=int32), 'currentDistance': 1.0391562599263318}
episode index:2483
target Thresh 31.999999999578588
target distance 23.0
model initialize at round 2483
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.38016237, 10.25117121]), 'previousTarget': array([16.28798697, 10.37514441]), 'currentState': array([ 9.25893579, 28.94042298,  6.08074918]), 'targetState': array([18,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.19702919540079278
running average episode reward sum: 0.4826422203490838
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  6.]), 'previousTarget': array([18.,  6.]), 'currentState': array([17.86491088,  6.88288456,  4.86459599]), 'targetState': array([18,  6], dtype=int32), 'currentDistance': 0.8931596766590852}
episode index:2484
target Thresh 31.99999999958278
target distance 6.0
model initialize at round 2484
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 21.]), 'previousTarget': array([27., 21.]), 'currentState': array([20.94351356, 17.03480997,  2.33680773]), 'targetState': array([27, 21], dtype=int32), 'currentDistance': 7.239044138480566}
done in step count: 16
reward sum = 0.704069289167474
running average episode reward sum: 0.4827313258093728
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 21.]), 'previousTarget': array([27., 21.]), 'currentState': array([26.09847228, 20.68592817,  0.40412997]), 'targetState': array([27, 21], dtype=int32), 'currentDistance': 0.9546692355355266}
episode index:2485
target Thresh 31.999999999586933
target distance 12.0
model initialize at round 2485
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  2.]), 'previousTarget': array([18.,  2.]), 'currentState': array([6.        , 2.        , 1.36143181]), 'targetState': array([18,  2], dtype=int32), 'currentDistance': 11.999999999999998}
done in step count: 29
reward sum = 0.5637816891866155
running average episode reward sum: 0.48276392852995903
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  2.]), 'previousTarget': array([18.,  2.]), 'currentState': array([17.00725532,  1.87788871,  0.1823846 ]), 'targetState': array([18,  2], dtype=int32), 'currentDistance': 1.0002265536458954}
episode index:2486
target Thresh 31.99999999959104
target distance 8.0
model initialize at round 2486
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  4.]), 'previousTarget': array([18.,  4.]), 'currentState': array([25.64458035,  4.17689068,  2.82130229]), 'targetState': array([18,  4], dtype=int32), 'currentDistance': 7.646626642614516}
done in step count: 15
reward sum = 0.7305157932898353
running average episode reward sum: 0.48286354729343306
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  4.]), 'previousTarget': array([18.,  4.]), 'currentState': array([18.78535101,  4.17377342,  3.19669688]), 'targetState': array([18,  4], dtype=int32), 'currentDistance': 0.8043465709769365}
episode index:2487
target Thresh 31.99999999959511
target distance 10.0
model initialize at round 2487
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 11.]), 'previousTarget': array([19., 11.]), 'currentState': array([9.        , 8.        , 4.32366776]), 'targetState': array([19, 11], dtype=int32), 'currentDistance': 10.440306508910549}
done in step count: 24
reward sum = 0.5907836087572098
running average episode reward sum: 0.48290692352392495
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 11.]), 'previousTarget': array([19., 11.]), 'currentState': array([18.14265911, 10.05142248,  0.3877655 ]), 'targetState': array([19, 11], dtype=int32), 'currentDistance': 1.2786057718777533}
episode index:2488
target Thresh 31.99999999959914
target distance 17.0
model initialize at round 2488
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([ 4.97518842, 21.99558637,  3.5245384 ]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 18.39015861208044}
done in step count: 54
reward sum = 0.2847572585976496
running average episode reward sum: 0.48282731337329166
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([11.05834829,  4.8873381 ,  5.8656913 ]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 0.9483673572069258}
episode index:2489
target Thresh 31.99999999960313
target distance 6.0
model initialize at round 2489
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([18.92847516, 13.06325189,  2.1650002 ]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 6.684253359263451}
done in step count: 13
reward sum = 0.7453742070008323
running average episode reward sum: 0.48293275389282075
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([21.79706172, 18.03337149,  1.17886533]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 0.9877016857201236}
episode index:2490
target Thresh 31.999999999607077
target distance 7.0
model initialize at round 2490
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 28.]), 'previousTarget': array([ 9., 28.]), 'currentState': array([ 2.41525898, 28.88012417,  6.08875306]), 'targetState': array([ 9, 28], dtype=int32), 'currentDistance': 6.643299848252957}
done in step count: 14
reward sum = 0.7707560618843797
running average episode reward sum: 0.48304829917904785
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 28.]), 'previousTarget': array([ 9., 28.]), 'currentState': array([ 8.09249037, 27.99298969,  6.23840282]), 'targetState': array([ 9, 28], dtype=int32), 'currentDistance': 0.9075367076996216}
episode index:2491
target Thresh 31.999999999610985
target distance 18.0
model initialize at round 2491
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.11445457,  5.88883127]), 'previousTarget': array([19.14213562,  5.85786438]), 'currentState': array([ 4.97829738, 20.0369428 ,  1.85317749]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.17974168737288765
running average episode reward sum: 0.4829265870555301
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  2.]), 'previousTarget': array([23.,  2.]), 'currentState': array([22.08945366,  2.72790058,  5.47672103]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 1.1657331979040269}
episode index:2492
target Thresh 31.999999999614857
target distance 18.0
model initialize at round 2492
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.39001864,  3.13579485]), 'previousTarget': array([19.21358457,  3.29018892]), 'currentState': array([ 3.04749328, 14.66496936,  4.81365547]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.24829943246105973
running average episode reward sum: 0.48283247267342233
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  2.]), 'previousTarget': array([21.,  2.]), 'currentState': array([20.06657261,  2.0776136 ,  0.03163942]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 0.9366485850225179}
episode index:2493
target Thresh 31.99999999961869
target distance 21.0
model initialize at round 2493
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8.71751776, 8.36315403]), 'previousTarget': array([8.72533058, 8.37523613]), 'currentState': array([25.00279571, 19.97305163,  4.7187974 ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.06864872544726647
running average episode reward sum: 0.4826113494985544
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5.99434134, 5.95972756]), 'previousTarget': array([5.99678893, 5.9615884 ]), 'currentState': array([24.01617875, 14.63229215,  4.70378596]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 20.0}
episode index:2494
target Thresh 31.999999999622485
target distance 7.0
model initialize at round 2494
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([26.        ,  6.        ,  4.39249694]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 19
reward sum = 0.6708579943109033
running average episode reward sum: 0.48268679905559336
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([19.96053621, 10.07866795,  2.5868552 ]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 0.9637522844175066}
episode index:2495
target Thresh 31.99999999962624
target distance 12.0
model initialize at round 2495
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 4.]), 'previousTarget': array([8., 4.]), 'currentState': array([19.71039647, 16.0788705 ,  3.04973605]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 16.8235697179155}
done in step count: 46
reward sum = 0.3918297671816997
running average episode reward sum: 0.4826503980011567
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 4.]), 'previousTarget': array([8., 4.]), 'currentState': array([8.16264654, 4.81438072, 4.49420986]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 0.8304636370251268}
episode index:2496
target Thresh 31.99999999962996
target distance 5.0
model initialize at round 2496
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'previousTarget': array([16., 23.]), 'currentState': array([21.        , 22.        ,  4.08315131]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 5.0990195135927845}
done in step count: 10
reward sum = 0.8029140049954051
running average episode reward sum: 0.4827786573551793
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'previousTarget': array([16., 23.]), 'currentState': array([16.9735199 , 22.96951521,  2.82498393]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 0.9739970847062737}
episode index:2497
target Thresh 31.99999999963364
target distance 6.0
model initialize at round 2497
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 19.]), 'previousTarget': array([ 5., 19.]), 'currentState': array([ 6.95856958, 25.02603158,  2.83310914]), 'targetState': array([ 5, 19], dtype=int32), 'currentDistance': 6.336327914127153}
done in step count: 15
reward sum = 0.7408690720139343
running average episode reward sum: 0.48288197617609957
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 19.]), 'previousTarget': array([ 5., 19.]), 'currentState': array([ 4.92719253, 19.94749994,  4.60437134]), 'targetState': array([ 5, 19], dtype=int32), 'currentDistance': 0.9502931508271809}
episode index:2498
target Thresh 31.999999999637286
target distance 12.0
model initialize at round 2498
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 16.]), 'previousTarget': array([ 8., 16.]), 'currentState': array([ 7.        , 28.        ,  3.01143003]), 'targetState': array([ 8, 16], dtype=int32), 'currentDistance': 12.041594578792294}
done in step count: 31
reward sum = 0.5382966870997877
running average episode reward sum: 0.4829041509303707
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 16.]), 'previousTarget': array([ 8., 16.]), 'currentState': array([ 7.24137021, 16.74160662,  4.77915094]), 'targetState': array([ 8, 16], dtype=int32), 'currentDistance': 1.0608956236341702}
episode index:2499
target Thresh 31.999999999640895
target distance 19.0
model initialize at round 2499
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8.23313766, 10.91410718]), 'currentState': array([26.64068049,  3.95934818,  3.13730383]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 19.926006802504975}
done in step count: 49
reward sum = 0.33550364642479463
running average episode reward sum: 0.48284519072856846
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.99218029, 10.72567048,  2.87018026]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.0294068272642158}
episode index:2500
target Thresh 31.99999999964447
target distance 6.0
model initialize at round 2500
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 12.]), 'previousTarget': array([17., 12.]), 'currentState': array([13.08389126, 18.01382009,  6.19645915]), 'targetState': array([17, 12], dtype=int32), 'currentDistance': 7.176485190884299}
done in step count: 13
reward sum = 0.7341429831968606
running average episode reward sum: 0.48294566965398555
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 12.]), 'previousTarget': array([17., 12.]), 'currentState': array([16.463729  , 12.77423598,  5.34656122]), 'targetState': array([17, 12], dtype=int32), 'currentDistance': 0.9418216010793143}
episode index:2501
target Thresh 31.999999999648008
target distance 2.0
model initialize at round 2501
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([ 5.03182564, 10.02602949,  0.48239988]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 2.8693398129545074}
done in step count: 10
reward sum = 0.8208168314276097
running average episode reward sum: 0.4830807100863492
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.90492847, 8.43890865, 3.74022915]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.005751627879036}
episode index:2502
target Thresh 31.999999999651507
target distance 6.0
model initialize at round 2502
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([12.        , 23.        ,  2.23777741]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 7.810249675906653}
done in step count: 17
reward sum = 0.6994711086629837
running average episode reward sum: 0.48316716250287994
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([ 6.85937175, 18.9363342 ,  3.97234937]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 1.270921531230764}
episode index:2503
target Thresh 31.999999999654975
target distance 9.0
model initialize at round 2503
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 13.]), 'previousTarget': array([15., 13.]), 'currentState': array([ 6.00115318, 15.00283695,  0.93451011]), 'targetState': array([15, 13], dtype=int32), 'currentDistance': 9.219034655976674}
done in step count: 19
reward sum = 0.6550070454852233
running average episode reward sum: 0.4832357886542308
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 13.]), 'previousTarget': array([15., 13.]), 'currentState': array([14.04849404, 13.12894604,  6.10828461]), 'targetState': array([15, 13], dtype=int32), 'currentDistance': 0.9602034491089191}
episode index:2504
target Thresh 31.99999999965841
target distance 4.0
model initialize at round 2504
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([12.        , 12.        ,  5.55838871]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 4.123105625617661}
done in step count: 11
reward sum = 0.7925736131788006
running average episode reward sum: 0.48335927680773355
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.81667834, 10.71083306,  3.56640263]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.8663608008315918}
episode index:2505
target Thresh 31.999999999661807
target distance 17.0
model initialize at round 2505
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 15.]), 'previousTarget': array([24., 15.]), 'currentState': array([ 6.98098663, 16.98820521,  3.94933534]), 'targetState': array([24, 15], dtype=int32), 'currentDistance': 17.13475346147673}
done in step count: 40
reward sum = 0.3659575109224949
running average episode reward sum: 0.48331242853722867
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 15.]), 'previousTarget': array([24., 15.]), 'currentState': array([23.09704705, 15.62869397,  6.27803131]), 'targetState': array([24, 15], dtype=int32), 'currentDistance': 1.1002636633956522}
episode index:2506
target Thresh 31.999999999665174
target distance 11.0
model initialize at round 2506
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([4.16265405, 3.85346082, 5.8023591 ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 11.99728856361171}
done in step count: 26
reward sum = 0.5715410255408505
running average episode reward sum: 0.4833476214359138
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.06668455,  8.53687   ,  0.43169818]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.0419055312799008}
episode index:2507
target Thresh 31.999999999668503
target distance 9.0
model initialize at round 2507
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([15.13025467,  9.43359318,  1.09174877]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 9.853818823130542}
done in step count: 22
reward sum = 0.6541830305504713
running average episode reward sum: 0.4834157376277458
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([19.31246566, 17.03157619,  1.11669053]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 1.1876650017673027}
episode index:2508
target Thresh 31.999999999671804
target distance 13.0
model initialize at round 2508
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([ 6.       , 22.       ,  5.9242968]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 13.152946437965905}
done in step count: 30
reward sum = 0.5199607517735056
running average episode reward sum: 0.4834303031973535
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([7.84132906, 9.77626648, 4.83194935]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.7923169255599875}
episode index:2509
target Thresh 31.99999999967507
target distance 8.0
model initialize at round 2509
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 12.]), 'previousTarget': array([ 3., 12.]), 'currentState': array([ 2.        , 20.        ,  0.77223623]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 22
reward sum = 0.6492652790121138
running average episode reward sum: 0.48349637290883346
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 12.]), 'previousTarget': array([ 3., 12.]), 'currentState': array([ 3.12621209, 12.84022836,  4.8373615 ]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 0.849654743278265}
episode index:2510
target Thresh 31.999999999678302
target distance 7.0
model initialize at round 2510
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  4.]), 'previousTarget': array([22.,  4.]), 'currentState': array([15.        , 11.        ,  1.62444115]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 9.899494936611665}
done in step count: 23
reward sum = 0.5987838874476463
running average episode reward sum: 0.4835422858974988
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  4.]), 'previousTarget': array([22.,  4.]), 'currentState': array([21.37524025,  4.94440106,  5.37754206]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 1.132350697038546}
episode index:2511
target Thresh 31.999999999681503
target distance 17.0
model initialize at round 2511
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9.95661205, 11.74047298]), 'previousTarget': array([10.00324289, 11.76756726]), 'currentState': array([24.90001048, 25.03313424,  3.05853873]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.2797359994402624
running average episode reward sum: 0.4834611528216798
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 8.97941644, 10.92526553,  3.85096947]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 1.3473577343008079}
episode index:2512
target Thresh 31.999999999684672
target distance 19.0
model initialize at round 2512
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([25.        , 21.        ,  1.82115802]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 19.235384061671343}
done in step count: 50
reward sum = 0.3361457067014847
running average episode reward sum: 0.4834025314742384
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([ 6.78964124, 17.9961087 ,  3.45347606]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 0.7896508288772798}
episode index:2513
target Thresh 31.99999999968781
target distance 17.0
model initialize at round 2513
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.67183089, 12.67845223]), 'previousTarget': array([12.85786438, 12.85786438]), 'currentState': array([26.79645382, 26.83807889,  3.96454006]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.2516971190077686
running average episode reward sum: 0.4833103654390489
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.83871669, 10.61542165,  3.81088139]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 1.0402833742822781}
episode index:2514
target Thresh 31.999999999690917
target distance 24.0
model initialize at round 2514
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.83261089,  6.01733854]), 'previousTarget': array([19.83261089,  6.01733854]), 'currentState': array([19.        , 26.        ,  2.03210926]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.20547275767981193
running average episode reward sum: 0.4831998932292043
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  2.]), 'previousTarget': array([20.,  2.]), 'currentState': array([19.8051217 ,  2.82914024,  4.91185736]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 0.8517341657280146}
episode index:2515
target Thresh 31.99999999969399
target distance 8.0
model initialize at round 2515
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([7.        , 4.        , 4.31670547]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 8.246211251235321}
done in step count: 20
reward sum = 0.6675082102908965
running average episode reward sum: 0.4832731477272415
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.09426627,  1.6411304 ,  5.93360025]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.9742386648744433}
episode index:2516
target Thresh 31.999999999697035
target distance 19.0
model initialize at round 2516
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([25.        , 28.        ,  6.07999376]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 19.026297590440446}
done in step count: 44
reward sum = 0.3506528767338734
running average episode reward sum: 0.4832204579096041
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([25.74668315,  9.79590182,  4.80738431]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 0.8352419596337239}
episode index:2517
target Thresh 31.99999999970005
target distance 12.0
model initialize at round 2517
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([18.99451268, 29.03052523,  1.49616027]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 13.446167305202648}
done in step count: 37
reward sum = 0.47048829698510974
running average episode reward sum: 0.48321540145173103
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([24.11137087, 17.74024885,  5.30092161]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 1.1565595964460547}
episode index:2518
target Thresh 31.999999999703036
target distance 9.0
model initialize at round 2518
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 18.]), 'previousTarget': array([14., 18.]), 'currentState': array([ 8.0504705 , 26.7284829 ,  5.00811276]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 10.563300385282151}
done in step count: 21
reward sum = 0.6494114257887063
running average episode reward sum: 0.48328137843638247
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 18.]), 'previousTarget': array([14., 18.]), 'currentState': array([13.15879089, 18.873017  ,  5.45258298]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 1.2123495599622947}
episode index:2519
target Thresh 31.99999999970599
target distance 14.0
model initialize at round 2519
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 29.]), 'previousTarget': array([ 2., 29.]), 'currentState': array([ 9.91895644, 14.99971746,  2.91721067]), 'targetState': array([ 2, 29], dtype=int32), 'currentDistance': 16.08470647414808}
done in step count: 37
reward sum = 0.4506685957257145
running average episode reward sum: 0.48326843685594173
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 29.]), 'previousTarget': array([ 2., 29.]), 'currentState': array([ 2.36997246, 28.10432696,  2.29254794]), 'targetState': array([ 2, 29], dtype=int32), 'currentDistance': 0.9690767859627395}
episode index:2520
target Thresh 31.999999999708916
target distance 10.0
model initialize at round 2520
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 19.]), 'previousTarget': array([10., 19.]), 'currentState': array([19.81575684, 22.08985792,  2.86280042]), 'targetState': array([10, 19], dtype=int32), 'currentDistance': 10.29059300542474}
done in step count: 21
reward sum = 0.6446655545812185
running average episode reward sum: 0.483332457926043
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 19.]), 'previousTarget': array([10., 19.]), 'currentState': array([10.93817147, 19.21284097,  3.63790253]), 'targetState': array([10, 19], dtype=int32), 'currentDistance': 0.9620119501861559}
episode index:2521
target Thresh 31.99999999971181
target distance 6.0
model initialize at round 2521
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([ 3.0003269 , 29.00019194,  0.27842152]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 7.810121431506759}
done in step count: 16
reward sum = 0.7154199764916861
running average episode reward sum: 0.4834244831118343
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([ 8.48206763, 24.93042345,  5.69474874]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 1.0648669988860708}
episode index:2522
target Thresh 31.999999999714678
target distance 21.0
model initialize at round 2522
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7.20101013, 3.17157288]), 'previousTarget': array([7.20101013, 3.17157288]), 'currentState': array([27.        ,  6.        ,  4.04861543]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.3234521479968358
running average episode reward sum: 0.4833610775093313
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 3.]), 'previousTarget': array([6., 3.]), 'currentState': array([6.81524343, 3.27003314, 3.25466337]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 0.8588013417469482}
episode index:2523
target Thresh 31.999999999717517
target distance 8.0
model initialize at round 2523
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  5.]), 'previousTarget': array([22.,  5.]), 'currentState': array([14.28603697,  8.78352712,  5.662165  ]), 'targetState': array([22,  5], dtype=int32), 'currentDistance': 8.591874243797172}
done in step count: 20
reward sum = 0.7015980428882211
running average episode reward sum: 0.4834475422341249
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  5.]), 'previousTarget': array([22.,  5.]), 'currentState': array([21.04756809,  5.0267204 ,  6.08608318]), 'targetState': array([22,  5], dtype=int32), 'currentDistance': 0.9528066506331425}
episode index:2524
target Thresh 31.99999999972033
target distance 15.0
model initialize at round 2524
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([16.14003481, 28.60776164,  5.06315986]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 15.739310406349025}
done in step count: 33
reward sum = 0.48429350057297116
running average episode reward sum: 0.48344787726713034
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([21.35812415, 14.80671718,  5.37719518]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 1.0309205620780946}
episode index:2525
target Thresh 31.999999999723112
target distance 17.0
model initialize at round 2525
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 12.]), 'previousTarget': array([ 3., 12.]), 'currentState': array([ 9.        , 29.        ,  6.05959911]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 18.027756377319946}
done in step count: 46
reward sum = 0.3717655760143677
running average episode reward sum: 0.48340366416291314
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 12.]), 'previousTarget': array([ 3., 12.]), 'currentState': array([ 2.62636881, 12.98273587,  4.44365026]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 1.051365807530744}
episode index:2526
target Thresh 31.999999999725866
target distance 8.0
model initialize at round 2526
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 12.]), 'previousTarget': array([11., 12.]), 'currentState': array([3.       , 4.       , 1.6362803]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 11.313708498984761}
done in step count: 25
reward sum = 0.5975974165326345
running average episode reward sum: 0.4834488536177488
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 12.]), 'previousTarget': array([11., 12.]), 'currentState': array([10.52351674, 11.20997585,  0.96150024]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 0.922591169653104}
episode index:2527
target Thresh 31.999999999728594
target distance 6.0
model initialize at round 2527
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([ 2.99043295, 16.06238257,  1.5367296 ]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 7.777850736561351}
done in step count: 20
reward sum = 0.6919010669829608
running average episode reward sum: 0.4835313109806306
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([ 9.0703407 , 20.01600714,  0.80651858]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 0.9865038040168146}
episode index:2528
target Thresh 31.999999999731294
target distance 11.0
model initialize at round 2528
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([17.96456662, 20.98490776,  3.79675317]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 11.033599628674974}
done in step count: 26
reward sum = 0.6047074063057275
running average episode reward sum: 0.4835792256090708
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([18.10135435, 10.79012709,  5.05689965]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 1.1966055412552252}
episode index:2529
target Thresh 31.99999999973397
target distance 5.0
model initialize at round 2529
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 12.]), 'previousTarget': array([25., 12.]), 'currentState': array([26.68605184,  7.2879535 ,  2.27845405]), 'targetState': array([25, 12], dtype=int32), 'currentDistance': 5.004613175962631}
done in step count: 10
reward sum = 0.8197889317946436
running average episode reward sum: 0.48371211482100185
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 12.]), 'previousTarget': array([25., 12.]), 'currentState': array([25.9499371 , 11.3306037 ,  1.83023634]), 'targetState': array([25, 12], dtype=int32), 'currentDistance': 1.162098055050733}
episode index:2530
target Thresh 31.999999999736616
target distance 3.0
model initialize at round 2530
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 21.]), 'previousTarget': array([ 6., 21.]), 'currentState': array([ 3.13655563, 18.40412502,  1.12975375]), 'targetState': array([ 6, 21], dtype=int32), 'currentDistance': 3.864955446032838}
done in step count: 7
reward sum = 0.8709542748670303
running average episode reward sum: 0.4838651144891354
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 21.]), 'previousTarget': array([ 6., 21.]), 'currentState': array([ 5.43708023, 20.05447407,  0.48335228]), 'targetState': array([ 6, 21], dtype=int32), 'currentDistance': 1.1004080832570409}
episode index:2531
target Thresh 31.999999999739238
target distance 10.0
model initialize at round 2531
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 24.]), 'previousTarget': array([ 6., 24.]), 'currentState': array([11.98867434, 13.97828254,  3.9791894 ]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 11.674718032582101}
done in step count: 30
reward sum = 0.5273033645899671
running average episode reward sum: 0.48388227019612623
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 24.]), 'previousTarget': array([ 6., 24.]), 'currentState': array([ 6.80309358, 24.08245736,  2.71008795]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 0.8073156225826768}
episode index:2532
target Thresh 31.99999999974183
target distance 25.0
model initialize at round 2532
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.45997905, 15.24340568]), 'previousTarget': array([10.46146996, 15.24620043]), 'currentState': array([26.99504699,  3.99211562,  3.92268759]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 99
reward sum = -0.10663520555801996
running average episode reward sum: 0.4836491405175814
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8.81573479, 15.3946249 ]), 'previousTarget': array([ 8.86303038, 15.38318116]), 'currentState': array([24.26276959,  2.69072329,  3.43783129]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 20.0}
episode index:2533
target Thresh 31.9999999997444
target distance 4.0
model initialize at round 2533
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 15.]), 'previousTarget': array([13., 15.]), 'currentState': array([16.56631109, 16.75128676,  3.63618254]), 'targetState': array([13, 15], dtype=int32), 'currentDistance': 3.973107110313092}
done in step count: 7
reward sum = 0.870616791698991
running average episode reward sum: 0.4838018507193104
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 15.]), 'previousTarget': array([13., 15.]), 'currentState': array([13.75667856, 15.64328551,  3.56258931]), 'targetState': array([13, 15], dtype=int32), 'currentDistance': 0.9931659904344221}
episode index:2534
target Thresh 31.999999999746944
target distance 12.0
model initialize at round 2534
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 17.]), 'previousTarget': array([18., 17.]), 'currentState': array([12.15250045, 28.89836217,  5.61096898]), 'targetState': array([18, 17], dtype=int32), 'currentDistance': 13.257611902870128}
done in step count: 30
reward sum = 0.5524124065709913
running average episode reward sum: 0.4838289160273387
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 17.]), 'previousTarget': array([18., 17.]), 'currentState': array([17.14038199, 17.57017075,  5.4935704 ]), 'targetState': array([18, 17], dtype=int32), 'currentDistance': 1.0315220781672756}
episode index:2535
target Thresh 31.999999999749463
target distance 7.0
model initialize at round 2535
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 12.]), 'previousTarget': array([22., 12.]), 'currentState': array([19.        , 19.        ,  4.06931257]), 'targetState': array([22, 12], dtype=int32), 'currentDistance': 7.6157731058639095}
done in step count: 17
reward sum = 0.7120197757118676
running average episode reward sum: 0.4839188966502427
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 12.]), 'previousTarget': array([22., 12.]), 'currentState': array([21.16869926, 12.48384584,  5.42929291]), 'targetState': array([22, 12], dtype=int32), 'currentDistance': 0.9618563916779213}
episode index:2536
target Thresh 31.999999999751953
target distance 7.0
model initialize at round 2536
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'previousTarget': array([10., 17.]), 'currentState': array([ 4.99885509, 24.01156128,  1.4405171 ]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 8.612400479394273}
done in step count: 23
reward sum = 0.640221220374443
running average episode reward sum: 0.4839805057648364
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'previousTarget': array([10., 17.]), 'currentState': array([ 9.36357889, 17.84967725,  5.36737609]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 1.0615946792373367}
episode index:2537
target Thresh 31.999999999754422
target distance 19.0
model initialize at round 2537
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.84673604, 20.16202207]), 'previousTarget': array([21.67985983, 19.90977806]), 'currentState': array([11.21676309,  3.22083926,  0.86665016]), 'targetState': array([23, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 96
reward sum = 0.20876312089752583
running average episode reward sum: 0.48387206707891556
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 22.]), 'previousTarget': array([23., 22.]), 'currentState': array([22.26222777, 21.2010667 ,  0.86457765]), 'targetState': array([23, 22], dtype=int32), 'currentDistance': 1.0874751912110057}
episode index:2538
target Thresh 31.999999999756866
target distance 20.0
model initialize at round 2538
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.2384301 , 13.79270645]), 'previousTarget': array([22.2384301 , 13.79270645]), 'currentState': array([ 4.        , 22.        ,  0.98624554]), 'targetState': array([24, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 94
reward sum = 0.2072464403212451
running average episode reward sum: 0.4837631164579004
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 13.]), 'previousTarget': array([24., 13.]), 'currentState': array([23.20036232, 13.22388686,  5.71868491]), 'targetState': array([24, 13], dtype=int32), 'currentDistance': 0.8303889171546188}
episode index:2539
target Thresh 31.999999999759286
target distance 10.0
model initialize at round 2539
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 16.]), 'previousTarget': array([15., 16.]), 'currentState': array([25.        , 15.        ,  4.49728107]), 'targetState': array([15, 16], dtype=int32), 'currentDistance': 10.049875621120892}
done in step count: 25
reward sum = 0.6129266487340392
running average episode reward sum: 0.483813968242261
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 16.]), 'previousTarget': array([15., 16.]), 'currentState': array([15.77255811, 15.92410745,  2.97260965]), 'targetState': array([15, 16], dtype=int32), 'currentDistance': 0.776276830578026}
episode index:2540
target Thresh 31.99999999976168
target distance 25.0
model initialize at round 2540
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.59490445,  9.06369443]), 'previousTarget': array([10.59490445,  9.06369443]), 'currentState': array([ 9.        , 29.        ,  2.95966887]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.15039423188461848
running average episode reward sum: 0.48356437823827564
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([13.48821811, 13.72882883,  4.37771265]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 10.041978874797445}
episode index:2541
target Thresh 31.99999999976405
target distance 10.0
model initialize at round 2541
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 16.]), 'previousTarget': array([ 7., 16.]), 'currentState': array([16.84693136, 20.08952299,  2.78257936]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 10.662375695976225}
done in step count: 22
reward sum = 0.6333274763335295
running average episode reward sum: 0.48362329369779383
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 16.]), 'previousTarget': array([ 7., 16.]), 'currentState': array([ 7.86604133, 16.53132142,  3.61625482]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 1.0160364313718877}
episode index:2542
target Thresh 31.9999999997664
target distance 20.0
model initialize at round 2542
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6.77128737, 25.30950412]), 'previousTarget': array([ 6.79270645, 25.2384301 ]), 'currentState': array([15.07303019,  7.11387096,  1.21847746]), 'targetState': array([ 6, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.31125166930080717
running average episode reward sum: 0.48355551091195154
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 27.]), 'previousTarget': array([ 6., 27.]), 'currentState': array([ 6.68970283, 26.27830257,  2.2834103 ]), 'targetState': array([ 6, 27], dtype=int32), 'currentDistance': 0.9982670838477663}
episode index:2543
target Thresh 31.999999999768722
target distance 11.0
model initialize at round 2543
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 20.]), 'previousTarget': array([ 9., 20.]), 'currentState': array([20.00872705, 26.99981152,  6.00909186]), 'targetState': array([ 9, 20], dtype=int32), 'currentDistance': 13.045667201627044}
done in step count: 32
reward sum = 0.5065719367927887
running average episode reward sum: 0.4835645582491689
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 20.]), 'previousTarget': array([ 9., 20.]), 'currentState': array([ 9.97171726, 20.73972898,  3.82295721]), 'targetState': array([ 9, 20], dtype=int32), 'currentDistance': 1.221242565728516}
episode index:2544
target Thresh 31.999999999771024
target distance 15.0
model initialize at round 2544
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 10.]), 'previousTarget': array([20., 10.]), 'currentState': array([ 5.36725693, 18.75477715,  5.67271028]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 17.05178268253632}
done in step count: 37
reward sum = 0.44116056708647927
running average episode reward sum: 0.48354789656305386
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 10.]), 'previousTarget': array([20., 10.]), 'currentState': array([19.30138426, 10.47514323,  6.04862017]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 0.844881670545897}
episode index:2545
target Thresh 31.999999999773305
target distance 16.0
model initialize at round 2545
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([ 2.93751353, 21.03794968,  2.84830356]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 16.038071408827495}
done in step count: 45
reward sum = 0.41263371345517813
running average episode reward sum: 0.48352004338822746
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.53707645, 5.94072801, 4.94973167]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.048459538239998}
episode index:2546
target Thresh 31.999999999775557
target distance 7.0
model initialize at round 2546
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([20.00655434, 12.0029458 ,  0.16989011]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 8.608532759055327}
done in step count: 22
reward sum = 0.6311312502300525
running average episode reward sum: 0.48357799831827925
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.3590659 ,  5.66912138,  4.29175369]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.7593758934748093}
episode index:2547
target Thresh 31.999999999777792
target distance 14.0
model initialize at round 2547
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 23.]), 'previousTarget': array([13., 23.]), 'currentState': array([15.89011168,  9.13652073,  2.24152582]), 'targetState': array([13, 23], dtype=int32), 'currentDistance': 14.161525447375897}
done in step count: 34
reward sum = 0.5256073156319134
running average episode reward sum: 0.4835944933407728
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 23.]), 'previousTarget': array([13., 23.]), 'currentState': array([13.40662009, 22.20173051,  1.81630913]), 'targetState': array([13, 23], dtype=int32), 'currentDistance': 0.8958649857644603}
episode index:2548
target Thresh 31.99999999978
target distance 16.0
model initialize at round 2548
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.93836831, 24.95356281]), 'previousTarget': array([24., 25.]), 'currentState': array([ 7.96495761, 12.91819201,  4.53003049]), 'targetState': array([24, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.3154071379947588
running average episode reward sum: 0.4835285116399701
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 25.]), 'previousTarget': array([24., 25.]), 'currentState': array([23.04054597, 24.2546101 ,  1.00820869]), 'targetState': array([24, 25], dtype=int32), 'currentDistance': 1.2149724903295565}
episode index:2549
target Thresh 31.999999999782194
target distance 9.0
model initialize at round 2549
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 13.]), 'previousTarget': array([24., 13.]), 'currentState': array([15.00237168, 21.00338778,  0.74569851]), 'targetState': array([24, 13], dtype=int32), 'currentDistance': 12.042073380762849}
done in step count: 28
reward sum = 0.5625945876899928
running average episode reward sum: 0.4835595179443035
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 13.]), 'previousTarget': array([24., 13.]), 'currentState': array([23.20044965, 13.61094586,  5.7248837 ]), 'targetState': array([24, 13], dtype=int32), 'currentDistance': 1.0062482836338575}
episode index:2550
target Thresh 31.99999999978436
target distance 4.0
model initialize at round 2550
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  3.]), 'previousTarget': array([10.,  3.]), 'currentState': array([6.        , 7.        , 3.82924628]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 5.656854249492381}
done in step count: 14
reward sum = 0.7654117670794961
running average episode reward sum: 0.48367000490986023
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  3.]), 'previousTarget': array([10.,  3.]), 'currentState': array([9.21473847, 3.60957952, 5.76462172]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 0.9940939953770416}
episode index:2551
target Thresh 31.999999999786503
target distance 10.0
model initialize at round 2551
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'previousTarget': array([ 5., 22.]), 'currentState': array([ 9.        , 12.        ,  0.65659279]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 10.770329614269007}
done in step count: 23
reward sum = 0.6184669137962824
running average episode reward sum: 0.48372282501522323
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'previousTarget': array([ 5., 22.]), 'currentState': array([ 5.86956503, 21.0083064 ,  2.0980275 ]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 1.3189387912058854}
episode index:2552
target Thresh 31.999999999788628
target distance 16.0
model initialize at round 2552
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 16.]), 'previousTarget': array([ 8., 16.]), 'currentState': array([23.9776127 , 26.90892251,  4.47003237]), 'targetState': array([ 8, 16], dtype=int32), 'currentDistance': 19.346542272896773}
done in step count: 76
reward sum = 0.31146035725591203
running average episode reward sum: 0.48365535048809466
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 16.]), 'previousTarget': array([ 8., 16.]), 'currentState': array([ 8.88946691, 16.96077737,  3.93075785]), 'targetState': array([ 8, 16], dtype=int32), 'currentDistance': 1.3092916187376935}
episode index:2553
target Thresh 31.99999999979073
target distance 18.0
model initialize at round 2553
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 24.]), 'previousTarget': array([ 4., 24.]), 'currentState': array([22.        , 26.        ,  0.97482297]), 'targetState': array([ 4, 24], dtype=int32), 'currentDistance': 18.110770276274835}
done in step count: 45
reward sum = 0.3688302686430511
running average episode reward sum: 0.48361039156803004
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 24.]), 'previousTarget': array([ 4., 24.]), 'currentState': array([ 4.92076989, 24.21455238,  3.37313497]), 'targetState': array([ 4, 24], dtype=int32), 'currentDistance': 0.9454363583772454}
episode index:2554
target Thresh 31.999999999792816
target distance 11.0
model initialize at round 2554
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'previousTarget': array([ 5., 22.]), 'currentState': array([ 8.84808006, 11.47632752,  1.85961429]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 11.205150713853893}
done in step count: 26
reward sum = 0.6103184192688497
running average episode reward sum: 0.4836599837510832
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'previousTarget': array([ 5., 22.]), 'currentState': array([ 5.18847303, 21.18625843,  2.23694886]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 0.8352828379141637}
episode index:2555
target Thresh 31.999999999794877
target distance 7.0
model initialize at round 2555
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 17.]), 'previousTarget': array([15., 17.]), 'currentState': array([ 8.35479707, 15.8453471 ,  6.04592526]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 6.744771702643016}
done in step count: 15
reward sum = 0.7544119719852688
running average episode reward sum: 0.4837659117589996
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 17.]), 'previousTarget': array([15., 17.]), 'currentState': array([14.12416937, 17.0313766 ,  0.49129517]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 0.8763924865468974}
episode index:2556
target Thresh 31.999999999796916
target distance 13.0
model initialize at round 2556
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  3.]), 'previousTarget': array([11.,  3.]), 'currentState': array([23.9424324 ,  9.03989731,  2.70525295]), 'targetState': array([11,  3], dtype=int32), 'currentDistance': 14.282398817499736}
done in step count: 31
reward sum = 0.5110189373365761
running average episode reward sum: 0.48377656996219764
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  3.]), 'previousTarget': array([11.,  3.]), 'currentState': array([11.74158448,  3.42952977,  3.69094838]), 'targetState': array([11,  3], dtype=int32), 'currentDistance': 0.8569967129388188}
episode index:2557
target Thresh 31.999999999798938
target distance 11.0
model initialize at round 2557
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 11.]), 'previousTarget': array([19., 11.]), 'currentState': array([8.35765809, 9.88101832, 6.05695273]), 'targetState': array([19, 11], dtype=int32), 'currentDistance': 10.701007491301906}
done in step count: 24
reward sum = 0.6341368199801936
running average episode reward sum: 0.4838353503570444
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 11.]), 'previousTarget': array([19., 11.]), 'currentState': array([18.0098219 , 10.7347178 ,  0.33847436]), 'targetState': array([19, 11], dtype=int32), 'currentDistance': 1.0250986836201341}
episode index:2558
target Thresh 31.999999999800938
target distance 17.0
model initialize at round 2558
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 2.]), 'previousTarget': array([5., 2.]), 'currentState': array([21.63979536,  9.12086312,  2.94228399]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 18.099433174477657}
done in step count: 43
reward sum = 0.4062911052016467
running average episode reward sum: 0.48380504779934397
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 2.]), 'previousTarget': array([5., 2.]), 'currentState': array([5.8414638 , 2.46596259, 3.70283944]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 0.9618640517642346}
episode index:2559
target Thresh 31.99999999980292
target distance 17.0
model initialize at round 2559
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 21.]), 'previousTarget': array([ 2., 21.]), 'currentState': array([2.92427695, 4.05486594, 2.26207256]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 16.97032281059354}
done in step count: 40
reward sum = 0.4290439495752544
running average episode reward sum: 0.48378365674535023
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 21.]), 'previousTarget': array([ 2., 21.]), 'currentState': array([ 2.12548445, 20.00520437,  1.80592299]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 1.002678755639786}
episode index:2560
target Thresh 31.99999999980488
target distance 3.0
model initialize at round 2560
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([17.70823227,  6.84078907,  3.53118005]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 2.712908065608036}
done in step count: 5
reward sum = 0.9119460226643799
running average episode reward sum: 0.48395084236265556
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.74645979,  6.56459321,  3.14559438]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.8641650833838257}
episode index:2561
target Thresh 31.99999999980682
target distance 8.0
model initialize at round 2561
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'previousTarget': array([11., 19.]), 'currentState': array([19.0364793 , 14.1435247 ,  1.55207068]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 9.389906915341815}
done in step count: 22
reward sum = 0.6526407125984297
running average episode reward sum: 0.48401668540334086
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'previousTarget': array([11., 19.]), 'currentState': array([11.66538601, 18.5551265 ,  2.86154925]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 0.8004067570571893}
episode index:2562
target Thresh 31.999999999808743
target distance 16.0
model initialize at round 2562
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([10.97527017,  6.994479  ,  3.11573574]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 16.12694623097748}
done in step count: 47
reward sum = 0.4271504660351829
running average episode reward sum: 0.48399449803721983
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([ 9.1400675 , 22.11263675,  2.16287843]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 0.8983498408947111}
episode index:2563
target Thresh 31.999999999810647
target distance 24.0
model initialize at round 2563
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.84026758, 12.48072962]), 'previousTarget': array([22.84555753, 12.48069469]), 'currentState': array([ 2.99430152, 10.00330527,  2.40600047]), 'targetState': array([27, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.05274239218911988
running average episode reward sum: 0.48378516227660107
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 13.]), 'previousTarget': array([27., 13.]), 'currentState': array([ 7.63029296, 14.16389315,  0.62406426]), 'targetState': array([27, 13], dtype=int32), 'currentDistance': 19.40464372435727}
episode index:2564
target Thresh 31.99999999981253
target distance 5.0
model initialize at round 2564
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([10.99590688,  9.03414102,  1.44446671]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 5.852090198094594}
done in step count: 15
reward sum = 0.7493700247005192
running average episode reward sum: 0.4838887041332966
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.15948138,  6.77582927,  5.90320698]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.1438455302110637}
episode index:2565
target Thresh 31.999999999814396
target distance 13.0
model initialize at round 2565
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 27.]), 'previousTarget': array([ 7., 27.]), 'currentState': array([19.93622142, 17.00846005,  3.02887119]), 'targetState': array([ 7, 27], dtype=int32), 'currentDistance': 16.345540531739484}
done in step count: 58
reward sum = 0.4146313784029029
running average episode reward sum: 0.483861713749146
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 27.]), 'previousTarget': array([ 7., 27.]), 'currentState': array([ 7.7884452 , 26.15877775,  2.69688764]), 'targetState': array([ 7, 27], dtype=int32), 'currentDistance': 1.1529530325804984}
episode index:2566
target Thresh 31.999999999816243
target distance 26.0
model initialize at round 2566
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4.45778872, 9.10027853]), 'previousTarget': array([4.45778872, 9.10027853]), 'currentState': array([11.        , 28.        ,  6.10058476]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.03840157612487704
running average episode reward sum: 0.4836582609677381
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3.35091808, 4.7296303 ]), 'previousTarget': array([3.36652758, 4.76956906]), 'currentState': array([12.22210825, 22.65452877,  4.81635543]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 20.0}
episode index:2567
target Thresh 31.999999999818073
target distance 16.0
model initialize at round 2567
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([20.        , 14.        ,  5.92415405]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 19.4164878389476}
done in step count: 99
reward sum = -0.11320909834921963
running average episode reward sum: 0.4834258359835804
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([9.78209215, 5.96960628, 3.64246257]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 6.500088543667122}
episode index:2568
target Thresh 31.99999999981988
target distance 6.0
model initialize at round 2568
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([16.       , 12.       ,  4.9267621]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 6.324555320336759}
done in step count: 16
reward sum = 0.7332573689829085
running average episode reward sum: 0.48352308453671367
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([21.31281952, 13.73633773,  0.5892565 ]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 0.7360263616684569}
episode index:2569
target Thresh 31.999999999821675
target distance 10.0
model initialize at round 2569
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  2.]), 'previousTarget': array([21.,  2.]), 'currentState': array([17.18184636, 11.8128873 ,  5.35078153]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 10.52953248538587}
done in step count: 23
reward sum = 0.6309529920004997
running average episode reward sum: 0.4835804502594622
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  2.]), 'previousTarget': array([21.,  2.]), 'currentState': array([20.88475537,  2.8152255 ,  5.42854364]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 0.8233309991155141}
episode index:2570
target Thresh 31.999999999823448
target distance 11.0
model initialize at round 2570
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 29.]), 'previousTarget': array([15., 29.]), 'currentState': array([18.        , 18.        ,  4.84436093]), 'targetState': array([15, 29], dtype=int32), 'currentDistance': 11.40175425099138}
done in step count: 30
reward sum = 0.5402287641421539
running average episode reward sum: 0.48360248383156745
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 29.]), 'previousTarget': array([15., 29.]), 'currentState': array([15.54200738, 28.03929485,  1.93754274]), 'targetState': array([15, 29], dtype=int32), 'currentDistance': 1.1030532124259034}
episode index:2571
target Thresh 31.999999999825206
target distance 5.0
model initialize at round 2571
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'previousTarget': array([10., 12.]), 'currentState': array([ 5.19624942, 13.04436413,  0.05246884]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 4.91596542319821}
done in step count: 10
reward sum = 0.832486424695867
running average episode reward sum: 0.48373813077591593
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'previousTarget': array([10., 12.]), 'currentState': array([ 9.1335521 , 12.16711645,  6.19476022]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 0.8824170589727274}
episode index:2572
target Thresh 31.999999999826944
target distance 16.0
model initialize at round 2572
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8.88018573, 17.1317656 ]), 'previousTarget': array([ 8.85786438, 17.14213562]), 'currentState': array([23.06733951,  3.0347919 ,  0.71836796]), 'targetState': array([ 7, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 71
reward sum = 0.2594406782898402
running average episode reward sum: 0.4836509572615412
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 19.]), 'previousTarget': array([ 7., 19.]), 'currentState': array([ 7.832054  , 18.55812871,  2.71858403]), 'targetState': array([ 7, 19], dtype=int32), 'currentDistance': 0.9421062074431142}
episode index:2573
target Thresh 31.999999999828667
target distance 7.0
model initialize at round 2573
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 29.]), 'previousTarget': array([13., 29.]), 'currentState': array([ 9.03950152, 22.19052318,  1.21016675]), 'targetState': array([13, 29], dtype=int32), 'currentDistance': 7.877469314825636}
done in step count: 17
reward sum = 0.7301548691930887
running average episode reward sum: 0.48374672412709346
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 29.]), 'previousTarget': array([13., 29.]), 'currentState': array([12.28721015, 28.1782607 ,  1.44318685]), 'targetState': array([13, 29], dtype=int32), 'currentDistance': 1.0878073581733132}
episode index:2574
target Thresh 31.999999999830372
target distance 7.0
model initialize at round 2574
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  9.]), 'previousTarget': array([17.,  9.]), 'currentState': array([18.05231726, 15.82717485,  4.97076342]), 'targetState': array([17,  9], dtype=int32), 'currentDistance': 6.9077990780840715}
done in step count: 17
reward sum = 0.7538888122802706
running average episode reward sum: 0.48385163367589085
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  9.]), 'previousTarget': array([17.,  9.]), 'currentState': array([17.32833605,  9.82285016,  4.60047441]), 'targetState': array([17,  9], dtype=int32), 'currentDistance': 0.8859384498174386}
episode index:2575
target Thresh 31.99999999983206
target distance 5.0
model initialize at round 2575
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  7.]), 'previousTarget': array([13.,  7.]), 'currentState': array([10.        , 12.        ,  3.95593777]), 'targetState': array([13,  7], dtype=int32), 'currentDistance': 5.830951894845301}
done in step count: 13
reward sum = 0.7829965929862267
running average episode reward sum: 0.4839677613774865
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  7.]), 'previousTarget': array([13.,  7.]), 'currentState': array([12.02786998,  7.81467387,  5.36476015]), 'targetState': array([13,  7], dtype=int32), 'currentDistance': 1.268357319088413}
episode index:2576
target Thresh 31.99999999983373
target distance 21.0
model initialize at round 2576
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7.94278962, 5.40132839]), 'previousTarget': array([7.94278962, 5.40132839]), 'currentState': array([26.        , 14.        ,  5.17367744]), 'targetState': array([5, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 75
reward sum = 0.21797645028674317
running average episode reward sum: 0.4838645439498222
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 4.]), 'previousTarget': array([5., 4.]), 'currentState': array([5.83768109, 4.0526133 , 3.67199992]), 'targetState': array([5, 4], dtype=int32), 'currentDistance': 0.8393317432486371}
episode index:2577
target Thresh 31.999999999835385
target distance 18.0
model initialize at round 2577
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 21.]), 'previousTarget': array([ 8., 21.]), 'currentState': array([15.81122823,  3.44486326,  2.05007535]), 'targetState': array([ 8, 21], dtype=int32), 'currentDistance': 19.214528675475517}
done in step count: 47
reward sum = 0.37129911811730665
running average episode reward sum: 0.4838208800918577
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 21.]), 'previousTarget': array([ 8., 21.]), 'currentState': array([ 8.11486897, 20.13324799,  2.3300617 ]), 'targetState': array([ 8, 21], dtype=int32), 'currentDistance': 0.8743305562686386}
episode index:2578
target Thresh 31.999999999837023
target distance 9.0
model initialize at round 2578
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([ 9.        , 11.        ,  0.94055465]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 9.48683298050514}
done in step count: 22
reward sum = 0.6513461488657308
running average episode reward sum: 0.48388583754388326
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([17.02647997,  8.27480394,  5.91378126]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 1.011562382271924}
episode index:2579
target Thresh 31.999999999838643
target distance 8.0
model initialize at round 2579
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 6.85884204, 19.023295  ,  3.2005969 ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 9.379851207053877}
done in step count: 21
reward sum = 0.6580893077931698
running average episode reward sum: 0.4839533582687861
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.67943488, 11.65745527,  4.4874765 ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.9454518454073199}
episode index:2580
target Thresh 31.99999999984025
target distance 11.0
model initialize at round 2580
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([ 4.02425052, 10.98479643,  5.9756937 ]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 11.015230263868425}
done in step count: 30
reward sum = 0.5675112297626359
running average episode reward sum: 0.4839857324925342
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([ 3.97354136, 21.24598573,  1.89886207]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 0.7544783467298594}
episode index:2581
target Thresh 31.99999999984184
target distance 17.0
model initialize at round 2581
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  8.]), 'previousTarget': array([19.86502556,  8.20859686]), 'currentState': array([ 9.10533371, 24.63296357,  5.02362473]), 'targetState': array([20,  8], dtype=int32), 'currentDistance': 19.883390827731073}
done in step count: 45
reward sum = 0.3374234627469492
running average episode reward sum: 0.48392896941362423
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  8.]), 'previousTarget': array([20.,  8.]), 'currentState': array([20.33981897,  8.9251656 ,  4.97663346]), 'targetState': array([20,  8], dtype=int32), 'currentDistance': 0.9856004895796999}
episode index:2582
target Thresh 31.999999999843414
target distance 18.0
model initialize at round 2582
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.94427191, 11.11145618]), 'previousTarget': array([24.94427191, 11.11145618]), 'currentState': array([16.        , 29.        ,  3.85354662]), 'targetState': array([25, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.3264788987899335
running average episode reward sum: 0.4838680131338628
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 11.]), 'previousTarget': array([25., 11.]), 'currentState': array([24.97655538, 11.8414694 ,  5.34101322]), 'targetState': array([25, 11], dtype=int32), 'currentDistance': 0.8417959373583973}
episode index:2583
target Thresh 31.99999999984497
target distance 11.0
model initialize at round 2583
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 17.]), 'previousTarget': array([ 3., 17.]), 'currentState': array([10.        , 28.        ,  5.29970884]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 13.038404810405298}
done in step count: 30
reward sum = 0.5388944183086127
running average episode reward sum: 0.4838893081823051
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 17.]), 'previousTarget': array([ 3., 17.]), 'currentState': array([ 3.84993004, 17.74136356,  4.36861081]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 1.1278302222676935}
episode index:2584
target Thresh 31.999999999846512
target distance 17.0
model initialize at round 2584
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.14900215, 24.88715666]), 'previousTarget': array([22.14900215, 24.88715666]), 'currentState': array([10.        ,  9.        ,  5.07009962]), 'targetState': array([23, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.29260957857902653
running average episode reward sum: 0.48381531215537926
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 26.]), 'previousTarget': array([23., 26.]), 'currentState': array([22.45936686, 25.00781864,  1.24304016]), 'targetState': array([23, 26], dtype=int32), 'currentDistance': 1.1299150569006782}
episode index:2585
target Thresh 31.99999999984804
target distance 7.0
model initialize at round 2585
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([ 6.11149563, 28.04862985,  0.17872521]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 7.532969956528784}
done in step count: 17
reward sum = 0.7303775458474377
running average episode reward sum: 0.4839106571800088
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([12.16089608, 25.65559186,  6.16346794]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 1.0648455630896962}
episode index:2586
target Thresh 31.999999999849553
target distance 15.0
model initialize at round 2586
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  8.]), 'previousTarget': array([12.,  8.]), 'currentState': array([ 5.93788099, 22.93900623,  4.12131003]), 'targetState': array([12,  8], dtype=int32), 'currentDistance': 16.122133667059554}
done in step count: 45
reward sum = 0.4440170481932405
running average episode reward sum: 0.48389523638024584
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  8.]), 'previousTarget': array([12.,  8.]), 'currentState': array([11.70161145,  8.77861693,  5.44983332]), 'targetState': array([12,  8], dtype=int32), 'currentDistance': 0.8338345489873819}
episode index:2587
target Thresh 31.99999999985105
target distance 5.0
model initialize at round 2587
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 16.]), 'previousTarget': array([12., 16.]), 'currentState': array([15.02149598, 10.95808543,  4.93376541]), 'targetState': array([12, 16], dtype=int32), 'currentDistance': 5.877953761969291}
done in step count: 17
reward sum = 0.7087198072236812
running average episode reward sum: 0.4839821083164295
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 16.]), 'previousTarget': array([12., 16.]), 'currentState': array([12.4961779 , 15.30190892,  2.11502742]), 'targetState': array([12, 16], dtype=int32), 'currentDistance': 0.8564599564754282}
episode index:2588
target Thresh 31.99999999985253
target distance 19.0
model initialize at round 2588
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'previousTarget': array([27.,  8.]), 'currentState': array([7.98068683, 6.02437556, 1.98833489]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 19.12164651263121}
done in step count: 51
reward sum = 0.34300863313241187
running average episode reward sum: 0.48392765737970345
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'previousTarget': array([27.,  8.]), 'currentState': array([2.61351352e+01, 8.01171366e+00, 2.25030761e-02]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 0.8649441037288077}
episode index:2589
target Thresh 31.999999999853998
target distance 20.0
model initialize at round 2589
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9.99875234, 4.02495322]), 'previousTarget': array([9.99875234, 4.02495322]), 'currentState': array([ 9.        , 24.        ,  3.17018783]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.332132164894786
running average episode reward sum: 0.4838690490814467
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  4.]), 'previousTarget': array([10.,  4.]), 'currentState': array([9.91546282, 4.98436508, 4.9859044 ]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 0.9879884320635357}
episode index:2590
target Thresh 31.99999999985545
target distance 13.0
model initialize at round 2590
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 16.]), 'previousTarget': array([14., 16.]), 'currentState': array([4.96690013, 3.08697379, 1.74127868]), 'targetState': array([14, 16], dtype=int32), 'currentDistance': 15.758906662654317}
done in step count: 34
reward sum = 0.47772383411183983
running average episode reward sum: 0.4838666773273095
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 16.]), 'previousTarget': array([14., 16.]), 'currentState': array([13.50939435, 15.10906505,  1.0796534 ]), 'targetState': array([14, 16], dtype=int32), 'currentDistance': 1.0170835719673155}
episode index:2591
target Thresh 31.99999999985689
target distance 3.0
model initialize at round 2591
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 18.]), 'previousTarget': array([13., 18.]), 'currentState': array([14.71856627, 20.98161956,  3.36916074]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 3.4414423199918187}
done in step count: 7
reward sum = 0.8650789108604757
running average episode reward sum: 0.4840137499482714
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 18.]), 'previousTarget': array([13., 18.]), 'currentState': array([12.75092828, 18.82711551,  4.45978038]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 0.8638036762148174}
episode index:2592
target Thresh 31.999999999858314
target distance 11.0
model initialize at round 2592
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  3.]), 'previousTarget': array([12.,  3.]), 'currentState': array([12.        , 14.        ,  3.53709197]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 11.0}
done in step count: 26
reward sum = 0.6008177234527244
running average episode reward sum: 0.48405879583084155
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  3.]), 'previousTarget': array([12.,  3.]), 'currentState': array([11.74702373,  3.91718756,  4.81068372]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 0.9514357664784762}
episode index:2593
target Thresh 31.999999999859725
target distance 3.0
model initialize at round 2593
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 19.]), 'previousTarget': array([ 8., 19.]), 'currentState': array([11.03339112, 19.02877942,  0.8798871 ]), 'targetState': array([ 8, 19], dtype=int32), 'currentDistance': 3.0335276438463827}
done in step count: 9
reward sum = 0.8366411354560602
running average episode reward sum: 0.4841947180897565
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 19.]), 'previousTarget': array([ 8., 19.]), 'currentState': array([ 8.71522824, 19.6790879 ,  3.44294833]), 'targetState': array([ 8, 19], dtype=int32), 'currentDistance': 0.9862615328774944}
episode index:2594
target Thresh 31.99999999986112
target distance 7.0
model initialize at round 2594
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 15.]), 'previousTarget': array([26., 15.]), 'currentState': array([23.24593548,  8.42470715,  1.10213573]), 'targetState': array([26, 15], dtype=int32), 'currentDistance': 7.128768995576847}
done in step count: 13
reward sum = 0.7480073857703262
running average episode reward sum: 0.48429638000408426
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 15.]), 'previousTarget': array([26., 15.]), 'currentState': array([26.10096074, 14.18714675,  1.18469531]), 'targetState': array([26, 15], dtype=int32), 'currentDistance': 0.819099187302941}
episode index:2595
target Thresh 31.999999999862503
target distance 23.0
model initialize at round 2595
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8.85056185, 16.62463684]), 'previousTarget': array([ 9.24989998, 16.32616523]), 'currentState': array([24.58498263,  4.27846324,  2.49885274]), 'targetState': array([ 2, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.14343029275301555
running average episode reward sum: 0.4841650756561447
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 22.]), 'previousTarget': array([ 2., 22.]), 'currentState': array([ 2.80426579, 21.72723468,  2.86860005]), 'targetState': array([ 2, 22], dtype=int32), 'currentDistance': 0.8492610774376528}
episode index:2596
target Thresh 31.99999999986387
target distance 5.0
model initialize at round 2596
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 17.]), 'previousTarget': array([19., 17.]), 'currentState': array([24.        , 20.        ,  1.58602792]), 'targetState': array([19, 17], dtype=int32), 'currentDistance': 5.8309518948453}
done in step count: 14
reward sum = 0.7418310259899209
running average episode reward sum: 0.4842642924256225
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 17.]), 'previousTarget': array([19., 17.]), 'currentState': array([19.34623598, 17.94622595,  3.71548789]), 'targetState': array([19, 17], dtype=int32), 'currentDistance': 1.0075827010592393}
episode index:2597
target Thresh 31.999999999865224
target distance 7.0
model initialize at round 2597
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 18.]), 'previousTarget': array([ 2., 18.]), 'currentState': array([ 4.99976234, 25.00012814,  2.84121472]), 'targetState': array([ 2, 18], dtype=int32), 'currentDistance': 7.6157972690098905}
done in step count: 19
reward sum = 0.6999683320184947
running average episode reward sum: 0.48434731938466513
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 18.]), 'previousTarget': array([ 2., 18.]), 'currentState': array([ 1.95001071, 18.81393297,  4.64055048]), 'targetState': array([ 2, 18], dtype=int32), 'currentDistance': 0.8154666181730048}
episode index:2598
target Thresh 31.999999999866564
target distance 8.0
model initialize at round 2598
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([11.01108425, 13.01132616,  0.54369235]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 8.072131144634174}
done in step count: 19
reward sum = 0.6719606841624494
running average episode reward sum: 0.48441950613525303
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([12.09873343,  5.99305322,  4.75899393]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 0.9979493873546194}
episode index:2599
target Thresh 31.999999999867892
target distance 9.0
model initialize at round 2599
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 25.]), 'previousTarget': array([22., 25.]), 'currentState': array([13.03655098, 27.03460901,  0.53735086]), 'targetState': array([22, 25], dtype=int32), 'currentDistance': 9.191466269495468}
done in step count: 19
reward sum = 0.667066192635617
running average episode reward sum: 0.4844897548608301
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 25.]), 'previousTarget': array([22., 25.]), 'currentState': array([21.28610665, 25.04962192,  6.0225418 ]), 'targetState': array([22, 25], dtype=int32), 'currentDistance': 0.7156158517141328}
episode index:2600
target Thresh 31.999999999869207
target distance 9.0
model initialize at round 2600
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  8.]), 'previousTarget': array([12.,  8.]), 'currentState': array([21.        , 12.        ,  6.15547049]), 'targetState': array([12,  8], dtype=int32), 'currentDistance': 9.848857801796104}
done in step count: 25
reward sum = 0.5908458206827479
running average episode reward sum: 0.4845306453128954
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  8.]), 'previousTarget': array([12.,  8.]), 'currentState': array([12.78194897,  8.67733148,  3.71197831]), 'targetState': array([12,  8], dtype=int32), 'currentDistance': 1.0345154034104214}
episode index:2601
target Thresh 31.999999999870507
target distance 16.0
model initialize at round 2601
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 18.]), 'previousTarget': array([21., 18.]), 'currentState': array([18.03084452,  2.4240276 ,  1.51292678]), 'targetState': array([21, 18], dtype=int32), 'currentDistance': 15.856443501377951}
done in step count: 33
reward sum = 0.47566270150833534
running average episode reward sum: 0.48452723718691365
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 18.]), 'previousTarget': array([21., 18.]), 'currentState': array([20.87042403, 17.11361356,  1.24387465]), 'targetState': array([21, 18], dtype=int32), 'currentDistance': 0.895807376292845}
episode index:2602
target Thresh 31.999999999871797
target distance 19.0
model initialize at round 2602
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6.70700992, 23.89990489]), 'previousTarget': array([ 7.09022194, 23.67985983]), 'currentState': array([23.51833173, 13.06572477,  2.90014185]), 'targetState': array([ 5, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.3038792489690608
running average episode reward sum: 0.48445783726827446
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 25.]), 'previousTarget': array([ 5., 25.]), 'currentState': array([ 5.93277318, 24.60961482,  2.89370861]), 'targetState': array([ 5, 25], dtype=int32), 'currentDistance': 1.011170805859405}
episode index:2603
target Thresh 31.999999999873072
target distance 18.0
model initialize at round 2603
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  5.]), 'previousTarget': array([22.,  5.]), 'currentState': array([22.        , 23.        ,  0.30196667]), 'targetState': array([22,  5], dtype=int32), 'currentDistance': 18.0}
done in step count: 48
reward sum = 0.3721002561240085
running average episode reward sum: 0.4844146891956384
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  5.]), 'previousTarget': array([22.,  5.]), 'currentState': array([21.66129497,  5.90655661,  4.91088659]), 'targetState': array([22,  5], dtype=int32), 'currentDistance': 0.9677633923834655}
episode index:2604
target Thresh 31.999999999874337
target distance 13.0
model initialize at round 2604
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([24.94495312,  3.92634863,  3.83603883]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 13.109990135827529}
done in step count: 32
reward sum = 0.5340625336529532
running average episode reward sum: 0.4844337478691345
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([12.73908793,  6.19558544,  3.20120811]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 0.7645290273945994}
episode index:2605
target Thresh 31.999999999875588
target distance 8.0
model initialize at round 2605
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  7.]), 'previousTarget': array([19.,  7.]), 'currentState': array([26.66876363,  3.06133253,  2.87179816]), 'targetState': array([19,  7], dtype=int32), 'currentDistance': 8.621080968673821}
done in step count: 15
reward sum = 0.7157032755642538
running average episode reward sum: 0.4845224928912738
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  7.]), 'previousTarget': array([19.,  7.]), 'currentState': array([19.97824399,  6.38457162,  2.61500046]), 'targetState': array([19,  7], dtype=int32), 'currentDistance': 1.155730678984076}
episode index:2606
target Thresh 31.999999999876824
target distance 9.0
model initialize at round 2606
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([17.06869994,  2.03636797,  0.71745738]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 9.474366387850566}
done in step count: 19
reward sum = 0.6486554543691788
running average episode reward sum: 0.48458545144956994
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.82778887, 10.45166714,  2.08844669]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.9929266502469138}
episode index:2607
target Thresh 31.99999999987805
target distance 15.0
model initialize at round 2607
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 14.]), 'previousTarget': array([12., 14.]), 'currentState': array([13.99786483, 29.001274  ,  2.8203145 ]), 'targetState': array([12, 14], dtype=int32), 'currentDistance': 15.13372675784445}
done in step count: 42
reward sum = 0.4457118839436479
running average episode reward sum: 0.48457054594055693
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 14.]), 'previousTarget': array([12., 14.]), 'currentState': array([12.5329316 , 14.86304076,  4.67255111]), 'targetState': array([12, 14], dtype=int32), 'currentDistance': 1.0143251200513932}
episode index:2608
target Thresh 31.999999999879265
target distance 3.0
model initialize at round 2608
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([17.        , 18.        ,  4.07188511]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 4.242640687119285}
done in step count: 17
reward sum = 0.7703056431856528
running average episode reward sum: 0.48468006495061633
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([13.76867438, 20.00032991,  1.79162469]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 1.0260856877731024}
episode index:2609
target Thresh 31.999999999880465
target distance 9.0
model initialize at round 2609
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 26.]), 'previousTarget': array([12., 26.]), 'currentState': array([14.18827261, 17.19161991,  0.96948478]), 'targetState': array([12, 26], dtype=int32), 'currentDistance': 9.07612785823532}
done in step count: 19
reward sum = 0.6761250977695816
running average episode reward sum: 0.48475341553790324
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 26.]), 'previousTarget': array([12., 26.]), 'currentState': array([12.41138282, 25.21306006,  1.98860099]), 'targetState': array([12, 26], dtype=int32), 'currentDistance': 0.887981020481291}
episode index:2610
target Thresh 31.999999999881656
target distance 5.0
model initialize at round 2610
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 7.]), 'previousTarget': array([7., 7.]), 'currentState': array([3.96698445, 2.06881591, 1.86446056]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 5.789279732519044}
done in step count: 18
reward sum = 0.7452395876999984
running average episode reward sum: 0.4848531804448976
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 7.]), 'previousTarget': array([7., 7.]), 'currentState': array([6.05776069, 7.3215295 , 0.32965032]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 0.995588332206186}
episode index:2611
target Thresh 31.99999999988283
target distance 17.0
model initialize at round 2611
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 22.]), 'previousTarget': array([26., 22.]), 'currentState': array([ 9.33323644, 20.92410077,  6.11299465]), 'targetState': array([26, 22], dtype=int32), 'currentDistance': 16.701454030822326}
done in step count: 39
reward sum = 0.4470712716630859
running average episode reward sum: 0.4848387157018724
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 22.]), 'previousTarget': array([26., 22.]), 'currentState': array([25.0358436 , 21.64418572,  0.26395297]), 'targetState': array([26, 22], dtype=int32), 'currentDistance': 1.0277165769450665}
episode index:2612
target Thresh 31.999999999883997
target distance 6.0
model initialize at round 2612
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([7.33836906, 8.79852343, 5.83209686]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 5.787712056156557}
done in step count: 13
reward sum = 0.7964444903670556
running average episode reward sum: 0.4849579678161722
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([12.0577753 ,  9.46482732,  0.426247  ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 1.083603796640088}
episode index:2613
target Thresh 31.99999999988515
target distance 15.0
model initialize at round 2613
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 3.]), 'previousTarget': array([6., 3.]), 'currentState': array([12.        , 18.        ,  0.30694842]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 16.15549442140351}
done in step count: 41
reward sum = 0.410889970908065
running average episode reward sum: 0.4849296326987628
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 3.]), 'previousTarget': array([6., 3.]), 'currentState': array([6.32238726, 3.6992629 , 4.64459485]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 0.770001399100208}
episode index:2614
target Thresh 31.999999999886295
target distance 10.0
model initialize at round 2614
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([8.03974818, 9.02335179, 0.3082405 ]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 11.639904777514452}
done in step count: 29
reward sum = 0.5823438761979826
running average episode reward sum: 0.4849668847995273
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([17.03202223,  3.09071279,  5.69049576]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 0.9722189917956999}
episode index:2615
target Thresh 31.999999999887425
target distance 22.0
model initialize at round 2615
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.90828821, 23.97654022]), 'previousTarget': array([18.90815322, 23.9793708 ]), 'currentState': array([18.00273299,  3.99705149,  5.7092559 ]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.27556636104844356
running average episode reward sum: 0.484886838727757
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 26.]), 'previousTarget': array([19., 26.]), 'currentState': array([19.18261291, 25.05534639,  1.62141089]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 0.962142360179534}
episode index:2616
target Thresh 31.999999999888544
target distance 21.0
model initialize at round 2616
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4.03004415, 24.25277062]), 'previousTarget': array([ 4.3102453 , 24.11990655]), 'currentState': array([22.79894998, 17.34421261,  2.20425695]), 'targetState': array([ 2, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.28239683682035865
running average episode reward sum: 0.4848094638703221
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 25.]), 'previousTarget': array([ 2., 25.]), 'currentState': array([ 2.97505491, 24.75323072,  2.55048222]), 'targetState': array([ 2, 25], dtype=int32), 'currentDistance': 1.0057967773841785}
episode index:2617
target Thresh 31.999999999889656
target distance 9.0
model initialize at round 2617
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 21.]), 'previousTarget': array([ 8., 21.]), 'currentState': array([ 7.17863038, 12.34999611,  1.1414938 ]), 'targetState': array([ 8, 21], dtype=int32), 'currentDistance': 8.688913358948033}
done in step count: 17
reward sum = 0.6985548151866396
running average episode reward sum: 0.4848911083895414
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 21.]), 'previousTarget': array([ 8., 21.]), 'currentState': array([ 8.34868924, 20.1863108 ,  1.45375419]), 'targetState': array([ 8, 21], dtype=int32), 'currentDistance': 0.8852538041960448}
episode index:2618
target Thresh 31.999999999890754
target distance 20.0
model initialize at round 2618
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6.44217143, 8.27033049]), 'previousTarget': array([6.50609905, 8.38262381]), 'currentState': array([18.99358897, 23.84151919,  4.62829047]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 75
reward sum = 0.2317796521366532
running average episode reward sum: 0.4847944640763483
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.37661055, 4.77393264, 4.13885201]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.8607015959917674}
episode index:2619
target Thresh 31.99999999989184
target distance 16.0
model initialize at round 2619
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([19.        , 25.        ,  2.60626292]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 17.08800749063506}
done in step count: 42
reward sum = 0.38395304652596707
running average episode reward sum: 0.4847559749856802
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([24.56046992,  9.77417734,  5.2408113 ]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 0.8902456114320058}
episode index:2620
target Thresh 31.999999999892918
target distance 22.0
model initialize at round 2620
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5.51432179, 13.44075572]), 'previousTarget': array([ 5.48906088, 13.42734309]), 'currentState': array([24.01957468, 21.02729746,  1.17012495]), 'targetState': array([ 2, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.2251797200161328
running average episode reward sum: 0.4846569378796254
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 12.]), 'previousTarget': array([ 2., 12.]), 'currentState': array([ 2.87405267, 12.43559298,  3.65686958]), 'targetState': array([ 2, 12], dtype=int32), 'currentDistance': 0.9765804122677169}
episode index:2621
target Thresh 31.99999999989398
target distance 21.0
model initialize at round 2621
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.71986011,  5.28336929]), 'previousTarget': array([17.71986011,  5.28336929]), 'currentState': array([27.        , 23.        ,  2.61628151]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.23938454587236335
running average episode reward sum: 0.48456339387046926
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.97007589,  2.77228637,  4.52078086]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.7728658926222334}
episode index:2622
target Thresh 31.999999999895035
target distance 4.0
model initialize at round 2622
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  6.]), 'previousTarget': array([22.,  6.]), 'currentState': array([18.23611846,  7.08192907,  0.21646518]), 'targetState': array([22,  6], dtype=int32), 'currentDistance': 3.9162960486631566}
done in step count: 8
reward sum = 0.8617276469129036
running average episode reward sum: 0.48470718504585714
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  6.]), 'previousTarget': array([22.,  6.]), 'currentState': array([21.26942132,  6.07575781,  5.82853587]), 'targetState': array([22,  6], dtype=int32), 'currentDistance': 0.7344960582392873}
episode index:2623
target Thresh 31.99999999989608
target distance 12.0
model initialize at round 2623
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 14.]), 'previousTarget': array([ 2., 14.]), 'currentState': array([ 8.9835105 , 26.04722711,  2.12250209]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 13.924981148254721}
done in step count: 31
reward sum = 0.4768939734389619
running average episode reward sum: 0.4847042074499704
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 14.]), 'previousTarget': array([ 2., 14.]), 'currentState': array([ 2.19490191, 14.70613749,  4.35635345]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 0.7325414073298606}
episode index:2624
target Thresh 31.999999999897113
target distance 16.0
model initialize at round 2624
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.0031253, 12.8691893]), 'previousTarget': array([25.05153389, 12.82990784]), 'currentState': array([ 9.92856246, 26.01291613,  3.21522188]), 'targetState': array([26, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.2724805140360623
running average episode reward sum: 0.4846233603286698
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 12.]), 'previousTarget': array([26., 12.]), 'currentState': array([25.11133279, 12.62473406,  5.81596184]), 'targetState': array([26, 12], dtype=int32), 'currentDistance': 1.0862882064497696}
episode index:2625
target Thresh 31.99999999989814
target distance 25.0
model initialize at round 2625
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.13615827, 17.3140973 ]), 'previousTarget': array([13.13615827, 17.3140973 ]), 'currentState': array([26.        ,  2.        ,  4.88689068]), 'targetState': array([ 5, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 94
reward sum = 0.05898453344871979
running average episode reward sum: 0.4844612739513355
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 27.]), 'previousTarget': array([ 5., 27.]), 'currentState': array([ 5.94076443, 26.41221775,  2.52593305]), 'targetState': array([ 5, 27], dtype=int32), 'currentDistance': 1.1092906259387538}
episode index:2626
target Thresh 31.999999999899153
target distance 13.0
model initialize at round 2626
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([21.        , 18.        ,  1.43850893]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 15.811388300841896}
done in step count: 37
reward sum = 0.4366989402645802
running average episode reward sum: 0.4844430926290337
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([8.90403993, 9.88867267, 3.86824596]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 1.2676858112218299}
episode index:2627
target Thresh 31.999999999900155
target distance 5.0
model initialize at round 2627
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 19.]), 'previousTarget': array([12., 19.]), 'currentState': array([17.03864655, 22.00218013,  6.0870376 ]), 'targetState': array([12, 19], dtype=int32), 'currentDistance': 5.8652403726981195}
done in step count: 18
reward sum = 0.725353372786264
running average episode reward sum: 0.4845347632074801
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 19.]), 'previousTarget': array([12., 19.]), 'currentState': array([12.87979748, 19.40084978,  3.55569847]), 'targetState': array([12, 19], dtype=int32), 'currentDistance': 0.9668113315596732}
episode index:2628
target Thresh 31.99999999990115
target distance 7.0
model initialize at round 2628
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 13.]), 'previousTarget': array([16., 13.]), 'currentState': array([22.99131762,  8.97889138,  4.08675706]), 'targetState': array([16, 13], dtype=int32), 'currentDistance': 8.0652239038024}
done in step count: 19
reward sum = 0.6887373082320307
running average episode reward sum: 0.4846124362942144
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 13.]), 'previousTarget': array([16., 13.]), 'currentState': array([16.71919935, 12.36841446,  2.68065339]), 'targetState': array([16, 13], dtype=int32), 'currentDistance': 0.9571562063877083}
episode index:2629
target Thresh 31.999999999902133
target distance 7.0
model initialize at round 2629
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 19.]), 'previousTarget': array([18., 19.]), 'currentState': array([10.99020899, 20.98827604,  4.26909232]), 'targetState': array([18, 19], dtype=int32), 'currentDistance': 7.28631673873289}
done in step count: 18
reward sum = 0.6939818821212846
running average episode reward sum: 0.4846920444485213
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 19.]), 'previousTarget': array([18., 19.]), 'currentState': array([17.01166676, 18.9652293 ,  6.17259483]), 'targetState': array([18, 19], dtype=int32), 'currentDistance': 0.9889446869101114}
episode index:2630
target Thresh 31.999999999903107
target distance 12.0
model initialize at round 2630
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 12.]), 'previousTarget': array([23., 12.]), 'currentState': array([10.99985065,  2.15073922,  1.4351249 ]), 'targetState': array([23, 12], dtype=int32), 'currentDistance': 15.524545802929559}
done in step count: 36
reward sum = 0.48684406168619376
running average episode reward sum: 0.4846928623950199
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 12.]), 'previousTarget': array([23., 12.]), 'currentState': array([22.0815056 , 11.26352914,  0.75803618]), 'targetState': array([23, 12], dtype=int32), 'currentDistance': 1.1772940516611086}
episode index:2631
target Thresh 31.99999999990407
target distance 22.0
model initialize at round 2631
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.76343395,  8.82527831]), 'previousTarget': array([13.76343395,  8.82527831]), 'currentState': array([ 2.        , 25.        ,  4.13436007]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 82
reward sum = 0.18385744621512956
running average episode reward sum: 0.4845785632247388
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([17.12181007,  3.78386655,  5.42302308]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 1.1771424368562018}
episode index:2632
target Thresh 31.999999999905025
target distance 7.0
model initialize at round 2632
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  7.]), 'previousTarget': array([24.,  7.]), 'currentState': array([19.93988725, 13.83724562,  4.59025881]), 'targetState': array([24,  7], dtype=int32), 'currentDistance': 7.951882995218053}
done in step count: 14
reward sum = 0.7291927506349448
running average episode reward sum: 0.48467146644821396
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  7.]), 'previousTarget': array([24.,  7.]), 'currentState': array([23.10117996,  7.90835124,  5.33200627]), 'targetState': array([24,  7], dtype=int32), 'currentDistance': 1.2778808413860594}
episode index:2633
target Thresh 31.99999999990597
target distance 13.0
model initialize at round 2633
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([26.603355  ,  9.05068505,  3.08073919]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 12.60345691129564}
done in step count: 25
reward sum = 0.5688086658503769
running average episode reward sum: 0.4847034091966582
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.74133778,  9.33614907,  3.34861078]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.8139888835316337}
episode index:2634
target Thresh 31.999999999906905
target distance 4.0
model initialize at round 2634
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([21.98494211,  3.02766744,  1.83428591]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 4.986983056707282}
done in step count: 10
reward sum = 0.8264925040890891
running average episode reward sum: 0.48483312042811644
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([24.23209712,  6.04961649,  0.77272192]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 1.221844359093741}
episode index:2635
target Thresh 31.999999999907832
target distance 8.0
model initialize at round 2635
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 14.]), 'previousTarget': array([ 9., 14.]), 'currentState': array([16.99270562, 19.02675077,  2.08950377]), 'targetState': array([ 9, 14], dtype=int32), 'currentDistance': 9.44201071917459}
done in step count: 20
reward sum = 0.6417541521720962
running average episode reward sum: 0.48489265040980994
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 14.]), 'previousTarget': array([ 9., 14.]), 'currentState': array([ 9.91631178, 14.57259143,  3.85625556]), 'targetState': array([ 9, 14], dtype=int32), 'currentDistance': 1.0805036861917692}
episode index:2636
target Thresh 31.99999999990875
target distance 8.0
model initialize at round 2636
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 16.]), 'previousTarget': array([19., 16.]), 'currentState': array([17.01645395,  7.94136819,  5.23848343]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 8.299156677106705}
done in step count: 22
reward sum = 0.6383454042037291
running average episode reward sum: 0.4849508425803802
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 16.]), 'previousTarget': array([19., 16.]), 'currentState': array([19.19457607, 15.24353089,  1.42972847]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 0.781092418861547}
episode index:2637
target Thresh 31.999999999909658
target distance 10.0
model initialize at round 2637
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 15.]), 'previousTarget': array([10., 15.]), 'currentState': array([ 8.10518887, 24.72529143,  4.94268572]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 9.908158390905792}
done in step count: 18
reward sum = 0.6661525465134502
running average episode reward sum: 0.48501953162660205
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 15.]), 'previousTarget': array([10., 15.]), 'currentState': array([ 9.58988295, 15.94378194,  5.04007756]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 1.0290385549997636}
episode index:2638
target Thresh 31.999999999910557
target distance 17.0
model initialize at round 2638
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.48601371, 24.63363617]), 'previousTarget': array([18.33935727, 24.53366395]), 'currentState': array([ 2.19982411, 13.02501747,  0.33363902]), 'targetState': array([19, 25], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 48
reward sum = 0.3514735375751806
running average episode reward sum: 0.4849689268543203
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 25.]), 'previousTarget': array([19., 25.]), 'currentState': array([18.17283071, 24.21236401,  0.65380024]), 'targetState': array([19, 25], dtype=int32), 'currentDistance': 1.142181895580699}
episode index:2639
target Thresh 31.999999999911445
target distance 24.0
model initialize at round 2639
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.15120673, 21.3990199 ]), 'previousTarget': array([13.1492875, 21.40285  ]), 'currentState': array([18.00573601,  1.99712449,  6.07099199]), 'targetState': array([12, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 77
reward sum = 0.2069153208208166
running average episode reward sum: 0.48486360351870156
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 26.]), 'previousTarget': array([12., 26.]), 'currentState': array([12.81508461, 25.03463007,  1.75696474]), 'targetState': array([12, 26], dtype=int32), 'currentDistance': 1.2634484610682966}
episode index:2640
target Thresh 31.999999999912326
target distance 15.0
model initialize at round 2640
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 13.]), 'previousTarget': array([ 5., 13.]), 'currentState': array([20.00573546, 12.02010736,  1.51068586]), 'targetState': array([ 5, 13], dtype=int32), 'currentDistance': 15.03769550769016}
done in step count: 39
reward sum = 0.4638969663689656
running average episode reward sum: 0.48485566461784974
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 13.]), 'previousTarget': array([ 5., 13.]), 'currentState': array([ 5.79186386, 13.11227557,  3.38299707]), 'targetState': array([ 5, 13], dtype=int32), 'currentDistance': 0.7997838353693972}
episode index:2641
target Thresh 31.9999999999132
target distance 15.0
model initialize at round 2641
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.62110536, 25.64636501]), 'previousTarget': array([20.62110536, 25.64636501]), 'currentState': array([ 6.       , 12.       ,  5.2701931]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.3166876428302219
running average episode reward sum: 0.48479201283064777
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([20.43133564, 25.12172111,  0.95095451]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 1.0463044280080058}
episode index:2642
target Thresh 31.999999999914063
target distance 18.0
model initialize at round 2642
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([ 5.93783622, 25.99536142,  3.34111357]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 18.421172245402953}
done in step count: 48
reward sum = 0.3940056538324683
running average episode reward sum: 0.4847576630920938
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.86132803, 8.9066957 , 4.68858567]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.9172387980811808}
episode index:2643
target Thresh 31.99999999991492
target distance 20.0
model initialize at round 2643
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2.88101468, 10.69704774]), 'previousTarget': array([ 3.12283287, 10.60700849]), 'currentState': array([21.79406373,  4.19346599,  2.38012826]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.3389569698587832
running average episode reward sum: 0.4847025191082688
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.88521708, 10.89105511,  3.16951264]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.8918958812915231}
episode index:2644
target Thresh 31.999999999915765
target distance 17.0
model initialize at round 2644
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 6.]), 'previousTarget': array([9., 6.]), 'currentState': array([16.        , 23.        ,  3.02337894]), 'targetState': array([9, 6], dtype=int32), 'currentDistance': 18.38477631085023}
done in step count: 46
reward sum = 0.37937456723623364
running average episode reward sum: 0.4846626975763701
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 6.]), 'previousTarget': array([9., 6.]), 'currentState': array([9.16352845, 6.70152168, 4.39446807]), 'targetState': array([9, 6], dtype=int32), 'currentDistance': 0.7203292474962925}
episode index:2645
target Thresh 31.999999999916604
target distance 21.0
model initialize at round 2645
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3.57722394, 8.9723667 ]), 'previousTarget': array([4.02263725, 8.95130299]), 'currentState': array([23.55434526,  8.01600703,  3.12704207]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.3490530696144083
running average episode reward sum: 0.4846114467721517
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.8278914 , 9.1574192 , 3.31452225]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.8427247351730764}
episode index:2646
target Thresh 31.99999999991743
target distance 19.0
model initialize at round 2646
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([25.00486074, 19.00288647,  0.7864055 ]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 19.03130349949992}
done in step count: 49
reward sum = 0.33557490214404134
running average episode reward sum: 0.4845551428263156
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([ 6.9193315 , 18.22481999,  3.32334018]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 0.9464219086041519}
episode index:2647
target Thresh 31.999999999918256
target distance 7.0
model initialize at round 2647
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  9.]), 'previousTarget': array([20.,  9.]), 'currentState': array([27.02459798,  3.15013886,  1.65867043]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 9.141435998135968}
done in step count: 19
reward sum = 0.6682886739587255
running average episode reward sum: 0.48462452860091243
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  9.]), 'previousTarget': array([20.,  9.]), 'currentState': array([20.73641638,  8.64863011,  2.49797083]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 0.8159472359853593}
episode index:2648
target Thresh 31.99999999991907
target distance 21.0
model initialize at round 2648
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.67443729, 19.14037813]), 'previousTarget': array([20.64100589, 19.09400392]), 'currentState': array([3.99067766, 8.11077411, 1.49980575]), 'targetState': array([25, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.23578518165565981
running average episode reward sum: 0.48453059151259786
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 22.]), 'previousTarget': array([25., 22.]), 'currentState': array([24.06211507, 21.22546901,  0.55287674]), 'targetState': array([25, 22], dtype=int32), 'currentDistance': 1.2163578432362354}
episode index:2649
target Thresh 31.999999999919872
target distance 11.0
model initialize at round 2649
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([ 4.99998027, 14.0476155 ,  1.34118342]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 11.44770399107695}
done in step count: 30
reward sum = 0.5244225396878304
running average episode reward sum: 0.484545645077947
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([8.71812815, 3.85372985, 5.04844954]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 1.115599701612048}
episode index:2650
target Thresh 31.99999999992067
target distance 10.0
model initialize at round 2650
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([20.04909288, 14.97966492,  5.67159453]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 11.1842322448857}
done in step count: 27
reward sum = 0.5810658734127401
running average episode reward sum: 0.48458205406637955
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.08831379,  5.86512161,  4.07431495]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.8696175768160527}
episode index:2651
target Thresh 31.99999999992146
target distance 27.0
model initialize at round 2651
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7.71938318, 21.4504954 ]), 'previousTarget': array([ 7.68176659, 21.17596225]), 'currentState': array([1.93575615, 2.30500877, 1.65677105]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 71
reward sum = 0.17936268147775658
running average episode reward sum: 0.4844669638052225
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([ 9.52543496, 28.09031034,  1.05705072]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 1.026034723670861}
episode index:2652
target Thresh 31.99999999992224
target distance 5.0
model initialize at round 2652
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 7.]), 'previousTarget': array([6., 7.]), 'currentState': array([6.25517861, 2.40660751, 1.14448896]), 'targetState': array([6, 7], dtype=int32), 'currentDistance': 4.600475054933249}
done in step count: 8
reward sum = 0.8419985767912485
running average episode reward sum: 0.48460172883084857
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 7.]), 'previousTarget': array([6., 7.]), 'currentState': array([6.24944888, 6.02769998, 1.64998371]), 'targetState': array([6, 7], dtype=int32), 'currentDistance': 1.0037888616931305}
episode index:2653
target Thresh 31.999999999923016
target distance 11.0
model initialize at round 2653
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 13.]), 'previousTarget': array([ 9., 13.]), 'currentState': array([19.99901821,  3.99962421,  3.27629983]), 'targetState': array([ 9, 13], dtype=int32), 'currentDistance': 14.212148533605648}
done in step count: 31
reward sum = 0.5169372471541929
running average episode reward sum: 0.4846139125227564
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 13.]), 'previousTarget': array([ 9., 13.]), 'currentState': array([ 9.99565078, 12.65771821,  2.41879083]), 'targetState': array([ 9, 13], dtype=int32), 'currentDistance': 1.0528424868592283}
episode index:2654
target Thresh 31.99999999992378
target distance 16.0
model initialize at round 2654
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 20.]), 'previousTarget': array([18., 20.]), 'currentState': array([ 2.        , 21.        ,  0.81547546]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 16.0312195418814}
done in step count: 40
reward sum = 0.45417585523188087
running average episode reward sum: 0.4846024480943982
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 20.]), 'previousTarget': array([18., 20.]), 'currentState': array([1.71906372e+01, 1.98800577e+01, 7.80461822e-03]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 0.8182018581831575}
episode index:2655
target Thresh 31.99999999992454
target distance 17.0
model initialize at round 2655
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 24.]), 'previousTarget': array([16., 24.]), 'currentState': array([11.90503524,  6.97372884,  3.15898395]), 'targetState': array([16, 24], dtype=int32), 'currentDistance': 17.511785913180685}
done in step count: 53
reward sum = 0.33967025302779086
running average episode reward sum: 0.4845478802498701
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 24.]), 'previousTarget': array([16., 24.]), 'currentState': array([15.94462251, 23.32795148,  1.18362626]), 'targetState': array([16, 24], dtype=int32), 'currentDistance': 0.6743262430025072}
episode index:2656
target Thresh 31.99999999992529
target distance 19.0
model initialize at round 2656
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.98323494, 10.68323413]), 'previousTarget': array([16.97927459, 10.69147429]), 'currentState': array([27.0709753 , 27.9527881 ,  5.44472924]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.02348674959748866
running average episode reward sum: 0.4843566741415347
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.04863978,  9.05093565]), 'previousTarget': array([16.04863978,  9.05093565]), 'currentState': array([29.86103576, 23.51529589,  5.24105544]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 20.0}
episode index:2657
target Thresh 31.999999999926033
target distance 26.0
model initialize at round 2657
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.49719013, 21.48782391]), 'previousTarget': array([24.49719013, 21.48782391]), 'currentState': array([20.        ,  2.        ,  0.30588516]), 'targetState': array([26, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 75
reward sum = 0.19740666817533142
running average episode reward sum: 0.48424871702868055
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 28.]), 'previousTarget': array([26., 28.]), 'currentState': array([26.14889205, 27.11945681,  1.70842612]), 'targetState': array([26, 28], dtype=int32), 'currentDistance': 0.8930426414003536}
episode index:2658
target Thresh 31.999999999926768
target distance 4.0
model initialize at round 2658
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([ 7.95826302, 17.03638917,  2.21541467]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 4.098222387540369}
done in step count: 9
reward sum = 0.8469497867839282
running average episode reward sum: 0.4843851220944027
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([ 8.73343763, 20.12906912,  0.95435429]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 0.9108106821752284}
episode index:2659
target Thresh 31.9999999999275
target distance 13.0
model initialize at round 2659
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 26.]), 'previousTarget': array([ 5., 26.]), 'currentState': array([18.00252417, 20.00388038,  1.20694524]), 'targetState': array([ 5, 26], dtype=int32), 'currentDistance': 14.318487534334425}
done in step count: 33
reward sum = 0.4886815299110911
running average episode reward sum: 0.48438673728531123
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 26.]), 'previousTarget': array([ 5., 26.]), 'currentState': array([ 5.82345592, 26.10729505,  2.73612264]), 'targetState': array([ 5, 26], dtype=int32), 'currentDistance': 0.8304166889837173}
episode index:2660
target Thresh 31.99999999992822
target distance 23.0
model initialize at round 2660
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.41305321, 21.36998171]), 'previousTarget': array([14.42128976, 21.41810404]), 'currentState': array([3.03399561, 4.92257924, 5.3453396 ]), 'targetState': array([19, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 87
reward sum = 0.13872353286556866
running average episode reward sum: 0.48425683754670923
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 28.]), 'previousTarget': array([19., 28.]), 'currentState': array([18.04120817, 27.0658605 ,  1.15460543]), 'targetState': array([19, 28], dtype=int32), 'currentDistance': 1.338618083497739}
episode index:2661
target Thresh 31.999999999928935
target distance 15.0
model initialize at round 2661
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([14.        , 29.        ,  2.27853253]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 15.524174696260026}
done in step count: 37
reward sum = 0.43863272439168033
running average episode reward sum: 0.48423969851096355
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([10.32721315, 14.98765585,  4.62843941]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 1.0404482344306831}
episode index:2662
target Thresh 31.999999999929642
target distance 9.0
model initialize at round 2662
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 25.]), 'previousTarget': array([15., 25.]), 'currentState': array([23.73266955, 18.32667095,  2.41017404]), 'targetState': array([15, 25], dtype=int32), 'currentDistance': 10.990579519138917}
done in step count: 24
reward sum = 0.622970564133604
running average episode reward sum: 0.48429179421716806
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 25.]), 'previousTarget': array([15., 25.]), 'currentState': array([15.79097327, 24.42879105,  2.59665288]), 'targetState': array([15, 25], dtype=int32), 'currentDistance': 0.9756630496412789}
episode index:2663
target Thresh 31.999999999930342
target distance 22.0
model initialize at round 2663
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.34361962, 10.93509629]), 'previousTarget': array([12.54654412, 11.20119853]), 'currentState': array([25.84243425, 25.69253522,  4.23557995]), 'targetState': array([6, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 77
reward sum = 0.16742584057393067
running average episode reward sum: 0.48417285054087555
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 4.]), 'previousTarget': array([6., 4.]), 'currentState': array([6.60651608, 4.99739159, 4.07315665]), 'targetState': array([6, 4], dtype=int32), 'currentDistance': 1.167326749729432}
episode index:2664
target Thresh 31.999999999931035
target distance 5.0
model initialize at round 2664
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([12.18854766, 19.86723265,  5.89554691]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 4.942998630945737}
done in step count: 9
reward sum = 0.8221395573267612
running average episode reward sum: 0.484299667316405
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([16.30191754, 20.48777217,  0.30026003]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 0.8658501433798348}
episode index:2665
target Thresh 31.99999999993172
target distance 21.0
model initialize at round 2665
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.97736275, 11.04869701]), 'previousTarget': array([25.97736275, 11.04869701]), 'currentState': array([ 6.        , 12.        ,  3.62585521]), 'targetState': array([27, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.27644143375777774
running average episode reward sum: 0.4842217009872382
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 11.]), 'previousTarget': array([27., 11.]), 'currentState': array([26.26530233, 11.26488032,  0.34410867]), 'targetState': array([27, 11], dtype=int32), 'currentDistance': 0.7809879926599318}
episode index:2666
target Thresh 31.9999999999324
target distance 15.0
model initialize at round 2666
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  9.]), 'previousTarget': array([22.,  9.]), 'currentState': array([ 7.01252811, 19.05248426,  1.14405429]), 'targetState': array([22,  9], dtype=int32), 'currentDistance': 18.0465163746905}
done in step count: 46
reward sum = 0.3769307381556003
running average episode reward sum: 0.48418147190481164
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  9.]), 'previousTarget': array([22.,  9.]), 'currentState': array([21.05000481,  9.53594189,  5.66223028]), 'targetState': array([22,  9], dtype=int32), 'currentDistance': 1.0907449597697034}
episode index:2667
target Thresh 31.999999999933074
target distance 17.0
model initialize at round 2667
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.83916926, 17.9035585 ]), 'previousTarget': array([13.85099785, 17.88715666]), 'currentState': array([25.99469652,  2.02139375,  1.7222266 ]), 'targetState': array([13, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.009713357436071778
running average episode reward sum: 0.4839963539028098
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 19.]), 'previousTarget': array([13., 19.]), 'currentState': array([25.82027683,  5.31504066,  1.62571487]), 'targetState': array([13, 19], dtype=int32), 'currentDistance': 18.75200282364012}
episode index:2668
target Thresh 31.99999999993374
target distance 21.0
model initialize at round 2668
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.03884345,  7.49650608]), 'previousTarget': array([9.99469707, 7.47290771]), 'currentState': array([27.05117474, 18.0122364 ,  6.26538879]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.09698682017217032
running average episode reward sum: 0.4837786756809758
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 5.]), 'previousTarget': array([6., 5.]), 'currentState': array([20.98313936, 13.88087945,  3.87245338]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 17.417361590111323}
episode index:2669
target Thresh 31.9999999999344
target distance 9.0
model initialize at round 2669
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  8.]), 'previousTarget': array([25.,  8.]), 'currentState': array([15.99366701,  1.98688732,  4.51496077]), 'targetState': array([25,  8], dtype=int32), 'currentDistance': 10.829199321856839}
done in step count: 28
reward sum = 0.5663753951686558
running average episode reward sum: 0.4838096107819076
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  8.]), 'previousTarget': array([25.,  8.]), 'currentState': array([24.10608102,  7.49640366,  0.84021412]), 'targetState': array([25,  8], dtype=int32), 'currentDistance': 1.0260118990637008}
episode index:2670
target Thresh 31.99999999993505
target distance 16.0
model initialize at round 2670
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([ 5.        , 13.        ,  0.98639601]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 17.46424919657298}
done in step count: 42
reward sum = 0.4001103342991368
running average episode reward sum: 0.4837782744747257
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([20.07793338,  6.22369718,  5.90333397]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 0.9488136147347119}
episode index:2671
target Thresh 31.999999999935696
target distance 13.0
model initialize at round 2671
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 16.]), 'previousTarget': array([ 3., 16.]), 'currentState': array([16.03440066, 18.02858038,  0.94574678]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 13.19131301004699}
done in step count: 33
reward sum = 0.48727648790030864
running average episode reward sum: 0.4837795836863371
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 16.]), 'previousTarget': array([ 3., 16.]), 'currentState': array([ 3.76564336, 16.0976051 ,  3.61660271]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 0.7718396890521823}
episode index:2672
target Thresh 31.999999999936335
target distance 8.0
model initialize at round 2672
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 25.]), 'previousTarget': array([16., 25.]), 'currentState': array([ 8.22573795, 25.03923261,  0.10117626]), 'targetState': array([16, 25], dtype=int32), 'currentDistance': 7.774361038873954}
done in step count: 18
reward sum = 0.7270548222298236
running average episode reward sum: 0.48387059574714647
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 25.]), 'previousTarget': array([16., 25.]), 'currentState': array([15.2860542, 25.0244127,  0.1886169]), 'targetState': array([16, 25], dtype=int32), 'currentDistance': 0.7143630643847507}
episode index:2673
target Thresh 31.99999999993697
target distance 2.0
model initialize at round 2673
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([1.96756207, 3.04286742, 1.97319442]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 2.2843765929064097}
done in step count: 9
reward sum = 0.8562704244523812
running average episode reward sum: 0.48400986269879387
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.2329412 , 2.90176801, 5.77238779]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 1.1838769987985323}
episode index:2674
target Thresh 31.999999999937597
target distance 22.0
model initialize at round 2674
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.30475358, 24.41006552]), 'previousTarget': array([11.26234812, 24.29527642]), 'currentState': array([6.11949712, 5.09392937, 0.90520513]), 'targetState': array([12, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.014566771523036011
running average episode reward sum: 0.4838234789102997
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.8976735 , 26.57821641]), 'previousTarget': array([11.8976735 , 26.57821641]), 'currentState': array([7.18236841, 7.14201571, 1.05315708]), 'targetState': array([12, 27], dtype=int32), 'currentDistance': 20.0}
episode index:2675
target Thresh 31.99999999993822
target distance 12.0
model initialize at round 2675
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 16.]), 'previousTarget': array([10., 16.]), 'currentState': array([21.98992534, 28.00064412,  3.32952091]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 16.963896042954893}
done in step count: 44
reward sum = 0.41750669710864957
running average episode reward sum: 0.48379869685432
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 16.]), 'previousTarget': array([10., 16.]), 'currentState': array([10.1011912 , 16.94430925,  4.10518793]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 0.9497155451511522}
episode index:2676
target Thresh 31.999999999938833
target distance 16.0
model initialize at round 2676
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  2.]), 'previousTarget': array([26.,  2.]), 'currentState': array([10.        ,  5.        ,  1.12265503]), 'targetState': array([26,  2], dtype=int32), 'currentDistance': 16.278820596099706}
done in step count: 43
reward sum = 0.42661463396300003
running average episode reward sum: 0.4837773356055747
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  2.]), 'previousTarget': array([26.,  2.]), 'currentState': array([25.16200365,  1.86138954,  6.17887631]), 'targetState': array([26,  2], dtype=int32), 'currentDistance': 0.8493825687654698}
episode index:2677
target Thresh 31.99999999993944
target distance 13.0
model initialize at round 2677
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 18.]), 'previousTarget': array([ 7., 18.]), 'currentState': array([20.01916546, 16.01027984,  0.72732848]), 'targetState': array([ 7, 18], dtype=int32), 'currentDistance': 13.170332401782632}
done in step count: 29
reward sum = 0.4912943727921139
running average episode reward sum: 0.4837801425649423
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 18.]), 'previousTarget': array([ 7., 18.]), 'currentState': array([ 7.88100941, 18.27375645,  3.11489259]), 'targetState': array([ 7, 18], dtype=int32), 'currentDistance': 0.9225617475314}
episode index:2678
target Thresh 31.999999999940044
target distance 11.0
model initialize at round 2678
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([ 2.97471961, 14.02779032,  2.53472078]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 12.118811913336483}
done in step count: 30
reward sum = 0.5271673622818199
running average episode reward sum: 0.4837963378690545
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([7.16743741, 3.98528259, 5.22762345]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 1.2899388551512763}
episode index:2679
target Thresh 31.99999999994064
target distance 4.0
model initialize at round 2679
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 12.]), 'previousTarget': array([ 6., 12.]), 'currentState': array([ 1.99546094, 11.03956568,  1.4325192 ]), 'targetState': array([ 6, 12], dtype=int32), 'currentDistance': 4.11810237712188}
done in step count: 9
reward sum = 0.8276609538250084
running average episode reward sum: 0.4839246455615754
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 12.]), 'previousTarget': array([ 6., 12.]), 'currentState': array([ 5.18035848, 11.46721539,  0.05730662]), 'targetState': array([ 6, 12], dtype=int32), 'currentDistance': 0.9775846059093952}
episode index:2680
target Thresh 31.99999999994123
target distance 14.0
model initialize at round 2680
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 29.]), 'previousTarget': array([11., 29.]), 'currentState': array([17.        , 15.        ,  5.69544196]), 'targetState': array([11, 29], dtype=int32), 'currentDistance': 15.231546211727817}
done in step count: 37
reward sum = 0.43581000784761836
running average episode reward sum: 0.4839066990350129
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 29.]), 'previousTarget': array([11., 29.]), 'currentState': array([11.59931559, 28.17462097,  2.0840134 ]), 'targetState': array([11, 29], dtype=int32), 'currentDistance': 1.020014567184311}
episode index:2681
target Thresh 31.999999999941817
target distance 18.0
model initialize at round 2681
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  6.]), 'previousTarget': array([25.,  6.]), 'currentState': array([7.10026977, 7.08604506, 0.65178214]), 'targetState': array([25,  6], dtype=int32), 'currentDistance': 17.932647213250373}
done in step count: 42
reward sum = 0.413230617925563
running average episode reward sum: 0.48388034702863353
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  6.]), 'previousTarget': array([25.,  6.]), 'currentState': array([24.23882349,  5.70243317,  0.08314276]), 'targetState': array([25,  6], dtype=int32), 'currentDistance': 0.8172733325446268}
episode index:2682
target Thresh 31.999999999942396
target distance 17.0
model initialize at round 2682
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.86599244,  3.20656109]), 'previousTarget': array([13.86502556,  3.20859686]), 'currentState': array([ 2.98091342, 19.98497146,  4.05711997]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.07658498055855344
running average episode reward sum: 0.4836714520127605
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([ 7.90914344, 11.58674435,  5.36976896]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 10.527616644179034}
episode index:2683
target Thresh 31.99999999994297
target distance 9.0
model initialize at round 2683
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  6.]), 'previousTarget': array([22.,  6.]), 'currentState': array([18.00478794, 14.99994959,  6.02015649]), 'targetState': array([22,  6], dtype=int32), 'currentDistance': 9.846868131039528}
done in step count: 25
reward sum = 0.6435019728906846
running average episode reward sum: 0.48373100138715613
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  6.]), 'previousTarget': array([22.,  6.]), 'currentState': array([21.44975777,  6.84299641,  5.35217924]), 'targetState': array([22,  6], dtype=int32), 'currentDistance': 1.0066823982087019}
episode index:2684
target Thresh 31.999999999943537
target distance 2.0
model initialize at round 2684
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 26.]), 'previousTarget': array([15., 26.]), 'currentState': array([17.01296523, 28.02546208,  1.14979357]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 2.855613009211092}
done in step count: 9
reward sum = 0.8315130757401588
running average episode reward sum: 0.4838605291615893
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 26.]), 'previousTarget': array([15., 26.]), 'currentState': array([15.40625317, 26.85804313,  4.2347358 ]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 0.9493574949060168}
episode index:2685
target Thresh 31.999999999944098
target distance 16.0
model initialize at round 2685
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 23.]), 'previousTarget': array([17., 23.]), 'currentState': array([16.94501038,  7.27603546,  1.75163049]), 'targetState': array([17, 23], dtype=int32), 'currentDistance': 15.724060693981771}
done in step count: 36
reward sum = 0.47963541743155025
running average episode reward sum: 0.48385895614903157
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 23.]), 'previousTarget': array([17., 23.]), 'currentState': array([17.08217786, 22.1041887 ,  1.26247718]), 'targetState': array([17, 23], dtype=int32), 'currentDistance': 0.8995727212879557}
episode index:2686
target Thresh 31.999999999944652
target distance 6.0
model initialize at round 2686
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 24.]), 'previousTarget': array([16., 24.]), 'currentState': array([ 9.98652757, 27.99526071,  3.69237354]), 'targetState': array([16, 24], dtype=int32), 'currentDistance': 7.2196924320051}
done in step count: 16
reward sum = 0.7022078478204127
running average episode reward sum: 0.4839402173666242
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 24.]), 'previousTarget': array([16., 24.]), 'currentState': array([15.01824895, 24.25085253,  5.83140452]), 'targetState': array([16, 24], dtype=int32), 'currentDistance': 1.013292709975805}
episode index:2687
target Thresh 31.999999999945203
target distance 18.0
model initialize at round 2687
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5.65952152, 21.78943194]), 'previousTarget': array([ 5.63557441, 21.80368799]), 'currentState': array([21.04014642,  9.00520816,  0.36859378]), 'targetState': array([ 3, 24], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.048064217609245265
running average episode reward sum: 0.4837422990500409
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3.4001677 , 23.69567636]), 'previousTarget': array([ 3.40995382, 23.68806857]), 'currentState': array([19.3196509 , 11.58906434,  2.10754834]), 'targetState': array([ 3, 24], dtype=int32), 'currentDistance': 20.0}
episode index:2688
target Thresh 31.99999999994575
target distance 20.0
model initialize at round 2688
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.97357344, 21.94354927]), 'previousTarget': array([17.94427191, 21.88854382]), 'currentState': array([9.04182601, 4.04874869, 0.76224843]), 'targetState': array([19, 24], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.051850884981844404
running average episode reward sum: 0.4835431197328108
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 24.]), 'previousTarget': array([19., 24.]), 'currentState': array([13.89284881, 12.57727543,  1.23807969]), 'targetState': array([19, 24], dtype=int32), 'currentDistance': 12.512458983375643}
episode index:2689
target Thresh 31.99999999994629
target distance 3.0
model initialize at round 2689
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 15.]), 'previousTarget': array([ 6., 15.]), 'currentState': array([ 9.        , 18.        ,  1.77508798]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 4.242640687119284}
done in step count: 10
reward sum = 0.7968279049944937
running average episode reward sum: 0.4836595824782612
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 15.]), 'previousTarget': array([ 6., 15.]), 'currentState': array([ 6.25795034, 15.98978148,  4.01824544]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 1.0228420022237015}
episode index:2690
target Thresh 31.999999999946823
target distance 3.0
model initialize at round 2690
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 27.]), 'previousTarget': array([ 7., 27.]), 'currentState': array([ 8.9928832 , 23.98810461,  3.92073345]), 'targetState': array([ 7, 27], dtype=int32), 'currentDistance': 3.611522846239662}
done in step count: 10
reward sum = 0.8297896814260978
running average episode reward sum: 0.4837882075614822
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 27.]), 'previousTarget': array([ 7., 27.]), 'currentState': array([ 7.45400771, 26.21226536,  1.9237011 ]), 'targetState': array([ 7, 27], dtype=int32), 'currentDistance': 0.9092023256335074}
episode index:2691
target Thresh 31.999999999947352
target distance 23.0
model initialize at round 2691
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.92535561, 12.24778806]), 'previousTarget': array([21.92535561, 12.24778806]), 'currentState': array([11.        , 29.        ,  3.81373304]), 'targetState': array([26,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.043610173793744524
running average episode reward sum: 0.4835922943440397
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.49210228,  8.04953807]), 'previousTarget': array([24.49210228,  8.04953807]), 'currentState': array([12.6397815 , 24.15923988,  5.09151383]), 'targetState': array([26,  6], dtype=int32), 'currentDistance': 20.0}
episode index:2692
target Thresh 31.999999999947878
target distance 17.0
model initialize at round 2692
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([16.        , 27.        ,  2.09324753]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 17.26267650163207}
done in step count: 46
reward sum = 0.37232900966111354
running average episode reward sum: 0.48355097860520463
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([12.71952695, 10.81509104,  4.49932081]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.8619968322723677}
episode index:2693
target Thresh 31.999999999948397
target distance 9.0
model initialize at round 2693
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  6.]), 'previousTarget': array([17.,  6.]), 'currentState': array([8.04089526, 1.9638404 , 5.78714114]), 'targetState': array([17,  6], dtype=int32), 'currentDistance': 9.826298491530524}
done in step count: 19
reward sum = 0.655753960910644
running average episode reward sum: 0.4836148995340485
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  6.]), 'previousTarget': array([17.,  6.]), 'currentState': array([16.1017733 ,  5.07195319,  0.52435304]), 'targetState': array([17,  6], dtype=int32), 'currentDistance': 1.2915425232872675}
episode index:2694
target Thresh 31.99999999994891
target distance 20.0
model initialize at round 2694
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.8342402 , 12.02436888]), 'previousTarget': array([22.77872706, 12.03319094]), 'currentState': array([ 3.04692752, 14.93336472,  5.57845783]), 'targetState': array([23, 12], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 61
reward sum = 0.3144274795706679
running average episode reward sum: 0.4835521212706113
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 12.]), 'previousTarget': array([23., 12.]), 'currentState': array([22.02604321, 11.85634675,  6.27758866]), 'targetState': array([23, 12], dtype=int32), 'currentDistance': 0.9844938207811995}
episode index:2695
target Thresh 31.999999999949416
target distance 11.0
model initialize at round 2695
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([ 7.32000971, 14.65773814,  5.49345542]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 14.397555154915269}
done in step count: 31
reward sum = 0.5210337883164851
running average episode reward sum: 0.4835660239661031
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([16.1547313 ,  4.57781799,  5.35129332]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 1.0238910130597874}
episode index:2696
target Thresh 31.99999999994992
target distance 20.0
model initialize at round 2696
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.38463841, 16.53075311]), 'previousTarget': array([23.38463841, 16.53075311]), 'currentState': array([ 7.        , 28.        ,  4.81849983]), 'targetState': array([27, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 87
reward sum = 0.2127498781220272
running average episode reward sum: 0.4834656101189232
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 14.]), 'previousTarget': array([27., 14.]), 'currentState': array([26.02969831, 13.94519897,  5.70798796]), 'targetState': array([27, 14], dtype=int32), 'currentDistance': 0.9718479937386582}
episode index:2697
target Thresh 31.99999999995042
target distance 18.0
model initialize at round 2697
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([ 7.22327561, 13.97814498,  6.04883876]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 19.097288746319613}
done in step count: 51
reward sum = 0.37796111828508006
running average episode reward sum: 0.4834265054147595
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([24.08139276,  7.15875318,  5.89505312]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 0.9322241299686851}
episode index:2698
target Thresh 31.999999999950912
target distance 14.0
model initialize at round 2698
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([14.        , 10.        ,  0.32295626]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 14.866068747318504}
done in step count: 33
reward sum = 0.46764747059880246
running average episode reward sum: 0.48342065916251203
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([ 9.71207799, 23.28636254,  1.8892745 ]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 1.0081336626583361}
episode index:2699
target Thresh 31.9999999999514
target distance 10.0
model initialize at round 2699
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([22.07968411, 19.95902292,  5.55570745]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 13.476750386789801}
done in step count: 29
reward sum = 0.49770202733117286
running average episode reward sum: 0.48342594855813004
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([12.95151313, 10.80424507,  3.93613992]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.8057053495510582}
episode index:2700
target Thresh 31.999999999951886
target distance 21.0
model initialize at round 2700
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.98417253,  8.81486795]), 'previousTarget': array([11.98417253,  8.81486795]), 'currentState': array([25.        , 24.        ,  4.85023645]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.10640886054964764
running average episode reward sum: 0.48320757210159254
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'previousTarget': array([7., 3.]), 'currentState': array([13.30547057, 13.09920795,  4.21790721]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 11.906005218536965}
episode index:2701
target Thresh 31.99999999995236
target distance 13.0
model initialize at round 2701
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 15.]), 'previousTarget': array([18., 15.]), 'currentState': array([5.        , 9.        , 2.57725501]), 'targetState': array([18, 15], dtype=int32), 'currentDistance': 14.317821063276352}
done in step count: 36
reward sum = 0.46866311913179237
running average episode reward sum: 0.48320218925445346
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 15.]), 'previousTarget': array([18., 15.]), 'currentState': array([17.28706957, 14.49384532,  0.46377135]), 'targetState': array([18, 15], dtype=int32), 'currentDistance': 0.8743353762344387}
episode index:2702
target Thresh 31.999999999952838
target distance 26.0
model initialize at round 2702
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.79868184, 22.31234796]), 'previousTarget': array([14.80053053, 22.31231517]), 'currentState': array([19.99319682,  2.99869957,  3.10446456]), 'targetState': array([13, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.16301262110260578
running average episode reward sum: 0.4829631160726714
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 29.]), 'previousTarget': array([13., 29.]), 'currentState': array([15.24727496, 22.81592756,  1.96296138]), 'targetState': array([13, 29], dtype=int32), 'currentDistance': 6.579741387518985}
episode index:2703
target Thresh 31.999999999953307
target distance 21.0
model initialize at round 2703
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8.3102453 , 23.11990655]), 'previousTarget': array([ 8.3102453 , 23.11990655]), 'currentState': array([27.        , 16.        ,  3.84982982]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 20.0}
done in step count: 82
reward sum = 0.2161028046508178
running average episode reward sum: 0.48286442512909816
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 24.]), 'previousTarget': array([ 6., 24.]), 'currentState': array([ 6.81777483, 23.94445895,  2.82771415]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 0.8196587595254583}
episode index:2704
target Thresh 31.999999999953772
target distance 9.0
model initialize at round 2704
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 25.]), 'previousTarget': array([16., 25.]), 'currentState': array([ 7.        , 19.        ,  4.02193782]), 'targetState': array([16, 25], dtype=int32), 'currentDistance': 10.816653826391969}
done in step count: 27
reward sum = 0.5560742227001475
running average episode reward sum: 0.4828914897492723
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 25.]), 'previousTarget': array([16., 25.]), 'currentState': array([15.46912799, 24.18705562,  0.76457419]), 'targetState': array([16, 25], dtype=int32), 'currentDistance': 0.970929274638491}
episode index:2705
target Thresh 31.99999999995423
target distance 5.0
model initialize at round 2705
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  4.]), 'previousTarget': array([20.,  4.]), 'currentState': array([21.97253451,  9.01576096,  2.87312627]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 5.389689262828045}
done in step count: 11
reward sum = 0.7783087086623512
running average episode reward sum: 0.4830006609314279
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  4.]), 'previousTarget': array([20.,  4.]), 'currentState': array([19.92022912,  4.97298958,  4.36464999]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 0.9762541254241619}
episode index:2706
target Thresh 31.999999999954685
target distance 10.0
model initialize at round 2706
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([16.2029764 , 25.55030148,  5.38223839]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 11.259996355298275}
done in step count: 24
reward sum = 0.6001055957486132
running average episode reward sum: 0.4830439209738428
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([25.0620698 , 20.11308186,  5.71268517]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 0.9447224797593845}
episode index:2707
target Thresh 31.999999999955136
target distance 10.0
model initialize at round 2707
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.97215666, 20.00629752,  3.16714522]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 10.053411294856952}
done in step count: 24
reward sum = 0.6189044318953976
running average episode reward sum: 0.4830940910295745
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 2.8251243 , 10.87102096,  4.63884026]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.8884025094078452}
episode index:2708
target Thresh 31.999999999955584
target distance 16.0
model initialize at round 2708
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 18.]), 'previousTarget': array([ 2., 18.]), 'currentState': array([13.        ,  2.        ,  0.90229136]), 'targetState': array([ 2, 18], dtype=int32), 'currentDistance': 19.416487838947603}
done in step count: 43
reward sum = 0.34647704944274643
running average episode reward sum: 0.48304366022795514
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 18.]), 'previousTarget': array([ 2., 18.]), 'currentState': array([ 2.66300037, 17.29498488,  2.26564344]), 'targetState': array([ 2, 18], dtype=int32), 'currentDistance': 0.9677891363306952}
episode index:2709
target Thresh 31.999999999956025
target distance 15.0
model initialize at round 2709
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([ 4.0046727 , 18.00457483,  0.53739867]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 15.137898750048382}
done in step count: 36
reward sum = 0.4406760030703426
running average episode reward sum: 0.48302802640612574
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.06135359, 3.8550606 , 4.48121451]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.8572589400851902}
episode index:2710
target Thresh 31.99999999995646
target distance 11.0
model initialize at round 2710
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([ 3.00333492, 19.99759052,  5.40501738]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 10.99759102256002}
done in step count: 23
reward sum = 0.613371254097095
running average episode reward sum: 0.4830761057966425
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.87824999, 9.85001465, 4.65605093]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.8586896823468839}
episode index:2711
target Thresh 31.999999999956895
target distance 18.0
model initialize at round 2711
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.09862434, 19.44446327]), 'previousTarget': array([20.06563667, 19.42900019]), 'currentState': array([3.07257795, 8.95095363, 5.94140339]), 'targetState': array([21, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.29350510647128025
running average episode reward sum: 0.4830062049856818
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 20.]), 'previousTarget': array([21., 20.]), 'currentState': array([20.22661525, 19.09005931,  0.45401772]), 'targetState': array([21, 20], dtype=int32), 'currentDistance': 1.194200997638848}
episode index:2712
target Thresh 31.999999999957325
target distance 22.0
model initialize at round 2712
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.62177138, 25.60645459]), 'previousTarget': array([20.51093912, 25.57265691]), 'currentState': array([2.13304972e+00, 1.79797140e+01, 1.60524209e-02]), 'targetState': array([24, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.11330297040174428
running average episode reward sum: 0.48278640801723827
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 27.]), 'previousTarget': array([24., 27.]), 'currentState': array([18.19197338, 24.64643205,  0.3984532 ]), 'targetState': array([24, 27], dtype=int32), 'currentDistance': 6.266773916462595}
episode index:2713
target Thresh 31.99999999995775
target distance 16.0
model initialize at round 2713
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 27.]), 'previousTarget': array([12., 27.]), 'currentState': array([19.02035766, 11.02686811,  1.17489272]), 'targetState': array([12, 27], dtype=int32), 'currentDistance': 17.44781831411335}
done in step count: 36
reward sum = 0.40763231662401483
running average episode reward sum: 0.4827587167529076
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 27.]), 'previousTarget': array([12., 27.]), 'currentState': array([12.55256162, 26.32625632,  1.82037082]), 'targetState': array([12, 27], dtype=int32), 'currentDistance': 0.871352336722338}
episode index:2714
target Thresh 31.99999999995817
target distance 6.0
model initialize at round 2714
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([12.43667784,  3.23583719,  0.7084589 ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 6.776639117663998}
done in step count: 12
reward sum = 0.7599198433679258
running average episode reward sum: 0.4828608018824159
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.71580355,  8.05306471,  1.04547778]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.9886627641565032}
episode index:2715
target Thresh 31.999999999958586
target distance 18.0
model initialize at round 2715
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.29018892,  3.78641543]), 'previousTarget': array([15.29018892,  3.78641543]), 'currentState': array([27.       , 20.       ,  5.8699962]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 70
reward sum = 0.22262088921815254
running average episode reward sum: 0.48276498453607414
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.20436862,  2.74445654,  4.14253097]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.7719987533294383}
episode index:2716
target Thresh 31.999999999958998
target distance 7.0
model initialize at round 2716
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 12.]), 'previousTarget': array([23., 12.]), 'currentState': array([16.14814816,  6.28794683,  1.0321728 ]), 'targetState': array([23, 12], dtype=int32), 'currentDistance': 8.920505877630529}
done in step count: 17
reward sum = 0.7035956517563111
running average episode reward sum: 0.4828462619255553
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 12.]), 'previousTarget': array([23., 12.]), 'currentState': array([22.08424019, 11.37711873,  0.74919585]), 'targetState': array([23, 12], dtype=int32), 'currentDistance': 1.1075184437838363}
episode index:2717
target Thresh 31.999999999959407
target distance 13.0
model initialize at round 2717
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([12.00028965, 15.00013885,  0.21533802]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 16.401506128654198}
done in step count: 39
reward sum = 0.40405146364659766
running average episode reward sum: 0.48281727193354684
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.82899675, 2.61845287, 3.90229833]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 1.034272484456103}
episode index:2718
target Thresh 31.99999999995981
target distance 11.0
model initialize at round 2718
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([26.64312363, 24.86329804,  3.66124645]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 12.664159687535864}
done in step count: 26
reward sum = 0.5798165551609578
running average episode reward sum: 0.48285294655040134
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([16.86300461, 18.94694195,  3.69831181]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 1.2812010042205415}
episode index:2719
target Thresh 31.99999999996021
target distance 18.0
model initialize at round 2719
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 22.]), 'previousTarget': array([18., 22.]), 'currentState': array([10.        ,  4.        ,  4.89707625]), 'targetState': array([18, 22], dtype=int32), 'currentDistance': 19.697715603592208}
done in step count: 63
reward sum = 0.2948365011029548
running average episode reward sum: 0.48278382285722216
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 22.]), 'previousTarget': array([18., 22.]), 'currentState': array([17.74144875, 21.08438482,  0.95114796]), 'targetState': array([18, 22], dtype=int32), 'currentDistance': 0.95141993966345}
episode index:2720
target Thresh 31.999999999960608
target distance 11.0
model initialize at round 2720
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 17.]), 'previousTarget': array([ 3., 17.]), 'currentState': array([ 3.        , 28.        ,  2.70630288]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 11.0}
done in step count: 25
reward sum = 0.5690020349109
running average episode reward sum: 0.4828155090799541
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 17.]), 'previousTarget': array([ 3., 17.]), 'currentState': array([ 2.62736621, 17.74941764,  4.86471863]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 0.8369484665728404}
episode index:2721
target Thresh 31.999999999961
target distance 17.0
model initialize at round 2721
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([21.97962715, 28.03076555,  2.40819812]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 18.807483205733483}
done in step count: 49
reward sum = 0.34976435560123637
running average episode reward sum: 0.4827666291558253
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.98306249, 11.92594396,  4.19997141]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.9260988633941518}
episode index:2722
target Thresh 31.999999999961386
target distance 13.0
model initialize at round 2722
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 29.]), 'previousTarget': array([16., 29.]), 'currentState': array([23.00378175, 16.00028342,  0.29616108]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 14.76636684766241}
done in step count: 37
reward sum = 0.4664256965420551
running average episode reward sum: 0.4827606280788463
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 29.]), 'previousTarget': array([16., 29.]), 'currentState': array([16.97797249, 28.6206758 ,  2.1888917 ]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 1.0489599850496956}
episode index:2723
target Thresh 31.99999999996177
target distance 12.0
model initialize at round 2723
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 26.]), 'previousTarget': array([20., 26.]), 'currentState': array([ 8.        , 24.        ,  2.47614479]), 'targetState': array([20, 26], dtype=int32), 'currentDistance': 12.165525060596439}
done in step count: 35
reward sum = 0.5252888423076154
running average episode reward sum: 0.482776240492293
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 26.]), 'previousTarget': array([20., 26.]), 'currentState': array([19.02734997, 25.54564876,  0.31891892]), 'targetState': array([20, 26], dtype=int32), 'currentDistance': 1.073537673293797}
episode index:2724
target Thresh 31.99999999996215
target distance 8.0
model initialize at round 2724
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 21.]), 'previousTarget': array([23., 21.]), 'currentState': array([25.007925  , 13.4979863 ,  1.70950761]), 'targetState': array([23, 21], dtype=int32), 'currentDistance': 7.766078312719106}
done in step count: 15
reward sum = 0.7241979358334919
running average episode reward sum: 0.4828648356098494
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 21.]), 'previousTarget': array([23., 21.]), 'currentState': array([23.33023757, 20.27612091,  1.90145548]), 'targetState': array([23, 21], dtype=int32), 'currentDistance': 0.7956492861745529}
episode index:2725
target Thresh 31.999999999962526
target distance 13.0
model initialize at round 2725
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 28.]), 'previousTarget': array([ 4., 28.]), 'currentState': array([16.99895167, 23.95750551,  4.46436703]), 'targetState': array([ 4, 28], dtype=int32), 'currentDistance': 13.613027081550547}
done in step count: 34
reward sum = 0.4993949909207695
running average episode reward sum: 0.48287089949661055
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 28.]), 'previousTarget': array([ 4., 28.]), 'currentState': array([ 4.80944564, 28.18783068,  2.72911617]), 'targetState': array([ 4, 28], dtype=int32), 'currentDistance': 0.8309528330308087}
episode index:2726
target Thresh 31.9999999999629
target distance 13.0
model initialize at round 2726
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 14.]), 'previousTarget': array([21., 14.]), 'currentState': array([ 9.        , 27.        ,  0.74125654]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 17.69180601295413}
done in step count: 46
reward sum = 0.3909039385638175
running average episode reward sum: 0.48283717490514266
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 14.]), 'previousTarget': array([21., 14.]), 'currentState': array([20.10175369, 14.60439318,  5.2610382 ]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 1.0826530151970353}
episode index:2727
target Thresh 31.99999999996327
target distance 13.0
model initialize at round 2727
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([22.10593115, 19.91742117,  5.38929218]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 12.948325327887217}
done in step count: 28
reward sum = 0.5583285746510098
running average episode reward sum: 0.4828648477056361
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([22.70270201,  7.94381173,  4.97172993]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 0.9895285143191256}
episode index:2728
target Thresh 31.999999999963634
target distance 8.0
model initialize at round 2728
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 28.]), 'previousTarget': array([ 9., 28.]), 'currentState': array([17.        , 25.        ,  3.81881654]), 'targetState': array([ 9, 28], dtype=int32), 'currentDistance': 8.54400374531753}
done in step count: 20
reward sum = 0.6823551899572124
running average episode reward sum: 0.48293794786769234
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 28.]), 'previousTarget': array([ 9., 28.]), 'currentState': array([ 9.90761891, 27.88597209,  2.52791441]), 'targetState': array([ 9, 28], dtype=int32), 'currentDistance': 0.9147537660214122}
episode index:2729
target Thresh 31.999999999963997
target distance 16.0
model initialize at round 2729
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 27.]), 'previousTarget': array([17., 27.]), 'currentState': array([12.0005318 , 10.99886683,  5.36217532]), 'targetState': array([17, 27], dtype=int32), 'currentDistance': 16.76397759826289}
done in step count: 39
reward sum = 0.40283615126982736
running average episode reward sum: 0.48290860655025725
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 27.]), 'previousTarget': array([17., 27.]), 'currentState': array([16.91253549, 26.0691919 ,  1.19888644]), 'targetState': array([17, 27], dtype=int32), 'currentDistance': 0.9349084271864523}
episode index:2730
target Thresh 31.999999999964356
target distance 16.0
model initialize at round 2730
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9.44551389, 19.63742029]), 'previousTarget': array([ 9.47772  , 19.6118525]), 'currentState': array([24.95756997,  7.01299491,  2.7689728 ]), 'targetState': array([ 9, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.3550302523362482
running average episode reward sum: 0.48286178181418477
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 20.]), 'previousTarget': array([ 9., 20.]), 'currentState': array([ 9.95543729, 19.33462684,  2.43314144]), 'targetState': array([ 9, 20], dtype=int32), 'currentDistance': 1.164294580124402}
episode index:2731
target Thresh 31.99999999996471
target distance 3.0
model initialize at round 2731
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  6.]), 'previousTarget': array([17.,  6.]), 'currentState': array([17.05843969,  3.01128253,  0.42567399]), 'targetState': array([17,  6], dtype=int32), 'currentDistance': 2.9892887676421287}
done in step count: 5
reward sum = 0.8833118010908954
running average episode reward sum: 0.48300835942006937
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  6.]), 'previousTarget': array([17.,  6.]), 'currentState': array([17.63339169,  5.05555445,  1.81859839]), 'targetState': array([17,  6], dtype=int32), 'currentDistance': 1.1371729937313673}
episode index:2732
target Thresh 31.99999999996506
target distance 20.0
model initialize at round 2732
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 24.]), 'previousTarget': array([15., 24.]), 'currentState': array([27.        ,  8.        ,  1.22277495]), 'targetState': array([12, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 97
reward sum = 0.170685227497328
running average episode reward sum: 0.4828940809232078
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 28.]), 'previousTarget': array([12., 28.]), 'currentState': array([12.74537704, 27.43489775,  2.30791863]), 'targetState': array([12, 28], dtype=int32), 'currentDistance': 0.9353755871307798}
episode index:2733
target Thresh 31.999999999965407
target distance 11.0
model initialize at round 2733
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([ 3.07291357, 15.05070233,  0.35511571]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 13.599879457585738}
done in step count: 30
reward sum = 0.5196480530826375
running average episode reward sum: 0.4829075242195353
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([10.25290896,  4.64548734,  5.28861486]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 0.9873190625135899}
episode index:2734
target Thresh 31.999999999965752
target distance 16.0
model initialize at round 2734
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 11.]), 'previousTarget': array([23., 11.]), 'currentState': array([26.        , 27.        ,  2.42492044]), 'targetState': array([23, 11], dtype=int32), 'currentDistance': 16.278820596099706}
done in step count: 35
reward sum = 0.40712249718030197
running average episode reward sum: 0.4828798148860658
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 11.]), 'previousTarget': array([23., 11.]), 'currentState': array([23.00255335, 11.77139967,  4.64488467]), 'targetState': array([23, 11], dtype=int32), 'currentDistance': 0.7714038942816053}
episode index:2735
target Thresh 31.999999999966093
target distance 17.0
model initialize at round 2735
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.71337793,  7.16065869]), 'previousTarget': array([23.56399985,  7.29270602]), 'currentState': array([ 9.17651472, 20.89672828,  5.80821053]), 'targetState': array([26,  5], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 61
reward sum = 0.29427270566412733
running average episode reward sum: 0.48281087953912794
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  5.]), 'previousTarget': array([26.,  5.]), 'currentState': array([25.18532573,  5.8100805 ,  5.42963157]), 'targetState': array([26,  5], dtype=int32), 'currentDistance': 1.148879707997913}
episode index:2736
target Thresh 31.99999999996643
target distance 17.0
model initialize at round 2736
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.15850381,  4.84147901]), 'previousTarget': array([20.14213562,  4.85786438]), 'currentState': array([ 6.01632543, 18.98357188,  5.51736915]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.11058075121707933
running average episode reward sum: 0.4825940758742553
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  2.]), 'previousTarget': array([23.,  2.]), 'currentState': array([19.12033067,  7.7369122 ,  5.25306993]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 6.925604353839296}
episode index:2737
target Thresh 31.999999999966764
target distance 10.0
model initialize at round 2737
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 13.]), 'previousTarget': array([17., 13.]), 'currentState': array([6.98823708, 7.96916389, 4.58290792]), 'targetState': array([17, 13], dtype=int32), 'currentDistance': 11.204673521086768}
done in step count: 24
reward sum = 0.5717226227472386
running average episode reward sum: 0.4826266283018934
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 13.]), 'previousTarget': array([17., 13.]), 'currentState': array([16.16741906, 12.19894504,  0.57195793]), 'targetState': array([17, 13], dtype=int32), 'currentDistance': 1.1553700983483652}
episode index:2738
target Thresh 31.999999999967095
target distance 16.0
model initialize at round 2738
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 26.]), 'previousTarget': array([ 3., 26.]), 'currentState': array([18.61257594, 22.05872175,  3.03146062]), 'targetState': array([ 3, 26], dtype=int32), 'currentDistance': 16.102366334538665}
done in step count: 36
reward sum = 0.46443129277079653
running average episode reward sum: 0.48261998524401417
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 26.]), 'previousTarget': array([ 3., 26.]), 'currentState': array([ 3.73657553, 26.13433431,  2.83343993]), 'targetState': array([ 3, 26], dtype=int32), 'currentDistance': 0.748725059211986}
episode index:2739
target Thresh 31.99999999996742
target distance 8.0
model initialize at round 2739
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 16.]), 'previousTarget': array([17., 16.]), 'currentState': array([20.02579786,  8.01272903,  0.69915739]), 'targetState': array([17, 16], dtype=int32), 'currentDistance': 8.541191382541303}
done in step count: 17
reward sum = 0.6828147553185006
running average episode reward sum: 0.4826930490287129
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 16.]), 'previousTarget': array([17., 16.]), 'currentState': array([17.83435325, 15.32259868,  1.99608521]), 'targetState': array([17, 16], dtype=int32), 'currentDistance': 1.0747175901583235}
episode index:2740
target Thresh 31.99999999996775
target distance 20.0
model initialize at round 2740
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.84552722,  6.61172774]), 'previousTarget': array([23.8507125,  6.59715  ]), 'currentState': array([18.94885783, 26.00303066,  3.30066964]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.307533996122047
running average episode reward sum: 0.4826291456894547
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  6.]), 'previousTarget': array([24.,  6.]), 'currentState': array([23.37278128,  6.8302979 ,  5.00353605]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 1.0405757687144859}
episode index:2741
target Thresh 31.999999999968068
target distance 22.0
model initialize at round 2741
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.61446557, 15.60010123]), 'previousTarget': array([23.51093912, 15.57265691]), 'currentState': array([5.13217759, 7.95778281, 6.06699291]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.08630135818229377
running average episode reward sum: 0.4824216582700996
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 17.]), 'previousTarget': array([27., 17.]), 'currentState': array([19.93134285, 13.8313636 ,  0.45774722]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 7.746364984543461}
episode index:2742
target Thresh 31.999999999968384
target distance 13.0
model initialize at round 2742
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([19.67489644, 21.02395586,  3.06059356]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 13.283897270874126}
done in step count: 33
reward sum = 0.5524418369549898
running average episode reward sum: 0.48244718513072116
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([ 7.72598583, 24.22416787,  2.61730154]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 1.0625304319830318}
episode index:2743
target Thresh 31.9999999999687
target distance 10.0
model initialize at round 2743
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 15.]), 'previousTarget': array([25., 15.]), 'currentState': array([2.50295268e+01, 2.50078732e+01, 8.08423758e-03]), 'targetState': array([25, 15], dtype=int32), 'currentDistance': 10.00791680327007}
done in step count: 23
reward sum = 0.6202475080216274
running average episode reward sum: 0.48249740390728485
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 15.]), 'previousTarget': array([25., 15.]), 'currentState': array([24.83586064, 15.95858923,  4.68115077]), 'targetState': array([25, 15], dtype=int32), 'currentDistance': 0.9725405034529001}
episode index:2744
target Thresh 31.999999999969013
target distance 14.0
model initialize at round 2744
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  6.]), 'previousTarget': array([17.,  6.]), 'currentState': array([ 3.        , 11.        ,  1.93642616]), 'targetState': array([17,  6], dtype=int32), 'currentDistance': 14.866068747318506}
done in step count: 37
reward sum = 0.44487397178895083
running average episode reward sum: 0.4824836977389357
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  6.]), 'previousTarget': array([17.,  6.]), 'currentState': array([16.01399233,  5.78928964,  5.97718414]), 'targetState': array([17,  6], dtype=int32), 'currentDistance': 1.0082707864297702}
episode index:2745
target Thresh 31.99999999996932
target distance 24.0
model initialize at round 2745
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.68593728, 20.42327333]), 'previousTarget': array([23.69230769, 20.46153846]), 'currentState': array([16.0206584 ,  1.95049607,  5.34477043]), 'targetState': array([26, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.203544399430515
running average episode reward sum: 0.4822338695899301
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 26.]), 'previousTarget': array([26., 26.]), 'currentState': array([25.31118602, 24.36576461,  0.89082594]), 'targetState': array([26, 26], dtype=int32), 'currentDistance': 1.7734683528413497}
episode index:2746
target Thresh 31.999999999969624
target distance 16.0
model initialize at round 2746
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9.71724957, 10.70702157]), 'previousTarget': array([ 9.85786438, 10.85786438]), 'currentState': array([23.90156324, 24.80685297,  4.19907209]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.29730170168196357
running average episode reward sum: 0.48216654808723336
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([8.58964005, 9.94854736, 4.07439653]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 1.1168784544670451}
episode index:2747
target Thresh 31.999999999969926
target distance 6.0
model initialize at round 2747
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 2.]), 'previousTarget': array([7., 2.]), 'currentState': array([12.56182134,  4.84766535,  3.57057364]), 'targetState': array([7, 2], dtype=int32), 'currentDistance': 6.248444169416064}
done in step count: 11
reward sum = 0.7896373508733867
running average episode reward sum: 0.48227843702565626
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 2.]), 'previousTarget': array([7., 2.]), 'currentState': array([7.73036206, 2.73035586, 3.56598141]), 'targetState': array([7, 2], dtype=int32), 'currentDistance': 1.0328835478384}
episode index:2748
target Thresh 31.99999999997023
target distance 11.0
model initialize at round 2748
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 16.]), 'previousTarget': array([20., 16.]), 'currentState': array([8.93615493, 8.04766323, 2.24780631]), 'targetState': array([20, 16], dtype=int32), 'currentDistance': 13.625282672221607}
done in step count: 31
reward sum = 0.5150128812039136
running average episode reward sum: 0.482290344789999
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 16.]), 'previousTarget': array([20., 16.]), 'currentState': array([19.25637459, 15.06765024,  0.58120275]), 'targetState': array([20, 16], dtype=int32), 'currentDistance': 1.1925832586994796}
episode index:2749
target Thresh 31.999999999970523
target distance 14.0
model initialize at round 2749
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  5.]), 'previousTarget': array([18.,  5.]), 'currentState': array([ 7.15663482, 18.9334263 ,  5.77516013]), 'targetState': array([18,  5], dtype=int32), 'currentDistance': 17.655563908879735}
done in step count: 52
reward sum = 0.4139429474440405
running average episode reward sum: 0.48226549119096407
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  5.]), 'previousTarget': array([18.,  5.]), 'currentState': array([17.07750678,  5.71906815,  5.27280374]), 'targetState': array([18,  5], dtype=int32), 'currentDistance': 1.169637871483655}
episode index:2750
target Thresh 31.999999999970814
target distance 18.0
model initialize at round 2750
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.31616567, 25.84072884]), 'previousTarget': array([12.45973695, 25.73247066]), 'currentState': array([26.79670382, 12.04529427,  2.75870544]), 'targetState': array([ 9, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.26082410301223935
running average episode reward sum: 0.48218499632067013
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 29.]), 'previousTarget': array([ 9., 29.]), 'currentState': array([ 9.77134003, 28.14705429,  2.37936546]), 'targetState': array([ 9, 29], dtype=int32), 'currentDistance': 1.1499921033457443}
episode index:2751
target Thresh 31.999999999971106
target distance 15.0
model initialize at round 2751
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 10.]), 'previousTarget': array([22., 10.]), 'currentState': array([ 7.        , 21.        ,  3.57167935]), 'targetState': array([22, 10], dtype=int32), 'currentDistance': 18.601075237738275}
done in step count: 46
reward sum = 0.3563299668862784
running average episode reward sum: 0.482139264115207
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 10.]), 'previousTarget': array([22., 10.]), 'currentState': array([21.08955103, 10.38890405,  5.78148461]), 'targetState': array([22, 10], dtype=int32), 'currentDistance': 0.9900321681226673}
episode index:2752
target Thresh 31.999999999971394
target distance 11.0
model initialize at round 2752
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'previousTarget': array([22., 18.]), 'currentState': array([21.22354024, 28.80599352,  5.41108382]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 10.833853687223906}
done in step count: 22
reward sum = 0.6207413698638807
running average episode reward sum: 0.48218960995819604
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'previousTarget': array([22., 18.]), 'currentState': array([21.4847011 , 18.84483199,  4.74403656]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 0.9895827637900352}
episode index:2753
target Thresh 31.999999999971678
target distance 8.0
model initialize at round 2753
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([14.99003743, 12.0121677 ,  2.48501512]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 8.950711145222481}
done in step count: 19
reward sum = 0.6463275715933883
running average episode reward sum: 0.4822492097990222
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([10.88446062,  4.92749497,  4.45375536]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 0.9346637157369558}
episode index:2754
target Thresh 31.999999999971962
target distance 16.0
model initialize at round 2754
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 27.]), 'previousTarget': array([20., 27.]), 'currentState': array([20.87654342, 11.15025335,  2.08563831]), 'targetState': array([20, 27], dtype=int32), 'currentDistance': 15.873966022961474}
done in step count: 37
reward sum = 0.46795225072938795
running average episode reward sum: 0.48224402034019476
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 27.]), 'previousTarget': array([20., 27.]), 'currentState': array([20.16612777, 26.21440079,  1.77458162]), 'targetState': array([20, 27], dtype=int32), 'currentDistance': 0.8029723282601423}
episode index:2755
target Thresh 31.99999999997224
target distance 9.0
model initialize at round 2755
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 12.]), 'previousTarget': array([27., 12.]), 'currentState': array([24.       ,  3.       ,  1.9649308]), 'targetState': array([27, 12], dtype=int32), 'currentDistance': 9.486832980505138}
done in step count: 21
reward sum = 0.6509200657845371
running average episode reward sum: 0.4823052235497174
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 12.]), 'previousTarget': array([27., 12.]), 'currentState': array([27.07948736, 11.27665387,  1.29963382]), 'targetState': array([27, 12], dtype=int32), 'currentDistance': 0.727700397486754}
episode index:2756
target Thresh 31.999999999972516
target distance 9.0
model initialize at round 2756
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([19.00038324, 12.9985629 ,  4.72838935]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 10.81617563225161}
done in step count: 24
reward sum = 0.6082176340496194
running average episode reward sum: 0.4823508936296956
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([10.68944836,  7.46452803,  3.72507973]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 0.8313394789917055}
episode index:2757
target Thresh 31.99999999997279
target distance 12.0
model initialize at round 2757
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 16.]), 'previousTarget': array([19., 16.]), 'currentState': array([7.35161287, 4.11303492, 0.44085717]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 16.64286218268583}
done in step count: 36
reward sum = 0.45914366932172823
running average episode reward sum: 0.48234247911761874
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 16.]), 'previousTarget': array([19., 16.]), 'currentState': array([18.56304551, 15.0007303 ,  0.84728457]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 1.0906278776960785}
episode index:2758
target Thresh 31.99999999997306
target distance 11.0
model initialize at round 2758
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([ 3.12574445, 16.04902729,  0.16814131]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 12.959097946638492}
done in step count: 32
reward sum = 0.5512436283046729
running average episode reward sum: 0.4823674523503796
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.22835637,  9.41703556,  5.74224789]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.8771274460784748}
episode index:2759
target Thresh 31.99999999997333
target distance 6.0
model initialize at round 2759
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([22.0118285 ,  5.98566311,  5.19244844]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 6.096799266061022}
done in step count: 16
reward sum = 0.7178359785942856
running average episode reward sum: 0.4824527670338013
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.61253826,  7.06609334,  2.8613019 ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.6160937065634994}
episode index:2760
target Thresh 31.999999999973593
target distance 12.0
model initialize at round 2760
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 20.]), 'previousTarget': array([16., 20.]), 'currentState': array([19.14320961,  8.11923299,  0.94679162]), 'targetState': array([16, 20], dtype=int32), 'currentDistance': 12.28952363964369}
done in step count: 25
reward sum = 0.5715394000252263
running average episode reward sum: 0.482485033108771
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 20.]), 'previousTarget': array([16., 20.]), 'currentState': array([16.62268266, 19.05398821,  1.97063272]), 'targetState': array([16, 20], dtype=int32), 'currentDistance': 1.132551107789916}
episode index:2761
target Thresh 31.999999999973856
target distance 17.0
model initialize at round 2761
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7.2694198, 3.5416142]), 'previousTarget': array([7.28585494, 3.56139529]), 'currentState': array([19.98270049, 18.9809307 ,  3.97529244]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.2999244311257058
running average episode reward sum: 0.48241893585968226
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 2.]), 'previousTarget': array([6., 2.]), 'currentState': array([6.39483477, 2.9461214 , 4.03327361]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 1.025202511667928}
episode index:2762
target Thresh 31.999999999974115
target distance 18.0
model initialize at round 2762
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([ 3.        , 25.        ,  3.49142605]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 19.313207915827967}
done in step count: 51
reward sum = 0.3456209245555546
running average episode reward sum: 0.48236942517879045
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([9.4991622 , 7.80554499, 5.1739674 ]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 0.9485469054706479}
episode index:2763
target Thresh 31.999999999974374
target distance 20.0
model initialize at round 2763
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.05553019, 21.88891851]), 'previousTarget': array([14.05572809, 21.88854382]), 'currentState': array([22.99973057,  4.00033892,  2.33986299]), 'targetState': array([13, 24], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.2989171691202781
running average episode reward sum: 0.48230305316140315
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 24.]), 'previousTarget': array([13., 24.]), 'currentState': array([13.37490069, 23.20860165,  1.88280167]), 'targetState': array([13, 24], dtype=int32), 'currentDistance': 0.875706495669802}
episode index:2764
target Thresh 31.99999999997463
target distance 22.0
model initialize at round 2764
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8.5334134 , 19.64608148]), 'previousTarget': array([ 8.52085402, 19.66475581]), 'currentState': array([24.002809  ,  6.96941789,  4.55148315]), 'targetState': array([ 2, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.11536120224119284
running average episode reward sum: 0.4821703436312331
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 25.]), 'previousTarget': array([ 2., 25.]), 'currentState': array([ 2.68620404, 24.26743933,  2.29639336]), 'targetState': array([ 2, 25], dtype=int32), 'currentDistance': 1.003753515700439}
episode index:2765
target Thresh 31.999999999974882
target distance 8.0
model initialize at round 2765
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([ 2.31477833, 10.70543351,  5.73229185]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 7.872174767343839}
done in step count: 16
reward sum = 0.7170428098535475
running average episode reward sum: 0.4822552577549578
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([9.25841057, 9.28738432, 6.06644081]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.7953267425808591}
episode index:2766
target Thresh 31.99999999997513
target distance 7.0
model initialize at round 2766
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'previousTarget': array([25.,  3.]), 'currentState': array([18.        ,  9.        ,  0.32882524]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 21
reward sum = 0.6651408435482823
running average episode reward sum: 0.4823213530154541
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'previousTarget': array([25.,  3.]), 'currentState': array([24.2950622 ,  3.68678595,  5.58039083]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 0.9841809989065985}
episode index:2767
target Thresh 31.99999999997538
target distance 12.0
model initialize at round 2767
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 27.]), 'previousTarget': array([ 4., 27.]), 'currentState': array([ 2.        , 15.        ,  2.91345879]), 'targetState': array([ 4, 27], dtype=int32), 'currentDistance': 12.165525060596439}
done in step count: 26
reward sum = 0.554731173588068
running average episode reward sum: 0.482347512632713
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 27.]), 'previousTarget': array([ 4., 27.]), 'currentState': array([ 3.86150316, 26.16856522,  1.33759745]), 'targetState': array([ 4, 27], dtype=int32), 'currentDistance': 0.8428909573931737}
episode index:2768
target Thresh 31.999999999975625
target distance 21.0
model initialize at round 2768
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8.37672111, 8.85777134]), 'previousTarget': array([8.40132839, 8.94278962]), 'currentState': array([17.0569086 , 26.87593842,  5.0072808 ]), 'targetState': array([7, 6], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.2914339864674498
running average episode reward sum: 0.48227856589159157
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 6.]), 'previousTarget': array([7., 6.]), 'currentState': array([7.42824299, 6.94285846, 4.21017929]), 'targetState': array([7, 6], dtype=int32), 'currentDistance': 1.0355549920867528}
episode index:2769
target Thresh 31.999999999975866
target distance 5.0
model initialize at round 2769
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([10.        , 14.        ,  3.54081416]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 5.830951894845301}
done in step count: 14
reward sum = 0.7312474935201126
running average episode reward sum: 0.48236844637087983
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.19209093, 10.90944712,  5.79334796]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.8129679478915788}
episode index:2770
target Thresh 31.999999999976108
target distance 10.0
model initialize at round 2770
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 17.]), 'previousTarget': array([11., 17.]), 'currentState': array([2.98900463, 7.00098078, 2.80012918]), 'targetState': array([11, 17], dtype=int32), 'currentDistance': 12.812354673153175}
done in step count: 32
reward sum = 0.5198403974908539
running average episode reward sum: 0.4823819692691549
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 17.]), 'previousTarget': array([11., 17.]), 'currentState': array([10.30116157, 16.36839115,  0.93166566]), 'targetState': array([11, 17], dtype=int32), 'currentDistance': 0.9419686237411959}
episode index:2771
target Thresh 31.999999999976342
target distance 15.0
model initialize at round 2771
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 3.]), 'previousTarget': array([6., 3.]), 'currentState': array([ 8.86958446, 17.96654199,  3.64522576]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 15.239156607063602}
done in step count: 34
reward sum = 0.46979845434885265
running average episode reward sum: 0.48237742976160786
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 3.]), 'previousTarget': array([6., 3.]), 'currentState': array([6.25036493, 3.68429591, 4.4605142 ]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 0.7286586908515436}
episode index:2772
target Thresh 31.99999999997658
target distance 16.0
model initialize at round 2772
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([ 7.08356833, 26.88724939,  5.2242734 ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 16.362858254868197}
done in step count: 37
reward sum = 0.46664880511005324
running average episode reward sum: 0.48237175770078866
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.43982246, 11.805071  ,  4.94589906]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.9807844827613894}
episode index:2773
target Thresh 31.99999999997681
target distance 9.0
model initialize at round 2773
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 20.]), 'previousTarget': array([ 3., 20.]), 'currentState': array([ 6.04144814, 11.4794685 ,  1.61000897]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 9.047091456583408}
done in step count: 19
reward sum = 0.6842389595613987
running average episode reward sum: 0.48244452886223804
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 20.]), 'previousTarget': array([ 3., 20.]), 'currentState': array([ 3.5195442 , 19.18602606,  2.01270998]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 0.9656499145699715}
episode index:2774
target Thresh 31.999999999977042
target distance 15.0
model initialize at round 2774
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 7.]), 'previousTarget': array([5., 7.]), 'currentState': array([ 2.        , 22.        ,  3.27219391]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 15.297058540778357}
done in step count: 36
reward sum = 0.45494693027361377
running average episode reward sum: 0.4824346198177016
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 7.]), 'previousTarget': array([5., 7.]), 'currentState': array([4.05986514, 7.78050738, 4.89025411]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 1.221902335592366}
episode index:2775
target Thresh 31.999999999977273
target distance 17.0
model initialize at round 2775
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([11.96919589,  6.35490514,  1.67059238]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 16.907847497084447}
done in step count: 37
reward sum = 0.4505506932475636
running average episode reward sum: 0.48242313425337513
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([ 9.45278504, 22.19411524,  1.67429669]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 0.9243725129528735}
episode index:2776
target Thresh 31.999999999977497
target distance 12.0
model initialize at round 2776
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([13.02683895,  9.99018572,  6.16266862]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 15.023962872021773}
done in step count: 38
reward sum = 0.45597989912628484
running average episode reward sum: 0.48241361202250477
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([ 4.98348082, 21.03486218,  2.30043266]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 1.3779424980413115}
episode index:2777
target Thresh 31.99999999997772
target distance 13.0
model initialize at round 2777
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  4.]), 'previousTarget': array([10.,  4.]), 'currentState': array([20.05697632, 17.00463754,  6.13802141]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 16.43968887348284}
done in step count: 46
reward sum = 0.4171365828708806
running average episode reward sum: 0.48239011417183825
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  4.]), 'previousTarget': array([10.,  4.]), 'currentState': array([10.88253504,  4.7266362 ,  4.1710179 ]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 1.1431833864418053}
episode index:2778
target Thresh 31.999999999977945
target distance 11.0
model initialize at round 2778
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([ 7.        , 25.        ,  4.07417202]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 13.038404810405298}
done in step count: 27
reward sum = 0.5362785950116217
running average episode reward sum: 0.48240950549275935
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([13.55116889, 14.83189487,  5.34658226]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 0.9452504620489854}
episode index:2779
target Thresh 31.99999999997816
target distance 2.0
model initialize at round 2779
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 23.]), 'previousTarget': array([24., 23.]), 'currentState': array([24.90435304, 21.00997124,  2.83632019]), 'targetState': array([24, 23], dtype=int32), 'currentDistance': 2.1858794295723762}
done in step count: 5
reward sum = 0.9153849644317009
running average episode reward sum: 0.48256525206072304
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 23.]), 'previousTarget': array([24., 23.]), 'currentState': array([24.07115618, 22.20536604,  1.77905574]), 'targetState': array([24, 23], dtype=int32), 'currentDistance': 0.7978134678186344}
episode index:2780
target Thresh 31.99999999997838
target distance 8.0
model initialize at round 2780
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([16.        , 13.        ,  0.56095706]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 8.246211251235321}
done in step count: 17
reward sum = 0.6941897853483953
running average episode reward sum: 0.4826413486206971
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([14.65577003, 20.15615572,  1.83055362]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 1.0686943000477624}
episode index:2781
target Thresh 31.999999999978595
target distance 12.0
model initialize at round 2781
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 27.]), 'previousTarget': array([15., 27.]), 'currentState': array([15.09166123, 15.038527  ,  0.62595086]), 'targetState': array([15, 27], dtype=int32), 'currentDistance': 11.961824194544823}
done in step count: 25
reward sum = 0.5775752837114667
running average episode reward sum: 0.4826754729683213
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 27.]), 'previousTarget': array([15., 27.]), 'currentState': array([15.2131596 , 26.11767555,  1.6881408 ]), 'targetState': array([15, 27], dtype=int32), 'currentDistance': 0.9077077981808034}
episode index:2782
target Thresh 31.999999999978808
target distance 8.0
model initialize at round 2782
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 27.]), 'previousTarget': array([16., 27.]), 'currentState': array([ 8.17522327, 19.91946936,  6.10488605]), 'targetState': array([16, 27], dtype=int32), 'currentDistance': 10.55277428081535}
done in step count: 21
reward sum = 0.6256345687472691
running average episode reward sum: 0.48272684166964325
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 27.]), 'previousTarget': array([16., 27.]), 'currentState': array([15.36420996, 26.04337008,  0.82136462]), 'targetState': array([16, 27], dtype=int32), 'currentDistance': 1.1486382249977367}
episode index:2783
target Thresh 31.999999999979018
target distance 15.0
model initialize at round 2783
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  3.]), 'previousTarget': array([22.,  3.]), 'currentState': array([21.        , 18.        ,  1.06122804]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 15.033296378372908}
done in step count: 41
reward sum = 0.4360300459171765
running average episode reward sum: 0.48271006839530695
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  3.]), 'previousTarget': array([22.,  3.]), 'currentState': array([21.90915739,  3.76803255,  4.78204639]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 0.7733863052891666}
episode index:2784
target Thresh 31.999999999979227
target distance 19.0
model initialize at round 2784
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9.24210575, 23.51705065]), 'previousTarget': array([ 9.24510704, 23.51905368]), 'currentState': array([22.98863024,  8.99007353,  3.61517256]), 'targetState': array([ 5, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 77
reward sum = 0.20277403627518742
running average episode reward sum: 0.4826095527643841
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 28.]), 'previousTarget': array([ 5., 28.]), 'currentState': array([ 5.68645837, 27.31056483,  2.12257424]), 'targetState': array([ 5, 28], dtype=int32), 'currentDistance': 0.972905929165271}
episode index:2785
target Thresh 31.999999999979433
target distance 14.0
model initialize at round 2785
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([2.00000000e+01, 2.50000000e+01, 1.53179169e-02]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 17.204650534085253}
done in step count: 41
reward sum = 0.39540486322830154
running average episode reward sum: 0.48257825172722113
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.59587441, 11.88992879,  3.88139262]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.0709993327050862}
episode index:2786
target Thresh 31.99999999997964
target distance 22.0
model initialize at round 2786
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.93814916, 22.04879293]), 'previousTarget': array([13.93592685, 22.0585156 ]), 'currentState': array([19.99711233,  2.98865219,  4.21070862]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 78
reward sum = 0.229340815685419
running average episode reward sum: 0.4824873879180924
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([13.37924105, 24.22115837,  1.75860955]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 0.8662667366153347}
episode index:2787
target Thresh 31.999999999979842
target distance 27.0
model initialize at round 2787
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.80395219, 19.20583067]), 'previousTarget': array([14.80395219, 19.20583067]), 'currentState': array([25.        ,  2.        ,  6.11272332]), 'targetState': array([ 9, 29], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 99
reward sum = -0.2932760170867986
running average episode reward sum: 0.48220913705546514
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 29.]), 'previousTarget': array([ 9., 29.]), 'currentState': array([ 9.68714127, 27.98425339,  1.88382753]), 'targetState': array([ 9, 29], dtype=int32), 'currentDistance': 1.2263377634229504}
episode index:2788
target Thresh 31.99999999998004
target distance 5.0
model initialize at round 2788
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 14.]), 'previousTarget': array([ 6., 14.]), 'currentState': array([10.51427744, 14.114976  ,  3.01148613]), 'targetState': array([ 6, 14], dtype=int32), 'currentDistance': 4.515741386750022}
done in step count: 8
reward sum = 0.845078495364333
running average episode reward sum: 0.4823392443908215
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 14.]), 'previousTarget': array([ 6., 14.]), 'currentState': array([ 6.84393524, 14.26546487,  3.16970043]), 'targetState': array([ 6, 14], dtype=int32), 'currentDistance': 0.8847023700693697}
episode index:2789
target Thresh 31.99999999998024
target distance 19.0
model initialize at round 2789
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 28.]), 'previousTarget': array([ 7., 28.]), 'currentState': array([26.        , 22.        ,  3.84491855]), 'targetState': array([ 7, 28], dtype=int32), 'currentDistance': 19.924858845171272}
done in step count: 48
reward sum = 0.35075840273067294
running average episode reward sum: 0.48229208279882857
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 28.]), 'previousTarget': array([ 7., 28.]), 'currentState': array([ 7.83998376, 27.74769039,  2.80144259]), 'targetState': array([ 7, 28], dtype=int32), 'currentDistance': 0.8770592047703155}
episode index:2790
target Thresh 31.99999999998044
target distance 20.0
model initialize at round 2790
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9.79270645, 6.7615699 ]), 'previousTarget': array([9.79270645, 6.7615699 ]), 'currentState': array([18.        , 25.        ,  1.84737554]), 'targetState': array([9, 5], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.26382426174310947
running average episode reward sum: 0.4822138069761644
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 5.]), 'previousTarget': array([9., 5.]), 'currentState': array([9.29003739, 5.91644798, 4.43406113]), 'targetState': array([9, 5], dtype=int32), 'currentDistance': 0.9612484507020647}
episode index:2791
target Thresh 31.99999999998063
target distance 13.0
model initialize at round 2791
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 25.]), 'previousTarget': array([23., 25.]), 'currentState': array([10.        , 24.        ,  4.40255547]), 'targetState': array([23, 25], dtype=int32), 'currentDistance': 13.038404810405298}
done in step count: 29
reward sum = 0.5138659933015566
running average episode reward sum: 0.4822251437191176
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 25.]), 'previousTarget': array([23., 25.]), 'currentState': array([22.04441176, 24.78805522,  0.13439263]), 'targetState': array([23, 25], dtype=int32), 'currentDistance': 0.9788102333565385}
episode index:2792
target Thresh 31.999999999980826
target distance 13.0
model initialize at round 2792
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 16.]), 'previousTarget': array([18., 16.]), 'currentState': array([21.86415204,  3.03728046,  2.6538001 ]), 'targetState': array([18, 16], dtype=int32), 'currentDistance': 13.526410045763658}
done in step count: 31
reward sum = 0.537126197100839
running average episode reward sum: 0.48224480037983436
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 16.]), 'previousTarget': array([18., 16.]), 'currentState': array([18.20201899, 15.13343831,  1.78057234]), 'targetState': array([18, 16], dtype=int32), 'currentDistance': 0.8897982016690532}
episode index:2793
target Thresh 31.999999999981014
target distance 18.0
model initialize at round 2793
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 13.]), 'previousTarget': array([26., 13.]), 'currentState': array([ 8.0148466 , 21.06405909,  1.09055305]), 'targetState': array([26, 13], dtype=int32), 'currentDistance': 19.710271224125016}
done in step count: 50
reward sum = 0.3380371090019747
running average episode reward sum: 0.4821931870328845
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 13.]), 'previousTarget': array([26., 13.]), 'currentState': array([25.1213295 , 13.14255614,  5.81275614]), 'targetState': array([26, 13], dtype=int32), 'currentDistance': 0.8901595895101404}
episode index:2794
target Thresh 31.999999999981206
target distance 20.0
model initialize at round 2794
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.25304229,  6.8434743 ]), 'previousTarget': array([16.25304229,  6.8434743 ]), 'currentState': array([22.        , 26.        ,  5.64810795]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.3234251905515773
running average episode reward sum: 0.4821363827407624
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.95670347,  6.952916  ,  4.41395832]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.9538991002131919}
episode index:2795
target Thresh 31.99999999998139
target distance 11.0
model initialize at round 2795
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'previousTarget': array([11., 19.]), 'currentState': array([21.77897992,  7.99411295,  3.01614645]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 15.405062735675987}
done in step count: 35
reward sum = 0.4934166795798303
running average episode reward sum: 0.48214041718169204
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'previousTarget': array([11., 19.]), 'currentState': array([11.89938663, 18.2502049 ,  2.43165443]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 1.1709350941173462}
episode index:2796
target Thresh 31.999999999981576
target distance 10.0
model initialize at round 2796
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 29.]), 'previousTarget': array([27., 29.]), 'currentState': array([23.25259388, 19.28346707,  0.9673011 ]), 'targetState': array([27, 29], dtype=int32), 'currentDistance': 10.414128136464736}
done in step count: 21
reward sum = 0.6437958289707255
running average episode reward sum: 0.4821982131816166
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 29.]), 'previousTarget': array([27., 29.]), 'currentState': array([26.78966442, 28.13114492,  1.31948122]), 'targetState': array([27, 29], dtype=int32), 'currentDistance': 0.893952014880751}
episode index:2797
target Thresh 31.99999999998176
target distance 12.0
model initialize at round 2797
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  9.]), 'previousTarget': array([21.,  9.]), 'currentState': array([ 9.        , 17.        ,  4.06427222]), 'targetState': array([21,  9], dtype=int32), 'currentDistance': 14.422205101855956}
done in step count: 33
reward sum = 0.4805980219814621
running average episode reward sum: 0.4821976412762556
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  9.]), 'previousTarget': array([21.,  9.]), 'currentState': array([20.18734489,  9.24305786,  5.75505878]), 'targetState': array([21,  9], dtype=int32), 'currentDistance': 0.8482248803308058}
episode index:2798
target Thresh 31.99999999998194
target distance 8.0
model initialize at round 2798
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([ 6.02742141, 12.00141623,  6.08228648]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 8.254277637803803}
done in step count: 20
reward sum = 0.6727663096447827
running average episode reward sum: 0.482265725830871
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.12026706, 4.85277756, 4.2819053 ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.861216429163554}
episode index:2799
target Thresh 31.999999999982123
target distance 11.0
model initialize at round 2799
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 15.]), 'previousTarget': array([ 5., 15.]), 'currentState': array([15.90268405, 16.94360619,  3.52416137]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 11.074571076089883}
done in step count: 23
reward sum = 0.6279414658764576
running average episode reward sum: 0.4823177528808873
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 15.]), 'previousTarget': array([ 5., 15.]), 'currentState': array([ 5.93476694, 15.14433209,  3.40561134]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 0.9458440590388448}
episode index:2800
target Thresh 31.9999999999823
target distance 6.0
model initialize at round 2800
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 28.]), 'previousTarget': array([13., 28.]), 'currentState': array([ 7.47785318, 22.10741964,  0.40064779]), 'targetState': array([13, 28], dtype=int32), 'currentDistance': 8.075680079965217}
done in step count: 14
reward sum = 0.7251786243406677
running average episode reward sum: 0.482404457940316
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 28.]), 'previousTarget': array([13., 28.]), 'currentState': array([12.01585015, 27.19816108,  0.81083591]), 'targetState': array([13, 28], dtype=int32), 'currentDistance': 1.2694473518854132}
episode index:2801
target Thresh 31.999999999982474
target distance 18.0
model initialize at round 2801
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3.11145618, 15.05572809]), 'previousTarget': array([ 3.11145618, 15.05572809]), 'currentState': array([21.        , 24.        ,  4.52883959]), 'targetState': array([ 3, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.34219766022592357
running average episode reward sum: 0.48235441982550004
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 15.]), 'previousTarget': array([ 3., 15.]), 'currentState': array([ 3.68809425, 15.40616034,  3.5341935 ]), 'targetState': array([ 3, 15], dtype=int32), 'currentDistance': 0.7990243524244008}
episode index:2802
target Thresh 31.99999999998265
target distance 8.0
model initialize at round 2802
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 25.]), 'previousTarget': array([15., 25.]), 'currentState': array([ 7.00003176, 17.9991457 ,  5.00204992]), 'targetState': array([15, 25], dtype=int32), 'currentDistance': 10.63068448902208}
done in step count: 25
reward sum = 0.5885336958930814
running average episode reward sum: 0.4823923004091845
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 25.]), 'previousTarget': array([15., 25.]), 'currentState': array([14.32368355, 24.16634226,  0.82672312]), 'targetState': array([15, 25], dtype=int32), 'currentDistance': 1.0734939087497801}
episode index:2803
target Thresh 31.999999999982823
target distance 13.0
model initialize at round 2803
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 14.]), 'previousTarget': array([13., 14.]), 'currentState': array([24.02561166, 26.85862003,  4.73791173]), 'targetState': array([13, 14], dtype=int32), 'currentDistance': 16.93836537072811}
done in step count: 43
reward sum = 0.438271966399778
running average episode reward sum: 0.482376565625301
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 14.]), 'previousTarget': array([13., 14.]), 'currentState': array([13.30276315, 14.74623707,  3.9460984 ]), 'targetState': array([13, 14], dtype=int32), 'currentDistance': 0.8053168908266153}
episode index:2804
target Thresh 31.999999999982993
target distance 4.0
model initialize at round 2804
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 14.]), 'previousTarget': array([15., 14.]), 'currentState': array([18.94715672,  9.98368349,  3.18857741]), 'targetState': array([15, 14], dtype=int32), 'currentDistance': 5.631238277504347}
done in step count: 11
reward sum = 0.7906511004030407
running average episode reward sum: 0.48248646742023066
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 14.]), 'previousTarget': array([15., 14.]), 'currentState': array([15.52074507, 13.36494975,  2.31346226]), 'targetState': array([15, 14], dtype=int32), 'currentDistance': 0.8212577179811228}
episode index:2805
target Thresh 31.999999999983164
target distance 19.0
model initialize at round 2805
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 24.]), 'previousTarget': array([ 7., 24.]), 'currentState': array([7.09348505, 5.03942081, 0.6487055 ]), 'targetState': array([ 7, 24], dtype=int32), 'currentDistance': 18.960809654955025}
done in step count: 48
reward sum = 0.3754236833087291
running average episode reward sum: 0.4824483124722223
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 24.]), 'previousTarget': array([ 7., 24.]), 'currentState': array([ 6.72344794, 23.03562181,  1.75838202]), 'targetState': array([ 7, 24], dtype=int32), 'currentDistance': 1.0032478981136783}
episode index:2806
target Thresh 31.99999999998333
target distance 2.0
model initialize at round 2806
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 23.]), 'previousTarget': array([ 5., 23.]), 'currentState': array([ 4.        , 25.        ,  1.20322213]), 'targetState': array([ 5, 23], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 8
reward sum = 0.8594925880367049
running average episode reward sum: 0.4825826353349101
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 23.]), 'previousTarget': array([ 5., 23.]), 'currentState': array([ 5.13306525, 23.87047793,  5.11872539]), 'targetState': array([ 5, 23], dtype=int32), 'currentDistance': 0.8805896859448346}
episode index:2807
target Thresh 31.999999999983494
target distance 13.0
model initialize at round 2807
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.00719557, 24.02704969,  1.06415245]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 13.064826219450138}
done in step count: 33
reward sum = 0.49301268964725675
running average episode reward sum: 0.48258634974171644
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.97216299, 11.78852857,  4.83888552]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.7890197717083207}
episode index:2808
target Thresh 31.99999999998366
target distance 16.0
model initialize at round 2808
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 20.]), 'previousTarget': array([14., 20.]), 'currentState': array([13.        ,  4.        ,  2.66199231]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 16.0312195418814}
done in step count: 40
reward sum = 0.4500858387468209
running average episode reward sum: 0.48257477960608286
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 20.]), 'previousTarget': array([14., 20.]), 'currentState': array([13.5559021 , 19.26569354,  1.64189857]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 0.8581543694921165}
episode index:2809
target Thresh 31.99999999998382
target distance 7.0
model initialize at round 2809
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'previousTarget': array([26., 11.]), 'currentState': array([22.94181694, 17.87599423,  4.52618647]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 7.525408982757757}
done in step count: 17
reward sum = 0.7256448373003948
running average episode reward sum: 0.4826612814059741
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'previousTarget': array([26., 11.]), 'currentState': array([25.54495303, 11.74526166,  5.09713345]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 0.873202543168065}
episode index:2810
target Thresh 31.999999999983984
target distance 15.0
model initialize at round 2810
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  6.]), 'previousTarget': array([10.,  6.]), 'currentState': array([ 8.04126445, 20.69610529,  4.82636479]), 'targetState': array([10,  6], dtype=int32), 'currentDistance': 14.826063386431475}
done in step count: 32
reward sum = 0.5155014139551545
running average episode reward sum: 0.4826729641283324
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  6.]), 'previousTarget': array([10.,  6.]), 'currentState': array([10.13389393,  6.99529516,  4.98688654]), 'targetState': array([10,  6], dtype=int32), 'currentDistance': 1.0042609443920405}
episode index:2811
target Thresh 31.999999999984144
target distance 3.0
model initialize at round 2811
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  5.]), 'previousTarget': array([18.,  5.]), 'currentState': array([20.67581421,  3.28057089,  2.5693025 ]), 'targetState': array([18,  5], dtype=int32), 'currentDistance': 3.180631722741121}
done in step count: 5
reward sum = 0.8998798968681534
running average episode reward sum: 0.4828213307473722
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  5.]), 'previousTarget': array([18.,  5.]), 'currentState': array([18.61856184,  4.17329876,  2.68998859]), 'targetState': array([18,  5], dtype=int32), 'currentDistance': 1.0324987566976565}
episode index:2812
target Thresh 31.9999999999843
target distance 12.0
model initialize at round 2812
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  9.]), 'previousTarget': array([18.,  9.]), 'currentState': array([15.93875258, 20.96697906,  3.88855982]), 'targetState': array([18,  9], dtype=int32), 'currentDistance': 12.143200932397292}
done in step count: 31
reward sum = 0.558609960346397
running average episode reward sum: 0.48284827302593564
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  9.]), 'previousTarget': array([18.,  9.]), 'currentState': array([17.80960325,  9.77882142,  4.95120163]), 'targetState': array([18,  9], dtype=int32), 'currentDistance': 0.8017566521204175}
episode index:2813
target Thresh 31.999999999984457
target distance 17.0
model initialize at round 2813
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.85786438,  7.85786438]), 'previousTarget': array([11.85786438,  7.85786438]), 'currentState': array([26.        , 22.        ,  1.50014827]), 'targetState': array([9, 5], dtype=int32), 'currentDistance': 20.0}
done in step count: 76
reward sum = 0.21261170863544154
running average episode reward sum: 0.48275224013169593
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 5.]), 'previousTarget': array([9., 5.]), 'currentState': array([9.53735695, 5.9684875 , 4.08394752]), 'targetState': array([9, 5], dtype=int32), 'currentDistance': 1.1075741675150077}
episode index:2814
target Thresh 31.999999999984613
target distance 22.0
model initialize at round 2814
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.43629355, 14.0543247 ]), 'previousTarget': array([12.54654412, 14.20119853]), 'currentState': array([25.91641259, 28.82884298,  4.2655621 ]), 'targetState': array([6, 7], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 99
reward sum = -0.18375430877455987
running average episode reward sum: 0.4825154704873243
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 7.]), 'previousTarget': array([6., 7.]), 'currentState': array([8.40823469, 8.88493612, 3.84339924]), 'targetState': array([6, 7], dtype=int32), 'currentDistance': 3.058198573854451}
episode index:2815
target Thresh 31.999999999984766
target distance 18.0
model initialize at round 2815
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 26.]), 'previousTarget': array([13., 26.]), 'currentState': array([8.        , 8.        , 6.03910637]), 'targetState': array([13, 26], dtype=int32), 'currentDistance': 18.68154169226941}
done in step count: 50
reward sum = 0.3519682804587308
running average episode reward sum: 0.4824691113999562
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 26.]), 'previousTarget': array([13., 26.]), 'currentState': array([12.85731674, 25.36050364,  1.30712854]), 'targetState': array([13, 26], dtype=int32), 'currentDistance': 0.6552206511351366}
episode index:2816
target Thresh 31.999999999984915
target distance 17.0
model initialize at round 2816
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.44205151, 15.71658629]), 'previousTarget': array([23.43860471, 15.71414506]), 'currentState': array([8.0053788 , 3.00009559, 0.20445892]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.3031000411135296
running average episode reward sum: 0.48240543760858723
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([24.27804474, 16.07498439,  0.70856357]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 1.1734024329859412}
episode index:2817
target Thresh 31.999999999985068
target distance 18.0
model initialize at round 2817
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  9.]), 'previousTarget': array([24.,  9.]), 'currentState': array([6.        , 2.        , 5.83448753]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 19.313207915827967}
done in step count: 52
reward sum = 0.36861886599693383
running average episode reward sum: 0.4823650591232744
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  9.]), 'previousTarget': array([24.,  9.]), 'currentState': array([23.35539574,  8.24382222,  0.32224155]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 0.9936395112287092}
episode index:2818
target Thresh 31.999999999985214
target distance 22.0
model initialize at round 2818
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.1367368 , 10.77628827]), 'previousTarget': array([11.17429997, 10.77104998]), 'currentState': array([26.91970369, 23.0603412 ,  2.70997968]), 'targetState': array([5, 6], dtype=int32), 'currentDistance': 20.0}
done in step count: 87
reward sum = 0.1713172427572333
running average episode reward sum: 0.4822547193515943
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 6.]), 'previousTarget': array([5., 6.]), 'currentState': array([5.97057413, 6.80349794, 3.84472524]), 'targetState': array([5, 6], dtype=int32), 'currentDistance': 1.2600091578186075}
episode index:2819
target Thresh 31.999999999985363
target distance 6.0
model initialize at round 2819
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 23.]), 'previousTarget': array([21., 23.]), 'currentState': array([27.02455629, 19.96667832,  5.0983761 ]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 6.745095918479228}
done in step count: 17
reward sum = 0.6923290265596269
running average episode reward sum: 0.4823292137867745
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 23.]), 'previousTarget': array([21., 23.]), 'currentState': array([21.27916866, 22.16393371,  2.53747988]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 0.8814431252069971}
episode index:2820
target Thresh 31.99999999998551
target distance 7.0
model initialize at round 2820
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 13.]), 'previousTarget': array([12., 13.]), 'currentState': array([19.01537081, 12.97147801,  4.99333245]), 'targetState': array([12, 13], dtype=int32), 'currentDistance': 7.015428791711132}
done in step count: 16
reward sum = 0.7115818716239464
running average episode reward sum: 0.48241048023762073
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 13.]), 'previousTarget': array([12., 13.]), 'currentState': array([12.95468321, 13.03576937,  2.97631251]), 'targetState': array([12, 13], dtype=int32), 'currentDistance': 0.9553530670126005}
episode index:2821
target Thresh 31.99999999998565
target distance 6.0
model initialize at round 2821
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 12.]), 'previousTarget': array([ 7., 12.]), 'currentState': array([3.25836316, 6.34342021, 0.97136702]), 'targetState': array([ 7, 12], dtype=int32), 'currentDistance': 6.782089731643501}
done in step count: 12
reward sum = 0.7678340826008299
running average episode reward sum: 0.48251162254887625
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 12.]), 'previousTarget': array([ 7., 12.]), 'currentState': array([ 6.6817885 , 11.20291398,  0.92809857]), 'targetState': array([ 7, 12], dtype=int32), 'currentDistance': 0.8582567692763119}
episode index:2822
target Thresh 31.999999999985796
target distance 16.0
model initialize at round 2822
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 6.]), 'previousTarget': array([8., 6.]), 'currentState': array([ 6.        , 22.        ,  2.51449266]), 'targetState': array([8, 6], dtype=int32), 'currentDistance': 16.1245154965971}
done in step count: 47
reward sum = 0.4141397993193624
running average episode reward sum: 0.48248740298698134
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 6.]), 'previousTarget': array([8., 6.]), 'currentState': array([7.87712644, 6.89832504, 4.89765646]), 'targetState': array([8, 6], dtype=int32), 'currentDistance': 0.9066894667188946}
episode index:2823
target Thresh 31.999999999985935
target distance 6.0
model initialize at round 2823
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 13.]), 'previousTarget': array([18., 13.]), 'currentState': array([23.99920038, 13.00377779,  2.0123105 ]), 'targetState': array([18, 13], dtype=int32), 'currentDistance': 5.999201565427825}
done in step count: 12
reward sum = 0.7704180155916569
running average episode reward sum: 0.48258936141920683
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 13.]), 'previousTarget': array([18., 13.]), 'currentState': array([18.78297922, 13.40300649,  3.39131758]), 'targetState': array([18, 13], dtype=int32), 'currentDistance': 0.8806081385712559}
episode index:2824
target Thresh 31.999999999986077
target distance 18.0
model initialize at round 2824
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 29.]), 'previousTarget': array([26., 29.]), 'currentState': array([21.10507389, 11.00934433,  0.34119767]), 'targetState': array([26, 29], dtype=int32), 'currentDistance': 18.64467734148456}
done in step count: 46
reward sum = 0.3773933997086887
running average episode reward sum: 0.4825521239106367
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 29.]), 'previousTarget': array([26., 29.]), 'currentState': array([25.9297091 , 28.16931008,  1.25165345]), 'targetState': array([26, 29], dtype=int32), 'currentDistance': 0.8336585345206816}
episode index:2825
target Thresh 31.999999999986215
target distance 15.0
model initialize at round 2825
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([15.05339433, 24.78683678,  4.77111644]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 16.837686960906197}
done in step count: 40
reward sum = 0.44880599645837815
running average episode reward sum: 0.4825401826058057
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 7.40587739, 10.91480946,  4.18695602]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 1.0008060757773902}
episode index:2826
target Thresh 31.99999999998635
target distance 14.0
model initialize at round 2826
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([ 1.95330329, 27.95538269,  4.15672278]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 19.800565509213975}
done in step count: 56
reward sum = 0.3418520328166512
running average episode reward sum: 0.48249041672331927
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([15.3670879 , 14.6945306 ,  5.56842605]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 0.9396544474500815}
episode index:2827
target Thresh 31.99999999998649
target distance 22.0
model initialize at round 2827
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7.26603883, 23.66452128]), 'previousTarget': array([ 7.44208854, 23.57704261]), 'currentState': array([24.80915145, 14.06039719,  2.914689  ]), 'targetState': array([ 3, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 75
reward sum = 0.2535338720862076
running average episode reward sum: 0.4824094561346922
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 26.]), 'previousTarget': array([ 3., 26.]), 'currentState': array([ 3.9438162 , 25.547821  ,  2.55913696]), 'targetState': array([ 3, 26], dtype=int32), 'currentDistance': 1.0465442463563428}
episode index:2828
target Thresh 31.99999999998662
target distance 13.0
model initialize at round 2828
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 11.]), 'previousTarget': array([24., 11.]), 'currentState': array([23.99183145, 23.64235773,  4.72867019]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 12.642360371502447}
done in step count: 30
reward sum = 0.5728581719869119
running average episode reward sum: 0.4824414281091893
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 11.]), 'previousTarget': array([24., 11.]), 'currentState': array([23.65591867, 11.93388094,  4.57982516]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 0.9952515169132803}
episode index:2829
target Thresh 31.999999999986755
target distance 18.0
model initialize at round 2829
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  8.]), 'previousTarget': array([20.,  8.]), 'currentState': array([17.06566759, 25.95397616,  5.48870757]), 'targetState': array([20,  8], dtype=int32), 'currentDistance': 18.192184222342593}
done in step count: 42
reward sum = 0.39963260144734053
running average episode reward sum: 0.48241216703969747
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  8.]), 'previousTarget': array([20.,  8.]), 'currentState': array([19.47491099,  8.78837285,  4.72360528]), 'targetState': array([20,  8], dtype=int32), 'currentDistance': 0.9472329260675428}
episode index:2830
target Thresh 31.999999999986887
target distance 7.0
model initialize at round 2830
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([ 9.97666341, 21.03392323,  2.41356978]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 7.033961940517253}
done in step count: 18
reward sum = 0.69346429351877
running average episode reward sum: 0.48248671741994437
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([ 9.70832544, 14.93489012,  4.7163473 ]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 0.9793332365420695}
episode index:2831
target Thresh 31.99999999998702
target distance 14.0
model initialize at round 2831
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([ 6.        , 23.        ,  3.56774023]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 14.0356688476182}
done in step count: 36
reward sum = 0.4975847916948862
running average episode reward sum: 0.48249204866086065
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([4.81671227, 9.60983821, 4.76377525]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 0.6367864911153666}
episode index:2832
target Thresh 31.999999999987146
target distance 15.0
model initialize at round 2832
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([17.86721205, 17.12150933,  2.65301752]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 16.484838167958404}
done in step count: 38
reward sum = 0.44353222003948706
running average episode reward sum: 0.48247829651521246
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.96227507, 10.7580825 ,  3.61082904]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.2250152565872454}
episode index:2833
target Thresh 31.999999999987274
target distance 22.0
model initialize at round 2833
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.44279949, 26.61527449]), 'previousTarget': array([21.42229124, 26.6773982 ]), 'currentState': array([25.09402078,  6.95138416,  6.04951301]), 'targetState': array([21, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.2538842106855031
running average episode reward sum: 0.48239763522875173
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 29.]), 'previousTarget': array([21., 29.]), 'currentState': array([21.63161935, 28.36812781,  1.74946139]), 'targetState': array([21, 29], dtype=int32), 'currentDistance': 0.8934234559050205}
episode index:2834
target Thresh 31.999999999987402
target distance 20.0
model initialize at round 2834
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.96690218,  5.22087971]), 'previousTarget': array([23.96680906,  5.22127294]), 'currentState': array([21.00308262, 25.00005496,  6.04851163]), 'targetState': array([24,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.32284741743268186
running average episode reward sum: 0.48234135649231574
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  5.]), 'previousTarget': array([24.,  5.]), 'currentState': array([23.47527722,  5.77683062,  4.78463756]), 'targetState': array([24,  5], dtype=int32), 'currentDistance': 0.937443232755772}
episode index:2835
target Thresh 31.999999999987526
target distance 19.0
model initialize at round 2835
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.78890931, 22.09123401]), 'previousTarget': array([14.67985983, 21.90977806]), 'currentState': array([4.07394417, 5.20367877, 1.2097491 ]), 'targetState': array([16, 24], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.2929697682212354
running average episode reward sum: 0.48227458230745285
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 24.]), 'previousTarget': array([16., 24.]), 'currentState': array([15.21540186, 24.02403482,  0.91512906]), 'targetState': array([16, 24], dtype=int32), 'currentDistance': 0.7849661879637683}
episode index:2836
target Thresh 31.99999999998765
target distance 5.0
model initialize at round 2836
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([20.95400539,  5.96261423,  3.59645933]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 5.037595746168343}
done in step count: 12
reward sum = 0.7718908812371017
running average episode reward sum: 0.4823766677141958
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([20.13197657, 10.14573901,  1.10945424]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 1.2178778727969977}
episode index:2837
target Thresh 31.999999999987775
target distance 8.0
model initialize at round 2837
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([18.0582322 ,  8.8962177 ,  4.97978801]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 8.058900475695683}
done in step count: 19
reward sum = 0.658164639854455
running average episode reward sum: 0.48243860850776177
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.73107667,  9.07867484,  2.60909685]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.735297781494736}
episode index:2838
target Thresh 31.999999999987896
target distance 10.0
model initialize at round 2838
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 23.]), 'previousTarget': array([15., 23.]), 'currentState': array([25.04875034, 28.99998448,  6.03036693]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 11.703725783531453}
done in step count: 31
reward sum = 0.5340627827603577
running average episode reward sum: 0.482456792436699
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 23.]), 'previousTarget': array([15., 23.]), 'currentState': array([15.75869795, 23.89593403,  3.57312382]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 1.1740188940709477}
episode index:2839
target Thresh 31.999999999988017
target distance 8.0
model initialize at round 2839
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([24.        , 15.        ,  4.77821758]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 8.0}
done in step count: 19
reward sum = 0.6825357726411365
running average episode reward sum: 0.4825272427818414
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([16.94513575, 15.35049632,  3.04741214]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 1.0080323667266444}
episode index:2840
target Thresh 31.999999999988134
target distance 12.0
model initialize at round 2840
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 2.]), 'previousTarget': array([5., 2.]), 'currentState': array([ 4.        , 14.        ,  3.28040421]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 12.041594578792296}
done in step count: 32
reward sum = 0.5500035311126996
running average episode reward sum: 0.48255099367530524
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 2.]), 'previousTarget': array([5., 2.]), 'currentState': array([4.83017988, 2.81583182, 4.92850242]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 0.8333189245873438}
episode index:2841
target Thresh 31.99999999998825
target distance 14.0
model initialize at round 2841
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([17.        , 16.        ,  1.52652049]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 19.104973174542803}
done in step count: 59
reward sum = 0.31448714988966975
running average episode reward sum: 0.48249185791042637
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.49122419, 2.89520533, 3.86001787]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 1.021123787205123}
episode index:2842
target Thresh 31.99999999998837
target distance 26.0
model initialize at round 2842
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4.51610732, 8.03196445]), 'previousTarget': array([4.53392998, 8.05891029]), 'currentState': array([ 2.91681702, 27.96791867,  3.74519467]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 90
reward sum = 0.18854097780438647
running average episode reward sum: 0.48238846329906304
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 2.]), 'previousTarget': array([5., 2.]), 'currentState': array([4.67795041, 2.83205388, 4.69291585]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 0.8922049084354573}
episode index:2843
target Thresh 31.999999999988486
target distance 11.0
model initialize at round 2843
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  4.]), 'previousTarget': array([13.,  4.]), 'currentState': array([11.19581028, 14.56063205,  5.08152052]), 'targetState': array([13,  4], dtype=int32), 'currentDistance': 10.71363849811379}
done in step count: 25
reward sum = 0.6231433901684479
running average episode reward sum: 0.4824379551861479
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  4.]), 'previousTarget': array([13.,  4.]), 'currentState': array([12.09193765,  4.7840771 ,  4.9783664 ]), 'targetState': array([13,  4], dtype=int32), 'currentDistance': 1.1997308545681413}
episode index:2844
target Thresh 31.9999999999886
target distance 17.0
model initialize at round 2844
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  9.]), 'previousTarget': array([24.,  9.]), 'currentState': array([ 7.        , 17.        ,  0.96010447]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 18.78829422805594}
done in step count: 48
reward sum = 0.3577714582931108
running average episode reward sum: 0.4823941356793314
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  9.]), 'previousTarget': array([24.,  9.]), 'currentState': array([23.34140729,  9.13437743,  5.83380403]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 0.6721619274310979}
episode index:2845
target Thresh 31.999999999988713
target distance 17.0
model initialize at round 2845
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.23585354,  4.97972134]), 'previousTarget': array([19.23243274,  5.00324289]), 'currentState': array([ 5.93007305, 19.91143944,  4.296525  ]), 'targetState': array([21,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.26639228759986233
running average episode reward sum: 0.48231823903559307
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  3.]), 'previousTarget': array([21.,  3.]), 'currentState': array([20.05279053,  3.78826307,  5.34776577]), 'targetState': array([21,  3], dtype=int32), 'currentDistance': 1.2323004745454564}
episode index:2846
target Thresh 31.999999999988827
target distance 14.0
model initialize at round 2846
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  4.]), 'previousTarget': array([22.,  4.]), 'currentState': array([26.        , 18.        ,  0.36538911]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 14.560219778561038}
done in step count: 35
reward sum = 0.45821596997051295
running average episode reward sum: 0.4823097731876601
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  4.]), 'previousTarget': array([22.,  4.]), 'currentState': array([22.34630829,  4.80250652,  4.21119238]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 0.8740401334030041}
episode index:2847
target Thresh 31.999999999988937
target distance 5.0
model initialize at round 2847
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 24.]), 'previousTarget': array([18., 24.]), 'currentState': array([23.        , 21.        ,  5.20186552]), 'targetState': array([18, 24], dtype=int32), 'currentDistance': 5.830951894845301}
done in step count: 15
reward sum = 0.7358794390865132
running average episode reward sum: 0.4823988074804617
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 24.]), 'previousTarget': array([18., 24.]), 'currentState': array([18.95535144, 23.14377477,  2.07801599]), 'targetState': array([18, 24], dtype=int32), 'currentDistance': 1.2828943927867673}
episode index:2848
target Thresh 31.999999999989047
target distance 12.0
model initialize at round 2848
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 23.]), 'previousTarget': array([24., 23.]), 'currentState': array([11.99845781, 20.00145428,  2.13302636]), 'targetState': array([24, 23], dtype=int32), 'currentDistance': 12.370460438530055}
done in step count: 33
reward sum = 0.5198422094157747
running average episode reward sum: 0.4824119501276837
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 23.]), 'previousTarget': array([24., 23.]), 'currentState': array([23.14654307, 22.40018248,  0.29637988]), 'targetState': array([24, 23], dtype=int32), 'currentDistance': 1.0431537663456616}
episode index:2849
target Thresh 31.999999999989157
target distance 7.0
model initialize at round 2849
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 23.]), 'previousTarget': array([15., 23.]), 'currentState': array([22.        , 27.        ,  3.15573636]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 17
reward sum = 0.7095644635337964
running average episode reward sum: 0.48249165276396655
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 23.]), 'previousTarget': array([15., 23.]), 'currentState': array([15.65881875, 23.31172537,  3.66904849]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 0.728844871843984}
episode index:2850
target Thresh 31.999999999989264
target distance 13.0
model initialize at round 2850
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 19.]), 'previousTarget': array([20., 19.]), 'currentState': array([10.98714802,  6.01339094,  2.09003824]), 'targetState': array([20, 19], dtype=int32), 'currentDistance': 15.807704317423216}
done in step count: 44
reward sum = 0.4414849832615244
running average episode reward sum: 0.48247726950563524
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 19.]), 'previousTarget': array([20., 19.]), 'currentState': array([19.86177136, 18.16723308,  0.94520112]), 'targetState': array([20, 19], dtype=int32), 'currentDistance': 0.8441610679596512}
episode index:2851
target Thresh 31.99999999998937
target distance 12.0
model initialize at round 2851
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  7.]), 'previousTarget': array([12.,  7.]), 'currentState': array([24.       ,  4.       ,  1.7023696]), 'targetState': array([12,  7], dtype=int32), 'currentDistance': 12.36931687685298}
done in step count: 31
reward sum = 0.5513966188983517
running average episode reward sum: 0.48250143477540824
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  7.]), 'previousTarget': array([12.,  7.]), 'currentState': array([12.92040705,  6.9908253 ,  2.94259697]), 'targetState': array([12,  7], dtype=int32), 'currentDistance': 0.9204527716900622}
episode index:2852
target Thresh 31.999999999989477
target distance 5.0
model initialize at round 2852
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([16.        , 11.        ,  1.35639346]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 7.071067811865475}
done in step count: 19
reward sum = 0.692442411751576
running average episode reward sum: 0.48257502081711035
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([20.18819539,  6.4196226 ,  5.47713333]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 0.9138434464967198}
episode index:2853
target Thresh 31.99999999998958
target distance 4.0
model initialize at round 2853
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  2.]), 'previousTarget': array([20.,  2.]), 'currentState': array([24.        ,  6.        ,  5.79742679]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 5.65685424949238}
done in step count: 13
reward sum = 0.7661708222226302
running average episode reward sum: 0.4826743886522209
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  2.]), 'previousTarget': array([20.,  2.]), 'currentState': array([20.78273946,  2.83699466,  3.70137255]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 1.1459673301770998}
episode index:2854
target Thresh 31.999999999989686
target distance 5.0
model initialize at round 2854
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([15.        , 10.        ,  0.66398044]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 5.0}
done in step count: 14
reward sum = 0.7558916409776861
running average episode reward sum: 0.4827700864638936
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.69236219, 10.41754455,  3.24218598]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.8085226418606444}
episode index:2855
target Thresh 31.999999999989786
target distance 15.0
model initialize at round 2855
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 28.]), 'previousTarget': array([ 2., 28.]), 'currentState': array([ 3.01695924, 12.9575733 ,  5.34516287]), 'targetState': array([ 2, 28], dtype=int32), 'currentDistance': 15.076763814602613}
done in step count: 41
reward sum = 0.4311603938739924
running average episode reward sum: 0.4827520158432389
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 28.]), 'previousTarget': array([ 2., 28.]), 'currentState': array([ 2.27951909, 27.31268006,  1.62955363]), 'targetState': array([ 2, 28], dtype=int32), 'currentDistance': 0.7419835665129089}
episode index:2856
target Thresh 31.99999999998989
target distance 13.0
model initialize at round 2856
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([ 3.02926662, 19.26997258,  1.21031213]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 13.254922640273858}
done in step count: 34
reward sum = 0.5168062263693445
running average episode reward sum: 0.4827639354129016
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([15.23896539, 22.36721501,  6.04333697]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 0.8449973593597274}
episode index:2857
target Thresh 31.99999999998999
target distance 4.0
model initialize at round 2857
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([12.35701125,  5.26579721,  0.72896051]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 4.574894533933962}
done in step count: 8
reward sum = 0.8401142276079836
running average episode reward sum: 0.4828889705046423
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.83641345,  8.24933665,  0.85075952]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.7682812154397735}
episode index:2858
target Thresh 31.999999999990088
target distance 15.0
model initialize at round 2858
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  7.]), 'previousTarget': array([19.,  7.]), 'currentState': array([4.01560296, 7.07104626, 1.10211146]), 'targetState': array([19,  7], dtype=int32), 'currentDistance': 14.984565466973404}
done in step count: 38
reward sum = 0.46857310087000276
running average episode reward sum: 0.4828839632050149
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  7.]), 'previousTarget': array([19.,  7.]), 'currentState': array([18.35797708,  6.92818756,  6.27123274]), 'targetState': array([19,  7], dtype=int32), 'currentDistance': 0.646026665014068}
episode index:2859
target Thresh 31.999999999990187
target distance 18.0
model initialize at round 2859
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7.24801054, 24.28155314]), 'previousTarget': array([ 7.29018892, 24.21358457]), 'currentState': array([19.00054615,  8.098911  ,  1.69960946]), 'targetState': array([ 6, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.294691119651072
running average episode reward sum: 0.48281816151146456
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 26.]), 'previousTarget': array([ 6., 26.]), 'currentState': array([ 6.61607345, 25.23964436,  1.84873031]), 'targetState': array([ 6, 26], dtype=int32), 'currentDistance': 0.9786149355388545}
episode index:2860
target Thresh 31.999999999990287
target distance 10.0
model initialize at round 2860
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([24.99483632,  3.06889768,  1.82684803]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 12.75927650461739}
done in step count: 33
reward sum = 0.5559595643080936
running average episode reward sum: 0.48284372648972274
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.88053328, 10.78257517,  2.58691575]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.9069798325056616}
episode index:2861
target Thresh 31.999999999990383
target distance 21.0
model initialize at round 2861
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.63241055, 22.45612429]), 'previousTarget': array([19.63241055, 22.45612429]), 'currentState': array([15.        ,  3.        ,  2.59112218]), 'targetState': array([20, 24], dtype=int32), 'currentDistance': 20.0}
done in step count: 74
reward sum = 0.2524934688620824
running average episode reward sum: 0.4827632407253525
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 24.]), 'previousTarget': array([20., 24.]), 'currentState': array([19.91738401, 23.04080239,  1.05810582]), 'targetState': array([20, 24], dtype=int32), 'currentDistance': 0.9627489031212767}
episode index:2862
target Thresh 31.99999999999048
target distance 6.0
model initialize at round 2862
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([ 8.08406563, 10.21000901,  1.01657799]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 7.025831704192023}
done in step count: 13
reward sum = 0.7601570552252044
running average episode reward sum: 0.4828601299375424
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([13.25878316, 13.30577552,  0.51266906]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 1.0155540490476}
episode index:2863
target Thresh 31.99999999999057
target distance 12.0
model initialize at round 2863
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 10.]), 'previousTarget': array([25., 10.]), 'currentState': array([13.       , 16.       ,  2.0044584]), 'targetState': array([25, 10], dtype=int32), 'currentDistance': 13.416407864998739}
done in step count: 31
reward sum = 0.4920024383691147
running average episode reward sum: 0.48286332208434113
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 10.]), 'previousTarget': array([25., 10.]), 'currentState': array([24.01016878, 10.59720003,  5.68303278]), 'targetState': array([25, 10], dtype=int32), 'currentDistance': 1.156033619001733}
episode index:2864
target Thresh 31.999999999990667
target distance 20.0
model initialize at round 2864
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 26.]), 'previousTarget': array([23.97504678, 26.00124766]), 'currentState': array([ 4.0824288 , 26.9858608 ,  6.13516468]), 'targetState': array([24, 26], dtype=int32), 'currentDistance': 19.941954866431985}
done in step count: 59
reward sum = 0.3610364565896007
running average episode reward sum: 0.48282079961819985
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 26.]), 'previousTarget': array([24., 26.]), 'currentState': array([23.01921576, 26.0476128 ,  6.20405351]), 'targetState': array([24, 26], dtype=int32), 'currentDistance': 0.981939261278042}
episode index:2865
target Thresh 31.99999999999076
target distance 13.0
model initialize at round 2865
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 26.]), 'previousTarget': array([ 3., 26.]), 'currentState': array([ 5.99709825, 12.99190457,  4.12287444]), 'targetState': array([ 3, 26], dtype=int32), 'currentDistance': 13.348900498276201}
done in step count: 33
reward sum = 0.48961769909987035
running average episode reward sum: 0.48282317118117324
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 26.]), 'previousTarget': array([ 3., 26.]), 'currentState': array([ 3.05088365, 25.24057134,  1.65049311]), 'targetState': array([ 3, 26], dtype=int32), 'currentDistance': 0.7611314153466858}
episode index:2866
target Thresh 31.99999999999085
target distance 4.0
model initialize at round 2866
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([12.04175897, 12.67908452,  4.98705718]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 5.404011007480065}
done in step count: 9
reward sum = 0.8180252784639289
running average episode reward sum: 0.4829400885537867
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.02022235,  9.48470007,  5.46839328]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.0931140873468292}
episode index:2867
target Thresh 31.99999999999094
target distance 14.0
model initialize at round 2867
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 18.]), 'previousTarget': array([25., 18.]), 'currentState': array([10.97423809, 24.0718217 ,  1.66269398]), 'targetState': array([25, 18], dtype=int32), 'currentDistance': 15.283619198550854}
done in step count: 46
reward sum = 0.41915806737791106
running average episode reward sum: 0.4829178493553293
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 18.]), 'previousTarget': array([25., 18.]), 'currentState': array([24.11367199, 18.23823708,  5.89063233]), 'targetState': array([25, 18], dtype=int32), 'currentDistance': 0.9177876878116731}
episode index:2868
target Thresh 31.999999999991033
target distance 21.0
model initialize at round 2868
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.40929929,  8.96045606]), 'previousTarget': array([17.40132839,  8.94278962]), 'currentState': array([26.00577463, 27.01871211,  1.4716225 ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.12475735326752975
running average episode reward sum: 0.4827060420347915
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([17.91604138, 13.14387861,  4.33972512]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 7.396365066969888}
episode index:2869
target Thresh 31.99999999999112
target distance 13.0
model initialize at round 2869
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([27.        , 12.        ,  4.88724038]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 13.152946437965907}
done in step count: 37
reward sum = 0.5090277662669662
running average episode reward sum: 0.4827152133672765
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.76869354, 10.13200006,  3.41094497]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.7799447236215084}
episode index:2870
target Thresh 31.99999999999121
target distance 14.0
model initialize at round 2870
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  2.]), 'previousTarget': array([22.,  2.]), 'currentState': array([13.96315801, 16.04320156,  2.52940965]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 16.18030714442598}
done in step count: 47
reward sum = 0.39402082581484943
running average episode reward sum: 0.48268432016367063
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  2.]), 'previousTarget': array([22.,  2.]), 'currentState': array([21.46311665,  2.82647788,  5.36384643]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 0.9855503153965223}
episode index:2871
target Thresh 31.999999999991296
target distance 19.0
model initialize at round 2871
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 19.]), 'previousTarget': array([ 3., 19.]), 'currentState': array([21.77422167, 17.95568847,  3.32451546]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 18.803244026054827}
done in step count: 54
reward sum = 0.3875069846654624
running average episode reward sum: 0.4826511804228983
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 19.]), 'previousTarget': array([ 3., 19.]), 'currentState': array([ 3.92412703, 18.79781459,  3.00326048]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 0.9459860990902612}
episode index:2872
target Thresh 31.999999999991385
target distance 6.0
model initialize at round 2872
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([12.        ,  3.        ,  2.13409781]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 5.999999999999999}
done in step count: 16
reward sum = 0.7317680888560517
running average episode reward sum: 0.48273789010213014
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([17.03397304,  2.95365614,  6.17242847]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 0.9671379657496907}
episode index:2873
target Thresh 31.99999999999147
target distance 13.0
model initialize at round 2873
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 14.]), 'previousTarget': array([ 9., 14.]), 'currentState': array([21.        , 27.        ,  1.76077533]), 'targetState': array([ 9, 14], dtype=int32), 'currentDistance': 17.69180601295413}
done in step count: 52
reward sum = 0.3614583448374643
running average episode reward sum: 0.48269569123460593
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 14.]), 'previousTarget': array([ 9., 14.]), 'currentState': array([ 9.3954222 , 14.91023084,  4.18026808]), 'targetState': array([ 9, 14], dtype=int32), 'currentDistance': 0.9924106494607788}
episode index:2874
target Thresh 31.999999999991555
target distance 5.0
model initialize at round 2874
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 23.]), 'previousTarget': array([10., 23.]), 'currentState': array([ 8.15941043, 27.63496114,  5.02919857]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 4.987046691993825}
done in step count: 11
reward sum = 0.8300215054521478
running average episode reward sum: 0.48281650021346423
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 23.]), 'previousTarget': array([10., 23.]), 'currentState': array([ 9.53002823, 23.92612544,  5.04102568]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 1.0385479239881392}
episode index:2875
target Thresh 31.99999999999164
target distance 10.0
model initialize at round 2875
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 27.]), 'previousTarget': array([ 4., 27.]), 'currentState': array([14.        , 27.        ,  5.11154938]), 'targetState': array([ 4, 27], dtype=int32), 'currentDistance': 10.0}
done in step count: 25
reward sum = 0.5955415162168269
running average episode reward sum: 0.4828556952816156
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 27.]), 'previousTarget': array([ 4., 27.]), 'currentState': array([ 4.79335131, 27.39192753,  3.16739618]), 'targetState': array([ 4, 27], dtype=int32), 'currentDistance': 0.8848804917802515}
episode index:2876
target Thresh 31.999999999991722
target distance 10.0
model initialize at round 2876
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 24.]), 'previousTarget': array([12., 24.]), 'currentState': array([ 7.19489519, 14.19322182,  0.83361457]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 10.92071107612344}
done in step count: 28
reward sum = 0.6149735260454618
running average episode reward sum: 0.482901617363911
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 24.]), 'previousTarget': array([12., 24.]), 'currentState': array([12.02990771, 23.12643456,  1.19099032]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 0.8740772543612116}
episode index:2877
target Thresh 31.999999999991804
target distance 13.0
model initialize at round 2877
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.06827978, 22.72736424,  4.81926686]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 12.727547391832717}
done in step count: 37
reward sum = 0.5469938857115966
running average episode reward sum: 0.4829238870888407
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 2.56387256, 10.96814561,  5.03017513]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.061844177883368}
episode index:2878
target Thresh 31.999999999991886
target distance 15.0
model initialize at round 2878
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 20.]), 'previousTarget': array([ 2., 20.]), 'currentState': array([3.        , 5.        , 2.83435664]), 'targetState': array([ 2, 20], dtype=int32), 'currentDistance': 15.033296378372908}
done in step count: 37
reward sum = 0.47705699064390594
running average episode reward sum: 0.4829218492644416
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 20.]), 'previousTarget': array([ 2., 20.]), 'currentState': array([ 2.1634831 , 19.11806442,  1.78277965]), 'targetState': array([ 2, 20], dtype=int32), 'currentDistance': 0.8969599117964446}
episode index:2879
target Thresh 31.999999999991967
target distance 13.0
model initialize at round 2879
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 22.]), 'previousTarget': array([26., 22.]), 'currentState': array([13.19642876, 13.22119244,  0.77693091]), 'targetState': array([26, 22], dtype=int32), 'currentDistance': 15.524139227248286}
done in step count: 41
reward sum = 0.47818817598826013
running average episode reward sum: 0.48292020562788734
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 22.]), 'previousTarget': array([26., 22.]), 'currentState': array([25.49923594, 21.02324781,  0.5594967 ]), 'targetState': array([26, 22], dtype=int32), 'currentDistance': 1.0976381402753312}
episode index:2880
target Thresh 31.999999999992045
target distance 8.0
model initialize at round 2880
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'previousTarget': array([11., 19.]), 'currentState': array([18.98770983, 12.13340541,  1.79384682]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 10.533452879842127}
done in step count: 24
reward sum = 0.6261554019071235
running average episode reward sum: 0.4829699228081301
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'previousTarget': array([11., 19.]), 'currentState': array([11.83048334, 18.86039147,  2.50544941]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 0.8421360429910706}
episode index:2881
target Thresh 31.999999999992127
target distance 15.0
model initialize at round 2881
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([22.96235234, 27.87611735,  4.27288216]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 18.497619207049738}
done in step count: 45
reward sum = 0.4115159570715804
running average episode reward sum: 0.4829451296208516
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([ 8.88902748, 17.9109129 ,  3.75470838]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 1.2728441283658773}
episode index:2882
target Thresh 31.999999999992205
target distance 22.0
model initialize at round 2882
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.54654412, 18.79880147]), 'previousTarget': array([12.54654412, 18.79880147]), 'currentState': array([26.        ,  4.        ,  1.69502924]), 'targetState': array([ 6, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.05874955615794273
running average episode reward sum: 0.4827572369098635
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 26.]), 'previousTarget': array([ 6., 26.]), 'currentState': array([18.8314893 , 16.0400224 ,  2.50805107]), 'targetState': array([ 6, 26], dtype=int32), 'currentDistance': 16.24340701864865}
episode index:2883
target Thresh 31.99999999999228
target distance 4.0
model initialize at round 2883
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'previousTarget': array([11., 21.]), 'currentState': array([14.03419958, 17.21064357,  1.58189851]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 4.854440162880645}
done in step count: 9
reward sum = 0.8373517926839004
running average episode reward sum: 0.48288018925236487
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'previousTarget': array([11., 21.]), 'currentState': array([11.704759  , 20.11148359,  2.39897955]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 1.1340840649594621}
episode index:2884
target Thresh 31.999999999992358
target distance 16.0
model initialize at round 2884
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 20.]), 'previousTarget': array([ 5., 20.]), 'currentState': array([20.99790584, 14.97594776,  4.40664256]), 'targetState': array([ 5, 20], dtype=int32), 'currentDistance': 16.768246539235804}
done in step count: 38
reward sum = 0.41978386093425524
running average episode reward sum: 0.48285831877461166
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 20.]), 'previousTarget': array([ 5., 20.]), 'currentState': array([ 5.84292367, 19.70173224,  2.92665015]), 'targetState': array([ 5, 20], dtype=int32), 'currentDistance': 0.8941386787108515}
episode index:2885
target Thresh 31.999999999992433
target distance 13.0
model initialize at round 2885
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 19.]), 'previousTarget': array([26., 19.]), 'currentState': array([13.        , 11.        ,  5.36902905]), 'targetState': array([26, 19], dtype=int32), 'currentDistance': 15.264337522473749}
done in step count: 42
reward sum = 0.4580433486989218
running average episode reward sum: 0.4828497203788821
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 19.]), 'previousTarget': array([26., 19.]), 'currentState': array([25.22671363, 18.36711464,  0.55710712]), 'targetState': array([26, 19], dtype=int32), 'currentDistance': 0.9992575638638035}
episode index:2886
target Thresh 31.99999999999251
target distance 7.0
model initialize at round 2886
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([19.94295952,  6.11704374,  2.17220569]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 7.517720648135208}
done in step count: 15
reward sum = 0.7380676702917911
running average episode reward sum: 0.4829381228554712
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.99150618,  9.29919924,  2.78923875]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 1.0356663013203988}
episode index:2887
target Thresh 31.999999999992585
target distance 23.0
model initialize at round 2887
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.71995654,  5.1330194 ]), 'previousTarget': array([12.73259233,  5.07518824]), 'currentState': array([10.93936493, 25.05359905,  2.65724769]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 90
reward sum = 0.1988066312700097
running average episode reward sum: 0.4828397393750053
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  2.]), 'previousTarget': array([13.,  2.]), 'currentState': array([12.83091583,  2.79937863,  4.89778963]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 0.8170652657481884}
episode index:2888
target Thresh 31.999999999992657
target distance 9.0
model initialize at round 2888
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 20.]), 'previousTarget': array([11., 20.]), 'currentState': array([20.00319171, 28.00814454,  1.44913015]), 'targetState': array([11, 20], dtype=int32), 'currentDistance': 12.049391683377094}
done in step count: 31
reward sum = 0.5324668407157247
running average episode reward sum: 0.48285691732631736
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 20.]), 'previousTarget': array([11., 20.]), 'currentState': array([11.53043411, 20.99972005,  3.93269704]), 'targetState': array([11, 20], dtype=int32), 'currentDistance': 1.131724581914705}
episode index:2889
target Thresh 31.99999999999273
target distance 8.0
model initialize at round 2889
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  8.]), 'previousTarget': array([11.,  8.]), 'currentState': array([10.95184207, 16.02932835,  2.8410944 ]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 8.029472763807194}
done in step count: 19
reward sum = 0.6749736114716132
running average episode reward sum: 0.4829233936910735
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  8.]), 'previousTarget': array([11.,  8.]), 'currentState': array([11.00429466,  8.91922487,  4.77770259]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 0.9192349051326787}
episode index:2890
target Thresh 31.999999999992802
target distance 20.0
model initialize at round 2890
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7.0992562 , 20.00992562]), 'previousTarget': array([ 7.0992562 , 20.00992562]), 'currentState': array([27.        , 22.        ,  1.29417691]), 'targetState': array([ 7, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.3081451267614327
running average episode reward sum: 0.48286293770112904
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 20.]), 'previousTarget': array([ 7., 20.]), 'currentState': array([ 7.8056765 , 20.59328575,  3.23941856]), 'targetState': array([ 7, 20], dtype=int32), 'currentDistance': 1.000551149185043}
episode index:2891
target Thresh 31.999999999992873
target distance 14.0
model initialize at round 2891
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([26.06151021, 22.93519673,  5.21922255]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 19.10616088965848}
done in step count: 50
reward sum = 0.37089571297093493
running average episode reward sum: 0.48282422151000515
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([12.69591102, 10.99848025,  3.81195668]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 1.217068182422904}
episode index:2892
target Thresh 31.999999999992944
target distance 7.0
model initialize at round 2892
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'previousTarget': array([ 2., 17.]), 'currentState': array([ 9.        , 15.        ,  0.71559656]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 7.280109889280518}
done in step count: 20
reward sum = 0.6904992994816048
running average episode reward sum: 0.4828960068808906
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'previousTarget': array([ 2., 17.]), 'currentState': array([ 2.9398051 , 16.94938045,  2.76329508]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 0.941167341342861}
episode index:2893
target Thresh 31.999999999993015
target distance 11.0
model initialize at round 2893
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([20.19384114, 28.74064911,  5.22208366]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 11.530387975983828}
done in step count: 27
reward sum = 0.5940697881000041
running average episode reward sum: 0.48293442214737964
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([16.28077958, 18.91326967,  4.38732466]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 0.9554573043856591}
episode index:2894
target Thresh 31.999999999993086
target distance 7.0
model initialize at round 2894
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([24.99933496, 25.00545357,  1.92432174]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 9.223253204591312}
done in step count: 25
reward sum = 0.6252532729507243
running average episode reward sum: 0.48298358237218214
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([19.15996005, 18.97991836,  3.94592871]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 0.9928883149130102}
episode index:2895
target Thresh 31.999999999993154
target distance 12.0
model initialize at round 2895
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 19.]), 'previousTarget': array([ 4., 19.]), 'currentState': array([3.01510203, 6.97205434, 5.43678367]), 'targetState': array([ 4, 19], dtype=int32), 'currentDistance': 12.068202053681087}
done in step count: 32
reward sum = 0.5330960988748387
running average episode reward sum: 0.48300088641793587
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 19.]), 'previousTarget': array([ 4., 19.]), 'currentState': array([ 4.28784226, 18.07028986,  1.75640629]), 'targetState': array([ 4, 19], dtype=int32), 'currentDistance': 0.9732492593097885}
episode index:2896
target Thresh 31.99999999999322
target distance 17.0
model initialize at round 2896
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.57118733,  9.60973776]), 'previousTarget': array([19.53366395,  9.66064273]), 'currentState': array([ 8.06602207, 25.96917455,  5.72576261]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.3446421302486714
running average episode reward sum: 0.48295312709582017
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  9.]), 'previousTarget': array([20.,  9.]), 'currentState': array([19.03529014,  9.70051495,  5.2081066 ]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 1.1922190655869105}
episode index:2897
target Thresh 31.99999999999329
target distance 3.0
model initialize at round 2897
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 19.]), 'previousTarget': array([18., 19.]), 'currentState': array([18.07093482, 21.93689051,  5.37813991]), 'targetState': array([18, 19], dtype=int32), 'currentDistance': 2.937747033943008}
done in step count: 6
reward sum = 0.8982360402291575
running average episode reward sum: 0.48309642692781923
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 19.]), 'previousTarget': array([18., 19.]), 'currentState': array([18.29595002, 19.83690745,  4.57895107]), 'targetState': array([18, 19], dtype=int32), 'currentDistance': 0.887693917273437}
episode index:2898
target Thresh 31.999999999993356
target distance 16.0
model initialize at round 2898
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2.43312027, 15.35314712]), 'previousTarget': array([ 2.47772  , 15.3881475]), 'currentState': array([17.93371287, 27.99164501,  3.40462419]), 'targetState': array([ 2, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.338806104058644
running average episode reward sum: 0.48304665448115863
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 15.]), 'previousTarget': array([ 2., 15.]), 'currentState': array([ 2.60294206, 15.77953551,  3.89790016]), 'targetState': array([ 2, 15], dtype=int32), 'currentDistance': 0.9855022768307166}
episode index:2899
target Thresh 31.999999999993424
target distance 7.0
model initialize at round 2899
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  7.]), 'previousTarget': array([11.,  7.]), 'currentState': array([11.       , 14.       ,  1.4800397]), 'targetState': array([11,  7], dtype=int32), 'currentDistance': 7.0}
done in step count: 19
reward sum = 0.6777783090605042
running average episode reward sum: 0.4831138033275653
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  7.]), 'previousTarget': array([11.,  7.]), 'currentState': array([11.27721055,  7.89206454,  4.59474286]), 'targetState': array([11,  7], dtype=int32), 'currentDistance': 0.9341439033560616}
episode index:2900
target Thresh 31.999999999993488
target distance 19.0
model initialize at round 2900
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  6.]), 'previousTarget': array([24.,  6.]), 'currentState': array([26.03420973, 24.96559893,  5.37576696]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 19.07437946534588}
done in step count: 55
reward sum = 0.37604987426168945
running average episode reward sum: 0.4830768974574978
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  6.]), 'previousTarget': array([24.,  6.]), 'currentState': array([23.59002848,  6.83041138,  4.43710509]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 0.9260991863137246}
episode index:2901
target Thresh 31.999999999993552
target distance 13.0
model initialize at round 2901
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 21.]), 'previousTarget': array([ 4., 21.]), 'currentState': array([16.83794965, 17.19998871,  2.33405927]), 'targetState': array([ 4, 21], dtype=int32), 'currentDistance': 13.388541259369399}
done in step count: 30
reward sum = 0.5512816819341932
running average episode reward sum: 0.48310040013995015
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 21.]), 'previousTarget': array([ 4., 21.]), 'currentState': array([ 4.97113555, 20.93669856,  2.86738714]), 'targetState': array([ 4, 21], dtype=int32), 'currentDistance': 0.97319644916452}
episode index:2902
target Thresh 31.999999999993616
target distance 9.0
model initialize at round 2902
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 20.]), 'previousTarget': array([10., 20.]), 'currentState': array([11.0423184 , 11.00047971,  0.26383513]), 'targetState': array([10, 20], dtype=int32), 'currentDistance': 9.059679530051799}
done in step count: 20
reward sum = 0.6560961542853505
running average episode reward sum: 0.4831599922013161
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 20.]), 'previousTarget': array([10., 20.]), 'currentState': array([10.5461185 , 19.13055912,  1.75496779]), 'targetState': array([10, 20], dtype=int32), 'currentDistance': 1.026729199451424}
episode index:2903
target Thresh 31.99999999999368
target distance 13.0
model initialize at round 2903
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([14.        ,  5.        ,  5.61836255]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 15.264337522473749}
done in step count: 44
reward sum = 0.42618818013080906
running average episode reward sum: 0.4831403738087298
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([ 6.54479849, 17.07619039,  2.08494164]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 1.0724875658333468}
episode index:2904
target Thresh 31.999999999993744
target distance 20.0
model initialize at round 2904
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.88854382, 10.94427191]), 'previousTarget': array([20.88854382, 10.94427191]), 'currentState': array([3.        , 2.        , 1.25778025]), 'targetState': array([23, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.28543145515112534
running average episode reward sum: 0.4830723156611713
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 12.]), 'previousTarget': array([23., 12.]), 'currentState': array([22.31226482, 11.33123305,  0.34508505]), 'targetState': array([23, 12], dtype=int32), 'currentDistance': 0.9592856266315384}
episode index:2905
target Thresh 31.999999999993804
target distance 10.0
model initialize at round 2905
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([27.03501274,  5.00838396,  0.47921782]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 12.219811594892725}
done in step count: 30
reward sum = 0.5506965009866311
running average episode reward sum: 0.4830955861998242
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([20.75382416, 14.2043947 ,  2.34985645]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 1.0960103320205508}
episode index:2906
target Thresh 31.999999999993868
target distance 14.0
model initialize at round 2906
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'previousTarget': array([10., 12.]), 'currentState': array([23.98453074, 25.94015743,  4.35441682]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 19.745761295716736}
done in step count: 58
reward sum = 0.35912876614053635
running average episode reward sum: 0.48305294195487775
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'previousTarget': array([10., 12.]), 'currentState': array([10.62863647, 12.81526671,  3.79814152]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 1.0294870684451451}
episode index:2907
target Thresh 31.99999999999393
target distance 15.0
model initialize at round 2907
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 27.]), 'previousTarget': array([19., 27.]), 'currentState': array([16.97537756, 12.25316128,  1.52581772]), 'targetState': array([19, 27], dtype=int32), 'currentDistance': 14.885172097929312}
done in step count: 33
reward sum = 0.5010007592057614
running average episode reward sum: 0.48305911383151146
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 27.]), 'previousTarget': array([19., 27.]), 'currentState': array([19.19098217, 26.15919845,  1.26523121]), 'targetState': array([19, 27], dtype=int32), 'currentDistance': 0.8622189013270234}
episode index:2908
target Thresh 31.99999999999399
target distance 7.0
model initialize at round 2908
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([6.01087556, 3.99338084, 5.94811064]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 7.079165307263191}
done in step count: 18
reward sum = 0.702428331819564
running average episode reward sum: 0.4831345243567738
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.46611888, 10.20416224,  1.84615121]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.9222930960059915}
episode index:2909
target Thresh 31.99999999999405
target distance 13.0
model initialize at round 2909
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  7.]), 'previousTarget': array([26.,  7.]), 'currentState': array([13.11318198, 18.05820658,  0.24415928]), 'targetState': array([26,  7], dtype=int32), 'currentDistance': 16.98098970717897}
done in step count: 45
reward sum = 0.4283110892072028
running average episode reward sum: 0.4831156846883375
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  7.]), 'previousTarget': array([26.,  7.]), 'currentState': array([25.14073265,  7.41787365,  5.41752829]), 'targetState': array([26,  7], dtype=int32), 'currentDistance': 0.9554887532323234}
episode index:2910
target Thresh 31.99999999999411
target distance 17.0
model initialize at round 2910
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([5.99754581, 1.99732326, 4.2048485 ]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 17.723154997824004}
done in step count: 46
reward sum = 0.3698002712784003
running average episode reward sum: 0.48307675806057726
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([22.11949551,  6.66723482,  0.25038443]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 0.9412867941231943}
episode index:2911
target Thresh 31.999999999994166
target distance 3.0
model initialize at round 2911
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 10.]), 'previousTarget': array([26., 10.]), 'currentState': array([23.4512571 , 10.784683  ,  5.84627842]), 'targetState': array([26, 10], dtype=int32), 'currentDistance': 2.66679916302669}
done in step count: 4
reward sum = 0.9157583986741458
running average episode reward sum: 0.48322534378880994
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 10.]), 'previousTarget': array([26., 10.]), 'currentState': array([25.19679464, 10.14638531,  6.06738259]), 'targetState': array([26, 10], dtype=int32), 'currentDistance': 0.8164358557037119}
episode index:2912
target Thresh 31.999999999994223
target distance 4.0
model initialize at round 2912
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 29.]), 'previousTarget': array([22., 29.]), 'currentState': array([23.77411021, 25.4459989 ,  2.06644337]), 'targetState': array([22, 29], dtype=int32), 'currentDistance': 3.9722022707880407}
done in step count: 7
reward sum = 0.8662221515914637
running average episode reward sum: 0.48335682226728666
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 29.]), 'previousTarget': array([22., 29.]), 'currentState': array([22.29197473, 28.22695859,  2.09249172]), 'targetState': array([22, 29], dtype=int32), 'currentDistance': 0.8263427059547417}
episode index:2913
target Thresh 31.999999999994284
target distance 4.0
model initialize at round 2913
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([ 9.0367246 , 12.99877676,  6.08135878]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 4.454735219086021}
done in step count: 12
reward sum = 0.8225734147271477
running average episode reward sum: 0.4834732315303134
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([11.12656325,  9.90096392,  5.37559875]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.909809999723924}
episode index:2914
target Thresh 31.99999999999434
target distance 20.0
model initialize at round 2914
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6.06750831, 23.66306976]), 'previousTarget': array([ 6.0776773 , 23.61161351]), 'currentState': array([9.9966731 , 4.0528259 , 1.76899794]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.33773600755129785
running average episode reward sum: 0.48342323591316794
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 24.]), 'previousTarget': array([ 6., 24.]), 'currentState': array([ 6.18169586, 23.13286636,  1.5459453 ]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 0.8859650869007132}
episode index:2915
target Thresh 31.999999999994394
target distance 17.0
model initialize at round 2915
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 14.]), 'previousTarget': array([26., 14.]), 'currentState': array([ 9.        , 14.        ,  3.99883056]), 'targetState': array([26, 14], dtype=int32), 'currentDistance': 17.0}
done in step count: 49
reward sum = 0.38799261152449316
running average episode reward sum: 0.4833905093615943
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 14.]), 'previousTarget': array([26., 14.]), 'currentState': array([25.12048339, 13.94864495,  0.1139509 ]), 'targetState': array([26, 14], dtype=int32), 'currentDistance': 0.8810146441657781}
episode index:2916
target Thresh 31.99999999999445
target distance 16.0
model initialize at round 2916
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8.94846611, 7.82990784]), 'previousTarget': array([8.94846611, 7.82990784]), 'currentState': array([24.        , 21.        ,  1.49071008]), 'targetState': array([8, 7], dtype=int32), 'currentDistance': 20.0}
done in step count: 74
reward sum = 0.24314617041728953
running average episode reward sum: 0.48330814928653626
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 7.]), 'previousTarget': array([8., 7.]), 'currentState': array([8.81381853, 7.72700863, 3.73476576]), 'targetState': array([8, 7], dtype=int32), 'currentDistance': 1.0912571380251082}
episode index:2917
target Thresh 31.999999999994508
target distance 2.0
model initialize at round 2917
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([22.87626898, 11.82449588,  3.99457692]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 2.0494337581234485}
done in step count: 4
reward sum = 0.9358728050087162
running average episode reward sum: 0.483463243411184
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([21.76161913, 11.00917467,  3.57992651]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 0.7616743832273815}
episode index:2918
target Thresh 31.99999999999456
target distance 21.0
model initialize at round 2918
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.21203488,  3.51826727]), 'previousTarget': array([25.23047895,  3.50557744]), 'currentState': array([6.002755  , 9.08635189, 1.29509997]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 70
reward sum = 0.27637276071383776
running average episode reward sum: 0.48339229771652925
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([26.21734549,  3.27707423,  5.77759317]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 0.830251890246403}
episode index:2919
target Thresh 31.999999999994614
target distance 2.0
model initialize at round 2919
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 12.]), 'previousTarget': array([23., 12.]), 'currentState': array([25.01173266, 12.01469587,  1.10072461]), 'targetState': array([23, 12], dtype=int32), 'currentDistance': 2.011786333253549}
done in step count: 9
reward sum = 0.8736022213760878
running average episode reward sum: 0.48352593125202914
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 12.]), 'previousTarget': array([23., 12.]), 'currentState': array([23.80690098, 12.54691357,  3.11129963]), 'targetState': array([23, 12], dtype=int32), 'currentDistance': 0.9747839004288197}
episode index:2920
target Thresh 31.999999999994667
target distance 12.0
model initialize at round 2920
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 14.]), 'previousTarget': array([13., 14.]), 'currentState': array([7.        , 2.        , 2.40433052]), 'targetState': array([13, 14], dtype=int32), 'currentDistance': 13.416407864998739}
done in step count: 33
reward sum = 0.518917613394377
running average episode reward sum: 0.48353804754170476
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 14.]), 'previousTarget': array([13., 14.]), 'currentState': array([12.83228607, 13.1032548 ,  0.88474311]), 'targetState': array([13, 14], dtype=int32), 'currentDistance': 0.91229376348748}
episode index:2921
target Thresh 31.99999999999472
target distance 3.0
model initialize at round 2921
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 14.]), 'previousTarget': array([18., 14.]), 'currentState': array([15.01655457, 11.97429698,  5.51163894]), 'targetState': array([18, 14], dtype=int32), 'currentDistance': 3.606164078086339}
done in step count: 9
reward sum = 0.8385546063212278
running average episode reward sum: 0.48365954533731714
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 14.]), 'previousTarget': array([18., 14.]), 'currentState': array([17.911244  , 13.10557484,  0.85151591]), 'targetState': array([18, 14], dtype=int32), 'currentDistance': 0.898818109585772}
episode index:2922
target Thresh 31.999999999994774
target distance 5.0
model initialize at round 2922
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 24.]), 'previousTarget': array([10., 24.]), 'currentState': array([ 9.05767492, 18.99599205,  0.18311947]), 'targetState': array([10, 24], dtype=int32), 'currentDistance': 5.091961518821329}
done in step count: 11
reward sum = 0.7974990487081575
running average episode reward sum: 0.48376691430870644
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 24.]), 'previousTarget': array([10., 24.]), 'currentState': array([10.55042011, 23.04829079,  1.74761592]), 'targetState': array([10, 24], dtype=int32), 'currentDistance': 1.0994147170895152}
episode index:2923
target Thresh 31.999999999994827
target distance 13.0
model initialize at round 2923
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([20.98972347,  9.97631353,  4.07016432]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 13.598638485411193}
done in step count: 32
reward sum = 0.5130811377415511
running average episode reward sum: 0.48377693969291735
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([ 8.86384473, 14.03017604,  2.83650239]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 0.8643716292087401}
episode index:2924
target Thresh 31.999999999994877
target distance 16.0
model initialize at round 2924
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  4.]), 'previousTarget': array([20.,  4.]), 'currentState': array([20.        , 20.        ,  6.05332041]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 16.0}
done in step count: 38
reward sum = 0.4464604344072223
running average episode reward sum: 0.48376418191333254
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  4.]), 'previousTarget': array([20.,  4.]), 'currentState': array([19.50199772,  4.99562688,  4.65516606]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 1.1132291518698094}
episode index:2925
target Thresh 31.99999999999493
target distance 11.0
model initialize at round 2925
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 13.]), 'previousTarget': array([ 4., 13.]), 'currentState': array([15.00136602, 12.04185737,  1.76374698]), 'targetState': array([ 4, 13], dtype=int32), 'currentDistance': 11.04301098845401}
done in step count: 26
reward sum = 0.5932092466977517
running average episode reward sum: 0.4838015862416936
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 13.]), 'previousTarget': array([ 4., 13.]), 'currentState': array([ 4.8492675 , 13.31491358,  3.13298293]), 'targetState': array([ 4, 13], dtype=int32), 'currentDistance': 0.9057736208120668}
episode index:2926
target Thresh 31.99999999999498
target distance 16.0
model initialize at round 2926
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'previousTarget': array([22., 18.]), 'currentState': array([13.94634482,  2.05539114,  2.15882057]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 17.863143994612788}
done in step count: 43
reward sum = 0.4172891050194574
running average episode reward sum: 0.4837788624694961
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'previousTarget': array([22., 18.]), 'currentState': array([21.17325285, 17.02127381,  1.07528666]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 1.2811775040831475}
episode index:2927
target Thresh 31.99999999999503
target distance 5.0
model initialize at round 2927
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 14.]), 'previousTarget': array([23., 14.]), 'currentState': array([27.00146446, 18.9978846 ,  5.12508604]), 'targetState': array([23, 14], dtype=int32), 'currentDistance': 6.402387702519673}
done in step count: 14
reward sum = 0.7594322010670792
running average episode reward sum: 0.48387300636929037
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 14.]), 'previousTarget': array([23., 14.]), 'currentState': array([23.43775721, 14.85716577,  3.8905664 ]), 'targetState': array([23, 14], dtype=int32), 'currentDistance': 0.9624783242330123}
episode index:2928
target Thresh 31.99999999999508
target distance 3.0
model initialize at round 2928
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 6.]), 'previousTarget': array([5., 6.]), 'currentState': array([2.        , 6.        , 1.26389048]), 'targetState': array([5, 6], dtype=int32), 'currentDistance': 3.0000000000000004}
done in step count: 7
reward sum = 0.8684434810987126
running average episode reward sum: 0.4840043039024858
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 6.]), 'previousTarget': array([5., 6.]), 'currentState': array([4.18228166, 6.06201143, 5.91418765]), 'targetState': array([5, 6], dtype=int32), 'currentDistance': 0.8200662744745872}
episode index:2929
target Thresh 31.99999999999513
target distance 16.0
model initialize at round 2929
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([15.03799695,  5.99564779,  0.11371874]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 16.290141656618303}
done in step count: 39
reward sum = 0.4306363275386525
running average episode reward sum: 0.48398608957608175
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([12.29895587, 21.13572955,  1.88593269]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 0.9145151846332109}
episode index:2930
target Thresh 31.999999999995175
target distance 18.0
model initialize at round 2930
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.94427191, 10.11145618]), 'previousTarget': array([26.94427191, 10.11145618]), 'currentState': array([18.        , 28.        ,  4.54689085]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.3409302692957348
running average episode reward sum: 0.48393728172201134
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 10.]), 'previousTarget': array([27., 10.]), 'currentState': array([26.46129826, 10.84662894,  5.10612665]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 1.003483993311839}
episode index:2931
target Thresh 31.999999999995225
target distance 9.0
model initialize at round 2931
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  9.]), 'previousTarget': array([20.,  9.]), 'currentState': array([11.00725703,  4.94553922,  5.09736109]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 9.864485705654417}
done in step count: 24
reward sum = 0.6273843680200923
running average episode reward sum: 0.483986206376274
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  9.]), 'previousTarget': array([20.,  9.]), 'currentState': array([19.06883594,  8.45415699,  0.5460261 ]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 1.0793567992550264}
episode index:2932
target Thresh 31.99999999999527
target distance 5.0
model initialize at round 2932
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([3.27885761, 4.25400907, 0.84811343]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 5.470744544099316}
done in step count: 9
reward sum = 0.8236024772900241
running average episode reward sum: 0.48410199780856644
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([5.3994289 , 8.07768345, 1.09804236]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 1.1006150380594901}
episode index:2933
target Thresh 31.999999999995318
target distance 15.0
model initialize at round 2933
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.55629531,  3.41290605]), 'previousTarget': array([21.62110536,  3.35363499]), 'currentState': array([ 6.9151526 , 17.03777081,  2.97526956]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.2954801070195965
running average episode reward sum: 0.48403770950223074
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  3.]), 'previousTarget': array([22.,  3.]), 'currentState': array([21.03408717,  3.70379062,  5.4521599 ]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 1.1951187540940118}
episode index:2934
target Thresh 31.999999999995364
target distance 9.0
model initialize at round 2934
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 15.]), 'previousTarget': array([21., 15.]), 'currentState': array([22.90141233,  6.42759459,  1.70085818]), 'targetState': array([21, 15], dtype=int32), 'currentDistance': 8.78074617257132}
done in step count: 17
reward sum = 0.6930689654142201
running average episode reward sum: 0.4841089296916385
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 15.]), 'previousTarget': array([21., 15.]), 'currentState': array([21.02073903, 14.19335796,  1.8677363 ]), 'targetState': array([21, 15], dtype=int32), 'currentDistance': 0.8069086020051546}
episode index:2935
target Thresh 31.99999999999541
target distance 3.0
model initialize at round 2935
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 8.]), 'previousTarget': array([6., 8.]), 'currentState': array([8.66098109, 7.32805885, 2.45436458]), 'targetState': array([6, 8], dtype=int32), 'currentDistance': 2.744508202998575}
done in step count: 6
reward sum = 0.8981530097234867
running average episode reward sum: 0.4842499528796603
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 8.]), 'previousTarget': array([6., 8.]), 'currentState': array([6.8465181 , 8.32833293, 2.91590712]), 'targetState': array([6, 8], dtype=int32), 'currentDistance': 0.9079622306364244}
episode index:2936
target Thresh 31.999999999995456
target distance 10.0
model initialize at round 2936
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 22.]), 'previousTarget': array([27., 22.]), 'currentState': array([22.99712808, 12.0019474 ,  2.32994175]), 'targetState': array([27, 22], dtype=int32), 'currentDistance': 10.769588635767708}
done in step count: 25
reward sum = 0.6055840758183878
running average episode reward sum: 0.484291265144876
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 22.]), 'previousTarget': array([27., 22.]), 'currentState': array([26.96280299, 21.12449315,  1.27050872]), 'targetState': array([27, 22], dtype=int32), 'currentDistance': 0.8762966750660914}
episode index:2937
target Thresh 31.999999999995502
target distance 14.0
model initialize at round 2937
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 13.]), 'previousTarget': array([10., 13.]), 'currentState': array([23.95945919, 25.00601884,  3.24670553]), 'targetState': array([10, 13], dtype=int32), 'currentDistance': 18.412251063121538}
done in step count: 47
reward sum = 0.3979284797182484
running average episode reward sum: 0.48426187005112975
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 13.]), 'previousTarget': array([10., 13.]), 'currentState': array([10.8314792 , 13.65513483,  3.92042645]), 'targetState': array([10, 13], dtype=int32), 'currentDistance': 1.0585647390450776}
episode index:2938
target Thresh 31.99999999999555
target distance 5.0
model initialize at round 2938
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 24.]), 'previousTarget': array([17., 24.]), 'currentState': array([15.21038455, 28.62553848,  5.14933437]), 'targetState': array([17, 24], dtype=int32), 'currentDistance': 4.959670319998686}
done in step count: 8
reward sum = 0.843388616437019
running average episode reward sum: 0.48438406356810354
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 24.]), 'previousTarget': array([17., 24.]), 'currentState': array([16.41836452, 24.98759695,  5.00205166]), 'targetState': array([17, 24], dtype=int32), 'currentDistance': 1.1461446587937536}
episode index:2939
target Thresh 31.99999999999559
target distance 10.0
model initialize at round 2939
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  5.]), 'previousTarget': array([13.,  5.]), 'currentState': array([19.75139814, 14.71991079,  4.04052031]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 11.834612059716072}
done in step count: 23
reward sum = 0.6091688702774004
running average episode reward sum: 0.48442650737990944
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  5.]), 'previousTarget': array([13.,  5.]), 'currentState': array([13.56984803,  5.9971017 ,  4.21093585]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 1.1484505076029932}
episode index:2940
target Thresh 31.999999999995634
target distance 15.0
model initialize at round 2940
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 26.]), 'previousTarget': array([ 8., 26.]), 'currentState': array([17.91171912, 11.06117903,  2.42114565]), 'targetState': array([ 8, 26], dtype=int32), 'currentDistance': 17.927926478659277}
done in step count: 50
reward sum = 0.4215704394989536
running average episode reward sum: 0.4844051350344892
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 26.]), 'previousTarget': array([ 8., 26.]), 'currentState': array([ 8.68056359, 25.10555503,  2.30160935]), 'targetState': array([ 8, 26], dtype=int32), 'currentDistance': 1.1239210780820317}
episode index:2941
target Thresh 31.99999999999568
target distance 6.0
model initialize at round 2941
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  8.]), 'previousTarget': array([20.,  8.]), 'currentState': array([17.        , 14.        ,  3.10207081]), 'targetState': array([20,  8], dtype=int32), 'currentDistance': 6.708203932499369}
done in step count: 16
reward sum = 0.7168073373425795
running average episode reward sum: 0.48448412966477744
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  8.]), 'previousTarget': array([20.,  8.]), 'currentState': array([19.08658205,  8.61988224,  5.31581781]), 'targetState': array([20,  8], dtype=int32), 'currentDistance': 1.1038959793023928}
episode index:2942
target Thresh 31.999999999995723
target distance 1.0
model initialize at round 2942
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  6.]), 'previousTarget': array([24.,  6.]), 'currentState': array([25.        ,  6.        ,  5.06616107]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 0.9999999999999999}
done in step count: 5
reward sum = 0.9438206949486977
running average episode reward sum: 0.4846402073288223
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  6.]), 'previousTarget': array([24.,  6.]), 'currentState': array([24.99449379,  5.98503771,  4.30192275]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 0.9946063398827187}
episode index:2943
target Thresh 31.999999999995765
target distance 19.0
model initialize at round 2943
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.41025423, 21.71573198]), 'previousTarget': array([23.39288577, 21.69765531]), 'currentState': array([11.02649492,  6.01087587,  0.52929398]), 'targetState': array([26, 25], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 80
reward sum = 0.24868240016083296
running average episode reward sum: 0.48456005861714835
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 25.]), 'previousTarget': array([26., 25.]), 'currentState': array([25.61670368, 24.21957361,  0.89839554]), 'targetState': array([26, 25], dtype=int32), 'currentDistance': 0.8694719142724563}
episode index:2944
target Thresh 31.999999999995808
target distance 10.0
model initialize at round 2944
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 25.]), 'previousTarget': array([14., 25.]), 'currentState': array([24.0090922 , 26.96936019,  4.76058319]), 'targetState': array([14, 25], dtype=int32), 'currentDistance': 10.200995354214434}
done in step count: 28
reward sum = 0.605201761458625
running average episode reward sum: 0.4846010235417125
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 25.]), 'previousTarget': array([14., 25.]), 'currentState': array([14.8078276 , 25.03950274,  3.4657766 ]), 'targetState': array([14, 25], dtype=int32), 'currentDistance': 0.8087928674516335}
episode index:2945
target Thresh 31.999999999995847
target distance 8.0
model initialize at round 2945
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([10.0640547 , 16.6868414 ,  4.79175365]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 11.140758902944546}
done in step count: 24
reward sum = 0.6036055546931085
running average episode reward sum: 0.48464141883402456
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.50144951, 9.8281896 , 4.17578011]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.968168177949131}
episode index:2946
target Thresh 31.99999999999589
target distance 11.0
model initialize at round 2946
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  6.]), 'previousTarget': array([20.,  6.]), 'currentState': array([ 9.07504221, 13.98856195,  6.02392946]), 'targetState': array([20,  6], dtype=int32), 'currentDistance': 13.53409859418402}
done in step count: 28
reward sum = 0.5534433457144347
running average episode reward sum: 0.4846647652632341
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  6.]), 'previousTarget': array([20.,  6.]), 'currentState': array([19.11327689,  6.69152834,  5.78205258]), 'targetState': array([20,  6], dtype=int32), 'currentDistance': 1.1244951445047753}
episode index:2947
target Thresh 31.99999999999593
target distance 11.0
model initialize at round 2947
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'previousTarget': array([10., 12.]), 'currentState': array([13.93631309, 22.96617562,  3.77152473]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 11.651247504859064}
done in step count: 27
reward sum = 0.5994345379971535
running average episode reward sum: 0.48470369666511126
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'previousTarget': array([10., 12.]), 'currentState': array([10.29911274, 12.81393209,  4.49673821]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 0.8671527416226495}
episode index:2948
target Thresh 31.99999999999597
target distance 10.0
model initialize at round 2948
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 11.]), 'previousTarget': array([24., 11.]), 'currentState': array([14.00624979,  3.23733102,  1.52153121]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 12.654409229403226}
done in step count: 30
reward sum = 0.5565957889448585
running average episode reward sum: 0.4847280751297704
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 11.]), 'previousTarget': array([24., 11.]), 'currentState': array([23.13853966, 10.63882183,  0.63646227]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 0.9341111195602728}
episode index:2949
target Thresh 31.99999999999601
target distance 12.0
model initialize at round 2949
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 21.]), 'previousTarget': array([15., 21.]), 'currentState': array([ 3.24864447, 16.97207539,  6.26057193]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 12.422501091007275}
done in step count: 33
reward sum = 0.5440589174323449
running average episode reward sum: 0.4847481872797034
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 21.]), 'previousTarget': array([15., 21.]), 'currentState': array([14.90881755, 20.01948596,  0.73482654]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 0.9847446473139746}
episode index:2950
target Thresh 31.99999999999605
target distance 21.0
model initialize at round 2950
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.12161403, 20.3829006 ]), 'previousTarget': array([16.12161403, 20.3829006 ]), 'currentState': array([24.        ,  2.        ,  0.21662396]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 91
reward sum = 0.20550234827811772
running average episode reward sum: 0.4846535597503908
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 23.]), 'previousTarget': array([15., 23.]), 'currentState': array([15.6836697 , 22.06035949,  2.00261525]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 1.162036376815603}
episode index:2951
target Thresh 31.99999999999609
target distance 14.0
model initialize at round 2951
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 14.]), 'previousTarget': array([17., 14.]), 'currentState': array([ 3.06181933, 17.78503562,  5.24491215]), 'targetState': array([17, 14], dtype=int32), 'currentDistance': 14.442969743151497}
done in step count: 30
reward sum = 0.5026317196922127
running average episode reward sum: 0.4846596499129727
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 14.]), 'previousTarget': array([17., 14.]), 'currentState': array([16.09907618, 13.81294173,  6.24362732]), 'targetState': array([17, 14], dtype=int32), 'currentDistance': 0.9201383160296738}
episode index:2952
target Thresh 31.999999999996128
target distance 17.0
model initialize at round 2952
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 26.]), 'previousTarget': array([ 3., 26.]), 'currentState': array([12.9497808 ,  9.09001628,  2.09726005]), 'targetState': array([ 3, 26], dtype=int32), 'currentDistance': 19.620032804505136}
done in step count: 57
reward sum = 0.3813675566870156
running average episode reward sum: 0.48462467121563924
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 26.]), 'previousTarget': array([ 3., 26.]), 'currentState': array([ 3.96359771, 25.07308519,  2.13960148]), 'targetState': array([ 3, 26], dtype=int32), 'currentDistance': 1.33704585864525}
episode index:2953
target Thresh 31.999999999996167
target distance 7.0
model initialize at round 2953
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 13.]), 'previousTarget': array([ 9., 13.]), 'currentState': array([2.        , 7.        , 2.36901236]), 'targetState': array([ 9, 13], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 21
reward sum = 0.6489205718670437
running average episode reward sum: 0.4846802893268956
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 13.]), 'previousTarget': array([ 9., 13.]), 'currentState': array([ 8.01249735, 12.17754767,  0.74500258]), 'targetState': array([ 9, 13], dtype=int32), 'currentDistance': 1.2851417556250024}
episode index:2954
target Thresh 31.999999999996206
target distance 13.0
model initialize at round 2954
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([19.98303251, 23.07308045,  2.05142987]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 17.729027182590087}
done in step count: 46
reward sum = 0.3761569896464982
running average episode reward sum: 0.48464356401397496
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.19146484, 11.88436083,  3.93520241]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.904849631547537}
episode index:2955
target Thresh 31.999999999996245
target distance 22.0
model initialize at round 2955
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6.06375361, 24.86678749]), 'previousTarget': array([ 6.05572809, 24.88854382]), 'currentState': array([14.99814445,  6.97330655,  4.39048719]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.1902463809266298
running average episode reward sum: 0.4844152521246175
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 29.]), 'previousTarget': array([ 4., 29.]), 'currentState': array([ 5.12458088, 27.23276098,  1.97213079]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 2.0947114111589933}
episode index:2956
target Thresh 31.99999999999628
target distance 11.0
model initialize at round 2956
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 13.]), 'previousTarget': array([11., 13.]), 'currentState': array([22.01648105, 19.00047838,  6.059703  ]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 12.544664027249395}
done in step count: 57
reward sum = 0.33305356781238893
running average episode reward sum: 0.4843640645411504
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 13.]), 'previousTarget': array([11., 13.]), 'currentState': array([11.74181505, 13.06335165,  2.94859233]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 0.7445152767277301}
episode index:2957
target Thresh 31.999999999996316
target distance 12.0
model initialize at round 2957
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([20.93620089, 25.03059123,  2.90433568]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 12.598355347185088}
done in step count: 32
reward sum = 0.5648726724893024
running average episode reward sum: 0.48439128178521673
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([ 9.7745757 , 21.60316797,  3.42032696]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 0.9817225255019232}
episode index:2958
target Thresh 31.999999999996355
target distance 19.0
model initialize at round 2958
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7.43306535, 15.64947848]), 'previousTarget': array([ 7.49385478, 15.70632169]), 'currentState': array([23.98741657, 26.8723772 ,  4.49561398]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 71
reward sum = 0.27445376606656136
running average episode reward sum: 0.48432033297963417
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 14.]), 'previousTarget': array([ 5., 14.]), 'currentState': array([ 5.79793137, 14.95497616,  3.77766591]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 1.2444572857343648}
episode index:2959
target Thresh 31.99999999999639
target distance 11.0
model initialize at round 2959
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 17.]), 'previousTarget': array([26., 17.]), 'currentState': array([25.        ,  6.        ,  0.38900515]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 11.04536101718726}
done in step count: 27
reward sum = 0.5910513493105813
running average episode reward sum: 0.4843563907554217
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 17.]), 'previousTarget': array([26., 17.]), 'currentState': array([25.84524728, 16.19725029,  1.73428465]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 0.8175301201270069}
episode index:2960
target Thresh 31.999999999996426
target distance 21.0
model initialize at round 2960
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.63424127,  3.5377166 ]), 'previousTarget': array([13.63241055,  3.54387571]), 'currentState': array([ 9.00619283, 22.99487897,  5.44490239]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.31659129336046243
running average episode reward sum: 0.48429973249895597
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.02990504,  2.72891313,  4.90318057]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.7295263261618876}
episode index:2961
target Thresh 31.99999999999646
target distance 4.0
model initialize at round 2961
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  6.]), 'previousTarget': array([27.,  6.]), 'currentState': array([23.05293708,  2.13468425,  1.02267468]), 'targetState': array([27,  6], dtype=int32), 'currentDistance': 5.524488350522347}
done in step count: 9
reward sum = 0.8310207515031552
running average episode reward sum: 0.4844167888861957
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  6.]), 'previousTarget': array([27.,  6.]), 'currentState': array([26.01172828,  5.1079897 ,  0.83985659]), 'targetState': array([27,  6], dtype=int32), 'currentDistance': 1.331301384867303}
episode index:2962
target Thresh 31.999999999996497
target distance 17.0
model initialize at round 2962
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 12.]), 'previousTarget': array([19., 12.]), 'currentState': array([14.03407219, 29.01635669,  0.2027508 ]), 'targetState': array([19, 12], dtype=int32), 'currentDistance': 17.726162413036068}
done in step count: 41
reward sum = 0.3871506772071152
running average episode reward sum: 0.4843839619838403
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 12.]), 'previousTarget': array([19., 12.]), 'currentState': array([18.75220572, 12.75540676,  5.08417284]), 'targetState': array([19, 12], dtype=int32), 'currentDistance': 0.7950103031317759}
episode index:2963
target Thresh 31.999999999996533
target distance 19.0
model initialize at round 2963
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.10111675, 24.86398076]), 'previousTarget': array([21.10111675, 24.86398076]), 'currentState': array([ 5.       , 13.       ,  5.3353014]), 'targetState': array([24, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.2441890333401026
running average episode reward sum: 0.48430292455852186
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 27.]), 'previousTarget': array([24., 27.]), 'currentState': array([23.02284948, 26.40402866,  0.73340521]), 'targetState': array([24, 27], dtype=int32), 'currentDistance': 1.1445544907197283}
episode index:2964
target Thresh 31.999999999996568
target distance 13.0
model initialize at round 2964
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  7.]), 'previousTarget': array([27.,  7.]), 'currentState': array([14.34169483,  3.16021909,  0.36768751]), 'targetState': array([27,  7], dtype=int32), 'currentDistance': 13.227872358014949}
done in step count: 28
reward sum = 0.5616399813470638
running average episode reward sum: 0.4843290078829025
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  7.]), 'previousTarget': array([27.,  7.]), 'currentState': array([26.02794956,  6.38783471,  0.29281061]), 'targetState': array([27,  7], dtype=int32), 'currentDistance': 1.1487507991102557}
episode index:2965
target Thresh 31.9999999999966
target distance 21.0
model initialize at round 2965
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.59543903,  5.94329262]), 'previousTarget': array([17.59867161,  5.94278962]), 'currentState': array([ 8.98180797, 23.99337184,  3.72444379]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.24965586584411384
running average episode reward sum: 0.4842498867965779
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'previousTarget': array([19.,  3.]), 'currentState': array([18.67858806,  3.80579611,  5.21886846]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 0.8675327105457377}
episode index:2966
target Thresh 31.999999999996636
target distance 13.0
model initialize at round 2966
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([22.        , 20.        ,  2.73620144]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 14.7648230602334}
done in step count: 44
reward sum = 0.45410678340153254
running average episode reward sum: 0.48423972734143966
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.87936836,  7.80308646,  4.39848781]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.8120959668851513}
episode index:2967
target Thresh 31.999999999996668
target distance 18.0
model initialize at round 2967
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  5.]), 'previousTarget': array([23.,  5.]), 'currentState': array([5.        , 5.        , 5.77570011]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 18.0}
done in step count: 46
reward sum = 0.41085679743606945
running average episode reward sum: 0.48421500263459827
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  5.]), 'previousTarget': array([23.,  5.]), 'currentState': array([22.33263351,  4.77198662,  0.21141959]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 0.705243319166667}
episode index:2968
target Thresh 31.9999999999967
target distance 20.0
model initialize at round 2968
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.98365487, 25.0016028 ]), 'previousTarget': array([23.9007438 , 25.00992562]), 'currentState': array([ 4.07912451, 26.9534407 ,  5.83825342]), 'targetState': array([24, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.3589374731635426
running average episode reward sum: 0.48417280744110847
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 25.]), 'previousTarget': array([24., 25.]), 'currentState': array([23.18455269, 25.2025786 ,  6.14437636]), 'targetState': array([24, 25], dtype=int32), 'currentDistance': 0.8402335408502504}
episode index:2969
target Thresh 31.999999999996735
target distance 9.0
model initialize at round 2969
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 23.]), 'previousTarget': array([11., 23.]), 'currentState': array([ 7.94483734, 14.16610225,  1.67577434]), 'targetState': array([11, 23], dtype=int32), 'currentDistance': 9.347286684082041}
done in step count: 21
reward sum = 0.6690851092608303
running average episode reward sum: 0.4842350674753912
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 23.]), 'previousTarget': array([11., 23.]), 'currentState': array([10.75644678, 22.10724129,  1.29043073]), 'targetState': array([11, 23], dtype=int32), 'currentDistance': 0.9253844021339764}
episode index:2970
target Thresh 31.999999999996767
target distance 12.0
model initialize at round 2970
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([13.        , 18.        ,  5.35271221]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 15.62049935181331}
done in step count: 38
reward sum = 0.4531887036239275
running average episode reward sum: 0.48422461767268116
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.70111846, 6.86492193, 4.16715505]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 1.1133988660128211}
episode index:2971
target Thresh 31.9999999999968
target distance 14.0
model initialize at round 2971
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([14.        , 23.        ,  2.98772502]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 14.035668847618199}
done in step count: 39
reward sum = 0.47937039023241773
running average episode reward sum: 0.48422298435254646
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.34720809,  9.73872419,  4.72836952]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.8162517311771068}
episode index:2972
target Thresh 31.99999999999683
target distance 14.0
model initialize at round 2972
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 15.]), 'previousTarget': array([ 6., 15.]), 'currentState': array([ 9.08065705, 28.7936659 ,  4.97439434]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 14.133494505948452}
done in step count: 33
reward sum = 0.5223887983190673
running average episode reward sum: 0.4842358218278127
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 15.]), 'previousTarget': array([ 6., 15.]), 'currentState': array([ 6.32060359, 15.85060136,  4.57098491]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 0.9090155904336337}
episode index:2973
target Thresh 31.999999999996863
target distance 4.0
model initialize at round 2973
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([18.        , 18.        ,  3.51130012]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 4.472135954999579}
done in step count: 13
reward sum = 0.8015205659957335
running average episode reward sum: 0.48434250802289275
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([16.53407532, 21.2643826 ,  1.89150172]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 0.9090486293957626}
episode index:2974
target Thresh 31.99999999999689
target distance 11.0
model initialize at round 2974
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 12.]), 'previousTarget': array([16., 12.]), 'currentState': array([26.86598264,  4.05777496,  2.59891507]), 'targetState': array([16, 12], dtype=int32), 'currentDistance': 13.459142517865198}
done in step count: 33
reward sum = 0.541931934598503
running average episode reward sum: 0.48436186581333834
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 12.]), 'previousTarget': array([16., 12.]), 'currentState': array([16.67601858, 11.75693282,  2.78134175]), 'targetState': array([16, 12], dtype=int32), 'currentDistance': 0.7183890109964546}
episode index:2975
target Thresh 31.999999999996923
target distance 23.0
model initialize at round 2975
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.01285711, 21.388617  ]), 'previousTarget': array([12.97452223, 21.34140113]), 'currentState': array([5.09802422, 3.02137931, 0.46723955]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 78
reward sum = 0.22867885127929735
running average episode reward sum: 0.48427595082189545
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 26.]), 'previousTarget': array([15., 26.]), 'currentState': array([14.90094759, 25.21157642,  1.19381342]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 0.7946213695646245}
episode index:2976
target Thresh 31.999999999996955
target distance 12.0
model initialize at round 2976
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([4.91580775, 7.003564  , 2.84678626]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 15.671803025590501}
done in step count: 37
reward sum = 0.43810666330593195
running average episode reward sum: 0.4842604421596462
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([14.31142345, 18.05996667,  0.97303729]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 1.1652468928567905}
episode index:2977
target Thresh 31.999999999996984
target distance 6.0
model initialize at round 2977
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 15.]), 'previousTarget': array([18., 15.]), 'currentState': array([19.84947243,  9.47665394,  1.91845444]), 'targetState': array([18, 15], dtype=int32), 'currentDistance': 5.824766092050249}
done in step count: 10
reward sum = 0.8016116731234297
running average episode reward sum: 0.4843670073815951
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 15.]), 'previousTarget': array([18., 15.]), 'currentState': array([18.42376855, 14.18091087,  1.94101101]), 'targetState': array([18, 15], dtype=int32), 'currentDistance': 0.9222184064308613}
episode index:2978
target Thresh 31.999999999997016
target distance 19.0
model initialize at round 2978
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 8.60954224, 28.86639986,  3.70324922]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 18.876243939047278}
done in step count: 55
reward sum = 0.3619151959894482
running average episode reward sum: 0.4843259023760925
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 7.86202739, 10.81704387,  4.69155373]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.8286115628900269}
episode index:2979
target Thresh 31.999999999997044
target distance 8.0
model initialize at round 2979
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  4.]), 'previousTarget': array([19.,  4.]), 'currentState': array([16.        , 12.        ,  0.28796434]), 'targetState': array([19,  4], dtype=int32), 'currentDistance': 8.544003745317532}
done in step count: 21
reward sum = 0.6633212324849934
running average episode reward sum: 0.48438596792310895
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  4.]), 'previousTarget': array([19.,  4.]), 'currentState': array([18.65693146,  4.81109633,  5.27258567]), 'targetState': array([19,  4], dtype=int32), 'currentDistance': 0.8806663835606894}
episode index:2980
target Thresh 31.999999999997073
target distance 17.0
model initialize at round 2980
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  5.]), 'previousTarget': array([17.,  5.]), 'currentState': array([16.94116864, 21.88362509,  4.3597054 ]), 'targetState': array([17,  5], dtype=int32), 'currentDistance': 16.88372758503851}
done in step count: 41
reward sum = 0.44556209864044993
running average episode reward sum: 0.48437294414944826
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  5.]), 'previousTarget': array([17.,  5.]), 'currentState': array([17.06389593,  5.88351877,  4.74669544]), 'targetState': array([17,  5], dtype=int32), 'currentDistance': 0.8858262256775996}
episode index:2981
target Thresh 31.999999999997105
target distance 7.0
model initialize at round 2981
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 16.]), 'previousTarget': array([12., 16.]), 'currentState': array([9.98323312, 9.1695514 , 1.48165643]), 'targetState': array([12, 16], dtype=int32), 'currentDistance': 7.121964381758848}
done in step count: 14
reward sum = 0.7513048226240284
running average episode reward sum: 0.4844624585285477
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 16.]), 'previousTarget': array([12., 16.]), 'currentState': array([11.69144913, 15.26414597,  1.43836359]), 'targetState': array([12, 16], dtype=int32), 'currentDistance': 0.7979253061869034}
episode index:2982
target Thresh 31.999999999997133
target distance 2.0
model initialize at round 2982
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 16.]), 'previousTarget': array([11., 16.]), 'currentState': array([ 9.        , 18.        ,  3.11065698]), 'targetState': array([11, 16], dtype=int32), 'currentDistance': 2.8284271247461903}
done in step count: 11
reward sum = 0.8289184077752626
running average episode reward sum: 0.48457793152527806
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 16.]), 'previousTarget': array([11., 16.]), 'currentState': array([10.19134787, 16.36281477,  5.61088238]), 'targetState': array([11, 16], dtype=int32), 'currentDistance': 0.8863141791203755}
episode index:2983
target Thresh 31.99999999999716
target distance 12.0
model initialize at round 2983
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 12.]), 'previousTarget': array([17., 12.]), 'currentState': array([ 7.01406972, 23.94006701,  5.03031577]), 'targetState': array([17, 12], dtype=int32), 'currentDistance': 15.565474736804052}
done in step count: 37
reward sum = 0.4884410875828742
running average episode reward sum: 0.4845792261486218
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 12.]), 'previousTarget': array([17., 12.]), 'currentState': array([16.54808089, 12.83943282,  5.59104933]), 'targetState': array([17, 12], dtype=int32), 'currentDistance': 0.9533511091981186}
episode index:2984
target Thresh 31.99999999999719
target distance 13.0
model initialize at round 2984
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 22.]), 'previousTarget': array([ 8., 22.]), 'currentState': array([21.        , 27.        ,  1.22757435]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 13.92838827718412}
done in step count: 33
reward sum = 0.468794235160262
running average episode reward sum: 0.48457393804443805
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 22.]), 'previousTarget': array([ 8., 22.]), 'currentState': array([ 8.85616371, 22.0593086 ,  3.69073611]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 0.8582154824075197}
episode index:2985
target Thresh 31.99999999999722
target distance 7.0
model initialize at round 2985
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  2.]), 'previousTarget': array([12.,  2.]), 'currentState': array([19.        ,  7.        ,  2.30097234]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 8.602325267042627}
done in step count: 20
reward sum = 0.6745306295945991
running average episode reward sum: 0.4846375538152184
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  2.]), 'previousTarget': array([12.,  2.]), 'currentState': array([12.87154237,  2.66996955,  3.86141697]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 1.0992930878437563}
episode index:2986
target Thresh 31.999999999997243
target distance 23.0
model initialize at round 2986
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.35983502,  6.94441919]), 'previousTarget': array([23.35234545,  6.95156206]), 'currentState': array([ 4.00076646, 11.96701458,  4.98812103]), 'targetState': array([27,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.2555726646720467
running average episode reward sum: 0.4845608665406475
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  6.]), 'previousTarget': array([27.,  6.]), 'currentState': array([26.12787204,  6.71783607,  6.28284676]), 'targetState': array([27,  6], dtype=int32), 'currentDistance': 1.1295555775658774}
episode index:2987
target Thresh 31.99999999999727
target distance 11.0
model initialize at round 2987
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 20.]), 'previousTarget': array([ 7., 20.]), 'currentState': array([18.        , 16.        ,  1.78817165]), 'targetState': array([ 7, 20], dtype=int32), 'currentDistance': 11.704699910719626}
done in step count: 26
reward sum = 0.5782117728892153
running average episode reward sum: 0.4845922088787829
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 20.]), 'previousTarget': array([ 7., 20.]), 'currentState': array([ 7.77777677, 19.72436355,  2.88051445]), 'targetState': array([ 7, 20], dtype=int32), 'currentDistance': 0.8251740147375116}
episode index:2988
target Thresh 31.9999999999973
target distance 19.0
model initialize at round 2988
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  2.]), 'previousTarget': array([21.,  2.]), 'currentState': array([1.93105389, 1.99178803, 3.50509602]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 19.068947876879555}
done in step count: 48
reward sum = 0.3148838254405602
running average episode reward sum: 0.4845354312329354
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  2.]), 'previousTarget': array([21.,  2.]), 'currentState': array([20.32281616,  2.03537255,  0.10829883]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 0.6781070543118587}
episode index:2989
target Thresh 31.999999999997325
target distance 15.0
model initialize at round 2989
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([ 1.98951223, 28.03757759,  1.59046698]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 16.179217727066057}
done in step count: 42
reward sum = 0.41950507486237776
running average episode reward sum: 0.4845136819498683
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([16.17846023, 22.56835719,  5.96001675]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 0.9989782201415904}
episode index:2990
target Thresh 31.999999999997353
target distance 10.0
model initialize at round 2990
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  6.]), 'previousTarget': array([19.,  6.]), 'currentState': array([26.0409728 , 15.70695428,  4.73047465]), 'targetState': array([19,  6], dtype=int32), 'currentDistance': 11.991674590413949}
done in step count: 25
reward sum = 0.5920587281170095
running average episode reward sum: 0.4845496381672429
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  6.]), 'previousTarget': array([19.,  6.]), 'currentState': array([19.51939399,  6.94075603,  4.00198135]), 'targetState': array([19,  6], dtype=int32), 'currentDistance': 1.074612501331176}
episode index:2991
target Thresh 31.999999999997378
target distance 25.0
model initialize at round 2991
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.78107393, 12.15457199]), 'previousTarget': array([17.78107393, 12.15457199]), 'currentState': array([ 7.        , 29.        ,  1.13593936]), 'targetState': array([23,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 84
reward sum = 0.11431440176309626
running average episode reward sum: 0.48442589644384576
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  4.]), 'previousTarget': array([23.,  4.]), 'currentState': array([22.06100324,  4.8888426 ,  5.0636109 ]), 'targetState': array([23,  4], dtype=int32), 'currentDistance': 1.2929640738706845}
episode index:2992
target Thresh 31.999999999997407
target distance 9.0
model initialize at round 2992
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([23.94548467, 11.91628643,  3.88266182]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 8.992289855478047}
done in step count: 21
reward sum = 0.6724947682794485
running average episode reward sum: 0.4844887326856886
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.94034054, 11.50977653,  3.17700773]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.0696319201792024}
episode index:2993
target Thresh 31.99999999999743
target distance 4.0
model initialize at round 2993
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 15.]), 'previousTarget': array([ 9., 15.]), 'currentState': array([ 4.99973332, 11.99390043,  4.90359774]), 'targetState': array([ 9, 15], dtype=int32), 'currentDistance': 5.003875311077707}
done in step count: 17
reward sum = 0.7471706320252512
running average episode reward sum: 0.4845764687910124
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 15.]), 'previousTarget': array([ 9., 15.]), 'currentState': array([ 8.98390885, 14.10555144,  1.03718431]), 'targetState': array([ 9, 15], dtype=int32), 'currentDistance': 0.894593283816338}
episode index:2994
target Thresh 31.999999999997456
target distance 18.0
model initialize at round 2994
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 12.]), 'previousTarget': array([ 7., 12.]), 'currentState': array([25.        , 17.        ,  4.45780522]), 'targetState': array([ 7, 12], dtype=int32), 'currentDistance': 18.681541692269406}
done in step count: 46
reward sum = 0.37975878293200926
running average episode reward sum: 0.4845414712331296
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 12.]), 'previousTarget': array([ 7., 12.]), 'currentState': array([ 7.97184781, 12.21205997,  3.44634461]), 'targetState': array([ 7, 12], dtype=int32), 'currentDistance': 0.9947148322106356}
episode index:2995
target Thresh 31.99999999999748
target distance 6.0
model initialize at round 2995
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 24.]), 'previousTarget': array([26., 24.]), 'currentState': array([20.        , 18.        ,  5.31880856]), 'targetState': array([26, 24], dtype=int32), 'currentDistance': 8.48528137423857}
done in step count: 23
reward sum = 0.6451702467130733
running average episode reward sum: 0.484595085644171
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 24.]), 'previousTarget': array([26., 24.]), 'currentState': array([25.11135603, 23.98051587,  0.93539098]), 'targetState': array([26, 24], dtype=int32), 'currentDistance': 0.8888575501834183}
episode index:2996
target Thresh 31.999999999997506
target distance 6.0
model initialize at round 2996
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  7.]), 'previousTarget': array([18.,  7.]), 'currentState': array([20.72932134, 12.57974873,  4.17882087]), 'targetState': array([18,  7], dtype=int32), 'currentDistance': 6.211504716273874}
done in step count: 11
reward sum = 0.7912406321978958
running average episode reward sum: 0.4846974031438553
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  7.]), 'previousTarget': array([18.,  7.]), 'currentState': array([18.52906824,  7.88871139,  4.46273717]), 'targetState': array([18,  7], dtype=int32), 'currentDistance': 1.0342732383084698}
episode index:2997
target Thresh 31.99999999999753
target distance 20.0
model initialize at round 2997
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.20729355,  4.7615699 ]), 'previousTarget': array([16.20729355,  4.7615699 ]), 'currentState': array([ 8.        , 23.        ,  0.63762218]), 'targetState': array([17,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.26883622287119696
running average episode reward sum: 0.4846254014159458
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  3.]), 'previousTarget': array([17.,  3.]), 'currentState': array([16.72432047,  3.69494342,  5.23825098]), 'targetState': array([17,  3], dtype=int32), 'currentDistance': 0.7476266188703314}
episode index:2998
target Thresh 31.999999999997556
target distance 11.0
model initialize at round 2998
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 17.]), 'previousTarget': array([15., 17.]), 'currentState': array([25.96655213, 26.06051374,  2.31656051]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 14.225265369069605}
done in step count: 35
reward sum = 0.49448590193141173
running average episode reward sum: 0.48462868934542747
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 17.]), 'previousTarget': array([15., 17.]), 'currentState': array([15.66498235, 17.88282022,  3.91976247]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 1.1052479698509177}
episode index:2999
target Thresh 31.99999999999758
target distance 16.0
model initialize at round 2999
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 19.]), 'previousTarget': array([21., 19.]), 'currentState': array([17.        ,  3.        ,  5.98390818]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 16.49242250247064}
done in step count: 41
reward sum = 0.4202888773968476
running average episode reward sum: 0.4846072427414446
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 19.]), 'previousTarget': array([21., 19.]), 'currentState': array([20.83087754, 18.06664001,  1.73205494]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 0.9485585287648965}
episode index:3000
target Thresh 31.999999999997605
target distance 19.0
model initialize at round 3000
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 28.]), 'previousTarget': array([ 4., 28.]), 'currentState': array([9.98495907, 9.14066227, 1.71712177]), 'targetState': array([ 4, 28], dtype=int32), 'currentDistance': 19.786216282659453}
done in step count: 50
reward sum = 0.38128711887048905
running average episode reward sum: 0.4845728141763426
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 28.]), 'previousTarget': array([ 4., 28.]), 'currentState': array([ 4.34327717, 27.00736731,  2.02371747]), 'targetState': array([ 4, 28], dtype=int32), 'currentDistance': 1.0503137076111586}
episode index:3001
target Thresh 31.999999999997627
target distance 11.0
model initialize at round 3001
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 15.]), 'previousTarget': array([18., 15.]), 'currentState': array([ 6.95524738, 14.9305401 ,  4.39253521]), 'targetState': array([18, 15], dtype=int32), 'currentDistance': 11.04497103258963}
done in step count: 26
reward sum = 0.5678459320887549
running average episode reward sum: 0.4846005533895047
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 15.]), 'previousTarget': array([18., 15.]), 'currentState': array([17.27293542, 15.2429685 ,  0.31184675]), 'targetState': array([18, 15], dtype=int32), 'currentDistance': 0.7665876300429628}
episode index:3002
target Thresh 31.99999999999765
target distance 22.0
model initialize at round 3002
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.58654742, 19.60720484]), 'previousTarget': array([21.57704261, 19.55791146]), 'currentState': array([11.95609204,  2.07853321,  1.8625921 ]), 'targetState': array([24, 24], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.19655035924440178
running average episode reward sum: 0.48450463257893356
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 24.]), 'previousTarget': array([24., 24.]), 'currentState': array([24.05769162, 23.00213937,  1.01692181]), 'targetState': array([24, 24], dtype=int32), 'currentDistance': 0.99952697166935}
episode index:3003
target Thresh 31.999999999997677
target distance 9.0
model initialize at round 3003
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 15.]), 'previousTarget': array([23., 15.]), 'currentState': array([13.94411933,  8.28595994,  1.54762834]), 'targetState': array([23, 15], dtype=int32), 'currentDistance': 11.273300695008318}
done in step count: 24
reward sum = 0.6041759488026289
running average episode reward sum: 0.484544469901245
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 15.]), 'previousTarget': array([23., 15.]), 'currentState': array([22.17469485, 14.29231297,  0.6992993 ]), 'targetState': array([23, 15], dtype=int32), 'currentDistance': 1.087175017649681}
episode index:3004
target Thresh 31.999999999997698
target distance 22.0
model initialize at round 3004
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9.11398674, 13.41833709]), 'previousTarget': array([ 9.19048507, 13.53288912]), 'currentState': array([22.95690824, 27.85348601,  4.29557386]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.033736775628306334
running average episode reward sum: 0.48437199694100225
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3.79789311, 8.29985869]), 'previousTarget': array([3.84225305, 8.34882533]), 'currentState': array([16.11557454, 24.05659452,  3.51970076]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 20.0}
episode index:3005
target Thresh 31.999999999997723
target distance 8.0
model initialize at round 3005
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  8.]), 'previousTarget': array([11.,  8.]), 'currentState': array([19.        ,  2.        ,  6.17995021]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 10.0}
done in step count: 26
reward sum = 0.5919850125002699
running average episode reward sum: 0.48440779634737596
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  8.]), 'previousTarget': array([11.,  8.]), 'currentState': array([11.92999436,  7.32194842,  2.76602957]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 1.1509315601676793}
episode index:3006
target Thresh 31.999999999997744
target distance 16.0
model initialize at round 3006
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 27.]), 'previousTarget': array([ 3., 27.]), 'currentState': array([19.        , 15.        ,  4.05003428]), 'targetState': array([ 3, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.3153578236140788
running average episode reward sum: 0.48435157753369673
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 27.]), 'previousTarget': array([ 3., 27.]), 'currentState': array([ 3.81922509, 26.71735141,  2.4020205 ]), 'targetState': array([ 3, 27], dtype=int32), 'currentDistance': 0.8666140796934478}
episode index:3007
target Thresh 31.999999999997765
target distance 10.0
model initialize at round 3007
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 19.]), 'previousTarget': array([ 9., 19.]), 'currentState': array([ 4.01811115, 29.00709944,  0.14710429]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 11.178607057229481}
done in step count: 24
reward sum = 0.5980780236337557
running average episode reward sum: 0.4843893855277459
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 19.]), 'previousTarget': array([ 9., 19.]), 'currentState': array([ 8.26705712, 19.9543455 ,  5.14957328]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 1.2033206524869946}
episode index:3008
target Thresh 31.99999999999779
target distance 8.0
model initialize at round 3008
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 8.]), 'previousTarget': array([5., 8.]), 'currentState': array([13.0145433 , 16.0208182 ,  1.20569229]), 'targetState': array([5, 8], dtype=int32), 'currentDistance': 11.338713722333276}
done in step count: 28
reward sum = 0.5471218805051996
running average episode reward sum: 0.48441023381454473
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 8.]), 'previousTarget': array([5., 8.]), 'currentState': array([5.63499396, 8.9470134 , 3.93643368]), 'targetState': array([5, 8], dtype=int32), 'currentDistance': 1.140198101346588}
episode index:3009
target Thresh 31.99999999999781
target distance 19.0
model initialize at round 3009
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.31424566, 25.69581618]), 'previousTarget': array([24.31492866, 25.69836445]), 'currentState': array([15.0063137 ,  7.99377103,  5.74214947]), 'targetState': array([25, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 73
reward sum = 0.25583657748712896
running average episode reward sum: 0.48433429572274156
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 27.]), 'previousTarget': array([25., 27.]), 'currentState': array([25.08738307, 26.02094409,  1.02601566]), 'targetState': array([25, 27], dtype=int32), 'currentDistance': 0.982947752443956}
episode index:3010
target Thresh 31.999999999997833
target distance 21.0
model initialize at round 3010
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.17877864,  9.23290339]), 'previousTarget': array([18.17157288,  9.20101013]), 'currentState': array([21.04888506, 29.0258943 ,  0.23462254]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.2591339484224405
running average episode reward sum: 0.4842595031796329
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([18.06091437,  8.9004784 ,  4.87278052]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 0.9025363744702478}
episode index:3011
target Thresh 31.999999999997854
target distance 12.0
model initialize at round 3011
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 27.]), 'previousTarget': array([22., 27.]), 'currentState': array([10.08536523, 21.3890354 ,  1.22279513]), 'targetState': array([22, 27], dtype=int32), 'currentDistance': 13.169716986856477}
done in step count: 29
reward sum = 0.5460659944535613
running average episode reward sum: 0.4842800232630572
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 27.]), 'previousTarget': array([22., 27.]), 'currentState': array([21.151074  , 26.55618438,  0.59350058]), 'targetState': array([22, 27], dtype=int32), 'currentDistance': 0.9579392777709786}
episode index:3012
target Thresh 31.999999999997875
target distance 17.0
model initialize at round 3012
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  5.]), 'previousTarget': array([23.,  5.]), 'currentState': array([22.99713392, 22.01096972,  2.07592437]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 17.010969966110427}
done in step count: 44
reward sum = 0.37224182290367036
running average episode reward sum: 0.48424283833097637
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  5.]), 'previousTarget': array([23.,  5.]), 'currentState': array([23.22210797,  5.81820302,  4.74694607]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 0.8478137372379321}
episode index:3013
target Thresh 31.999999999997897
target distance 10.0
model initialize at round 3013
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([13.        , 12.        ,  5.05072233]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 10.44030650891055}
done in step count: 25
reward sum = 0.5672873040751825
running average episode reward sum: 0.4842703912393188
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([15.87488503, 21.31779083,  1.36280214]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 0.6935871331108447}
episode index:3014
target Thresh 31.999999999997918
target distance 12.0
model initialize at round 3014
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([5.97201534, 2.49876843, 1.55285011]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 11.67865786343335}
done in step count: 22
reward sum = 0.6001815574379781
running average episode reward sum: 0.48430883607056213
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([ 7.8963671 , 13.08653317,  1.52882062]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 0.9193266155431795}
episode index:3015
target Thresh 31.99999999999794
target distance 5.0
model initialize at round 3015
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 13.]), 'previousTarget': array([17., 13.]), 'currentState': array([12.       , 17.       ,  2.7948018]), 'targetState': array([17, 13], dtype=int32), 'currentDistance': 6.4031242374328485}
done in step count: 18
reward sum = 0.6873793942657314
running average episode reward sum: 0.4843761671574968
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 13.]), 'previousTarget': array([17., 13.]), 'currentState': array([16.16813113, 12.8693473 ,  5.74123691]), 'targetState': array([17, 13], dtype=int32), 'currentDistance': 0.8420664724727634}
episode index:3016
target Thresh 31.999999999997957
target distance 6.0
model initialize at round 3016
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 22.]), 'previousTarget': array([11., 22.]), 'currentState': array([10.8380853 , 27.81562776,  4.17658922]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 5.8178812808065095}
done in step count: 11
reward sum = 0.79833361585382
running average episode reward sum: 0.48448022995123113
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 22.]), 'previousTarget': array([11., 22.]), 'currentState': array([10.02749931, 22.94411358,  4.89561737]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 1.3553995893013346}
episode index:3017
target Thresh 31.99999999999798
target distance 12.0
model initialize at round 3017
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 25.]), 'previousTarget': array([12., 25.]), 'currentState': array([24.        , 29.        ,  5.49948645]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 12.649110640673518}
done in step count: 33
reward sum = 0.5176417478814406
running average episode reward sum: 0.4844912178630702
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 25.]), 'previousTarget': array([12., 25.]), 'currentState': array([12.96055085, 25.02721519,  3.77032012]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 0.9609363125885413}
episode index:3018
target Thresh 31.999999999998
target distance 7.0
model initialize at round 3018
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 26.]), 'previousTarget': array([14., 26.]), 'currentState': array([ 7.        , 25.        ,  4.27876663]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 7.0710678118654755}
done in step count: 17
reward sum = 0.6892498408084822
running average episode reward sum: 0.4845590411896503
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 26.]), 'previousTarget': array([14., 26.]), 'currentState': array([13.11383844, 26.23631286,  0.61523708]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 0.9171292562542753}
episode index:3019
target Thresh 31.999999999998018
target distance 19.0
model initialize at round 3019
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'previousTarget': array([19.,  3.]), 'currentState': array([23.96729845, 21.95417702,  4.26014975]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 19.59425630921918}
done in step count: 53
reward sum = 0.36265618822618584
running average episode reward sum: 0.4845186760065498
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'previousTarget': array([19.,  3.]), 'currentState': array([19.11148299,  3.76586679,  4.35838441]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 0.7739382346404148}
episode index:3020
target Thresh 31.99999999999804
target distance 6.0
model initialize at round 3020
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([ 8.14505446, 14.53769065,  4.90034336]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 5.603298102765285}
done in step count: 10
reward sum = 0.807140007668182
running average episode reward sum: 0.48462546890018166
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([8.33924395, 9.93298516, 5.01629389]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 1.1432671875522669}
episode index:3021
target Thresh 31.999999999998057
target distance 18.0
model initialize at round 3021
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4.19631201, 24.36442559]), 'previousTarget': array([ 4.19631201, 24.36442559]), 'currentState': array([17.       ,  9.       ,  0.1142165]), 'targetState': array([ 2, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 89
reward sum = 0.2028433915961196
running average episode reward sum: 0.4845322253272816
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 27.]), 'previousTarget': array([ 2., 27.]), 'currentState': array([ 2.71213037, 26.15896341,  2.45071432]), 'targetState': array([ 2, 27], dtype=int32), 'currentDistance': 1.1020309475937775}
episode index:3022
target Thresh 31.999999999998078
target distance 7.0
model initialize at round 3022
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  7.]), 'previousTarget': array([19.,  7.]), 'currentState': array([25.5767947 ,  5.13472519,  2.90089744]), 'targetState': array([19,  7], dtype=int32), 'currentDistance': 6.836188901383966}
done in step count: 13
reward sum = 0.7575744791143587
running average episode reward sum: 0.4846225469461327
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  7.]), 'previousTarget': array([19.,  7.]), 'currentState': array([19.82385389,  7.23280097,  3.05412791]), 'targetState': array([19,  7], dtype=int32), 'currentDistance': 0.8561142014927076}
episode index:3023
target Thresh 31.999999999998096
target distance 11.0
model initialize at round 3023
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  6.]), 'previousTarget': array([13.,  6.]), 'currentState': array([2.22275506, 5.33703115, 0.80308154]), 'targetState': array([13,  6], dtype=int32), 'currentDistance': 10.797617150814727}
done in step count: 23
reward sum = 0.6104669299751841
running average episode reward sum: 0.48466416215216085
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  6.]), 'previousTarget': array([13.,  6.]), 'currentState': array([12.19991033,  6.37780633,  0.37466983]), 'targetState': array([13,  6], dtype=int32), 'currentDistance': 0.8848056863534867}
episode index:3024
target Thresh 31.999999999998117
target distance 20.0
model initialize at round 3024
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.07336616, 12.28090007]), 'previousTarget': array([23.1565257 , 12.25304229]), 'currentState': array([ 3.93346253, 18.08297636,  2.03183812]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 63
reward sum = 0.2774517057455414
running average episode reward sum: 0.4845956621665719
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([2.31490823e+01, 1.21985009e+01, 1.46143087e-02]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 0.8737639968902596}
episode index:3025
target Thresh 31.999999999998135
target distance 4.0
model initialize at round 3025
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 23.]), 'previousTarget': array([27., 23.]), 'currentState': array([23.14865252, 20.94629037,  6.18897057]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 4.364699375628107}
done in step count: 8
reward sum = 0.840772821262162
running average episode reward sum: 0.48471336777103174
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 23.]), 'previousTarget': array([27., 23.]), 'currentState': array([26.22078679, 22.68822015,  0.84716881]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 0.8392734377082885}
episode index:3026
target Thresh 31.999999999998153
target distance 20.0
model initialize at round 3026
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8.95010909, 5.95687239]), 'previousTarget': array([9., 6.]), 'currentState': array([24.96121048, 17.94205625,  4.0572239 ]), 'targetState': array([5, 3], dtype=int32), 'currentDistance': 20.0}
done in step count: 81
reward sum = 0.22669251340294533
running average episode reward sum: 0.48462812797771554
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 3.]), 'previousTarget': array([5., 3.]), 'currentState': array([4.97798642, 3.98644654, 4.09372196]), 'targetState': array([5, 3], dtype=int32), 'currentDistance': 0.9866921330442866}
episode index:3027
target Thresh 31.99999999999817
target distance 16.0
model initialize at round 3027
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 24.]), 'previousTarget': array([18., 24.]), 'currentState': array([7.9718683 , 8.01470407, 2.42795688]), 'targetState': array([18, 24], dtype=int32), 'currentDistance': 18.870429555645533}
done in step count: 45
reward sum = 0.35546400663991956
running average episode reward sum: 0.48458547139867403
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 24.]), 'previousTarget': array([18., 24.]), 'currentState': array([18.06868907, 23.06312608,  1.07496203]), 'targetState': array([18, 24], dtype=int32), 'currentDistance': 0.9393885891157221}
episode index:3028
target Thresh 31.999999999998188
target distance 17.0
model initialize at round 3028
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 24.]), 'previousTarget': array([ 7., 24.]), 'currentState': array([1.99737667, 6.95671748, 4.90435362]), 'targetState': array([ 7, 24], dtype=int32), 'currentDistance': 17.762311759717168}
done in step count: 43
reward sum = 0.35491437639791407
running average episode reward sum: 0.4845426615290798
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 24.]), 'previousTarget': array([ 7., 24.]), 'currentState': array([ 6.68068534, 23.20496658,  1.53328615]), 'targetState': array([ 7, 24], dtype=int32), 'currentDistance': 0.8567613366215034}
episode index:3029
target Thresh 31.999999999998206
target distance 18.0
model initialize at round 3029
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.69949973, 12.87078981]), 'previousTarget': array([20.64100589, 12.90599608]), 'currentState': array([ 4.08087821, 23.99829711,  6.09053024]), 'targetState': array([22, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 72
reward sum = 0.2785244688183452
running average episode reward sum: 0.48447466872620504
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 12.]), 'previousTarget': array([22., 12.]), 'currentState': array([21.15865487, 12.30970068,  5.80611227]), 'targetState': array([22, 12], dtype=int32), 'currentDistance': 0.8965356352304568}
episode index:3030
target Thresh 31.999999999998224
target distance 5.0
model initialize at round 3030
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 18.]), 'previousTarget': array([14., 18.]), 'currentState': array([15.00985778, 12.99415175,  6.0002346 ]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 5.106694569627659}
done in step count: 14
reward sum = 0.7610910541992646
running average episode reward sum: 0.4845659311430553
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 18.]), 'previousTarget': array([14., 18.]), 'currentState': array([14.90990964, 17.4327535 ,  1.99318829]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 1.0722425803592033}
episode index:3031
target Thresh 31.99999999999824
target distance 20.0
model initialize at round 3031
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5.22127294, 10.03319094]), 'previousTarget': array([ 5.22127294, 10.03319094]), 'currentState': array([25.        , 13.        ,  5.33191648]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.30934040839396143
running average episode reward sum: 0.4845081390841011
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 5.84692721, 10.29389448,  3.2201045 ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.8964706678031711}
episode index:3032
target Thresh 31.99999999999826
target distance 13.0
model initialize at round 3032
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([18.04674698,  8.00418413,  0.31885269]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 13.037902802373123}
done in step count: 34
reward sum = 0.5219494715860249
running average episode reward sum: 0.48452048373708556
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([17.31113536, 20.17572497,  1.86585341]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 0.8810417379306508}
episode index:3033
target Thresh 31.999999999998277
target distance 13.0
model initialize at round 3033
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 26.]), 'previousTarget': array([11., 26.]), 'currentState': array([ 3.84705902, 13.34414979,  1.75096267]), 'targetState': array([11, 26], dtype=int32), 'currentDistance': 14.537369407357302}
done in step count: 31
reward sum = 0.5033622299407946
running average episode reward sum: 0.4845266939368891
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 26.]), 'previousTarget': array([11., 26.]), 'currentState': array([10.60512261, 25.09551814,  1.15756952]), 'targetState': array([11, 26], dtype=int32), 'currentDistance': 0.9869222772340964}
episode index:3034
target Thresh 31.999999999998295
target distance 22.0
model initialize at round 3034
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.53273618, 16.56831273]), 'previousTarget': array([22.55791146, 16.57704261]), 'currentState': array([4.96656267, 7.00643277, 2.71474218]), 'targetState': array([27, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 73
reward sum = 0.20011016148115018
running average episode reward sum: 0.4844329817350915
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 19.]), 'previousTarget': array([27., 19.]), 'currentState': array([26.13183852, 18.34535475,  0.47444733]), 'targetState': array([27, 19], dtype=int32), 'currentDistance': 1.0873199898333146}
episode index:3035
target Thresh 31.999999999998312
target distance 18.0
model initialize at round 3035
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([19.94509832,  5.04997   ,  2.5207703 ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 18.05073878708227}
done in step count: 40
reward sum = 0.40648950697070857
running average episode reward sum: 0.48440730865381204
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.9102369 , 7.52816284, 3.06514627]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 1.052372179631429}
episode index:3036
target Thresh 31.99999999999833
target distance 16.0
model initialize at round 3036
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  2.]), 'previousTarget': array([10.,  2.]), 'currentState': array([11.00548709, 18.02779978,  1.27521771]), 'targetState': array([10,  2], dtype=int32), 'currentDistance': 16.05930789734703}
done in step count: 42
reward sum = 0.39570134539498564
running average episode reward sum: 0.4843781002365388
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  2.]), 'previousTarget': array([10.,  2.]), 'currentState': array([9.93669018, 2.79980028, 4.62894644]), 'targetState': array([10,  2], dtype=int32), 'currentDistance': 0.8023020801262116}
episode index:3037
target Thresh 31.999999999998344
target distance 3.0
model initialize at round 3037
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([14.00552355, 20.97792816,  5.18068078]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 3.1501162969892444}
done in step count: 7
reward sum = 0.8842477098751769
running average episode reward sum: 0.4845097228861895
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([16.00059425, 19.64526405,  6.17744342]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 1.0604949111333526}
episode index:3038
target Thresh 31.999999999998362
target distance 11.0
model initialize at round 3038
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([24.00021485,  3.9992327 ,  4.76604259]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 11.705164061239042}
done in step count: 28
reward sum = 0.5602071790602068
running average episode reward sum: 0.48453463155883636
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([13.93158678,  7.5899657 ,  2.72697976]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 1.0178320410629786}
episode index:3039
target Thresh 31.999999999998376
target distance 22.0
model initialize at round 3039
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.72300852, 24.95994298]), 'previousTarget': array([16.70226409, 24.81660336]), 'currentState': array([14.03217146,  5.14178475,  1.33129183]), 'targetState': array([17, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 70
reward sum = 0.2816797054263705
running average episode reward sum: 0.4844679029647138
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 27.]), 'previousTarget': array([17., 27.]), 'currentState': array([16.76017981, 26.25744495,  1.61416854]), 'targetState': array([17, 27], dtype=int32), 'currentDistance': 0.7803215569529866}
episode index:3040
target Thresh 31.999999999998394
target distance 7.0
model initialize at round 3040
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 8.]), 'previousTarget': array([5., 8.]), 'currentState': array([11.99653861,  3.07884228,  1.86717093]), 'targetState': array([5, 8], dtype=int32), 'currentDistance': 8.55390821692674}
done in step count: 19
reward sum = 0.6896362237976837
running average episode reward sum: 0.4845353703507161
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 8.]), 'previousTarget': array([5., 8.]), 'currentState': array([5.71191078, 7.60141104, 2.74684338]), 'targetState': array([5, 8], dtype=int32), 'currentDistance': 0.8158983466862253}
episode index:3041
target Thresh 31.999999999998412
target distance 7.0
model initialize at round 3041
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  7.]), 'previousTarget': array([17.,  7.]), 'currentState': array([23.8170847 , 10.10118806,  2.87930495]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 7.489326488286232}
done in step count: 15
reward sum = 0.7354732972748603
running average episode reward sum: 0.48461786145095415
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  7.]), 'previousTarget': array([17.,  7.]), 'currentState': array([17.69475775,  7.97682814,  3.5288818 ]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 1.1986999394030475}
episode index:3042
target Thresh 31.999999999998426
target distance 17.0
model initialize at round 3042
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 19.]), 'previousTarget': array([ 2., 19.]), 'currentState': array([2.97467517, 2.01211751, 2.44280601]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 17.015820376606705}
done in step count: 40
reward sum = 0.424755283500712
running average episode reward sum: 0.48459818922684955
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 19.]), 'previousTarget': array([ 2., 19.]), 'currentState': array([ 1.64063104, 18.21728771,  1.91093742]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 0.8612691647419756}
episode index:3043
target Thresh 31.99999999999844
target distance 9.0
model initialize at round 3043
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 12.]), 'previousTarget': array([13., 12.]), 'currentState': array([20.       , 21.       ,  2.8470962]), 'targetState': array([13, 12], dtype=int32), 'currentDistance': 11.40175425099138}
done in step count: 30
reward sum = 0.5718471643290007
running average episode reward sum: 0.48462685183365056
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 12.]), 'previousTarget': array([13., 12.]), 'currentState': array([13.26095614, 12.7790281 ,  4.28987927]), 'targetState': array([13, 12], dtype=int32), 'currentDistance': 0.8215734260732626}
episode index:3044
target Thresh 31.999999999998458
target distance 23.0
model initialize at round 3044
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.97452223, 20.34140113]), 'previousTarget': array([14.97452223, 20.34140113]), 'currentState': array([7.        , 2.        , 3.20947114]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 70
reward sum = 0.19894817470503862
running average episode reward sum: 0.4845330328920648
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 25.]), 'previousTarget': array([17., 25.]), 'currentState': array([16.57889932, 24.3347812 ,  1.18981211]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 0.7873003497474161}
episode index:3045
target Thresh 31.999999999998472
target distance 13.0
model initialize at round 3045
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 17.]), 'previousTarget': array([24., 17.]), 'currentState': array([24.0078758 ,  3.99842726,  0.0426746 ]), 'targetState': array([24, 17], dtype=int32), 'currentDistance': 13.001575123614591}
done in step count: 30
reward sum = 0.527175687068957
running average episode reward sum: 0.48454703245023184
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 17.]), 'previousTarget': array([24., 17.]), 'currentState': array([23.99843135, 16.17271786,  1.6348742 ]), 'targetState': array([24, 17], dtype=int32), 'currentDistance': 0.8272836270300034}
episode index:3046
target Thresh 31.999999999998487
target distance 7.0
model initialize at round 3046
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([16.98355639, 10.00531131,  3.0658288 ]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 7.07866825063105}
done in step count: 17
reward sum = 0.7089131048192984
running average episode reward sum: 0.48462066752485244
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([18.04982322,  3.8898003 ,  5.0859772 ]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 0.891194102401036}
episode index:3047
target Thresh 31.999999999998504
target distance 10.0
model initialize at round 3047
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([8.       , 9.       , 3.5514431]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 10.04987562112089}
done in step count: 28
reward sum = 0.5745185499765222
running average episode reward sum: 0.4846501615807749
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([17.1955518 ,  7.76449489,  6.21723143]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 0.838212125105041}
episode index:3048
target Thresh 31.99999999999852
target distance 4.0
model initialize at round 3048
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 27.]), 'previousTarget': array([10., 27.]), 'currentState': array([11.06642611, 22.95315384,  5.91026592]), 'targetState': array([10, 27], dtype=int32), 'currentDistance': 4.1850004194383255}
done in step count: 10
reward sum = 0.8068044731720813
running average episode reward sum: 0.4847558205875284
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 27.]), 'previousTarget': array([10., 27.]), 'currentState': array([10.90209281, 26.06785758,  2.05247869]), 'targetState': array([10, 27], dtype=int32), 'currentDistance': 1.2971742087964189}
episode index:3049
target Thresh 31.999999999998533
target distance 14.0
model initialize at round 3049
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 18.]), 'previousTarget': array([ 4., 18.]), 'currentState': array([9.74389829, 4.39773894, 2.12446226]), 'targetState': array([ 4, 18], dtype=int32), 'currentDistance': 14.765292866465428}
done in step count: 32
reward sum = 0.5078710003469084
running average episode reward sum: 0.4847633993349905
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 18.]), 'previousTarget': array([ 4., 18.]), 'currentState': array([ 4.4529806 , 17.02800705,  1.85051057]), 'targetState': array([ 4, 18], dtype=int32), 'currentDistance': 1.0723626770236583}
episode index:3050
target Thresh 31.999999999998547
target distance 12.0
model initialize at round 3050
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([8.        , 5.        , 1.56297523]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 12.0}
done in step count: 26
reward sum = 0.5584438364318872
running average episode reward sum: 0.48478754893744774
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([1.91121777e+01, 4.83528742e+00, 3.60843726e-03]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 0.9029720842239902}
episode index:3051
target Thresh 31.99999999999856
target distance 5.0
model initialize at round 3051
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 2.]), 'previousTarget': array([8., 2.]), 'currentState': array([11.64782419,  6.66755813,  3.93129292]), 'targetState': array([8, 2], dtype=int32), 'currentDistance': 5.923910884619327}
done in step count: 11
reward sum = 0.7961779098347479
running average episode reward sum: 0.48488957723394094
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 2.]), 'previousTarget': array([8., 2.]), 'currentState': array([8.49512363, 2.71020258, 4.13754889]), 'targetState': array([8, 2], dtype=int32), 'currentDistance': 0.8657569629731486}
episode index:3052
target Thresh 31.999999999998575
target distance 16.0
model initialize at round 3052
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([18.01911081, 11.0010531 ,  6.0932943 ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 17.90610962553635}
done in step count: 45
reward sum = 0.35689005296227005
running average episode reward sum: 0.4848476514153128
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.73950392, 3.04830601, 3.73491206]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.741079964436838}
episode index:3053
target Thresh 31.99999999999859
target distance 5.0
model initialize at round 3053
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 28.]), 'previousTarget': array([22., 28.]), 'currentState': array([17.08635008, 28.11121089,  0.78357333]), 'targetState': array([22, 28], dtype=int32), 'currentDistance': 4.914908284179442}
done in step count: 10
reward sum = 0.82413767041824
running average episode reward sum: 0.48495874834360453
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 28.]), 'previousTarget': array([22., 28.]), 'currentState': array([21.18918924, 28.3016707 ,  6.26570695]), 'targetState': array([22, 28], dtype=int32), 'currentDistance': 0.8651123047713788}
episode index:3054
target Thresh 31.999999999998604
target distance 4.0
model initialize at round 3054
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 25.]), 'previousTarget': array([23., 25.]), 'currentState': array([25.        , 21.        ,  3.27526954]), 'targetState': array([23, 25], dtype=int32), 'currentDistance': 4.47213595499958}
done in step count: 10
reward sum = 0.8208012945458145
running average episode reward sum: 0.48506868043728774
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 25.]), 'previousTarget': array([23., 25.]), 'currentState': array([23.38241327, 24.30174615,  1.76259011]), 'targetState': array([23, 25], dtype=int32), 'currentDistance': 0.7961145338177812}
episode index:3055
target Thresh 31.999999999998618
target distance 17.0
model initialize at round 3055
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 23.]), 'previousTarget': array([ 7., 23.]), 'currentState': array([23.92089769, 12.96931954,  3.27249637]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 19.670570130077603}
done in step count: 50
reward sum = 0.3614004004900705
running average episode reward sum: 0.48502821306819505
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 23.]), 'previousTarget': array([ 7., 23.]), 'currentState': array([ 7.73339245, 22.44929667,  2.80231907]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 0.9171361106191738}
episode index:3056
target Thresh 31.999999999998632
target distance 20.0
model initialize at round 3056
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.99808274,  4.0382607 ]), 'previousTarget': array([19.99875234,  4.02495322]), 'currentState': array([18.997131  , 24.01319738,  2.03735745]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.29163558954937013
running average episode reward sum: 0.48496495084264096
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  4.]), 'previousTarget': array([20.,  4.]), 'currentState': array([19.78938883,  4.8941985 ,  4.83216443]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 0.9186664337105905}
episode index:3057
target Thresh 31.999999999998646
target distance 19.0
model initialize at round 3057
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.67775637,  7.09224114]), 'previousTarget': array([13.67985983,  7.09022194]), 'currentState': array([ 2.99311303, 23.99899709,  3.53870058]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.263782506866163
running average episode reward sum: 0.4848926217242706
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.69841418,  5.74335375,  5.38833421]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.802202475319983}
episode index:3058
target Thresh 31.999999999998657
target distance 18.0
model initialize at round 3058
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.21358457,  8.29018892]), 'previousTarget': array([22.21358457,  8.29018892]), 'currentState': array([ 6.        , 20.        ,  1.25544244]), 'targetState': array([24,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.26355442246220717
running average episode reward sum: 0.4848202653335344
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  7.]), 'previousTarget': array([24.,  7.]), 'currentState': array([23.05305806,  7.43951422,  5.58590033]), 'targetState': array([24,  7], dtype=int32), 'currentDistance': 1.0439692469708646}
episode index:3059
target Thresh 31.99999999999867
target distance 9.0
model initialize at round 3059
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 23.]), 'previousTarget': array([11., 23.]), 'currentState': array([ 2.27421038, 24.79934988,  5.70459957]), 'targetState': array([11, 23], dtype=int32), 'currentDistance': 8.909380704590196}
done in step count: 20
reward sum = 0.6863346548917136
running average episode reward sum: 0.484886119709207
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 23.]), 'previousTarget': array([11., 23.]), 'currentState': array([10.24890113, 22.8401275 ,  6.16781861]), 'targetState': array([11, 23], dtype=int32), 'currentDistance': 0.7679249497194353}
episode index:3060
target Thresh 31.999999999998685
target distance 23.0
model initialize at round 3060
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9.13659391, 24.30300841]), 'previousTarget': array([ 9.17676768, 24.13347761]), 'currentState': array([15.01386113,  5.18605965,  1.48821872]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 80
reward sum = 0.2421428210694646
running average episode reward sum: 0.4848068177495076
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 28.]), 'previousTarget': array([ 8., 28.]), 'currentState': array([ 8.06594141, 27.1797366 ,  2.04361635]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 0.8229096642082756}
episode index:3061
target Thresh 31.9999999999987
target distance 7.0
model initialize at round 3061
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([13.       ,  4.       ,  0.4176523]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 7.280109889280519}
done in step count: 20
reward sum = 0.683855471677772
running average episode reward sum: 0.48487182384158084
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.8177078 , 10.95704324,  2.45907034]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.8188353466958852}
episode index:3062
target Thresh 31.99999999999871
target distance 4.0
model initialize at round 3062
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 25.]), 'previousTarget': array([ 4., 25.]), 'currentState': array([ 4.75296135, 28.56565071,  4.25713816]), 'targetState': array([ 4, 25], dtype=int32), 'currentDistance': 3.6442853608353265}
done in step count: 6
reward sum = 0.8736507482364909
running average episode reward sum: 0.4849987513389347
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 25.]), 'previousTarget': array([ 4., 25.]), 'currentState': array([ 4.33192896, 25.7035572 ,  4.73797552]), 'targetState': array([ 4, 25], dtype=int32), 'currentDistance': 0.7779264541101851}
episode index:3063
target Thresh 31.999999999998725
target distance 9.0
model initialize at round 3063
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 10.]), 'previousTarget': array([17., 10.]), 'currentState': array([8.34424524, 2.93808203, 0.05922532]), 'targetState': array([17, 10], dtype=int32), 'currentDistance': 11.171068703973168}
done in step count: 24
reward sum = 0.6112290564831698
running average episode reward sum: 0.48503994921920374
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 10.]), 'previousTarget': array([17., 10.]), 'currentState': array([16.24423529,  9.242982  ,  0.74873723]), 'targetState': array([17, 10], dtype=int32), 'currentDistance': 1.0696992793589069}
episode index:3064
target Thresh 31.999999999998735
target distance 13.0
model initialize at round 3064
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([22.        , 16.        ,  5.96975258]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 13.038404810405298}
done in step count: 28
reward sum = 0.5345875228226603
running average episode reward sum: 0.485056114822337
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([22.97276894,  3.8053721 ,  4.81435838]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 0.8058323311416422}
episode index:3065
target Thresh 31.99999999999875
target distance 15.0
model initialize at round 3065
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([19.      , 24.      ,  2.616694]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 18.027756377319943}
done in step count: 46
reward sum = 0.3930868706673589
running average episode reward sum: 0.48502611833044035
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([ 4.7057758 , 14.56933877,  3.70618342]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 0.9067889025581942}
episode index:3066
target Thresh 31.99999999999876
target distance 9.0
model initialize at round 3066
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([13.89915156, 18.70948441,  4.27233119]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 12.451908175099131}
done in step count: 25
reward sum = 0.5869019609110322
running average episode reward sum: 0.4850593351033718
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 5.81433523, 10.82352648,  3.81753709]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 1.1581613605421763}
episode index:3067
target Thresh 31.999999999998774
target distance 10.0
model initialize at round 3067
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 27.]), 'previousTarget': array([14., 27.]), 'currentState': array([24.00633095, 22.14913677,  1.75626531]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 11.120140874738276}
done in step count: 26
reward sum = 0.5942624590609434
running average episode reward sum: 0.48509492934194987
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 27.]), 'previousTarget': array([14., 27.]), 'currentState': array([14.89904462, 27.24556254,  2.72110445]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 0.9319775678173089}
episode index:3068
target Thresh 31.999999999998785
target distance 9.0
model initialize at round 3068
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([23.98549514, 17.75086056,  4.45307922]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 11.866548039574145}
done in step count: 25
reward sum = 0.5926776250656263
running average episode reward sum: 0.4851299839837627
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.66393899, 10.77636626,  3.84979877]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.0215476235094847}
episode index:3069
target Thresh 31.9999999999988
target distance 23.0
model initialize at round 3069
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.58850977,  6.17477054]), 'previousTarget': array([20.58678368,  6.16799178]), 'currentState': array([18.01776157, 26.00886376,  0.22372442]), 'targetState': array([21,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.24674067735653638
running average episode reward sum: 0.4850523327438189
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  3.]), 'previousTarget': array([21.,  3.]), 'currentState': array([20.92022687,  3.79369264,  4.87437439]), 'targetState': array([21,  3], dtype=int32), 'currentDistance': 0.7976915176176339}
episode index:3070
target Thresh 31.99999999999881
target distance 14.0
model initialize at round 3070
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  4.]), 'previousTarget': array([26.,  4.]), 'currentState': array([24.34351544, 17.77803949,  5.54013002]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 13.877258851824262}
done in step count: 31
reward sum = 0.5189280640078773
running average episode reward sum: 0.48506336359086033
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  4.]), 'previousTarget': array([26.,  4.]), 'currentState': array([26.00199918,  4.78322645,  4.82936663]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 0.7832289989852662}
episode index:3071
target Thresh 31.99999999999882
target distance 17.0
model initialize at round 3071
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([21.96181471, 21.00440871,  3.27118444]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 18.37435113048829}
done in step count: 47
reward sum = 0.3851211798135077
running average episode reward sum: 0.485030830327912
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.26315711,  4.73905246,  4.60967892]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.7845063452134697}
episode index:3072
target Thresh 31.999999999998835
target distance 20.0
model initialize at round 3072
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3.02168666, 8.14735214]), 'previousTarget': array([3.03319094, 8.22127294]), 'currentState': array([ 5.9338311 , 27.93420168,  4.15204811]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.356821860264011
running average episode reward sum: 0.48498910921822636
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.92033538, 8.95390792, 4.67837075]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.957228692546622}
episode index:3073
target Thresh 31.999999999998845
target distance 17.0
model initialize at round 3073
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([ 8.01678851, 20.98681211,  5.37767404]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 17.712141387084188}
done in step count: 50
reward sum = 0.3878219621104659
running average episode reward sum: 0.4849574998665322
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.6949566 , 4.94979187, 4.34238304]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.9975750972879963}
episode index:3074
target Thresh 31.999999999998856
target distance 4.0
model initialize at round 3074
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([26.74557079,  5.33430093,  2.26526709]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 4.59730922034266}
done in step count: 11
reward sum = 0.8322587337695156
running average episode reward sum: 0.4850704433572324
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([23.96560851,  7.85218056,  2.42041871]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 0.9768574046615408}
episode index:3075
target Thresh 31.999999999998867
target distance 10.0
model initialize at round 3075
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 8.]), 'previousTarget': array([5., 8.]), 'currentState': array([ 9.08801165, 17.8530184 ,  5.09710214]), 'targetState': array([5, 8], dtype=int32), 'currentDistance': 10.667418192836232}
done in step count: 26
reward sum = 0.6230212279865042
running average episode reward sum: 0.4851152908164747
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 8.]), 'previousTarget': array([5., 8.]), 'currentState': array([5.11035345, 8.99627746, 4.36433263]), 'targetState': array([5, 8], dtype=int32), 'currentDistance': 1.002370520409005}
episode index:3076
target Thresh 31.99999999999888
target distance 22.0
model initialize at round 3076
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.48948393,  6.43415998]), 'previousTarget': array([23.50265712,  6.43242207]), 'currentState': array([3.97891104, 2.03671238, 1.83970761]), 'targetState': array([26,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.2626270818894559
running average episode reward sum: 0.4850429839562449
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  7.]), 'previousTarget': array([26.,  7.]), 'currentState': array([25.26409316,  6.70789582,  0.24644452]), 'targetState': array([26,  7], dtype=int32), 'currentDistance': 0.7917598973674299}
episode index:3077
target Thresh 31.99999999999889
target distance 3.0
model initialize at round 3077
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([6.67978671, 5.91597841, 3.28826207]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 2.890736860400787}
done in step count: 4
reward sum = 0.9230729648362319
running average episode reward sum: 0.4851852938915535
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.98451848, 6.11095025, 2.87369822]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.3265316033505345}
episode index:3078
target Thresh 31.999999999998902
target distance 15.0
model initialize at round 3078
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5.37889464, 5.35363499]), 'previousTarget': array([5.37889464, 5.35363499]), 'currentState': array([20.        , 19.        ,  4.63388276]), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.3424827953622301
running average episode reward sum: 0.48513894686377523
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.93048772, 5.78697944, 4.0250616 ]), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.2186648575871282}
episode index:3079
target Thresh 31.999999999998913
target distance 5.0
model initialize at round 3079
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 16.]), 'previousTarget': array([18., 16.]), 'currentState': array([23.        , 15.        ,  4.28884304]), 'targetState': array([18, 16], dtype=int32), 'currentDistance': 5.099019513592786}
done in step count: 13
reward sum = 0.7922465486343901
running average episode reward sum: 0.48523865712409037
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 16.]), 'previousTarget': array([18., 16.]), 'currentState': array([18.75125314, 15.37955182,  3.15700082]), 'targetState': array([18, 16], dtype=int32), 'currentDistance': 0.97433937293234}
episode index:3080
target Thresh 31.999999999998924
target distance 17.0
model initialize at round 3080
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.21232796, 23.98898453]), 'previousTarget': array([24.23243274, 23.99675711]), 'currentState': array([10.92462464,  9.04117732,  2.41198528]), 'targetState': array([26, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.2671428847025521
running average episode reward sum: 0.48516786979126936
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 26.]), 'previousTarget': array([26., 26.]), 'currentState': array([25.64601763, 25.08484363,  0.9587384 ]), 'targetState': array([26, 26], dtype=int32), 'currentDistance': 0.9812312115471187}
episode index:3081
target Thresh 31.999999999998934
target distance 8.0
model initialize at round 3081
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([18.94780535, 16.92496017,  4.3571496 ]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 10.608413786813477}
done in step count: 22
reward sum = 0.6280730209477676
running average episode reward sum: 0.48521423745874387
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([25.0235995 ,  9.79337575,  5.65262502]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 1.2580949929752279}
episode index:3082
target Thresh 31.999999999998945
target distance 10.0
model initialize at round 3082
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 14.]), 'previousTarget': array([19., 14.]), 'currentState': array([12.13938817,  4.39421623,  1.17863108]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 11.80419741005924}
done in step count: 27
reward sum = 0.5950716637363989
running average episode reward sum: 0.48524987074654075
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 14.]), 'previousTarget': array([19., 14.]), 'currentState': array([18.7533859 , 13.17593418,  0.87926612]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 0.860176135763434}
episode index:3083
target Thresh 31.999999999998956
target distance 12.0
model initialize at round 3083
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  8.]), 'previousTarget': array([10.,  8.]), 'currentState': array([ 1.96031251, 19.79918681,  4.69206163]), 'targetState': array([10,  8], dtype=int32), 'currentDistance': 14.27786344077327}
done in step count: 31
reward sum = 0.5272242121977936
running average episode reward sum: 0.48526348110369094
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  8.]), 'previousTarget': array([10.,  8.]), 'currentState': array([9.2336066 , 8.83681375, 5.47043043]), 'targetState': array([10,  8], dtype=int32), 'currentDistance': 1.1347317300041768}
episode index:3084
target Thresh 31.999999999998966
target distance 6.0
model initialize at round 3084
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 20.]), 'previousTarget': array([16., 20.]), 'currentState': array([21.72768169, 23.07521513,  3.03751355]), 'targetState': array([16, 20], dtype=int32), 'currentDistance': 6.501021887151933}
done in step count: 13
reward sum = 0.7766709100191617
running average episode reward sum: 0.4853579405620104
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 20.]), 'previousTarget': array([16., 20.]), 'currentState': array([16.65374884, 20.94787253,  3.85028753]), 'targetState': array([16, 20], dtype=int32), 'currentDistance': 1.1514555513592994}
episode index:3085
target Thresh 31.999999999998977
target distance 4.0
model initialize at round 3085
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 15.]), 'previousTarget': array([ 4., 15.]), 'currentState': array([ 2.02818346, 19.04269979,  1.0289636 ]), 'targetState': array([ 4, 15], dtype=int32), 'currentDistance': 4.497941984453806}
done in step count: 72
reward sum = 0.4167315376224142
running average episode reward sum: 0.48533570258309283
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 15.]), 'previousTarget': array([ 4., 15.]), 'currentState': array([ 4.25887592, 15.80209611,  5.15320873]), 'targetState': array([ 4, 15], dtype=int32), 'currentDistance': 0.8428374133860022}
episode index:3086
target Thresh 31.999999999998987
target distance 6.0
model initialize at round 3086
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  6.]), 'previousTarget': array([22.,  6.]), 'currentState': array([15.98818648,  9.98833852,  4.1730144 ]), 'targetState': array([22,  6], dtype=int32), 'currentDistance': 7.214481680462012}
done in step count: 15
reward sum = 0.7178879230908425
running average episode reward sum: 0.485411035339979
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  6.]), 'previousTarget': array([22.,  6.]), 'currentState': array([21.20077187,  6.77055134,  6.09171381]), 'targetState': array([22,  6], dtype=int32), 'currentDistance': 1.1101869129111026}
episode index:3087
target Thresh 31.999999999998998
target distance 19.0
model initialize at round 3087
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 27.]), 'previousTarget': array([16., 27.]), 'currentState': array([14.97570243,  8.0031166 ,  2.7881915 ]), 'targetState': array([16, 27], dtype=int32), 'currentDistance': 19.02447803069012}
done in step count: 45
reward sum = 0.3681453858868163
running average episode reward sum: 0.4853730607125655
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 27.]), 'previousTarget': array([16., 27.]), 'currentState': array([15.5793999 , 26.03080874,  1.64512587]), 'targetState': array([16, 27], dtype=int32), 'currentDistance': 1.0565207702017845}
episode index:3088
target Thresh 31.999999999999005
target distance 13.0
model initialize at round 3088
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 24.]), 'previousTarget': array([15., 24.]), 'currentState': array([ 2.12721581, 22.22114389,  0.89404607]), 'targetState': array([15, 24], dtype=int32), 'currentDistance': 12.99511069402319}
done in step count: 29
reward sum = 0.5557908030969846
running average episode reward sum: 0.4853958570033988
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 24.]), 'previousTarget': array([15., 24.]), 'currentState': array([14.04887364, 23.36905924,  6.22581319]), 'targetState': array([15, 24], dtype=int32), 'currentDistance': 1.141370924062277}
episode index:3089
target Thresh 31.999999999999016
target distance 5.0
model initialize at round 3089
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 16.]), 'previousTarget': array([12., 16.]), 'currentState': array([ 9.        , 21.        ,  0.19564375]), 'targetState': array([12, 16], dtype=int32), 'currentDistance': 5.830951894845301}
done in step count: 13
reward sum = 0.7726441973267838
running average episode reward sum: 0.4854888176313352
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 16.]), 'previousTarget': array([12., 16.]), 'currentState': array([11.12781658, 16.74773281,  5.21279651]), 'targetState': array([12, 16], dtype=int32), 'currentDistance': 1.148829088509409}
episode index:3090
target Thresh 31.999999999999027
target distance 18.0
model initialize at round 3090
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([13.99522952, 27.99682466,  3.97060508]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 19.311978339791175}
done in step count: 43
reward sum = 0.3551345265894386
running average episode reward sum: 0.4854466454245924
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([20.75643804, 10.8765099 ,  5.19612445]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 0.90972085657158}
episode index:3091
target Thresh 31.999999999999037
target distance 14.0
model initialize at round 3091
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([26.01096521, 15.99781929,  5.85644027]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 17.209252432051215}
done in step count: 40
reward sum = 0.4083364141753181
running average episode reward sum: 0.4854217067987032
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.82823315,  2.84750293,  4.14410386]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.1850026845738775}
episode index:3092
target Thresh 31.999999999999044
target distance 19.0
model initialize at round 3092
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.23886  ,  7.5672925]), 'previousTarget': array([12.23886  ,  7.5672925]), 'currentState': array([20.        , 26.        ,  3.12387872]), 'targetState': array([12,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.31872025877966803
running average episode reward sum: 0.4853678104365891
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  7.]), 'previousTarget': array([12.,  7.]), 'currentState': array([12.4599965 ,  7.97875144,  4.09308132]), 'targetState': array([12,  7], dtype=int32), 'currentDistance': 1.081457889813867}
episode index:3093
target Thresh 31.999999999999055
target distance 12.0
model initialize at round 3093
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([ 4.1880687 , 13.10478242,  0.65171295]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 14.803591948872514}
done in step count: 37
reward sum = 0.5028488591872033
running average episode reward sum: 0.485373460420025
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([12.66152497, 24.15818843,  1.08966036]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 0.9073103439387751}
episode index:3094
target Thresh 31.999999999999066
target distance 9.0
model initialize at round 3094
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 13.]), 'previousTarget': array([17., 13.]), 'currentState': array([10.32119573, 21.6976222 ,  5.46177374]), 'targetState': array([17, 13], dtype=int32), 'currentDistance': 10.96608674195892}
done in step count: 25
reward sum = 0.6231453005948586
running average episode reward sum: 0.48541797474641424
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 13.]), 'previousTarget': array([17., 13.]), 'currentState': array([16.17874004, 13.59514635,  5.45164987]), 'targetState': array([17, 13], dtype=int32), 'currentDistance': 1.014232268854063}
episode index:3095
target Thresh 31.999999999999073
target distance 7.0
model initialize at round 3095
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([ 9.        , 15.        ,  6.03620434]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 20
reward sum = 0.6621274957919112
running average episode reward sum: 0.48547505146509823
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([15.66313911, 20.1931236 ,  0.9507191 ]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 0.8743710831970924}
episode index:3096
target Thresh 31.999999999999083
target distance 18.0
model initialize at round 3096
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 26.]), 'previousTarget': array([ 8., 26.]), 'currentState': array([7.05625644, 8.11794383, 1.24522519]), 'targetState': array([ 8, 26], dtype=int32), 'currentDistance': 17.906942364208582}
done in step count: 44
reward sum = 0.4156669186461144
running average episode reward sum: 0.48545251089912506
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 26.]), 'previousTarget': array([ 8., 26.]), 'currentState': array([ 7.98277928, 25.18183847,  1.61827399]), 'targetState': array([ 8, 26], dtype=int32), 'currentDistance': 0.8183427443043062}
episode index:3097
target Thresh 31.99999999999909
target distance 10.0
model initialize at round 3097
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([19.91830942,  6.90740066,  3.83150652]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 9.918741669494564}
done in step count: 21
reward sum = 0.6536060810714561
running average episode reward sum: 0.48550678900440986
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([10.97114993,  7.37416915,  2.91580522]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 1.040737598015521}
episode index:3098
target Thresh 31.9999999999991
target distance 17.0
model initialize at round 3098
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  4.]), 'previousTarget': array([19.,  4.]), 'currentState': array([2.        , 2.        , 5.07241023]), 'targetState': array([19,  4], dtype=int32), 'currentDistance': 17.11724276862369}
done in step count: 43
reward sum = 0.42225140716192644
running average episode reward sum: 0.4854863774581554
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  4.]), 'previousTarget': array([19.,  4.]), 'currentState': array([18.02885954,  3.54276482,  0.11141904]), 'targetState': array([19,  4], dtype=int32), 'currentDistance': 1.0733954560021626}
episode index:3099
target Thresh 31.99999999999911
target distance 19.0
model initialize at round 3099
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.76686234,  6.08589282]), 'previousTarget': array([22.76686234,  6.08589282]), 'currentState': array([ 4.        , 13.        ,  0.98053253]), 'targetState': array([23,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.3295506063484382
running average episode reward sum: 0.48543607559650714
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  6.]), 'previousTarget': array([23.,  6.]), 'currentState': array([22.23352948,  6.11812826,  6.03381491]), 'targetState': array([23,  6], dtype=int32), 'currentDistance': 0.7755200481080042}
episode index:3100
target Thresh 31.99999999999912
target distance 17.0
model initialize at round 3100
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([ 2.00026268, 24.00056157,  0.88382047]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 18.788690503033454}
done in step count: 44
reward sum = 0.34467898498400634
running average episode reward sum: 0.48539068472562275
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([9.46229186, 7.78032041, 5.05990664]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 0.9476444349260542}
episode index:3101
target Thresh 31.999999999999126
target distance 6.0
model initialize at round 3101
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([24.66757189, 15.63771073,  3.89905639]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 7.994069890274427}
done in step count: 14
reward sum = 0.7337374535356264
running average episode reward sum: 0.4854707449347814
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([19.7304137 , 10.90852211,  3.94935861]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 1.1657257860508397}
episode index:3102
target Thresh 31.999999999999137
target distance 11.0
model initialize at round 3102
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'previousTarget': array([10., 12.]), 'currentState': array([20.61615572, 16.97490565,  3.30869129]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 11.724011624079791}
done in step count: 23
reward sum = 0.6117088009137042
running average episode reward sum: 0.4855114275180811
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'previousTarget': array([10., 12.]), 'currentState': array([10.94686382, 12.92984002,  3.6608106 ]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 1.327084606224423}
episode index:3103
target Thresh 31.999999999999144
target distance 14.0
model initialize at round 3103
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  7.]), 'previousTarget': array([13.,  7.]), 'currentState': array([27.09596627, 18.9912589 ,  5.93985086]), 'targetState': array([13,  7], dtype=int32), 'currentDistance': 18.50639228020013}
done in step count: 49
reward sum = 0.3308789150454722
running average episode reward sum: 0.4854616103426711
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  7.]), 'previousTarget': array([13.,  7.]), 'currentState': array([12.87174565,  7.97601843,  4.07999192]), 'targetState': array([13,  7], dtype=int32), 'currentDistance': 0.9844090368860159}
episode index:3104
target Thresh 31.999999999999154
target distance 18.0
model initialize at round 3104
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 21.]), 'previousTarget': array([19.88854382, 20.94427191]), 'currentState': array([ 2.36436829, 12.11252107,  0.29275191]), 'targetState': array([20, 21], dtype=int32), 'currentDistance': 19.748488230952045}
done in step count: 52
reward sum = 0.3705853897065735
running average episode reward sum: 0.4854246131701635
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 21.]), 'previousTarget': array([20., 21.]), 'currentState': array([19.08113415, 20.07327338,  0.69461375]), 'targetState': array([20, 21], dtype=int32), 'currentDistance': 1.3050427854513966}
episode index:3105
target Thresh 31.99999999999916
target distance 19.0
model initialize at round 3105
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.31715769, 26.02502791]), 'previousTarget': array([23.30852571, 26.02072541]), 'currentState': array([ 6.01173167, 15.9989504 ,  0.10844484]), 'targetState': array([25, 27], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 61
reward sum = 0.3153490409142164
running average episode reward sum: 0.48536985606383515
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 27.]), 'previousTarget': array([25., 27.]), 'currentState': array([24.47335982, 26.159886  ,  0.52581101]), 'targetState': array([25, 27], dtype=int32), 'currentDistance': 0.9915348777775361}
episode index:3106
target Thresh 31.99999999999917
target distance 19.0
model initialize at round 3106
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.68269922,  6.08521351]), 'previousTarget': array([22.67985983,  6.09022194]), 'currentState': array([12.00096235, 22.99380592,  5.00478089]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.31356608210715464
running average episode reward sum: 0.48531456035287385
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  4.]), 'previousTarget': array([24.,  4.]), 'currentState': array([23.20699359,  4.7148226 ,  5.48609863]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 1.0676284548198045}
episode index:3107
target Thresh 31.99999999999918
target distance 17.0
model initialize at round 3107
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.89492234,  9.16102838]), 'previousTarget': array([14.86502556,  9.20859686]), 'currentState': array([ 3.96524995, 25.91042423,  4.59482193]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.33838989488398813
running average episode reward sum: 0.48526728729448615
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.07196725,  9.14301328,  5.56095675]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.9389875274296215}
episode index:3108
target Thresh 31.999999999999186
target distance 9.0
model initialize at round 3108
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 26.]), 'previousTarget': array([16., 26.]), 'currentState': array([16.00329917, 16.99201991,  5.34497377]), 'targetState': array([16, 26], dtype=int32), 'currentDistance': 9.00798069627409}
done in step count: 22
reward sum = 0.620143181507242
running average episode reward sum: 0.48531066969854303
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 26.]), 'previousTarget': array([16., 26.]), 'currentState': array([16.95856882, 25.10215119,  1.75391753]), 'targetState': array([16, 26], dtype=int32), 'currentDistance': 1.3133874776200645}
episode index:3109
target Thresh 31.999999999999194
target distance 15.0
model initialize at round 3109
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 25.]), 'previousTarget': array([10., 25.]), 'currentState': array([25.        , 19.        ,  3.95554855]), 'targetState': array([10, 25], dtype=int32), 'currentDistance': 16.15549442140351}
done in step count: 35
reward sum = 0.4530288394264609
running average episode reward sum: 0.4853002896888092
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 25.]), 'previousTarget': array([10., 25.]), 'currentState': array([10.9846104 , 24.41462565,  2.87032742]), 'targetState': array([10, 25], dtype=int32), 'currentDistance': 1.1454784024257154}
episode index:3110
target Thresh 31.999999999999204
target distance 3.0
model initialize at round 3110
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  4.]), 'previousTarget': array([25.,  4.]), 'currentState': array([22.        ,  3.        ,  4.54695797]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 3.16227766016838}
done in step count: 8
reward sum = 0.8445129696923384
running average episode reward sum: 0.485415755031144
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  4.]), 'previousTarget': array([25.,  4.]), 'currentState': array([24.26672476,  3.03170595,  0.35128819]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 1.2146134939023046}
episode index:3111
target Thresh 31.99999999999921
target distance 18.0
model initialize at round 3111
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.85786438, 12.85786438]), 'previousTarget': array([10.85786438, 12.85786438]), 'currentState': array([25.        , 27.        ,  4.81904745]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 20.0}
done in step count: 71
reward sum = 0.216183413478923
running average episode reward sum: 0.4853292407825733
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([7.13210286, 9.86749495, 4.03797221]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.8774956706730336}
episode index:3112
target Thresh 31.99999999999922
target distance 10.0
model initialize at round 3112
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([11.        , 20.        ,  2.43314114]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 10.049875621120892}
done in step count: 27
reward sum = 0.5963049486875429
running average episode reward sum: 0.48536488990172033
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([ 9.70634432, 10.81094204,  4.75789915]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.86247356883538}
episode index:3113
target Thresh 31.999999999999226
target distance 6.0
model initialize at round 3113
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([18.        , 27.        ,  3.12147379]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 7.211102550927978}
done in step count: 16
reward sum = 0.7246306378233964
running average episode reward sum: 0.4854417254020163
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([14.06372804, 21.78928744,  4.21897493]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 0.7918560056952205}
episode index:3114
target Thresh 31.999999999999233
target distance 8.0
model initialize at round 3114
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  2.]), 'previousTarget': array([17.,  2.]), 'currentState': array([9.        , 6.        , 4.29836226]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 8.944271909999157}
done in step count: 22
reward sum = 0.6343240117768171
running average episode reward sum: 0.4854895206785412
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  2.]), 'previousTarget': array([17.,  2.]), 'currentState': array([16.04665346,  1.7383463 ,  6.04920342]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 0.9886011758952871}
episode index:3115
target Thresh 31.99999999999924
target distance 8.0
model initialize at round 3115
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 19.]), 'previousTarget': array([12., 19.]), 'currentState': array([18.99344356, 11.03677066,  1.90629256]), 'targetState': array([12, 19], dtype=int32), 'currentDistance': 10.598173166036132}
done in step count: 25
reward sum = 0.6206268763414333
running average episode reward sum: 0.48553288953465895
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 19.]), 'previousTarget': array([12., 19.]), 'currentState': array([12.86256625, 18.8796608 ,  2.43207978]), 'targetState': array([12, 19], dtype=int32), 'currentDistance': 0.870920240295551}
episode index:3116
target Thresh 31.99999999999925
target distance 2.0
model initialize at round 3116
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([5.94698426, 7.98607925, 3.17412871]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 2.2726017999053196}
done in step count: 6
reward sum = 0.8844150456820448
running average episode reward sum: 0.4856608594275519
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([6.04079233, 9.21383474, 1.09730771]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 1.2402157811785512}
episode index:3117
target Thresh 31.999999999999257
target distance 3.0
model initialize at round 3117
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 21.]), 'previousTarget': array([15., 21.]), 'currentState': array([12.       , 19.       ,  5.4893136]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 8
reward sum = 0.8522479948432149
running average episode reward sum: 0.4857784306704691
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 21.]), 'previousTarget': array([15., 21.]), 'currentState': array([14.44574595, 20.00239325,  0.81705932]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 1.1412347557419766}
episode index:3118
target Thresh 31.999999999999265
target distance 10.0
model initialize at round 3118
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([17.        ,  6.        ,  4.76008654]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 10.44030650891055}
done in step count: 25
reward sum = 0.5877460645699222
running average episode reward sum: 0.4858111230827485
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([7.89270696, 9.13545596, 2.77733375]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.9029252650062751}
episode index:3119
target Thresh 31.99999999999927
target distance 12.0
model initialize at round 3119
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 16.]), 'previousTarget': array([18., 16.]), 'currentState': array([10.        ,  4.        ,  5.00480223]), 'targetState': array([18, 16], dtype=int32), 'currentDistance': 14.422205101855958}
done in step count: 38
reward sum = 0.4530852674555166
running average episode reward sum: 0.4858006340264577
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 16.]), 'previousTarget': array([18., 16.]), 'currentState': array([17.98426813, 15.03995328,  1.18493335]), 'targetState': array([18, 16], dtype=int32), 'currentDistance': 0.9601756082403466}
episode index:3120
target Thresh 31.99999999999928
target distance 19.0
model initialize at round 3120
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([16.        ,  2.        ,  0.98873782]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 19.4164878389476}
done in step count: 43
reward sum = 0.3630643649665938
running average episode reward sum: 0.48576130808315116
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([12.72548568, 20.35784425,  1.79486005]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 0.9688619521959909}
episode index:3121
target Thresh 31.999999999999286
target distance 13.0
model initialize at round 3121
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 13.]), 'previousTarget': array([ 6., 13.]), 'currentState': array([19.07557733,  8.06401058,  0.95128688]), 'targetState': array([ 6, 13], dtype=int32), 'currentDistance': 13.976219594515598}
done in step count: 34
reward sum = 0.4840314200411112
running average episode reward sum: 0.48576075398704543
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 13.]), 'previousTarget': array([ 6., 13.]), 'currentState': array([ 6.79075334, 12.90207847,  2.87323437]), 'targetState': array([ 6, 13], dtype=int32), 'currentDistance': 0.7967932377590001}
episode index:3122
target Thresh 31.999999999999293
target distance 10.0
model initialize at round 3122
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  2.]), 'previousTarget': array([21.,  2.]), 'currentState': array([11.41574505,  6.94541412,  6.10111248]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 10.784946167407787}
done in step count: 22
reward sum = 0.6327404168235233
running average episode reward sum: 0.4858078175998653
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  2.]), 'previousTarget': array([21.,  2.]), 'currentState': array([20.00626224,  2.20075945,  5.70927956]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 1.013814126621431}
episode index:3123
target Thresh 31.9999999999993
target distance 14.0
model initialize at round 3123
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([23.        , 21.        ,  1.15982866]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 14.142135623730951}
done in step count: 36
reward sum = 0.4568263890078892
running average episode reward sum: 0.4857985405740676
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([24.67716947,  7.78042817,  4.92493501]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 0.8445636055784355}
episode index:3124
target Thresh 31.999999999999307
target distance 14.0
model initialize at round 3124
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([12.9693398 , 22.96270888,  4.26744577]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 14.340051238982928}
done in step count: 36
reward sum = 0.4721433971646758
running average episode reward sum: 0.4857941709281766
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([26.13087954, 20.02062495,  6.22433727]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 0.8693651498633629}
episode index:3125
target Thresh 31.999999999999314
target distance 13.0
model initialize at round 3125
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([13.14482717,  8.46848608,  1.37008411]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 13.111109737558541}
done in step count: 26
reward sum = 0.5510410913596335
running average episode reward sum: 0.48581504326356734
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([17.22453485, 20.01270601,  1.2678082 ]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 1.0125044835354478}
episode index:3126
target Thresh 31.99999999999932
target distance 18.0
model initialize at round 3126
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([ 3.9697074 , 12.97954212,  3.98807764]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 19.00887593677573}
done in step count: 47
reward sum = 0.3254622171474212
running average episode reward sum: 0.4857637631784646
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([21.14237124, 18.24039077,  0.29589808]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 1.1456584505380258}
episode index:3127
target Thresh 31.99999999999933
target distance 15.0
model initialize at round 3127
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 14.]), 'previousTarget': array([20., 14.]), 'currentState': array([ 5.01809445, 10.98574007,  5.85745561]), 'targetState': array([20, 14], dtype=int32), 'currentDistance': 15.282122129238278}
done in step count: 39
reward sum = 0.47128561141534964
running average episode reward sum: 0.485759134613323
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 14.]), 'previousTarget': array([20., 14.]), 'currentState': array([19.23835399, 13.37494256,  0.32120477]), 'targetState': array([20, 14], dtype=int32), 'currentDistance': 0.9852925702377425}
episode index:3128
target Thresh 31.999999999999336
target distance 16.0
model initialize at round 3128
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 22.]), 'previousTarget': array([10., 22.]), 'currentState': array([26.        , 10.        ,  0.06097963]), 'targetState': array([10, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.2963234366450562
running average episode reward sum: 0.48569859268364307
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 22.]), 'previousTarget': array([10., 22.]), 'currentState': array([10.942573  , 21.89082212,  2.50672079]), 'targetState': array([10, 22], dtype=int32), 'currentDistance': 0.948874951684223}
episode index:3129
target Thresh 31.99999999999934
target distance 10.0
model initialize at round 3129
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([14.99462352,  8.99482461,  4.12483469]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 10.20229796838263}
done in step count: 26
reward sum = 0.596841092560433
running average episode reward sum: 0.4857341014695462
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([24.09592708,  6.81483019,  6.17580814]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 0.9228411047306899}
episode index:3130
target Thresh 31.999999999999346
target distance 22.0
model initialize at round 3130
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.52454686, 24.26673649]), 'previousTarget': array([20.52454686, 24.26673649]), 'currentState': array([ 4.       , 13.       ,  4.9395203]), 'targetState': array([26, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 72
reward sum = 0.1662089793873916
running average episode reward sum: 0.4856320493705101
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 28.]), 'previousTarget': array([26., 28.]), 'currentState': array([25.41813141, 27.22003209,  0.89839695]), 'targetState': array([26, 28], dtype=int32), 'currentDistance': 0.9730986610883602}
episode index:3131
target Thresh 31.999999999999353
target distance 13.0
model initialize at round 3131
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 16.]), 'previousTarget': array([12., 16.]), 'currentState': array([24.97826509, 20.7025459 ,  4.44644773]), 'targetState': array([12, 16], dtype=int32), 'currentDistance': 13.803959670093024}
done in step count: 32
reward sum = 0.5156095720824476
running average episode reward sum: 0.48564162073791495
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 16.]), 'previousTarget': array([12., 16.]), 'currentState': array([12.8005333 , 16.29452629,  3.50911332]), 'targetState': array([12, 16], dtype=int32), 'currentDistance': 0.8529943151562742}
episode index:3132
target Thresh 31.99999999999936
target distance 9.0
model initialize at round 3132
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 23.]), 'previousTarget': array([ 4., 23.]), 'currentState': array([12.94566584, 20.94513945,  3.71154022]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 9.178637651597102}
done in step count: 19
reward sum = 0.6668767113167696
running average episode reward sum: 0.485699467878221
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 23.]), 'previousTarget': array([ 4., 23.]), 'currentState': array([ 4.76825388, 22.93385543,  2.68313532]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 0.7710960581474429}
episode index:3133
target Thresh 31.999999999999368
target distance 12.0
model initialize at round 3133
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([21.        , 23.        ,  3.71720237]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 13.416407864998739}
done in step count: 32
reward sum = 0.5376678636304558
running average episode reward sum: 0.485716050008327
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.04649268, 11.71156446,  4.27749107]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.7130817279419067}
episode index:3134
target Thresh 31.99999999999937
target distance 18.0
model initialize at round 3134
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([13.01745592,  6.9638777 ,  5.40898454]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 18.036130742686655}
done in step count: 46
reward sum = 0.35179862705205867
running average episode reward sum: 0.48567333312700117
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([12.84557429, 24.0126383 ,  1.51177749]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 0.9993650088272981}
episode index:3135
target Thresh 31.99999999999938
target distance 8.0
model initialize at round 3135
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 18.]), 'previousTarget': array([14., 18.]), 'currentState': array([ 5.97387923, 18.99284236,  3.66154838]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 8.087295627801078}
done in step count: 21
reward sum = 0.6478337228968397
running average episode reward sum: 0.4857250424349635
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 18.]), 'previousTarget': array([14., 18.]), 'currentState': array([13.13167193, 18.16271663,  6.23078663]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 0.8834423226917147}
episode index:3136
target Thresh 31.999999999999385
target distance 5.0
model initialize at round 3136
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  2.]), 'previousTarget': array([22.,  2.]), 'currentState': array([17.04996822,  2.05410121,  0.61340627]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 4.950327421481096}
done in step count: 9
reward sum = 0.8285312465019877
running average episode reward sum: 0.48583432079137634
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  2.]), 'previousTarget': array([22.,  2.]), 'currentState': array([21.04052823,  2.19320237,  6.24050123]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 0.9787304169285702}
episode index:3137
target Thresh 31.999999999999392
target distance 5.0
model initialize at round 3137
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([5.99711168, 5.00236201, 2.20360494]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 7.071440947597789}
done in step count: 15
reward sum = 0.722664098584274
running average episode reward sum: 0.4859097923585507
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([10.11043771,  9.50632528,  0.69897617]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 1.0173670888137778}
episode index:3138
target Thresh 31.999999999999396
target distance 1.0
model initialize at round 3138
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 14.]), 'previousTarget': array([12., 14.]), 'currentState': array([10.97783197, 15.04362282,  2.02145097]), 'targetState': array([12, 14], dtype=int32), 'currentDistance': 1.4608134965616009}
done in step count: 10
reward sum = 0.8701168582695252
running average episode reward sum: 0.48603219027696765
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 14.]), 'previousTarget': array([12., 14.]), 'currentState': array([11.52642641, 14.89916454,  5.7406642 ]), 'targetState': array([12, 14], dtype=int32), 'currentDistance': 1.0162523371809487}
episode index:3139
target Thresh 31.999999999999403
target distance 4.0
model initialize at round 3139
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 13.]), 'previousTarget': array([ 7., 13.]), 'currentState': array([ 2.9367742 , 14.94633293,  4.09789896]), 'targetState': array([ 7, 13], dtype=int32), 'currentDistance': 4.505331926031611}
done in step count: 10
reward sum = 0.7960351137146258
running average episode reward sum: 0.48613091732264846
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 13.]), 'previousTarget': array([ 7., 13.]), 'currentState': array([ 6.20656935, 13.19175859,  6.20708495]), 'targetState': array([ 7, 13], dtype=int32), 'currentDistance': 0.8162741940370466}
episode index:3140
target Thresh 31.99999999999941
target distance 21.0
model initialize at round 3140
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4.03154379, 16.32083026]), 'previousTarget': array([ 4.02633404, 16.32455532]), 'currentState': array([22.9996326 ,  9.97956802,  4.45727208]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.2789078137556685
running average episode reward sum: 0.48606494371438136
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'previousTarget': array([ 2., 17.]), 'currentState': array([ 2.7588462 , 16.42670608,  2.72964903]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 0.9510591368487867}
episode index:3141
target Thresh 31.999999999999414
target distance 13.0
model initialize at round 3141
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 15.]), 'previousTarget': array([23., 15.]), 'currentState': array([10.01376363,  6.98701099,  5.77923393]), 'targetState': array([23, 15], dtype=int32), 'currentDistance': 15.259434064146026}
done in step count: 39
reward sum = 0.4639870605928743
running average episode reward sum: 0.4860579170170162
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 15.]), 'previousTarget': array([23., 15.]), 'currentState': array([22.15017083, 14.76583309,  0.55431186]), 'targetState': array([23, 15], dtype=int32), 'currentDistance': 0.8815008545185293}
episode index:3142
target Thresh 31.99999999999942
target distance 14.0
model initialize at round 3142
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'previousTarget': array([22., 18.]), 'currentState': array([26.01401685,  4.01569239,  1.0942362 ]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 14.5489927725715}
done in step count: 35
reward sum = 0.481434484245838
running average episode reward sum: 0.4860564459916356
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'previousTarget': array([22., 18.]), 'currentState': array([22.02608459, 17.14731149,  1.74885405]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 0.8530873925725012}
episode index:3143
target Thresh 31.999999999999428
target distance 8.0
model initialize at round 3143
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 5.]), 'previousTarget': array([7., 5.]), 'currentState': array([ 8.        , 13.        ,  3.45178962]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 18
reward sum = 0.6906979576796326
running average episode reward sum: 0.4861215355309766
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 5.]), 'previousTarget': array([7., 5.]), 'currentState': array([7.08231835, 5.81488576, 4.58752379]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 0.8190330324963063}
episode index:3144
target Thresh 31.99999999999943
target distance 22.0
model initialize at round 3144
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.10327855, 12.76014879]), 'previousTarget': array([10.12677025, 12.73750984]), 'currentState': array([26.98802238,  2.04075394,  2.06381378]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 89
reward sum = 0.18644493821948765
running average episode reward sum: 0.48602624885456597
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 16.]), 'previousTarget': array([ 5., 16.]), 'currentState': array([ 5.79059422, 15.65408296,  2.69065827]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 0.8629587626128832}
episode index:3145
target Thresh 31.99999999999944
target distance 5.0
model initialize at round 3145
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 19.]), 'previousTarget': array([16., 19.]), 'currentState': array([20.97456063, 21.94629377,  4.09414464]), 'targetState': array([16, 19], dtype=int32), 'currentDistance': 5.781600163904005}
done in step count: 11
reward sum = 0.8028666570321485
running average episode reward sum: 0.48612696099956837
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 19.]), 'previousTarget': array([16., 19.]), 'currentState': array([16.7636615 , 19.58549207,  3.65068555]), 'targetState': array([16, 19], dtype=int32), 'currentDistance': 0.9622784651631521}
episode index:3146
target Thresh 31.999999999999442
target distance 16.0
model initialize at round 3146
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 25.]), 'previousTarget': array([25., 25.]), 'currentState': array([20.02729908,  9.03102295,  0.9926835 ]), 'targetState': array([25, 25], dtype=int32), 'currentDistance': 16.72530963662169}
done in step count: 37
reward sum = 0.4549309376493633
running average episode reward sum: 0.48611704805919653
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 25.]), 'previousTarget': array([25., 25.]), 'currentState': array([24.48047734, 24.08156628,  1.31968221]), 'targetState': array([25, 25], dtype=int32), 'currentDistance': 1.0551892200501556}
episode index:3147
target Thresh 31.99999999999945
target distance 14.0
model initialize at round 3147
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  9.]), 'previousTarget': array([19.,  9.]), 'currentState': array([5.        , 2.        , 6.24061489]), 'targetState': array([19,  9], dtype=int32), 'currentDistance': 15.65247584249853}
done in step count: 35
reward sum = 0.46527072833500116
running average episode reward sum: 0.48611042597542137
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  9.]), 'previousTarget': array([19.,  9.]), 'currentState': array([18.10954533,  8.71140035,  0.39461745]), 'targetState': array([19,  9], dtype=int32), 'currentDistance': 0.9360551682967413}
episode index:3148
target Thresh 31.999999999999456
target distance 8.0
model initialize at round 3148
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 9.99809693, 17.78878568,  4.5346595 ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 9.830684068545459}
done in step count: 18
reward sum = 0.6743919347484035
running average episode reward sum: 0.4861702168642029
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.9484993 , 10.96279246,  4.03916223]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.3515251554215262}
episode index:3149
target Thresh 31.99999999999946
target distance 5.0
model initialize at round 3149
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.00128221,  9.9961743 ,  5.25889143]), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.072867249560674}
done in step count: 16
reward sum = 0.7160417735752681
running average episode reward sum: 0.48624319196157145
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.02674099, 14.38975794,  0.97673581]), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.148750828895117}
episode index:3150
target Thresh 31.999999999999467
target distance 22.0
model initialize at round 3150
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.44864102, 17.78336807]), 'previousTarget': array([15.45345588, 17.79880147]), 'currentState': array([2.00554177, 2.9751581 , 5.18437672]), 'targetState': array([22, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 88
reward sum = 0.11483455542889032
running average episode reward sum: 0.48612532187698476
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 25.]), 'previousTarget': array([22., 25.]), 'currentState': array([21.07740667, 24.56927633,  0.78754814]), 'targetState': array([22, 25], dtype=int32), 'currentDistance': 1.0181853153321283}
episode index:3151
target Thresh 31.99999999999947
target distance 15.0
model initialize at round 3151
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 23.]), 'previousTarget': array([15., 23.]), 'currentState': array([12.04925212,  7.96271706,  5.88773108]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 15.324059227374368}
done in step count: 38
reward sum = 0.44364437935110185
running average episode reward sum: 0.4861118444205996
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 23.]), 'previousTarget': array([15., 23.]), 'currentState': array([14.60532351, 22.12492527,  1.28934582]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 0.9599611037024461}
episode index:3152
target Thresh 31.999999999999478
target distance 13.0
model initialize at round 3152
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([23.        , 21.        ,  1.60343942]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 14.764823060233402}
done in step count: 39
reward sum = 0.44832989167091625
running average episode reward sum: 0.48609986156213164
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([10.98178889, 14.30379576,  3.54107158]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 1.0277165427343873}
episode index:3153
target Thresh 31.99999999999948
target distance 7.0
model initialize at round 3153
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([15.        , 10.        ,  0.07678818]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 7.071067811865475}
done in step count: 16
reward sum = 0.7223581143639005
running average episode reward sum: 0.48617476906143464
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([15.79845626, 16.09785019,  1.46040751]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 0.9243885281248413}
episode index:3154
target Thresh 31.999999999999485
target distance 17.0
model initialize at round 3154
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 15.]), 'previousTarget': array([24., 15.]), 'currentState': array([ 6.96098438, 20.07998074,  1.7721467 ]), 'targetState': array([24, 15], dtype=int32), 'currentDistance': 17.780164726896427}
done in step count: 40
reward sum = 0.36926567130969035
running average episode reward sum: 0.48613771387989685
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 15.]), 'previousTarget': array([24., 15.]), 'currentState': array([23.0448029 , 15.64513642,  6.07672822]), 'targetState': array([24, 15], dtype=int32), 'currentDistance': 1.152650201122359}
episode index:3155
target Thresh 31.999999999999492
target distance 13.0
model initialize at round 3155
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([16.        ,  8.        ,  5.95819015]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 14.317821063276353}
done in step count: 35
reward sum = 0.46005985479157047
running average episode reward sum: 0.48612945093341764
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.98006308, 1.90629   , 3.6640762 ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.9845329849116566}
episode index:3156
target Thresh 31.999999999999496
target distance 20.0
model initialize at round 3156
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.14749698, 18.71110031]), 'previousTarget': array([22.14985851, 18.71008489]), 'currentState': array([ 4.99657354, 28.99924033,  3.61227155]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.23670911169394243
running average episode reward sum: 0.4860504454411023
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([24.26315289, 17.8301543 ,  5.71215329]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 1.1099999240424334}
episode index:3157
target Thresh 31.999999999999503
target distance 19.0
model initialize at round 3157
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([22.04438752, 11.9525161 ,  5.21159649]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 19.677807597288364}
done in step count: 52
reward sum = 0.30566764449179484
running average episode reward sum: 0.48599332612477897
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.96461669, 6.79359649, 3.42642271]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.9864521119776661}
episode index:3158
target Thresh 31.999999999999506
target distance 12.0
model initialize at round 3158
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 17.]), 'previousTarget': array([23., 17.]), 'currentState': array([18.04176862,  5.00377725,  0.3426873 ]), 'targetState': array([23, 17], dtype=int32), 'currentDistance': 12.980501486317747}
done in step count: 31
reward sum = 0.5392236906052049
running average episode reward sum: 0.4860101765092299
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 17.]), 'previousTarget': array([23., 17.]), 'currentState': array([22.37546069, 16.11836805,  0.95940464]), 'targetState': array([23, 17], dtype=int32), 'currentDistance': 1.0804278085726235}
episode index:3159
target Thresh 31.99999999999951
target distance 18.0
model initialize at round 3159
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6.34392115, 21.10528251]), 'previousTarget': array([ 6.35899411, 21.09400392]), 'currentState': array([22.99195575, 10.02182891,  2.05937693]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.3282105132807336
running average episode reward sum: 0.48596023990694237
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'previousTarget': array([ 5., 22.]), 'currentState': array([ 5.75388899, 21.14753825,  2.63818833]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 1.1379980877093792}
episode index:3160
target Thresh 31.999999999999517
target distance 14.0
model initialize at round 3160
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([16.        , 24.        ,  2.18580511]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 17.204650534085253}
done in step count: 43
reward sum = 0.3915709759984306
running average episode reward sum: 0.4859303793362658
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 6.83206009, 10.64436201,  3.93113989]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 1.0523908005287161}
episode index:3161
target Thresh 31.99999999999952
target distance 9.0
model initialize at round 3161
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([19.01128565, 28.80723385,  4.85827872]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 11.243197777908984}
done in step count: 29
reward sum = 0.6075189975675153
running average episode reward sum: 0.4859688324097102
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([25.74515774, 20.92253514,  5.51350589]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 0.9570870701962468}
episode index:3162
target Thresh 31.999999999999527
target distance 13.0
model initialize at round 3162
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 27.]), 'previousTarget': array([12., 27.]), 'currentState': array([25.        , 29.        ,  4.87215135]), 'targetState': array([12, 27], dtype=int32), 'currentDistance': 13.152946437965905}
done in step count: 40
reward sum = 0.5013672652225263
running average episode reward sum: 0.4859737007096827
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 27.]), 'previousTarget': array([12., 27.]), 'currentState': array([12.96241879, 26.69388487,  3.16230885]), 'targetState': array([12, 27], dtype=int32), 'currentDistance': 1.0099289040056307}
episode index:3163
target Thresh 31.99999999999953
target distance 13.0
model initialize at round 3163
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 15.]), 'previousTarget': array([17., 15.]), 'currentState': array([13.28510458, 27.86293631,  5.61384591]), 'targetState': array([17, 15], dtype=int32), 'currentDistance': 13.388636175246447}
done in step count: 35
reward sum = 0.5223820531741107
running average episode reward sum: 0.4859852077743048
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 15.]), 'previousTarget': array([17., 15.]), 'currentState': array([17.30132108, 15.92548942,  5.01201497]), 'targetState': array([17, 15], dtype=int32), 'currentDistance': 0.9733062523842698}
episode index:3164
target Thresh 31.999999999999535
target distance 18.0
model initialize at round 3164
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 20.]), 'previousTarget': array([ 8., 20.]), 'currentState': array([15.00208771,  2.00039824,  0.44099158]), 'targetState': array([ 8, 20], dtype=int32), 'currentDistance': 19.31359354629404}
done in step count: 50
reward sum = 0.34932059092435197
running average episode reward sum: 0.4859420278005766
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 20.]), 'previousTarget': array([ 8., 20.]), 'currentState': array([ 8.3850668 , 19.15555334,  1.71144345]), 'targetState': array([ 8, 20], dtype=int32), 'currentDistance': 0.9280983757961344}
episode index:3165
target Thresh 31.99999999999954
target distance 2.0
model initialize at round 3165
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 15.]), 'previousTarget': array([14., 15.]), 'currentState': array([15.696018  , 14.26595509,  2.41054236]), 'targetState': array([14, 15], dtype=int32), 'currentDistance': 1.848052753525294}
done in step count: 4
reward sum = 0.9401483486477599
running average episode reward sum: 0.4860854915784816
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 15.]), 'previousTarget': array([14., 15.]), 'currentState': array([14.96355754, 15.00608109,  2.25875551]), 'targetState': array([14, 15], dtype=int32), 'currentDistance': 0.9635767311443966}
episode index:3166
target Thresh 31.999999999999545
target distance 24.0
model initialize at round 3166
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.52179687,  7.90472876]), 'previousTarget': array([16.48069469,  8.15444247]), 'currentState': array([14.09060701, 27.75641171,  4.99568975]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.26671275142623024
running average episode reward sum: 0.48601622326772936
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([17.0088744 ,  4.76604973,  4.84753432]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 0.7661011284554444}
episode index:3167
target Thresh 31.99999999999955
target distance 8.0
model initialize at round 3167
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 19.]), 'previousTarget': array([ 6., 19.]), 'currentState': array([14.03046076, 12.90649258,  4.77480602]), 'targetState': array([ 6, 19], dtype=int32), 'currentDistance': 10.080631561586848}
done in step count: 27
reward sum = 0.5816500736392265
running average episode reward sum: 0.4860464107204981
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 19.]), 'previousTarget': array([ 6., 19.]), 'currentState': array([ 6.29418647, 18.19336599,  2.52673255]), 'targetState': array([ 6, 19], dtype=int32), 'currentDistance': 0.8586059037396776}
episode index:3168
target Thresh 31.999999999999552
target distance 12.0
model initialize at round 3168
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 21.]), 'previousTarget': array([22., 21.]), 'currentState': array([10.16391974, 15.24134784,  0.89662775]), 'targetState': array([22, 21], dtype=int32), 'currentDistance': 13.1626316027445}
done in step count: 32
reward sum = 0.5437084116900929
running average episode reward sum: 0.4860646063661181
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 21.]), 'previousTarget': array([22., 21.]), 'currentState': array([21.13374281, 21.04386027,  0.27111681]), 'targetState': array([22, 21], dtype=int32), 'currentDistance': 0.8673668406588968}
episode index:3169
target Thresh 31.99999999999956
target distance 7.0
model initialize at round 3169
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 16.]), 'previousTarget': array([16., 16.]), 'currentState': array([ 9.32517749, 13.04074791,  0.17610832]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 7.301399079271353}
done in step count: 17
reward sum = 0.7439004482485226
running average episode reward sum: 0.4861459425938412
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 16.]), 'previousTarget': array([16., 16.]), 'currentState': array([15.05934393, 15.84803747,  0.67381248]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 0.952851742406745}
episode index:3170
target Thresh 31.999999999999563
target distance 16.0
model initialize at round 3170
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([15.02594075,  8.97963597,  5.87014484]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 17.91836456429455}
done in step count: 48
reward sum = 0.33404990060442863
running average episode reward sum: 0.4860979779006878
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([ 6.73752117, 24.10139166,  1.88604035]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 0.9361581464248452}
episode index:3171
target Thresh 31.999999999999567
target distance 15.0
model initialize at round 3171
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 23.]), 'previousTarget': array([22., 23.]), 'currentState': array([ 7.        , 12.        ,  5.29088879]), 'targetState': array([22, 23], dtype=int32), 'currentDistance': 18.60107523773827}
done in step count: 48
reward sum = 0.34957805251723517
running average episode reward sum: 0.48605493883215584
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 23.]), 'previousTarget': array([22., 23.]), 'currentState': array([21.12474498, 22.63618317,  0.55310383]), 'targetState': array([22, 23], dtype=int32), 'currentDistance': 0.9478576049471688}
episode index:3172
target Thresh 31.99999999999957
target distance 10.0
model initialize at round 3172
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 13.]), 'previousTarget': array([ 4., 13.]), 'currentState': array([14.        , 14.        ,  1.46341032]), 'targetState': array([ 4, 13], dtype=int32), 'currentDistance': 10.049875621120892}
done in step count: 24
reward sum = 0.6030675281189614
running average episode reward sum: 0.48609181642096355
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 13.]), 'previousTarget': array([ 4., 13.]), 'currentState': array([ 4.71158324, 13.10555103,  3.26952632]), 'targetState': array([ 4, 13], dtype=int32), 'currentDistance': 0.7193689827265399}
episode index:3173
target Thresh 31.999999999999574
target distance 7.0
model initialize at round 3173
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([ 8.97173251, 15.92952965,  4.54592246]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 9.869899956641444}
done in step count: 21
reward sum = 0.6553410690433545
running average episode reward sum: 0.48614514006703236
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.15973004,  9.8122498 ,  5.68669559]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.1686758958106327}
episode index:3174
target Thresh 31.99999999999958
target distance 24.0
model initialize at round 3174
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.49576569, 19.90756121]), 'previousTarget': array([17.4397641 , 19.83810726]), 'currentState': array([3.04401629, 6.0819712 , 0.97241932]), 'targetState': array([27, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 88
reward sum = 0.07738518543582823
running average episode reward sum: 0.4860163967742351
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 29.]), 'previousTarget': array([27., 29.]), 'currentState': array([26.11097895, 28.58638383,  0.60733781]), 'targetState': array([27, 29], dtype=int32), 'currentDistance': 0.980528822466288}
episode index:3175
target Thresh 31.999999999999584
target distance 5.0
model initialize at round 3175
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 4.]), 'previousTarget': array([8., 4.]), 'currentState': array([4.93276094, 8.75274111, 4.58112359]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 5.656545185959254}
done in step count: 11
reward sum = 0.7974830360314649
running average episode reward sum: 0.4861144656153111
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 4.]), 'previousTarget': array([8., 4.]), 'currentState': array([7.02984535, 4.38798681, 5.57140059]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 1.044860667944697}
episode index:3176
target Thresh 31.999999999999588
target distance 7.0
model initialize at round 3176
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 25.]), 'previousTarget': array([18., 25.]), 'currentState': array([25.        , 18.        ,  0.37009394]), 'targetState': array([18, 25], dtype=int32), 'currentDistance': 9.899494936611665}
done in step count: 24
reward sum = 0.6186898572669443
running average episode reward sum: 0.4861561953577258
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 25.]), 'previousTarget': array([18., 25.]), 'currentState': array([18.99512196, 24.30412398,  2.30400674]), 'targetState': array([18, 25], dtype=int32), 'currentDistance': 1.2142945124278164}
episode index:3177
target Thresh 31.99999999999959
target distance 22.0
model initialize at round 3177
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.51093912, 14.42734309]), 'previousTarget': array([21.51093912, 14.42734309]), 'currentState': array([ 3.        , 22.        ,  2.05394912]), 'targetState': array([25, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.21247002123915504
running average episode reward sum: 0.48607007636020577
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 13.]), 'previousTarget': array([25., 13.]), 'currentState': array([24.17587794, 13.44943026,  5.99203009]), 'targetState': array([25, 13], dtype=int32), 'currentDistance': 0.9387037464472971}
episode index:3178
target Thresh 31.999999999999595
target distance 21.0
model initialize at round 3178
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.06678952,  3.00270987]), 'previousTarget': array([24.,  3.]), 'currentState': array([4.06687384, 3.06078589, 0.63213593]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.32926424966306317
running average episode reward sum: 0.4860207508406408
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'previousTarget': array([25.,  3.]), 'currentState': array([24.16704445,  2.86550819,  6.19255658]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 0.8437434466908821}
episode index:3179
target Thresh 31.9999999999996
target distance 10.0
model initialize at round 3179
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  3.]), 'previousTarget': array([11.,  3.]), 'currentState': array([20.99387534,  4.02912813,  2.03054333]), 'targetState': array([11,  3], dtype=int32), 'currentDistance': 10.046723300859767}
done in step count: 23
reward sum = 0.6221820922611425
running average episode reward sum: 0.48606356887253405
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  3.]), 'previousTarget': array([11.,  3.]), 'currentState': array([11.63760626,  3.34584622,  3.33489152]), 'targetState': array([11,  3], dtype=int32), 'currentDistance': 0.725362910417774}
episode index:3180
target Thresh 31.999999999999606
target distance 8.0
model initialize at round 3180
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 12.]), 'previousTarget': array([15., 12.]), 'currentState': array([23.02335948, 13.92060742,  4.74928218]), 'targetState': array([15, 12], dtype=int32), 'currentDistance': 8.250032136020678}
done in step count: 17
reward sum = 0.6717137013485376
running average episode reward sum: 0.48612193106444734
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 12.]), 'previousTarget': array([15., 12.]), 'currentState': array([15.59637637, 12.69627308,  3.39607032]), 'targetState': array([15, 12], dtype=int32), 'currentDistance': 0.9167665895353647}
episode index:3181
target Thresh 31.99999999999961
target distance 13.0
model initialize at round 3181
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 24.]), 'previousTarget': array([15., 24.]), 'currentState': array([ 2.15953739, 18.92739991,  5.98523831]), 'targetState': array([15, 24], dtype=int32), 'currentDistance': 13.806112840588662}
done in step count: 31
reward sum = 0.5322646273257858
running average episode reward sum: 0.48613643222606306
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 24.]), 'previousTarget': array([15., 24.]), 'currentState': array([14.19244101, 23.70533167,  0.60893778]), 'targetState': array([15, 24], dtype=int32), 'currentDistance': 0.8596400080556701}
episode index:3182
target Thresh 31.999999999999613
target distance 25.0
model initialize at round 3182
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.20063923,  9.01598083]), 'previousTarget': array([26.20063923,  9.01598083]), 'currentState': array([27.        , 29.        ,  5.11681399]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.22551938268087474
running average episode reward sum: 0.48605455442224743
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  4.]), 'previousTarget': array([26.,  4.]), 'currentState': array([25.95714618,  4.94589775,  4.68664324]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 0.9468679961221832}
episode index:3183
target Thresh 31.999999999999616
target distance 18.0
model initialize at round 3183
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([9.02064961, 3.00682368, 0.56954828]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 17.993188168334836}
done in step count: 43
reward sum = 0.3784810609870894
running average episode reward sum: 0.4860207687773243
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([ 9.06763592, 20.09865539,  1.7088748 ]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 0.9038787082211047}
episode index:3184
target Thresh 31.99999999999962
target distance 7.0
model initialize at round 3184
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 17.]), 'previousTarget': array([ 4., 17.]), 'currentState': array([11.        , 18.        ,  4.72269937]), 'targetState': array([ 4, 17], dtype=int32), 'currentDistance': 7.0710678118654755}
done in step count: 16
reward sum = 0.7240687427558828
running average episode reward sum: 0.4860955091145232
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 17.]), 'previousTarget': array([ 4., 17.]), 'currentState': array([ 4.95241449, 17.12658797,  3.27335541]), 'targetState': array([ 4, 17], dtype=int32), 'currentDistance': 0.9607902313303505}
episode index:3185
target Thresh 31.999999999999623
target distance 16.0
model initialize at round 3185
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([ 5.05118392, 18.0050341 ,  6.13193635]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 16.039516956287716}
done in step count: 39
reward sum = 0.4209733379409792
running average episode reward sum: 0.48607506901057673
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.93013871, 2.90337779, 4.63983049]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.9060750698371811}
episode index:3186
target Thresh 31.999999999999627
target distance 22.0
model initialize at round 3186
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8.11145618, 21.94427191]), 'previousTarget': array([ 8.11145618, 21.94427191]), 'currentState': array([26.        , 13.        ,  4.04284811]), 'targetState': array([ 4, 24], dtype=int32), 'currentDistance': 20.0}
done in step count: 68
reward sum = 0.21302623713698898
running average episode reward sum: 0.4859893931926057
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 24.]), 'previousTarget': array([ 4., 24.]), 'currentState': array([ 4.67655342, 23.6946891 ,  2.5874326 ]), 'targetState': array([ 4, 24], dtype=int32), 'currentDistance': 0.742252835981163}
episode index:3187
target Thresh 31.99999999999963
target distance 18.0
model initialize at round 3187
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 25.]), 'previousTarget': array([26., 25.]), 'currentState': array([25.92379112,  7.15429342,  1.83979088]), 'targetState': array([26, 25], dtype=int32), 'currentDistance': 17.8458692971685}
done in step count: 43
reward sum = 0.39379149921706563
running average episode reward sum: 0.48596047289963973
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 25.]), 'previousTarget': array([26., 25.]), 'currentState': array([26.05716413, 24.01882217,  1.45501553]), 'targetState': array([26, 25], dtype=int32), 'currentDistance': 0.9828416299718743}
episode index:3188
target Thresh 31.999999999999634
target distance 13.0
model initialize at round 3188
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 25.]), 'previousTarget': array([17., 25.]), 'currentState': array([ 4.        , 17.        ,  4.68208793]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 15.264337522473747}
done in step count: 40
reward sum = 0.4070235088160001
running average episode reward sum: 0.48593572001030655
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 25.]), 'previousTarget': array([17., 25.]), 'currentState': array([16.09585813, 24.41508358,  0.63240249]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 1.076847126718115}
episode index:3189
target Thresh 31.999999999999638
target distance 18.0
model initialize at round 3189
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.54026305, 19.73247066]), 'previousTarget': array([16.54026305, 19.73247066]), 'currentState': array([2.        , 6.        , 5.54460526]), 'targetState': array([20, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.17947141669142763
running average episode reward sum: 0.4858396496957865
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 23.]), 'previousTarget': array([20., 23.]), 'currentState': array([19.3653534 , 22.13814345,  0.68046624]), 'targetState': array([20, 23], dtype=int32), 'currentDistance': 1.0703144502487025}
episode index:3190
target Thresh 31.99999999999964
target distance 11.0
model initialize at round 3190
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 13.]), 'previousTarget': array([25., 13.]), 'currentState': array([25.        , 24.        ,  3.87573814]), 'targetState': array([25, 13], dtype=int32), 'currentDistance': 11.0}
done in step count: 26
reward sum = 0.5800016774517005
running average episode reward sum: 0.48586915832247285
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 13.]), 'previousTarget': array([25., 13.]), 'currentState': array([24.75065649, 13.79237593,  4.62325526]), 'targetState': array([25, 13], dtype=int32), 'currentDistance': 0.8306815236158899}
episode index:3191
target Thresh 31.999999999999645
target distance 16.0
model initialize at round 3191
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 13.]), 'previousTarget': array([ 5., 13.]), 'currentState': array([15.80232681, 28.62372933,  4.06610003]), 'targetState': array([ 5, 13], dtype=int32), 'currentDistance': 18.994504015770385}
done in step count: 44
reward sum = 0.3578725958453086
running average episode reward sum: 0.48582905914876445
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 13.]), 'previousTarget': array([ 5., 13.]), 'currentState': array([ 5.39221812, 13.80514416,  4.20414215]), 'targetState': array([ 5, 13], dtype=int32), 'currentDistance': 0.8955959860802534}
episode index:3192
target Thresh 31.99999999999965
target distance 3.0
model initialize at round 3192
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([13.01142636, 12.00217461,  0.44056624]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 3.613861511660452}
done in step count: 9
reward sum = 0.8147889380037595
running average episode reward sum: 0.48593208447881614
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([10.85732859, 13.93659935,  2.92294484]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 0.8596696760251076}
episode index:3193
target Thresh 31.999999999999652
target distance 5.0
model initialize at round 3193
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 25.]), 'previousTarget': array([12., 25.]), 'currentState': array([17.00290709, 22.01678332,  1.6517849 ]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 5.824831424498869}
done in step count: 11
reward sum = 0.786074895066388
running average episode reward sum: 0.486026055302419
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 25.]), 'previousTarget': array([12., 25.]), 'currentState': array([12.96552544, 24.43610674,  2.76763187]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 1.1181301262488819}
episode index:3194
target Thresh 31.999999999999655
target distance 11.0
model initialize at round 3194
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 24.]), 'previousTarget': array([ 3., 24.]), 'currentState': array([ 3.74685228, 13.22465152,  2.30427361]), 'targetState': array([ 3, 24], dtype=int32), 'currentDistance': 10.80120007908825}
done in step count: 26
reward sum = 0.608887223779812
running average episode reward sum: 0.48606450950225544
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 24.]), 'previousTarget': array([ 3., 24.]), 'currentState': array([ 3.34314465, 23.22647948,  1.76947418]), 'targetState': array([ 3, 24], dtype=int32), 'currentDistance': 0.8462164292522864}
episode index:3195
target Thresh 31.99999999999966
target distance 8.0
model initialize at round 3195
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([ 4.99277915, 27.9971584 ,  3.76900578]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 8.549768669054892}
done in step count: 19
reward sum = 0.6414436107752808
running average episode reward sum: 0.48611312624232833
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([12.07592089, 25.22047692,  6.06690556]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 0.9500169828610744}
episode index:3196
target Thresh 31.999999999999662
target distance 24.0
model initialize at round 3196
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8.99800872, 6.04747027]), 'previousTarget': array([9., 6.]), 'currentState': array([ 8.9881691 , 26.04746785,  2.06755984]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.19229393440805337
running average episode reward sum: 0.48602122158426325
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 2.]), 'previousTarget': array([9., 2.]), 'currentState': array([9.05079023, 2.92721136, 4.64782039]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 0.9286013985532324}
episode index:3197
target Thresh 31.999999999999666
target distance 2.0
model initialize at round 3197
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'previousTarget': array([22., 18.]), 'currentState': array([22.        , 20.        ,  2.99735153]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 2.0}
done in step count: 8
reward sum = 0.8859711473346166
running average episode reward sum: 0.48614628410013266
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'previousTarget': array([22., 18.]), 'currentState': array([21.34488162, 18.85828037,  4.52590208]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 1.0797339012565454}
episode index:3198
target Thresh 31.99999999999967
target distance 17.0
model initialize at round 3198
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 25.]), 'previousTarget': array([24., 25.]), 'currentState': array([ 7.        , 20.        ,  2.37478399]), 'targetState': array([24, 25], dtype=int32), 'currentDistance': 17.72004514666935}
done in step count: 46
reward sum = 0.3650083728077709
running average episode reward sum: 0.4861084166692816
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 25.]), 'previousTarget': array([24., 25.]), 'currentState': array([23.15829687, 24.7241497 ,  0.22570945]), 'targetState': array([24, 25], dtype=int32), 'currentDistance': 0.8857525306639858}
episode index:3199
target Thresh 31.999999999999673
target distance 9.0
model initialize at round 3199
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([24.64368563, 19.93093128,  3.28156944]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 8.856737372230532}
done in step count: 18
reward sum = 0.6982453093803846
running average episode reward sum: 0.48617470944825386
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([16.96206606, 18.32004916,  3.43138996]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 1.0139046153276308}
episode index:3200
target Thresh 31.999999999999677
target distance 6.0
model initialize at round 3200
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 16.]), 'previousTarget': array([21., 16.]), 'currentState': array([15.21893648, 19.73757053,  5.48530258]), 'targetState': array([21, 16], dtype=int32), 'currentDistance': 6.884048876861108}
done in step count: 13
reward sum = 0.7704597873208476
running average episode reward sum: 0.48626352078154733
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 16.]), 'previousTarget': array([21., 16.]), 'currentState': array([20.21489407, 16.67527076,  5.79539279]), 'targetState': array([21, 16], dtype=int32), 'currentDistance': 1.0355587444053835}
episode index:3201
target Thresh 31.99999999999968
target distance 3.0
model initialize at round 3201
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 26.]), 'previousTarget': array([20., 26.]), 'currentState': array([21.99878574, 29.01081987,  1.93505335]), 'targetState': array([20, 26], dtype=int32), 'currentDistance': 3.613887210239051}
done in step count: 11
reward sum = 0.8040668454899751
running average episode reward sum: 0.486362772288327
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 26.]), 'previousTarget': array([20., 26.]), 'currentState': array([19.67947737, 26.94956845,  4.46914303]), 'targetState': array([20, 26], dtype=int32), 'currentDistance': 1.0022050700280698}
episode index:3202
target Thresh 31.999999999999684
target distance 8.0
model initialize at round 3202
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 5.98039724, 18.99705556,  3.54318357]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 8.061791822027864}
done in step count: 18
reward sum = 0.679587302002679
running average episode reward sum: 0.4864230983981348
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.69497745, 11.8578813 ,  5.30997409]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.9104938652511108}
episode index:3203
target Thresh 31.999999999999684
target distance 8.0
model initialize at round 3203
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 20.]), 'previousTarget': array([13., 20.]), 'currentState': array([20.80350285, 21.76261959,  3.87002221]), 'targetState': array([13, 20], dtype=int32), 'currentDistance': 8.0000927787739}
done in step count: 17
reward sum = 0.7109384313603979
running average episode reward sum: 0.4864931718478733
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 20.]), 'previousTarget': array([13., 20.]), 'currentState': array([13.87073039, 20.0324677 ,  3.37527364]), 'targetState': array([13, 20], dtype=int32), 'currentDistance': 0.8713355080271359}
episode index:3204
target Thresh 31.999999999999687
target distance 8.0
model initialize at round 3204
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  7.]), 'previousTarget': array([19.,  7.]), 'currentState': array([27.03452831, 13.71845558,  4.67416385]), 'targetState': array([19,  7], dtype=int32), 'currentDistance': 10.473360993172026}
done in step count: 22
reward sum = 0.6338624123739269
running average episode reward sum: 0.4865391528901591
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  7.]), 'previousTarget': array([19.,  7.]), 'currentState': array([19.8981834 ,  7.84621713,  3.70432279]), 'targetState': array([19,  7], dtype=int32), 'currentDistance': 1.2340246513407325}
episode index:3205
target Thresh 31.99999999999969
target distance 17.0
model initialize at round 3205
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([25.90816616, 18.95317524,  3.83557436]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 17.060224171313344}
done in step count: 47
reward sum = 0.41986322912475205
running average episode reward sum: 0.48651835565879126
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([24.06541573,  2.89008958,  4.71654257]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 0.8924901554652891}
episode index:3206
target Thresh 31.999999999999694
target distance 11.0
model initialize at round 3206
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 14.]), 'previousTarget': array([ 3., 14.]), 'currentState': array([14.01055864,  5.0707473 ,  1.67514539]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 14.176175626941443}
done in step count: 31
reward sum = 0.5160923541386401
running average episode reward sum: 0.48652757736084296
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 14.]), 'previousTarget': array([ 3., 14.]), 'currentState': array([ 3.77398199, 13.03045776,  2.51092009]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 1.2405886824588128}
episode index:3207
target Thresh 31.999999999999698
target distance 17.0
model initialize at round 3207
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  2.]), 'previousTarget': array([19.,  2.]), 'currentState': array([11.        , 19.        ,  3.56256342]), 'targetState': array([19,  2], dtype=int32), 'currentDistance': 18.788294228055936}
done in step count: 54
reward sum = 0.35358381786542253
running average episode reward sum: 0.4864861360393045
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  2.]), 'previousTarget': array([19.,  2.]), 'currentState': array([18.6257799 ,  2.85548226,  5.28254645]), 'targetState': array([19,  2], dtype=int32), 'currentDistance': 0.9337508076857153}
episode index:3208
target Thresh 31.9999999999997
target distance 17.0
model initialize at round 3208
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8.28585494, 9.56139529]), 'previousTarget': array([8.28585494, 9.56139529]), 'currentState': array([21.        , 25.        ,  0.31252465]), 'targetState': array([7, 8], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.2600023760963895
running average episode reward sum: 0.486415558364034
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 8.]), 'previousTarget': array([7., 8.]), 'currentState': array([7.87955496, 8.96383952, 4.03206941]), 'targetState': array([7, 8], dtype=int32), 'currentDistance': 1.3048385164577228}
episode index:3209
target Thresh 31.999999999999705
target distance 8.0
model initialize at round 3209
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 23.]), 'previousTarget': array([17., 23.]), 'currentState': array([25.        , 24.        ,  0.77106151]), 'targetState': array([17, 23], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 24
reward sum = 0.6301606252003402
running average episode reward sum: 0.486460338758687
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 23.]), 'previousTarget': array([17., 23.]), 'currentState': array([17.90370417, 22.8889942 ,  3.47682513]), 'targetState': array([17, 23], dtype=int32), 'currentDistance': 0.9104962967059865}
episode index:3210
target Thresh 31.999999999999705
target distance 20.0
model initialize at round 3210
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7.85395981, 28.41498734]), 'previousTarget': array([ 7.8507125, 28.40285  ]), 'currentState': array([3.00989637, 9.01047628, 0.85902822]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.32811429727332686
running average episode reward sum: 0.4864110251362998
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 29.]), 'previousTarget': array([ 8., 29.]), 'currentState': array([ 8.04345102, 28.08139013,  1.16819663]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 0.9196369289194496}
episode index:3211
target Thresh 31.99999999999971
target distance 4.0
model initialize at round 3211
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 19.]), 'previousTarget': array([ 5., 19.]), 'currentState': array([ 5.02484948, 15.00715258,  0.50066464]), 'targetState': array([ 5, 19], dtype=int32), 'currentDistance': 3.9929247414408553}
done in step count: 9
reward sum = 0.8444445978664719
running average episode reward sum: 0.48652249262469643
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 19.]), 'previousTarget': array([ 5., 19.]), 'currentState': array([ 5.95520591, 18.12176922,  1.88604502]), 'targetState': array([ 5, 19], dtype=int32), 'currentDistance': 1.2975776006170707}
episode index:3212
target Thresh 31.999999999999712
target distance 18.0
model initialize at round 3212
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7.26675693, 13.46172947]), 'previousTarget': array([ 7.26752934, 13.45973695]), 'currentState': array([20.99333166, 28.00755864,  2.52911404]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.075497137325087
running average episode reward sum: 0.4863475721049486
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 9.94268888, 25.07452675,  4.24424467]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 16.203607865830218}
episode index:3213
target Thresh 31.999999999999716
target distance 15.0
model initialize at round 3213
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'previousTarget': array([25.,  3.]), 'currentState': array([10.03058026, 11.08604963,  0.98474333]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 17.01375108064753}
done in step count: 38
reward sum = 0.41153096562760166
running average episode reward sum: 0.48632429375819153
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'previousTarget': array([25.,  3.]), 'currentState': array([24.25395148,  3.46437542,  5.88562244]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 0.8787678507650065}
episode index:3214
target Thresh 31.99999999999972
target distance 12.0
model initialize at round 3214
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 16.]), 'previousTarget': array([15., 16.]), 'currentState': array([ 6.97011477, 27.96078543,  4.31369185]), 'targetState': array([15, 16], dtype=int32), 'currentDistance': 14.406229378772471}
done in step count: 33
reward sum = 0.5114078955161947
running average episode reward sum: 0.48633209581161546
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 16.]), 'previousTarget': array([15., 16.]), 'currentState': array([14.22055102, 16.99303473,  5.31315698]), 'targetState': array([15, 16], dtype=int32), 'currentDistance': 1.2624019514845886}
episode index:3215
target Thresh 31.99999999999972
target distance 18.0
model initialize at round 3215
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6.71985274, 14.0597235 ]), 'previousTarget': array([ 6.71272322, 14.05181363]), 'currentState': array([20.0073984 , 29.00767086,  0.88917252]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.21343255369069475
running average episode reward sum: 0.48624723898881667
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.92029225, 11.96906002,  4.05488627]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.3364187766551785}
episode index:3216
target Thresh 31.999999999999723
target distance 17.0
model initialize at round 3216
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 23.]), 'previousTarget': array([ 4., 23.]), 'currentState': array([21.0610406 , 22.90564633,  5.03507981]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 17.061301502929506}
done in step count: 46
reward sum = 0.3886492927192552
running average episode reward sum: 0.4862169008022237
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 23.]), 'previousTarget': array([ 4., 23.]), 'currentState': array([ 4.85568984, 22.79527453,  2.88630002]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 0.8798395378412786}
episode index:3217
target Thresh 31.999999999999726
target distance 14.0
model initialize at round 3217
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 16.]), 'previousTarget': array([11., 16.]), 'currentState': array([15.99366275,  1.99784345,  3.217098  ]), 'targetState': array([11, 16], dtype=int32), 'currentDistance': 14.865969721373025}
done in step count: 34
reward sum = 0.4661385743305516
running average episode reward sum: 0.4862106614217166
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 16.]), 'previousTarget': array([11., 16.]), 'currentState': array([11.04233595, 15.24421958,  1.87788378]), 'targetState': array([11, 16], dtype=int32), 'currentDistance': 0.75696523946806}
episode index:3218
target Thresh 31.99999999999973
target distance 22.0
model initialize at round 3218
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6.65615552, 21.28853356]), 'previousTarget': array([ 6.70472358, 21.26234812]), 'currentState': array([25.97512345, 16.1138376 ,  2.03179434]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 82
reward sum = 0.23470949402833044
running average episode reward sum: 0.4861325312050675
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([ 4.8743111 , 22.10700334,  3.39118327]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 0.8808346126112326}
episode index:3219
target Thresh 31.999999999999734
target distance 12.0
model initialize at round 3219
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 29.]), 'previousTarget': array([15., 29.]), 'currentState': array([ 7.99311819, 16.94342008,  4.83500618]), 'targetState': array([15, 29], dtype=int32), 'currentDistance': 13.944802330774626}
done in step count: 38
reward sum = 0.46876158234979715
running average episode reward sum: 0.4861271365004541
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 29.]), 'previousTarget': array([15., 29.]), 'currentState': array([14.43683455, 28.11594479,  1.10348955]), 'targetState': array([15, 29], dtype=int32), 'currentDistance': 1.048193178561076}
episode index:3220
target Thresh 31.999999999999734
target distance 22.0
model initialize at round 3220
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8.86220213, 11.927824  ]), 'previousTarget': array([ 8.86353984, 11.92760259]), 'currentState': array([23.99716016, 25.00196052,  2.78986454]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.058064277689593616
running average episode reward sum: 0.48595818542495267
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3.44784954, 7.33321084]), 'previousTarget': array([3.72793634, 7.59639017]), 'currentState': array([18.16044681, 20.88088524,  3.98736815]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:3221
target Thresh 31.999999999999737
target distance 7.0
model initialize at round 3221
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  5.]), 'previousTarget': array([10.,  5.]), 'currentState': array([17.        ,  2.        ,  1.15734649]), 'targetState': array([10,  5], dtype=int32), 'currentDistance': 7.615773105863908}
done in step count: 21
reward sum = 0.6877990689194298
running average episode reward sum: 0.4860208300194575
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  5.]), 'previousTarget': array([10.,  5.]), 'currentState': array([10.96803756,  4.64499835,  2.82689635]), 'targetState': array([10,  5], dtype=int32), 'currentDistance': 1.031078506686203}
episode index:3222
target Thresh 31.99999999999974
target distance 11.0
model initialize at round 3222
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 22.]), 'previousTarget': array([21., 22.]), 'currentState': array([10.        , 14.        ,  5.49221277]), 'targetState': array([21, 22], dtype=int32), 'currentDistance': 13.601470508735444}
done in step count: 32
reward sum = 0.5209983445842358
running average episode reward sum: 0.48603168249062256
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 22.]), 'previousTarget': array([21., 22.]), 'currentState': array([20.23223785, 21.12576365,  0.75692971]), 'targetState': array([21, 22], dtype=int32), 'currentDistance': 1.163506735483063}
episode index:3223
target Thresh 31.99999999999974
target distance 17.0
model initialize at round 3223
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 23.]), 'previousTarget': array([18., 23.]), 'currentState': array([10.14501061,  6.04109691,  0.50990115]), 'targetState': array([18, 23], dtype=int32), 'currentDistance': 18.689709793983855}
done in step count: 40
reward sum = 0.3890114251679665
running average episode reward sum: 0.4860015893586987
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 23.]), 'previousTarget': array([18., 23.]), 'currentState': array([17.69038147, 22.06619202,  1.44574069]), 'targetState': array([18, 23], dtype=int32), 'currentDistance': 0.9837992555370334}
episode index:3224
target Thresh 31.999999999999744
target distance 15.0
model initialize at round 3224
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 28.]), 'previousTarget': array([ 7., 28.]), 'currentState': array([12.80901458, 13.4325055 ,  1.92789393]), 'targetState': array([ 7, 28], dtype=int32), 'currentDistance': 15.683001832617165}
done in step count: 36
reward sum = 0.48084964094084337
running average episode reward sum: 0.48599999185531323
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 28.]), 'previousTarget': array([ 7., 28.]), 'currentState': array([ 7.22577921, 27.03228866,  2.03056619]), 'targetState': array([ 7, 28], dtype=int32), 'currentDistance': 0.9937009055740263}
episode index:3225
target Thresh 31.999999999999748
target distance 12.0
model initialize at round 3225
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 23.]), 'previousTarget': array([18., 23.]), 'currentState': array([14.06057914, 11.00717802,  0.3678739 ]), 'targetState': array([18, 23], dtype=int32), 'currentDistance': 12.623264857391154}
done in step count: 39
reward sum = 0.4938661663624683
running average episode reward sum: 0.4860024302231084
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 23.]), 'previousTarget': array([18., 23.]), 'currentState': array([18.90449805, 22.47858894,  2.27877898]), 'targetState': array([18, 23], dtype=int32), 'currentDistance': 1.0440240488212464}
episode index:3226
target Thresh 31.99999999999975
target distance 12.0
model initialize at round 3226
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([ 9.95993838, 15.00235317,  3.33542132]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 12.161325587421352}
done in step count: 35
reward sum = 0.5206415277819764
running average episode reward sum: 0.48601316437171665
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([7.0118294 , 3.56631268, 5.28072671]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 1.1389430093166626}
episode index:3227
target Thresh 31.99999999999975
target distance 13.0
model initialize at round 3227
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 19.]), 'previousTarget': array([25., 19.]), 'currentState': array([26.07009405,  6.00303295,  0.29574269]), 'targetState': array([25, 19], dtype=int32), 'currentDistance': 13.040945283610144}
done in step count: 38
reward sum = 0.5041181678501401
running average episode reward sum: 0.48601877310885366
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 19.]), 'previousTarget': array([25., 19.]), 'currentState': array([25.01885895, 18.07914858,  1.89918428]), 'targetState': array([25, 19], dtype=int32), 'currentDistance': 0.9210445182838961}
episode index:3228
target Thresh 31.999999999999755
target distance 15.0
model initialize at round 3228
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 25.]), 'previousTarget': array([27., 25.]), 'currentState': array([21.08405849, 10.04349046,  0.71480349]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 16.08401509349831}
done in step count: 55
reward sum = 0.36548807912538794
running average episode reward sum: 0.4859814455480041
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 25.]), 'previousTarget': array([27., 25.]), 'currentState': array([27.52419692, 24.04628267,  2.03809612]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 1.0882826650633304}
episode index:3229
target Thresh 31.99999999999976
target distance 17.0
model initialize at round 3229
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([16.96593839, 27.91552185,  4.36357535]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 17.64870124493473}
done in step count: 59
reward sum = 0.38152438081434037
running average episode reward sum: 0.48594910589947976
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([21.34287566, 11.91640405,  5.41965754]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 1.1276563201749568}
episode index:3230
target Thresh 31.99999999999976
target distance 8.0
model initialize at round 3230
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 20.]), 'previousTarget': array([ 2., 20.]), 'currentState': array([ 1.99069671, 12.00071646,  2.87904608]), 'targetState': array([ 2, 20], dtype=int32), 'currentDistance': 7.999288951900893}
done in step count: 21
reward sum = 0.6767276467654857
running average episode reward sum: 0.4860081521826324
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 20.]), 'previousTarget': array([ 2., 20.]), 'currentState': array([ 1.63508142, 19.04949755,  1.63772325]), 'targetState': array([ 2, 20], dtype=int32), 'currentDistance': 1.0181456101288067}
episode index:3231
target Thresh 31.999999999999762
target distance 1.0
model initialize at round 3231
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.08189109,  8.1316709 ,  1.14749944]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.8721820788841269}
done in step count: 0
reward sum = 0.9962541464676588
running average episode reward sum: 0.4861660253244285
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.08189109,  8.1316709 ,  1.14749944]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.8721820788841269}
episode index:3232
target Thresh 31.999999999999766
target distance 6.0
model initialize at round 3232
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([20.99128765,  5.01032915,  2.13889104]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 6.074014941777773}
done in step count: 14
reward sum = 0.7838128552982693
running average episode reward sum: 0.4862580905362979
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([21.5944604 , 10.03483413,  1.20946523]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 1.0469037753183625}
episode index:3233
target Thresh 31.999999999999766
target distance 21.0
model initialize at round 3233
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.40613587, 11.1785457 ]), 'previousTarget': array([19.41826648, 11.16928442]), 'currentState': array([ 5.98292022, 26.00478196,  3.12110448]), 'targetState': array([25,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.3394245825880638
running average episode reward sum: 0.486002777402988
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  5.]), 'previousTarget': array([25.,  5.]), 'currentState': array([18.50220965, 11.40800493,  5.47831718]), 'targetState': array([25,  5], dtype=int32), 'currentDistance': 9.125996197051526}
episode index:3234
target Thresh 31.99999999999977
target distance 4.0
model initialize at round 3234
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 13.]), 'previousTarget': array([ 7., 13.]), 'currentState': array([4.03969847, 8.95972576, 5.74308777]), 'targetState': array([ 7, 13], dtype=int32), 'currentDistance': 5.008712513040991}
done in step count: 15
reward sum = 0.7741847356376068
running average episode reward sum: 0.48609185992485343
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 13.]), 'previousTarget': array([ 7., 13.]), 'currentState': array([ 7.19381913, 12.13897232,  1.30544393]), 'targetState': array([ 7, 13], dtype=int32), 'currentDistance': 0.8825726734444842}
episode index:3235
target Thresh 31.999999999999773
target distance 17.0
model initialize at round 3235
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 28.]), 'previousTarget': array([23., 28.]), 'currentState': array([ 6.        , 21.        ,  1.73138571]), 'targetState': array([23, 28], dtype=int32), 'currentDistance': 18.384776310850235}
done in step count: 50
reward sum = 0.36814830937765525
running average episode reward sum: 0.48605541259773744
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 28.]), 'previousTarget': array([23., 28.]), 'currentState': array([22.08208042, 27.53847239,  0.34141901]), 'targetState': array([23, 28], dtype=int32), 'currentDistance': 1.027416221453603}
episode index:3236
target Thresh 31.999999999999773
target distance 10.0
model initialize at round 3236
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([5.        , 5.        , 2.41456366]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 11.661903789690601}
done in step count: 33
reward sum = 0.5337909458667262
running average episode reward sum: 0.48607015944150295
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.05919829, 10.79612542,  0.424051  ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.962638406543593}
episode index:3237
target Thresh 31.999999999999776
target distance 19.0
model initialize at round 3237
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5.89888325, 18.86398076]), 'previousTarget': array([ 5.89888325, 18.86398076]), 'currentState': array([22.        ,  7.        ,  4.82874918]), 'targetState': array([ 3, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 73
reward sum = 0.21559462422540138
running average episode reward sum: 0.48598662777528423
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 21.]), 'previousTarget': array([ 3., 21.]), 'currentState': array([ 3.74501649, 20.03124795,  2.25412735]), 'targetState': array([ 3, 21], dtype=int32), 'currentDistance': 1.2221006929511304}
episode index:3238
target Thresh 31.99999999999978
target distance 14.0
model initialize at round 3238
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 23.]), 'previousTarget': array([ 4., 23.]), 'currentState': array([12.00442294,  8.99818513,  6.13726619]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 16.128285902364162}
done in step count: 39
reward sum = 0.41878041510971276
running average episode reward sum: 0.4859658787130226
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 23.]), 'previousTarget': array([ 4., 23.]), 'currentState': array([ 4.44275205, 22.10294194,  2.05498176]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 1.00037119553564}
episode index:3239
target Thresh 31.99999999999978
target distance 24.0
model initialize at round 3239
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.83074946, 16.16260672]), 'previousTarget': array([12.85786438, 16.14213562]), 'currentState': array([26.96810711,  2.01569474,  2.5409089 ]), 'targetState': array([ 3, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 91
reward sum = -0.00947365910275183
running average episode reward sum: 0.4858129652754251
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 26.]), 'previousTarget': array([ 3., 26.]), 'currentState': array([ 3.57018543, 25.09490418,  2.25689186]), 'targetState': array([ 3, 26], dtype=int32), 'currentDistance': 1.0697241989533703}
episode index:3240
target Thresh 31.999999999999783
target distance 18.0
model initialize at round 3240
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 23.]), 'previousTarget': array([26., 23.]), 'currentState': array([ 8.10101979, 22.08656174,  0.67218486]), 'targetState': array([26, 23], dtype=int32), 'currentDistance': 17.92227279349834}
done in step count: 41
reward sum = 0.4138409900326317
running average episode reward sum: 0.48579075855674486
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 23.]), 'previousTarget': array([26., 23.]), 'currentState': array([25.07881722, 23.22336391,  6.18766785]), 'targetState': array([26, 23], dtype=int32), 'currentDistance': 0.9478761240619842}
episode index:3241
target Thresh 31.999999999999783
target distance 11.0
model initialize at round 3241
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([22.58633842,  1.81126901,  3.42911267]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 13.383791559272725}
done in step count: 33
reward sum = 0.5242213039031343
running average episode reward sum: 0.4858026125189122
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([12.65841578,  9.05026417,  2.55091819]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 1.1556424551590245}
episode index:3242
target Thresh 31.999999999999787
target distance 15.0
model initialize at round 3242
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([25.99585529, 28.96920635,  4.53281   ]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 19.182744403979587}
done in step count: 48
reward sum = 0.387259210630574
running average episode reward sum: 0.4857722260243429
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([14.90955693, 14.71246292,  3.9502418 ]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 1.1553775229729526}
episode index:3243
target Thresh 31.99999999999979
target distance 9.0
model initialize at round 3243
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 21.]), 'previousTarget': array([ 6., 21.]), 'currentState': array([15.       , 14.       ,  0.0906178]), 'targetState': array([ 6, 21], dtype=int32), 'currentDistance': 11.40175425099138}
done in step count: 30
reward sum = 0.5447661766285152
running average episode reward sum: 0.4857904115824823
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 21.]), 'previousTarget': array([ 6., 21.]), 'currentState': array([ 6.52300612, 20.14012249,  2.5782827 ]), 'targetState': array([ 6, 21], dtype=int32), 'currentDistance': 1.0064416241818988}
episode index:3244
target Thresh 31.99999999999979
target distance 11.0
model initialize at round 3244
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 26.]), 'previousTarget': array([19., 26.]), 'currentState': array([19.05071222, 15.47145935,  1.5244286 ]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 10.528662785192983}
done in step count: 22
reward sum = 0.6251545245395645
running average episode reward sum: 0.4858333589208358
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 26.]), 'previousTarget': array([19., 26.]), 'currentState': array([19.05342524, 25.1921971 ,  1.10154475]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 0.8095676514043819}
episode index:3245
target Thresh 31.999999999999794
target distance 8.0
model initialize at round 3245
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 14.]), 'previousTarget': array([19., 14.]), 'currentState': array([13.        ,  6.        ,  5.07633108]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 25
reward sum = 0.6116169380977776
running average episode reward sum: 0.48587210925329943
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 14.]), 'previousTarget': array([19., 14.]), 'currentState': array([18.18699411, 13.09738369,  1.10295838]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 1.21478178524074}
episode index:3246
target Thresh 31.999999999999794
target distance 12.0
model initialize at round 3246
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 19.]), 'previousTarget': array([20., 19.]), 'currentState': array([23.        ,  7.        ,  4.23888284]), 'targetState': array([20, 19], dtype=int32), 'currentDistance': 12.36931687685298}
done in step count: 35
reward sum = 0.5057157334817326
running average episode reward sum: 0.48587822062509756
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 19.]), 'previousTarget': array([20., 19.]), 'currentState': array([19.8274536 , 18.10735528,  1.42894406]), 'targetState': array([20, 19], dtype=int32), 'currentDistance': 0.9091682219025625}
episode index:3247
target Thresh 31.999999999999797
target distance 2.0
model initialize at round 3247
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  4.]), 'previousTarget': array([18.,  4.]), 'currentState': array([16.        ,  5.        ,  2.02208281]), 'targetState': array([18,  4], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 8
reward sum = 0.8600018344085851
running average episode reward sum: 0.4859934064667797
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  4.]), 'previousTarget': array([18.,  4.]), 'currentState': array([17.07017564,  4.68741223,  5.73073855]), 'targetState': array([18,  4], dtype=int32), 'currentDistance': 1.1563342514694948}
episode index:3248
target Thresh 31.9999999999998
target distance 16.0
model initialize at round 3248
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'previousTarget': array([22., 18.]), 'currentState': array([19.93208377,  1.97215516,  3.27817965]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 16.16069577276048}
done in step count: 41
reward sum = 0.42273553783428497
running average episode reward sum: 0.4859739365164465
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'previousTarget': array([22., 18.]), 'currentState': array([21.68307931, 17.10025293,  1.29362101]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 0.9539305606670371}
episode index:3249
target Thresh 31.9999999999998
target distance 17.0
model initialize at round 3249
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.44389646, 20.36638478]), 'previousTarget': array([15.46633605, 20.33935727]), 'currentState': array([26.91948942,  3.98619045,  3.06245631]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.3073824034364346
running average episode reward sum: 0.48591898527549876
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 21.]), 'previousTarget': array([15., 21.]), 'currentState': array([15.2412708 , 20.2163248 ,  1.81985107]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 0.8199746468504229}
episode index:3250
target Thresh 31.999999999999805
target distance 15.0
model initialize at round 3250
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'previousTarget': array([16., 23.]), 'currentState': array([17.        ,  8.        ,  2.80663228]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 15.03329637837291}
done in step count: 35
reward sum = 0.4760557811354189
running average episode reward sum: 0.485915951376963
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'previousTarget': array([16., 23.]), 'currentState': array([16.4498777 , 22.08487488,  1.652408  ]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 1.0197273813519443}
episode index:3251
target Thresh 31.999999999999805
target distance 10.0
model initialize at round 3251
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 21.]), 'previousTarget': array([ 6., 21.]), 'currentState': array([15.96460586, 24.95105121,  3.99418327]), 'targetState': array([ 6, 21], dtype=int32), 'currentDistance': 10.719336530933473}
done in step count: 25
reward sum = 0.6310799310136778
running average episode reward sum: 0.4859605897470849
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 21.]), 'previousTarget': array([ 6., 21.]), 'currentState': array([ 6.76043665, 21.83529593,  3.35779951]), 'targetState': array([ 6, 21], dtype=int32), 'currentDistance': 1.1295942612266536}
episode index:3252
target Thresh 31.999999999999808
target distance 15.0
model initialize at round 3252
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 12.]), 'previousTarget': array([ 2., 12.]), 'currentState': array([16.7650978 ,  6.86209365,  3.5140028 ]), 'targetState': array([ 2, 12], dtype=int32), 'currentDistance': 15.633495920249542}
done in step count: 43
reward sum = 0.4582385065588789
running average episode reward sum: 0.4859520677418011
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 12.]), 'previousTarget': array([ 2., 12.]), 'currentState': array([ 2.976838  , 12.11806857,  2.99060307]), 'targetState': array([ 2, 12], dtype=int32), 'currentDistance': 0.9839474884346657}
episode index:3253
target Thresh 31.999999999999808
target distance 19.0
model initialize at round 3253
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3.23313766, 7.08589282]), 'previousTarget': array([3.23313766, 7.08589282]), 'currentState': array([22.        , 14.        ,  0.97756231]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 20.0}
done in step count: 72
reward sum = 0.24613456180534743
running average episode reward sum: 0.48587836844679916
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.959789  , 7.25307241, 3.52813319]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.9925928535094629}
episode index:3254
target Thresh 31.99999999999981
target distance 5.0
model initialize at round 3254
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 14.]), 'previousTarget': array([27., 14.]), 'currentState': array([21.98487774,  9.07762306,  1.57169834]), 'targetState': array([27, 14], dtype=int32), 'currentDistance': 7.027179093988229}
done in step count: 15
reward sum = 0.7494165976257288
running average episode reward sum: 0.48595933257250695
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 14.]), 'previousTarget': array([27., 14.]), 'currentState': array([26.12273264, 13.22752049,  0.8182814 ]), 'targetState': array([27, 14], dtype=int32), 'currentDistance': 1.1688980328770877}
episode index:3255
target Thresh 31.99999999999981
target distance 21.0
model initialize at round 3255
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.58173352, 21.83071558]), 'previousTarget': array([13.58173352, 21.83071558]), 'currentState': array([27.        ,  7.        ,  0.94363803]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 76
reward sum = 0.15427166993128216
running average episode reward sum: 0.4858574628972485
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 28.]), 'previousTarget': array([ 8., 28.]), 'currentState': array([ 8.77061056, 27.42174238,  2.38467398]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 0.9634430480858581}
episode index:3256
target Thresh 31.999999999999815
target distance 20.0
model initialize at round 3256
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.49855525, 13.06188447]), 'previousTarget': array([15.37929463, 13.13411708]), 'currentState': array([ 2.2079935 , 28.00715023,  6.26378706]), 'targetState': array([20,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 75
reward sum = 0.21634665068847092
running average episode reward sum: 0.48577471472033457
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  8.]), 'previousTarget': array([20.,  8.]), 'currentState': array([19.05619531,  8.71758708,  5.47741798]), 'targetState': array([20,  8], dtype=int32), 'currentDistance': 1.1856215733083577}
episode index:3257
target Thresh 31.999999999999815
target distance 13.0
model initialize at round 3257
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([22.67171591, 25.27650064,  2.45711922]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 13.207453637639002}
done in step count: 30
reward sum = 0.5496675473897853
running average episode reward sum: 0.4857943257800858
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([10.84025873, 28.73326287,  2.90133655]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 0.8815800740226742}
episode index:3258
target Thresh 31.99999999999982
target distance 18.0
model initialize at round 3258
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.71362155,  7.04460798]), 'previousTarget': array([10.71272322,  7.05181363]), 'currentState': array([24.02089326, 21.97499712,  5.15598345]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 71
reward sum = 0.23956877666143392
running average episode reward sum: 0.48571877329493124
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 4.]), 'previousTarget': array([8., 4.]), 'currentState': array([8.60894044, 4.97761639, 4.00562816]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 1.151756162755909}
episode index:3259
target Thresh 31.99999999999982
target distance 8.0
model initialize at round 3259
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([10.        , 17.        ,  6.21804169]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 9.433981132056603}
done in step count: 21
reward sum = 0.6533414274788237
running average episode reward sum: 0.48577019128701215
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.71586586,  9.86419583,  5.13635038]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.9097069030364855}
episode index:3260
target Thresh 31.999999999999822
target distance 12.0
model initialize at round 3260
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 26.]), 'previousTarget': array([22., 26.]), 'currentState': array([11.23294168, 14.23729047,  0.72594523]), 'targetState': array([22, 26], dtype=int32), 'currentDistance': 15.946500571818092}
done in step count: 38
reward sum = 0.47929020235207886
running average episode reward sum: 0.48576820416989014
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 26.]), 'previousTarget': array([22., 26.]), 'currentState': array([21.14620081, 25.31109573,  0.69615415]), 'targetState': array([22, 26], dtype=int32), 'currentDistance': 1.0970698046236211}
episode index:3261
target Thresh 31.999999999999822
target distance 18.0
model initialize at round 3261
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8.25033394, 3.72044288]), 'previousTarget': array([8.29018892, 3.78641543]), 'currentState': array([20.00824865, 19.89917712,  4.75381099]), 'targetState': array([7, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.30855114790673854
running average episode reward sum: 0.4857138764395826
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 2.]), 'previousTarget': array([7., 2.]), 'currentState': array([7.58020861, 2.98735898, 4.30596135]), 'targetState': array([7, 2], dtype=int32), 'currentDistance': 1.1452160503051902}
episode index:3262
target Thresh 31.999999999999826
target distance 12.0
model initialize at round 3262
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 23.]), 'previousTarget': array([13., 23.]), 'currentState': array([ 6.0675595 , 11.00104863,  0.26802033]), 'targetState': array([13, 23], dtype=int32), 'currentDistance': 13.857617590240096}
done in step count: 31
reward sum = 0.514256992491277
running average episode reward sum: 0.4857226239468004
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 23.]), 'previousTarget': array([13., 23.]), 'currentState': array([12.83535757, 22.20797374,  1.18108816]), 'targetState': array([13, 23], dtype=int32), 'currentDistance': 0.808957808341196}
episode index:3263
target Thresh 31.999999999999826
target distance 17.0
model initialize at round 3263
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.81247575, 23.67106501]), 'previousTarget': array([23.70729398, 23.56399985]), 'currentState': array([10.11984157,  9.09328159,  0.70450666]), 'targetState': array([26, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 70
reward sum = 0.29574494851703936
running average episode reward sum: 0.4856644200021222
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 26.]), 'previousTarget': array([26., 26.]), 'currentState': array([25.11526014, 25.01604096,  0.79366865]), 'targetState': array([26, 26], dtype=int32), 'currentDistance': 1.323230898017057}
episode index:3264
target Thresh 31.99999999999983
target distance 16.0
model initialize at round 3264
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 11.]), 'previousTarget': array([18., 11.]), 'currentState': array([ 2.04496402, 20.90353469,  5.15852366]), 'targetState': array([18, 11], dtype=int32), 'currentDistance': 18.77879582391209}
done in step count: 47
reward sum = 0.39894256003176065
running average episode reward sum: 0.4856378589424069
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 11.]), 'previousTarget': array([18., 11.]), 'currentState': array([17.02830654, 11.33020306,  5.79266538]), 'targetState': array([18, 11], dtype=int32), 'currentDistance': 1.0262661676910645}
episode index:3265
target Thresh 31.99999999999983
target distance 15.0
model initialize at round 3265
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 2.]), 'previousTarget': array([6., 2.]), 'currentState': array([18.99028321, 17.05315442,  1.97662288]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 19.88328232290999}
done in step count: 54
reward sum = 0.3294980689675429
running average episode reward sum: 0.48559005129085303
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 2.]), 'previousTarget': array([6., 2.]), 'currentState': array([6.88685906, 2.87161277, 4.11461196]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 1.2434740885456914}
episode index:3266
target Thresh 31.999999999999833
target distance 17.0
model initialize at round 3266
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4.43600015, 7.29270602]), 'previousTarget': array([4.43600015, 7.29270602]), 'currentState': array([19.       , 21.       ,  2.0273844]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.14241260651304602
running average episode reward sum: 0.4853978251941882
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([10.73040296, 17.20686282,  3.9464253 ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 15.007579277558698}
episode index:3267
target Thresh 31.999999999999833
target distance 23.0
model initialize at round 3267
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.34227418,  7.10713736]), 'previousTarget': array([16.25132013,  7.26830308]), 'currentState': array([ 7.1097116 , 24.84860821,  5.23344302]), 'targetState': array([19,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.23857703623353288
running average episode reward sum: 0.48532229863697873
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  2.]), 'previousTarget': array([19.,  2.]), 'currentState': array([18.20849233,  2.75502082,  5.08153146]), 'targetState': array([19,  2], dtype=int32), 'currentDistance': 1.0938650861091779}
episode index:3268
target Thresh 31.999999999999837
target distance 8.0
model initialize at round 3268
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([17.        , 22.        ,  1.79422727]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 8.54400374531753}
done in step count: 23
reward sum = 0.6381732558138091
running average episode reward sum: 0.4853690563479536
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([14.32877147, 14.8745468 ,  4.27532497]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 0.9343033662855554}
episode index:3269
target Thresh 31.999999999999837
target distance 12.0
model initialize at round 3269
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 17.]), 'previousTarget': array([ 4., 17.]), 'currentState': array([7.       , 5.       , 0.9406454]), 'targetState': array([ 4, 17], dtype=int32), 'currentDistance': 12.369316876852983}
done in step count: 30
reward sum = 0.5592630845446345
running average episode reward sum: 0.48539165391009326
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 17.]), 'previousTarget': array([ 4., 17.]), 'currentState': array([ 4.09795644, 16.13987833,  2.11622757]), 'targetState': array([ 4, 17], dtype=int32), 'currentDistance': 0.8656816713477117}
episode index:3270
target Thresh 31.99999999999984
target distance 13.0
model initialize at round 3270
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 16.]), 'previousTarget': array([ 5., 16.]), 'currentState': array([4.22229841, 3.4442503 , 1.30605584]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 12.57981201752995}
done in step count: 29
reward sum = 0.5573118930945151
running average episode reward sum: 0.4854136411431059
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 16.]), 'previousTarget': array([ 5., 16.]), 'currentState': array([ 4.68913686, 15.22845164,  1.4194972 ]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 0.8318189492761967}
episode index:3271
target Thresh 31.99999999999984
target distance 17.0
model initialize at round 3271
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 25.]), 'previousTarget': array([16., 25.]), 'currentState': array([15.02996881,  7.99171642,  6.26601171]), 'targetState': array([16, 25], dtype=int32), 'currentDistance': 17.035922953437897}
done in step count: 38
reward sum = 0.4094154697274591
running average episode reward sum: 0.48539041431810115
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 25.]), 'previousTarget': array([16., 25.]), 'currentState': array([16.33790937, 24.17695155,  1.44934004]), 'targetState': array([16, 25], dtype=int32), 'currentDistance': 0.889714273427026}
episode index:3272
target Thresh 31.999999999999844
target distance 14.0
model initialize at round 3272
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 18.]), 'previousTarget': array([ 9., 18.]), 'currentState': array([22.87473847, 27.02618182,  3.13848373]), 'targetState': array([ 9, 18], dtype=int32), 'currentDistance': 16.552351063441844}
done in step count: 41
reward sum = 0.45920940791160847
running average episode reward sum: 0.48538241523273407
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 18.]), 'previousTarget': array([ 9., 18.]), 'currentState': array([ 9.99233422, 18.58821322,  3.82493813]), 'targetState': array([ 9, 18], dtype=int32), 'currentDistance': 1.1535692435586864}
episode index:3273
target Thresh 31.999999999999844
target distance 10.0
model initialize at round 3273
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 22.]), 'previousTarget': array([19., 22.]), 'currentState': array([ 9.41634076, 18.01326329,  0.11983373]), 'targetState': array([19, 22], dtype=int32), 'currentDistance': 10.379816668572039}
done in step count: 23
reward sum = 0.6416573113253339
running average episode reward sum: 0.48543014733294565
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 22.]), 'previousTarget': array([19., 22.]), 'currentState': array([18.05728652, 21.76002693,  0.44903237]), 'targetState': array([19, 22], dtype=int32), 'currentDistance': 0.9727773526942433}
episode index:3274
target Thresh 31.999999999999844
target distance 14.0
model initialize at round 3274
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 16.]), 'previousTarget': array([ 8., 16.]), 'currentState': array([21.86185072,  2.16624115,  2.34376262]), 'targetState': array([ 8, 16], dtype=int32), 'currentDistance': 19.583763415808864}
done in step count: 50
reward sum = 0.3772379117489233
running average episode reward sum: 0.48539711153582077
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 16.]), 'previousTarget': array([ 8., 16.]), 'currentState': array([ 8.51410428, 15.18308853,  2.21338695]), 'targetState': array([ 8, 16], dtype=int32), 'currentDistance': 0.9652189125455916}
episode index:3275
target Thresh 31.999999999999847
target distance 9.0
model initialize at round 3275
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 28.]), 'previousTarget': array([25., 28.]), 'currentState': array([16.21557091, 23.28837421,  0.8699964 ]), 'targetState': array([25, 28], dtype=int32), 'currentDistance': 9.968230132671408}
done in step count: 21
reward sum = 0.6550776063295997
running average episode reward sum: 0.4854489065586516
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 28.]), 'previousTarget': array([25., 28.]), 'currentState': array([24.06780886, 27.7519577 ,  0.31865689]), 'targetState': array([25, 28], dtype=int32), 'currentDistance': 0.9646270236610719}
episode index:3276
target Thresh 31.999999999999847
target distance 15.0
model initialize at round 3276
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 21.]), 'previousTarget': array([24., 21.]), 'currentState': array([13.24170508,  6.16595673,  0.64242144]), 'targetState': array([24, 21], dtype=int32), 'currentDistance': 18.324566825951074}
done in step count: 48
reward sum = 0.4114803768853882
running average episode reward sum: 0.48542633453250783
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 21.]), 'previousTarget': array([24., 21.]), 'currentState': array([23.64785561, 20.03266422,  0.99692893]), 'targetState': array([24, 21], dtype=int32), 'currentDistance': 1.0294387750693574}
episode index:3277
target Thresh 31.99999999999985
target distance 12.0
model initialize at round 3277
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([14.        , 16.        ,  6.22008491]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 16.278820596099706}
done in step count: 38
reward sum = 0.413167135495708
running average episode reward sum: 0.48540429084762776
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.65676958, 5.9800882 , 3.94393076]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 1.1797962385724272}
episode index:3278
target Thresh 31.99999999999985
target distance 8.0
model initialize at round 3278
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([20.        , 18.        ,  2.91659501]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 20
reward sum = 0.6682427175326523
running average episode reward sum: 0.48546005127052655
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([19.1706212 , 10.85862376,  4.49991279]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 0.875412110535807}
episode index:3279
target Thresh 31.999999999999854
target distance 20.0
model initialize at round 3279
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.60979441, 18.79160197]), 'previousTarget': array([15.49390095, 18.61737619]), 'currentState': array([3.06296311, 3.21671757, 1.19214299]), 'targetState': array([19, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 87
reward sum = 0.23086028550483093
running average episode reward sum: 0.4853824293907199
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 23.]), 'previousTarget': array([19., 23.]), 'currentState': array([18.06835547, 22.06344515,  0.91075855]), 'targetState': array([19, 23], dtype=int32), 'currentDistance': 1.3210210102803837}
episode index:3280
target Thresh 31.999999999999854
target distance 12.0
model initialize at round 3280
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  9.]), 'previousTarget': array([24.,  9.]), 'currentState': array([12.        , 13.        ,  4.88214159]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 12.649110640673518}
done in step count: 33
reward sum = 0.544760996966179
running average episode reward sum: 0.48540052709494896
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  9.]), 'previousTarget': array([24.,  9.]), 'currentState': array([23.03308339,  9.57957197,  5.94737552]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 1.1273115760506591}
episode index:3281
target Thresh 31.999999999999854
target distance 13.0
model initialize at round 3281
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 21.]), 'previousTarget': array([10., 21.]), 'currentState': array([3.88691074, 8.11274817, 2.10963464]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 14.263629273315736}
done in step count: 32
reward sum = 0.505228877454708
running average episode reward sum: 0.4854065686398484
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 21.]), 'previousTarget': array([10., 21.]), 'currentState': array([ 9.37719628, 20.13361227,  1.12944158]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 1.0670108550963397}
episode index:3282
target Thresh 31.999999999999858
target distance 14.0
model initialize at round 3282
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'previousTarget': array([11., 21.]), 'currentState': array([24.91831135, 15.90072775,  3.77135468]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 14.823021564893601}
done in step count: 34
reward sum = 0.48859363129549127
running average episode reward sum: 0.4854075394173859
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'previousTarget': array([11., 21.]), 'currentState': array([11.8566217 , 20.91348979,  2.70209767]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 0.8609789463542318}
episode index:3283
target Thresh 31.999999999999858
target distance 25.0
model initialize at round 3283
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.82595172, 23.09924659]), 'previousTarget': array([12.79936077, 22.98401917]), 'currentState': array([12.11610731,  3.11184754,  0.99040714]), 'targetState': array([13, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 75
reward sum = 0.24254707736749392
running average episode reward sum: 0.4853335867797337
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 28.]), 'previousTarget': array([13., 28.]), 'currentState': array([13.40482039, 27.08019588,  1.80875341]), 'targetState': array([13, 28], dtype=int32), 'currentDistance': 1.004947348785231}
episode index:3284
target Thresh 31.99999999999986
target distance 13.0
model initialize at round 3284
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([16.        , 19.        ,  5.99470857]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 13.601470508735444}
done in step count: 32
reward sum = 0.5024046000903243
running average episode reward sum: 0.4853387834352316
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([12.17806989,  6.86553136,  4.34395818]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 0.8836591102819902}
episode index:3285
target Thresh 31.99999999999986
target distance 14.0
model initialize at round 3285
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 25.]), 'previousTarget': array([ 9., 25.]), 'currentState': array([23.00069129, 25.99914233,  5.13828373]), 'targetState': array([ 9, 25], dtype=int32), 'currentDistance': 14.03629730244936}
done in step count: 33
reward sum = 0.48304523799956134
running average episode reward sum: 0.48533808546035767
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 25.]), 'previousTarget': array([ 9., 25.]), 'currentState': array([ 9.87870102, 25.66900792,  3.30672701]), 'targetState': array([ 9, 25], dtype=int32), 'currentDistance': 1.1043944368472747}
episode index:3286
target Thresh 31.99999999999986
target distance 7.0
model initialize at round 3286
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 21.]), 'previousTarget': array([19., 21.]), 'currentState': array([25.74221889, 20.90151093,  3.37643248]), 'targetState': array([19, 21], dtype=int32), 'currentDistance': 6.742938206557653}
done in step count: 14
reward sum = 0.7582252885976836
running average episode reward sum: 0.4854211056012574
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 21.]), 'previousTarget': array([19., 21.]), 'currentState': array([19.81545201, 21.53325725,  3.34738306]), 'targetState': array([19, 21], dtype=int32), 'currentDistance': 0.9743332493701464}
episode index:3287
target Thresh 31.999999999999865
target distance 20.0
model initialize at round 3287
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.97504678, 14.99875234]), 'previousTarget': array([25.97504678, 14.99875234]), 'currentState': array([ 6.        , 14.        ,  3.92670763]), 'targetState': array([26, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.28495226177126454
running average episode reward sum: 0.4853601357582434
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 15.]), 'previousTarget': array([26., 15.]), 'currentState': array([25.18867042, 14.72936566,  0.09428301]), 'targetState': array([26, 15], dtype=int32), 'currentDistance': 0.855276932244863}
episode index:3288
target Thresh 31.999999999999865
target distance 9.0
model initialize at round 3288
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 22.]), 'previousTarget': array([23., 22.]), 'currentState': array([17.99386952, 13.02126911,  1.67295596]), 'targetState': array([23, 22], dtype=int32), 'currentDistance': 10.280026791839791}
done in step count: 21
reward sum = 0.6477612311109255
running average episode reward sum: 0.48540951280152483
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 22.]), 'previousTarget': array([23., 22.]), 'currentState': array([22.61984638, 21.02127229,  1.15102875]), 'targetState': array([23, 22], dtype=int32), 'currentDistance': 1.049964140825495}
episode index:3289
target Thresh 31.99999999999987
target distance 6.0
model initialize at round 3289
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 25.]), 'previousTarget': array([27., 25.]), 'currentState': array([22.00205766, 18.98965124,  5.15507331]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 7.816886838551977}
done in step count: 18
reward sum = 0.6785106091046497
running average episode reward sum: 0.4854682061438662
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 25.]), 'previousTarget': array([27., 25.]), 'currentState': array([26.19600094, 24.08429569,  1.19250702]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 1.218576579470555}
episode index:3290
target Thresh 31.99999999999987
target distance 10.0
model initialize at round 3290
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 26.]), 'previousTarget': array([11., 26.]), 'currentState': array([14.9685535 , 16.00208776,  2.90055707]), 'targetState': array([11, 26], dtype=int32), 'currentDistance': 10.75674979071475}
done in step count: 25
reward sum = 0.6195269085473718
running average episode reward sum: 0.48550894108838266
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 26.]), 'previousTarget': array([11., 26.]), 'currentState': array([11.88511745, 25.03496443,  2.02985772]), 'targetState': array([11, 26], dtype=int32), 'currentDistance': 1.3094756819113977}
episode index:3291
target Thresh 31.99999999999987
target distance 13.0
model initialize at round 3291
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'previousTarget': array([25.,  3.]), 'currentState': array([13.97990577, 16.05204986,  1.71443641]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 17.082110013301115}
done in step count: 48
reward sum = 0.3769966135581743
running average episode reward sum: 0.4854759786559616
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'previousTarget': array([25.,  3.]), 'currentState': array([24.37969491,  3.768306  ,  5.49993067]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 0.9874576019192585}
episode index:3292
target Thresh 31.999999999999872
target distance 4.0
model initialize at round 3292
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 25.]), 'previousTarget': array([ 9., 25.]), 'currentState': array([10.0261751 , 29.02523256,  0.53734088]), 'targetState': array([ 9, 25], dtype=int32), 'currentDistance': 4.15397790848701}
done in step count: 12
reward sum = 0.7812853790260549
running average episode reward sum: 0.48556580841617114
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 25.]), 'previousTarget': array([ 9., 25.]), 'currentState': array([ 9.05758148, 25.86844498,  4.41755003]), 'targetState': array([ 9, 25], dtype=int32), 'currentDistance': 0.8703518330157378}
episode index:3293
target Thresh 31.999999999999872
target distance 15.0
model initialize at round 3293
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 14.]), 'previousTarget': array([27., 14.]), 'currentState': array([12.        , 12.        ,  2.47970012]), 'targetState': array([27, 14], dtype=int32), 'currentDistance': 15.132745950421556}
done in step count: 38
reward sum = 0.4393837673985801
running average episode reward sum: 0.48555178836728907
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 14.]), 'previousTarget': array([27., 14.]), 'currentState': array([26.04303725, 13.80008269,  0.19399011]), 'targetState': array([27, 14], dtype=int32), 'currentDistance': 0.9776219313839978}
episode index:3294
target Thresh 31.999999999999872
target distance 3.0
model initialize at round 3294
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 15.]), 'previousTarget': array([ 8., 15.]), 'currentState': array([11.        , 13.        ,  4.41110754]), 'targetState': array([ 8, 15], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 10
reward sum = 0.822252536282222
running average episode reward sum: 0.48565397372325714
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 15.]), 'previousTarget': array([ 8., 15.]), 'currentState': array([ 8.90180732, 14.51283199,  2.55723977]), 'targetState': array([ 8, 15], dtype=int32), 'currentDistance': 1.0249824936367609}
episode index:3295
target Thresh 31.999999999999876
target distance 13.0
model initialize at round 3295
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  3.]), 'previousTarget': array([22.,  3.]), 'currentState': array([9.04816791, 8.9051731 , 5.41498995]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 14.234501180604033}
done in step count: 38
reward sum = 0.494520162559199
running average episode reward sum: 0.4856566637077341
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  3.]), 'previousTarget': array([22.,  3.]), 'currentState': array([21.09921069,  3.33538148,  6.01570806]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 0.9611982717145447}
episode index:3296
target Thresh 31.999999999999876
target distance 8.0
model initialize at round 3296
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 19.]), 'previousTarget': array([19., 19.]), 'currentState': array([27.        , 17.        ,  1.04339802]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 8.246211251235321}
done in step count: 19
reward sum = 0.6633777710970193
running average episode reward sum: 0.48571056759229253
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 19.]), 'previousTarget': array([19., 19.]), 'currentState': array([19.76879992, 18.71943967,  3.10744416]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 0.8183931871465131}
episode index:3297
target Thresh 31.999999999999876
target distance 13.0
model initialize at round 3297
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([ 3.0472311 , 17.06037996,  0.66913572]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 13.10229817968034}
done in step count: 31
reward sum = 0.4936785968013612
running average episode reward sum: 0.48571298361085197
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.83860252, 4.82128333, 4.61496237]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.8369919045494013}
episode index:3298
target Thresh 31.99999999999988
target distance 13.0
model initialize at round 3298
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 16.]), 'previousTarget': array([19., 16.]), 'currentState': array([6.33472863, 8.14568397, 0.32127897]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 14.902999000833479}
done in step count: 33
reward sum = 0.5036970984070768
running average episode reward sum: 0.4857184349945428
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 16.]), 'previousTarget': array([19., 16.]), 'currentState': array([18.17967336, 15.57862958,  0.7223447 ]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 0.9222195123959891}
episode index:3299
target Thresh 31.99999999999988
target distance 12.0
model initialize at round 3299
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([ 5.36585575, 22.76276348,  5.86158133]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 11.95768265646982}
done in step count: 27
reward sum = 0.5882050349457556
running average episode reward sum: 0.4857494915399826
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([16.0597999 , 20.3613165 ,  6.07483847]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 1.0072367376032563}
episode index:3300
target Thresh 31.99999999999988
target distance 19.0
model initialize at round 3300
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.07741869, 14.43727342]), 'previousTarget': array([24.07475678, 14.43827311]), 'currentState': array([ 6.00464215, 23.00317957,  0.3979654 ]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.3290406779732585
running average episode reward sum: 0.485702018406518
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 14.]), 'previousTarget': array([25., 14.]), 'currentState': array([24.12369988, 14.27161678,  6.05277332]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 0.9174298709686525}
episode index:3301
target Thresh 31.999999999999883
target distance 17.0
model initialize at round 3301
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 11.]), 'previousTarget': array([24., 11.]), 'currentState': array([18.07691212, 28.02225323,  0.05053073]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 18.02332031377662}
done in step count: 42
reward sum = 0.3905318122304289
running average episode reward sum: 0.48567319641797285
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 11.]), 'previousTarget': array([24., 11.]), 'currentState': array([23.23263468, 11.77730813,  5.46095813]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 1.0922716942904014}
episode index:3302
target Thresh 31.999999999999883
target distance 12.0
model initialize at round 3302
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 13.]), 'previousTarget': array([25., 13.]), 'currentState': array([12.97933215, 17.98042692,  4.15229225]), 'targetState': array([25, 13], dtype=int32), 'currentDistance': 13.011575913506707}
done in step count: 29
reward sum = 0.511096237492205
running average episode reward sum: 0.4856808933725821
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 13.]), 'previousTarget': array([25., 13.]), 'currentState': array([2.42291369e+01, 1.31835873e+01, 1.12047044e-02]), 'targetState': array([25, 13], dtype=int32), 'currentDistance': 0.7924230423603452}
episode index:3303
target Thresh 31.999999999999883
target distance 11.0
model initialize at round 3303
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 24.]), 'previousTarget': array([25., 24.]), 'currentState': array([26.97893657, 12.92628891,  4.21731663]), 'targetState': array([25, 24], dtype=int32), 'currentDistance': 11.249145177546854}
done in step count: 30
reward sum = 0.5441564802718585
running average episode reward sum: 0.48569859179476704
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 24.]), 'previousTarget': array([25., 24.]), 'currentState': array([25.42194868, 23.25354648,  1.66348894]), 'targetState': array([25, 24], dtype=int32), 'currentDistance': 0.8574576078877693}
episode index:3304
target Thresh 31.999999999999886
target distance 9.0
model initialize at round 3304
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 17.]), 'previousTarget': array([22., 17.]), 'currentState': array([12.97921143, 16.04804655,  1.72665024]), 'targetState': array([22, 17], dtype=int32), 'currentDistance': 9.070878774537407}
done in step count: 20
reward sum = 0.6463259686586151
running average episode reward sum: 0.4857471931190829
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 17.]), 'previousTarget': array([22., 17.]), 'currentState': array([21.00577352, 16.63090136,  0.39635337]), 'targetState': array([22, 17], dtype=int32), 'currentDistance': 1.0605282216343663}
episode index:3305
target Thresh 31.999999999999886
target distance 14.0
model initialize at round 3305
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([15.92659985, 21.9875127 ,  3.56260562]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 15.674294279436342}
done in step count: 35
reward sum = 0.44935237734532163
running average episode reward sum: 0.4857361844028779
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([22.34529272,  8.869307  ,  5.3391907 ]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 1.0882721569629412}
episode index:3306
target Thresh 31.999999999999886
target distance 13.0
model initialize at round 3306
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 12.]), 'previousTarget': array([13., 12.]), 'currentState': array([10.01477679, 25.01334383,  0.48198473]), 'targetState': array([13, 12], dtype=int32), 'currentDistance': 13.351354810915408}
done in step count: 30
reward sum = 0.5058667646650921
running average episode reward sum: 0.4857422716663379
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 12.]), 'previousTarget': array([13., 12.]), 'currentState': array([12.5114852 , 12.94065054,  5.14605503]), 'targetState': array([13, 12], dtype=int32), 'currentDistance': 1.0599387471990638}
episode index:3307
target Thresh 31.99999999999989
target distance 8.0
model initialize at round 3307
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 21.]), 'previousTarget': array([10., 21.]), 'currentState': array([ 2.36831318, 22.66459908,  5.69530648]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 7.8111160447314765}
done in step count: 16
reward sum = 0.7158035037945413
running average episode reward sum: 0.48581181859261613
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 21.]), 'previousTarget': array([10., 21.]), 'currentState': array([ 9.07205473, 20.67212899,  0.11672325]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 0.9841655435565141}
episode index:3308
target Thresh 31.99999999999989
target distance 12.0
model initialize at round 3308
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([10.15871757, 26.69439829,  5.16270892]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 15.284298848095096}
done in step count: 33
reward sum = 0.4947460813945097
running average episode reward sum: 0.4858145185813746
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([19.01104706, 15.25693258,  5.4801472 ]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 1.0217838634062115}
episode index:3309
target Thresh 31.99999999999989
target distance 12.0
model initialize at round 3309
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 14.]), 'previousTarget': array([12., 14.]), 'currentState': array([ 7.95793402, 25.9464505 ,  4.28095335]), 'targetState': array([12, 14], dtype=int32), 'currentDistance': 12.61173964758366}
done in step count: 26
reward sum = 0.5565835772975523
running average episode reward sum: 0.48583589896165136
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 14.]), 'previousTarget': array([12., 14.]), 'currentState': array([11.58452739, 14.87134639,  5.25963203]), 'targetState': array([12, 14], dtype=int32), 'currentDistance': 0.9653300066649181}
episode index:3310
target Thresh 31.999999999999893
target distance 5.0
model initialize at round 3310
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([ 5.        , 14.        ,  1.12237102]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 5.8309518948453}
done in step count: 16
reward sum = 0.7373165956368386
running average episode reward sum: 0.48591185205638865
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([7.37080275, 9.8788468 , 5.35050197]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 1.080861169678706}
episode index:3311
target Thresh 31.999999999999893
target distance 9.0
model initialize at round 3311
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 16.]), 'previousTarget': array([24., 16.]), 'currentState': array([22.98899065, 24.64088294,  4.76195706]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 8.699827464924502}
done in step count: 18
reward sum = 0.6966490359441414
running average episode reward sum: 0.48597548043316635
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 16.]), 'previousTarget': array([24., 16.]), 'currentState': array([23.91308918, 16.80227497,  5.02428392]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 0.8069687802297424}
episode index:3312
target Thresh 31.999999999999893
target distance 6.0
model initialize at round 3312
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 21.]), 'previousTarget': array([13., 21.]), 'currentState': array([ 7.        , 15.        ,  3.24171829]), 'targetState': array([13, 21], dtype=int32), 'currentDistance': 8.48528137423857}
done in step count: 26
reward sum = 0.6287334267505947
running average episode reward sum: 0.486018570667491
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 21.]), 'previousTarget': array([13., 21.]), 'currentState': array([12.33196052, 20.13245284,  0.98437401]), 'targetState': array([13, 21], dtype=int32), 'currentDistance': 1.0949496888018817}
episode index:3313
target Thresh 31.999999999999897
target distance 16.0
model initialize at round 3313
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 12.]), 'previousTarget': array([19., 12.]), 'currentState': array([ 3.01452102, 15.97650866,  5.51854277]), 'targetState': array([19, 12], dtype=int32), 'currentDistance': 16.472648824897057}
done in step count: 44
reward sum = 0.441904349272769
running average episode reward sum: 0.4860052591945294
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 12.]), 'previousTarget': array([19., 12.]), 'currentState': array([18.019876  , 12.18106123,  6.15396721]), 'targetState': array([19, 12], dtype=int32), 'currentDistance': 0.9967076970563054}
episode index:3314
target Thresh 31.999999999999897
target distance 15.0
model initialize at round 3314
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  8.]), 'previousTarget': array([21.,  8.]), 'currentState': array([6.08707865, 2.22643467, 1.04469556]), 'targetState': array([21,  8], dtype=int32), 'currentDistance': 15.991537754612942}
done in step count: 39
reward sum = 0.4601429848826229
running average episode reward sum: 0.48599745760348506
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  8.]), 'previousTarget': array([21.,  8.]), 'currentState': array([20.21437147,  7.43470965,  0.56195766]), 'targetState': array([21,  8], dtype=int32), 'currentDistance': 0.9678663980037074}
episode index:3315
target Thresh 31.999999999999897
target distance 2.0
model initialize at round 3315
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([23.00468139,  4.00581696,  1.14564538]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 2.8276339031416997}
done in step count: 7
reward sum = 0.8743973466831639
running average episode reward sum: 0.4861145866412051
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([21.92395582,  5.89560559,  2.53186651]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 0.929834688994576}
episode index:3316
target Thresh 31.999999999999897
target distance 16.0
model initialize at round 3316
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  6.]), 'previousTarget': array([11.,  6.]), 'currentState': array([27.       , 15.       ,  4.8868041]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 18.35755975068582}
done in step count: 50
reward sum = 0.35956757653520743
running average episode reward sum: 0.4860764355980619
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  6.]), 'previousTarget': array([11.,  6.]), 'currentState': array([10.97803004,  6.8765023 ,  4.16712092]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 0.8767776031762082}
episode index:3317
target Thresh 31.9999999999999
target distance 7.0
model initialize at round 3317
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 15.]), 'previousTarget': array([11., 15.]), 'currentState': array([18.        ,  8.        ,  3.61218452]), 'targetState': array([11, 15], dtype=int32), 'currentDistance': 9.899494936611665}
done in step count: 21
reward sum = 0.6305933307065744
running average episode reward sum: 0.4861199910215425
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 15.]), 'previousTarget': array([11., 15.]), 'currentState': array([11.79174423, 14.52539121,  2.4117028 ]), 'targetState': array([11, 15], dtype=int32), 'currentDistance': 0.9230993614614353}
episode index:3318
target Thresh 31.9999999999999
target distance 6.0
model initialize at round 3318
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 12.]), 'previousTarget': array([22., 12.]), 'currentState': array([23.86943323, 17.54248136,  4.31467497]), 'targetState': array([22, 12], dtype=int32), 'currentDistance': 5.849263218172579}
done in step count: 13
reward sum = 0.785404338612583
running average episode reward sum: 0.4862101640699279
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 12.]), 'previousTarget': array([22., 12.]), 'currentState': array([22.35355466, 12.88271682,  4.517011  ]), 'targetState': array([22, 12], dtype=int32), 'currentDistance': 0.9508889986929339}
episode index:3319
target Thresh 31.9999999999999
target distance 14.0
model initialize at round 3319
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([ 7.00026366, 18.00597658,  1.30206516]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 15.65770381127615}
done in step count: 41
reward sum = 0.42281728552418546
running average episode reward sum: 0.486191069829402
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.34697514,  4.82760069,  5.36715618]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.0542126752385301}
episode index:3320
target Thresh 31.999999999999904
target distance 13.0
model initialize at round 3320
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  3.]), 'previousTarget': array([20.,  3.]), 'currentState': array([7.        , 5.        , 5.01811647]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 13.152946437965907}
done in step count: 30
reward sum = 0.5293859065974889
running average episode reward sum: 0.4862040764047613
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  3.]), 'previousTarget': array([20.,  3.]), 'currentState': array([19.20163434,  2.67387084,  6.25408517]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 0.8624082337191653}
episode index:3321
target Thresh 31.999999999999904
target distance 17.0
model initialize at round 3321
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.72000322, 26.45578864]), 'previousTarget': array([14.71414506, 26.43860471]), 'currentState': array([ 1.95664644, 11.05784321,  1.96147823]), 'targetState': array([16, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 68
reward sum = 0.26880206075620505
running average episode reward sum: 0.4861386332934884
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 28.]), 'previousTarget': array([16., 28.]), 'currentState': array([15.46433421, 27.00919028,  1.24269746]), 'targetState': array([16, 28], dtype=int32), 'currentDistance': 1.12633997256301}
episode index:3322
target Thresh 31.999999999999904
target distance 6.0
model initialize at round 3322
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 18.]), 'previousTarget': array([21., 18.]), 'currentState': array([26.99342912, 18.9850248 ,  4.11917511]), 'targetState': array([21, 18], dtype=int32), 'currentDistance': 6.073834573488642}
done in step count: 14
reward sum = 0.7714394336914824
running average episode reward sum: 0.4862244896884321
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 18.]), 'previousTarget': array([21., 18.]), 'currentState': array([21.85849516, 18.06398604,  3.20819273]), 'targetState': array([21, 18], dtype=int32), 'currentDistance': 0.8608763908888433}
episode index:3323
target Thresh 31.999999999999904
target distance 10.0
model initialize at round 3323
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 9.04385195, 20.73554924,  4.740363  ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 11.459016769951171}
done in step count: 26
reward sum = 0.6041192266089448
running average episode reward sum: 0.4862599574191543
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.46395591, 11.86949815,  4.42080937]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.9855364680231733}
episode index:3324
target Thresh 31.999999999999908
target distance 19.0
model initialize at round 3324
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6.50068701, 16.71214481]), 'previousTarget': array([ 6.49385478, 16.70632169]), 'currentState': array([23.00329942, 28.01098474,  1.5315057 ]), 'targetState': array([ 4, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 72
reward sum = 0.2244122432560232
running average episode reward sum: 0.48618120622692473
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 15.]), 'previousTarget': array([ 4., 15.]), 'currentState': array([ 4.68285621, 15.89849026,  3.82253049]), 'targetState': array([ 4, 15], dtype=int32), 'currentDistance': 1.1285288483311289}
episode index:3325
target Thresh 31.999999999999908
target distance 4.0
model initialize at round 3325
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  7.]), 'previousTarget': array([21.,  7.]), 'currentState': array([20.        , 11.        ,  3.22879374]), 'targetState': array([21,  7], dtype=int32), 'currentDistance': 4.123105625617661}
done in step count: 11
reward sum = 0.8134357250655523
running average episode reward sum: 0.48627959904678003
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  7.]), 'previousTarget': array([21.,  7.]), 'currentState': array([20.64166766,  7.8658423 ,  5.19790105]), 'targetState': array([21,  7], dtype=int32), 'currentDistance': 0.9370618716045723}
episode index:3326
target Thresh 31.999999999999908
target distance 16.0
model initialize at round 3326
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([12.81055841, 26.80416392,  4.00567733]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 18.09412989319596}
done in step count: 46
reward sum = 0.408120490089896
running average episode reward sum: 0.4862561066785934
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.75742528, 11.90330117,  4.53213609]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.9353050280404059}
episode index:3327
target Thresh 31.999999999999908
target distance 4.0
model initialize at round 3327
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([ 7.99908432, 11.99940938,  3.93601924]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 4.123850794569394}
done in step count: 11
reward sum = 0.7992506148802083
running average episode reward sum: 0.48635015550918287
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.01349327, 10.52449327,  6.21280471]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.0951265577955733}
episode index:3328
target Thresh 31.99999999999991
target distance 18.0
model initialize at round 3328
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.14213562, 19.14213562]), 'previousTarget': array([21.14213562, 19.14213562]), 'currentState': array([7.        , 5.        , 4.97129804]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 79
reward sum = 0.1897061996761003
running average episode reward sum: 0.4862610464806959
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 23.]), 'previousTarget': array([25., 23.]), 'currentState': array([24.20437435, 22.15499042,  1.04755108]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 1.1606297249731137}
episode index:3329
target Thresh 31.99999999999991
target distance 9.0
model initialize at round 3329
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 15.]), 'previousTarget': array([27., 15.]), 'currentState': array([22.0377091 ,  5.99337637,  0.07862299]), 'targetState': array([27, 15], dtype=int32), 'currentDistance': 10.283170727580032}
done in step count: 25
reward sum = 0.6225232970810037
running average episode reward sum: 0.48630196607547077
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 15.]), 'previousTarget': array([27., 15.]), 'currentState': array([26.44162934, 14.00924119,  1.29734848]), 'targetState': array([27, 15], dtype=int32), 'currentDistance': 1.1372690117169928}
episode index:3330
target Thresh 31.99999999999991
target distance 14.0
model initialize at round 3330
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 15.]), 'previousTarget': array([25., 15.]), 'currentState': array([21.      , 29.      ,  4.143543]), 'targetState': array([25, 15], dtype=int32), 'currentDistance': 14.560219778561036}
done in step count: 34
reward sum = 0.48516628682630536
running average episode reward sum: 0.4863016251330363
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 15.]), 'previousTarget': array([25., 15.]), 'currentState': array([24.73366253, 15.90317852,  5.30432004]), 'targetState': array([25, 15], dtype=int32), 'currentDistance': 0.9416300139342908}
episode index:3331
target Thresh 31.99999999999991
target distance 15.0
model initialize at round 3331
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 11.]), 'previousTarget': array([25., 11.]), 'currentState': array([16.        , 26.        ,  0.89118859]), 'targetState': array([25, 11], dtype=int32), 'currentDistance': 17.4928556845359}
done in step count: 42
reward sum = 0.38923448737744765
running average episode reward sum: 0.48627249333899203
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 11.]), 'previousTarget': array([25., 11.]), 'currentState': array([24.44362489, 11.9110743 ,  5.35234074]), 'targetState': array([25, 11], dtype=int32), 'currentDistance': 1.0675250124348912}
episode index:3332
target Thresh 31.999999999999915
target distance 23.0
model initialize at round 3332
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4.9299723 , 14.73567469]), 'previousTarget': array([ 5.07518824, 14.73259233]), 'currentState': array([24.84907959, 12.93868698,  3.49522442]), 'targetState': array([ 2, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.2903309996787914
running average episode reward sum: 0.4862137050120613
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 15.]), 'previousTarget': array([ 2., 15.]), 'currentState': array([ 2.92319896, 14.78425722,  3.06445529]), 'targetState': array([ 2, 15], dtype=int32), 'currentDistance': 0.9480723946692798}
episode index:3333
target Thresh 31.999999999999915
target distance 24.0
model initialize at round 3333
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.43389075, 17.92158089]), 'previousTarget': array([11.63557441, 17.80368799]), 'currentState': array([26.75339812,  5.06418248,  2.84307209]), 'targetState': array([ 3, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.25277769761017055
running average episode reward sum: 0.4859920519218927
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 25.]), 'previousTarget': array([ 3., 25.]), 'currentState': array([ 3.59830487, 23.93057112,  2.52098243]), 'targetState': array([ 3, 25], dtype=int32), 'currentDistance': 1.2254170072525414}
episode index:3334
target Thresh 31.999999999999915
target distance 15.0
model initialize at round 3334
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 21.]), 'previousTarget': array([19., 21.]), 'currentState': array([ 4.        , 23.        ,  5.44516087]), 'targetState': array([19, 21], dtype=int32), 'currentDistance': 15.132745950421556}
done in step count: 33
reward sum = 0.476579763672747
running average episode reward sum: 0.48598922964655566
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 21.]), 'previousTarget': array([19., 21.]), 'currentState': array([18.32217387, 21.24363003,  6.05818828]), 'targetState': array([19, 21], dtype=int32), 'currentDistance': 0.7202803966209658}
episode index:3335
target Thresh 31.999999999999915
target distance 5.0
model initialize at round 3335
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'previousTarget': array([19.,  3.]), 'currentState': array([24.03291992,  5.06441671,  1.30511284]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 5.439862066915594}
done in step count: 14
reward sum = 0.7543513359224003
running average episode reward sum: 0.4860696739230172
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'previousTarget': array([19.,  3.]), 'currentState': array([19.84280952,  3.69859158,  3.90483057]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 1.0946954325294753}
episode index:3336
target Thresh 31.99999999999992
target distance 7.0
model initialize at round 3336
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([3.        , 7.        , 2.93674302]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 21
reward sum = 0.6442503071615082
running average episode reward sum: 0.4861170759707363
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.14194305, 10.80019289,  0.56795239]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.8810133981019349}
episode index:3337
target Thresh 31.99999999999992
target distance 16.0
model initialize at round 3337
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.58992104, 19.49586917]), 'previousTarget': array([19.6118525, 19.52228  ]), 'currentState': array([6.96930846, 3.98071084, 3.4733955 ]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 61
reward sum = 0.26939906674304753
running average episode reward sum: 0.4860521514622798
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 20.]), 'previousTarget': array([20., 20.]), 'currentState': array([20.14922715, 19.07957579,  1.04279991]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 0.9324427471247768}
episode index:3338
target Thresh 31.99999999999992
target distance 16.0
model initialize at round 3338
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([ 8.02602812, 19.95700756,  5.4716976 ]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 17.846056900086904}
done in step count: 45
reward sum = 0.4201937238811734
running average episode reward sum: 0.48603242746480113
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([23.07370848, 12.56695891,  5.97471985]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 1.0860287204163754}
episode index:3339
target Thresh 31.99999999999992
target distance 25.0
model initialize at round 3339
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9.25566398, 5.77294527]), 'previousTarget': array([9.25566398, 5.77294527]), 'currentState': array([27.        , 15.        ,  1.63503408]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 99
reward sum = -0.14029433864194252
running average episode reward sum: 0.48584490448093687
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([10.75212929,  7.83750809,  3.72965547]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 10.520278883164487}
episode index:3340
target Thresh 31.99999999999992
target distance 14.0
model initialize at round 3340
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 21.]), 'previousTarget': array([ 6., 21.]), 'currentState': array([10.02001103,  7.01961556,  1.02385971]), 'targetState': array([ 6, 21], dtype=int32), 'currentDistance': 14.546877247796218}
done in step count: 37
reward sum = 0.4957453157124494
running average episode reward sum: 0.48584786778869843
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 21.]), 'previousTarget': array([ 6., 21.]), 'currentState': array([ 6.040394  , 20.01675993,  1.92647198]), 'targetState': array([ 6, 21], dtype=int32), 'currentDistance': 0.9840694645848727}
episode index:3341
target Thresh 31.999999999999922
target distance 5.0
model initialize at round 3341
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 25.]), 'previousTarget': array([11., 25.]), 'currentState': array([16.        , 24.        ,  4.40863454]), 'targetState': array([11, 25], dtype=int32), 'currentDistance': 5.099019513592785}
done in step count: 15
reward sum = 0.782065846470538
running average episode reward sum: 0.4859365027314518
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 25.]), 'previousTarget': array([11., 25.]), 'currentState': array([11.83723381, 24.48129731,  2.9140174 ]), 'targetState': array([11, 25], dtype=int32), 'currentDistance': 0.9848923434232693}
episode index:3342
target Thresh 31.999999999999922
target distance 18.0
model initialize at round 3342
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4.60887504, 13.16566852]), 'previousTarget': array([ 4.63557441, 13.19631201]), 'currentState': array([19.99762354, 25.94011252,  4.48306811]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 82
reward sum = 0.24159317141012004
running average episode reward sum: 0.48586341169605807
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.81290279, 11.37909428,  3.75991583]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.8969522986085546}
episode index:3343
target Thresh 31.999999999999922
target distance 10.0
model initialize at round 3343
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 24.]), 'previousTarget': array([18., 24.]), 'currentState': array([16.        , 14.        ,  5.80187032]), 'targetState': array([18, 24], dtype=int32), 'currentDistance': 10.198039027185569}
done in step count: 26
reward sum = 0.6035764624277713
running average episode reward sum: 0.4858986129672099
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 24.]), 'previousTarget': array([18., 24.]), 'currentState': array([17.81422644, 23.0260682 ,  1.43696674]), 'targetState': array([18, 24], dtype=int32), 'currentDistance': 0.9914912820709332}
episode index:3344
target Thresh 31.999999999999922
target distance 16.0
model initialize at round 3344
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 26.]), 'previousTarget': array([ 8., 26.]), 'currentState': array([24.        , 23.        ,  0.91456717]), 'targetState': array([ 8, 26], dtype=int32), 'currentDistance': 16.278820596099706}
done in step count: 43
reward sum = 0.4013229400724625
running average episode reward sum: 0.4858733287600665
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 26.]), 'previousTarget': array([ 8., 26.]), 'currentState': array([ 8.91310793, 26.76058507,  2.99060788]), 'targetState': array([ 8, 26], dtype=int32), 'currentDistance': 1.1883836650057242}
episode index:3345
target Thresh 31.999999999999925
target distance 13.0
model initialize at round 3345
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 19.]), 'previousTarget': array([17., 19.]), 'currentState': array([4.96164845, 6.0261807 , 2.29010582]), 'targetState': array([17, 19], dtype=int32), 'currentDistance': 17.69864105388045}
done in step count: 52
reward sum = 0.3836590048839939
running average episode reward sum: 0.4858427805461167
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 19.]), 'previousTarget': array([17., 19.]), 'currentState': array([16.13008464, 18.59731521,  0.67987735]), 'targetState': array([17, 19], dtype=int32), 'currentDistance': 0.9585967770988063}
episode index:3346
target Thresh 31.999999999999925
target distance 9.0
model initialize at round 3346
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 20.]), 'previousTarget': array([ 5., 20.]), 'currentState': array([10.75481088, 11.11477145,  2.58408791]), 'targetState': array([ 5, 20], dtype=int32), 'currentDistance': 10.586082123092233}
done in step count: 23
reward sum = 0.6368379146631666
running average episode reward sum: 0.485887894120696
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 20.]), 'previousTarget': array([ 5., 20.]), 'currentState': array([ 5.39781946, 19.03055064,  2.2363641 ]), 'targetState': array([ 5, 20], dtype=int32), 'currentDistance': 1.04789903540195}
episode index:3347
target Thresh 31.999999999999925
target distance 13.0
model initialize at round 3347
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([16.98859131, 11.9655778 ,  4.17158648]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 16.421574784396682}
done in step count: 49
reward sum = 0.40414909454024406
running average episode reward sum: 0.48586347990337814
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([ 7.27646093, 24.09113218,  1.97696456]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 0.9499849267956819}
episode index:3348
target Thresh 31.999999999999925
target distance 8.0
model initialize at round 3348
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  6.]), 'previousTarget': array([23.,  6.]), 'currentState': array([17.        , 14.        ,  3.88089782]), 'targetState': array([23,  6], dtype=int32), 'currentDistance': 10.0}
done in step count: 26
reward sum = 0.6181895205481142
running average episode reward sum: 0.48590299200867665
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  6.]), 'previousTarget': array([23.,  6.]), 'currentState': array([22.42807384,  6.81043539,  5.2276497 ]), 'targetState': array([23,  6], dtype=int32), 'currentDistance': 0.9919198835346514}
episode index:3349
target Thresh 31.999999999999925
target distance 13.0
model initialize at round 3349
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 21.]), 'previousTarget': array([ 6., 21.]), 'currentState': array([14.02212719,  8.04044663,  1.28628737]), 'targetState': array([ 6, 21], dtype=int32), 'currentDistance': 15.241540216305566}
done in step count: 36
reward sum = 0.48133498366187344
running average episode reward sum: 0.4859016284240955
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 21.]), 'previousTarget': array([ 6., 21.]), 'currentState': array([ 6.27064302, 20.06367256,  2.05931292]), 'targetState': array([ 6, 21], dtype=int32), 'currentDistance': 0.9746572338753152}
episode index:3350
target Thresh 31.99999999999993
target distance 26.0
model initialize at round 3350
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.80053053,  9.68768483]), 'previousTarget': array([15.80053053,  9.68768483]), 'currentState': array([21.        , 29.        ,  3.31548381]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 88
reward sum = 0.16581243227902337
running average episode reward sum: 0.48580610792390294
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.83755107,  3.80195035,  4.88590185]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.8182383593205359}
episode index:3351
target Thresh 31.99999999999993
target distance 18.0
model initialize at round 3351
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 14.]), 'previousTarget': array([ 2., 14.]), 'currentState': array([20.        , 10.        ,  1.95760441]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 18.439088914585774}
done in step count: 53
reward sum = 0.37242413985683864
running average episode reward sum: 0.48577228275443185
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 14.]), 'previousTarget': array([ 2., 14.]), 'currentState': array([ 2.78658671, 13.95756379,  3.01942177]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 0.7877305919035889}
episode index:3352
target Thresh 31.99999999999993
target distance 12.0
model initialize at round 3352
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 12.]), 'previousTarget': array([20., 12.]), 'currentState': array([ 8.23581314, 21.79099789,  5.55890992]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 15.305545796954153}
done in step count: 44
reward sum = 0.4888757777979665
running average episode reward sum: 0.4857732083419784
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 12.]), 'previousTarget': array([20., 12.]), 'currentState': array([19.23786611, 12.87937003,  5.71502664]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 1.163675094754611}
episode index:3353
target Thresh 31.99999999999993
target distance 14.0
model initialize at round 3353
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 21.]), 'previousTarget': array([15., 21.]), 'currentState': array([11.        ,  7.        ,  2.70787793]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 14.560219778561036}
done in step count: 38
reward sum = 0.48958693601816217
running average episode reward sum: 0.4857743454104567
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 21.]), 'previousTarget': array([15., 21.]), 'currentState': array([14.09584547, 20.05878807,  1.07605917]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 1.3051342122167853}
episode index:3354
target Thresh 31.99999999999993
target distance 12.0
model initialize at round 3354
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 18.]), 'previousTarget': array([23., 18.]), 'currentState': array([18.97538276,  5.99335166,  3.17686898]), 'targetState': array([23, 18], dtype=int32), 'currentDistance': 12.663220292477238}
done in step count: 34
reward sum = 0.5243504712188423
running average episode reward sum: 0.48578584351054865
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 18.]), 'previousTarget': array([23., 18.]), 'currentState': array([22.40650335, 17.09722541,  1.23997021]), 'targetState': array([23, 18], dtype=int32), 'currentDistance': 1.0803889304035703}
episode index:3355
target Thresh 31.999999999999932
target distance 6.0
model initialize at round 3355
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 15.]), 'previousTarget': array([19., 15.]), 'currentState': array([13.       , 15.       ,  4.1335752]), 'targetState': array([19, 15], dtype=int32), 'currentDistance': 5.999999999999999}
done in step count: 15
reward sum = 0.7275987642866671
running average episode reward sum: 0.4858578974201959
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 15.]), 'previousTarget': array([19., 15.]), 'currentState': array([18.25255764, 14.53035806,  0.13163731]), 'targetState': array([19, 15], dtype=int32), 'currentDistance': 0.8827421125122317}
episode index:3356
target Thresh 31.999999999999932
target distance 19.0
model initialize at round 3356
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([15.98622246,  3.30791102,  1.68097886]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 19.1124085295867}
done in step count: 58
reward sum = 0.370473693998002
running average episode reward sum: 0.4858235261948691
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([11.63900532, 21.10582209,  1.60021797]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 0.9642983398287986}
episode index:3357
target Thresh 31.999999999999932
target distance 13.0
model initialize at round 3357
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 23.]), 'previousTarget': array([20., 23.]), 'currentState': array([ 6.99377725, 10.00413133,  2.32542828]), 'targetState': array([20, 23], dtype=int32), 'currentDistance': 18.386256629792737}
done in step count: 53
reward sum = 0.3720695514327827
running average episode reward sum: 0.4857896506812413
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 23.]), 'previousTarget': array([20., 23.]), 'currentState': array([19.19909189, 22.56846307,  0.61896212]), 'targetState': array([20, 23], dtype=int32), 'currentDistance': 0.9097680601012603}
episode index:3358
target Thresh 31.999999999999932
target distance 15.0
model initialize at round 3358
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5.3700695, 27.6559677]), 'previousTarget': array([ 5.37889464, 27.64636501]), 'currentState': array([20.01811453, 14.03852386,  1.36086404]), 'targetState': array([ 5, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 70
reward sum = 0.3038547118850366
running average episode reward sum: 0.4857354872579617
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 28.]), 'previousTarget': array([ 5., 28.]), 'currentState': array([ 5.67376852, 27.04877613,  2.40288383]), 'targetState': array([ 5, 28], dtype=int32), 'currentDistance': 1.16567185380859}
episode index:3359
target Thresh 31.999999999999932
target distance 20.0
model initialize at round 3359
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 23.]), 'previousTarget': array([26.97504678, 23.00124766]), 'currentState': array([ 7.13205403, 23.93651919,  5.86375222]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 19.8900061691456}
done in step count: 60
reward sum = 0.3559445057238445
running average episode reward sum: 0.48569685898964793
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 23.]), 'previousTarget': array([27., 23.]), 'currentState': array([26.21344252, 23.20916682,  6.08840923]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 0.8138939946264204}
episode index:3360
target Thresh 31.999999999999936
target distance 7.0
model initialize at round 3360
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([10.        , 15.        ,  5.53484526]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 8.602325267042627}
done in step count: 21
reward sum = 0.6594324778142233
running average episode reward sum: 0.4857485506346419
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.83560354, 10.39542827,  3.68305445]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.9244440427315125}
episode index:3361
target Thresh 31.999999999999936
target distance 14.0
model initialize at round 3361
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 24.]), 'previousTarget': array([24., 24.]), 'currentState': array([10.        , 18.        ,  5.58788776]), 'targetState': array([24, 24], dtype=int32), 'currentDistance': 15.231546211727816}
done in step count: 41
reward sum = 0.4638816686024287
running average episode reward sum: 0.4857420465055425
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 24.]), 'previousTarget': array([24., 24.]), 'currentState': array([23.08523614, 23.84266642,  0.35383815]), 'targetState': array([24, 24], dtype=int32), 'currentDistance': 0.928195439210956}
episode index:3362
target Thresh 31.999999999999936
target distance 11.0
model initialize at round 3362
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([ 7.        , 15.        ,  1.68090621]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 14.212670403551895}
done in step count: 43
reward sum = 0.4512288483056092
running average episode reward sum: 0.4857317838834194
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.77378954,  4.89982888,  5.27416803]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.9278271324947487}
episode index:3363
target Thresh 31.999999999999936
target distance 19.0
model initialize at round 3363
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9.17592753, 17.56803761]), 'previousTarget': array([ 9.24510704, 17.51905368]), 'currentState': array([22.89135075,  3.01169309,  2.87947774]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.09698234255805771
running average episode reward sum: 0.4855585632750837
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'previousTarget': array([ 5., 22.]), 'currentState': array([11.31728282, 15.24288296,  2.30087847]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 9.250226640009817}
episode index:3364
target Thresh 31.999999999999936
target distance 20.0
model initialize at round 3364
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8.05442597, 5.98001713]), 'previousTarget': array([8.23112767, 6.10023299]), 'currentState': array([24.8367419 , 16.85907371,  3.81006186]), 'targetState': array([5, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 82
reward sum = 0.25893528757330697
running average episode reward sum: 0.48549121609062545
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 4.]), 'previousTarget': array([5., 4.]), 'currentState': array([5.88441509, 4.52224935, 3.58476751]), 'targetState': array([5, 4], dtype=int32), 'currentDistance': 1.0271000175559934}
episode index:3365
target Thresh 31.999999999999936
target distance 9.0
model initialize at round 3365
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 29.]), 'previousTarget': array([25., 29.]), 'currentState': array([16.95254151, 20.01742662,  2.56073961]), 'targetState': array([25, 29], dtype=int32), 'currentDistance': 12.060191236131748}
done in step count: 29
reward sum = 0.5557991103955366
running average episode reward sum: 0.4855121037597594
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 29.]), 'previousTarget': array([25., 29.]), 'currentState': array([24.04657656, 28.13555898,  0.92592864]), 'targetState': array([25, 29], dtype=int32), 'currentDistance': 1.2869632983762034}
episode index:3366
target Thresh 31.99999999999994
target distance 16.0
model initialize at round 3366
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 27.]), 'previousTarget': array([ 4., 27.]), 'currentState': array([12.92380055, 11.14412956,  2.05101554]), 'targetState': array([ 4, 27], dtype=int32), 'currentDistance': 18.194582814574513}
done in step count: 42
reward sum = 0.41831540120356636
running average episode reward sum: 0.4854921463191428
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 27.]), 'previousTarget': array([ 4., 27.]), 'currentState': array([ 4.52760863, 26.08991862,  2.22719053]), 'targetState': array([ 4, 27], dtype=int32), 'currentDistance': 1.0519595949847422}
episode index:3367
target Thresh 31.99999999999994
target distance 24.0
model initialize at round 3367
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.16189274, 13.5602359 ]), 'previousTarget': array([13.16189274, 13.5602359 ]), 'currentState': array([27.        , 28.        ,  5.93530123]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.22044381346529082
running average episode reward sum: 0.4852825453809645
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([8.05724587, 9.58421257, 3.98109286]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 6.902512159220926}
episode index:3368
target Thresh 31.99999999999994
target distance 11.0
model initialize at round 3368
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 15.]), 'previousTarget': array([ 8., 15.]), 'currentState': array([18.88248322,  4.42951281,  1.85610106]), 'targetState': array([ 8, 15], dtype=int32), 'currentDistance': 15.171145000328776}
done in step count: 37
reward sum = 0.481761315290609
running average episode reward sum: 0.4852815001954227
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 15.]), 'previousTarget': array([ 8., 15.]), 'currentState': array([ 8.8454197 , 14.9312633 ,  2.69930367]), 'targetState': array([ 8, 15], dtype=int32), 'currentDistance': 0.8482094107646424}
episode index:3369
target Thresh 31.99999999999994
target distance 9.0
model initialize at round 3369
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([ 6.06445046, 13.8987602 ,  5.06245071]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 9.135091065330718}
done in step count: 20
reward sum = 0.6686507600835092
running average episode reward sum: 0.48533591243871294
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.36042505, 5.79193096, 4.59182782]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.8700924474703398}
episode index:3370
target Thresh 31.99999999999994
target distance 13.0
model initialize at round 3370
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 22.]), 'previousTarget': array([21., 22.]), 'currentState': array([ 8.        , 19.        ,  2.50386667]), 'targetState': array([21, 22], dtype=int32), 'currentDistance': 13.341664064126334}
done in step count: 39
reward sum = 0.4837223620856177
running average episode reward sum: 0.4853354337824231
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 22.]), 'previousTarget': array([21., 22.]), 'currentState': array([20.04922872, 22.10408184,  0.28618628]), 'targetState': array([21, 22], dtype=int32), 'currentDistance': 0.9564512823243144}
episode index:3371
target Thresh 31.99999999999994
target distance 17.0
model initialize at round 3371
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7.13497444, 2.20859686]), 'previousTarget': array([7.13497444, 2.20859686]), 'currentState': array([18.        , 19.        ,  0.41353658]), 'targetState': array([7, 2], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 51
reward sum = 0.2993591878857985
running average episode reward sum: 0.485280280684589
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 2.]), 'previousTarget': array([7., 2.]), 'currentState': array([7.41767164, 2.92152473, 4.07245429]), 'targetState': array([7, 2], dtype=int32), 'currentDistance': 1.0117595671760466}
episode index:3372
target Thresh 31.999999999999943
target distance 18.0
model initialize at round 3372
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([6.16787065, 5.83494332, 5.48837372]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 18.23983627799502}
done in step count: 60
reward sum = 0.2966463737005186
running average episode reward sum: 0.48522435601604935
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([23.10816454,  1.67451947,  0.73403758]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 0.9493724550004529}
episode index:3373
target Thresh 31.999999999999943
target distance 14.0
model initialize at round 3373
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 13.]), 'previousTarget': array([ 7., 13.]), 'currentState': array([13.96006952, 27.01391488,  3.05877471]), 'targetState': array([ 7, 13], dtype=int32), 'currentDistance': 15.647120436988951}
done in step count: 47
reward sum = 0.3863951661517093
running average episode reward sum: 0.4851950646141927
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 13.]), 'previousTarget': array([ 7., 13.]), 'currentState': array([ 6.01565691, 13.01636223,  5.54829422]), 'targetState': array([ 7, 13], dtype=int32), 'currentDistance': 0.9844790711797043}
episode index:3374
target Thresh 31.999999999999943
target distance 8.0
model initialize at round 3374
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  4.]), 'previousTarget': array([18.,  4.]), 'currentState': array([10.25632237,  7.01229759,  6.16643441]), 'targetState': array([18,  4], dtype=int32), 'currentDistance': 8.308939764863755}
done in step count: 19
reward sum = 0.7062234386060583
running average episode reward sum: 0.4852605545027829
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  4.]), 'previousTarget': array([18.,  4.]), 'currentState': array([17.1109164 ,  4.51978137,  5.56650272]), 'targetState': array([18,  4], dtype=int32), 'currentDistance': 1.0298749006871917}
episode index:3375
target Thresh 31.999999999999943
target distance 7.0
model initialize at round 3375
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 18.]), 'previousTarget': array([14., 18.]), 'currentState': array([ 9.        , 11.        ,  1.93664026]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 8.602325267042627}
done in step count: 20
reward sum = 0.6750931938405789
running average episode reward sum: 0.4853167845499801
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 18.]), 'previousTarget': array([14., 18.]), 'currentState': array([13.22725652, 17.20238319,  1.00400475]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 1.1105516948079182}
episode index:3376
target Thresh 31.999999999999943
target distance 10.0
model initialize at round 3376
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([10.96628058,  7.28422246,  1.51608142]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 10.942333589376105}
done in step count: 24
reward sum = 0.6217447063290706
running average episode reward sum: 0.485357183697679
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([15.20426272, 16.1007483 ,  1.08752093]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 1.2007711902958338}
episode index:3377
target Thresh 31.999999999999943
target distance 11.0
model initialize at round 3377
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 22.]), 'previousTarget': array([ 6., 22.]), 'currentState': array([ 8.        , 11.        ,  0.42335909]), 'targetState': array([ 6, 22], dtype=int32), 'currentDistance': 11.180339887498949}
done in step count: 33
reward sum = 0.5631382881696461
running average episode reward sum: 0.48538020948349075
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 22.]), 'previousTarget': array([ 6., 22.]), 'currentState': array([ 6.29859653, 21.09513126,  2.18096686]), 'targetState': array([ 6, 22], dtype=int32), 'currentDistance': 0.9528626963041561}
episode index:3378
target Thresh 31.999999999999947
target distance 4.0
model initialize at round 3378
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 17.]), 'previousTarget': array([ 5., 17.]), 'currentState': array([ 9.0245251 , 20.97088654,  5.21414262]), 'targetState': array([ 5, 17], dtype=int32), 'currentDistance': 5.6537370114555285}
done in step count: 14
reward sum = 0.7635112039890459
running average episode reward sum: 0.48546252111252464
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 17.]), 'previousTarget': array([ 5., 17.]), 'currentState': array([ 5.07785104, 17.84301523,  3.82602913]), 'targetState': array([ 5, 17], dtype=int32), 'currentDistance': 0.8466023093721807}
episode index:3379
target Thresh 31.999999999999947
target distance 17.0
model initialize at round 3379
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 2.]), 'previousTarget': array([5., 2.]), 'currentState': array([22.09513452,  6.94542072,  5.5098238 ]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 17.796089749162537}
done in step count: 54
reward sum = 0.3637073195813405
running average episode reward sum: 0.48542649886355094
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 2.]), 'previousTarget': array([5., 2.]), 'currentState': array([5.87554612, 1.93056538, 3.34124316]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 0.8782950422362761}
episode index:3380
target Thresh 31.999999999999947
target distance 5.0
model initialize at round 3380
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 14.]), 'previousTarget': array([19., 14.]), 'currentState': array([13.98740121, 15.96609028,  4.59715274]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 5.384390184690231}
done in step count: 13
reward sum = 0.7795212530429196
running average episode reward sum: 0.48551348341077943
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 14.]), 'previousTarget': array([19., 14.]), 'currentState': array([18.04527061, 13.86936474,  0.01928549]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 0.96362533365955}
episode index:3381
target Thresh 31.999999999999947
target distance 15.0
model initialize at round 3381
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'previousTarget': array([10., 17.]), 'currentState': array([18.04391351,  2.08930801,  1.31862211]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 16.942056547431637}
done in step count: 70
reward sum = 0.3230421758604727
running average episode reward sum: 0.48546544340263326
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'previousTarget': array([10., 17.]), 'currentState': array([10.828935  , 16.22001169,  2.6243915 ]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 1.138206919724382}
episode index:3382
target Thresh 31.999999999999947
target distance 4.0
model initialize at round 3382
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 22.]), 'previousTarget': array([10., 22.]), 'currentState': array([13.99871703, 23.00156538,  2.46671513]), 'targetState': array([10, 22], dtype=int32), 'currentDistance': 4.122241032390113}
done in step count: 9
reward sum = 0.8499761896590535
running average episode reward sum: 0.4855731911845595
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 22.]), 'previousTarget': array([10., 22.]), 'currentState': array([10.96100866, 22.81909077,  3.89522904]), 'targetState': array([10, 22], dtype=int32), 'currentDistance': 1.2627142764493384}
episode index:3383
target Thresh 31.999999999999947
target distance 18.0
model initialize at round 3383
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.21295565, 19.27881227]), 'previousTarget': array([10.21295565, 19.27881227]), 'currentState': array([26.        ,  7.        ,  3.78977209]), 'targetState': array([ 8, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.263972526975691
running average episode reward sum: 0.4855077063547105
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 21.]), 'previousTarget': array([ 8., 21.]), 'currentState': array([ 8.63871339, 20.04844875,  2.55089451]), 'targetState': array([ 8, 21], dtype=int32), 'currentDistance': 1.1460386441436423}
episode index:3384
target Thresh 31.99999999999995
target distance 16.0
model initialize at round 3384
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 9.06453303, 26.62542715,  4.97881895]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 16.761042164956322}
done in step count: 47
reward sum = 0.4039521960221252
running average episode reward sum: 0.48548361314634053
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.87998368, 11.86214251,  4.37172562]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.2319338429197162}
episode index:3385
target Thresh 31.99999999999995
target distance 13.0
model initialize at round 3385
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  4.]), 'previousTarget': array([22.,  4.]), 'currentState': array([18.94014319, 16.89951171,  4.30789813]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 13.257455488336143}
done in step count: 31
reward sum = 0.5526117164369357
running average episode reward sum: 0.4855034383392793
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  4.]), 'previousTarget': array([22.,  4.]), 'currentState': array([21.35821067,  4.8889943 ,  5.04321244]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 1.0964508214533613}
episode index:3386
target Thresh 31.99999999999995
target distance 14.0
model initialize at round 3386
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 28.]), 'previousTarget': array([ 7., 28.]), 'currentState': array([21.0232339 , 25.00604692,  0.50681011]), 'targetState': array([ 7, 28], dtype=int32), 'currentDistance': 14.339276268653823}
done in step count: 42
reward sum = 0.44791080212120615
running average episode reward sum: 0.48549233924385027
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 28.]), 'previousTarget': array([ 7., 28.]), 'currentState': array([ 7.94635622, 27.48835096,  2.89010971]), 'targetState': array([ 7, 28], dtype=int32), 'currentDistance': 1.0758135725441034}
episode index:3387
target Thresh 31.99999999999995
target distance 14.0
model initialize at round 3387
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([14.        , 17.        ,  6.02605328]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 15.231546211727817}
done in step count: 42
reward sum = 0.4416336572164886
running average episode reward sum: 0.4854793939421893
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([8.45324265, 3.91129713, 4.61148525]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 1.0177874855029814}
episode index:3388
target Thresh 31.99999999999995
target distance 18.0
model initialize at round 3388
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8.20775032, 26.35374725]), 'previousTarget': array([ 8.19631201, 26.36442559]), 'currentState': array([21.02013456, 10.9965727 ,  0.08389633]), 'targetState': array([ 6, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.22990049413225522
running average episode reward sum: 0.4854039796902536
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 29.]), 'previousTarget': array([ 6., 29.]), 'currentState': array([ 6.93162735, 28.00113937,  2.50771306]), 'targetState': array([ 6, 29], dtype=int32), 'currentDistance': 1.3658887479770254}
episode index:3389
target Thresh 31.99999999999995
target distance 9.0
model initialize at round 3389
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 15.]), 'previousTarget': array([ 6., 15.]), 'currentState': array([15.        , 20.        ,  2.46436834]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 10.295630140987003}
done in step count: 25
reward sum = 0.6131123159555548
running average episode reward sum: 0.4854416517658481
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 15.]), 'previousTarget': array([ 6., 15.]), 'currentState': array([ 6.90850117, 15.28122281,  3.73681773]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 0.951031360137014}
episode index:3390
target Thresh 31.99999999999995
target distance 8.0
model initialize at round 3390
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  3.]), 'previousTarget': array([10.,  3.]), 'currentState': array([1.98018853, 6.03262312, 1.9425157 ]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 8.57404099565136}
done in step count: 22
reward sum = 0.639997059783302
running average episode reward sum: 0.4854872298867615
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  3.]), 'previousTarget': array([10.,  3.]), 'currentState': array([9.218343  , 3.77132854, 6.07073131]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 1.0981508877033364}
episode index:3391
target Thresh 31.99999999999995
target distance 24.0
model initialize at round 3391
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.85399587, 21.92753806]), 'previousTarget': array([17.92091492, 21.57960839]), 'currentState': array([21.95871893,  2.35328843,  1.7527103 ]), 'targetState': array([17, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.25966224186156556
running average episode reward sum: 0.48542065412378244
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 26.]), 'previousTarget': array([17., 26.]), 'currentState': array([16.83405873, 25.22327829,  1.89870262]), 'targetState': array([17, 26], dtype=int32), 'currentDistance': 0.7942500346415308}
episode index:3392
target Thresh 31.999999999999954
target distance 3.0
model initialize at round 3392
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 15.]), 'previousTarget': array([22., 15.]), 'currentState': array([19.01556917, 18.13928008,  1.33927983]), 'targetState': array([22, 15], dtype=int32), 'currentDistance': 4.331501680348566}
done in step count: 13
reward sum = 0.7876068309631873
running average episode reward sum: 0.4855097157733077
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 15.]), 'previousTarget': array([22., 15.]), 'currentState': array([21.8007153 , 15.83548836,  5.40836562]), 'targetState': array([22, 15], dtype=int32), 'currentDistance': 0.8589267683170725}
episode index:3393
target Thresh 31.999999999999954
target distance 8.0
model initialize at round 3393
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 18.]), 'previousTarget': array([24., 18.]), 'currentState': array([15.97628969, 11.05133609,  1.76335555]), 'targetState': array([24, 18], dtype=int32), 'currentDistance': 10.614323213580535}
done in step count: 28
reward sum = 0.6032775322621917
running average episode reward sum: 0.48554441459961556
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 18.]), 'previousTarget': array([24., 18.]), 'currentState': array([23.07961565, 17.63979082,  0.63555761]), 'targetState': array([24, 18], dtype=int32), 'currentDistance': 0.9883612719923772}
episode index:3394
target Thresh 31.999999999999954
target distance 11.0
model initialize at round 3394
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 23.]), 'previousTarget': array([ 7., 23.]), 'currentState': array([17.7509113 , 15.86696084,  3.40365827]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 12.902028580823343}
done in step count: 28
reward sum = 0.5500428587727326
running average episode reward sum: 0.48556341266859143
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 23.]), 'previousTarget': array([ 7., 23.]), 'currentState': array([ 7.69372609, 22.39180076,  2.58945454]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 0.9225845262211119}
episode index:3395
target Thresh 31.999999999999954
target distance 13.0
model initialize at round 3395
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([ 4.92277353, 16.01968286,  3.1129626 ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 15.87161560931784}
done in step count: 42
reward sum = 0.42403585300496016
running average episode reward sum: 0.4855452950126245
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.62118696,  3.86232386,  5.53569123]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.941860794082537}
episode index:3396
target Thresh 31.999999999999954
target distance 10.0
model initialize at round 3396
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 12.]), 'previousTarget': array([27., 12.]), 'currentState': array([26.97062678, 21.98916839,  3.72816867]), 'targetState': array([27, 12], dtype=int32), 'currentDistance': 9.98921157450988}
done in step count: 23
reward sum = 0.6298557649265122
running average episode reward sum: 0.4855877767523695
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 12.]), 'previousTarget': array([27., 12.]), 'currentState': array([27.30547189, 12.796537  ,  4.69369097]), 'targetState': array([27, 12], dtype=int32), 'currentDistance': 0.8531027291837665}
episode index:3397
target Thresh 31.999999999999954
target distance 16.0
model initialize at round 3397
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.60804015,  6.29301137]), 'previousTarget': array([16.59074408,  6.32117742]), 'currentState': array([ 1.9546756 , 19.90473085,  4.4959501 ]), 'targetState': array([18,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.29901114528140255
running average episode reward sum: 0.48553286897383185
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  5.]), 'previousTarget': array([18.,  5.]), 'currentState': array([17.1293836 ,  5.78556771,  5.51696386]), 'targetState': array([18,  5], dtype=int32), 'currentDistance': 1.172642123903965}
episode index:3398
target Thresh 31.999999999999954
target distance 8.0
model initialize at round 3398
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 20.]), 'previousTarget': array([11., 20.]), 'currentState': array([ 3.        , 17.        ,  4.51374379]), 'targetState': array([11, 20], dtype=int32), 'currentDistance': 8.54400374531753}
done in step count: 22
reward sum = 0.6464367690764796
running average episode reward sum: 0.48558020757345016
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 20.]), 'previousTarget': array([11., 20.]), 'currentState': array([10.28867275, 19.16072014,  0.43250437]), 'targetState': array([11, 20], dtype=int32), 'currentDistance': 1.100171414292376}
episode index:3399
target Thresh 31.999999999999957
target distance 10.0
model initialize at round 3399
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([26.94781047, 21.07231203,  2.44845569]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 9.948073291899261}
done in step count: 21
reward sum = 0.6455857028734557
running average episode reward sum: 0.4856272680132443
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([17.95026191, 20.81581366,  3.27740953]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 0.9679474668885761}
episode index:3400
target Thresh 31.999999999999957
target distance 2.0
model initialize at round 3400
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 21.]), 'previousTarget': array([25., 21.]), 'currentState': array([26.        , 23.        ,  0.44971405]), 'targetState': array([25, 21], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 16
reward sum = 0.8233000857670583
running average episode reward sum: 0.48572655434601514
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 21.]), 'previousTarget': array([25., 21.]), 'currentState': array([25.90362879, 21.98168485,  4.32769966]), 'targetState': array([25, 21], dtype=int32), 'currentDistance': 1.3342601468833937}
episode index:3401
target Thresh 31.999999999999957
target distance 10.0
model initialize at round 3401
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 16.]), 'previousTarget': array([15., 16.]), 'currentState': array([ 5.2397455 , 15.94141022,  6.149676  ]), 'targetState': array([15, 16], dtype=int32), 'currentDistance': 9.760430353191596}
done in step count: 22
reward sum = 0.6586796015043725
running average episode reward sum: 0.48577739298421574
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 16.]), 'previousTarget': array([15., 16.]), 'currentState': array([14.1006834 , 16.06818142,  0.04916771]), 'targetState': array([15, 16], dtype=int32), 'currentDistance': 0.901897473983085}
episode index:3402
target Thresh 31.999999999999957
target distance 12.0
model initialize at round 3402
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([14.        ,  5.        ,  0.12847232]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 12.64911064067352}
done in step count: 37
reward sum = 0.4897057360204383
running average episode reward sum: 0.48577854736065895
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.89016088, 8.95529464, 2.94544025]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.891282761533916}
episode index:3403
target Thresh 31.999999999999957
target distance 4.0
model initialize at round 3403
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'previousTarget': array([7., 3.]), 'currentState': array([10.        ,  7.        ,  5.37355042]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 5.0}
done in step count: 12
reward sum = 0.7905668852032433
running average episode reward sum: 0.4858680856502719
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'previousTarget': array([7., 3.]), 'currentState': array([7.37736395, 3.99534815, 4.18733222]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 1.0644817985545}
episode index:3404
target Thresh 31.999999999999957
target distance 6.0
model initialize at round 3404
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  6.]), 'previousTarget': array([17.,  6.]), 'currentState': array([20.        , 12.        ,  3.45043802]), 'targetState': array([17,  6], dtype=int32), 'currentDistance': 6.708203932499369}
done in step count: 15
reward sum = 0.7531876672811687
running average episode reward sum: 0.4859465936037612
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  6.]), 'previousTarget': array([17.,  6.]), 'currentState': array([17.45207651,  6.95305975,  4.37080316]), 'targetState': array([17,  6], dtype=int32), 'currentDistance': 1.0548440904262277}
episode index:3405
target Thresh 31.999999999999957
target distance 8.0
model initialize at round 3405
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 19.]), 'previousTarget': array([19., 19.]), 'currentState': array([21.        , 27.        ,  2.81494087]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 8.24621125123532}
done in step count: 20
reward sum = 0.6728744937765381
running average episode reward sum: 0.4860014755474408
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 19.]), 'previousTarget': array([19., 19.]), 'currentState': array([18.66119348, 19.92425509,  4.61565003]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 0.9843969355258083}
episode index:3406
target Thresh 31.999999999999957
target distance 5.0
model initialize at round 3406
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 26.]), 'previousTarget': array([ 7., 26.]), 'currentState': array([11.50253613, 26.94979191,  3.23177628]), 'targetState': array([ 7, 26], dtype=int32), 'currentDistance': 4.601623217440289}
done in step count: 9
reward sum = 0.8391194187562176
running average episode reward sum: 0.4861051203796124
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 26.]), 'previousTarget': array([ 7., 26.]), 'currentState': array([ 7.82205127, 26.38588582,  3.67203777]), 'targetState': array([ 7, 26], dtype=int32), 'currentDistance': 0.9081168180306008}
episode index:3407
target Thresh 31.999999999999957
target distance 8.0
model initialize at round 3407
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  4.]), 'previousTarget': array([26.,  4.]), 'currentState': array([17.96550973,  8.96985424,  4.11237669]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 9.447353340129673}
done in step count: 20
reward sum = 0.6426530723622766
running average episode reward sum: 0.4861510558115322
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  4.]), 'previousTarget': array([26.,  4.]), 'currentState': array([25.03252   ,  4.90704929,  5.81504389]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 1.3261809682447596}
episode index:3408
target Thresh 31.99999999999996
target distance 13.0
model initialize at round 3408
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([17.00138464, 27.00302883,  0.88950896]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 13.930718365105806}
done in step count: 37
reward sum = 0.46529443158664563
running average episode reward sum: 0.4861449377052767
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([22.02442418, 14.95823719,  5.10140343]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 0.9585484125584889}
episode index:3409
target Thresh 31.99999999999996
target distance 6.0
model initialize at round 3409
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 16.]), 'previousTarget': array([27., 16.]), 'currentState': array([22.21711636, 21.66412566,  5.31868171]), 'targetState': array([27, 16], dtype=int32), 'currentDistance': 7.413386231920659}
done in step count: 17
reward sum = 0.7413804396320249
running average episode reward sum: 0.48621978682607636
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 16.]), 'previousTarget': array([27., 16.]), 'currentState': array([26.37034724, 16.94884386,  5.50769182]), 'targetState': array([27, 16], dtype=int32), 'currentDistance': 1.1387568942239692}
episode index:3410
target Thresh 31.99999999999996
target distance 17.0
model initialize at round 3410
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 14.]), 'previousTarget': array([27., 14.]), 'currentState': array([10.        , 12.        ,  1.76036954]), 'targetState': array([27, 14], dtype=int32), 'currentDistance': 17.11724276862369}
done in step count: 41
reward sum = 0.4097432332364944
running average episode reward sum: 0.4861973662592075
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 14.]), 'previousTarget': array([27., 14.]), 'currentState': array([26.00088939, 14.20484581,  0.05757912]), 'targetState': array([27, 14], dtype=int32), 'currentDistance': 1.019894022614318}
episode index:3411
target Thresh 31.99999999999996
target distance 12.0
model initialize at round 3411
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 18.]), 'previousTarget': array([12., 18.]), 'currentState': array([24.        , 19.        ,  1.44728351]), 'targetState': array([12, 18], dtype=int32), 'currentDistance': 12.041594578792294}
done in step count: 29
reward sum = 0.5406730334921612
running average episode reward sum: 0.48621333216402374
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 18.]), 'previousTarget': array([12., 18.]), 'currentState': array([12.87859997, 17.90752999,  3.30880926]), 'targetState': array([12, 18], dtype=int32), 'currentDistance': 0.8834526631932315}
episode index:3412
target Thresh 31.99999999999996
target distance 10.0
model initialize at round 3412
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 25.]), 'previousTarget': array([15., 25.]), 'currentState': array([25.        , 17.        ,  0.17900005]), 'targetState': array([15, 25], dtype=int32), 'currentDistance': 12.806248474865697}
done in step count: 29
reward sum = 0.5149916482755735
running average episode reward sum: 0.48622176413475665
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 25.]), 'previousTarget': array([15., 25.]), 'currentState': array([15.83482202, 24.17663351,  2.64260514]), 'targetState': array([15, 25], dtype=int32), 'currentDistance': 1.1725443219294305}
episode index:3413
target Thresh 31.99999999999996
target distance 22.0
model initialize at round 3413
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7.29986591, 14.58858316]), 'previousTarget': array([ 7.3226018 , 14.57770876]), 'currentState': array([26.98734028, 11.06674375,  2.010746  ]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.25783698077947326
running average episode reward sum: 0.4861548675959883
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 15.]), 'previousTarget': array([ 5., 15.]), 'currentState': array([ 5.6193742 , 15.9779837 ,  3.52744078]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 1.1576167387468264}
episode index:3414
target Thresh 31.99999999999996
target distance 24.0
model initialize at round 3414
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.4,  6.8]), 'previousTarget': array([17.4,  6.8]), 'currentState': array([23.        , 26.        ,  3.70071235]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.21606479143734014
running average episode reward sum: 0.4860757782618276
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.41515267,  2.93810331,  4.76199468]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.0258603987390467}
episode index:3415
target Thresh 31.99999999999996
target distance 1.0
model initialize at round 3415
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([22.99059702, 20.01342232,  2.41978738]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 1.4171475805820162}
done in step count: 4
reward sum = 0.9371161330205813
running average episode reward sum: 0.48620781583640565
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([22.66395363, 19.96237841,  3.48228946]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 1.1691905877305147}
episode index:3416
target Thresh 31.99999999999996
target distance 7.0
model initialize at round 3416
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 18.]), 'previousTarget': array([12., 18.]), 'currentState': array([19.01323581, 16.0099511 ,  0.89652914]), 'targetState': array([12, 18], dtype=int32), 'currentDistance': 7.290114615003528}
done in step count: 19
reward sum = 0.6894323093482796
running average episode reward sum: 0.48626729037357624
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 18.]), 'previousTarget': array([12., 18.]), 'currentState': array([12.87788235, 17.68847136,  3.34087727]), 'targetState': array([12, 18], dtype=int32), 'currentDistance': 0.9315189268701641}
episode index:3417
target Thresh 31.999999999999964
target distance 19.0
model initialize at round 3417
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.24398548, 26.42259677]), 'previousTarget': array([11.23886  , 26.4327075]), 'currentState': array([19.02865221,  7.99981295,  0.24209874]), 'targetState': array([11, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.30707995221154744
running average episode reward sum: 0.48621486575737904
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 27.]), 'previousTarget': array([11., 27.]), 'currentState': array([11.20115962, 26.0039884 ,  2.39980995]), 'targetState': array([11, 27], dtype=int32), 'currentDistance': 1.016122184903167}
episode index:3418
target Thresh 31.999999999999964
target distance 7.0
model initialize at round 3418
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 19.]), 'previousTarget': array([14., 19.]), 'currentState': array([ 9.26290494, 12.16370957,  0.70928332]), 'targetState': array([14, 19], dtype=int32), 'currentDistance': 8.31714713348505}
done in step count: 17
reward sum = 0.7067051910497308
running average episode reward sum: 0.4862793554693686
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 19.]), 'previousTarget': array([14., 19.]), 'currentState': array([13.72949474, 18.04129682,  0.77686815]), 'targetState': array([14, 19], dtype=int32), 'currentDistance': 0.9961349700265144}
episode index:3419
target Thresh 31.999999999999964
target distance 18.0
model initialize at round 3419
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 20.]), 'previousTarget': array([ 4., 20.]), 'currentState': array([22.       , 19.       ,  1.4227435]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 18.02775637731995}
done in step count: 47
reward sum = 0.3669806176711225
running average episode reward sum: 0.4862444727974978
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 20.]), 'previousTarget': array([ 4., 20.]), 'currentState': array([ 4.85413112, 19.72164146,  3.22988004]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 0.8983448373033442}
episode index:3420
target Thresh 31.999999999999964
target distance 19.0
model initialize at round 3420
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4.70632169, 22.50614522]), 'previousTarget': array([ 4.70632169, 22.50614522]), 'currentState': array([16.        ,  6.        ,  1.51061592]), 'targetState': array([ 3, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.27084104660341646
running average episode reward sum: 0.48618150775037877
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 25.]), 'previousTarget': array([ 3., 25.]), 'currentState': array([ 3.9679841 , 24.85823984,  2.38639222]), 'targetState': array([ 3, 25], dtype=int32), 'currentDistance': 0.9783093348320993}
episode index:3421
target Thresh 31.999999999999964
target distance 12.0
model initialize at round 3421
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 16.]), 'previousTarget': array([ 5., 16.]), 'currentState': array([12.70467995,  4.15330545,  2.42171338]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 14.131746705025122}
done in step count: 34
reward sum = 0.510606666929594
running average episode reward sum: 0.4861886454357029
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 16.]), 'previousTarget': array([ 5., 16.]), 'currentState': array([ 5.37809244, 15.12063476,  2.27182881]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 0.9572027540175291}
episode index:3422
target Thresh 31.999999999999964
target distance 2.0
model initialize at round 3422
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([12.33418538,  9.68430894,  5.55169196]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.8008934066758886}
done in step count: 3
reward sum = 0.9510712759115989
running average episode reward sum: 0.4863244568965489
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.01937943,  9.12260096,  5.65612429]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.9882548776926977}
episode index:3423
target Thresh 31.999999999999964
target distance 9.0
model initialize at round 3423
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  5.]), 'previousTarget': array([25.,  5.]), 'currentState': array([24.        , 14.        ,  0.17417908]), 'targetState': array([25,  5], dtype=int32), 'currentDistance': 9.055385138137416}
done in step count: 21
reward sum = 0.6425623213121061
running average episode reward sum: 0.48637008711396
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  5.]), 'previousTarget': array([25.,  5.]), 'currentState': array([24.75727352,  5.95698862,  5.02481796]), 'targetState': array([25,  5], dtype=int32), 'currentDistance': 0.9872909199652578}
episode index:3424
target Thresh 31.999999999999964
target distance 19.0
model initialize at round 3424
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.55979023, 20.06847467]), 'previousTarget': array([16.56172689, 20.07475678]), 'currentState': array([8.01454067, 1.98592203, 5.76645374]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 55
reward sum = 0.27952123406473256
running average episode reward sum: 0.48630969328825224
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([17.44636596, 20.17190667,  1.37247598]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 0.9407343583589424}
episode index:3425
target Thresh 31.999999999999964
target distance 23.0
model initialize at round 3425
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5.29677131, 16.42270875]), 'previousTarget': array([ 5.2957649, 16.4268235]), 'currentState': array([24.99701906, 12.97303648,  4.35536808]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.23690193318022912
running average episode reward sum: 0.486236894759324
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'previousTarget': array([ 2., 17.]), 'currentState': array([ 2.89585192, 16.56783501,  3.13167026]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 0.9946442797363099}
episode index:3426
target Thresh 31.999999999999964
target distance 17.0
model initialize at round 3426
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.00231998,  4.76281932]), 'previousTarget': array([22.99675711,  4.76756726]), 'currentState': array([ 8.00616054, 17.99592938,  5.49337065]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.2920474731089356
running average episode reward sum: 0.48618023020675605
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'previousTarget': array([25.,  3.]), 'currentState': array([24.23222646,  3.91412548,  5.916225  ]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 1.193776190152841}
episode index:3427
target Thresh 31.999999999999968
target distance 17.0
model initialize at round 3427
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 18.]), 'previousTarget': array([27., 18.]), 'currentState': array([10.00034357, 15.0163623 ,  1.33748341]), 'targetState': array([27, 18], dtype=int32), 'currentDistance': 17.25950209969902}
done in step count: 43
reward sum = 0.40409507367564457
running average episode reward sum: 0.48615628471185196
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 18.]), 'previousTarget': array([27., 18.]), 'currentState': array([26.02584009, 17.94102426,  0.35113874]), 'targetState': array([27, 18], dtype=int32), 'currentDistance': 0.9759434734508831}
episode index:3428
target Thresh 31.999999999999968
target distance 22.0
model initialize at round 3428
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.11677028, 25.08202016]), 'previousTarget': array([22.0585156 , 25.06407315]), 'currentState': array([ 3.05937902, 19.0144145 ,  0.11270958]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.28351148834815004
running average episode reward sum: 0.4860971873667473
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 26.]), 'previousTarget': array([25., 26.]), 'currentState': array([24.42086573, 25.07713649,  0.36071496]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 1.0895290547920669}
episode index:3429
target Thresh 31.999999999999968
target distance 20.0
model initialize at round 3429
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.71530665, 25.95652732]), 'previousTarget': array([22.77872706, 25.96680906]), 'currentState': array([ 2.94447955, 22.93752159,  4.20519674]), 'targetState': array([23, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.3021836148260714
running average episode reward sum: 0.48604356824938855
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 26.]), 'previousTarget': array([23., 26.]), 'currentState': array([22.08481855, 25.80402077,  0.5163001 ]), 'targetState': array([23, 26], dtype=int32), 'currentDistance': 0.9359299906720467}
episode index:3430
target Thresh 31.999999999999968
target distance 15.0
model initialize at round 3430
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 29.]), 'previousTarget': array([17., 29.]), 'currentState': array([27.        , 14.        ,  0.41544997]), 'targetState': array([17, 29], dtype=int32), 'currentDistance': 18.027756377319946}
done in step count: 40
reward sum = 0.37651376747660126
running average episode reward sum: 0.48601164467003183
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 29.]), 'previousTarget': array([17., 29.]), 'currentState': array([17.69932732, 28.14702763,  2.54259433]), 'targetState': array([17, 29], dtype=int32), 'currentDistance': 1.1030052456694492}
episode index:3431
target Thresh 31.999999999999968
target distance 8.0
model initialize at round 3431
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([ 4.96574153, 23.95335466,  4.33141708]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 8.102146362014784}
done in step count: 21
reward sum = 0.6454828904972294
running average episode reward sum: 0.4860581106507507
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([12.09599184, 25.03005184,  0.56108955]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 0.9045075272215296}
episode index:3432
target Thresh 31.999999999999968
target distance 12.0
model initialize at round 3432
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 12.]), 'previousTarget': array([26., 12.]), 'currentState': array([14.0166739 ,  7.95458517,  5.31675673]), 'targetState': array([26, 12], dtype=int32), 'currentDistance': 12.6477462658161}
done in step count: 30
reward sum = 0.5264631586738078
running average episode reward sum: 0.48606988025401987
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 12.]), 'previousTarget': array([26., 12.]), 'currentState': array([25.10288907, 12.04022975,  0.67652004]), 'targetState': array([26, 12], dtype=int32), 'currentDistance': 0.8980125047482364}
episode index:3433
target Thresh 31.999999999999968
target distance 11.0
model initialize at round 3433
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 16.]), 'previousTarget': array([ 7., 16.]), 'currentState': array([6.0797951 , 5.18131338, 1.33969808]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 10.857751025524646}
done in step count: 24
reward sum = 0.6119470217703415
running average episode reward sum: 0.48610653638142715
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 16.]), 'previousTarget': array([ 7., 16.]), 'currentState': array([ 6.49357973, 15.21676352,  1.78032725]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 0.9326954858775098}
episode index:3434
target Thresh 31.999999999999968
target distance 2.0
model initialize at round 3434
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 12.]), 'previousTarget': array([ 3., 12.]), 'currentState': array([ 4.97124086, 13.04690661,  2.37330329]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 2.231995517304269}
done in step count: 5
reward sum = 0.9130232734798114
running average episode reward sum: 0.4862308207299274
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 12.]), 'previousTarget': array([ 3., 12.]), 'currentState': array([ 3.88253073, 12.86076687,  3.63449599]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 1.2327936138212199}
episode index:3435
target Thresh 31.999999999999968
target distance 15.0
model initialize at round 3435
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 23.]), 'previousTarget': array([18., 23.]), 'currentState': array([8.97070241, 8.00155961, 2.83590937]), 'targetState': array([18, 23], dtype=int32), 'currentDistance': 17.506611008596163}
done in step count: 46
reward sum = 0.37724804555024194
running average episode reward sum: 0.48619910280932793
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 23.]), 'previousTarget': array([18., 23.]), 'currentState': array([17.42293969, 22.02054712,  1.27933203]), 'targetState': array([18, 23], dtype=int32), 'currentDistance': 1.136805411222732}
episode index:3436
target Thresh 31.999999999999968
target distance 3.0
model initialize at round 3436
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 10.]), 'previousTarget': array([17., 10.]), 'currentState': array([15.35011215,  7.17578901,  0.57267225]), 'targetState': array([17, 10], dtype=int32), 'currentDistance': 3.2708252218316276}
done in step count: 9
reward sum = 0.8746203369500275
running average episode reward sum: 0.486312114515508
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 10.]), 'previousTarget': array([17., 10.]), 'currentState': array([16.85303028,  9.00834429,  1.00971809]), 'targetState': array([17, 10], dtype=int32), 'currentDistance': 1.0024874791234393}
episode index:3437
target Thresh 31.999999999999968
target distance 8.0
model initialize at round 3437
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([18.91855847,  2.91079609,  3.71994781]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 7.993117877893009}
done in step count: 19
reward sum = 0.6942633449242727
running average episode reward sum: 0.4863726006209207
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([11.93350585,  4.65273209,  2.95518192]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 1.13907521755582}
episode index:3438
target Thresh 31.99999999999997
target distance 17.0
model initialize at round 3438
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  3.]), 'previousTarget': array([20.,  3.]), 'currentState': array([16.        , 20.        ,  3.27022228]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 17.46424919657298}
done in step count: 47
reward sum = 0.3904153843458579
running average episode reward sum: 0.4863446979700702
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  3.]), 'previousTarget': array([20.,  3.]), 'currentState': array([19.4827068 ,  3.84039463,  5.09492994]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 0.9868411198037207}
episode index:3439
target Thresh 31.99999999999997
target distance 17.0
model initialize at round 3439
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 19.]), 'previousTarget': array([20., 19.]), 'currentState': array([26.       ,  2.       ,  0.9169668]), 'targetState': array([20, 19], dtype=int32), 'currentDistance': 18.027756377319946}
done in step count: 43
reward sum = 0.4039199725124609
running average episode reward sum: 0.48632073729406505
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 19.]), 'previousTarget': array([20., 19.]), 'currentState': array([20.98767858, 18.25251607,  2.06733553]), 'targetState': array([20, 19], dtype=int32), 'currentDistance': 1.2386449049097144}
episode index:3440
target Thresh 31.99999999999997
target distance 23.0
model initialize at round 3440
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.44586568, 21.44760376]), 'previousTarget': array([18.41810404, 21.42128976]), 'currentState': array([ 2.01953975, 10.03814205,  0.85527083]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 78
reward sum = 0.161978332779172
running average episode reward sum: 0.48622647911199157
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 26.]), 'previousTarget': array([25., 26.]), 'currentState': array([24.3435743 , 25.04503165,  1.01333248]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 1.1588180415741232}
episode index:3441
target Thresh 31.99999999999997
target distance 3.0
model initialize at round 3441
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  3.]), 'previousTarget': array([12.,  3.]), 'currentState': array([12.99923756,  6.00094823,  2.48373973]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 3.162936287010298}
done in step count: 10
reward sum = 0.8328146562574471
running average episode reward sum: 0.4863271729461419
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  3.]), 'previousTarget': array([12.,  3.]), 'currentState': array([11.57738596,  3.83403822,  4.58270802]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 0.9349985960500343}
episode index:3442
target Thresh 31.99999999999997
target distance 18.0
model initialize at round 3442
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([ 1.93840723, 22.07634608,  2.49291015]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 18.10749201272743}
done in step count: 45
reward sum = 0.35529404198539044
running average episode reward sum: 0.4862891151096735
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.53564881, 4.95058465, 4.9635633 ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.057938187049232}
episode index:3443
target Thresh 31.99999999999997
target distance 20.0
model initialize at round 3443
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 24.]), 'previousTarget': array([24.97504678, 23.99875234]), 'currentState': array([ 5.02724162, 23.0080013 ,  0.05259013]), 'targetState': array([25, 24], dtype=int32), 'currentDistance': 19.997378300669432}
done in step count: 52
reward sum = 0.3413474812558096
running average episode reward sum: 0.486247029850134
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 24.]), 'previousTarget': array([25., 24.]), 'currentState': array([24.08686433, 23.55133953,  0.30552795]), 'targetState': array([25, 24], dtype=int32), 'currentDistance': 1.0174050169968496}
episode index:3444
target Thresh 31.99999999999997
target distance 15.0
model initialize at round 3444
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([25.1036101 , 23.79321391,  4.94900393]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 14.914270766476173}
done in step count: 31
reward sum = 0.49671475337112964
running average episode reward sum: 0.4862500683765552
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([26.5314107 ,  9.85685742,  5.03283825]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 0.9766169025014712}
episode index:3445
target Thresh 31.99999999999997
target distance 19.0
model initialize at round 3445
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.68468255, 20.37981349]), 'previousTarget': array([20.69765531, 20.39288577]), 'currentState': array([4.99354508, 7.97867624, 4.64550504]), 'targetState': array([24, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.20186890809294472
running average episode reward sum: 0.48616754337357104
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 23.]), 'previousTarget': array([24., 23.]), 'currentState': array([23.30949727, 22.01274381,  0.9719149 ]), 'targetState': array([24, 23], dtype=int32), 'currentDistance': 1.2047691913188108}
episode index:3446
target Thresh 31.99999999999997
target distance 3.0
model initialize at round 3446
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 25.]), 'previousTarget': array([ 4., 25.]), 'currentState': array([ 6.99894391, 25.00951173,  1.93387365]), 'targetState': array([ 4, 25], dtype=int32), 'currentDistance': 2.9989589945620203}
done in step count: 7
reward sum = 0.8702105105188385
running average episode reward sum: 0.48627895705710605
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 25.]), 'previousTarget': array([ 4., 25.]), 'currentState': array([ 4.7960587 , 25.32330225,  3.43356022]), 'targetState': array([ 4, 25], dtype=int32), 'currentDistance': 0.8592053322302748}
episode index:3447
target Thresh 31.99999999999997
target distance 21.0
model initialize at round 3447
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3.88009345, 7.3102453 ]), 'previousTarget': array([3.88009345, 7.3102453 ]), 'currentState': array([11.        , 26.        ,  3.15796852]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.2688758229842439
running average episode reward sum: 0.48621590510406865
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.0186628 , 5.98317521, 4.62224615]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.9833523199729957}
episode index:3448
target Thresh 31.99999999999997
target distance 16.0
model initialize at round 3448
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 29.]), 'previousTarget': array([ 5., 29.]), 'currentState': array([12.       , 13.       ,  3.8335948]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 17.46424919657298}
done in step count: 41
reward sum = 0.38159307024529937
running average episode reward sum: 0.4861855708521525
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 29.]), 'previousTarget': array([ 5., 29.]), 'currentState': array([ 5.79103989, 28.00945254,  2.12781413]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 1.2676467826026365}
episode index:3449
target Thresh 31.99999999999997
target distance 11.0
model initialize at round 3449
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 18.]), 'previousTarget': array([13., 18.]), 'currentState': array([ 2.        , 10.        ,  2.00973487]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 13.601470508735444}
done in step count: 31
reward sum = 0.4992362969381231
running average episode reward sum: 0.4861893536713079
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 18.]), 'previousTarget': array([13., 18.]), 'currentState': array([12.10517977, 17.04446038,  0.66228637]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 1.3091062673116358}
episode index:3450
target Thresh 31.999999999999975
target distance 18.0
model initialize at round 3450
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  8.]), 'previousTarget': array([25.,  8.]), 'currentState': array([22.93354743, 25.86442284,  4.48274446]), 'targetState': array([25,  8], dtype=int32), 'currentDistance': 17.983543296539846}
done in step count: 46
reward sum = 0.4015528151776473
running average episode reward sum: 0.4861648284500695
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  8.]), 'previousTarget': array([25.,  8.]), 'currentState': array([24.68386837,  8.96652354,  4.98731673]), 'targetState': array([25,  8], dtype=int32), 'currentDistance': 1.0169104945728062}
episode index:3451
target Thresh 31.999999999999975
target distance 9.0
model initialize at round 3451
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 28.]), 'previousTarget': array([ 8., 28.]), 'currentState': array([12.04534309, 18.97250834,  5.95610154]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 9.892441883996423}
done in step count: 27
reward sum = 0.5875889115615537
running average episode reward sum: 0.48619420970241933
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 28.]), 'previousTarget': array([ 8., 28.]), 'currentState': array([ 8.41468239, 27.11376913,  2.12509161]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 0.9784511444394941}
episode index:3452
target Thresh 31.999999999999975
target distance 5.0
model initialize at round 3452
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([9.79350757, 8.25084063, 2.42067754]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 5.10267315637795}
done in step count: 11
reward sum = 0.818714409229635
running average episode reward sum: 0.4862905086307504
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([5.91199418, 9.90997404, 2.74540896]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.9164267862577768}
episode index:3453
target Thresh 31.999999999999975
target distance 14.0
model initialize at round 3453
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([21.06965658, 22.65107988,  4.65693331]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 16.96319443965593}
done in step count: 40
reward sum = 0.4115380985855614
running average episode reward sum: 0.48626886635801003
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([11.32829745,  9.8139737 ,  4.14202035]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.8776858243042733}
episode index:3454
target Thresh 31.999999999999975
target distance 19.0
model initialize at round 3454
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  4.]), 'previousTarget': array([25.,  4.]), 'currentState': array([6.03289829, 5.93493884, 5.38097793]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 19.06554315107909}
done in step count: 45
reward sum = 0.37055174654564804
running average episode reward sum: 0.48623537370393993
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  4.]), 'previousTarget': array([25.,  4.]), 'currentState': array([24.21076159,  4.09679053,  0.11186815]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 0.7951513498758868}
episode index:3455
target Thresh 31.999999999999975
target distance 8.0
model initialize at round 3455
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  6.]), 'previousTarget': array([27.,  6.]), 'currentState': array([19.        , 12.        ,  2.23973332]), 'targetState': array([27,  6], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 25
reward sum = 0.5945504547225834
running average episode reward sum: 0.48626671487321615
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  6.]), 'previousTarget': array([27.,  6.]), 'currentState': array([26.06785801,  6.7240389 ,  5.61394278]), 'targetState': array([27,  6], dtype=int32), 'currentDistance': 1.1803054706332465}
episode index:3456
target Thresh 31.999999999999975
target distance 22.0
model initialize at round 3456
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.91309269, 21.24502109]), 'previousTarget': array([20.82570003, 21.22895002]), 'currentState': array([5.15208087, 8.93281145, 6.11967564]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 79
reward sum = 0.15179115895078993
running average episode reward sum: 0.48616996174740695
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 26.]), 'previousTarget': array([27., 26.]), 'currentState': array([26.9950925 , 25.07964836,  0.8894504 ]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 0.9203647189418727}
episode index:3457
target Thresh 31.999999999999975
target distance 21.0
model initialize at round 3457
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.57395977, 10.11891195]), 'previousTarget': array([17.48275862, 10.20689655]), 'currentState': array([ 3.08993333, 23.91068406,  5.3395232 ]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 81
reward sum = 0.1509499033781042
running average episode reward sum: 0.48607302130253444
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  4.]), 'previousTarget': array([24.,  4.]), 'currentState': array([23.22059684,  4.18789838,  5.99194172]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 0.8017325536650161}
episode index:3458
target Thresh 31.999999999999975
target distance 8.0
model initialize at round 3458
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([11.9999994 , 11.00002512,  1.84712291]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 8.246216763522646}
done in step count: 17
reward sum = 0.6817698308673183
running average episode reward sum: 0.4861295974255656
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.96362233, 9.45588834, 3.5475613 ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.0660216591954594}
episode index:3459
target Thresh 31.999999999999975
target distance 12.0
model initialize at round 3459
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([22.86844658, 26.65578553,  4.40327534]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 12.003554558382756}
done in step count: 25
reward sum = 0.5907133634782434
running average episode reward sum: 0.48615982394754614
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([20.03549818, 15.78137368,  4.55037872]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 0.7821796120243973}
episode index:3460
target Thresh 31.999999999999975
target distance 16.0
model initialize at round 3460
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 14.]), 'previousTarget': array([21., 14.]), 'currentState': array([ 4.98349387, 19.05959187,  1.59008777]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 16.79666450793464}
done in step count: 41
reward sum = 0.38854690806227626
running average episode reward sum: 0.4861316202734967
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 14.]), 'previousTarget': array([21., 14.]), 'currentState': array([20.19798061, 14.35893819,  6.16256281]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 0.8786761242089387}
episode index:3461
target Thresh 31.999999999999975
target distance 14.0
model initialize at round 3461
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 12.]), 'previousTarget': array([ 7., 12.]), 'currentState': array([20.99005442, 25.01213741,  2.50976467]), 'targetState': array([ 7, 12], dtype=int32), 'currentDistance': 19.105950445291672}
done in step count: 47
reward sum = 0.3669009366203902
running average episode reward sum: 0.4860971804457517
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 12.]), 'previousTarget': array([ 7., 12.]), 'currentState': array([ 7.91981127, 12.89036634,  4.07124025]), 'targetState': array([ 7, 12], dtype=int32), 'currentDistance': 1.280158192208019}
episode index:3462
target Thresh 31.999999999999975
target distance 4.0
model initialize at round 3462
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 26.]), 'previousTarget': array([13., 26.]), 'currentState': array([12.        , 22.        ,  6.12866926]), 'targetState': array([13, 26], dtype=int32), 'currentDistance': 4.123105625617661}
done in step count: 10
reward sum = 0.8160450405042581
running average episode reward sum: 0.4861924584879285
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 26.]), 'previousTarget': array([13., 26.]), 'currentState': array([13.15000716, 25.24632024,  1.56215725]), 'targetState': array([13, 26], dtype=int32), 'currentDistance': 0.7684629616214154}
episode index:3463
target Thresh 31.999999999999975
target distance 19.0
model initialize at round 3463
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 19.]), 'previousTarget': array([ 3., 19.]), 'currentState': array([22.        , 25.        ,  1.34519422]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 19.924858845171272}
done in step count: 56
reward sum = 0.31824976122451126
running average episode reward sum: 0.48614397618502336
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 19.]), 'previousTarget': array([ 3., 19.]), 'currentState': array([ 3.94177009, 19.61001032,  3.54376076]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 1.1220710774453435}
episode index:3464
target Thresh 31.999999999999975
target distance 16.0
model initialize at round 3464
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6.8692323 , 5.87250521]), 'previousTarget': array([6.85786438, 5.85786438]), 'currentState': array([20.99899234, 20.02700559,  1.86059189]), 'targetState': array([5, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.2581278651105465
running average episode reward sum: 0.48607817066956177
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 4.]), 'previousTarget': array([5., 4.]), 'currentState': array([5.5435804 , 4.88575135, 4.24542776]), 'targetState': array([5, 4], dtype=int32), 'currentDistance': 1.0392473771113804}
episode index:3465
target Thresh 31.99999999999998
target distance 16.0
model initialize at round 3465
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 12.]), 'previousTarget': array([22., 12.]), 'currentState': array([16.91610135, 27.98046222,  3.62288857]), 'targetState': array([22, 12], dtype=int32), 'currentDistance': 16.769651104985627}
done in step count: 40
reward sum = 0.4121511813397758
running average episode reward sum: 0.48605684147471767
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 12.]), 'previousTarget': array([22., 12.]), 'currentState': array([21.62155489, 12.82261268,  5.30793019]), 'targetState': array([22, 12], dtype=int32), 'currentDistance': 0.905490101476178}
episode index:3466
target Thresh 31.99999999999998
target distance 2.0
model initialize at round 3466
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  5.]), 'previousTarget': array([21.,  5.]), 'currentState': array([21.00712914,  6.99806431,  5.87714115]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 1.9980770237306509}
done in step count: 7
reward sum = 0.9125334178756985
running average episode reward sum: 0.4861798517361543
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  5.]), 'previousTarget': array([21.,  5.]), 'currentState': array([21.35862497,  5.98083134,  4.75359447]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 1.0443380601342687}
episode index:3467
target Thresh 31.99999999999998
target distance 21.0
model initialize at round 3467
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 22.]), 'previousTarget': array([26., 22.]), 'currentState': array([ 6.        , 22.        ,  4.83831596]), 'targetState': array([27, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.3009432704337799
running average episode reward sum: 0.48612643865042704
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 22.]), 'previousTarget': array([27., 22.]), 'currentState': array([26.26905325, 21.68814061,  0.05080376]), 'targetState': array([27, 22], dtype=int32), 'currentDistance': 0.7946945489771291}
episode index:3468
target Thresh 31.99999999999998
target distance 20.0
model initialize at round 3468
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4.07442981, 11.37606426]), 'previousTarget': array([ 4.12283287, 11.39299151]), 'currentState': array([22.9515212 , 17.98328916,  3.28434265]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.3190828966645235
running average episode reward sum: 0.48607828542414105
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.83641815, 11.48738435,  3.64771279]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.968059311924763}
episode index:3469
target Thresh 31.99999999999998
target distance 13.0
model initialize at round 3469
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 13.]), 'previousTarget': array([ 8., 13.]), 'currentState': array([17.9966577 , 26.01260212,  2.08254504]), 'targetState': array([ 8, 13], dtype=int32), 'currentDistance': 16.409173627830956}
done in step count: 41
reward sum = 0.41598068563830204
running average episode reward sum: 0.4860580843867388
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 13.]), 'previousTarget': array([ 8., 13.]), 'currentState': array([ 8.50077308, 13.92296122,  4.28653611]), 'targetState': array([ 8, 13], dtype=int32), 'currentDistance': 1.0500624224157922}
episode index:3470
target Thresh 31.99999999999998
target distance 22.0
model initialize at round 3470
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5.081685 , 27.3700465]), 'previousTarget': array([ 5.09184678, 26.9793708 ]), 'currentState': array([6.08272723, 7.39511435, 1.43880767]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.30836499596465383
running average episode reward sum: 0.4860068907571156
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 29.]), 'previousTarget': array([ 5., 29.]), 'currentState': array([ 5.18124441, 28.25230577,  1.82657721]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 0.7693479030068285}
episode index:3471
target Thresh 31.99999999999998
target distance 8.0
model initialize at round 3471
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 23.]), 'previousTarget': array([15., 23.]), 'currentState': array([ 6.99887439, 22.94934891,  4.94266987]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 8.001285929061352}
done in step count: 18
reward sum = 0.6899488761011356
running average episode reward sum: 0.48606562980819396
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 23.]), 'previousTarget': array([15., 23.]), 'currentState': array([14.0595881 , 22.6067472 ,  0.03454484]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 1.0193244388549942}
episode index:3472
target Thresh 31.99999999999998
target distance 19.0
model initialize at round 3472
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 10.]), 'previousTarget': array([26., 10.]), 'currentState': array([26.00011771, 28.9999904 ,  5.94927796]), 'targetState': array([26, 10], dtype=int32), 'currentDistance': 18.999990396529657}
done in step count: 49
reward sum = 0.3406530011062063
running average episode reward sum: 0.4860237603498865
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 10.]), 'previousTarget': array([26., 10.]), 'currentState': array([25.77596489, 10.80985814,  4.77395835]), 'targetState': array([26, 10], dtype=int32), 'currentDistance': 0.8402749202948774}
episode index:3473
target Thresh 31.99999999999998
target distance 18.0
model initialize at round 3473
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.80368799, 25.36442559]), 'previousTarget': array([17.80368799, 25.36442559]), 'currentState': array([ 5.        , 10.        ,  0.69379598]), 'targetState': array([20, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.27928386022322793
running average episode reward sum: 0.4859642497280883
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 28.]), 'previousTarget': array([20., 28.]), 'currentState': array([19.29046697, 27.19001983,  0.98095978]), 'targetState': array([20, 28], dtype=int32), 'currentDistance': 1.0768031372400078}
episode index:3474
target Thresh 31.99999999999998
target distance 16.0
model initialize at round 3474
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.29779486, 18.61489046]), 'previousTarget': array([13.32117742, 18.59074408]), 'currentState': array([26.97243707,  4.0202284 ,  2.26214045]), 'targetState': array([12, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.3050372518078778
running average episode reward sum: 0.48591218440494577
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 20.]), 'previousTarget': array([12., 20.]), 'currentState': array([12.95189198, 19.57960267,  2.35914747]), 'targetState': array([12, 20], dtype=int32), 'currentDistance': 1.0405922662208869}
episode index:3475
target Thresh 31.99999999999998
target distance 4.0
model initialize at round 3475
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 28.]), 'previousTarget': array([17., 28.]), 'currentState': array([20.57612853, 27.12683472,  2.85578496]), 'targetState': array([17, 28], dtype=int32), 'currentDistance': 3.6811836187470255}
done in step count: 7
reward sum = 0.87758590833113
running average episode reward sum: 0.4860248638422088
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 28.]), 'previousTarget': array([17., 28.]), 'currentState': array([17.7972164 , 27.89209891,  2.8838445 ]), 'targetState': array([17, 28], dtype=int32), 'currentDistance': 0.8044853225873694}
episode index:3476
target Thresh 31.99999999999998
target distance 8.0
model initialize at round 3476
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 25.]), 'previousTarget': array([23., 25.]), 'currentState': array([15.1079442 , 26.68554702,  5.27030888]), 'targetState': array([23, 25], dtype=int32), 'currentDistance': 8.070044208155101}
done in step count: 16
reward sum = 0.7061458303668131
running average episode reward sum: 0.4860881715691356
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 25.]), 'previousTarget': array([23., 25.]), 'currentState': array([22.08199152, 25.1517804 ,  6.16960388]), 'targetState': array([23, 25], dtype=int32), 'currentDistance': 0.930471307878201}
episode index:3477
target Thresh 31.99999999999998
target distance 5.0
model initialize at round 3477
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 23.]), 'previousTarget': array([ 4., 23.]), 'currentState': array([ 8.02753677, 17.98428606,  6.01234239]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 6.432607466697988}
done in step count: 14
reward sum = 0.7242453282299101
running average episode reward sum: 0.48615664688732446
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 23.]), 'previousTarget': array([ 4., 23.]), 'currentState': array([ 4.99920156, 22.00587482,  2.30419151]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 1.4094994233454665}
episode index:3478
target Thresh 31.99999999999998
target distance 18.0
model initialize at round 3478
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([20.10633366,  9.99129762,  5.95473132]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 18.54102950867264}
done in step count: 50
reward sum = 0.31832258386375734
running average episode reward sum: 0.48610840484563905
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.98454208, 6.20628974, 3.53312089]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.0059217498602437}
episode index:3479
target Thresh 31.99999999999998
target distance 23.0
model initialize at round 3479
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.01248501, 12.60814841]), 'previousTarget': array([22.83200822, 12.58678368]), 'currentState': array([ 3.18233406, 10.00716528,  0.04288578]), 'targetState': array([26, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.27900172089382963
running average episode reward sum: 0.48604889143071034
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 13.]), 'previousTarget': array([26., 13.]), 'currentState': array([25.17071787, 12.98334646,  0.22372436]), 'targetState': array([26, 13], dtype=int32), 'currentDistance': 0.8294493264688918}
episode index:3480
target Thresh 31.99999999999998
target distance 6.0
model initialize at round 3480
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([15.        , 22.        ,  5.46765184]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 6.082762530298219}
done in step count: 16
reward sum = 0.7168948841488973
running average episode reward sum: 0.48611520742976755
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([ 9.97832374, 22.68168504,  3.10078835]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 1.0288059857728304}
episode index:3481
target Thresh 31.999999999999982
target distance 8.0
model initialize at round 3481
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([17.27189291,  4.29029844,  0.78612392]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 10.232640090658547}
done in step count: 20
reward sum = 0.6634634243942448
running average episode reward sum: 0.4861661402893208
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([23.07864182, 11.07380295,  0.91308414]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 1.306423315551709}
episode index:3482
target Thresh 31.999999999999982
target distance 2.0
model initialize at round 3482
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 19.]), 'previousTarget': array([24., 19.]), 'currentState': array([25.66031501, 19.16526397,  2.74134752]), 'targetState': array([24, 19], dtype=int32), 'currentDistance': 1.668519733749462}
done in step count: 3
reward sum = 0.9543996292170317
running average episode reward sum: 0.4863005742511146
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 19.]), 'previousTarget': array([24., 19.]), 'currentState': array([24.86712254, 19.43234795,  2.8785423 ]), 'targetState': array([24, 19], dtype=int32), 'currentDistance': 0.9689304662572805}
episode index:3483
target Thresh 31.999999999999982
target distance 6.0
model initialize at round 3483
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 14.]), 'previousTarget': array([26., 14.]), 'currentState': array([20.       , 10.       ,  4.5771389]), 'targetState': array([26, 14], dtype=int32), 'currentDistance': 7.211102550927978}
done in step count: 18
reward sum = 0.6912147495685586
running average episode reward sum: 0.4863593900304824
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 14.]), 'previousTarget': array([26., 14.]), 'currentState': array([25.18165934, 13.54448246,  0.70310596]), 'targetState': array([26, 14], dtype=int32), 'currentDistance': 0.9365776354728491}
episode index:3484
target Thresh 31.999999999999982
target distance 14.0
model initialize at round 3484
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([10.93431068,  4.98490264,  3.13521296]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 18.498815657505023}
done in step count: 47
reward sum = 0.33770545513403644
running average episode reward sum: 0.48631673466896264
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([24.13886756, 16.46418777,  0.73401983]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 1.0142207926561075}
episode index:3485
target Thresh 31.999999999999982
target distance 5.0
model initialize at round 3485
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 19.]), 'previousTarget': array([20., 19.]), 'currentState': array([14.98041826, 21.98495029,  4.04120666]), 'targetState': array([20, 19], dtype=int32), 'currentDistance': 5.84004529853665}
done in step count: 13
reward sum = 0.7545329534245543
running average episode reward sum: 0.48639367563819835
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 19.]), 'previousTarget': array([20., 19.]), 'currentState': array([19.27222499, 19.57213169,  5.94615183]), 'targetState': array([20, 19], dtype=int32), 'currentDistance': 0.9257381595791446}
episode index:3486
target Thresh 31.999999999999982
target distance 13.0
model initialize at round 3486
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 18.]), 'previousTarget': array([26., 18.]), 'currentState': array([13.        , 12.        ,  4.33305955]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 14.317821063276353}
done in step count: 40
reward sum = 0.44492769820497347
running average episode reward sum: 0.4863817840473084
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 18.]), 'previousTarget': array([26., 18.]), 'currentState': array([25.15329486, 17.68289495,  0.61572628]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 0.9041378197998039}
episode index:3487
target Thresh 31.999999999999982
target distance 3.0
model initialize at round 3487
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 23.]), 'previousTarget': array([ 4., 23.]), 'currentState': array([ 6.        , 20.        ,  6.20556641]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 10
reward sum = 0.8220871511213713
running average episode reward sum: 0.4864780298520888
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 23.]), 'previousTarget': array([ 4., 23.]), 'currentState': array([ 4.78680344, 22.20985016,  2.34288219]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 1.115076864190951}
episode index:3488
target Thresh 31.999999999999982
target distance 15.0
model initialize at round 3488
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 14.]), 'previousTarget': array([ 9., 14.]), 'currentState': array([23.70768852, 18.70320225,  3.68210602]), 'targetState': array([ 9, 14], dtype=int32), 'currentDistance': 15.44137989837443}
done in step count: 33
reward sum = 0.47469859407098824
running average episode reward sum: 0.48647465368820775
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 14.]), 'previousTarget': array([ 9., 14.]), 'currentState': array([ 9.94946227, 14.15336077,  3.58457714]), 'targetState': array([ 9, 14], dtype=int32), 'currentDistance': 0.9617682262132407}
episode index:3489
target Thresh 31.999999999999982
target distance 3.0
model initialize at round 3489
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 20.]), 'previousTarget': array([12., 20.]), 'currentState': array([11.01024523, 23.03779862,  1.09187022]), 'targetState': array([12, 20], dtype=int32), 'currentDistance': 3.1949702548298893}
done in step count: 10
reward sum = 0.8227967970769289
running average episode reward sum: 0.4865710210645369
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 20.]), 'previousTarget': array([12., 20.]), 'currentState': array([11.87428024, 20.85946851,  4.74151184]), 'targetState': array([12, 20], dtype=int32), 'currentDistance': 0.868614751104095}
episode index:3490
target Thresh 31.999999999999982
target distance 19.0
model initialize at round 3490
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.86398076, 18.10111675]), 'previousTarget': array([24.86398076, 18.10111675]), 'currentState': array([13.        ,  2.        ,  3.43706536]), 'targetState': array([27, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.20038087854804443
running average episode reward sum: 0.4864890416481758
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 21.]), 'previousTarget': array([27., 21.]), 'currentState': array([26.41808331, 20.05259601,  1.13383285]), 'targetState': array([27, 21], dtype=int32), 'currentDistance': 1.1118459232084332}
episode index:3491
target Thresh 31.999999999999982
target distance 9.0
model initialize at round 3491
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 20.]), 'previousTarget': array([25., 20.]), 'currentState': array([16.        , 25.        ,  1.88606867]), 'targetState': array([25, 20], dtype=int32), 'currentDistance': 10.295630140987}
done in step count: 25
reward sum = 0.5887398555246961
running average episode reward sum: 0.48651832309544857
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 20.]), 'previousTarget': array([25., 20.]), 'currentState': array([24.21272334, 20.7555752 ,  5.84731585]), 'targetState': array([25, 20], dtype=int32), 'currentDistance': 1.0911912862569413}
episode index:3492
target Thresh 31.999999999999982
target distance 4.0
model initialize at round 3492
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([21.51828635,  3.94442634,  3.324715  ]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 3.6428395460777714}
done in step count: 8
reward sum = 0.8698001527706793
running average episode reward sum: 0.4866280516467441
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([18.81973482,  3.00029581,  3.4574231 ]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 0.8197348723605703}
episode index:3493
target Thresh 31.999999999999982
target distance 6.0
model initialize at round 3493
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 15.]), 'previousTarget': array([17., 15.]), 'currentState': array([11.31897072, 12.04394493,  0.25972099]), 'targetState': array([17, 15], dtype=int32), 'currentDistance': 6.404088948003625}
done in step count: 15
reward sum = 0.7749893787724987
running average episode reward sum: 0.4867105820780909
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 15.]), 'previousTarget': array([17., 15.]), 'currentState': array([16.05575356, 14.80617763,  0.54493929]), 'targetState': array([17, 15], dtype=int32), 'currentDistance': 0.96393383916272}
episode index:3494
target Thresh 31.999999999999982
target distance 11.0
model initialize at round 3494
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([10.01646045, 19.83693708,  5.06549358]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 13.492802668727604}
done in step count: 30
reward sum = 0.534686497742556
running average episode reward sum: 0.4867243090925872
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([20.18491821, 12.55266959,  5.71704929]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 0.9847852566616668}
episode index:3495
target Thresh 31.999999999999982
target distance 14.0
model initialize at round 3495
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([24.01516737,  5.00279876,  0.43497246]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 16.64917070798753}
done in step count: 41
reward sum = 0.40452833109617764
running average episode reward sum: 0.4867007976572335
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([15.36145458, 18.23702888,  2.21326578]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 0.8442596366668569}
episode index:3496
target Thresh 31.999999999999982
target distance 14.0
model initialize at round 3496
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 26.]), 'previousTarget': array([ 9., 26.]), 'currentState': array([10.0004942 , 12.35328743,  1.57872042]), 'targetState': array([ 9, 26], dtype=int32), 'currentDistance': 13.683338503354618}
done in step count: 30
reward sum = 0.5399062726048002
running average episode reward sum: 0.48671601226259453
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 26.]), 'previousTarget': array([ 9., 26.]), 'currentState': array([ 9.24420032, 25.20676418,  1.52765265]), 'targetState': array([ 9, 26], dtype=int32), 'currentDistance': 0.8299740171768534}
episode index:3497
target Thresh 31.999999999999982
target distance 8.0
model initialize at round 3497
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 14.]), 'previousTarget': array([ 5., 14.]), 'currentState': array([12.64493104,  9.91380394,  3.14679727]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 8.668446739211545}
done in step count: 17
reward sum = 0.6901868873344579
running average episode reward sum: 0.48677418003705764
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 14.]), 'previousTarget': array([ 5., 14.]), 'currentState': array([ 5.86406062, 13.71557415,  2.70983262]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 0.9096696217174636}
episode index:3498
target Thresh 31.999999999999982
target distance 22.0
model initialize at round 3498
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3.43591412, 20.51087302]), 'previousTarget': array([ 3.42734309, 20.51093912]), 'currentState': array([11.04735119,  2.01584596,  0.57543299]), 'targetState': array([ 2, 24], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.23162819238343918
running average episode reward sum: 0.48670126034924577
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 24.]), 'previousTarget': array([ 2., 24.]), 'currentState': array([ 2.89278237, 23.13535947,  1.97417201]), 'targetState': array([ 2, 24], dtype=int32), 'currentDistance': 1.2428449628317473}
episode index:3499
target Thresh 31.999999999999982
target distance 4.0
model initialize at round 3499
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  2.]), 'previousTarget': array([21.,  2.]), 'currentState': array([24.67421313,  2.72585018,  3.66100955]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 3.745223706083337}
done in step count: 7
reward sum = 0.8721897931762759
running average episode reward sum: 0.48681139993005346
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  2.]), 'previousTarget': array([21.,  2.]), 'currentState': array([21.97677519,  2.00399768,  3.37841685]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 0.9767833667267262}
episode index:3500
target Thresh 31.999999999999982
target distance 11.0
model initialize at round 3500
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 3.]), 'previousTarget': array([6., 3.]), 'currentState': array([16.        , 14.        ,  5.56080723]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 14.866068747318506}
done in step count: 39
reward sum = 0.4709355112546959
running average episode reward sum: 0.4868068652574813
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 3.]), 'previousTarget': array([6., 3.]), 'currentState': array([6.98471036, 3.72041003, 3.96713241]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 1.220100446672322}
episode index:3501
target Thresh 31.999999999999986
target distance 12.0
model initialize at round 3501
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 3.]), 'previousTarget': array([5., 3.]), 'currentState': array([16.99999331,  3.99722405,  4.49960789]), 'targetState': array([5, 3], dtype=int32), 'currentDistance': 12.04135769590218}
done in step count: 26
reward sum = 0.5611988501281111
running average episode reward sum: 0.4868281079716077
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 3.]), 'previousTarget': array([5., 3.]), 'currentState': array([5.77261699, 3.05649569, 3.09169795]), 'targetState': array([5, 3], dtype=int32), 'currentDistance': 0.7746797876346099}
episode index:3502
target Thresh 31.999999999999986
target distance 17.0
model initialize at round 3502
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 25.]), 'previousTarget': array([18., 25.]), 'currentState': array([18.96761647,  7.98332366,  3.36585829]), 'targetState': array([18, 25], dtype=int32), 'currentDistance': 17.044164846018656}
done in step count: 40
reward sum = 0.39456216865683563
running average episode reward sum: 0.4868017688510497
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 25.]), 'previousTarget': array([18., 25.]), 'currentState': array([18.11439233, 24.24903081,  1.70597379]), 'targetState': array([18, 25], dtype=int32), 'currentDistance': 0.7596317045514177}
episode index:3503
target Thresh 31.999999999999986
target distance 4.0
model initialize at round 3503
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 15.]), 'previousTarget': array([ 6., 15.]), 'currentState': array([ 2.        , 18.        ,  1.37539107]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 5.0}
done in step count: 12
reward sum = 0.7789414103692761
running average episode reward sum: 0.4868851420364145
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 15.]), 'previousTarget': array([ 6., 15.]), 'currentState': array([ 5.43757028, 15.82433112,  5.50773529]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 0.9979223384512763}
episode index:3504
target Thresh 31.999999999999986
target distance 16.0
model initialize at round 3504
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([14.33153574, 25.88462072,  5.7050043 ]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 17.227582277727265}
done in step count: 38
reward sum = 0.4255772296432227
running average episode reward sum: 0.4868676504779571
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([20.53034507, 10.93947462,  5.08326765]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 1.0503277204315962}
episode index:3505
target Thresh 31.999999999999986
target distance 15.0
model initialize at round 3505
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 24.]), 'previousTarget': array([ 7., 24.]), 'currentState': array([18.        ,  9.        ,  4.67210436]), 'targetState': array([ 7, 24], dtype=int32), 'currentDistance': 18.60107523773827}
done in step count: 47
reward sum = 0.33290392937444674
running average episode reward sum: 0.4868237361251038
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 24.]), 'previousTarget': array([ 7., 24.]), 'currentState': array([ 7.15627905, 23.19474668,  2.17191857]), 'targetState': array([ 7, 24], dtype=int32), 'currentDistance': 0.8202780339012081}
episode index:3506
target Thresh 31.999999999999986
target distance 9.0
model initialize at round 3506
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 13.]), 'previousTarget': array([ 5., 13.]), 'currentState': array([13.84400786, 18.99252643,  3.43621945]), 'targetState': array([ 5, 13], dtype=int32), 'currentDistance': 10.683016801210815}
done in step count: 22
reward sum = 0.6264840802457
running average episode reward sum: 0.48686355943394916
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 13.]), 'previousTarget': array([ 5., 13.]), 'currentState': array([ 5.46769019, 13.90254331,  3.90025605]), 'targetState': array([ 5, 13], dtype=int32), 'currentDistance': 1.0165227661652354}
episode index:3507
target Thresh 31.999999999999986
target distance 8.0
model initialize at round 3507
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 18.]), 'previousTarget': array([ 7., 18.]), 'currentState': array([15.00027399, 24.01486002,  1.80486023]), 'targetState': array([ 7, 18], dtype=int32), 'currentDistance': 10.009142071796528}
done in step count: 25
reward sum = 0.5852384656346548
running average episode reward sum: 0.4868916024516802
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 18.]), 'previousTarget': array([ 7., 18.]), 'currentState': array([ 7.91295048, 18.08500036,  3.84141498]), 'targetState': array([ 7, 18], dtype=int32), 'currentDistance': 0.9168989235480421}
episode index:3508
target Thresh 31.999999999999986
target distance 5.0
model initialize at round 3508
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 27.]), 'previousTarget': array([22., 27.]), 'currentState': array([26.93021098, 25.16762569,  2.21156836]), 'targetState': array([22, 27], dtype=int32), 'currentDistance': 5.259712532422022}
done in step count: 14
reward sum = 0.7893072976958718
running average episode reward sum: 0.48697778532293823
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 27.]), 'previousTarget': array([22., 27.]), 'currentState': array([22.95933483, 27.52301223,  2.81311043]), 'targetState': array([22, 27], dtype=int32), 'currentDistance': 1.0926413438578968}
episode index:3509
target Thresh 31.999999999999986
target distance 11.0
model initialize at round 3509
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 18.]), 'previousTarget': array([23., 18.]), 'currentState': array([17.        , 29.        ,  2.82942963]), 'targetState': array([23, 18], dtype=int32), 'currentDistance': 12.52996408614167}
done in step count: 32
reward sum = 0.5179740097125198
running average episode reward sum: 0.4869866161560977
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 18.]), 'previousTarget': array([23., 18.]), 'currentState': array([22.46614788, 18.9509722 ,  5.35403375]), 'targetState': array([23, 18], dtype=int32), 'currentDistance': 1.090571510551528}
episode index:3510
target Thresh 31.999999999999986
target distance 22.0
model initialize at round 3510
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.06407315,  5.9414844 ]), 'previousTarget': array([21.06407315,  5.9414844 ]), 'currentState': array([15.        , 25.        ,  3.78472099]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.26015688942886006
running average episode reward sum: 0.48692201070843966
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  3.]), 'previousTarget': array([22.,  3.]), 'currentState': array([21.53550571,  3.89110203,  5.06605217]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 1.004896894299328}
episode index:3511
target Thresh 31.999999999999986
target distance 20.0
model initialize at round 3511
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.57385555, 26.42406832]), 'previousTarget': array([20.56953382, 26.42781353]), 'currentState': array([ 2.02896792, 18.93493333,  5.3837471 ]), 'targetState': array([22, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.2901377147820239
running average episode reward sum: 0.48686597873351756
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 27.]), 'previousTarget': array([22., 27.]), 'currentState': array([21.03870816, 26.88442389,  0.48914097]), 'targetState': array([22, 27], dtype=int32), 'currentDistance': 0.9682147657085571}
episode index:3512
target Thresh 31.999999999999986
target distance 9.0
model initialize at round 3512
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([16.        , 16.        ,  3.74791956]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 12.727922061357857}
done in step count: 30
reward sum = 0.532226165947021
running average episode reward sum: 0.48687889082779984
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([ 7.3626261 , 24.0494092 ,  2.26780361]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 1.017408746873526}
episode index:3513
target Thresh 31.999999999999986
target distance 16.0
model initialize at round 3513
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 10.]), 'previousTarget': array([22., 10.]), 'currentState': array([6.       , 7.       , 1.4549534]), 'targetState': array([22, 10], dtype=int32), 'currentDistance': 16.278820596099706}
done in step count: 37
reward sum = 0.43147558750049053
running average episode reward sum: 0.4868631243783612
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 10.]), 'previousTarget': array([22., 10.]), 'currentState': array([21.09717052, 10.06655694,  6.22634241]), 'targetState': array([22, 10], dtype=int32), 'currentDistance': 0.9052794602208454}
episode index:3514
target Thresh 31.999999999999986
target distance 3.0
model initialize at round 3514
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 6.]), 'previousTarget': array([9., 6.]), 'currentState': array([12.00563019,  8.9598196 ,  4.62343833]), 'targetState': array([9, 6], dtype=int32), 'currentDistance': 4.218334381575017}
done in step count: 7
reward sum = 0.862254663604838
running average episode reward sum: 0.48696992140232315
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 6.]), 'previousTarget': array([9., 6.]), 'currentState': array([9.98327685, 6.76230474, 3.7631149 ]), 'targetState': array([9, 6], dtype=int32), 'currentDistance': 1.2441631268475357}
episode index:3515
target Thresh 31.999999999999986
target distance 4.0
model initialize at round 3515
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([1.80000000e+01, 9.00000000e+00, 6.89581037e-03]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 4.47213595499958}
done in step count: 11
reward sum = 0.8001325259553461
running average episode reward sum: 0.48705898926482405
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.91755729,  5.72343506,  4.0767368 ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.1684475462711894}
episode index:3516
target Thresh 31.999999999999986
target distance 19.0
model initialize at round 3516
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4.50721967, 16.71622985]), 'previousTarget': array([ 4.49385478, 16.70632169]), 'currentState': array([21.01102153, 28.01333232,  1.13249874]), 'targetState': array([ 2, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.21891776782713696
running average episode reward sum: 0.48698274780294243
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 15.]), 'previousTarget': array([ 2., 15.]), 'currentState': array([ 2.8870747 , 15.84476291,  3.88654703]), 'targetState': array([ 2, 15], dtype=int32), 'currentDistance': 1.2249595508313484}
episode index:3517
target Thresh 31.999999999999986
target distance 17.0
model initialize at round 3517
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 20.]), 'previousTarget': array([ 8., 20.]), 'currentState': array([10.03040332,  3.00776338,  0.47561639]), 'targetState': array([ 8, 20], dtype=int32), 'currentDistance': 17.113113191061714}
done in step count: 45
reward sum = 0.4088344031969091
running average episode reward sum: 0.48696053394717037
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 20.]), 'previousTarget': array([ 8., 20.]), 'currentState': array([ 8.23037841, 19.27025992,  1.64697031]), 'targetState': array([ 8, 20], dtype=int32), 'currentDistance': 0.7652416635789562}
episode index:3518
target Thresh 31.999999999999986
target distance 14.0
model initialize at round 3518
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 25.]), 'previousTarget': array([ 8., 25.]), 'currentState': array([22.00097187, 18.00356829,  1.55034208]), 'targetState': array([ 8, 25], dtype=int32), 'currentDistance': 15.651749737241012}
done in step count: 38
reward sum = 0.463370921566269
running average episode reward sum: 0.4869538304483409
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 25.]), 'previousTarget': array([ 8., 25.]), 'currentState': array([ 8.96746203, 24.89249882,  2.77698752]), 'targetState': array([ 8, 25], dtype=int32), 'currentDistance': 0.973416294881277}
episode index:3519
target Thresh 31.999999999999986
target distance 7.0
model initialize at round 3519
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([15.37655894, 15.2170491 ,  0.4558734 ]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 7.627626691744965}
done in step count: 16
reward sum = 0.7424449169645913
running average episode reward sum: 0.48702641314337397
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([21.10488373, 18.44250762,  0.45864059]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 1.0545287552221023}
episode index:3520
target Thresh 31.999999999999986
target distance 14.0
model initialize at round 3520
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([9.       , 5.       , 5.3222881]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 15.231546211727817}
done in step count: 41
reward sum = 0.44024043266499346
running average episode reward sum: 0.487013125446561
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([14.8969185 , 18.02467792,  0.89001881]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 0.9807542786547998}
episode index:3521
target Thresh 31.999999999999986
target distance 13.0
model initialize at round 3521
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 24.]), 'previousTarget': array([ 7., 24.]), 'currentState': array([ 3.        , 11.        ,  3.23110485]), 'targetState': array([ 7, 24], dtype=int32), 'currentDistance': 13.601470508735442}
done in step count: 32
reward sum = 0.48931119723359817
running average episode reward sum: 0.4870137779371308
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 24.]), 'previousTarget': array([ 7., 24.]), 'currentState': array([ 6.66868737, 23.0950353 ,  1.20488494]), 'targetState': array([ 7, 24], dtype=int32), 'currentDistance': 0.9637059545821932}
episode index:3522
target Thresh 31.999999999999986
target distance 11.0
model initialize at round 3522
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([14.92481622,  4.10723355,  2.39909381]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 11.970412392768083}
done in step count: 29
reward sum = 0.5882755362739422
running average episode reward sum: 0.4870425209851969
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.85971794, 8.77747572, 2.79989816]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.888049545912028}
episode index:3523
target Thresh 31.999999999999986
target distance 16.0
model initialize at round 3523
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([24.        , 15.        ,  2.54064894]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.3293368425877465
running average episode reward sum: 0.4869977690900784
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([8.77363101, 3.48918234, 3.83925411]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 0.9153165010643033}
episode index:3524
target Thresh 31.999999999999986
target distance 14.0
model initialize at round 3524
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  7.]), 'previousTarget': array([22.,  7.]), 'currentState': array([8.        , 5.        , 1.98240474]), 'targetState': array([22,  7], dtype=int32), 'currentDistance': 14.142135623730951}
done in step count: 32
reward sum = 0.4795265680837044
running average episode reward sum: 0.4869956496004312
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  7.]), 'previousTarget': array([22.,  7.]), 'currentState': array([21.04755055,  7.03825446,  0.02736389]), 'targetState': array([22,  7], dtype=int32), 'currentDistance': 0.9532173687995031}
episode index:3525
target Thresh 31.999999999999986
target distance 8.0
model initialize at round 3525
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([5.00739711, 7.97919644, 5.30651665]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 7.992629966135979}
done in step count: 17
reward sum = 0.7037516946740534
running average episode reward sum: 0.4870571232377181
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([12.016003  ,  8.07395644,  0.07413793]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 0.9867723352657373}
episode index:3526
target Thresh 31.999999999999986
target distance 7.0
model initialize at round 3526
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 21.]), 'previousTarget': array([21., 21.]), 'currentState': array([14.38795926, 27.75634552,  5.68909308]), 'targetState': array([21, 21], dtype=int32), 'currentDistance': 9.453427295492448}
done in step count: 21
reward sum = 0.6787055398942214
running average episode reward sum: 0.487111460753073
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 21.]), 'previousTarget': array([21., 21.]), 'currentState': array([20.29261845, 21.89789233,  5.48914013]), 'targetState': array([21, 21], dtype=int32), 'currentDistance': 1.1430657409274998}
episode index:3527
target Thresh 31.99999999999999
target distance 13.0
model initialize at round 3527
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 19.]), 'previousTarget': array([26., 19.]), 'currentState': array([17.00538078,  5.99117164,  5.51225519]), 'targetState': array([26, 19], dtype=int32), 'currentDistance': 15.815586944812349}
done in step count: 37
reward sum = 0.4359591513769345
running average episode reward sum: 0.48709696179916817
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 19.]), 'previousTarget': array([26., 19.]), 'currentState': array([25.54344525, 18.14243032,  1.11573352]), 'targetState': array([26, 19], dtype=int32), 'currentDistance': 0.9715286943158794}
episode index:3528
target Thresh 31.99999999999999
target distance 5.0
model initialize at round 3528
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 8.        , 16.        ,  6.23843775]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 5.830951894845301}
done in step count: 16
reward sum = 0.7500089129934461
running average episode reward sum: 0.4871714622103879
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.75691895, 11.81096029,  4.01199442]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.10931640737901}
episode index:3529
target Thresh 31.99999999999999
target distance 17.0
model initialize at round 3529
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 28.]), 'previousTarget': array([11., 28.]), 'currentState': array([ 2.        , 11.        ,  0.16918751]), 'targetState': array([11, 28], dtype=int32), 'currentDistance': 19.235384061671343}
done in step count: 43
reward sum = 0.3585920025446038
running average episode reward sum: 0.48713503743427855
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 28.]), 'previousTarget': array([11., 28.]), 'currentState': array([10.66518187, 27.14578687,  1.28186945]), 'targetState': array([11, 28], dtype=int32), 'currentDistance': 0.9174874620385253}
episode index:3530
target Thresh 31.99999999999999
target distance 1.0
model initialize at round 3530
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 26.]), 'previousTarget': array([23., 26.]), 'currentState': array([23.83708423, 26.0946627 ,  2.6442073 ]), 'targetState': array([23, 26], dtype=int32), 'currentDistance': 0.8424197546545831}
done in step count: 0
reward sum = 0.9984477252954306
running average episode reward sum: 0.4872798441994616
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 26.]), 'previousTarget': array([23., 26.]), 'currentState': array([23.83708423, 26.0946627 ,  2.6442073 ]), 'targetState': array([23, 26], dtype=int32), 'currentDistance': 0.8424197546545831}
episode index:3531
target Thresh 31.99999999999999
target distance 14.0
model initialize at round 3531
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  7.]), 'previousTarget': array([20.,  7.]), 'currentState': array([13.95453386, 20.95731951,  4.1398755 ]), 'targetState': array([20,  7], dtype=int32), 'currentDistance': 15.210339533774325}
done in step count: 36
reward sum = 0.47586081013481935
running average episode reward sum: 0.4872766111773595
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  7.]), 'previousTarget': array([20.,  7.]), 'currentState': array([19.46782607,  7.75155928,  5.21199417]), 'targetState': array([20,  7], dtype=int32), 'currentDistance': 0.9208965395856024}
episode index:3532
target Thresh 31.99999999999999
target distance 16.0
model initialize at round 3532
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  8.]), 'previousTarget': array([17.,  8.]), 'currentState': array([ 5.90064616, 23.97288946,  3.66047621]), 'targetState': array([17,  8], dtype=int32), 'currentDistance': 19.450677454206314}
done in step count: 48
reward sum = 0.3439930657949303
running average episode reward sum: 0.4872360554045368
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  8.]), 'previousTarget': array([17.,  8.]), 'currentState': array([16.17598393,  8.7234358 ,  5.52068131]), 'targetState': array([17,  8], dtype=int32), 'currentDistance': 1.0965226085554542}
episode index:3533
target Thresh 31.99999999999999
target distance 7.0
model initialize at round 3533
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  7.]), 'previousTarget': array([17.,  7.]), 'currentState': array([10.        ,  3.        ,  5.76133633]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 18
reward sum = 0.7075894041317511
running average episode reward sum: 0.4872984077952349
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  7.]), 'previousTarget': array([17.,  7.]), 'currentState': array([16.03087053,  6.41550707,  0.67494674]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 1.1317437538787547}
episode index:3534
target Thresh 31.99999999999999
target distance 6.0
model initialize at round 3534
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 27.]), 'previousTarget': array([13., 27.]), 'currentState': array([ 7.98438568, 21.02358151,  1.95566976]), 'targetState': array([13, 27], dtype=int32), 'currentDistance': 7.802176933522081}
done in step count: 17
reward sum = 0.7052946902805188
running average episode reward sum: 0.48736007576764945
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 27.]), 'previousTarget': array([13., 27.]), 'currentState': array([12.03507977, 26.46120468,  0.43091454]), 'targetState': array([13, 27], dtype=int32), 'currentDistance': 1.1051567552856387}
episode index:3535
target Thresh 31.99999999999999
target distance 14.0
model initialize at round 3535
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  3.]), 'previousTarget': array([10.,  3.]), 'currentState': array([23.59579591, 11.84990149,  3.4871819 ]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 16.22240496024534}
done in step count: 40
reward sum = 0.47214402714944115
running average episode reward sum: 0.4873557725864791
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  3.]), 'previousTarget': array([10.,  3.]), 'currentState': array([10.92818505,  3.75857267,  3.85075386]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 1.1987326627853025}
episode index:3536
target Thresh 31.99999999999999
target distance 5.0
model initialize at round 3536
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 28.]), 'previousTarget': array([16., 28.]), 'currentState': array([20.9759108 , 24.15786066,  1.95707005]), 'targetState': array([16, 28], dtype=int32), 'currentDistance': 6.286630497085005}
done in step count: 13
reward sum = 0.788025912374121
running average episode reward sum: 0.4874407796941375
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 28.]), 'previousTarget': array([16., 28.]), 'currentState': array([16.97787422, 27.15914289,  2.54581777]), 'targetState': array([16, 28], dtype=int32), 'currentDistance': 1.289681615020476}
episode index:3537
target Thresh 31.99999999999999
target distance 20.0
model initialize at round 3537
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.9223227 , 24.61161351]), 'previousTarget': array([15.9223227 , 24.61161351]), 'currentState': array([12.        ,  5.        ,  0.59184039]), 'targetState': array([16, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 82
reward sum = 0.24791439393059816
running average episode reward sum: 0.4873730786241082
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 25.]), 'previousTarget': array([16., 25.]), 'currentState': array([16.09335804, 24.31273264,  1.66673291]), 'targetState': array([16, 25], dtype=int32), 'currentDistance': 0.6935792327470137}
episode index:3538
target Thresh 31.99999999999999
target distance 7.0
model initialize at round 3538
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 21.]), 'previousTarget': array([21., 21.]), 'currentState': array([25.92892646, 14.26445919,  1.96589601]), 'targetState': array([21, 21], dtype=int32), 'currentDistance': 8.346366038454002}
done in step count: 16
reward sum = 0.7255577635426614
running average episode reward sum: 0.48744038144550367
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 21.]), 'previousTarget': array([21., 21.]), 'currentState': array([21.67977089, 20.0278271 ,  2.23097876]), 'targetState': array([21, 21], dtype=int32), 'currentDistance': 1.1862582357236024}
episode index:3539
target Thresh 31.99999999999999
target distance 25.0
model initialize at round 3539
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.50630739, 12.32743329]), 'previousTarget': array([15.49390095, 12.38262381]), 'currentState': array([ 2.98007617, 27.91889029,  4.67244485]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.20219075059298786
running average episode reward sum: 0.48724557039125554
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.91987884,  8.70904649]), 'previousTarget': array([16.91987884,  8.70904649]), 'currentState': array([ 2.33982744, 22.39926567,  0.31865003]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 20.0}
episode index:3540
target Thresh 31.99999999999999
target distance 19.0
model initialize at round 3540
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.11728508,  9.70177604]), 'previousTarget': array([14.11728508,  9.70177604]), 'currentState': array([27.        , 25.        ,  3.04086497]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 98
reward sum = 0.1691589400679934
running average episode reward sum: 0.4871557407865328
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  6.]), 'previousTarget': array([11.,  6.]), 'currentState': array([11.61274108,  6.81474538,  4.13118159]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 1.0194418428460121}
episode index:3541
target Thresh 31.99999999999999
target distance 18.0
model initialize at round 3541
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([23.        , 12.        ,  5.71915957]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 18.110770276274835}
done in step count: 74
reward sum = 0.2621574959989567
running average episode reward sum: 0.4870922178489869
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 5.81246771, 10.27778053,  2.99468281]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.8586418353391574}
episode index:3542
target Thresh 31.99999999999999
target distance 9.0
model initialize at round 3542
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 12.]), 'previousTarget': array([ 4., 12.]), 'currentState': array([ 8.        , 21.        ,  0.02262276]), 'targetState': array([ 4, 12], dtype=int32), 'currentDistance': 9.848857801796104}
done in step count: 27
reward sum = 0.6050388370450882
running average episode reward sum: 0.487125507891097
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 12.]), 'previousTarget': array([ 4., 12.]), 'currentState': array([ 4.09084534, 12.83905636,  4.16477397]), 'targetState': array([ 4, 12], dtype=int32), 'currentDistance': 0.8439599864187827}
episode index:3543
target Thresh 31.99999999999999
target distance 6.0
model initialize at round 3543
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([23.99243533, 15.00745312,  2.58697954]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 6.718254111861046}
done in step count: 18
reward sum = 0.7063256318758082
running average episode reward sum: 0.48718735894188275
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([26.12620259,  9.88635718,  5.17773364]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 1.244648934282845}
episode index:3544
target Thresh 31.99999999999999
target distance 10.0
model initialize at round 3544
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([ 5.32190854, 10.9448271 ,  6.03903398]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 12.521410939849273}
done in step count: 29
reward sum = 0.5755861955220887
running average episode reward sum: 0.4872122951440211
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.24206336,  3.55060072,  5.61925848]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.9368186080989546}
episode index:3545
target Thresh 31.99999999999999
target distance 17.0
model initialize at round 3545
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.7967634 , 14.13166758]), 'previousTarget': array([18.79140314, 14.13497444]), 'currentState': array([ 2.01144825, 25.00609608,  0.58496431]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.05268955443993177
running average episode reward sum: 0.48706003855925406
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 14.]), 'previousTarget': array([19., 14.]), 'currentState': array([12.59148316, 22.97393563,  5.252449  ]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 11.027266604329542}
episode index:3546
target Thresh 31.99999999999999
target distance 6.0
model initialize at round 3546
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 29.]), 'previousTarget': array([17., 29.]), 'currentState': array([15.        , 23.        ,  2.74212176]), 'targetState': array([17, 29], dtype=int32), 'currentDistance': 6.324555320336759}
done in step count: 15
reward sum = 0.7348714592077694
running average episode reward sum: 0.48712990363414793
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 29.]), 'previousTarget': array([17., 29.]), 'currentState': array([17.13459023, 28.15468378,  0.81317857]), 'targetState': array([17, 29], dtype=int32), 'currentDistance': 0.8559638141268715}
episode index:3547
target Thresh 31.99999999999999
target distance 12.0
model initialize at round 3547
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 24.]), 'previousTarget': array([19., 24.]), 'currentState': array([ 7.3075514 , 14.14179877,  0.45200115]), 'targetState': array([19, 24], dtype=int32), 'currentDistance': 15.293707388316692}
done in step count: 36
reward sum = 0.4993823035861602
running average episode reward sum: 0.48713335695995175
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 24.]), 'previousTarget': array([19., 24.]), 'currentState': array([18.34924244, 23.21155779,  0.6680575 ]), 'targetState': array([19, 24], dtype=int32), 'currentDistance': 1.0223142961216236}
episode index:3548
target Thresh 31.99999999999999
target distance 12.0
model initialize at round 3548
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 16.]), 'previousTarget': array([19., 16.]), 'currentState': array([22.98939576, 28.00474974,  2.95431167]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 12.650268571298055}
done in step count: 29
reward sum = 0.5455844933836631
running average episode reward sum: 0.4871498267081692
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 16.]), 'previousTarget': array([19., 16.]), 'currentState': array([19.25018582, 16.98336282,  4.45265895]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 1.0146897964465522}
episode index:3549
target Thresh 31.99999999999999
target distance 18.0
model initialize at round 3549
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([16.        , 21.        ,  5.83722389]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 18.11077027627483}
done in step count: 48
reward sum = 0.38723031668381525
running average episode reward sum: 0.48712168036731723
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.0250451 ,  3.78765872,  4.70595277]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.788056800613322}
episode index:3550
target Thresh 31.99999999999999
target distance 7.0
model initialize at round 3550
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([ 9.        , 25.        ,  2.05698648]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 7.6157731058639095}
done in step count: 22
reward sum = 0.6649230923266758
running average episode reward sum: 0.4871717511676437
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([15.19114759, 22.39408977,  5.95913488]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 0.8997493937886092}
episode index:3551
target Thresh 31.99999999999999
target distance 3.0
model initialize at round 3551
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 26.]), 'previousTarget': array([20., 26.]), 'currentState': array([18.99417937, 23.00351951,  2.39399329]), 'targetState': array([20, 26], dtype=int32), 'currentDistance': 3.1607863669206684}
done in step count: 6
reward sum = 0.8782734922905266
running average episode reward sum: 0.48728185863980666
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 26.]), 'previousTarget': array([20., 26.]), 'currentState': array([19.44131504, 25.17876292,  0.93664227]), 'targetState': array([20, 26], dtype=int32), 'currentDistance': 0.9932568748375321}
episode index:3552
target Thresh 31.99999999999999
target distance 16.0
model initialize at round 3552
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([18.82030872, 21.6899856 ,  4.22347215]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 16.148387130118884}
done in step count: 39
reward sum = 0.4612754555858841
running average episode reward sum: 0.4872745390780127
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.20968967,  6.86365866,  4.55310131]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.8887497067214526}
episode index:3553
target Thresh 31.99999999999999
target distance 21.0
model initialize at round 3553
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.16928442,  8.58173352]), 'previousTarget': array([10.16928442,  8.58173352]), 'currentState': array([25.        , 22.        ,  5.58155286]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.02749262462977492
running average episode reward sum: 0.48712969744500545
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9.23140626, 6.8258895 ]), 'previousTarget': array([9.2460547 , 6.85011242]), 'currentState': array([25.37490752, 18.63213221,  4.77521924]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 20.0}
episode index:3554
target Thresh 31.99999999999999
target distance 12.0
model initialize at round 3554
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([13.97711284, 22.99660355,  3.54141712]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 12.040132679796798}
done in step count: 26
reward sum = 0.5649029663971848
running average episode reward sum: 0.487151574595203
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.90471409, 11.96486796,  4.74213125]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.9695615396480819}
episode index:3555
target Thresh 31.99999999999999
target distance 24.0
model initialize at round 3555
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.6176698 , 18.99023733]), 'previousTarget': array([20.5999788 , 18.95996608]), 'currentState': array([10.01555381,  2.03160719,  1.22763392]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.08558863865478782
running average episode reward sum: 0.4869905115431079
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 26.]), 'previousTarget': array([25., 26.]), 'currentState': array([16.56961192, 15.72200252,  0.88978211]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 13.293181534967328}
episode index:3556
target Thresh 31.99999999999999
target distance 14.0
model initialize at round 3556
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([ 7.09416775, 13.86712297,  5.58141899]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 14.030620759275015}
done in step count: 29
reward sum = 0.5180697927781781
running average episode reward sum: 0.48699924904134656
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([20.20327711, 12.51876075,  6.06716122]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 0.9507260835917769}
episode index:3557
target Thresh 31.99999999999999
target distance 22.0
model initialize at round 3557
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.99403103,  5.90665672]), 'previousTarget': array([13.,  6.]), 'currentState': array([12.93141938, 25.90655872,  4.33174348]), 'targetState': array([13,  4], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 63
reward sum = 0.29932361393426643
running average episode reward sum: 0.48694650153288477
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  4.]), 'previousTarget': array([13.,  4.]), 'currentState': array([13.16785159,  4.92671599,  4.63519117]), 'targetState': array([13,  4], dtype=int32), 'currentDistance': 0.9417943925350709}
episode index:3558
target Thresh 31.99999999999999
target distance 12.0
model initialize at round 3558
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([16.21749066,  6.28947268,  1.09617597]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 12.036561317504388}
done in step count: 24
reward sum = 0.59342973902623
running average episode reward sum: 0.4869764209589858
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([18.59324313, 17.00319511,  1.31310479]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 1.076601664755042}
episode index:3559
target Thresh 31.99999999999999
target distance 8.0
model initialize at round 3559
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([ 7.74854723, 11.81001568,  3.94094434]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 9.14029790650045}
done in step count: 19
reward sum = 0.696841125562762
running average episode reward sum: 0.4870353717187059
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.95384679, 4.92410494, 4.08521813]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.3280789307693062}
episode index:3560
target Thresh 31.999999999999993
target distance 5.0
model initialize at round 3560
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  2.]), 'previousTarget': array([11.,  2.]), 'currentState': array([11.90410518,  6.63866547,  4.53231823]), 'targetState': array([11,  2], dtype=int32), 'currentDistance': 4.7259521247385425}
done in step count: 8
reward sum = 0.8486828280458735
running average episode reward sum: 0.4871369295553606
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  2.]), 'previousTarget': array([11.,  2.]), 'currentState': array([11.46923448,  2.95509888,  4.48886478]), 'targetState': array([11,  2], dtype=int32), 'currentDistance': 1.064140435028558}
episode index:3561
target Thresh 31.999999999999993
target distance 9.0
model initialize at round 3561
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 26.]), 'previousTarget': array([15., 26.]), 'currentState': array([ 5.99729279, 25.9854081 ,  4.78144646]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 9.002719035515499}
done in step count: 22
reward sum = 0.6459974240360911
running average episode reward sum: 0.48718152823432764
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 26.]), 'previousTarget': array([15., 26.]), 'currentState': array([14.18667003, 25.98267437,  6.19307132]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 0.8135144897854435}
episode index:3562
target Thresh 31.999999999999993
target distance 13.0
model initialize at round 3562
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 14.]), 'previousTarget': array([ 3., 14.]), 'currentState': array([ 5.9545309 , 26.63254537,  4.51019898]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 12.973451939868415}
done in step count: 27
reward sum = 0.5667980903358966
running average episode reward sum: 0.48720387360679507
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 14.]), 'previousTarget': array([ 3., 14.]), 'currentState': array([ 3.35456115, 14.96937686,  4.29216766]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 1.0321846301984545}
episode index:3563
target Thresh 31.999999999999993
target distance 7.0
model initialize at round 3563
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 14.]), 'previousTarget': array([24., 14.]), 'currentState': array([18.        , 21.        ,  3.52091283]), 'targetState': array([24, 14], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 19
reward sum = 0.6495351278537835
running average episode reward sum: 0.4872494210967634
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 14.]), 'previousTarget': array([24., 14.]), 'currentState': array([23.06813143, 14.9040128 ,  5.47963376]), 'targetState': array([24, 14], dtype=int32), 'currentDistance': 1.298313586140062}
episode index:3564
target Thresh 31.999999999999993
target distance 7.0
model initialize at round 3564
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 27.]), 'previousTarget': array([ 3., 27.]), 'currentState': array([ 9.9984321 , 24.96736213,  4.4360176 ]), 'targetState': array([ 3, 27], dtype=int32), 'currentDistance': 7.287638060468747}
done in step count: 17
reward sum = 0.7100512496114924
running average episode reward sum: 0.4873119181033594
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 27.]), 'previousTarget': array([ 3., 27.]), 'currentState': array([ 3.93852095, 26.66938664,  2.74393732]), 'targetState': array([ 3, 27], dtype=int32), 'currentDistance': 0.9950511373848833}
episode index:3565
target Thresh 31.999999999999993
target distance 16.0
model initialize at round 3565
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 13.]), 'previousTarget': array([ 7., 13.]), 'currentState': array([23.        , 19.        ,  2.48457563]), 'targetState': array([ 7, 13], dtype=int32), 'currentDistance': 17.08800749063506}
done in step count: 44
reward sum = 0.4225287863412516
running average episode reward sum: 0.48729375121279234
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 13.]), 'previousTarget': array([ 7., 13.]), 'currentState': array([ 7.84652283, 13.38525733,  3.50654223]), 'targetState': array([ 7, 13], dtype=int32), 'currentDistance': 0.9300667304671892}
episode index:3566
target Thresh 31.999999999999993
target distance 5.0
model initialize at round 3566
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 20.]), 'previousTarget': array([10., 20.]), 'currentState': array([15.        , 23.        ,  1.72794473]), 'targetState': array([10, 20], dtype=int32), 'currentDistance': 5.830951894845301}
done in step count: 13
reward sum = 0.7590248701292267
running average episode reward sum: 0.48736993038826654
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 20.]), 'previousTarget': array([10., 20.]), 'currentState': array([10.9821538 , 20.75109188,  3.74218937]), 'targetState': array([10, 20], dtype=int32), 'currentDistance': 1.2364324070367332}
episode index:3567
target Thresh 31.999999999999993
target distance 5.0
model initialize at round 3567
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 12.]), 'previousTarget': array([ 2., 12.]), 'currentState': array([ 6.59100885, 12.00145401,  3.16102192]), 'targetState': array([ 2, 12], dtype=int32), 'currentDistance': 4.591009077742503}
done in step count: 9
reward sum = 0.8475746162426526
running average episode reward sum: 0.48747088461636473
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 12.]), 'previousTarget': array([ 2., 12.]), 'currentState': array([ 2.94484431, 11.85919019,  3.11468716]), 'targetState': array([ 2, 12], dtype=int32), 'currentDistance': 0.9552791064041397}
episode index:3568
target Thresh 31.999999999999993
target distance 15.0
model initialize at round 3568
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 18.]), 'previousTarget': array([ 3., 18.]), 'currentState': array([4.05011085, 3.2318163 , 1.56911656]), 'targetState': array([ 3, 18], dtype=int32), 'currentDistance': 14.805471375298566}
done in step count: 35
reward sum = 0.5005239219079853
running average episode reward sum: 0.48747454195379586
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 18.]), 'previousTarget': array([ 3., 18.]), 'currentState': array([ 3.11554523, 17.11005132,  1.60951277]), 'targetState': array([ 3, 18], dtype=int32), 'currentDistance': 0.897418164634354}
episode index:3569
target Thresh 31.999999999999993
target distance 1.0
model initialize at round 3569
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 21.]), 'previousTarget': array([ 4., 21.]), 'currentState': array([ 4.96858191, 21.91531638,  4.20596215]), 'targetState': array([ 4, 21], dtype=int32), 'currentDistance': 1.3326496105389678}
done in step count: 0
reward sum = 0.9960872265305716
running average episode reward sum: 0.48761701049289297
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 21.]), 'previousTarget': array([ 4., 21.]), 'currentState': array([ 4.96858191, 21.91531638,  4.20596215]), 'targetState': array([ 4, 21], dtype=int32), 'currentDistance': 1.3326496105389678}
episode index:3570
target Thresh 31.999999999999993
target distance 4.0
model initialize at round 3570
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([16.33561106,  3.03014809,  0.18613262]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 4.160296003518007}
done in step count: 7
reward sum = 0.8711923818799869
running average episode reward sum: 0.4877244244865606
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([19.02125329,  4.50195023,  0.53342186]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 1.0981797173016872}
episode index:3571
target Thresh 31.999999999999993
target distance 8.0
model initialize at round 3571
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 26.]), 'previousTarget': array([18., 26.]), 'currentState': array([14.88085428, 18.34040576,  1.77503556]), 'targetState': array([18, 26], dtype=int32), 'currentDistance': 8.270335778399117}
done in step count: 17
reward sum = 0.7133183151105215
running average episode reward sum: 0.4877875806709458
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 26.]), 'previousTarget': array([18., 26.]), 'currentState': array([17.20212289, 25.13286557,  0.85304802]), 'targetState': array([18, 26], dtype=int32), 'currentDistance': 1.1783590299926927}
episode index:3572
target Thresh 31.999999999999993
target distance 7.0
model initialize at round 3572
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([13.        , 21.        ,  3.41892767]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 7.071067811865475}
done in step count: 16
reward sum = 0.7228069294408114
running average episode reward sum: 0.48785335714695194
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([13.80925065, 14.99089364,  4.83610313]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 1.0090864764665959}
episode index:3573
target Thresh 31.999999999999993
target distance 14.0
model initialize at round 3573
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'previousTarget': array([25.,  3.]), 'currentState': array([10.9950033 ,  6.99006403,  4.49893236]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 14.562298700691981}
done in step count: 35
reward sum = 0.4807062299062028
running average episode reward sum: 0.4878513573911487
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'previousTarget': array([25.,  3.]), 'currentState': array([24.05384604,  3.02372821,  5.94382043]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 0.9464514520955438}
episode index:3574
target Thresh 31.999999999999993
target distance 21.0
model initialize at round 3574
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.53040808, 22.6565873 ]), 'previousTarget': array([23.45612429, 22.63241055]), 'currentState': array([ 4.05507442, 18.1056117 ,  0.89998949]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.3193365407477093
running average episode reward sum: 0.48780422037950016
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 23.]), 'previousTarget': array([25., 23.]), 'currentState': array([24.02375939, 22.74628284,  0.28369701]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 1.0086714700347958}
episode index:3575
target Thresh 31.999999999999993
target distance 22.0
model initialize at round 3575
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6.31935038, 19.42050636]), 'previousTarget': array([ 6.3226018 , 19.42229124]), 'currentState': array([25.99852874, 22.98841038,  4.33361816]), 'targetState': array([ 4, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.22286604110746533
running average episode reward sum: 0.4877301325217619
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 19.]), 'previousTarget': array([ 4., 19.]), 'currentState': array([ 4.95177418, 18.88073707,  2.83493765]), 'targetState': array([ 4, 19], dtype=int32), 'currentDistance': 0.959217251966225}
episode index:3576
target Thresh 31.999999999999993
target distance 6.0
model initialize at round 3576
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 19.]), 'previousTarget': array([ 4., 19.]), 'currentState': array([ 2.32356933, 13.14853502,  0.66961384]), 'targetState': array([ 4, 19], dtype=int32), 'currentDistance': 6.086876222404147}
done in step count: 12
reward sum = 0.7751387596310069
running average episode reward sum: 0.48781048159280166
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 19.]), 'previousTarget': array([ 4., 19.]), 'currentState': array([ 3.89813139, 18.23026644,  1.33224006]), 'targetState': array([ 4, 19], dtype=int32), 'currentDistance': 0.7764450820021146}
episode index:3577
target Thresh 31.999999999999993
target distance 19.0
model initialize at round 3577
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([22.97024476, 10.00764921,  2.94237867]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 19.20718979908217}
done in step count: 60
reward sum = 0.3540886083445618
running average episode reward sum: 0.48777310823527
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.97401933, 7.30673013, 3.32466779]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.0211743398425384}
episode index:3578
target Thresh 31.999999999999993
target distance 1.0
model initialize at round 3578
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 26.]), 'previousTarget': array([26., 26.]), 'currentState': array([26.        , 25.        ,  4.15311084]), 'targetState': array([26, 26], dtype=int32), 'currentDistance': 1.0}
done in step count: 10
reward sum = 0.8856574789046481
running average episode reward sum: 0.48788428017454616
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 26.]), 'previousTarget': array([26., 26.]), 'currentState': array([25.97108173, 25.04249127,  2.03931873]), 'targetState': array([26, 26], dtype=int32), 'currentDistance': 0.957945314582512}
episode index:3579
target Thresh 31.999999999999993
target distance 11.0
model initialize at round 3579
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 18.]), 'previousTarget': array([25., 18.]), 'currentState': array([19.83466716,  7.24630852,  1.96264166]), 'targetState': array([25, 18], dtype=int32), 'currentDistance': 11.929901247476446}
done in step count: 25
reward sum = 0.5757252825898704
running average episode reward sum: 0.4879088167673996
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 18.]), 'previousTarget': array([25., 18.]), 'currentState': array([24.65363747, 17.17538388,  1.04771844]), 'targetState': array([25, 18], dtype=int32), 'currentDistance': 0.8944041254468663}
episode index:3580
target Thresh 31.999999999999993
target distance 15.0
model initialize at round 3580
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 17.]), 'previousTarget': array([ 4., 17.]), 'currentState': array([18.99649445, 27.00340961,  2.59911218]), 'targetState': array([ 4, 17], dtype=int32), 'currentDistance': 18.026731528982186}
done in step count: 46
reward sum = 0.39797669368495936
running average episode reward sum: 0.48788370307762513
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 17.]), 'previousTarget': array([ 4., 17.]), 'currentState': array([ 4.94081787, 17.70400336,  3.85283547]), 'targetState': array([ 4, 17], dtype=int32), 'currentDistance': 1.1750570207516569}
episode index:3581
target Thresh 31.999999999999993
target distance 17.0
model initialize at round 3581
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.52185312, 18.79945614]), 'previousTarget': array([18.43860471, 18.71414506]), 'currentState': array([2.99725784, 6.19045375, 1.45833161]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.3174903659526681
running average episode reward sum: 0.48783613374844453
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 20.]), 'previousTarget': array([20., 20.]), 'currentState': array([19.04045078, 19.24515666,  0.77306467]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 1.2208698435873053}
episode index:3582
target Thresh 31.999999999999993
target distance 2.0
model initialize at round 3582
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([ 2.00529381, 23.00587005,  0.71121022]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 2.233971202340157}
done in step count: 6
reward sum = 0.9218210239383541
running average episode reward sum: 0.48795725707810955
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([ 3.03049977, 22.96067519,  5.89691513]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 1.3648543910888797}
episode index:3583
target Thresh 31.999999999999993
target distance 1.0
model initialize at round 3583
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 22.]), 'previousTarget': array([27., 22.]), 'currentState': array([27.01017526, 23.01844623,  1.02711467]), 'targetState': array([27, 22], dtype=int32), 'currentDistance': 1.0184970553579842}
done in step count: 18
reward sum = 0.8162693865111332
running average episode reward sum: 0.48804886202493797
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 22.]), 'previousTarget': array([27., 22.]), 'currentState': array([27.40198194, 22.92918861,  4.71264474]), 'targetState': array([27, 22], dtype=int32), 'currentDistance': 1.0124134317659454}
episode index:3584
target Thresh 31.999999999999993
target distance 13.0
model initialize at round 3584
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  9.]), 'previousTarget': array([23.,  9.]), 'currentState': array([26.        , 22.        ,  3.33224756]), 'targetState': array([23,  9], dtype=int32), 'currentDistance': 13.341664064126334}
done in step count: 35
reward sum = 0.5129444940925393
running average episode reward sum: 0.48805580641324137
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  9.]), 'previousTarget': array([23.,  9.]), 'currentState': array([22.6163918 ,  9.97089203,  4.34589665]), 'targetState': array([23,  9], dtype=int32), 'currentDistance': 1.0439284398757234}
episode index:3585
target Thresh 31.999999999999993
target distance 20.0
model initialize at round 3585
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.91558267, 15.00834746]), 'previousTarget': array([23.9007438 , 15.00992562]), 'currentState': array([ 4.01265035, 16.97641383,  5.44102979]), 'targetState': array([24, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 77
reward sum = 0.27658517193134036
running average episode reward sum: 0.4879968352379815
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 15.]), 'previousTarget': array([24., 15.]), 'currentState': array([23.06240326, 14.93184438,  0.12784477]), 'targetState': array([24, 15], dtype=int32), 'currentDistance': 0.9400706499516722}
episode index:3586
target Thresh 31.999999999999993
target distance 8.0
model initialize at round 3586
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 19.]), 'previousTarget': array([24., 19.]), 'currentState': array([26.84467529, 11.30479898,  1.99899953]), 'targetState': array([24, 19], dtype=int32), 'currentDistance': 8.204163346034095}
done in step count: 19
reward sum = 0.7137186052343897
running average episode reward sum: 0.48805976296867465
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 19.]), 'previousTarget': array([24., 19.]), 'currentState': array([24.15135573, 18.14095251,  1.91969862]), 'targetState': array([24, 19], dtype=int32), 'currentDistance': 0.8722792871794712}
episode index:3587
target Thresh 31.999999999999993
target distance 9.0
model initialize at round 3587
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([20.63131664, 25.82818936,  3.49665682]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 9.442174578878115}
done in step count: 20
reward sum = 0.6813572204712717
running average episode reward sum: 0.48811363628458954
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([12.91547891, 22.52936848,  3.59747094]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 1.0575124684887172}
episode index:3588
target Thresh 31.999999999999993
target distance 18.0
model initialize at round 3588
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 20.]), 'previousTarget': array([18., 20.]), 'currentState': array([19.06852978,  2.00008315,  0.25371331]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 18.03160454806105}
done in step count: 43
reward sum = 0.3806476620072307
running average episode reward sum: 0.4880836931321021
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 20.]), 'previousTarget': array([18., 20.]), 'currentState': array([18.27940803, 19.23273706,  1.70456494]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 0.8165545098004713}
episode index:3589
target Thresh 31.999999999999993
target distance 5.0
model initialize at round 3589
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 2.]), 'previousTarget': array([8., 2.]), 'currentState': array([3.0357445 , 3.03463825, 0.57343522]), 'targetState': array([8, 2], dtype=int32), 'currentDistance': 5.070927826726117}
done in step count: 10
reward sum = 0.8213240119384239
running average episode reward sum: 0.48817651773344095
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 2.]), 'previousTarget': array([8., 2.]), 'currentState': array([7.04602989, 2.26622684, 5.88800577]), 'targetState': array([8, 2], dtype=int32), 'currentDistance': 0.9904219811563201}
episode index:3590
target Thresh 31.999999999999993
target distance 10.0
model initialize at round 3590
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 21.]), 'previousTarget': array([19., 21.]), 'currentState': array([20.88307691, 11.09890417,  2.29135585]), 'targetState': array([19, 21], dtype=int32), 'currentDistance': 10.078575162414142}
done in step count: 21
reward sum = 0.6564926783800713
running average episode reward sum: 0.48822338940168003
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 21.]), 'previousTarget': array([19., 21.]), 'currentState': array([19.41587774, 20.05507543,  1.7331131 ]), 'targetState': array([19, 21], dtype=int32), 'currentDistance': 1.032393689192078}
episode index:3591
target Thresh 31.999999999999993
target distance 15.0
model initialize at round 3591
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([25.58802853,  5.02209771,  3.02286574]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 15.413957559013433}
done in step count: 37
reward sum = 0.4835131861268965
running average episode reward sum: 0.48822207809787305
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([11.95464213,  9.61344842,  2.93797548]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 1.0299338484545362}
episode index:3592
target Thresh 31.999999999999993
target distance 14.0
model initialize at round 3592
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 18.]), 'previousTarget': array([ 8., 18.]), 'currentState': array([22.        , 16.        ,  4.53775445]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 14.142135623730951}
done in step count: 37
reward sum = 0.49125867757331887
running average episode reward sum: 0.4882229232410613
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 18.]), 'previousTarget': array([ 8., 18.]), 'currentState': array([ 8.78087069, 17.9562255 ,  3.12419054]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 0.7820966926024702}
episode index:3593
target Thresh 31.999999999999993
target distance 20.0
model initialize at round 3593
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6.99999739, 23.99309244]), 'previousTarget': array([ 7., 24.]), 'currentState': array([6.99244846, 3.99309386, 3.66283631]), 'targetState': array([ 7, 24], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.3217483823377641
running average episode reward sum: 0.48817660311281885
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 24.]), 'previousTarget': array([ 7., 24.]), 'currentState': array([ 7.02215956, 23.04003936,  1.44979593]), 'targetState': array([ 7, 24], dtype=int32), 'currentDistance': 0.960216366595464}
episode index:3594
target Thresh 31.999999999999993
target distance 23.0
model initialize at round 3594
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.54352728, 20.24859289]), 'previousTarget': array([23.54352728, 20.24859289]), 'currentState': array([ 4.        , 16.        ,  2.22650933]), 'targetState': array([27, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.10318725608206167
running average episode reward sum: 0.48801210690720137
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 21.]), 'previousTarget': array([27., 21.]), 'currentState': array([16.51210273, 17.99525067,  0.20357192]), 'targetState': array([27, 21], dtype=int32), 'currentDistance': 10.90983536164441}
episode index:3595
target Thresh 31.999999999999993
target distance 9.0
model initialize at round 3595
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 17.]), 'previousTarget': array([26., 17.]), 'currentState': array([17.        , 18.        ,  2.24131227]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 9.055385138137416}
done in step count: 23
reward sum = 0.6161805909643372
running average episode reward sum: 0.48804774886606045
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 17.]), 'previousTarget': array([26., 17.]), 'currentState': array([25.05891347, 17.04599337,  6.16270367]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 0.9422097731469006}
episode index:3596
target Thresh 31.999999999999993
target distance 8.0
model initialize at round 3596
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([13.        , 17.        ,  0.17270654]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 8.246211251235321}
done in step count: 20
reward sum = 0.6586319957935615
running average episode reward sum: 0.4880951728991234
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([11.40380635,  9.79891401,  4.47068171]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.8951665587761415}
episode index:3597
target Thresh 31.999999999999993
target distance 13.0
model initialize at round 3597
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 10.]), 'previousTarget': array([22., 10.]), 'currentState': array([ 9.02269221, 13.03182159,  0.71737361]), 'targetState': array([22, 10], dtype=int32), 'currentDistance': 13.32675728405837}
done in step count: 32
reward sum = 0.529638296453797
running average episode reward sum: 0.48810671907020586
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 10.]), 'previousTarget': array([22., 10.]), 'currentState': array([21.22615761, 10.08608854,  6.00096037]), 'targetState': array([22, 10], dtype=int32), 'currentDistance': 0.7786162558534375}
episode index:3598
target Thresh 31.999999999999993
target distance 14.0
model initialize at round 3598
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([26.01833423,  4.01438198,  0.90271938]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 15.662456818132267}
done in step count: 38
reward sum = 0.43540733068934306
running average episode reward sum: 0.4880920762837705
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.83425828, 10.42263753,  2.75562664]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.0145611324713597}
episode index:3599
target Thresh 31.999999999999993
target distance 14.0
model initialize at round 3599
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([18.0292101 , 18.02326219,  0.9250266 ]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 14.59470364676965}
done in step count: 38
reward sum = 0.44020850149134794
running average episode reward sum: 0.4880787752907726
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([ 4.75769633, 14.32549826,  3.47074886]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 0.8246531670545922}
episode index:3600
target Thresh 31.999999999999993
target distance 14.0
model initialize at round 3600
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 18.]), 'previousTarget': array([10., 18.]), 'currentState': array([18.05382383,  4.00066287,  0.25190615]), 'targetState': array([10, 18], dtype=int32), 'currentDistance': 16.150712633346586}
done in step count: 39
reward sum = 0.4257906103150302
running average episode reward sum: 0.48806147782757464
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 18.]), 'previousTarget': array([10., 18.]), 'currentState': array([10.45162891, 17.03935234,  2.13639104]), 'targetState': array([10, 18], dtype=int32), 'currentDistance': 1.0615142979010967}
episode index:3601
target Thresh 31.999999999999993
target distance 11.0
model initialize at round 3601
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 22.]), 'previousTarget': array([ 3., 22.]), 'currentState': array([13.98533064, 27.06617886,  2.04143167]), 'targetState': array([ 3, 22], dtype=int32), 'currentDistance': 12.097258263218828}
done in step count: 28
reward sum = 0.5514140810284532
running average episode reward sum: 0.4880790660017004
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 22.]), 'previousTarget': array([ 3., 22.]), 'currentState': array([ 3.86402696, 22.48689651,  3.65991856]), 'targetState': array([ 3, 22], dtype=int32), 'currentDistance': 0.991771550686083}
episode index:3602
target Thresh 31.999999999999993
target distance 20.0
model initialize at round 3602
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  9.]), 'previousTarget': array([18.,  9.]), 'currentState': array([ 6.        , 25.        ,  4.77662086]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 78
reward sum = 0.22123474337471508
running average episode reward sum: 0.48800500429683585
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  5.]), 'previousTarget': array([21.,  5.]), 'currentState': array([20.42812163,  5.97200039,  5.36684507]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 1.1277542439777235}
episode index:3603
target Thresh 31.999999999999993
target distance 20.0
model initialize at round 3603
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5.11152109, 18.01132603]), 'previousTarget': array([ 5.0992562 , 18.00992562]), 'currentState': array([25.00916885, 20.03212137,  1.54524672]), 'targetState': array([ 5, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.31128328022180374
running average episode reward sum: 0.48795596941224234
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 18.]), 'previousTarget': array([ 5., 18.]), 'currentState': array([ 5.78529523, 18.40043575,  3.18257486]), 'targetState': array([ 5, 18], dtype=int32), 'currentDistance': 0.8814972420617923}
episode index:3604
target Thresh 31.999999999999993
target distance 6.0
model initialize at round 3604
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 24.]), 'previousTarget': array([17., 24.]), 'currentState': array([11.2491693 , 21.36228724,  0.81407461]), 'targetState': array([17, 24], dtype=int32), 'currentDistance': 6.326893576738539}
done in step count: 12
reward sum = 0.7775715992785281
running average episode reward sum: 0.48803630661886266
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 24.]), 'previousTarget': array([17., 24.]), 'currentState': array([16.12503102, 23.50567811,  0.54863492]), 'targetState': array([17, 24], dtype=int32), 'currentDistance': 1.004950171220083}
episode index:3605
target Thresh 31.999999999999993
target distance 25.0
model initialize at round 3605
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7.97505471, 17.84451849]), 'previousTarget': array([ 8.18225176, 17.77438937]), 'currentState': array([26.78832158, 11.05769378,  2.91366298]), 'targetState': array([ 2, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 80
reward sum = 0.22192490660934544
running average episode reward sum: 0.4879625097802577
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 20.]), 'previousTarget': array([ 2., 20.]), 'currentState': array([ 2.82656881, 19.62002048,  2.68271069]), 'targetState': array([ 2, 20], dtype=int32), 'currentDistance': 0.9097254700734169}
episode index:3606
target Thresh 31.999999999999993
target distance 13.0
model initialize at round 3606
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  9.]), 'previousTarget': array([19.,  9.]), 'currentState': array([ 6.        , 11.        ,  3.63863102]), 'targetState': array([19,  9], dtype=int32), 'currentDistance': 13.152946437965907}
done in step count: 33
reward sum = 0.4906272262819322
running average episode reward sum: 0.4879632485428032
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  9.]), 'previousTarget': array([19.,  9.]), 'currentState': array([18.06452233,  9.16618936,  6.19397125]), 'targetState': array([19,  9], dtype=int32), 'currentDistance': 0.9501249220618813}
episode index:3607
target Thresh 31.999999999999993
target distance 9.0
model initialize at round 3607
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 18.]), 'previousTarget': array([12., 18.]), 'currentState': array([21.03637893, 10.00238892,  0.31807345]), 'targetState': array([12, 18], dtype=int32), 'currentDistance': 12.067225333553885}
done in step count: 33
reward sum = 0.5377943049394323
running average episode reward sum: 0.48797705981120576
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 18.]), 'previousTarget': array([12., 18.]), 'currentState': array([12.89876842, 17.06171692,  2.41477588]), 'targetState': array([12, 18], dtype=int32), 'currentDistance': 1.2992920424182806}
episode index:3608
target Thresh 31.999999999999993
target distance 11.0
model initialize at round 3608
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([23.        ,  7.        ,  3.22958055]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 13.038404810405298}
done in step count: 32
reward sum = 0.5352218355128816
running average episode reward sum: 0.4879901506329574
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([16.29424322, 17.0465669 ,  2.05916063]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 0.9978044674699017}
episode index:3609
target Thresh 31.999999999999993
target distance 4.0
model initialize at round 3609
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([19.        , 13.        ,  1.78351939]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 5.0}
done in step count: 12
reward sum = 0.7818841889458548
running average episode reward sum: 0.4880715617239028
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.97361148, 10.66805259,  3.92119349]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.1807682156080381}
episode index:3610
target Thresh 31.999999999999993
target distance 14.0
model initialize at round 3610
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 26.]), 'previousTarget': array([ 5., 26.]), 'currentState': array([18.00770642, 11.9651801 ,  4.72660789]), 'targetState': array([ 5, 26], dtype=int32), 'currentDistance': 19.135741320558328}
done in step count: 54
reward sum = 0.33472355842298324
running average episode reward sum: 0.4880290948163147
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 26.]), 'previousTarget': array([ 5., 26.]), 'currentState': array([ 5.80238044, 25.33575045,  2.36958456]), 'targetState': array([ 5, 26], dtype=int32), 'currentDistance': 1.0416534148659702}
episode index:3611
target Thresh 31.999999999999996
target distance 10.0
model initialize at round 3611
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 27.]), 'previousTarget': array([16., 27.]), 'currentState': array([17.79962857, 17.41749349,  1.92912143]), 'targetState': array([16, 27], dtype=int32), 'currentDistance': 9.750030463077836}
done in step count: 20
reward sum = 0.6664673371092364
running average episode reward sum: 0.4880784963230403
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 27.]), 'previousTarget': array([16., 27.]), 'currentState': array([16.49573213, 26.04643156,  1.73784306]), 'targetState': array([16, 27], dtype=int32), 'currentDistance': 1.0747293199361216}
episode index:3612
target Thresh 31.999999999999996
target distance 22.0
model initialize at round 3612
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.69267704, 25.80792995]), 'previousTarget': array([23.70226409, 25.81660336]), 'currentState': array([20.91588175,  6.00163277,  2.8696847 ]), 'targetState': array([24, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.2745227603270819
running average episode reward sum: 0.48801938872935197
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 28.]), 'previousTarget': array([24., 28.]), 'currentState': array([24.02412615, 27.01016874,  1.52479287]), 'targetState': array([24, 28], dtype=int32), 'currentDistance': 0.9901252401440959}
episode index:3613
target Thresh 31.999999999999996
target distance 15.0
model initialize at round 3613
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 15.]), 'previousTarget': array([23., 15.]), 'currentState': array([7.95659654, 4.03446888, 2.21792865]), 'targetState': array([23, 15], dtype=int32), 'currentDistance': 18.615769131624763}
done in step count: 48
reward sum = 0.3697199316951696
running average episode reward sum: 0.4879866550666419
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 15.]), 'previousTarget': array([23., 15.]), 'currentState': array([22.04869658, 14.20956342,  0.6050865 ]), 'targetState': array([23, 15], dtype=int32), 'currentDistance': 1.2368379764320745}
episode index:3614
target Thresh 31.999999999999996
target distance 1.0
model initialize at round 3614
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 14.]), 'previousTarget': array([ 9., 14.]), 'currentState': array([ 8.96485708, 14.9429676 ,  4.27621177]), 'targetState': array([ 9, 14], dtype=int32), 'currentDistance': 0.9436222365775451}
done in step count: 0
reward sum = 0.9977068554541084
running average episode reward sum: 0.48812765650519996
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 14.]), 'previousTarget': array([ 9., 14.]), 'currentState': array([ 8.96485708, 14.9429676 ,  4.27621177]), 'targetState': array([ 9, 14], dtype=int32), 'currentDistance': 0.9436222365775451}
episode index:3615
target Thresh 31.999999999999996
target distance 15.0
model initialize at round 3615
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 13.]), 'previousTarget': array([24., 13.]), 'currentState': array([12.11228638, 27.75644551,  5.25892809]), 'targetState': array([24, 13], dtype=int32), 'currentDistance': 18.949153525257184}
done in step count: 48
reward sum = 0.3969314867234673
running average episode reward sum: 0.48810243632550365
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 13.]), 'previousTarget': array([24., 13.]), 'currentState': array([23.09093741, 13.96261696,  5.46941189]), 'targetState': array([24, 13], dtype=int32), 'currentDistance': 1.3240189652457368}
episode index:3616
target Thresh 31.999999999999996
target distance 16.0
model initialize at round 3616
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8.32117742, 4.40925592]), 'previousTarget': array([8.32117742, 4.40925592]), 'currentState': array([22.        , 19.        ,  5.72414798]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.27787600153479297
running average episode reward sum: 0.48804431455752173
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'previousTarget': array([7., 3.]), 'currentState': array([7.65016805, 3.95561262, 3.88340249]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 1.1558174435417723}
episode index:3617
target Thresh 31.999999999999996
target distance 7.0
model initialize at round 3617
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  7.]), 'previousTarget': array([19.,  7.]), 'currentState': array([26.        ,  5.        ,  0.23543554]), 'targetState': array([19,  7], dtype=int32), 'currentDistance': 7.280109889280519}
done in step count: 19
reward sum = 0.6778339603229864
running average episode reward sum: 0.488096771618264
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  7.]), 'previousTarget': array([19.,  7.]), 'currentState': array([19.95920295,  7.16767257,  2.96528319]), 'targetState': array([19,  7], dtype=int32), 'currentDistance': 0.9737475992931977}
episode index:3618
target Thresh 31.999999999999996
target distance 8.0
model initialize at round 3618
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 29.]), 'previousTarget': array([ 2., 29.]), 'currentState': array([ 7.57737242, 21.25239244,  2.50771217]), 'targetState': array([ 2, 29], dtype=int32), 'currentDistance': 9.546334687745828}
done in step count: 24
reward sum = 0.6475906784185004
running average episode reward sum: 0.48814084288292275
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 29.]), 'previousTarget': array([ 2., 29.]), 'currentState': array([ 2.41018448, 28.06599005,  1.98435085]), 'targetState': array([ 2, 29], dtype=int32), 'currentDistance': 1.0201107279439936}
episode index:3619
target Thresh 31.999999999999996
target distance 14.0
model initialize at round 3619
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 27.]), 'previousTarget': array([14., 27.]), 'currentState': array([25.        , 13.        ,  3.25611854]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 17.804493814764857}
done in step count: 43
reward sum = 0.39801910906746263
running average episode reward sum: 0.4881159473763439
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 27.]), 'previousTarget': array([14., 27.]), 'currentState': array([14.5174269 , 26.11428125,  1.94479826]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 1.0257817948607721}
episode index:3620
target Thresh 31.999999999999996
target distance 12.0
model initialize at round 3620
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([9.        , 6.        , 0.69117436]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 13.892443989449804}
done in step count: 36
reward sum = 0.5248752040671082
running average episode reward sum: 0.4881260990628092
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([15.36531901, 17.04714645,  0.80670869]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 1.1448798364132784}
episode index:3621
target Thresh 31.999999999999996
target distance 20.0
model initialize at round 3621
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8.99884905, 2.02299627]), 'previousTarget': array([8.99875234, 2.02495322]), 'currentState': array([ 7.99911518, 21.99799394,  4.41361107]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.34936510781938424
running average episode reward sum: 0.4880877884633494
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 2.]), 'previousTarget': array([9., 2.]), 'currentState': array([9.16690892, 2.99438397, 4.56809924]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 1.0082946374443067}
episode index:3622
target Thresh 31.999999999999996
target distance 3.0
model initialize at round 3622
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([20.        ,  2.        ,  1.03299606]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 12
reward sum = 0.8236751396968738
running average episode reward sum: 0.48818041538888995
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([17.94833245,  3.51632572,  2.59383054]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 1.0645540137377836}
episode index:3623
target Thresh 31.999999999999996
target distance 19.0
model initialize at round 3623
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.23084332, 20.94790359]), 'previousTarget': array([20.10111675, 20.86398076]), 'currentState': array([4.16211233, 9.04009589, 0.29287279]), 'targetState': array([23, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.2815050990873709
running average episode reward sum: 0.488123385776224
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 23.]), 'previousTarget': array([23., 23.]), 'currentState': array([22.11623566, 22.70281741,  0.66502168]), 'targetState': array([23, 23], dtype=int32), 'currentDistance': 0.9323930996664234}
episode index:3624
target Thresh 31.999999999999996
target distance 21.0
model initialize at round 3624
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.97143354, 17.49125068]), 'previousTarget': array([23.00530293, 17.52709229]), 'currentState': array([5.99424535, 6.91887719, 4.89407015]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 83
reward sum = 0.18989490397127798
running average episode reward sum: 0.4880411158502088
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([26.1587262 , 19.83063322,  0.38924221]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 0.8581530833772465}
episode index:3625
target Thresh 31.999999999999996
target distance 8.0
model initialize at round 3625
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  7.]), 'previousTarget': array([11.,  7.]), 'currentState': array([16.69671713, 14.94296282,  3.57998729]), 'targetState': array([11,  7], dtype=int32), 'currentDistance': 9.7746224646576}
done in step count: 23
reward sum = 0.6331709149380021
running average episode reward sum: 0.48808114061553914
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  7.]), 'previousTarget': array([11.,  7.]), 'currentState': array([11.11031425,  7.94696187,  3.83542993]), 'targetState': array([11,  7], dtype=int32), 'currentDistance': 0.9533656216851093}
episode index:3626
target Thresh 31.999999999999996
target distance 5.0
model initialize at round 3626
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 14.]), 'previousTarget': array([ 5., 14.]), 'currentState': array([10.      , 10.      ,  4.614995]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 6.4031242374328485}
done in step count: 15
reward sum = 0.7251400226070458
running average episode reward sum: 0.48814650010878197
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 14.]), 'previousTarget': array([ 5., 14.]), 'currentState': array([ 5.71963205, 13.11696512,  2.35572677]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 1.139131640597404}
episode index:3627
target Thresh 31.999999999999996
target distance 4.0
model initialize at round 3627
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 16.]), 'previousTarget': array([19., 16.]), 'currentState': array([21.72466953, 19.88533111,  3.64772157]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 4.745484369683467}
done in step count: 8
reward sum = 0.8486901315018164
running average episode reward sum: 0.48824587817697185
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 16.]), 'previousTarget': array([19., 16.]), 'currentState': array([19.68527837, 16.95986323,  4.14643482]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 1.1793828302033977}
episode index:3628
target Thresh 31.999999999999996
target distance 14.0
model initialize at round 3628
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 12.]), 'previousTarget': array([ 3., 12.]), 'currentState': array([ 2.99069936, 26.02804688,  2.14042717]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 14.028049964335523}
done in step count: 39
reward sum = 0.44298849254637185
running average episode reward sum: 0.4882334071420778
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 12.]), 'previousTarget': array([ 3., 12.]), 'currentState': array([ 2.78847411, 12.83329962,  4.39651529]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 0.8597275459621263}
episode index:3629
target Thresh 31.999999999999996
target distance 16.0
model initialize at round 3629
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 12.]), 'previousTarget': array([12., 12.]), 'currentState': array([17.08308295, 27.6634025 ,  4.83427519]), 'targetState': array([12, 12], dtype=int32), 'currentDistance': 16.467541106398258}
done in step count: 37
reward sum = 0.45262615433791664
running average episode reward sum: 0.4882235979815257
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 12.]), 'previousTarget': array([12., 12.]), 'currentState': array([12.65072648, 12.81495096,  4.29644797]), 'targetState': array([12, 12], dtype=int32), 'currentDistance': 1.0428758406002607}
episode index:3630
target Thresh 31.999999999999996
target distance 8.0
model initialize at round 3630
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 15.]), 'previousTarget': array([19., 15.]), 'currentState': array([12.06795948, 22.79601119,  5.19488817]), 'targetState': array([19, 15], dtype=int32), 'currentDistance': 10.43220859826564}
done in step count: 25
reward sum = 0.6225295841159149
running average episode reward sum: 0.4882605866860518
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 15.]), 'previousTarget': array([19., 15.]), 'currentState': array([18.82110033, 15.88985407,  5.12883818]), 'targetState': array([19, 15], dtype=int32), 'currentDistance': 0.907659276973103}
episode index:3631
target Thresh 31.999999999999996
target distance 9.0
model initialize at round 3631
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  6.]), 'previousTarget': array([25.,  6.]), 'currentState': array([22.95585257, 14.78818276,  4.64446896]), 'targetState': array([25,  6], dtype=int32), 'currentDistance': 9.022787533307849}
done in step count: 20
reward sum = 0.6765874130542209
running average episode reward sum: 0.48831243878582276
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  6.]), 'previousTarget': array([25.,  6.]), 'currentState': array([25.03022485,  6.93025302,  4.48724116]), 'targetState': array([25,  6], dtype=int32), 'currentDistance': 0.9307439063060803}
episode index:3632
target Thresh 31.999999999999996
target distance 7.0
model initialize at round 3632
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  3.]), 'previousTarget': array([17.,  3.]), 'currentState': array([23.61146323,  4.95150192,  3.29072787]), 'targetState': array([17,  3], dtype=int32), 'currentDistance': 6.893461093107933}
done in step count: 16
reward sum = 0.7509911093292003
running average episode reward sum: 0.4883847423009737
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  3.]), 'previousTarget': array([17.,  3.]), 'currentState': array([17.92212675,  3.09929452,  3.35955669]), 'targetState': array([17,  3], dtype=int32), 'currentDistance': 0.9274573546890693}
episode index:3633
target Thresh 31.999999999999996
target distance 11.0
model initialize at round 3633
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 28.]), 'previousTarget': array([10., 28.]), 'currentState': array([20.64430113, 22.11620439,  2.76900713]), 'targetState': array([10, 28], dtype=int32), 'currentDistance': 12.162244746794949}
done in step count: 25
reward sum = 0.5935269334854245
running average episode reward sum: 0.4884136752099403
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 28.]), 'previousTarget': array([10., 28.]), 'currentState': array([10.80988214, 27.08485068,  2.55388799]), 'targetState': array([10, 28], dtype=int32), 'currentDistance': 1.2220504766083975}
episode index:3634
target Thresh 31.999999999999996
target distance 18.0
model initialize at round 3634
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.29512096, 25.78347247]), 'previousTarget': array([22.21358457, 25.70981108]), 'currentState': array([ 6.01483846, 14.16657092,  1.35522717]), 'targetState': array([24, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.3023331723714875
running average episode reward sum: 0.48836248387490905
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 27.]), 'previousTarget': array([24., 27.]), 'currentState': array([23.14820871, 26.64400639,  0.26316144]), 'targetState': array([24, 27], dtype=int32), 'currentDistance': 0.9231900423236251}
episode index:3635
target Thresh 31.999999999999996
target distance 4.0
model initialize at round 3635
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 16.]), 'previousTarget': array([14., 16.]), 'currentState': array([10.41279006, 18.83891526,  5.75268027]), 'targetState': array([14, 16], dtype=int32), 'currentDistance': 4.574660095044443}
done in step count: 7
reward sum = 0.858783987307856
running average episode reward sum: 0.48846435997596316
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 16.]), 'previousTarget': array([14., 16.]), 'currentState': array([13.03871702, 16.81174765,  5.5190174 ]), 'targetState': array([14, 16], dtype=int32), 'currentDistance': 1.2581729648428843}
episode index:3636
target Thresh 31.999999999999996
target distance 9.0
model initialize at round 3636
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([ 5.95440825, 12.98720471,  3.16270328]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 9.22226601904015}
done in step count: 20
reward sum = 0.6460917083962431
running average episode reward sum: 0.48850769991228987
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([ 3.89910347, 21.13067985,  1.5895603 ]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 0.8751557719376635}
episode index:3637
target Thresh 31.999999999999996
target distance 19.0
model initialize at round 3637
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5.4625245 , 26.10830596]), 'previousTarget': array([ 5.70177604, 25.88271492]), 'currentState': array([20.81329891, 13.28825429,  2.23967665]), 'targetState': array([ 2, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.2523166467477896
running average episode reward sum: 0.48844277658816554
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 29.]), 'previousTarget': array([ 2., 29.]), 'currentState': array([ 2.30587552, 28.08387924,  2.17231987]), 'targetState': array([ 2, 29], dtype=int32), 'currentDistance': 0.9658349109197474}
episode index:3638
target Thresh 31.999999999999996
target distance 19.0
model initialize at round 3638
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.70675449, 12.31672402]), 'previousTarget': array([20.69836445, 12.31492866]), 'currentState': array([3.02318016, 2.9737481 , 5.67984554]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.3134445797564454
running average episode reward sum: 0.48839468694902516
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([21.01625622, 12.50837211,  0.38297398]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 1.0997498841627047}
episode index:3639
target Thresh 31.999999999999996
target distance 6.0
model initialize at round 3639
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([8.99487308, 9.00499807, 2.60667282]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 7.209612529010464}
done in step count: 16
reward sum = 0.7255446539205702
running average episode reward sum: 0.4884598380388525
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.79611892, 5.50356522, 3.43507715]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.9420102272807177}
episode index:3640
target Thresh 31.999999999999996
target distance 21.0
model initialize at round 3640
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7.28336929, 14.71986011]), 'previousTarget': array([ 7.28336929, 14.71986011]), 'currentState': array([25.       , 24.       ,  4.9592104]), 'targetState': array([ 4, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.2506315386281345
running average episode reward sum: 0.48839451853887705
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 13.]), 'previousTarget': array([ 4., 13.]), 'currentState': array([ 4.90557203, 13.66663781,  3.42130463]), 'targetState': array([ 4, 13], dtype=int32), 'currentDistance': 1.1244850711814263}
episode index:3641
target Thresh 31.999999999999996
target distance 8.0
model initialize at round 3641
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 15.]), 'previousTarget': array([21., 15.]), 'currentState': array([23.0952317 ,  7.25105275,  1.34875187]), 'targetState': array([21, 15], dtype=int32), 'currentDistance': 8.027214916124432}
done in step count: 16
reward sum = 0.7214167948001323
running average episode reward sum: 0.4884585004928203
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 15.]), 'previousTarget': array([21., 15.]), 'currentState': array([21.21659194, 14.0048824 ,  1.98615295]), 'targetState': array([21, 15], dtype=int32), 'currentDistance': 1.0184159758386535}
episode index:3642
target Thresh 31.999999999999996
target distance 22.0
model initialize at round 3642
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.42627774, 22.67376241]), 'previousTarget': array([11.42229124, 22.6773982 ]), 'currentState': array([15.03120566,  3.00133259,  0.29460176]), 'targetState': array([11, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.27647796331394114
running average episode reward sum: 0.4884003120390243
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 25.]), 'previousTarget': array([11., 25.]), 'currentState': array([11.4142691 , 24.15780092,  1.70059126]), 'targetState': array([11, 25], dtype=int32), 'currentDistance': 0.9385724154869008}
episode index:3643
target Thresh 31.999999999999996
target distance 6.0
model initialize at round 3643
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([ 8.95941629, 12.99384999,  3.03948736]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 8.518361905183015}
done in step count: 26
reward sum = 0.5995848220849334
running average episode reward sum: 0.48843082370478885
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([15.22160568, 18.06424904,  0.84378057]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 0.9616334719947134}
episode index:3644
target Thresh 31.999999999999996
target distance 23.0
model initialize at round 3644
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.17676768, 25.13347761]), 'previousTarget': array([13.17676768, 25.13347761]), 'currentState': array([19.        ,  6.        ,  0.72734013]), 'targetState': array([12, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.2532800865271857
running average episode reward sum: 0.48836631047099527
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 29.]), 'previousTarget': array([12., 29.]), 'currentState': array([12.35260934, 28.05623581,  2.11500013]), 'targetState': array([12, 29], dtype=int32), 'currentDistance': 1.0074840883875362}
episode index:3645
target Thresh 31.999999999999996
target distance 21.0
model initialize at round 3645
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.17550915, 10.04365006]), 'previousTarget': array([19.12086431, 10.0913656 ]), 'currentState': array([ 7.09124221, 25.98010175,  5.95628279]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 72
reward sum = 0.19951441864944053
running average episode reward sum: 0.48828708614520766
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  5.]), 'previousTarget': array([23.,  5.]), 'currentState': array([23.09160817,  5.89211811,  5.01205072]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 0.8968092219107484}
episode index:3646
target Thresh 31.999999999999996
target distance 7.0
model initialize at round 3646
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 16.]), 'previousTarget': array([25., 16.]), 'currentState': array([26.        ,  9.        ,  0.08976143]), 'targetState': array([25, 16], dtype=int32), 'currentDistance': 7.0710678118654755}
done in step count: 18
reward sum = 0.7040323500878037
running average episode reward sum: 0.4883462430588196
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 16.]), 'previousTarget': array([25., 16.]), 'currentState': array([24.98747191, 15.05752511,  1.68590017]), 'targetState': array([25, 16], dtype=int32), 'currentDistance': 0.9425581543700857}
episode index:3647
target Thresh 31.999999999999996
target distance 12.0
model initialize at round 3647
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([25.        , 22.        ,  2.88270438]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 14.999999999999998}
done in step count: 37
reward sum = 0.46923759420178324
running average episode reward sum: 0.48834100494235655
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.91737981, 10.33504748,  3.91539109]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.9766486223726232}
episode index:3648
target Thresh 31.999999999999996
target distance 22.0
model initialize at round 3648
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6.03418878, 9.09141219]), 'previousTarget': array([6.0206292 , 9.09184678]), 'currentState': array([26.01402516,  9.98926424,  5.40321517]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 20.0}
done in step count: 74
reward sum = 0.22715690296353833
running average episode reward sum: 0.48826942804403406
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.87190749, 9.52060178, 3.10651883]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.0155042485802879}
episode index:3649
target Thresh 31.999999999999996
target distance 4.0
model initialize at round 3649
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 10.]), 'previousTarget': array([18., 10.]), 'currentState': array([21.        , 14.        ,  3.06951118]), 'targetState': array([18, 10], dtype=int32), 'currentDistance': 5.0}
done in step count: 13
reward sum = 0.802366118513668
running average episode reward sum: 0.4883554819318339
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 10.]), 'previousTarget': array([18., 10.]), 'currentState': array([18.46644775, 10.75804164,  4.24213523]), 'targetState': array([18, 10], dtype=int32), 'currentDistance': 0.890056529908407}
episode index:3650
target Thresh 31.999999999999996
target distance 6.0
model initialize at round 3650
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 19.]), 'previousTarget': array([16., 19.]), 'currentState': array([19.01715777, 25.05523154,  1.42455891]), 'targetState': array([16, 19], dtype=int32), 'currentDistance': 6.765284180782079}
done in step count: 18
reward sum = 0.6925844035002962
running average episode reward sum: 0.48841141973560515
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 19.]), 'previousTarget': array([16., 19.]), 'currentState': array([16.63564509, 19.84199989,  4.34502653]), 'targetState': array([16, 19], dtype=int32), 'currentDistance': 1.0549921735411907}
episode index:3651
target Thresh 31.999999999999996
target distance 12.0
model initialize at round 3651
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([10.0958051 , 12.49040339,  1.44104724]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 11.561643605069776}
done in step count: 24
reward sum = 0.6009478934541126
running average episode reward sum: 0.48844223476126736
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([ 8.92599527, 23.13881535,  1.48516539]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 0.8643585493010814}
episode index:3652
target Thresh 31.999999999999996
target distance 11.0
model initialize at round 3652
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  2.]), 'previousTarget': array([10.,  2.]), 'currentState': array([19.        , 13.        ,  1.73693204]), 'targetState': array([10,  2], dtype=int32), 'currentDistance': 14.212670403551895}
done in step count: 35
reward sum = 0.464791713921061
running average episode reward sum: 0.48843576048783727
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  2.]), 'previousTarget': array([10.,  2.]), 'currentState': array([10.82871978,  2.51745682,  3.76598584]), 'targetState': array([10,  2], dtype=int32), 'currentDistance': 0.9770046260351365}
episode index:3653
target Thresh 31.999999999999996
target distance 14.0
model initialize at round 3653
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([ 4.34173494, 22.85702347,  5.72514677]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 16.339603834240236}
done in step count: 38
reward sum = 0.44649516206196893
running average episode reward sum: 0.4884242824915521
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.23079063,  9.95797637,  4.86274411]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.9853847177890902}
episode index:3654
target Thresh 31.999999999999996
target distance 11.0
model initialize at round 3654
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([18.40869938, 19.9878374 ,  6.07412047]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 13.947867830941536}
done in step count: 32
reward sum = 0.5197611896365092
running average episode reward sum: 0.48843285620075727
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([26.48409675,  9.86881819,  5.01187956]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 1.0104460502673493}
episode index:3655
target Thresh 31.999999999999996
target distance 8.0
model initialize at round 3655
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'previousTarget': array([13., 22.]), 'currentState': array([ 5.39270758, 28.96837889,  6.05057465]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 10.31645298897034}
done in step count: 21
reward sum = 0.6335167857068805
running average episode reward sum: 0.48847253998891543
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'previousTarget': array([13., 22.]), 'currentState': array([12.19347423, 22.2905349 ,  5.51380887]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 0.8572597852232097}
episode index:3656
target Thresh 31.999999999999996
target distance 19.0
model initialize at round 3656
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5.1790397 , 23.36996965]), 'previousTarget': array([ 5.30163555, 23.31492866]), 'currentState': array([22.81858131, 13.94412372,  3.28870961]), 'targetState': array([ 4, 24], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.3200794265837292
running average episode reward sum: 0.4884264931982659
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 24.]), 'previousTarget': array([ 4., 24.]), 'currentState': array([ 4.83673413, 23.31211694,  2.34094336]), 'targetState': array([ 4, 24], dtype=int32), 'currentDistance': 1.0831930149834728}
episode index:3657
target Thresh 31.999999999999996
target distance 8.0
model initialize at round 3657
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([17.69174433,  9.36818286,  2.09399205]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 7.817073097378043}
done in step count: 16
reward sum = 0.7161109399360573
running average episode reward sum: 0.48848873607599635
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([16.16072698, 16.23134283,  1.63094716]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 0.785281479270354}
episode index:3658
target Thresh 31.999999999999996
target distance 3.0
model initialize at round 3658
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.24230126,  8.09039847,  0.59310727]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 3.0066407209368164}
done in step count: 6
reward sum = 0.8905525499642131
running average episode reward sum: 0.48859861959987944
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.34999997, 10.03237914,  1.42250954]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.0289752716307976}
episode index:3659
target Thresh 31.999999999999996
target distance 14.0
model initialize at round 3659
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  5.]), 'previousTarget': array([25.,  5.]), 'currentState': array([10.98451764, 14.99039102,  3.94955063]), 'targetState': array([25,  5], dtype=int32), 'currentDistance': 17.211672164726245}
done in step count: 37
reward sum = 0.4008602589409956
running average episode reward sum: 0.48857464737019124
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  5.]), 'previousTarget': array([25.,  5.]), 'currentState': array([24.06256834,  5.62941946,  5.36699926]), 'targetState': array([25,  5], dtype=int32), 'currentDistance': 1.1291354973408303}
episode index:3660
target Thresh 31.999999999999996
target distance 13.0
model initialize at round 3660
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 16.]), 'previousTarget': array([ 4., 16.]), 'currentState': array([16.01882968,  2.99014322,  6.00224847]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 17.71182204523567}
done in step count: 46
reward sum = 0.37045738998236494
running average episode reward sum: 0.48854238371070263
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 16.]), 'previousTarget': array([ 4., 16.]), 'currentState': array([ 4.6876113 , 15.24917259,  2.00015836]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 1.0181115340862754}
episode index:3661
target Thresh 31.999999999999996
target distance 6.0
model initialize at round 3661
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 15.]), 'previousTarget': array([11., 15.]), 'currentState': array([ 5.45300285, 18.95330091,  5.99314398]), 'targetState': array([11, 15], dtype=int32), 'currentDistance': 6.8115905285967075}
done in step count: 13
reward sum = 0.7645510363215814
running average episode reward sum: 0.4886177547245232
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 15.]), 'previousTarget': array([11., 15.]), 'currentState': array([10.06040445, 15.73704273,  5.42816054]), 'targetState': array([11, 15], dtype=int32), 'currentDistance': 1.1941824767947116}
episode index:3662
target Thresh 31.999999999999996
target distance 3.0
model initialize at round 3662
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 26.]), 'previousTarget': array([15., 26.]), 'currentState': array([14.18234298, 23.29955318,  1.11309904]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 2.8215201627928295}
done in step count: 5
reward sum = 0.9148949217288691
running average episode reward sum: 0.48873412850748915
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 26.]), 'previousTarget': array([15., 26.]), 'currentState': array([14.81143618, 25.09415241,  1.10685628]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 0.9252654566519102}
episode index:3663
target Thresh 31.999999999999996
target distance 14.0
model initialize at round 3663
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 19.]), 'previousTarget': array([ 9., 19.]), 'currentState': array([3.02017306, 4.97454587, 5.63505602]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 15.247022454031336}
done in step count: 38
reward sum = 0.43968665784014693
running average episode reward sum: 0.4887207421890756
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 19.]), 'previousTarget': array([ 9., 19.]), 'currentState': array([ 8.64395146, 18.15872685,  0.98403768]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 0.9135157778312732}
episode index:3664
target Thresh 31.999999999999996
target distance 12.0
model initialize at round 3664
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([15.74551498, 13.81718936,  3.80014584]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 13.580544648086917}
done in step count: 31
reward sum = 0.5488874334272424
running average episode reward sum: 0.48873715874875856
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.77959157, 7.77834366, 3.45987671]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.101626923416702}
episode index:3665
target Thresh 31.999999999999996
target distance 11.0
model initialize at round 3665
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([14.96594185, 23.99189857,  3.61514011]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 13.049896864738502}
done in step count: 33
reward sum = 0.5224891795571308
running average episode reward sum: 0.48874636551930095
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([21.04855742, 13.46632651,  5.42385433]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 1.0595769875018575}
episode index:3666
target Thresh 31.999999999999996
target distance 23.0
model initialize at round 3666
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4.12883146, 22.98219293]), 'previousTarget': array([ 4.13125551, 22.98112317]), 'currentState': array([4.98186298, 3.00039278, 2.86743951]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.2738582184489635
running average episode reward sum: 0.4886877649883301
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 26.]), 'previousTarget': array([ 4., 26.]), 'currentState': array([ 3.89133972, 25.04295974,  1.34158248]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 0.9631890357613335}
episode index:3667
target Thresh 31.999999999999996
target distance 13.0
model initialize at round 3667
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  7.]), 'previousTarget': array([20.,  7.]), 'currentState': array([13.01429545, 20.02999582,  0.88937432]), 'targetState': array([20,  7], dtype=int32), 'currentDistance': 14.784480347924463}
done in step count: 35
reward sum = 0.4578869068274809
running average episode reward sum: 0.4886793678078064
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  7.]), 'previousTarget': array([20.,  7.]), 'currentState': array([19.57838516,  7.77252498,  5.04034744]), 'targetState': array([20,  7], dtype=int32), 'currentDistance': 0.8800874486031575}
episode index:3668
target Thresh 31.999999999999996
target distance 6.0
model initialize at round 3668
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 29.]), 'previousTarget': array([ 8., 29.]), 'currentState': array([11.9994878 , 22.99735862,  4.2801078 ]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 7.213016444691844}
done in step count: 17
reward sum = 0.6917158075294594
running average episode reward sum: 0.48873470616695647
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 29.]), 'previousTarget': array([ 8., 29.]), 'currentState': array([ 8.42583034, 28.13954981,  1.76700741]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 0.9600552130630955}
episode index:3669
target Thresh 31.999999999999996
target distance 8.0
model initialize at round 3669
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 23.]), 'previousTarget': array([12., 23.]), 'currentState': array([17.        , 15.        ,  4.47672701]), 'targetState': array([12, 23], dtype=int32), 'currentDistance': 9.433981132056603}
done in step count: 23
reward sum = 0.6061126809646608
running average episode reward sum: 0.4887666892663564
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 23.]), 'previousTarget': array([12., 23.]), 'currentState': array([12.22884516, 22.14482104,  2.006885  ]), 'targetState': array([12, 23], dtype=int32), 'currentDistance': 0.8852689794026953}
episode index:3670
target Thresh 31.999999999999996
target distance 9.0
model initialize at round 3670
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([15.05257231, 18.02822807,  0.24025947]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 9.258615187096796}
done in step count: 22
reward sum = 0.6281769158944244
running average episode reward sum: 0.4888046653564213
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.42607891,  9.9557378 ,  4.24221338]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 1.0464119500117388}
episode index:3671
target Thresh 31.999999999999996
target distance 9.0
model initialize at round 3671
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 29.]), 'previousTarget': array([11., 29.]), 'currentState': array([12.04599681, 20.19833597,  1.42416269]), 'targetState': array([11, 29], dtype=int32), 'currentDistance': 8.863599666214107}
done in step count: 18
reward sum = 0.699175428745165
running average episode reward sum: 0.48886195586932674
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 29.]), 'previousTarget': array([11., 29.]), 'currentState': array([10.91318508, 28.05303986,  1.57451252]), 'targetState': array([11, 29], dtype=int32), 'currentDistance': 0.9509313041745198}
episode index:3672
target Thresh 31.999999999999996
target distance 1.0
model initialize at round 3672
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 28.]), 'previousTarget': array([16., 28.]), 'currentState': array([14.98215951, 28.99809497,  3.50047088]), 'targetState': array([16, 28], dtype=int32), 'currentDistance': 1.4255500088081094}
done in step count: 4
reward sum = 0.9197921809782336
running average episode reward sum: 0.4889792796441998
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 28.]), 'previousTarget': array([16., 28.]), 'currentState': array([15.0138237 , 28.56350878,  5.10517848]), 'targetState': array([16, 28], dtype=int32), 'currentDistance': 1.1358194620619517}
episode index:3673
target Thresh 31.999999999999996
target distance 10.0
model initialize at round 3673
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 20.]), 'previousTarget': array([ 3., 20.]), 'currentState': array([ 9.04409187, 10.29434742,  1.67460656]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 11.43375434731833}
done in step count: 23
reward sum = 0.5992248124576061
running average episode reward sum: 0.48900928659379517
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 20.]), 'previousTarget': array([ 3., 20.]), 'currentState': array([ 3.37621864, 19.12239248,  1.73557274]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 0.9548483769694812}
episode index:3674
target Thresh 31.999999999999996
target distance 6.0
model initialize at round 3674
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([14.0592017 , 15.97755179,  5.66825342]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 7.22552723816657}
done in step count: 15
reward sum = 0.7176192852373838
running average episode reward sum: 0.4890714933961472
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.84135485, 10.65786836,  3.89581153]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 1.0680209572951076}
episode index:3675
target Thresh 31.999999999999996
target distance 21.0
model initialize at round 3675
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.74182377, 24.63824754]), 'previousTarget': array([24.74224216, 24.64677133]), 'currentState': array([21.01636491,  4.98828682,  5.91446686]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.29140887542305216
running average episode reward sum: 0.48901772228135576
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 26.]), 'previousTarget': array([25., 26.]), 'currentState': array([24.38541465, 25.097728  ,  0.92739979]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 1.0917004743462855}
episode index:3676
target Thresh 31.999999999999996
target distance 5.0
model initialize at round 3676
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([17.26081323,  3.98799477,  0.13014936]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 6.2093540124570685}
done in step count: 13
reward sum = 0.7935566496956443
running average episode reward sum: 0.48910054494314914
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([21.02959359,  7.09794592,  0.72346041]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 1.3249113819023324}
episode index:3677
target Thresh 31.999999999999996
target distance 17.0
model initialize at round 3677
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.56139529, 15.71414506]), 'previousTarget': array([11.56139529, 15.71414506]), 'currentState': array([27.        ,  3.        ,  1.64896339]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.29924215086242834
running average episode reward sum: 0.4890489249338831
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'previousTarget': array([10., 17.]), 'currentState': array([10.34162513, 16.09675106,  2.19207991]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 0.9656947637640811}
episode index:3678
target Thresh 31.999999999999996
target distance 9.0
model initialize at round 3678
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 14.]), 'previousTarget': array([23., 14.]), 'currentState': array([13.96610863, 18.00668625,  3.19930935]), 'targetState': array([23, 14], dtype=int32), 'currentDistance': 9.882546629618407}
done in step count: 23
reward sum = 0.5861695401702925
running average episode reward sum: 0.48907532357895955
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 14.]), 'previousTarget': array([23., 14.]), 'currentState': array([22.06517482, 14.81515912,  5.58736238]), 'targetState': array([23, 14], dtype=int32), 'currentDistance': 1.2403154852630562}
episode index:3679
target Thresh 31.999999999999996
target distance 16.0
model initialize at round 3679
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 24.]), 'previousTarget': array([26., 24.]), 'currentState': array([ 9.96835646, 18.05633907,  1.83004928]), 'targetState': array([26, 24], dtype=int32), 'currentDistance': 17.097973556756358}
done in step count: 41
reward sum = 0.4049746591609091
running average episode reward sum: 0.48905247013754155
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 24.]), 'previousTarget': array([26., 24.]), 'currentState': array([2.52667850e+01, 2.40374385e+01, 2.46123038e-03]), 'targetState': array([26, 24], dtype=int32), 'currentDistance': 0.7341701503114644}
episode index:3680
target Thresh 31.999999999999996
target distance 14.0
model initialize at round 3680
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 5.]), 'previousTarget': array([9., 5.]), 'currentState': array([22.81622654,  5.12602397,  2.79298973]), 'targetState': array([9, 5], dtype=int32), 'currentDistance': 13.816801287744443}
done in step count: 31
reward sum = 0.5260863897011917
running average episode reward sum: 0.4890625309687189
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 5.]), 'previousTarget': array([9., 5.]), 'currentState': array([9.90453077, 4.54802569, 2.83940576]), 'targetState': array([9, 5], dtype=int32), 'currentDistance': 1.0111660073487412}
episode index:3681
target Thresh 31.999999999999996
target distance 14.0
model initialize at round 3681
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 15.]), 'previousTarget': array([26., 15.]), 'currentState': array([12.32562662, 12.11693537,  0.36301174]), 'targetState': array([26, 15], dtype=int32), 'currentDistance': 13.97499727839091}
done in step count: 30
reward sum = 0.5236282741052061
running average episode reward sum: 0.48907191873165656
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 15.]), 'previousTarget': array([26., 15.]), 'currentState': array([25.24596863, 15.4074808 ,  6.09472827]), 'targetState': array([26, 15], dtype=int32), 'currentDistance': 0.8570903735297354}
episode index:3682
target Thresh 31.999999999999996
target distance 16.0
model initialize at round 3682
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 13.]), 'previousTarget': array([ 4., 13.]), 'currentState': array([19.95846146,  1.96434977,  3.6603708 ]), 'targetState': array([ 4, 13], dtype=int32), 'currentDistance': 19.40252736656801}
done in step count: 59
reward sum = 0.34571470269587823
running average episode reward sum: 0.4890329946979787
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 13.]), 'previousTarget': array([ 4., 13.]), 'currentState': array([ 4.39488872, 12.08094231,  2.10849636]), 'targetState': array([ 4, 13], dtype=int32), 'currentDistance': 1.0003020246464605}
episode index:3683
target Thresh 31.999999999999996
target distance 4.0
model initialize at round 3683
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  6.]), 'previousTarget': array([25.,  6.]), 'currentState': array([22.95166333, 10.00780798,  3.23394275]), 'targetState': array([25,  6], dtype=int32), 'currentDistance': 4.500911898810966}
done in step count: 12
reward sum = 0.7885707318946611
running average episode reward sum: 0.4891143024442318
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  6.]), 'previousTarget': array([25.,  6.]), 'currentState': array([24.66786301,  6.93195063,  5.36609951]), 'targetState': array([25,  6], dtype=int32), 'currentDistance': 0.9893669523931181}
episode index:3684
target Thresh 31.999999999999996
target distance 5.0
model initialize at round 3684
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 17.]), 'previousTarget': array([19., 17.]), 'currentState': array([14.        , 21.        ,  1.34491119]), 'targetState': array([19, 17], dtype=int32), 'currentDistance': 6.403124237432849}
done in step count: 16
reward sum = 0.7234835226993428
running average episode reward sum: 0.48917790331811384
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 17.]), 'previousTarget': array([19., 17.]), 'currentState': array([18.57416792, 17.94153162,  5.32595074]), 'targetState': array([19, 17], dtype=int32), 'currentDistance': 1.0333512219843421}
episode index:3685
target Thresh 31.999999999999996
target distance 15.0
model initialize at round 3685
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([18.89798226, 28.80194379,  4.46116069]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 15.12349344895176}
done in step count: 34
reward sum = 0.49081397265859406
running average episode reward sum: 0.4891783471784883
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([22.13217098, 14.88505245,  4.68627568]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 0.8948670362157747}
episode index:3686
target Thresh 31.999999999999996
target distance 8.0
model initialize at round 3686
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 20.]), 'previousTarget': array([18., 20.]), 'currentState': array([25.554408  , 19.13561832,  2.93486467]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 7.603698830490686}
done in step count: 14
reward sum = 0.7404008771679879
running average episode reward sum: 0.4892464845611814
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 20.]), 'previousTarget': array([18., 20.]), 'currentState': array([18.97399382, 19.36688645,  2.94634972]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 1.1616784035629073}
episode index:3687
target Thresh 31.999999999999996
target distance 12.0
model initialize at round 3687
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 26.]), 'previousTarget': array([22., 26.]), 'currentState': array([11.17077296, 14.44980935,  1.20383038]), 'targetState': array([22, 26], dtype=int32), 'currentDistance': 15.832847576454313}
done in step count: 36
reward sum = 0.46717436743040885
running average episode reward sum: 0.4892404997138033
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 26.]), 'previousTarget': array([22., 26.]), 'currentState': array([21.23614526, 25.5141596 ,  0.38077727]), 'targetState': array([22, 26], dtype=int32), 'currentDistance': 0.9052706509842535}
episode index:3688
target Thresh 31.999999999999996
target distance 7.0
model initialize at round 3688
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([26.94336761,  6.26891525,  1.92772374]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 9.670463058124492}
done in step count: 23
reward sum = 0.6610246566427456
running average episode reward sum: 0.48928706630554325
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([20.47251608, 12.20096906,  2.2479867 ]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 0.9282897676886719}
episode index:3689
target Thresh 31.999999999999996
target distance 9.0
model initialize at round 3689
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 10.]), 'previousTarget': array([23., 10.]), 'currentState': array([14.        , 14.        ,  1.64244664]), 'targetState': array([23, 10], dtype=int32), 'currentDistance': 9.848857801796106}
done in step count: 24
reward sum = 0.6204475112500017
running average episode reward sum: 0.48932261114157155
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 10.]), 'previousTarget': array([23., 10.]), 'currentState': array([22.07905118, 10.95486678,  5.57736541]), 'targetState': array([23, 10], dtype=int32), 'currentDistance': 1.3266187425065736}
episode index:3690
target Thresh 31.999999999999996
target distance 20.0
model initialize at round 3690
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.00992562,  3.0992562 ]), 'previousTarget': array([20.00992562,  3.0992562 ]), 'currentState': array([22.        , 23.        ,  5.63107425]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.32800356288239096
running average episode reward sum: 0.48927890508677363
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  3.]), 'previousTarget': array([20.,  3.]), 'currentState': array([20.47802939,  3.74293349,  4.41338165]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 0.8834377527501146}
episode index:3691
target Thresh 31.999999999999996
target distance 10.0
model initialize at round 3691
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([11.11914538, 13.04878958,  0.15587917]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 10.453587684525885}
done in step count: 23
reward sum = 0.6061140091387751
running average episode reward sum: 0.48931055056457756
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.03016207,  3.87080954,  4.54137322]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.8713317431679167}
episode index:3692
target Thresh 31.999999999999996
target distance 4.0
model initialize at round 3692
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 23.]), 'previousTarget': array([11., 23.]), 'currentState': array([ 6.95357314, 23.92302329,  4.39595643]), 'targetState': array([11, 23], dtype=int32), 'currentDistance': 4.1503665290259875}
done in step count: 12
reward sum = 0.8057702502905184
running average episode reward sum: 0.4893962423327135
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 23.]), 'previousTarget': array([11., 23.]), 'currentState': array([10.18091987, 22.9996484 ,  6.2525546 ]), 'targetState': array([11, 23], dtype=int32), 'currentDistance': 0.8190802037477389}
episode index:3693
target Thresh 31.999999999999996
target distance 15.0
model initialize at round 3693
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 24.]), 'previousTarget': array([21., 24.]), 'currentState': array([12.        ,  9.        ,  3.08509415]), 'targetState': array([21, 24], dtype=int32), 'currentDistance': 17.4928556845359}
done in step count: 47
reward sum = 0.3704475016893791
running average episode reward sum: 0.48936404180736337
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 24.]), 'previousTarget': array([21., 24.]), 'currentState': array([20.17509735, 23.43345212,  0.51296837]), 'targetState': array([21, 24], dtype=int32), 'currentDistance': 1.0007201855046892}
episode index:3694
target Thresh 31.999999999999996
target distance 23.0
model initialize at round 3694
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.85155069, 24.57507782]), 'previousTarget': array([23.7042351, 24.5731765]), 'currentState': array([ 4.17705418, 28.16870932,  0.86453676]), 'targetState': array([27, 24], dtype=int32), 'currentDistance': 20.0}
done in step count: 92
reward sum = 0.19246902888209438
running average episode reward sum: 0.4892836913302523
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 24.]), 'previousTarget': array([27., 24.]), 'currentState': array([26.18640286, 24.40398169,  5.68156586]), 'targetState': array([27, 24], dtype=int32), 'currentDistance': 0.9083730086238998}
episode index:3695
target Thresh 31.999999999999996
target distance 6.0
model initialize at round 3695
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 10.]), 'previousTarget': array([18., 10.]), 'currentState': array([24.        , 13.        ,  1.94958931]), 'targetState': array([18, 10], dtype=int32), 'currentDistance': 6.708203932499369}
done in step count: 18
reward sum = 0.7262729400712337
running average episode reward sum: 0.48934781179798525
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 10.]), 'previousTarget': array([18., 10.]), 'currentState': array([18.93467042, 10.38271216,  3.61825368]), 'targetState': array([18, 10], dtype=int32), 'currentDistance': 1.0099888035005136}
episode index:3696
target Thresh 31.999999999999996
target distance 22.0
model initialize at round 3696
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.17458624, 22.21853056]), 'previousTarget': array([24.17458624, 22.21853056]), 'currentState': array([14.        ,  5.        ,  3.29888865]), 'targetState': array([27, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 75
reward sum = 0.18326519274036623
running average episode reward sum: 0.4892650196370284
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 27.]), 'previousTarget': array([27., 27.]), 'currentState': array([26.19918097, 26.62914371,  0.64836259]), 'targetState': array([27, 27], dtype=int32), 'currentDistance': 0.8825222437961938}
episode index:3697
target Thresh 31.999999999999996
target distance 7.0
model initialize at round 3697
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  3.]), 'previousTarget': array([22.,  3.]), 'currentState': array([19.98965063,  9.99891503,  3.49854517]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 7.281917072101071}
done in step count: 17
reward sum = 0.7026821166830033
running average episode reward sum: 0.4893227311289284
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  3.]), 'previousTarget': array([22.,  3.]), 'currentState': array([21.95419709,  3.72148463,  4.83506736]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 0.7229370522367952}
episode index:3698
target Thresh 31.999999999999996
target distance 5.0
model initialize at round 3698
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([7.73697412, 9.69732375, 3.78777069]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 5.031881525163533}
done in step count: 9
reward sum = 0.8187004077068502
running average episode reward sum: 0.4894117761888305
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.72544365, 8.27275465, 3.32638428]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.7750248899638787}
episode index:3699
target Thresh 31.999999999999996
target distance 14.0
model initialize at round 3699
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 29.]), 'previousTarget': array([ 5., 29.]), 'currentState': array([18.92852619, 18.1696236 ,  2.07514994]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 17.64372111265224}
done in step count: 42
reward sum = 0.4248452728217216
running average episode reward sum: 0.48939432578251507
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 29.]), 'previousTarget': array([ 5., 29.]), 'currentState': array([ 5.57605472, 28.10285349,  2.18417039]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 1.0661664552533083}
episode index:3700
target Thresh 31.999999999999996
target distance 11.0
model initialize at round 3700
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 26.]), 'previousTarget': array([19., 26.]), 'currentState': array([ 8.14481799, 15.21731207,  0.90208697]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 15.30037043515949}
done in step count: 34
reward sum = 0.4964726746266074
running average episode reward sum: 0.4893962383328648
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 26.]), 'previousTarget': array([19., 26.]), 'currentState': array([18.05065096, 25.68915452,  0.39550767]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 0.9989436978966542}
episode index:3701
target Thresh 31.999999999999996
target distance 15.0
model initialize at round 3701
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 19.]), 'previousTarget': array([27., 19.]), 'currentState': array([12.34152811, 15.27476543,  0.54382306]), 'targetState': array([27, 19], dtype=int32), 'currentDistance': 15.124422989957168}
done in step count: 43
reward sum = 0.47058285397668287
running average episode reward sum: 0.4893911563813909
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 19.]), 'previousTarget': array([27., 19.]), 'currentState': array([26.22676162, 19.19869202,  6.14040108]), 'targetState': array([27, 19], dtype=int32), 'currentDistance': 0.7983583813870458}
episode index:3702
target Thresh 31.999999999999996
target distance 20.0
model initialize at round 3702
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.28991511,  5.85014149]), 'previousTarget': array([23.28991511,  5.85014149]), 'currentState': array([13.        , 23.        ,  0.14918105]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 80
reward sum = 0.25197196886799217
running average episode reward sum: 0.48932704101884344
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'previousTarget': array([25.,  3.]), 'currentState': array([24.99858512,  3.98299997,  4.95354142]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 0.9830009847565322}
episode index:3703
target Thresh 31.999999999999996
target distance 13.0
model initialize at round 3703
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 16.]), 'previousTarget': array([10., 16.]), 'currentState': array([22.76504988,  4.18785655,  2.43709999]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 17.391757569629203}
done in step count: 42
reward sum = 0.44125945308176995
running average episode reward sum: 0.4893140638082773
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 16.]), 'previousTarget': array([10., 16.]), 'currentState': array([10.54429472, 15.07045898,  2.40136118]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 1.0771737364371161}
episode index:3704
target Thresh 31.999999999999996
target distance 20.0
model initialize at round 3704
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.87762951, 24.52007949]), 'previousTarget': array([12.8507125, 24.40285  ]), 'currentState': array([7.93612162, 5.14015432, 1.96835084]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.3500266564106752
running average episode reward sum: 0.48927646936633457
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([12.32528652, 24.05349081,  0.98482338]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 1.1623759873202097}
episode index:3705
target Thresh 31.999999999999996
target distance 20.0
model initialize at round 3705
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.01019556,  2.10174569]), 'previousTarget': array([12.00992562,  2.0992562 ]), 'currentState': array([14.00433471, 22.0020826 ,  0.19538414]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.31057420611609277
running average episode reward sum: 0.48922824965148026
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  2.]), 'previousTarget': array([12.,  2.]), 'currentState': array([12.42151838,  2.97439812,  4.42852036]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 1.0616635274849955}
episode index:3706
target Thresh 31.999999999999996
target distance 7.0
model initialize at round 3706
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([16.60876542, 16.06795006,  3.22212696]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 8.328259014356693}
done in step count: 18
reward sum = 0.6948783979532062
running average episode reward sum: 0.4892837258177338
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.83772082, 11.38333724,  3.53473108]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.9212619634694423}
episode index:3707
target Thresh 31.999999999999996
target distance 16.0
model initialize at round 3707
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 27.]), 'previousTarget': array([12., 27.]), 'currentState': array([10.99538235, 10.99680806,  3.49392009]), 'targetState': array([12, 27], dtype=int32), 'currentDistance': 16.034693912135676}
done in step count: 39
reward sum = 0.42243244530451646
running average episode reward sum: 0.48926569688555654
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 27.]), 'previousTarget': array([12., 27.]), 'currentState': array([11.66932917, 26.17596681,  1.42338283]), 'targetState': array([12, 27], dtype=int32), 'currentDistance': 0.8879042146103566}
episode index:3708
target Thresh 31.999999999999996
target distance 20.0
model initialize at round 3708
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.99028233,  2.00002267]), 'previousTarget': array([25.,  2.]), 'currentState': array([4.99033676, 2.04668109, 1.56003582]), 'targetState': array([25,  2], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 48
reward sum = 0.32786638055417583
running average episode reward sum: 0.4892221812974381
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  2.]), 'previousTarget': array([25.,  2.]), 'currentState': array([24.0276865 ,  2.46967528,  5.96752331]), 'targetState': array([25,  2], dtype=int32), 'currentDistance': 1.079809430366118}
episode index:3709
target Thresh 31.999999999999996
target distance 14.0
model initialize at round 3709
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  2.]), 'previousTarget': array([22.,  2.]), 'currentState': array([8.        , 9.        , 0.91503251]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 15.652475842498527}
done in step count: 36
reward sum = 0.458259396236212
running average episode reward sum: 0.48921383553327064
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  2.]), 'previousTarget': array([22.,  2.]), 'currentState': array([21.30536952,  2.8485014 ,  5.59703625]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 1.0965701668480194}
episode index:3710
target Thresh 31.999999999999996
target distance 12.0
model initialize at round 3710
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([22.87630376, 15.72986507,  4.19497551]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 16.692403229100805}
done in step count: 37
reward sum = 0.46343508781273146
running average episode reward sum: 0.48920688895614306
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([11.86069503,  4.86645757,  3.69320202]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 1.2212881153672293}
episode index:3711
target Thresh 31.999999999999996
target distance 10.0
model initialize at round 3711
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 8.]), 'previousTarget': array([9., 8.]), 'currentState': array([19.02970559,  5.94084498,  4.92526722]), 'targetState': array([9, 8], dtype=int32), 'currentDistance': 10.238901976917914}
done in step count: 24
reward sum = 0.5951851476111559
running average episode reward sum: 0.4892354391335824
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 8.]), 'previousTarget': array([9., 8.]), 'currentState': array([9.73599108, 7.59852857, 2.71431951]), 'targetState': array([9, 8], dtype=int32), 'currentDistance': 0.838368758500879}
episode index:3712
target Thresh 31.999999999999996
target distance 11.0
model initialize at round 3712
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 16.]), 'previousTarget': array([15., 16.]), 'currentState': array([ 4.03802271, 27.03974598,  0.55505347]), 'targetState': array([15, 16], dtype=int32), 'currentDistance': 15.55766490958201}
done in step count: 35
reward sum = 0.45131861172027304
running average episode reward sum: 0.48922522722207873
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 16.]), 'previousTarget': array([15., 16.]), 'currentState': array([14.17361219, 16.53823404,  5.40430095]), 'targetState': array([15, 16], dtype=int32), 'currentDistance': 0.9862112808474237}
episode index:3713
target Thresh 31.999999999999996
target distance 25.0
model initialize at round 3713
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.19461843, 13.19964509]), 'previousTarget': array([21.04848294, 13.09551454]), 'currentState': array([2.09211101, 7.27560845, 1.1371301 ]), 'targetState': array([27, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.20687399563484726
running average episode reward sum: 0.48914920373484466
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 15.]), 'previousTarget': array([27., 15.]), 'currentState': array([26.05301013, 15.08677092,  0.07295471]), 'targetState': array([27, 15], dtype=int32), 'currentDistance': 0.9509568895539334}
episode index:3714
target Thresh 31.999999999999996
target distance 12.0
model initialize at round 3714
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([17.7570346 , 15.62194781,  4.05060217]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 13.032043576364163}
done in step count: 36
reward sum = 0.5212484809232174
running average episode reward sum: 0.4891578441863086
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 6.09710119, 10.98152272,  3.68761243]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.9863140966095603}
episode index:3715
target Thresh 31.999999999999996
target distance 13.0
model initialize at round 3715
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  2.]), 'previousTarget': array([18.,  2.]), 'currentState': array([26.19672246, 14.70314664,  5.08633581]), 'targetState': array([18,  2], dtype=int32), 'currentDistance': 15.118075061827021}
done in step count: 36
reward sum = 0.4755971589006516
running average episode reward sum: 0.4891541949168561
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  2.]), 'previousTarget': array([18.,  2.]), 'currentState': array([18.86347953,  2.41106308,  3.93634392]), 'targetState': array([18,  2], dtype=int32), 'currentDistance': 0.9563314035707087}
episode index:3716
target Thresh 31.999999999999996
target distance 13.0
model initialize at round 3716
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  9.]), 'previousTarget': array([22.,  9.]), 'currentState': array([15.00786238, 22.02221847,  0.98413208]), 'targetState': array([22,  9], dtype=int32), 'currentDistance': 14.780668538943907}
done in step count: 45
reward sum = 0.43762777193934177
running average episode reward sum: 0.4891403325485543
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  9.]), 'previousTarget': array([22.,  9.]), 'currentState': array([22.03020251,  9.8162922 ,  4.81377638]), 'targetState': array([22,  9], dtype=int32), 'currentDistance': 0.8168507537735751}
episode index:3717
target Thresh 31.999999999999996
target distance 18.0
model initialize at round 3717
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 18.]), 'previousTarget': array([ 5., 18.]), 'currentState': array([23.        , 15.        ,  1.11125338]), 'targetState': array([ 5, 18], dtype=int32), 'currentDistance': 18.24828759089466}
done in step count: 46
reward sum = 0.3666785100686348
running average episode reward sum: 0.48910739499543976
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 18.]), 'previousTarget': array([ 5., 18.]), 'currentState': array([ 5.87231961, 18.00411322,  2.69350412]), 'targetState': array([ 5, 18], dtype=int32), 'currentDistance': 0.8723293031448337}
episode index:3718
target Thresh 31.999999999999996
target distance 13.0
model initialize at round 3718
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 13.]), 'previousTarget': array([ 9., 13.]), 'currentState': array([21.79107089, 24.11968383,  2.80583239]), 'targetState': array([ 9, 13], dtype=int32), 'currentDistance': 16.948712729832565}
done in step count: 43
reward sum = 0.4301276194817675
running average episode reward sum: 0.48909153595389265
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 13.]), 'previousTarget': array([ 9., 13.]), 'currentState': array([ 9.91647694, 13.45386228,  3.59006419]), 'targetState': array([ 9, 13], dtype=int32), 'currentDistance': 1.0227027679384126}
episode index:3719
target Thresh 31.999999999999996
target distance 11.0
model initialize at round 3719
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 26.]), 'previousTarget': array([16., 26.]), 'currentState': array([26.54070188, 21.09332193,  2.82747009]), 'targetState': array([16, 26], dtype=int32), 'currentDistance': 11.626774524884693}
done in step count: 24
reward sum = 0.6091689445761594
running average episode reward sum: 0.4891238148271782
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 26.]), 'previousTarget': array([16., 26.]), 'currentState': array([16.81395143, 25.14549603,  2.58950135]), 'targetState': array([16, 26], dtype=int32), 'currentDistance': 1.180124556419166}
episode index:3720
target Thresh 31.999999999999996
target distance 22.0
model initialize at round 3720
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8.90682828, 7.02683095]), 'previousTarget': array([8.90815322, 7.0206292 ]), 'currentState': array([ 7.98841493, 27.00573274,  2.93458223]), 'targetState': array([9, 5], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.272109050502613
running average episode reward sum: 0.4890654932027964
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 5.]), 'previousTarget': array([9., 5.]), 'currentState': array([8.77068467, 5.74026411, 4.6025758 ]), 'targetState': array([9, 5], dtype=int32), 'currentDistance': 0.774968694158433}
episode index:3721
target Thresh 32.0
target distance 6.0
model initialize at round 3721
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 21.]), 'previousTarget': array([18., 21.]), 'currentState': array([12.03258408, 23.12638258,  1.06797212]), 'targetState': array([18, 21], dtype=int32), 'currentDistance': 6.334947164118521}
done in step count: 13
reward sum = 0.7450395163183163
running average episode reward sum: 0.4891342664492004
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 21.]), 'previousTarget': array([18., 21.]), 'currentState': array([17.30987841, 21.38343897,  5.73562525]), 'targetState': array([18, 21], dtype=int32), 'currentDistance': 0.7894892370087206}
episode index:3722
target Thresh 32.0
target distance 19.0
model initialize at round 3722
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.82260933, 12.39408343]), 'previousTarget': array([20.69836445, 12.31492866]), 'currentState': array([3.03931957, 3.24232922, 1.28414971]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.3129508274687198
running average episode reward sum: 0.48908694347337967
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([21.16381472, 12.71887153,  0.22501931]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 0.8821785774266382}
episode index:3723
target Thresh 32.0
target distance 19.0
model initialize at round 3723
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3.53505764, 17.77612658]), 'previousTarget': array([ 3.5672925, 17.76114  ]), 'currentState': array([21.98515693, 10.05642215,  2.07918712]), 'targetState': array([ 3, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.3427359662284739
running average episode reward sum: 0.4890476440702527
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 18.]), 'previousTarget': array([ 3., 18.]), 'currentState': array([ 3.85896674, 17.28296197,  2.64311775]), 'targetState': array([ 3, 18], dtype=int32), 'currentDistance': 1.1189134867239667}
episode index:3724
target Thresh 32.0
target distance 1.0
model initialize at round 3724
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 10.]), 'previousTarget': array([25., 10.]), 'currentState': array([25.        , 11.        ,  0.73934951]), 'targetState': array([25, 10], dtype=int32), 'currentDistance': 0.9999999999999999}
done in step count: 5
reward sum = 0.9259804284448603
running average episode reward sum: 0.4891649414620311
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 10.]), 'previousTarget': array([25., 10.]), 'currentState': array([25.08931271, 10.90794534,  5.32828913]), 'targetState': array([25, 10], dtype=int32), 'currentDistance': 0.9123275139747752}
episode index:3725
target Thresh 32.0
target distance 15.0
model initialize at round 3725
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 10.]), 'previousTarget': array([20., 10.]), 'currentState': array([14.      , 25.      ,  0.793064]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 16.15549442140351}
done in step count: 39
reward sum = 0.4169476401974458
running average episode reward sum: 0.48914555947028
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 10.]), 'previousTarget': array([20., 10.]), 'currentState': array([20.2009476 , 10.92545343,  4.7505882 ]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 0.947018473586633}
episode index:3726
target Thresh 32.0
target distance 3.0
model initialize at round 3726
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([10.        , 10.        ,  1.64625585]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 10
reward sum = 0.8584542783111236
running average episode reward sum: 0.4892446495477796
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.99219904, 11.30202625,  2.99474203]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.0371493614605556}
episode index:3727
target Thresh 32.0
target distance 22.0
model initialize at round 3727
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.76343395, 23.17472169]), 'previousTarget': array([15.76343395, 23.17472169]), 'currentState': array([4.        , 7.        , 3.16059208]), 'targetState': array([20, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 79
reward sum = 0.1459536754237965
running average episode reward sum: 0.4891525650590124
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 29.]), 'previousTarget': array([20., 29.]), 'currentState': array([19.22573554, 28.53938766,  0.55562765]), 'targetState': array([20, 29], dtype=int32), 'currentDistance': 0.9009157411867458}
episode index:3728
target Thresh 32.0
target distance 21.0
model initialize at round 3728
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.23047895, 17.49442256]), 'previousTarget': array([22.23047895, 17.49442256]), 'currentState': array([ 3.        , 12.        ,  4.70023489]), 'targetState': array([24, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.26142329555981186
running average episode reward sum: 0.4890914952629547
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 18.]), 'previousTarget': array([24., 18.]), 'currentState': array([23.24595634, 18.61966223,  5.91887932]), 'targetState': array([24, 18], dtype=int32), 'currentDistance': 0.9759933994798075}
episode index:3729
target Thresh 32.0
target distance 20.0
model initialize at round 3729
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.85316153,  6.59376452]), 'previousTarget': array([24.8507125,  6.59715  ]), 'currentState': array([20.05178632, 26.00888224,  6.20054955]), 'targetState': array([25,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.31310006615185204
running average episode reward sum: 0.48904431257418496
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  6.]), 'previousTarget': array([25.,  6.]), 'currentState': array([25.2786016 ,  6.94112238,  4.3784641 ]), 'targetState': array([25,  6], dtype=int32), 'currentDistance': 0.9814938511062475}
episode index:3730
target Thresh 32.0
target distance 10.0
model initialize at round 3730
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 19.]), 'previousTarget': array([18., 19.]), 'currentState': array([ 8.31291538, 17.3508652 ,  0.71932763]), 'targetState': array([18, 19], dtype=int32), 'currentDistance': 9.826456840143694}
done in step count: 22
reward sum = 0.6502852040574904
running average episode reward sum: 0.48908752910902376
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 19.]), 'previousTarget': array([18., 19.]), 'currentState': array([1.70593713e+01, 1.90750474e+01, 7.74320029e-03]), 'targetState': array([18, 19], dtype=int32), 'currentDistance': 0.9436177756720452}
episode index:3731
target Thresh 32.0
target distance 9.0
model initialize at round 3731
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 28.]), 'previousTarget': array([11., 28.]), 'currentState': array([13.75225019, 19.06553372,  2.7097804 ]), 'targetState': array([11, 28], dtype=int32), 'currentDistance': 9.348773656251396}
done in step count: 19
reward sum = 0.6598060947809424
running average episode reward sum: 0.48913327363358744
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 28.]), 'previousTarget': array([11., 28.]), 'currentState': array([10.96082873, 27.30520518,  1.54531213]), 'targetState': array([11, 28], dtype=int32), 'currentDistance': 0.6958981450358376}
episode index:3732
target Thresh 32.0
target distance 14.0
model initialize at round 3732
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 15.]), 'previousTarget': array([13., 15.]), 'currentState': array([24.08508586, 29.04665831,  0.27764231]), 'targetState': array([13, 15], dtype=int32), 'currentDistance': 17.89379048906874}
done in step count: 46
reward sum = 0.3627620407342619
running average episode reward sum: 0.4890994211736626
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 15.]), 'previousTarget': array([13., 15.]), 'currentState': array([13.80006019, 15.61582216,  4.04140328]), 'targetState': array([13, 15], dtype=int32), 'currentDistance': 1.0096203479241799}
episode index:3733
target Thresh 32.0
target distance 14.0
model initialize at round 3733
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 20.]), 'previousTarget': array([20., 20.]), 'currentState': array([20.99304862,  6.00073073,  2.81557107]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 14.034446392312358}
done in step count: 31
reward sum = 0.49541807463932663
running average episode reward sum: 0.4891011133679491
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 20.]), 'previousTarget': array([20., 20.]), 'currentState': array([19.74931319, 19.25375161,  1.19837021]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 0.7872296546255348}
episode index:3734
target Thresh 32.0
target distance 4.0
model initialize at round 3734
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([ 6.43922571, 16.76170221,  5.72598328]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 4.506230424408604}
done in step count: 7
reward sum = 0.8559903348306508
running average episode reward sum: 0.4891993434138561
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([ 9.15408639, 14.66720329,  5.47146198]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 1.0773718292106036}
episode index:3735
target Thresh 32.0
target distance 16.0
model initialize at round 3735
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 13.]), 'previousTarget': array([24., 13.]), 'currentState': array([16.2910763 , 28.79542796,  5.4829978 ]), 'targetState': array([24, 13], dtype=int32), 'currentDistance': 17.57620690361353}
done in step count: 46
reward sum = 0.4198639887239033
running average episode reward sum: 0.489180784700074
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 13.]), 'previousTarget': array([24., 13.]), 'currentState': array([23.50972306, 13.99094205,  5.09661749]), 'targetState': array([24, 13], dtype=int32), 'currentDistance': 1.1055937912134959}
episode index:3736
target Thresh 32.0
target distance 10.0
model initialize at round 3736
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([16.85114753,  8.40791358,  1.91155588]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 9.769077182741473}
done in step count: 23
reward sum = 0.6570109344565016
running average episode reward sum: 0.4892256950960484
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([14.94906752, 17.19634498,  1.79379589]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 0.8052673475205749}
episode index:3737
target Thresh 32.0
target distance 16.0
model initialize at round 3737
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 15.]), 'previousTarget': array([11., 15.]), 'currentState': array([26.78927082, 21.07397398,  3.05649328]), 'targetState': array([11, 15], dtype=int32), 'currentDistance': 16.917276165488616}
done in step count: 39
reward sum = 0.43250144364557597
running average episode reward sum: 0.48921052006890814
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 15.]), 'previousTarget': array([11., 15.]), 'currentState': array([11.99534283, 15.22811569,  3.53308623]), 'targetState': array([11, 15], dtype=int32), 'currentDistance': 1.0211484319149164}
episode index:3738
target Thresh 32.0
target distance 2.0
model initialize at round 3738
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([10.80865236,  8.38669032,  2.1399413 ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 2.423631880857036}
done in step count: 3
reward sum = 0.9405876730625856
running average episode reward sum: 0.4893312414256863
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([9.97910771, 9.17319283, 2.59171821]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 1.2815077058422342}
episode index:3739
target Thresh 32.0
target distance 3.0
model initialize at round 3739
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 22.]), 'previousTarget': array([18., 22.]), 'currentState': array([20.       , 19.       ,  0.0222966]), 'targetState': array([18, 22], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 10
reward sum = 0.8264688812987165
running average episode reward sum: 0.48942138517966305
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 22.]), 'previousTarget': array([18., 22.]), 'currentState': array([18.80628   , 21.16632085,  2.47208546]), 'targetState': array([18, 22], dtype=int32), 'currentDistance': 1.1597880645409873}
episode index:3740
target Thresh 32.0
target distance 15.0
model initialize at round 3740
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 15.]), 'previousTarget': array([26., 15.]), 'currentState': array([11.25548317, 19.24698142,  0.5792895 ]), 'targetState': array([26, 15], dtype=int32), 'currentDistance': 15.343976922036745}
done in step count: 34
reward sum = 0.4538076462331271
running average episode reward sum: 0.48941186533498343
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 15.]), 'previousTarget': array([26., 15.]), 'currentState': array([25.65963634, 15.81787209,  5.43710729]), 'targetState': array([26, 15], dtype=int32), 'currentDistance': 0.8858680327373059}
episode index:3741
target Thresh 32.0
target distance 8.0
model initialize at round 3741
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 19.]), 'previousTarget': array([ 8., 19.]), 'currentState': array([16.03795926, 13.15563231,  1.58406377]), 'targetState': array([ 8, 19], dtype=int32), 'currentDistance': 9.938079427586876}
done in step count: 22
reward sum = 0.6499183565672647
running average episode reward sum: 0.48945475857155
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 19.]), 'previousTarget': array([ 8., 19.]), 'currentState': array([ 8.95494793, 18.26496   ,  2.49232693]), 'targetState': array([ 8, 19], dtype=int32), 'currentDistance': 1.2050764913484933}
episode index:3742
target Thresh 32.0
target distance 14.0
model initialize at round 3742
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 10.]), 'previousTarget': array([23., 10.]), 'currentState': array([ 9.        , 12.        ,  3.80973095]), 'targetState': array([23, 10], dtype=int32), 'currentDistance': 14.142135623730951}
done in step count: 36
reward sum = 0.4669532320465817
running average episode reward sum: 0.48944874694276963
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 10.]), 'previousTarget': array([23., 10.]), 'currentState': array([22.01499496, 10.45191651,  6.03523627]), 'targetState': array([23, 10], dtype=int32), 'currentDistance': 1.0837266556382126}
episode index:3743
target Thresh 32.0
target distance 16.0
model initialize at round 3743
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([10.        , 21.        ,  0.62130275]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 16.76305461424021}
done in step count: 46
reward sum = 0.39124352696067405
running average episode reward sum: 0.489422516916065
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.24639887,  5.78837635,  4.98835092]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.8259840606431987}
episode index:3744
target Thresh 32.0
target distance 19.0
model initialize at round 3744
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 22.]), 'previousTarget': array([ 6., 22.]), 'currentState': array([9.        , 3.        , 3.66636226]), 'targetState': array([ 6, 22], dtype=int32), 'currentDistance': 19.235384061671343}
done in step count: 50
reward sum = 0.3337834571677735
running average episode reward sum: 0.48938095775458346
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 22.]), 'previousTarget': array([ 6., 22.]), 'currentState': array([ 5.89937627, 21.25960774,  1.41853642]), 'targetState': array([ 6, 22], dtype=int32), 'currentDistance': 0.7471986601777022}
episode index:3745
target Thresh 32.0
target distance 4.0
model initialize at round 3745
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 25.]), 'previousTarget': array([25., 25.]), 'currentState': array([20.95510415, 23.0501168 ,  2.0668759 ]), 'targetState': array([25, 25], dtype=int32), 'currentDistance': 4.490348200982959}
done in step count: 11
reward sum = 0.7929776450738222
running average episode reward sum: 0.48946200331980483
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 25.]), 'previousTarget': array([25., 25.]), 'currentState': array([24.30910112, 25.12019297,  6.21161388]), 'targetState': array([25, 25], dtype=int32), 'currentDistance': 0.7012756996750993}
episode index:3746
target Thresh 32.0
target distance 11.0
model initialize at round 3746
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([ 6.97940717, 20.05116344,  2.20594835]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 11.22703282898785}
done in step count: 26
reward sum = 0.5529525207771503
running average episode reward sum: 0.4894789476799482
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([5.24559699, 9.85983048, 4.63733952]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 0.8942182793834726}
episode index:3747
target Thresh 32.0
target distance 3.0
model initialize at round 3747
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  7.]), 'previousTarget': array([19.,  7.]), 'currentState': array([19.19846159,  9.83993803,  5.46392184]), 'targetState': array([19,  7], dtype=int32), 'currentDistance': 2.846864065932425}
done in step count: 5
reward sum = 0.90204612288462
running average episode reward sum: 0.4895890243008673
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  7.]), 'previousTarget': array([19.,  7.]), 'currentState': array([19.50980034,  7.88400575,  4.4404608 ]), 'targetState': array([19,  7], dtype=int32), 'currentDistance': 1.020471727481365}
episode index:3748
target Thresh 32.0
target distance 23.0
model initialize at round 3748
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.15916375, 10.17035536]), 'previousTarget': array([23.13347761, 10.17676768]), 'currentState': array([ 4.02763687, 15.99999337,  6.18185724]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.26225022769185224
running average episode reward sum: 0.48952838445114494
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([26.19557336,  9.38249739,  5.87393361]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 0.8907336714206592}
episode index:3749
target Thresh 32.0
target distance 18.0
model initialize at round 3749
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([23.        , 26.        ,  6.25467745]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 19.697715603592208}
done in step count: 51
reward sum = 0.3264298105234376
running average episode reward sum: 0.48948489149809754
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.23459735,  8.99101002,  4.55245158]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.0183991198521067}
episode index:3750
target Thresh 32.0
target distance 15.0
model initialize at round 3750
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 3.]), 'previousTarget': array([5., 3.]), 'currentState': array([19.921213  , 11.66355708,  4.30487108]), 'targetState': array([5, 3], dtype=int32), 'currentDistance': 17.253979796498935}
done in step count: 47
reward sum = 0.41895181822441774
running average episode reward sum: 0.489466087692906
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 3.]), 'previousTarget': array([5., 3.]), 'currentState': array([5.81881856, 3.19206318, 3.48115635]), 'targetState': array([5, 3], dtype=int32), 'currentDistance': 0.8410422674914424}
episode index:3751
target Thresh 32.0
target distance 13.0
model initialize at round 3751
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 21.]), 'previousTarget': array([15., 21.]), 'currentState': array([5.11296845, 8.27499234, 1.13316268]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 16.11456523726632}
done in step count: 38
reward sum = 0.4742152443122553
running average episode reward sum: 0.4894620229691905
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 21.]), 'previousTarget': array([15., 21.]), 'currentState': array([14.3789146 , 20.07878677,  0.86513366]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 1.1110269541295454}
episode index:3752
target Thresh 32.0
target distance 8.0
model initialize at round 3752
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 17.]), 'previousTarget': array([22., 17.]), 'currentState': array([14.1077455 , 17.73627767,  5.32659057]), 'targetState': array([22, 17], dtype=int32), 'currentDistance': 7.926524198994103}
done in step count: 19
reward sum = 0.7072012826078993
running average episode reward sum: 0.4895200403578499
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 17.]), 'previousTarget': array([22., 17.]), 'currentState': array([21.09067502, 17.07081546,  0.05468929]), 'targetState': array([22, 17], dtype=int32), 'currentDistance': 0.9120782565496511}
episode index:3753
target Thresh 32.0
target distance 8.0
model initialize at round 3753
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 28.]), 'previousTarget': array([ 9., 28.]), 'currentState': array([ 2.18187743, 20.03620122,  0.33652489]), 'targetState': array([ 9, 28], dtype=int32), 'currentDistance': 10.483743910735866}
done in step count: 23
reward sum = 0.6436789944599256
running average episode reward sum: 0.48956110560934213
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 28.]), 'previousTarget': array([ 9., 28.]), 'currentState': array([ 8.49135761, 27.06978089,  0.82272042]), 'targetState': array([ 9, 28], dtype=int32), 'currentDistance': 1.060200304731367}
episode index:3754
target Thresh 32.0
target distance 16.0
model initialize at round 3754
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([4.        , 3.        , 3.74877083]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 16.1245154965971}
done in step count: 41
reward sum = 0.3988111567210172
running average episode reward sum: 0.4895369378466555
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([19.00233071,  5.02912991,  6.27011508]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 0.9980944623402243}
episode index:3755
target Thresh 32.0
target distance 10.0
model initialize at round 3755
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 29.]), 'previousTarget': array([19., 29.]), 'currentState': array([12.19239691, 19.2425341 ,  0.87835803]), 'targetState': array([19, 29], dtype=int32), 'currentDistance': 11.897545984994048}
done in step count: 29
reward sum = 0.6034934193552638
running average episode reward sum: 0.4895672776979624
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 29.]), 'previousTarget': array([19., 29.]), 'currentState': array([18.15894396, 28.15442992,  1.02958305]), 'targetState': array([19, 29], dtype=int32), 'currentDistance': 1.1926290388330392}
episode index:3756
target Thresh 32.0
target distance 12.0
model initialize at round 3756
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  6.]), 'previousTarget': array([19.,  6.]), 'currentState': array([6.98059992, 2.03759113, 1.80986583]), 'targetState': array([19,  6], dtype=int32), 'currentDistance': 12.65569683598218}
done in step count: 28
reward sum = 0.5227607085791426
running average episode reward sum: 0.48957611278736385
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  6.]), 'previousTarget': array([19.,  6.]), 'currentState': array([18.07066773,  6.54608963,  6.0911304 ]), 'targetState': array([19,  6], dtype=int32), 'currentDistance': 1.0779018321418845}
episode index:3757
target Thresh 32.0
target distance 10.0
model initialize at round 3757
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'previousTarget': array([13., 22.]), 'currentState': array([22.56671479, 21.91188246,  3.32671294]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 9.567120600983452}
done in step count: 21
reward sum = 0.6734118501801638
running average episode reward sum: 0.4896250312911937
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'previousTarget': array([13., 22.]), 'currentState': array([13.99780037, 22.57009894,  2.98575706]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 1.1491816135501458}
episode index:3758
target Thresh 32.0
target distance 18.0
model initialize at round 3758
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.36915575, 24.05696131]), 'previousTarget': array([25.09400392, 23.64100589]), 'currentState': array([14.24890585,  7.43348267,  1.00835845]), 'targetState': array([26, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.3233251911209779
running average episode reward sum: 0.4895807908442211
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 25.]), 'previousTarget': array([26., 25.]), 'currentState': array([25.27689941, 24.31012359,  1.00259699]), 'targetState': array([26, 25], dtype=int32), 'currentDistance': 0.9994017804185755}
episode index:3759
target Thresh 32.0
target distance 14.0
model initialize at round 3759
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 10.]), 'previousTarget': array([26., 10.]), 'currentState': array([12.18109977, 17.14366857,  0.4399572 ]), 'targetState': array([26, 10], dtype=int32), 'currentDistance': 15.55615647502183}
done in step count: 39
reward sum = 0.46019353108513505
running average episode reward sum: 0.4895729750836469
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 10.]), 'previousTarget': array([26., 10.]), 'currentState': array([25.2157085 , 10.21793771,  5.7527301 ]), 'targetState': array([26, 10], dtype=int32), 'currentDistance': 0.8140086002904757}
episode index:3760
target Thresh 32.0
target distance 5.0
model initialize at round 3760
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 19.]), 'previousTarget': array([ 8., 19.]), 'currentState': array([ 5.18846413, 13.9719574 ,  0.05196291]), 'targetState': array([ 8, 19], dtype=int32), 'currentDistance': 5.760724459003827}
done in step count: 12
reward sum = 0.7816487121006386
running average episode reward sum: 0.48965063414693244
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 19.]), 'previousTarget': array([ 8., 19.]), 'currentState': array([ 8.08987396, 18.05281834,  1.28709533]), 'targetState': array([ 8, 19], dtype=int32), 'currentDistance': 0.9514359823710061}
episode index:3761
target Thresh 32.0
target distance 3.0
model initialize at round 3761
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 20.]), 'previousTarget': array([ 9., 20.]), 'currentState': array([11.62118434, 17.32557647,  2.35346901]), 'targetState': array([ 9, 20], dtype=int32), 'currentDistance': 3.7447494594307056}
done in step count: 6
reward sum = 0.8795822674079614
running average episode reward sum: 0.48975428423551864
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 20.]), 'previousTarget': array([ 9., 20.]), 'currentState': array([ 9.91443057, 19.50369104,  2.3329016 ]), 'targetState': array([ 9, 20], dtype=int32), 'currentDistance': 1.0404354153792903}
episode index:3762
target Thresh 32.0
target distance 21.0
model initialize at round 3762
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.82842712,  4.20101013]), 'previousTarget': array([25.82842712,  4.20101013]), 'currentState': array([23.        , 24.        ,  3.67924818]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 72
reward sum = 0.28629009827474683
running average episode reward sum: 0.48970021456080143
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  3.]), 'previousTarget': array([26.,  3.]), 'currentState': array([25.72225833,  3.82504525,  5.08966529]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 0.870540116003086}
episode index:3763
target Thresh 32.0
target distance 24.0
model initialize at round 3763
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8.661226  , 6.94093794]), 'previousTarget': array([8.6609096 , 7.06908484]), 'currentState': array([ 6.94828761, 26.86744898,  4.52321246]), 'targetState': array([9, 3], dtype=int32), 'currentDistance': 20.0}
done in step count: 68
reward sum = 0.2634197365654958
running average episode reward sum: 0.48964009753689197
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 3.]), 'previousTarget': array([9., 3.]), 'currentState': array([9.07551981, 3.91158528, 4.93755557]), 'targetState': array([9, 3], dtype=int32), 'currentDistance': 0.9147081350386775}
episode index:3764
target Thresh 32.0
target distance 11.0
model initialize at round 3764
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  2.]), 'previousTarget': array([22.,  2.]), 'currentState': array([15.        , 13.        ,  0.71258545]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 13.038404810405297}
done in step count: 30
reward sum = 0.5111819229805746
running average episode reward sum: 0.4896458191372754
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  2.]), 'previousTarget': array([22.,  2.]), 'currentState': array([22.08835333,  2.78121698,  4.94451006]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 0.7861973577422257}
episode index:3765
target Thresh 32.0
target distance 13.0
model initialize at round 3765
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 16.]), 'previousTarget': array([17., 16.]), 'currentState': array([ 4.91371641, 28.92443842,  4.11333895]), 'targetState': array([17, 16], dtype=int32), 'currentDistance': 17.695178986617453}
done in step count: 42
reward sum = 0.37227328192025777
running average episode reward sum: 0.48961465277051575
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 16.]), 'previousTarget': array([17., 16.]), 'currentState': array([16.14341056, 16.0904625 ,  5.77411282]), 'targetState': array([17, 16], dtype=int32), 'currentDistance': 0.861352962589104}
episode index:3766
target Thresh 32.0
target distance 2.0
model initialize at round 3766
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 24.]), 'previousTarget': array([23., 24.]), 'currentState': array([21.36651717, 22.08695271,  0.31029998]), 'targetState': array([23, 24], dtype=int32), 'currentDistance': 2.5155548290850422}
done in step count: 6
reward sum = 0.9100743431094016
running average episode reward sum: 0.48972626935940317
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 24.]), 'previousTarget': array([23., 24.]), 'currentState': array([22.82883696, 23.11811077,  0.88498747]), 'targetState': array([23, 24], dtype=int32), 'currentDistance': 0.8983459202388542}
episode index:3767
target Thresh 32.0
target distance 20.0
model initialize at round 3767
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3.43046618, 14.57218647]), 'previousTarget': array([ 3.43046618, 14.57218647]), 'currentState': array([22.        , 22.        ,  6.03151924]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 75
reward sum = 0.21122644340977792
running average episode reward sum: 0.4896523575159983
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 14.]), 'previousTarget': array([ 2., 14.]), 'currentState': array([ 2.27404073, 14.96650895,  4.04522955]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 1.0046083190714699}
episode index:3768
target Thresh 32.0
target distance 11.0
model initialize at round 3768
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 26.]), 'previousTarget': array([16., 26.]), 'currentState': array([ 5.        , 15.        ,  3.01797056]), 'targetState': array([16, 26], dtype=int32), 'currentDistance': 15.556349186104045}
done in step count: 44
reward sum = 0.42153014422181034
running average episode reward sum: 0.48963428316914387
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 26.]), 'previousTarget': array([16., 26.]), 'currentState': array([15.31210271, 25.18432354,  1.16191571]), 'targetState': array([16, 26], dtype=int32), 'currentDistance': 1.0670195702811622}
episode index:3769
target Thresh 32.0
target distance 14.0
model initialize at round 3769
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([24.        , 23.        ,  5.94837821]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 19.79898987322333}
done in step count: 54
reward sum = 0.3242892936305644
running average episode reward sum: 0.4895904250817331
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.69894699,  9.72843428,  4.06941618]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 1.0095263275087136}
episode index:3770
target Thresh 32.0
target distance 2.0
model initialize at round 3770
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 26.]), 'previousTarget': array([18., 26.]), 'currentState': array([16.0208693 , 24.95532919,  5.40193748]), 'targetState': array([18, 26], dtype=int32), 'currentDistance': 2.237922120239212}
done in step count: 5
reward sum = 0.9016072025646502
running average episode reward sum: 0.4896996843703788
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 26.]), 'previousTarget': array([18., 26.]), 'currentState': array([17.47196176, 25.1040924 ,  0.51140317]), 'targetState': array([18, 26], dtype=int32), 'currentDistance': 1.039939806101573}
episode index:3771
target Thresh 32.0
target distance 9.0
model initialize at round 3771
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 16.]), 'previousTarget': array([ 3., 16.]), 'currentState': array([ 5.9713903 , 25.07146067,  2.20410943]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 9.545708932172367}
done in step count: 22
reward sum = 0.6165100663100747
running average episode reward sum: 0.48973330324151876
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 16.]), 'previousTarget': array([ 3., 16.]), 'currentState': array([ 3.47861935, 16.99419587,  4.55399947]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 1.1034046903953465}
episode index:3772
target Thresh 32.0
target distance 5.0
model initialize at round 3772
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 14.]), 'previousTarget': array([ 2., 14.]), 'currentState': array([5.000376  , 8.96204944, 4.60034195]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 5.863719122470566}
done in step count: 16
reward sum = 0.7341166482599861
running average episode reward sum: 0.48979807486755067
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 14.]), 'previousTarget': array([ 2., 14.]), 'currentState': array([ 2.19149325, 13.04454712,  1.78335292]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 0.9744536225797779}
episode index:3773
target Thresh 32.0
target distance 7.0
model initialize at round 3773
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([18.6927958 , 14.78280324,  3.85968386]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 7.297778364263128}
done in step count: 16
reward sum = 0.743153293529665
running average episode reward sum: 0.4898652066160038
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.33166581,  8.83811602,  4.56538407]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.9013549062982067}
episode index:3774
target Thresh 32.0
target distance 10.0
model initialize at round 3774
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 15.]), 'previousTarget': array([17., 15.]), 'currentState': array([26.78731267, 15.29694684,  2.44484341]), 'targetState': array([17, 15], dtype=int32), 'currentDistance': 9.791816312476277}
done in step count: 21
reward sum = 0.6511996523147037
running average episode reward sum: 0.48990794421751344
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 15.]), 'previousTarget': array([17., 15.]), 'currentState': array([17.98957885, 15.06768323,  3.19362482]), 'targetState': array([17, 15], dtype=int32), 'currentDistance': 0.9918907812348219}
episode index:3775
target Thresh 32.0
target distance 13.0
model initialize at round 3775
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([19.00115295, 17.01807759,  1.73484075]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 15.28034007193301}
done in step count: 38
reward sum = 0.41835453830214087
running average episode reward sum: 0.48988899469264174
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([11.27348158,  4.77992989,  4.59811369]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 0.8264882370606218}
episode index:3776
target Thresh 32.0
target distance 9.0
model initialize at round 3776
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([ 7.95455065, 14.98055748,  3.79831934]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 9.849673753499712}
done in step count: 21
reward sum = 0.6251890046285444
running average episode reward sum: 0.4899248167762891
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([11.91421647,  6.86962135,  5.09045038]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 0.8738421493500145}
episode index:3777
target Thresh 32.0
target distance 7.0
model initialize at round 3777
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 12.]), 'previousTarget': array([20., 12.]), 'currentState': array([19.9583583 , 19.0215668 ,  2.91621709]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 7.021690279572802}
done in step count: 16
reward sum = 0.7075701982180309
running average episode reward sum: 0.48998242540028103
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 12.]), 'previousTarget': array([20., 12.]), 'currentState': array([19.83772714, 12.8037215 ,  4.75821299]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 0.8199394702519922}
episode index:3778
target Thresh 32.0
target distance 12.0
model initialize at round 3778
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 25.]), 'previousTarget': array([ 9., 25.]), 'currentState': array([21.0259396 , 20.02763299,  1.05603313]), 'targetState': array([ 9, 25], dtype=int32), 'currentDistance': 13.013364551354899}
done in step count: 30
reward sum = 0.5225302735073925
running average episode reward sum: 0.4899910382206322
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 25.]), 'previousTarget': array([ 9., 25.]), 'currentState': array([ 9.79531263, 24.52516167,  2.84801417]), 'targetState': array([ 9, 25], dtype=int32), 'currentDistance': 0.9262794510151784}
episode index:3779
target Thresh 32.0
target distance 8.0
model initialize at round 3779
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([ 1.9575188 , 28.9678896 ,  4.04134202]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 10.634086990284045}
done in step count: 23
reward sum = 0.6057421744313747
running average episode reward sum: 0.49002166021433874
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([ 8.45663537, 21.83313487,  5.37743363]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 0.9946651847421556}
episode index:3780
target Thresh 32.0
target distance 16.0
model initialize at round 3780
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.08764269,  7.80031087]), 'previousTarget': array([22.05153389,  7.82990784]), 'currentState': array([ 7.05243946, 20.98904333,  5.93521577]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.3331526782481643
running average episode reward sum: 0.4899801714595209
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([22.24893165,  7.83988494,  5.41715812]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 1.126725512273727}
episode index:3781
target Thresh 32.0
target distance 12.0
model initialize at round 3781
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([13.31713701, 19.95618477,  5.94775029]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 14.776415805809641}
done in step count: 32
reward sum = 0.5012955886061017
running average episode reward sum: 0.48998316337309744
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([21.4979846 ,  8.84292598,  5.15040438]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 0.981093102145709}
episode index:3782
target Thresh 32.0
target distance 13.0
model initialize at round 3782
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 14.]), 'previousTarget': array([17., 14.]), 'currentState': array([26.86437381, 26.83582092,  3.96358548]), 'targetState': array([17, 14], dtype=int32), 'currentDistance': 16.18839613024681}
done in step count: 39
reward sum = 0.46993830178408963
running average episode reward sum: 0.4899778647049534
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 14.]), 'previousTarget': array([17., 14.]), 'currentState': array([17.6944798 , 14.80686088,  4.06700009]), 'targetState': array([17, 14], dtype=int32), 'currentDistance': 1.0645781685221152}
episode index:3783
target Thresh 32.0
target distance 21.0
model initialize at round 3783
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6.71633185, 21.62198847]), 'previousTarget': array([ 6.72533058, 21.62476387]), 'currentState': array([22.97598135,  9.97622538,  3.70889473]), 'targetState': array([ 2, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.2532394175139932
running average episode reward sum: 0.4897814542181091
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 25.]), 'previousTarget': array([ 2., 25.]), 'currentState': array([ 4.63189933, 23.12368917,  2.5465933 ]), 'targetState': array([ 2, 25], dtype=int32), 'currentDistance': 3.23224943682965}
episode index:3784
target Thresh 32.0
target distance 17.0
model initialize at round 3784
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  6.]), 'previousTarget': array([10.,  6.]), 'currentState': array([ 9.9941204 , 23.01606753,  2.17409182]), 'targetState': array([10,  6], dtype=int32), 'currentDistance': 17.016068546735006}
done in step count: 50
reward sum = 0.3701441439344614
running average episode reward sum: 0.48974984594590737
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  6.]), 'previousTarget': array([10.,  6.]), 'currentState': array([10.03080406,  6.7744165 ,  4.80946028]), 'targetState': array([10,  6], dtype=int32), 'currentDistance': 0.775028904913017}
episode index:3785
target Thresh 32.0
target distance 6.0
model initialize at round 3785
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  8.]), 'previousTarget': array([25.,  8.]), 'currentState': array([19.1005835 ,  8.76707167,  5.35431352]), 'targetState': array([25,  8], dtype=int32), 'currentDistance': 5.949076732292134}
done in step count: 13
reward sum = 0.781315599576281
running average episode reward sum: 0.4898268575025979
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  8.]), 'previousTarget': array([25.,  8.]), 'currentState': array([24.10697211,  7.83780645,  6.27520906]), 'targetState': array([25,  8], dtype=int32), 'currentDistance': 0.9076373536182574}
episode index:3786
target Thresh 32.0
target distance 5.0
model initialize at round 3786
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 25.]), 'previousTarget': array([25., 25.]), 'currentState': array([23.8562305, 20.4769144,  1.7115306]), 'targetState': array([25, 25], dtype=int32), 'currentDistance': 4.665459459586425}
done in step count: 8
reward sum = 0.8337764668877019
running average episode reward sum: 0.48991768127058977
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 25.]), 'previousTarget': array([25., 25.]), 'currentState': array([24.87558965, 24.16007394,  1.30623689]), 'targetState': array([25, 25], dtype=int32), 'currentDistance': 0.8490899326233184}
episode index:3787
target Thresh 32.0
target distance 21.0
model initialize at round 3787
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.10875718, 26.88835022]), 'previousTarget': array([21.10381815, 26.90990945]), 'currentState': array([23.0561403 ,  6.98338353,  6.24791956]), 'targetState': array([21, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.2721543261485097
running average episode reward sum: 0.4898601935844435
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 28.]), 'previousTarget': array([21., 28.]), 'currentState': array([21.02815625, 27.2301296 ,  1.79972957]), 'targetState': array([21, 28], dtype=int32), 'currentDistance': 0.7703851030971431}
episode index:3788
target Thresh 32.0
target distance 15.0
model initialize at round 3788
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  7.]), 'previousTarget': array([22.,  7.]), 'currentState': array([22.98267437, 22.04325262,  2.20429254]), 'targetState': array([22,  7], dtype=int32), 'currentDistance': 15.07531420170064}
done in step count: 39
reward sum = 0.440311215871449
running average episode reward sum: 0.4898471165251368
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  7.]), 'previousTarget': array([22.,  7.]), 'currentState': array([21.65700624,  7.88897414,  4.85296664]), 'targetState': array([22,  7], dtype=int32), 'currentDistance': 0.9528482282353324}
episode index:3789
target Thresh 32.0
target distance 13.0
model initialize at round 3789
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([27.05406501, 19.9175829 ,  5.11265907]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 14.332703882095126}
done in step count: 32
reward sum = 0.4898810443908661
running average episode reward sum: 0.48984712547708026
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([14.91509162, 14.17156412,  3.44873805]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 0.9310354031974976}
episode index:3790
target Thresh 32.0
target distance 20.0
model initialize at round 3790
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7.95905169, 28.7228197 ]), 'previousTarget': array([ 7.96680906, 28.77872706]), 'currentState': array([5.03614172, 8.93755758, 5.48957491]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.30136510209973183
running average episode reward sum: 0.4897974071907765
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 29.]), 'previousTarget': array([ 8., 29.]), 'currentState': array([ 7.93868104, 28.08770111,  1.68135283]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 0.9143573068354328}
episode index:3791
target Thresh 32.0
target distance 19.0
model initialize at round 3791
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  7.]), 'previousTarget': array([22.,  7.]), 'currentState': array([17.02391022, 25.96419956,  5.23075248]), 'targetState': array([22,  7], dtype=int32), 'currentDistance': 19.606181025371647}
done in step count: 51
reward sum = 0.37898722725484274
running average episode reward sum: 0.48976818509691156
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  7.]), 'previousTarget': array([22.,  7.]), 'currentState': array([21.62569933,  7.80913649,  5.08102355]), 'targetState': array([22,  7], dtype=int32), 'currentDistance': 0.8915171596918184}
episode index:3792
target Thresh 32.0
target distance 16.0
model initialize at round 3792
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 7.]), 'previousTarget': array([5., 7.]), 'currentState': array([10.        , 23.        ,  6.11325643]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 16.76305461424021}
done in step count: 46
reward sum = 0.4188337973062486
running average episode reward sum: 0.4897494837028196
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 7.]), 'previousTarget': array([5., 7.]), 'currentState': array([5.0089901 , 7.99031421, 4.44156664]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 0.9903550176619003}
episode index:3793
target Thresh 32.0
target distance 7.0
model initialize at round 3793
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([10.00032388, 10.98084213,  4.55660021]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 7.275181785074291}
done in step count: 15
reward sum = 0.7279480357177842
running average episode reward sum: 0.4898122666632875
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.76837798, 9.53233259, 3.38010895]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.9347634489194964}
episode index:3794
target Thresh 32.0
target distance 17.0
model initialize at round 3794
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([24.003482  ,  5.01037421,  1.49485138]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 17.72046177842216}
done in step count: 44
reward sum = 0.3876264748557024
running average episode reward sum: 0.48978534023593373
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 7.68641538, 10.30308718,  3.16106694]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.7503518595354668}
episode index:3795
target Thresh 32.0
target distance 15.0
model initialize at round 3795
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 21.]), 'previousTarget': array([25., 21.]), 'currentState': array([ 9.99948323, 24.96931011,  4.9472385 ]), 'targetState': array([25, 21], dtype=int32), 'currentDistance': 15.516794967527003}
done in step count: 38
reward sum = 0.46219029760471
running average episode reward sum: 0.4897780707304987
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 21.]), 'previousTarget': array([25., 21.]), 'currentState': array([24.09758879, 20.67644723,  6.15662158]), 'targetState': array([25, 21], dtype=int32), 'currentDistance': 0.9586617656779962}
episode index:3796
target Thresh 32.0
target distance 1.0
model initialize at round 3796
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 17.]), 'previousTarget': array([20., 17.]), 'currentState': array([20.65860014, 16.94273864,  3.23179451]), 'targetState': array([20, 17], dtype=int32), 'currentDistance': 0.6610847206313968}
done in step count: 0
reward sum = 0.9942922485545829
running average episode reward sum: 0.48991094251817957
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 17.]), 'previousTarget': array([20., 17.]), 'currentState': array([20.65860014, 16.94273864,  3.23179451]), 'targetState': array([20, 17], dtype=int32), 'currentDistance': 0.6610847206313968}
episode index:3797
target Thresh 32.0
target distance 9.0
model initialize at round 3797
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 15.]), 'previousTarget': array([23., 15.]), 'currentState': array([21.        , 24.        ,  2.67195508]), 'targetState': array([23, 15], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 23
reward sum = 0.6250344347934467
running average episode reward sum: 0.4899465200569566
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 15.]), 'previousTarget': array([23., 15.]), 'currentState': array([22.63700147, 15.88879225,  5.20780516]), 'targetState': array([23, 15], dtype=int32), 'currentDistance': 0.960062288941881}
episode index:3798
target Thresh 32.0
target distance 8.0
model initialize at round 3798
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 20.]), 'previousTarget': array([12., 20.]), 'currentState': array([ 9.        , 28.        ,  2.65853977]), 'targetState': array([12, 20], dtype=int32), 'currentDistance': 8.54400374531753}
done in step count: 20
reward sum = 0.6526958561426309
running average episode reward sum: 0.4899893601033071
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 20.]), 'previousTarget': array([12., 20.]), 'currentState': array([11.11632462, 20.99518709,  5.20081902]), 'targetState': array([12, 20], dtype=int32), 'currentDistance': 1.3308942554984922}
episode index:3799
target Thresh 32.0
target distance 21.0
model initialize at round 3799
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5.37523613, 12.72533058]), 'previousTarget': array([ 5.37523613, 12.72533058]), 'currentState': array([17.       , 29.       ,  2.7319563]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 83
reward sum = 0.1793494169330161
running average episode reward sum: 0.4899076127498413
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.07855667, 8.93666903, 4.09703781]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.9399574603461551}
episode index:3800
target Thresh 32.0
target distance 16.0
model initialize at round 3800
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([ 5.        , 18.        ,  0.34329677]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 18.35755975068582}
done in step count: 55
reward sum = 0.35862946736433565
running average episode reward sum: 0.4898730749583692
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.84480916,  2.78467756,  5.24686564]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.7998769123784276}
episode index:3801
target Thresh 32.0
target distance 12.0
model initialize at round 3801
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 17.]), 'previousTarget': array([ 5., 17.]), 'currentState': array([17.02522964, 28.95015156,  5.07028064]), 'targetState': array([ 5, 17], dtype=int32), 'currentDistance': 16.953237746127474}
done in step count: 40
reward sum = 0.43045788070346713
running average episode reward sum: 0.48985744760585603
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 17.]), 'previousTarget': array([ 5., 17.]), 'currentState': array([ 5.8810305 , 17.25653399,  3.50415731]), 'targetState': array([ 5, 17], dtype=int32), 'currentDistance': 0.9176188906609829}
episode index:3802
target Thresh 32.0
target distance 8.0
model initialize at round 3802
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 16.]), 'previousTarget': array([13., 16.]), 'currentState': array([19.99965638,  7.99129178,  4.63516766]), 'targetState': array([13, 16], dtype=int32), 'currentDistance': 10.636474825557604}
done in step count: 30
reward sum = 0.5763556391035896
running average episode reward sum: 0.48988019233146685
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 16.]), 'previousTarget': array([13., 16.]), 'currentState': array([13.56387248, 15.2466009 ,  2.33356022]), 'targetState': array([13, 16], dtype=int32), 'currentDistance': 0.9410432385296541}
episode index:3803
target Thresh 32.0
target distance 5.0
model initialize at round 3803
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([ 3.00034716, 13.00249206,  1.6506772 ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 5.002492075865853}
done in step count: 17
reward sum = 0.7245316553236315
running average episode reward sum: 0.48994187778440906
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.66074949, 8.65119158, 5.0191804 ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.7342624775138888}
episode index:3804
target Thresh 32.0
target distance 16.0
model initialize at round 3804
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  2.]), 'previousTarget': array([21.,  2.]), 'currentState': array([23.98564114, 18.03930456,  2.17130584]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 16.314819756806596}
done in step count: 43
reward sum = 0.4029351275724737
running average episode reward sum: 0.4899190113585978
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  2.]), 'previousTarget': array([21.,  2.]), 'currentState': array([20.61568709,  2.88527818,  4.75914213]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 0.9650978584733234}
episode index:3805
target Thresh 32.0
target distance 14.0
model initialize at round 3805
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 25.]), 'previousTarget': array([17., 25.]), 'currentState': array([ 9.        , 11.        ,  6.11226654]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 16.1245154965971}
done in step count: 39
reward sum = 0.4433936211261317
running average episode reward sum: 0.4899067871362561
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 25.]), 'previousTarget': array([17., 25.]), 'currentState': array([16.9449273 , 24.13741601,  1.2349365 ]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 0.864340297782718}
episode index:3806
target Thresh 32.0
target distance 12.0
model initialize at round 3806
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 14.]), 'previousTarget': array([12., 14.]), 'currentState': array([5.2981993 , 2.13745448, 0.58712941]), 'targetState': array([12, 14], dtype=int32), 'currentDistance': 13.624761236049203}
done in step count: 31
reward sum = 0.5357715256367207
running average episode reward sum: 0.4899188346115648
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 14.]), 'previousTarget': array([12., 14.]), 'currentState': array([11.86679347, 13.19170378,  1.28287615]), 'targetState': array([12, 14], dtype=int32), 'currentDistance': 0.8191988511986271}
episode index:3807
target Thresh 32.0
target distance 15.0
model initialize at round 3807
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.64428464, 23.61892395]), 'previousTarget': array([18.64636501, 23.62110536]), 'currentState': array([4.99700968, 8.99866793, 3.32129744]), 'targetState': array([19, 24], dtype=int32), 'currentDistance': 20.0}
done in step count: 72
reward sum = 0.2720469080276364
running average episode reward sum: 0.48986162034513003
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 24.]), 'previousTarget': array([19., 24.]), 'currentState': array([19.00350921, 23.15424207,  1.0584366 ]), 'targetState': array([19, 24], dtype=int32), 'currentDistance': 0.8457652121682739}
episode index:3808
target Thresh 32.0
target distance 9.0
model initialize at round 3808
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([ 7.01145917, 13.01722376,  0.83762661]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 12.046871329339101}
done in step count: 29
reward sum = 0.5649145896287849
running average episode reward sum: 0.4898813244588826
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.16239118,  4.74355258,  5.42067994]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.1200263304641211}
episode index:3809
target Thresh 32.0
target distance 9.0
model initialize at round 3809
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([11.89157716, 22.78867509,  4.39970708]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 9.701543590001124}
done in step count: 21
reward sum = 0.6572523908664742
running average episode reward sum: 0.4899252538726379
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([15.18604738, 14.59659477,  5.45646223]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 1.0091799603191016}
episode index:3810
target Thresh 32.0
target distance 2.0
model initialize at round 3810
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 29.]), 'previousTarget': array([22., 29.]), 'currentState': array([23.99071893, 29.02223853,  2.20302802]), 'targetState': array([22, 29], dtype=int32), 'currentDistance': 1.9908431395702708}
done in step count: 4
reward sum = 0.9267188452882651
running average episode reward sum: 0.49003986777749636
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 29.]), 'previousTarget': array([22., 29.]), 'currentState': array([22.84293406, 29.47516923,  3.09114099]), 'targetState': array([22, 29], dtype=int32), 'currentDistance': 0.9676381715700294}
episode index:3811
target Thresh 32.0
target distance 2.0
model initialize at round 3811
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([11.98860248,  3.96942533,  4.60122445]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 2.232824547321964}
done in step count: 6
reward sum = 0.8982415354927774
running average episode reward sum: 0.4901469511111048
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.11410756,  2.89474003,  5.88615242]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.8921239126561304}
episode index:3812
target Thresh 32.0
target distance 23.0
model initialize at round 3812
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7.37514441, 17.71201303]), 'previousTarget': array([ 7.37514441, 17.71201303]), 'currentState': array([26.        , 25.        ,  1.83691445]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 74
reward sum = 0.192592730957302
running average episode reward sum: 0.4900689143368709
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 16.]), 'previousTarget': array([ 3., 16.]), 'currentState': array([ 3.83282726, 16.58483301,  3.62537121]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 1.0176595174596401}
episode index:3813
target Thresh 32.0
target distance 8.0
model initialize at round 3813
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 27.]), 'previousTarget': array([ 8., 27.]), 'currentState': array([15.97329658, 24.97861263,  3.60187328]), 'targetState': array([ 8, 27], dtype=int32), 'currentDistance': 8.225537389213843}
done in step count: 18
reward sum = 0.6998848153342763
running average episode reward sum: 0.4901239263717418
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 27.]), 'previousTarget': array([ 8., 27.]), 'currentState': array([ 8.78655661, 27.14083405,  2.90630068]), 'targetState': array([ 8, 27], dtype=int32), 'currentDistance': 0.7990654130079679}
episode index:3814
target Thresh 32.0
target distance 21.0
model initialize at round 3814
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5.16133427, 17.35882353]), 'previousTarget': array([ 5.28336929, 17.28013989]), 'currentState': array([22.91191148,  8.14378101,  2.27089635]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.2340975234962605
running average episode reward sum: 0.49005681591227246
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 19.]), 'previousTarget': array([ 2., 19.]), 'currentState': array([ 2.81483328, 19.46236525,  3.0747106 ]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 0.936875067648993}
episode index:3815
target Thresh 32.0
target distance 11.0
model initialize at round 3815
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 14.]), 'previousTarget': array([ 5., 14.]), 'currentState': array([16.        ,  3.        ,  1.24245206]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 15.556349186104045}
done in step count: 44
reward sum = 0.4477484508418593
running average episode reward sum: 0.49004572881450764
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 14.]), 'previousTarget': array([ 5., 14.]), 'currentState': array([ 5.94626011, 14.14974433,  2.65758313]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 0.958035260102422}
episode index:3816
target Thresh 32.0
target distance 17.0
model initialize at round 3816
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 17.]), 'previousTarget': array([26., 17.]), 'currentState': array([ 9.        , 10.        ,  1.72638345]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 18.384776310850235}
done in step count: 50
reward sum = 0.3719977770818218
running average episode reward sum: 0.49001480192120594
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 17.]), 'previousTarget': array([26., 17.]), 'currentState': array([25.30876283, 16.25188815,  0.52849762]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 1.018567702275216}
episode index:3817
target Thresh 32.0
target distance 12.0
model initialize at round 3817
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 14.]), 'previousTarget': array([ 5., 14.]), 'currentState': array([15.92249138, 25.68546851,  4.30114725]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 15.99534282790025}
done in step count: 38
reward sum = 0.4594240988885919
running average episode reward sum: 0.4900067896888768
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 14.]), 'previousTarget': array([ 5., 14.]), 'currentState': array([ 5.06215564, 14.66581795,  4.17940355]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 0.6687128459232081}
episode index:3818
target Thresh 32.0
target distance 6.0
model initialize at round 3818
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 17.]), 'previousTarget': array([19., 17.]), 'currentState': array([12.97628871, 12.08458457,  1.63705409]), 'targetState': array([19, 17], dtype=int32), 'currentDistance': 7.774728710900552}
done in step count: 16
reward sum = 0.7100827359117079
running average episode reward sum: 0.49006441627861835
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 17.]), 'previousTarget': array([19., 17.]), 'currentState': array([18.60344025, 16.20868484,  0.67272937]), 'targetState': array([19, 17], dtype=int32), 'currentDistance': 0.8851210718523054}
episode index:3819
target Thresh 32.0
target distance 13.0
model initialize at round 3819
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([4.44311645, 9.78284955, 5.74721138]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 13.824495406269333}
done in step count: 32
reward sum = 0.5263275524445372
running average episode reward sum: 0.49007390924620103
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([16.22346385,  3.96127944,  5.98211922]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 0.7775009163045723}
episode index:3820
target Thresh 32.0
target distance 7.0
model initialize at round 3820
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 23.]), 'previousTarget': array([22., 23.]), 'currentState': array([15.00324481, 27.07304817,  1.32870474]), 'targetState': array([22, 23], dtype=int32), 'currentDistance': 8.095943708704311}
done in step count: 19
reward sum = 0.6755695200103744
running average episode reward sum: 0.49012245559814144
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 23.]), 'previousTarget': array([22., 23.]), 'currentState': array([21.04699583, 23.28791592,  5.67288564]), 'targetState': array([22, 23], dtype=int32), 'currentDistance': 0.9955463466735128}
episode index:3821
target Thresh 32.0
target distance 17.0
model initialize at round 3821
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6.20859686, 9.13497444]), 'currentState': array([22.8975329 , 19.68060369,  4.20447311]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 19.99004535020292}
done in step count: 56
reward sum = 0.34292561143610073
running average episode reward sum: 0.49008394255675947
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.80497005, 9.16901115, 3.38178012]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.8225214636259187}
episode index:3822
target Thresh 32.0
target distance 11.0
model initialize at round 3822
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([13.        , 12.        ,  3.73820829]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 14.866068747318504}
done in step count: 35
reward sum = 0.4567784549161822
running average episode reward sum: 0.49007523068450193
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([23.81911384,  2.90008785,  5.31997214]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 0.9180838439049236}
episode index:3823
target Thresh 32.0
target distance 3.0
model initialize at round 3823
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 28.]), 'previousTarget': array([10., 28.]), 'currentState': array([ 6.99343954, 27.99846492,  3.62394691]), 'targetState': array([10, 28], dtype=int32), 'currentDistance': 3.0065608525717593}
done in step count: 9
reward sum = 0.82605396818332
running average episode reward sum: 0.490163091233011
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 28.]), 'previousTarget': array([10., 28.]), 'currentState': array([ 9.07269447, 27.29556642,  0.16023366]), 'targetState': array([10, 28], dtype=int32), 'currentDistance': 1.1645266105448118}
episode index:3824
target Thresh 32.0
target distance 14.0
model initialize at round 3824
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 15.]), 'previousTarget': array([17., 15.]), 'currentState': array([ 3.        , 23.        ,  4.29948246]), 'targetState': array([17, 15], dtype=int32), 'currentDistance': 16.1245154965971}
done in step count: 41
reward sum = 0.4448324025062339
running average episode reward sum: 0.4901512400725596
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 15.]), 'previousTarget': array([17., 15.]), 'currentState': array([16.07839207, 15.8124864 ,  5.47134286]), 'targetState': array([17, 15], dtype=int32), 'currentDistance': 1.228615211470764}
episode index:3825
target Thresh 32.0
target distance 11.0
model initialize at round 3825
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  3.]), 'previousTarget': array([21.,  3.]), 'currentState': array([21.        , 14.        ,  0.49767709]), 'targetState': array([21,  3], dtype=int32), 'currentDistance': 11.0}
done in step count: 30
reward sum = 0.5663958780319227
running average episode reward sum: 0.4901711681012996
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  3.]), 'previousTarget': array([21.,  3.]), 'currentState': array([21.22590904,  3.8861028 ,  4.69890366]), 'targetState': array([21,  3], dtype=int32), 'currentDistance': 0.9144468594526661}
episode index:3826
target Thresh 32.0
target distance 14.0
model initialize at round 3826
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 25.]), 'previousTarget': array([ 9., 25.]), 'currentState': array([ 2.94607251, 11.18201971,  1.7437062 ]), 'targetState': array([ 9, 25], dtype=int32), 'currentDistance': 15.08597418791466}
done in step count: 35
reward sum = 0.49553498027658793
running average episode reward sum: 0.4901725696722887
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 25.]), 'previousTarget': array([ 9., 25.]), 'currentState': array([ 9.00692502, 24.08729398,  1.22417283]), 'targetState': array([ 9, 25], dtype=int32), 'currentDistance': 0.912732290232311}
episode index:3827
target Thresh 32.0
target distance 6.0
model initialize at round 3827
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 13.]), 'previousTarget': array([11., 13.]), 'currentState': array([ 6.01381872, 19.02771468,  0.920856  ]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 7.822745555463362}
done in step count: 20
reward sum = 0.6842171632139247
running average episode reward sum: 0.49022326052744586
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 13.]), 'previousTarget': array([11., 13.]), 'currentState': array([10.83084051, 13.99907984,  4.90581279]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 1.0132992991014365}
episode index:3828
target Thresh 32.0
target distance 19.0
model initialize at round 3828
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8.30083934, 21.31551707]), 'previousTarget': array([ 8.30163555, 21.31492866]), 'currentState': array([26.00015337, 12.00239283,  1.73309553]), 'targetState': array([ 7, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.3124985904276262
running average episode reward sum: 0.4901768451004153
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 22.]), 'previousTarget': array([ 7., 22.]), 'currentState': array([ 7.90804815, 21.40875634,  2.83144267]), 'targetState': array([ 7, 22], dtype=int32), 'currentDistance': 1.083568418083319}
episode index:3829
target Thresh 32.0
target distance 12.0
model initialize at round 3829
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 21.]), 'previousTarget': array([10., 21.]), 'currentState': array([3.95966991, 9.01177913, 2.74329987]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 13.42397210744969}
done in step count: 31
reward sum = 0.5205052572592832
running average episode reward sum: 0.4901847637458876
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 21.]), 'previousTarget': array([10., 21.]), 'currentState': array([ 9.15232867, 20.462991  ,  0.76278395]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 1.003456700287493}
episode index:3830
target Thresh 32.0
target distance 4.0
model initialize at round 3830
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 13.]), 'previousTarget': array([ 9., 13.]), 'currentState': array([11.99199907,  8.98337703,  4.07737929]), 'targetState': array([ 9, 13], dtype=int32), 'currentDistance': 5.008524590065215}
done in step count: 12
reward sum = 0.7816235227697332
running average episode reward sum: 0.4902608375540379
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 13.]), 'previousTarget': array([ 9., 13.]), 'currentState': array([ 9.44538942, 12.30705193,  1.96640377]), 'targetState': array([ 9, 13], dtype=int32), 'currentDistance': 0.8237407174113479}
episode index:3831
target Thresh 32.0
target distance 24.0
model initialize at round 3831
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7.06910774, 12.66090297]), 'previousTarget': array([ 7.06908484, 12.6609096 ]), 'currentState': array([27.000021 , 10.9999704,  5.1567018]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.22704160446235638
running average episode reward sum: 0.4901921477750474
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 13.]), 'previousTarget': array([ 3., 13.]), 'currentState': array([ 3.91387455, 13.19541934,  3.19803164]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 0.934534862870903}
episode index:3832
target Thresh 32.0
target distance 10.0
model initialize at round 3832
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([ 2.99411736, 14.00668975,  2.13342556]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 11.670390269233701}
done in step count: 33
reward sum = 0.5221484938042966
running average episode reward sum: 0.49020048493811264
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([12.30943482,  8.15686773,  5.86484858]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 0.7081579987728481}
episode index:3833
target Thresh 32.0
target distance 16.0
model initialize at round 3833
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([ 9.15557769, 13.65680345,  5.27457109]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 16.514585577352786}
done in step count: 37
reward sum = 0.445874431173358
running average episode reward sum: 0.4901889236304014
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([24.13703253,  8.89872531,  6.20338007]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 0.8688897582665167}
episode index:3834
target Thresh 32.0
target distance 2.0
model initialize at round 3834
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 25.]), 'previousTarget': array([10., 25.]), 'currentState': array([ 8.31675348, 25.71952751,  5.68549576]), 'targetState': array([10, 25], dtype=int32), 'currentDistance': 1.8305842512668244}
done in step count: 2
reward sum = 0.9523279849035371
running average episode reward sum: 0.49030942925263693
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 25.]), 'previousTarget': array([10., 25.]), 'currentState': array([ 9.14149754, 25.26867274,  5.83653458]), 'targetState': array([10, 25], dtype=int32), 'currentDistance': 0.8995618458357866}
episode index:3835
target Thresh 32.0
target distance 12.0
model initialize at round 3835
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 14.]), 'previousTarget': array([20., 14.]), 'currentState': array([10.22054724, 25.90313031,  5.74956159]), 'targetState': array([20, 14], dtype=int32), 'currentDistance': 15.405265575817209}
done in step count: 36
reward sum = 0.49288270934843864
running average episode reward sum: 0.49031010007643666
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 14.]), 'previousTarget': array([20., 14.]), 'currentState': array([19.1313667 , 14.61693125,  5.60186647]), 'targetState': array([20, 14], dtype=int32), 'currentDistance': 1.0654238446233026}
episode index:3836
target Thresh 32.0
target distance 20.0
model initialize at round 3836
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.58088616, 25.08416657]), 'previousTarget': array([22.61161351, 25.0776773 ]), 'currentState': array([ 2.972372  , 29.02195432,  2.26814145]), 'targetState': array([23, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 81
reward sum = 0.25205649045276757
running average episode reward sum: 0.4902480063548772
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 25.]), 'previousTarget': array([23., 25.]), 'currentState': array([22.1784739 , 24.74511616,  5.94662758]), 'targetState': array([23, 25], dtype=int32), 'currentDistance': 0.8601574838155872}
episode index:3837
target Thresh 32.0
target distance 9.0
model initialize at round 3837
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 20.]), 'previousTarget': array([ 2., 20.]), 'currentState': array([11.00976071, 16.9829422 ,  4.97962666]), 'targetState': array([ 2, 20], dtype=int32), 'currentDistance': 9.501495975421651}
done in step count: 24
reward sum = 0.604878623542092
running average episode reward sum: 0.49027787363397757
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 20.]), 'previousTarget': array([ 2., 20.]), 'currentState': array([ 2.85180178, 20.00374042,  3.05184983]), 'targetState': array([ 2, 20], dtype=int32), 'currentDistance': 0.8518099937100542}
episode index:3838
target Thresh 32.0
target distance 14.0
model initialize at round 3838
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([24.11417706, 26.85317494,  5.2386336 ]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 14.013572010867597}
done in step count: 34
reward sum = 0.5212926212587949
running average episode reward sum: 0.49028595249504164
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([22.02208295, 13.69166648,  4.5363297 ]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 0.6920189117899325}
episode index:3839
target Thresh 32.0
target distance 15.0
model initialize at round 3839
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 7.]), 'previousTarget': array([5., 7.]), 'currentState': array([20.        ,  2.        ,  4.26207459]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 15.811388300841896}
done in step count: 39
reward sum = 0.45779586003397715
running average episode reward sum: 0.4902774915334632
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 7.]), 'previousTarget': array([5., 7.]), 'currentState': array([5.97189544, 6.68953183, 2.83854328]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 1.0202799763318884}
episode index:3840
target Thresh 32.0
target distance 1.0
model initialize at round 3840
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  2.]), 'previousTarget': array([26.,  2.]), 'currentState': array([27.        ,  2.        ,  0.12715633]), 'targetState': array([26,  2], dtype=int32), 'currentDistance': 1.0}
done in step count: 5
reward sum = 0.9159163647842751
running average episode reward sum: 0.4903883061320706
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  2.]), 'previousTarget': array([26.,  2.]), 'currentState': array([26.9958016 ,  2.04334012,  2.19599984]), 'targetState': array([26,  2], dtype=int32), 'currentDistance': 0.9967442966526266}
episode index:3841
target Thresh 32.0
target distance 14.0
model initialize at round 3841
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 16.]), 'previousTarget': array([24., 16.]), 'currentState': array([ 9.99376999, 15.00288986,  2.45477176]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 14.04167752166239}
done in step count: 36
reward sum = 0.47300826445981636
running average episode reward sum: 0.4903837824356437
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 16.]), 'previousTarget': array([24., 16.]), 'currentState': array([23.07531994, 15.77665726,  0.02429353]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 0.9512703025590724}
episode index:3842
target Thresh 32.0
target distance 11.0
model initialize at round 3842
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 18.]), 'previousTarget': array([18., 18.]), 'currentState': array([26.        , 29.        ,  2.75981513]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 13.601470508735444}
done in step count: 39
reward sum = 0.5010911287819502
running average episode reward sum: 0.4903865686303734
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 18.]), 'previousTarget': array([18., 18.]), 'currentState': array([17.91967471, 18.96722573,  4.2564827 ]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 0.9705553935183779}
episode index:3843
target Thresh 32.0
target distance 18.0
model initialize at round 3843
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([11.        , 27.        ,  5.97038293]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 18.439088914585774}
done in step count: 46
reward sum = 0.37001695078086105
running average episode reward sum: 0.4903552549940962
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([6.8894533 , 9.81543192, 4.65456647]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.8228911153626741}
episode index:3844
target Thresh 32.0
target distance 16.0
model initialize at round 3844
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.84479898,  4.9729318 ]), 'previousTarget': array([10.82990784,  4.94846611]), 'currentState': array([23.95751021, 20.07448153,  2.34172273]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.26671173747057536
running average episode reward sum: 0.4902970902301109
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  4.]), 'previousTarget': array([10.,  4.]), 'currentState': array([9.84799011, 4.90779522, 4.3265943 ]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 0.920434224377537}
episode index:3845
target Thresh 32.0
target distance 8.0
model initialize at round 3845
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([4.13403844, 9.32490213, 1.07852842]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 8.59376437581381}
done in step count: 16
reward sum = 0.7108457075744995
running average episode reward sum: 0.4903544351644178
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([ 7.61384586, 16.03796745,  1.25067497]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 1.036639591540226}
episode index:3846
target Thresh 32.0
target distance 26.0
model initialize at round 3846
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.80591991, 22.25010738]), 'previousTarget': array([10.80053053, 22.31231517]), 'currentState': array([15.97506749,  2.92965414,  4.14291784]), 'targetState': array([ 9, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.18824262805181177
running average episode reward sum: 0.490178038735196
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 29.]), 'previousTarget': array([ 9., 29.]), 'currentState': array([10.08487273, 24.050293  ,  1.69735468]), 'targetState': array([ 9, 29], dtype=int32), 'currentDistance': 5.067203193383647}
episode index:3847
target Thresh 32.0
target distance 13.0
model initialize at round 3847
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 17.]), 'previousTarget': array([20., 17.]), 'currentState': array([26.        ,  4.        ,  3.86700362]), 'targetState': array([20, 17], dtype=int32), 'currentDistance': 14.317821063276353}
done in step count: 39
reward sum = 0.4852220130731051
running average episode reward sum: 0.4901767507867391
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 17.]), 'previousTarget': array([20., 17.]), 'currentState': array([20.8707708 , 16.14752775,  2.0670274 ]), 'targetState': array([20, 17], dtype=int32), 'currentDistance': 1.2185855421908955}
episode index:3848
target Thresh 32.0
target distance 11.0
model initialize at round 3848
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([5.29580971, 9.97813112, 6.05098796]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 13.350290858788824}
done in step count: 30
reward sum = 0.5506362298774071
running average episode reward sum: 0.49019245862750055
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.06745643,  2.37519404,  5.7343293 ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.0051905652275395}
episode index:3849
target Thresh 32.0
target distance 11.0
model initialize at round 3849
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 21.]), 'previousTarget': array([15., 21.]), 'currentState': array([25.95599647, 14.91367563,  4.06882846]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 12.533044447812593}
done in step count: 30
reward sum = 0.5484475465549684
running average episode reward sum: 0.49020758981917006
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 21.]), 'previousTarget': array([15., 21.]), 'currentState': array([15.73298301, 20.73789157,  2.54428107]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 0.7784374840232289}
episode index:3850
target Thresh 32.0
target distance 11.0
model initialize at round 3850
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 7.]), 'previousTarget': array([8., 7.]), 'currentState': array([18.73024625, 16.90358847,  3.70131949]), 'targetState': array([8, 7], dtype=int32), 'currentDistance': 14.602028940689621}
done in step count: 33
reward sum = 0.5166142508483624
running average episode reward sum: 0.4902144469111018
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 7.]), 'previousTarget': array([8., 7.]), 'currentState': array([8.76667323, 7.86649793, 3.87399624]), 'targetState': array([8, 7], dtype=int32), 'currentDistance': 1.1569816353916749}
episode index:3851
target Thresh 32.0
target distance 8.0
model initialize at round 3851
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 14.]), 'previousTarget': array([ 6., 14.]), 'currentState': array([ 7.00341336, 21.53970911,  4.67884006]), 'targetState': array([ 6, 14], dtype=int32), 'currentDistance': 7.606185105747473}
done in step count: 15
reward sum = 0.7335848333643074
running average episode reward sum: 0.4902776271775746
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 14.]), 'previousTarget': array([ 6., 14.]), 'currentState': array([ 5.78464747, 14.84293254,  4.54879163]), 'targetState': array([ 6, 14], dtype=int32), 'currentDistance': 0.8700068892380693}
episode index:3852
target Thresh 32.0
target distance 11.0
model initialize at round 3852
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 16.]), 'previousTarget': array([23., 16.]), 'currentState': array([26.05303221, 26.93521922,  5.20088035]), 'targetState': array([23, 16], dtype=int32), 'currentDistance': 11.353414691088291}
done in step count: 27
reward sum = 0.5987372616914647
running average episode reward sum: 0.4903057765766179
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 16.]), 'previousTarget': array([23., 16.]), 'currentState': array([22.96555207, 16.8286831 ,  4.39597624]), 'targetState': array([23, 16], dtype=int32), 'currentDistance': 0.8293987833531765}
episode index:3853
target Thresh 32.0
target distance 27.0
model initialize at round 3853
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.27382973, 20.1652124 ]), 'previousTarget': array([15.32368762, 20.0200334 ]), 'currentState': array([23.98328554,  2.16117453,  1.85571614]), 'targetState': array([11, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.13050771950495077
running average episode reward sum: 0.4902124195301541
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 29.]), 'previousTarget': array([11., 29.]), 'currentState': array([11.95119419, 28.18099646,  2.25090808]), 'targetState': array([11, 29], dtype=int32), 'currentDistance': 1.2552040423961528}
episode index:3854
target Thresh 32.0
target distance 7.0
model initialize at round 3854
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 28.]), 'previousTarget': array([11., 28.]), 'currentState': array([ 4.02560714, 20.96131485,  5.54961014]), 'targetState': array([11, 28], dtype=int32), 'currentDistance': 9.90884677653446}
done in step count: 24
reward sum = 0.625037702364813
running average episode reward sum: 0.4902473936631851
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 28.]), 'previousTarget': array([11., 28.]), 'currentState': array([10.48105441, 27.22887887,  1.01387244]), 'targetState': array([11, 28], dtype=int32), 'currentDistance': 0.9294795940580411}
episode index:3855
target Thresh 32.0
target distance 13.0
model initialize at round 3855
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 14.]), 'previousTarget': array([ 9., 14.]), 'currentState': array([20.00588554, 26.99631733,  5.68976335]), 'targetState': array([ 9, 14], dtype=int32), 'currentDistance': 17.030378171570856}
done in step count: 50
reward sum = 0.4154560854531854
running average episode reward sum: 0.49022799757703106
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 14.]), 'previousTarget': array([ 9., 14.]), 'currentState': array([ 9.80978421, 14.91485309,  4.04803206]), 'targetState': array([ 9, 14], dtype=int32), 'currentDistance': 1.221763748821258}
episode index:3856
target Thresh 32.0
target distance 2.0
model initialize at round 3856
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 27.]), 'previousTarget': array([ 8., 27.]), 'currentState': array([ 5.99780746, 24.93087959,  4.92935926]), 'targetState': array([ 8, 27], dtype=int32), 'currentDistance': 2.8792419559554117}
done in step count: 8
reward sum = 0.8401812090500965
running average episode reward sum: 0.4903187295478563
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 27.]), 'previousTarget': array([ 8., 27.]), 'currentState': array([ 7.87595121, 26.0663578 ,  1.10187648]), 'targetState': array([ 8, 27], dtype=int32), 'currentDistance': 0.9418470509058748}
episode index:3857
target Thresh 32.0
target distance 14.0
model initialize at round 3857
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([17.88638549, 16.20694838,  2.28030938]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 14.060665816121913}
done in step count: 41
reward sum = 0.4976912057620496
running average episode reward sum: 0.49032064050592117
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([ 4.86981565, 14.42616911,  3.33283538]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 0.968606927320299}
episode index:3858
target Thresh 32.0
target distance 20.0
model initialize at round 3858
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.72460163, 21.73065709]), 'previousTarget': array([17.638375  , 21.52431817]), 'currentState': array([7.92583837, 4.29551332, 1.6242474 ]), 'targetState': array([19, 24], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.2688843483347828
running average episode reward sum: 0.49026325872510457
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 24.]), 'previousTarget': array([19., 24.]), 'currentState': array([18.82158889, 23.18554708,  1.0887583 ]), 'targetState': array([19, 24], dtype=int32), 'currentDistance': 0.8337650050556116}
episode index:3859
target Thresh 32.0
target distance 25.0
model initialize at round 3859
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5.53286811, 9.58889523]), 'previousTarget': array([5.60740149, 9.74071961]), 'currentState': array([10.82291229, 28.87659682,  3.94037318]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 71
reward sum = 0.18821553460818735
running average episode reward sum: 0.4901850080193748
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.57797555, 4.78701587, 4.61537953]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.8930277822246716}
episode index:3860
target Thresh 32.0
target distance 11.0
model initialize at round 3860
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 28.]), 'previousTarget': array([17., 28.]), 'currentState': array([17.90506568, 17.0778569 ,  2.24994791]), 'targetState': array([17, 28], dtype=int32), 'currentDistance': 10.959578174360134}
done in step count: 23
reward sum = 0.6169880728770201
running average episode reward sum: 0.490217850046015
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 28.]), 'previousTarget': array([17., 28.]), 'currentState': array([17.10638517, 27.13982864,  1.63472785]), 'targetState': array([17, 28], dtype=int32), 'currentDistance': 0.8667252048800446}
episode index:3861
target Thresh 32.0
target distance 13.0
model initialize at round 3861
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([ 8.        , 16.        ,  2.74035215]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 14.7648230602334}
done in step count: 39
reward sum = 0.44148735562585745
running average episode reward sum: 0.4902052321033894
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.80134461,  3.78899478,  5.08893784]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.8136195183363809}
episode index:3862
target Thresh 32.0
target distance 20.0
model initialize at round 3862
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5.00124766, 23.97504678]), 'previousTarget': array([ 5.00124766, 23.97504678]), 'currentState': array([6.       , 4.       , 5.6915946]), 'targetState': array([ 5, 24], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.3052759653083818
running average episode reward sum: 0.4901573601730774
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 24.]), 'previousTarget': array([ 5., 24.]), 'currentState': array([ 5.22118354, 23.37005527,  1.5958311 ]), 'targetState': array([ 5, 24], dtype=int32), 'currentDistance': 0.6676470048809701}
episode index:3863
target Thresh 32.0
target distance 20.0
model initialize at round 3863
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6.66125044, 7.79781613]), 'previousTarget': array([6.8434743 , 7.74695771]), 'currentState': array([25.7871893 ,  1.94987094,  3.17535329]), 'targetState': array([6, 8], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.3334785241076839
running average episode reward sum: 0.49011681182005845
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 8.]), 'previousTarget': array([6., 8.]), 'currentState': array([6.76687075, 8.34812453, 3.0175603 ]), 'targetState': array([6, 8], dtype=int32), 'currentDistance': 0.8421884783981064}
episode index:3864
target Thresh 32.0
target distance 22.0
model initialize at round 3864
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.07928456, 23.10292413]), 'previousTarget': array([10.09184678, 22.9793708 ]), 'currentState': array([10.91441623,  3.12036786,  1.97865534]), 'targetState': array([10, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.29440393396134923
running average episode reward sum: 0.49006617459422175
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 25.]), 'previousTarget': array([10., 25.]), 'currentState': array([10.41311457, 24.24655129,  1.53716488]), 'targetState': array([10, 25], dtype=int32), 'currentDistance': 0.8592721324530342}
episode index:3865
target Thresh 32.0
target distance 17.0
model initialize at round 3865
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  2.]), 'previousTarget': array([18.,  2.]), 'currentState': array([16.9665819 , 19.00780586,  3.14066994]), 'targetState': array([18,  2], dtype=int32), 'currentDistance': 17.039172902353386}
done in step count: 44
reward sum = 0.41519787424144283
running average episode reward sum: 0.4900468087638149
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  2.]), 'previousTarget': array([18.,  2.]), 'currentState': array([17.50131469,  2.8821426 ,  4.93758821]), 'targetState': array([18,  2], dtype=int32), 'currentDistance': 1.013342292743894}
episode index:3866
target Thresh 32.0
target distance 16.0
model initialize at round 3866
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 18.]), 'previousTarget': array([ 9., 18.]), 'currentState': array([24.88065892, 13.12259221,  2.52122673]), 'targetState': array([ 9, 18], dtype=int32), 'currentDistance': 16.612779252011652}
done in step count: 39
reward sum = 0.45301556704591156
running average episode reward sum: 0.4900372325440792
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 18.]), 'previousTarget': array([ 9., 18.]), 'currentState': array([ 9.76865688, 18.03688869,  2.93218767]), 'targetState': array([ 9, 18], dtype=int32), 'currentDistance': 0.7695415364205487}
episode index:3867
target Thresh 32.0
target distance 11.0
model initialize at round 3867
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 25.]), 'previousTarget': array([19., 25.]), 'currentState': array([ 7.95075017, 20.01957693,  2.52013516]), 'targetState': array([19, 25], dtype=int32), 'currentDistance': 12.119840578692525}
done in step count: 31
reward sum = 0.5426878443315897
running average episode reward sum: 0.4900508443878712
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 25.]), 'previousTarget': array([19., 25.]), 'currentState': array([18.01263963, 24.15291026,  0.43405738]), 'targetState': array([19, 25], dtype=int32), 'currentDistance': 1.3009387072004341}
episode index:3868
target Thresh 32.0
target distance 16.0
model initialize at round 3868
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4.40430393, 27.68449469]), 'previousTarget': array([ 4.40925592, 27.67882258]), 'currentState': array([19.00038663, 14.01136887,  1.72723043]), 'targetState': array([ 3, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.2919850304455942
running average episode reward sum: 0.4899996513628151
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 29.]), 'previousTarget': array([ 3., 29.]), 'currentState': array([ 3.47532968, 28.10504462,  2.08861384]), 'targetState': array([ 3, 29], dtype=int32), 'currentDistance': 1.0133525757152724}
episode index:3869
target Thresh 32.0
target distance 5.0
model initialize at round 3869
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([15.98231971, 10.97189881,  4.40330005]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 5.1109395242798845}
done in step count: 13
reward sum = 0.7715147234965659
running average episode reward sum: 0.49007239427551114
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([20.26902435, 10.02912082,  6.23542056]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 0.7315554822198912}
episode index:3870
target Thresh 32.0
target distance 7.0
model initialize at round 3870
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  3.]), 'previousTarget': array([13.,  3.]), 'currentState': array([20.        ,  9.        ,  2.57339546]), 'targetState': array([13,  3], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 23
reward sum = 0.6415019625885028
running average episode reward sum: 0.4901115132546672
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  3.]), 'previousTarget': array([13.,  3.]), 'currentState': array([13.82822477,  3.14356367,  3.5403911 ]), 'targetState': array([13,  3], dtype=int32), 'currentDistance': 0.8405752794960566}
episode index:3871
target Thresh 32.0
target distance 6.0
model initialize at round 3871
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 16.]), 'previousTarget': array([ 4., 16.]), 'currentState': array([9.93441606, 9.9766855 , 3.3131479 ]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 8.455626031857392}
done in step count: 17
reward sum = 0.7025789191896825
running average episode reward sum: 0.4901663860351256
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 16.]), 'previousTarget': array([ 4., 16.]), 'currentState': array([ 4.59420019, 15.07382   ,  2.18345322]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 1.1004014053988738}
episode index:3872
target Thresh 32.0
target distance 20.0
model initialize at round 3872
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.20825459, 24.75941953]), 'previousTarget': array([24.1565257 , 24.74695771]), 'currentState': array([ 5.0721834 , 18.94471582,  5.77605015]), 'targetState': array([25, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.33415509756523887
running average episode reward sum: 0.4901261042668658
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 25.]), 'previousTarget': array([25., 25.]), 'currentState': array([24.28872754, 24.55813708,  0.18091231]), 'targetState': array([25, 25], dtype=int32), 'currentDistance': 0.8373478108136857}
episode index:3873
target Thresh 32.0
target distance 2.0
model initialize at round 3873
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([10.        , 13.        ,  1.37610412]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 2.82842712474619}
done in step count: 8
reward sum = 0.8418876790458654
running average episode reward sum: 0.4902169048798702
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.64309269, 11.74478895,  5.14324229]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.8258894647153945}
episode index:3874
target Thresh 32.0
target distance 12.0
model initialize at round 3874
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 28.]), 'previousTarget': array([12., 28.]), 'currentState': array([24.03591807, 19.99229064,  0.03781211]), 'targetState': array([12, 28], dtype=int32), 'currentDistance': 14.456373439362519}
done in step count: 45
reward sum = 0.4444091124545757
running average episode reward sum: 0.49020508351408304
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 28.]), 'previousTarget': array([12., 28.]), 'currentState': array([12.7267658 , 27.72032191,  2.63364394]), 'targetState': array([12, 28], dtype=int32), 'currentDistance': 0.778722265254718}
episode index:3875
target Thresh 32.0
target distance 21.0
model initialize at round 3875
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.67441997, 22.38746452]), 'previousTarget': array([19.62476387, 22.27466942]), 'currentState': array([7.97777807, 6.16437702, 1.56332552]), 'targetState': array([23, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 71
reward sum = 0.2175052285111016
running average episode reward sum: 0.4901347275143403
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 27.]), 'previousTarget': array([23., 27.]), 'currentState': array([22.42638915, 26.06940659,  0.93566546]), 'targetState': array([23, 27], dtype=int32), 'currentDistance': 1.0931758833553185}
episode index:3876
target Thresh 32.0
target distance 11.0
model initialize at round 3876
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 14.]), 'previousTarget': array([13., 14.]), 'currentState': array([13.00674477,  2.97625385,  5.24163675]), 'targetState': array([13, 14], dtype=int32), 'currentDistance': 11.023748214933386}
done in step count: 32
reward sum = 0.5504372092391715
running average episode reward sum: 0.4901502814172871
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 14.]), 'previousTarget': array([13., 14.]), 'currentState': array([13.31192462, 13.13994551,  1.70775851]), 'targetState': array([13, 14], dtype=int32), 'currentDistance': 0.9148719544315831}
episode index:3877
target Thresh 32.0
target distance 11.0
model initialize at round 3877
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 22.]), 'previousTarget': array([22., 22.]), 'currentState': array([16.19466158, 11.32023059,  1.02223128]), 'targetState': array([22, 22], dtype=int32), 'currentDistance': 12.155633629808609}
done in step count: 31
reward sum = 0.588558645863341
running average episode reward sum: 0.4901756574782582
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 22.]), 'previousTarget': array([22., 22.]), 'currentState': array([21.39521185, 21.12458147,  1.02108865]), 'targetState': array([22, 22], dtype=int32), 'currentDistance': 1.0640142382240019}
episode index:3878
target Thresh 32.0
target distance 18.0
model initialize at round 3878
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.88854382,  9.05572809]), 'previousTarget': array([23.88854382,  9.05572809]), 'currentState': array([ 6.        , 18.        ,  2.80344719]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.2894864069396772
running average episode reward sum: 0.49012392011024103
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  9.]), 'previousTarget': array([24.,  9.]), 'currentState': array([23.09147708,  9.34872747,  5.86698619]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 0.9731519640556906}
episode index:3879
target Thresh 32.0
target distance 22.0
model initialize at round 3879
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.57265691, 21.51093912]), 'previousTarget': array([25.57265691, 21.51093912]), 'currentState': array([18.        ,  3.        ,  2.98633036]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.13522362936088464
running average episode reward sum: 0.48996274806140827
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 25.]), 'previousTarget': array([27., 25.]), 'currentState': array([24.21332687, 18.47627409,  1.08619729]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 7.093979626405423}
episode index:3880
target Thresh 32.0
target distance 8.0
model initialize at round 3880
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([ 8.00195386, 13.9984656 ,  5.41150835]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 9.999945101897524}
done in step count: 25
reward sum = 0.6270142844967176
running average episode reward sum: 0.4899980615209381
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.15482115, 6.944101  , 3.93918291]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.9567111812536129}
episode index:3881
target Thresh 32.0
target distance 25.0
model initialize at round 3881
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.21872355, 12.15879861]), 'previousTarget': array([11.21892607, 12.15457199]), 'currentState': array([21.99553763, 29.00695217,  2.39393318]), 'targetState': array([6, 4], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 84
reward sum = 0.09445847401277008
running average episode reward sum: 0.4898961708492462
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 4.]), 'previousTarget': array([6., 4.]), 'currentState': array([6.60805781, 4.85421754, 4.24215073]), 'targetState': array([6, 4], dtype=int32), 'currentDistance': 1.0485332165489767}
episode index:3882
target Thresh 32.0
target distance 17.0
model initialize at round 3882
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 20.]), 'previousTarget': array([ 4., 20.]), 'currentState': array([21.00545489, 20.02125534,  1.57208169]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 17.00546817225463}
done in step count: 42
reward sum = 0.41299782928245843
running average episode reward sum: 0.4898763670013021
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 20.]), 'previousTarget': array([ 4., 20.]), 'currentState': array([ 4.80075755, 20.15477873,  3.2008238 ]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 0.8155790030273411}
episode index:3883
target Thresh 32.0
target distance 14.0
model initialize at round 3883
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 10.]), 'previousTarget': array([27., 10.]), 'currentState': array([13.4550516 ,  8.20633487,  0.35985753]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 13.663193685601867}
done in step count: 30
reward sum = 0.5182827123895055
running average episode reward sum: 0.48988368068446075
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 10.]), 'previousTarget': array([27., 10.]), 'currentState': array([26.20040296, 10.97941862,  5.76311894]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 1.2643639726193432}
episode index:3884
target Thresh 32.0
target distance 20.0
model initialize at round 3884
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2.02495322, 5.99875234]), 'previousTarget': array([2.02495322, 5.99875234]), 'currentState': array([22.        ,  5.        ,  5.14337244]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.29771852716892233
running average episode reward sum: 0.48983421732448246
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.76457931, 6.08363069, 3.10697876]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.7691395325931425}
episode index:3885
target Thresh 32.0
target distance 9.0
model initialize at round 3885
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 25.]), 'previousTarget': array([16., 25.]), 'currentState': array([24.66383869, 23.90405233,  3.32092449]), 'targetState': array([16, 25], dtype=int32), 'currentDistance': 8.732880518064423}
done in step count: 23
reward sum = 0.6849866804384285
running average episode reward sum: 0.4898844366922421
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 25.]), 'previousTarget': array([16., 25.]), 'currentState': array([16.79781122, 25.12157118,  2.96283767]), 'targetState': array([16, 25], dtype=int32), 'currentDistance': 0.8070206271876542}
episode index:3886
target Thresh 32.0
target distance 26.0
model initialize at round 3886
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.45778872,  9.10027853]), 'previousTarget': array([15.45778872,  9.10027853]), 'currentState': array([22.        , 28.        ,  3.44383776]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 87
reward sum = 0.12299107248759666
running average episode reward sum: 0.48979004683780303
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  2.]), 'previousTarget': array([13.,  2.]), 'currentState': array([13.06458109,  2.95984594,  4.47389727]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 0.9620160886611822}
episode index:3887
target Thresh 32.0
target distance 18.0
model initialize at round 3887
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.64125695, 15.08648675]), 'previousTarget': array([23.64100589, 15.09400392]), 'currentState': array([7.04370243, 3.9275807 , 5.37284341]), 'targetState': array([25, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.292888074381668
running average episode reward sum: 0.48973940332636884
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 16.]), 'previousTarget': array([25., 16.]), 'currentState': array([24.16699226, 15.39184053,  0.69465767]), 'targetState': array([25, 16], dtype=int32), 'currentDistance': 1.0313873355283945}
episode index:3888
target Thresh 32.0
target distance 11.0
model initialize at round 3888
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 15.]), 'previousTarget': array([12., 15.]), 'currentState': array([ 7.78119656, 25.68933106,  4.26238579]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 11.491740559992389}
done in step count: 25
reward sum = 0.6005420601385607
running average episode reward sum: 0.4897678946240835
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 15.]), 'previousTarget': array([12., 15.]), 'currentState': array([11.3208756 , 15.81949071,  5.07449977]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 1.0643190156142623}
episode index:3889
target Thresh 32.0
target distance 8.0
model initialize at round 3889
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([11.        ,  4.        ,  4.16445801]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 8.0}
done in step count: 17
reward sum = 0.7047686005794648
running average episode reward sum: 0.4898231647284422
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.84660861, 3.91937852, 3.06889241]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.8504386888008011}
episode index:3890
target Thresh 32.0
target distance 10.0
model initialize at round 3890
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 12.]), 'previousTarget': array([14., 12.]), 'currentState': array([5.08866308, 1.96787914, 6.12408078]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 13.418471394786822}
done in step count: 32
reward sum = 0.5253559033809294
running average episode reward sum: 0.48983229676099227
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 12.]), 'previousTarget': array([14., 12.]), 'currentState': array([13.72623256, 11.0519825 ,  0.91303251]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 0.9867551828881073}
episode index:3891
target Thresh 32.0
target distance 9.0
model initialize at round 3891
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 23.]), 'previousTarget': array([21., 23.]), 'currentState': array([12.00010662, 19.00363326,  1.34044504]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 9.847285312093375}
done in step count: 25
reward sum = 0.6464371521014792
running average episode reward sum: 0.4898725343908331
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 23.]), 'previousTarget': array([21., 23.]), 'currentState': array([20.14151655, 22.46086503,  0.40937732]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 1.0137358364442988}
episode index:3892
target Thresh 32.0
target distance 9.0
model initialize at round 3892
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 6.38351972, 16.91078297,  6.02770772]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 10.448975438457056}
done in step count: 25
reward sum = 0.6386018898362685
running average episode reward sum: 0.48991073869482626
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.11550335, 11.24552442,  5.50322106]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.917941478440063}
episode index:3893
target Thresh 32.0
target distance 9.0
model initialize at round 3893
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 23.]), 'previousTarget': array([17., 23.]), 'currentState': array([24.08174145, 14.19712951,  1.34767476]), 'targetState': array([17, 23], dtype=int32), 'currentDistance': 11.29785779804821}
done in step count: 25
reward sum = 0.6063937065712135
running average episode reward sum: 0.48994065214317667
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 23.]), 'previousTarget': array([17., 23.]), 'currentState': array([17.84748948, 22.46191811,  2.29836757]), 'targetState': array([17, 23], dtype=int32), 'currentDistance': 1.0038777532164422}
episode index:3894
target Thresh 32.0
target distance 5.0
model initialize at round 3894
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 24.]), 'previousTarget': array([ 2., 24.]), 'currentState': array([ 7.        , 27.        ,  2.05926463]), 'targetState': array([ 2, 24], dtype=int32), 'currentDistance': 5.8309518948453}
done in step count: 14
reward sum = 0.768140988245746
running average episode reward sum: 0.49001207713319017
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 24.]), 'previousTarget': array([ 2., 24.]), 'currentState': array([ 2.89595361, 24.89935728,  3.8687534 ]), 'targetState': array([ 2, 24], dtype=int32), 'currentDistance': 1.2694787822243958}
episode index:3895
target Thresh 32.0
target distance 11.0
model initialize at round 3895
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([13.99403875,  6.97960567,  4.67264673]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 11.005980142970708}
done in step count: 29
reward sum = 0.5788996743242989
running average episode reward sum: 0.4900348922248716
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([24.18919306,  6.58471231,  0.05509824]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 0.9109729722411115}
episode index:3896
target Thresh 32.0
target distance 18.0
model initialize at round 3896
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 13.]), 'previousTarget': array([ 8., 13.]), 'currentState': array([25.99422571, 18.03297458,  1.90695798]), 'targetState': array([ 8, 13], dtype=int32), 'currentDistance': 18.68483320821065}
done in step count: 49
reward sum = 0.36111853040232
running average episode reward sum: 0.49000181130061643
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 13.]), 'previousTarget': array([ 8., 13.]), 'currentState': array([ 8.26922419, 13.90384212,  3.51908935]), 'targetState': array([ 8, 13], dtype=int32), 'currentDistance': 0.9430865474557095}
episode index:3897
target Thresh 32.0
target distance 5.0
model initialize at round 3897
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 13.]), 'previousTarget': array([19., 13.]), 'currentState': array([20.        ,  8.        ,  3.47373796]), 'targetState': array([19, 13], dtype=int32), 'currentDistance': 5.099019513592785}
done in step count: 13
reward sum = 0.7788059561051195
running average episode reward sum: 0.4900759016404842
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 13.]), 'previousTarget': array([19., 13.]), 'currentState': array([19.133045  , 12.07035998,  1.44122752]), 'targetState': array([19, 13], dtype=int32), 'currentDistance': 0.9391120996740855}
episode index:3898
target Thresh 32.0
target distance 1.0
model initialize at round 3898
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.16425573, 11.9708881 ,  6.00573681]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.281051208641436}
done in step count: 0
reward sum = 0.9972501514654363
running average episode reward sum: 0.4902059796732682
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.16425573, 11.9708881 ,  6.00573681]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.281051208641436}
episode index:3899
target Thresh 32.0
target distance 3.0
model initialize at round 3899
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([ 2.98678227, 16.01058157,  2.26760942]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 3.6107155464600686}
done in step count: 10
reward sum = 0.8295016000336441
running average episode reward sum: 0.49029297855028364
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([ 5.23915398, 17.75006962,  0.03615841]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 0.8008444704822516}
episode index:3900
target Thresh 32.0
target distance 10.0
model initialize at round 3900
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 18.]), 'previousTarget': array([12., 18.]), 'currentState': array([21.99117879, 23.01492954,  2.34406191]), 'targetState': array([12, 18], dtype=int32), 'currentDistance': 11.179140033192962}
done in step count: 29
reward sum = 0.5951303167815387
running average episode reward sum: 0.4903198530281691
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 18.]), 'previousTarget': array([12., 18.]), 'currentState': array([12.79663124, 18.90499776,  3.51519208]), 'targetState': array([12, 18], dtype=int32), 'currentDistance': 1.205670883893309}
episode index:3901
target Thresh 32.0
target distance 20.0
model initialize at round 3901
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.03320264,  9.06922922]), 'previousTarget': array([17.89976701,  9.23112767]), 'currentState': array([ 7.24243631, 25.90845017,  5.71822464]), 'targetState': array([20,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.2731734034243938
running average episode reward sum: 0.4902642029898288
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  6.]), 'previousTarget': array([20.,  6.]), 'currentState': array([19.37505366,  6.93010882,  5.37990995]), 'targetState': array([20,  6], dtype=int32), 'currentDistance': 1.1205625137354702}
episode index:3902
target Thresh 32.0
target distance 18.0
model initialize at round 3902
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([26.99114227,  1.9873754 ,  3.87566391]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 18.012626775266142}
done in step count: 49
reward sum = 0.3580041812995141
running average episode reward sum: 0.49023031623049235
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([27.21189999, 19.26916582,  1.48565508]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 0.7609337735620865}
episode index:3903
target Thresh 32.0
target distance 22.0
model initialize at round 3903
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4.99980344, 26.99976237]), 'previousTarget': array([ 5., 27.]), 'currentState': array([4.99783807, 6.99976247, 3.04903769]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.0278252588406662
running average episode reward sum: 0.490097617568845
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4.89621329, 27.91562139]), 'previousTarget': array([ 4.85685543, 27.42725009]), 'currentState': array([2.99070578, 8.0066023 , 1.63912033]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 20.0}
episode index:3904
target Thresh 32.0
target distance 3.0
model initialize at round 3904
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([ 2.92563103, 21.0161165 ,  3.18068266]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 4.306820554768155}
done in step count: 13
reward sum = 0.7592141775489601
running average episode reward sum: 0.4901665334612855
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([ 5.17354313, 17.61730401,  6.06491224]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 0.9107618687302995}
episode index:3905
target Thresh 32.0
target distance 15.0
model initialize at round 3905
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.60873928,  6.36576231]), 'previousTarget': array([19.62110536,  6.35363499]), 'currentState': array([ 4.99854688, 20.02381041,  1.37924945]), 'targetState': array([20,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.30888250842049975
running average episode reward sum: 0.4901201217805275
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  6.]), 'previousTarget': array([20.,  6.]), 'currentState': array([19.06332119,  6.75331708,  5.43930733]), 'targetState': array([20,  6], dtype=int32), 'currentDistance': 1.2020207187841774}
episode index:3906
target Thresh 32.0
target distance 5.0
model initialize at round 3906
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 21.]), 'previousTarget': array([ 4., 21.]), 'currentState': array([ 8.79862696, 19.22049411,  2.37177724]), 'targetState': array([ 4, 21], dtype=int32), 'currentDistance': 5.1179548524870135}
done in step count: 12
reward sum = 0.817830728851469
running average episode reward sum: 0.490203999591398
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 21.]), 'previousTarget': array([ 4., 21.]), 'currentState': array([ 4.79115051, 21.02246498,  2.7706812 ]), 'targetState': array([ 4, 21], dtype=int32), 'currentDistance': 0.7914693981594801}
episode index:3907
target Thresh 32.0
target distance 9.0
model initialize at round 3907
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 29.]), 'previousTarget': array([ 5., 29.]), 'currentState': array([ 7.        , 20.        ,  5.47527108]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 28
reward sum = 0.6017787725818837
running average episode reward sum: 0.49023254994272614
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 29.]), 'previousTarget': array([ 5., 29.]), 'currentState': array([ 5.60198317, 28.29818573,  1.94276575]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 0.9246226355122594}
episode index:3908
target Thresh 32.0
target distance 10.0
model initialize at round 3908
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 13.]), 'previousTarget': array([13., 13.]), 'currentState': array([2.96298269, 9.9408396 , 4.4049522 ]), 'targetState': array([13, 13], dtype=int32), 'currentDistance': 10.492863235405759}
done in step count: 26
reward sum = 0.5946978612687674
running average episode reward sum: 0.49025927424851434
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 13.]), 'previousTarget': array([13., 13.]), 'currentState': array([12.03194662, 12.14083819,  0.45952842]), 'targetState': array([13, 13], dtype=int32), 'currentDistance': 1.2943285395907156}
episode index:3909
target Thresh 32.0
target distance 14.0
model initialize at round 3909
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  6.]), 'previousTarget': array([26.,  6.]), 'currentState': array([12.32563094, 11.93057914,  6.05189284]), 'targetState': array([26,  6], dtype=int32), 'currentDistance': 14.905037338931608}
done in step count: 37
reward sum = 0.5077033125238899
running average episode reward sum: 0.49026373563937764
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  6.]), 'previousTarget': array([26.,  6.]), 'currentState': array([25.05686216,  6.15074804,  5.81202088]), 'targetState': array([26,  6], dtype=int32), 'currentDistance': 0.9551093949901172}
episode index:3910
target Thresh 32.0
target distance 8.0
model initialize at round 3910
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  4.]), 'previousTarget': array([19.,  4.]), 'currentState': array([26.        , 12.        ,  0.04307479]), 'targetState': array([19,  4], dtype=int32), 'currentDistance': 10.63014581273465}
done in step count: 28
reward sum = 0.5630786573315222
running average episode reward sum: 0.49028235361986655
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  4.]), 'previousTarget': array([19.,  4.]), 'currentState': array([18.94625027,  4.77067598,  4.07756168]), 'targetState': array([19,  4], dtype=int32), 'currentDistance': 0.7725480536899118}
episode index:3911
target Thresh 32.0
target distance 13.0
model initialize at round 3911
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 14.]), 'previousTarget': array([27., 14.]), 'currentState': array([14.00299449, 18.02046068,  1.20975876]), 'targetState': array([27, 14], dtype=int32), 'currentDistance': 13.604640980635192}
done in step count: 33
reward sum = 0.5164294155661449
running average episode reward sum: 0.49028903742915747
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 14.]), 'previousTarget': array([27., 14.]), 'currentState': array([26.03151896, 14.36355349,  6.03765173]), 'targetState': array([27, 14], dtype=int32), 'currentDistance': 1.0344692702313831}
episode index:3912
target Thresh 32.0
target distance 14.0
model initialize at round 3912
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 7.]), 'previousTarget': array([6., 7.]), 'currentState': array([16.86144346, 20.62197823,  4.37106376]), 'targetState': array([6, 7], dtype=int32), 'currentDistance': 17.422090716721634}
done in step count: 46
reward sum = 0.41582335364764633
running average episode reward sum: 0.4902700070985208
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 7.]), 'previousTarget': array([6., 7.]), 'currentState': array([6.4747989 , 7.83187596, 3.94141713]), 'targetState': array([6, 7], dtype=int32), 'currentDistance': 0.9578369448553595}
episode index:3913
target Thresh 32.0
target distance 8.0
model initialize at round 3913
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([11.79496351, 12.09623539,  2.81429049]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 8.805714082482245}
done in step count: 20
reward sum = 0.6994836063988538
running average episode reward sum: 0.490323459729921
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.98200285, 8.84443841, 3.72826017]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.2951470284490398}
episode index:3914
target Thresh 32.0
target distance 20.0
model initialize at round 3914
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5.37932857, 9.55341377]), 'previousTarget': array([5.43046618, 9.57218647]), 'currentState': array([23.94104545, 17.0007399 ,  3.23291828]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.3351287560182906
running average episode reward sum: 0.4902838186817188
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.68499088, 9.31296901, 3.51671602]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.7531016559002993}
episode index:3915
target Thresh 32.0
target distance 5.0
model initialize at round 3915
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 17.]), 'previousTarget': array([ 7., 17.]), 'currentState': array([11.04685291, 12.03879195,  0.94405383]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 6.402390476449277}
done in step count: 13
reward sum = 0.7571790204938346
running average episode reward sum: 0.4903519737383613
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 17.]), 'previousTarget': array([ 7., 17.]), 'currentState': array([ 7.98140711, 16.31591414,  2.60164287]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 1.1962998709904855}
episode index:3916
target Thresh 32.0
target distance 9.0
model initialize at round 3916
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 20.]), 'previousTarget': array([24., 20.]), 'currentState': array([27.00588844, 10.99867216,  0.03070944]), 'targetState': array([24, 20], dtype=int32), 'currentDistance': 9.489956177531784}
done in step count: 23
reward sum = 0.6253000515224769
running average episode reward sum: 0.49038642563465545
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 20.]), 'previousTarget': array([24., 20.]), 'currentState': array([24.30066822, 19.10674648,  2.00666787]), 'targetState': array([24, 20], dtype=int32), 'currentDistance': 0.9424984000068137}
episode index:3917
target Thresh 32.0
target distance 6.0
model initialize at round 3917
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 28.]), 'previousTarget': array([11., 28.]), 'currentState': array([ 5.       , 27.       ,  4.6445322]), 'targetState': array([11, 28], dtype=int32), 'currentDistance': 6.082762530298219}
done in step count: 14
reward sum = 0.7389539675780032
running average episode reward sum: 0.49044986809048585
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 28.]), 'previousTarget': array([11., 28.]), 'currentState': array([10.38880118, 27.37464563,  0.04156663]), 'targetState': array([11, 28], dtype=int32), 'currentDistance': 0.874432434401042}
episode index:3918
target Thresh 32.0
target distance 3.0
model initialize at round 3918
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 18.]), 'previousTarget': array([26., 18.]), 'currentState': array([22.94887332, 15.03173019,  2.33364987]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 4.256759291803187}
done in step count: 9
reward sum = 0.8193693567256393
running average episode reward sum: 0.4905337975338732
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 18.]), 'previousTarget': array([26., 18.]), 'currentState': array([25.33660933, 17.07400094,  0.29160636]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 1.1391055428995545}
episode index:3919
target Thresh 32.0
target distance 4.0
model initialize at round 3919
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 21.]), 'previousTarget': array([24., 21.]), 'currentState': array([20.49506802, 20.93285344,  6.07846387]), 'targetState': array([24, 21], dtype=int32), 'currentDistance': 3.505575113580365}
done in step count: 7
reward sum = 0.8813249247042179
running average episode reward sum: 0.49063348914794724
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 21.]), 'previousTarget': array([24., 21.]), 'currentState': array([23.11614171, 20.38940376,  6.18228928]), 'targetState': array([24, 21], dtype=int32), 'currentDistance': 1.074259398224469}
episode index:3920
target Thresh 32.0
target distance 4.0
model initialize at round 3920
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([23.01343398, 25.02753507,  0.8643868 ]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 5.014041821211306}
done in step count: 12
reward sum = 0.7764227763861455
running average episode reward sum: 0.49070637598478434
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([25.79974509, 21.89279267,  4.80319494]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 0.9149758389362933}
episode index:3921
target Thresh 32.0
target distance 7.0
model initialize at round 3921
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([11.01918133,  8.97479907,  5.11047888]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 8.603344249236445}
done in step count: 18
reward sum = 0.6729688910762395
running average episode reward sum: 0.49075284781423145
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.71913369, 4.55058238, 3.56219393]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.9057009594960057}
episode index:3922
target Thresh 32.0
target distance 9.0
model initialize at round 3922
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 21.]), 'previousTarget': array([ 7., 21.]), 'currentState': array([ 3.25090517, 12.04939476,  0.32143349]), 'targetState': array([ 7, 21], dtype=int32), 'currentDistance': 9.70407369327229}
done in step count: 22
reward sum = 0.6533333981136358
running average episode reward sum: 0.49079429072789427
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 21.]), 'previousTarget': array([ 7., 21.]), 'currentState': array([ 6.75350466, 20.19610165,  1.05249115]), 'targetState': array([ 7, 21], dtype=int32), 'currentDistance': 0.8408403606713968}
episode index:3923
target Thresh 32.0
target distance 19.0
model initialize at round 3923
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  8.]), 'previousTarget': array([12.,  8.]), 'currentState': array([15.16767768, 26.76003127,  5.13372907]), 'targetState': array([12,  8], dtype=int32), 'currentDistance': 19.025586853034067}
done in step count: 48
reward sum = 0.375939911509403
running average episode reward sum: 0.4907650210084196
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  8.]), 'previousTarget': array([12.,  8.]), 'currentState': array([11.89491096,  8.92904818,  4.56766736]), 'targetState': array([12,  8], dtype=int32), 'currentDistance': 0.9349728474279275}
episode index:3924
target Thresh 32.0
target distance 7.0
model initialize at round 3924
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([14.        , 15.        ,  0.20834708]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 7.0710678118654755}
done in step count: 18
reward sum = 0.6989410135555914
running average episode reward sum: 0.49081805947785845
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([12.80412247,  8.84266939,  4.38828746]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 0.8651356619545533}
episode index:3925
target Thresh 32.0
target distance 13.0
model initialize at round 3925
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([11.01951636, 18.78035955,  4.89893487]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 15.67123701102036}
done in step count: 36
reward sum = 0.4659109350708322
running average episode reward sum: 0.49081171533002166
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([23.29850978,  9.94582822,  5.82149779]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 0.7035787894208795}
episode index:3926
target Thresh 32.0
target distance 21.0
model initialize at round 3926
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.69573782, 11.24940521]), 'previousTarget': array([23.64677133, 11.25775784]), 'currentState': array([ 4.05166889, 15.00580721,  6.28059522]), 'targetState': array([25, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.3323330008070632
running average episode reward sum: 0.4907713591511261
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 11.]), 'previousTarget': array([25., 11.]), 'currentState': array([24.00818588, 11.11801536,  6.28179365]), 'targetState': array([25, 11], dtype=int32), 'currentDistance': 0.9988107244850608}
episode index:3927
target Thresh 32.0
target distance 10.0
model initialize at round 3927
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 15.]), 'previousTarget': array([21., 15.]), 'currentState': array([21.07146201,  4.97493132,  6.19830036]), 'targetState': array([21, 15], dtype=int32), 'currentDistance': 10.025323381775916}
done in step count: 23
reward sum = 0.6096361224927336
running average episode reward sum: 0.4908016200379238
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 15.]), 'previousTarget': array([21., 15.]), 'currentState': array([21.09596723, 14.30694188,  1.53259884]), 'targetState': array([21, 15], dtype=int32), 'currentDistance': 0.6996708226638063}
episode index:3928
target Thresh 32.0
target distance 22.0
model initialize at round 3928
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.27605889, 24.20732955]), 'previousTarget': array([16.27605889, 24.20732955]), 'currentState': array([8.        , 6.        , 2.00049028]), 'targetState': array([18, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.2501450211447789
running average episode reward sum: 0.4907403686765359
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 28.]), 'previousTarget': array([18., 28.]), 'currentState': array([17.92178111, 27.22739521,  1.14106394]), 'targetState': array([18, 28], dtype=int32), 'currentDistance': 0.7765541557324654}
episode index:3929
target Thresh 32.0
target distance 15.0
model initialize at round 3929
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 13.]), 'previousTarget': array([17., 13.]), 'currentState': array([14.00518118, 28.00573685,  0.58374822]), 'targetState': array([17, 13], dtype=int32), 'currentDistance': 15.30166912582526}
done in step count: 42
reward sum = 0.4422445651133203
running average episode reward sum: 0.4907280287774104
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 13.]), 'previousTarget': array([17., 13.]), 'currentState': array([17.20505959, 13.83804115,  4.63787621]), 'targetState': array([17, 13], dtype=int32), 'currentDistance': 0.8627643917958667}
episode index:3930
target Thresh 32.0
target distance 13.0
model initialize at round 3930
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  2.]), 'previousTarget': array([20.,  2.]), 'currentState': array([ 7.29481795, 12.90881476,  5.89078397]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 16.745861886314916}
done in step count: 37
reward sum = 0.4559631660678287
running average episode reward sum: 0.49071918500668804
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  2.]), 'previousTarget': array([20.,  2.]), 'currentState': array([19.09912078,  2.86767258,  5.51748383]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 1.2507753877234888}
episode index:3931
target Thresh 32.0
target distance 20.0
model initialize at round 3931
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 8.]), 'previousTarget': array([5., 8.]), 'currentState': array([25.        ,  8.        ,  0.92760357]), 'targetState': array([5, 8], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.315057011627604
running average episode reward sum: 0.49067450998802603
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 8.]), 'previousTarget': array([5., 8.]), 'currentState': array([5.7371115 , 8.30732011, 3.17058942]), 'targetState': array([5, 8], dtype=int32), 'currentDistance': 0.798610674796919}
episode index:3932
target Thresh 32.0
target distance 8.0
model initialize at round 3932
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([7.13889038, 2.20580989, 1.04429162]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 7.795427502723125}
done in step count: 17
reward sum = 0.7279903575386042
running average episode reward sum: 0.49073484963906866
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([6.86473333, 9.03842708, 1.46259724]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.9710404508031617}
episode index:3933
target Thresh 32.0
target distance 14.0
model initialize at round 3933
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 22.]), 'previousTarget': array([ 8., 22.]), 'currentState': array([16.        ,  8.        ,  3.48373571]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 16.124515496597102}
done in step count: 43
reward sum = 0.43305847215377113
running average episode reward sum: 0.49072018863818273
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 22.]), 'previousTarget': array([ 8., 22.]), 'currentState': array([ 7.9996162 , 21.19434596,  1.67494248]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 0.8056541313135215}
episode index:3934
target Thresh 32.0
target distance 7.0
model initialize at round 3934
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([15.20475757, 28.86557891,  5.52758896]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 6.911482067136795}
done in step count: 14
reward sum = 0.7490473207886861
running average episode reward sum: 0.4907858372105208
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([16.19625904, 22.86276714,  4.48138226]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 0.8848077477832587}
episode index:3935
target Thresh 32.0
target distance 22.0
model initialize at round 3935
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.17458624,  6.78146944]), 'previousTarget': array([13.17458624,  6.78146944]), 'currentState': array([ 3.        , 24.        ,  1.19000456]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.18939047944104725
running average episode reward sum: 0.4907092631866973
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.66879557,  2.91452722,  4.95299582]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.9726543057905773}
episode index:3936
target Thresh 32.0
target distance 10.0
model initialize at round 3936
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 4.]), 'previousTarget': array([8., 4.]), 'currentState': array([13.93286694, 13.6916112 ,  4.34997147]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 11.363372642854737}
done in step count: 24
reward sum = 0.618383794567785
running average episode reward sum: 0.49074169258252687
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 4.]), 'previousTarget': array([8., 4.]), 'currentState': array([8.7418943 , 4.81758605, 4.01986583]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 1.1040172598577367}
episode index:3937
target Thresh 32.0
target distance 16.0
model initialize at round 3937
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 25.]), 'previousTarget': array([ 3., 25.]), 'currentState': array([19.        , 23.        ,  1.45967034]), 'targetState': array([ 3, 25], dtype=int32), 'currentDistance': 16.1245154965971}
done in step count: 42
reward sum = 0.43768762379821613
running average episode reward sum: 0.4907282202440849
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 25.]), 'previousTarget': array([ 3., 25.]), 'currentState': array([ 3.89045052, 24.84939735,  2.84614845]), 'targetState': array([ 3, 25], dtype=int32), 'currentDistance': 0.9030964977953995}
episode index:3938
target Thresh 32.0
target distance 19.0
model initialize at round 3938
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 13.]), 'previousTarget': array([ 6., 13.]), 'currentState': array([24.57551084, 10.76411864,  3.46733141]), 'targetState': array([ 6, 13], dtype=int32), 'currentDistance': 18.70959028111173}
done in step count: 45
reward sum = 0.38828890433510976
running average episode reward sum: 0.49070221381709606
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 13.]), 'previousTarget': array([ 6., 13.]), 'currentState': array([ 6.83017156, 12.87477341,  2.91359853]), 'targetState': array([ 6, 13], dtype=int32), 'currentDistance': 0.8395632886160403}
episode index:3939
target Thresh 32.0
target distance 24.0
model initialize at round 3939
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.3390904 ,  8.06908484]), 'previousTarget': array([23.3390904 ,  8.06908484]), 'currentState': array([25.       , 28.       ,  0.4499602]), 'targetState': array([23,  4], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 67
reward sum = 0.2251243661817911
running average episode reward sum: 0.49063480827201095
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  4.]), 'previousTarget': array([23.,  4.]), 'currentState': array([23.03049978,  4.95853712,  4.55415506]), 'targetState': array([23,  4], dtype=int32), 'currentDistance': 0.9590222337384822}
episode index:3940
target Thresh 32.0
target distance 19.0
model initialize at round 3940
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.60711423, 20.69765531]), 'previousTarget': array([10.60711423, 20.69765531]), 'currentState': array([23.        ,  5.        ,  6.19024229]), 'targetState': array([ 8, 24], dtype=int32), 'currentDistance': 20.0}
done in step count: 72
reward sum = 0.21921464029274668
running average episode reward sum: 0.4905659373844242
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 24.]), 'previousTarget': array([ 8., 24.]), 'currentState': array([ 8.79403858, 23.21814449,  2.33922419]), 'targetState': array([ 8, 24], dtype=int32), 'currentDistance': 1.1143586968689128}
episode index:3941
target Thresh 32.0
target distance 6.0
model initialize at round 3941
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 15.]), 'previousTarget': array([10., 15.]), 'currentState': array([ 4.        , 15.        ,  5.19300127]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 6.0}
done in step count: 14
reward sum = 0.7642829046189286
running average episode reward sum: 0.4906353734491717
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 15.]), 'previousTarget': array([10., 15.]), 'currentState': array([ 9.21138448, 14.94486418,  6.21816348]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 0.7905405778379841}
episode index:3942
target Thresh 32.0
target distance 6.0
model initialize at round 3942
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 19.]), 'previousTarget': array([20., 19.]), 'currentState': array([26.        , 19.        ,  5.14225143]), 'targetState': array([20, 19], dtype=int32), 'currentDistance': 6.0}
done in step count: 15
reward sum = 0.7342850089659748
running average episode reward sum: 0.49069716640771005
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 19.]), 'previousTarget': array([20., 19.]), 'currentState': array([20.77472226, 18.60074032,  2.79392007]), 'targetState': array([20, 19], dtype=int32), 'currentDistance': 0.8715519907437077}
episode index:3943
target Thresh 32.0
target distance 14.0
model initialize at round 3943
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 16.]), 'previousTarget': array([ 3., 16.]), 'currentState': array([11.        ,  2.        ,  3.73717225]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 16.1245154965971}
done in step count: 41
reward sum = 0.44645571268158557
running average episode reward sum: 0.4906859490005787
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 16.]), 'previousTarget': array([ 3., 16.]), 'currentState': array([ 3.35285908, 15.04155049,  1.87617415]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 1.0213398022754845}
episode index:3944
target Thresh 32.0
target distance 18.0
model initialize at round 3944
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([25.98523169, 15.05885281,  1.8637494 ]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 18.08968245812112}
done in step count: 64
reward sum = 0.34197251344540214
running average episode reward sum: 0.4906482523122251
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([ 8.82868864, 16.70332795,  2.9758727 ]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 0.8801926841659657}
episode index:3945
target Thresh 32.0
target distance 13.0
model initialize at round 3945
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 21.]), 'previousTarget': array([23., 21.]), 'currentState': array([15.9225128 ,  8.49249174,  1.59357417]), 'targetState': array([23, 21], dtype=int32), 'currentDistance': 14.371102528442663}
done in step count: 33
reward sum = 0.5076499611427262
running average episode reward sum: 0.4906525609054411
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 21.]), 'previousTarget': array([23., 21.]), 'currentState': array([22.745686  , 20.08235091,  1.08513738]), 'targetState': array([23, 21], dtype=int32), 'currentDistance': 0.9522370852719678}
episode index:3946
target Thresh 32.0
target distance 10.0
model initialize at round 3946
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 13.]), 'previousTarget': array([10., 13.]), 'currentState': array([ 3.9999778 , 22.866989  ,  4.78370005]), 'targetState': array([10, 13], dtype=int32), 'currentDistance': 11.548062103143971}
done in step count: 25
reward sum = 0.6184380723274266
running average episode reward sum: 0.49068493625670084
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 13.]), 'previousTarget': array([10., 13.]), 'currentState': array([ 9.15100943, 13.90949515,  5.39782606]), 'targetState': array([10, 13], dtype=int32), 'currentDistance': 1.244172983552425}
episode index:3947
target Thresh 32.0
target distance 17.0
model initialize at round 3947
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.23730194, 10.49633137]), 'previousTarget': array([13.28585494, 10.56139529]), 'currentState': array([25.98228934, 25.90948472,  4.38012445]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.3171175674940454
running average episode reward sum: 0.4906409728907528
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([12.41081903,  9.72346756,  4.07171889]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.83197210702489}
episode index:3948
target Thresh 32.0
target distance 9.0
model initialize at round 3948
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  5.]), 'previousTarget': array([11.,  5.]), 'currentState': array([ 2.3814734 , 11.99725892,  6.12428332]), 'targetState': array([11,  5], dtype=int32), 'currentDistance': 11.101379785438784}
done in step count: 21
reward sum = 0.6229092766087649
running average episode reward sum: 0.4906744670167893
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  5.]), 'previousTarget': array([11.,  5.]), 'currentState': array([10.05945096,  5.63724041,  5.54100884]), 'targetState': array([11,  5], dtype=int32), 'currentDistance': 1.136093236850886}
episode index:3949
target Thresh 32.0
target distance 16.0
model initialize at round 3949
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7.94846611, 14.82990784]), 'previousTarget': array([ 7.94846611, 14.82990784]), 'currentState': array([23.        , 28.        ,  5.75744724]), 'targetState': array([ 7, 14], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 57
reward sum = 0.2962860300757375
running average episode reward sum: 0.49062525475427254
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 14.]), 'previousTarget': array([ 7., 14.]), 'currentState': array([ 7.57355255, 14.88550891,  3.9950979 ]), 'targetState': array([ 7, 14], dtype=int32), 'currentDistance': 1.0550301173272403}
episode index:3950
target Thresh 32.0
target distance 19.0
model initialize at round 3950
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.69965682, 27.31817799]), 'previousTarget': array([23.69836445, 27.31492866]), 'currentState': array([ 5.98689015, 18.03066503,  1.86875504]), 'targetState': array([25, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.30152460411996984
running average episode reward sum: 0.49057739328866024
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 28.]), 'previousTarget': array([25., 28.]), 'currentState': array([24.60697804, 27.15535536,  0.60624139]), 'targetState': array([25, 28], dtype=int32), 'currentDistance': 0.931606585108909}
episode index:3951
target Thresh 32.0
target distance 13.0
model initialize at round 3951
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 28.]), 'previousTarget': array([ 2., 28.]), 'currentState': array([ 2.09562296, 15.21630541,  1.21155468]), 'targetState': array([ 2, 28], dtype=int32), 'currentDistance': 12.784052222445366}
done in step count: 32
reward sum = 0.5598855725935716
running average episode reward sum: 0.49059493078342364
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 28.]), 'previousTarget': array([ 2., 28.]), 'currentState': array([ 1.71208411, 27.16451507,  1.37353297]), 'targetState': array([ 2, 28], dtype=int32), 'currentDistance': 0.8837027966087759}
episode index:3952
target Thresh 32.0
target distance 11.0
model initialize at round 3952
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([ 9.30799272, 20.97662927,  6.07163084]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 13.339626463947118}
done in step count: 32
reward sum = 0.5398445874525353
running average episode reward sum: 0.49060738958855116
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([19.18294085, 13.19262062,  5.53939944]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 0.8394571813102727}
episode index:3953
target Thresh 32.0
target distance 18.0
model initialize at round 3953
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 20.]), 'previousTarget': array([ 2., 20.]), 'currentState': array([19.80709819, 13.14205741,  2.51621965]), 'targetState': array([ 2, 20], dtype=int32), 'currentDistance': 19.082036642656117}
done in step count: 48
reward sum = 0.3896383250512226
running average episode reward sum: 0.4905818536592296
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 20.]), 'previousTarget': array([ 2., 20.]), 'currentState': array([ 2.95007624, 19.91803038,  3.00519061]), 'targetState': array([ 2, 20], dtype=int32), 'currentDistance': 0.9536057222369086}
episode index:3954
target Thresh 32.0
target distance 5.0
model initialize at round 3954
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 20.]), 'previousTarget': array([20., 20.]), 'currentState': array([19.05054899, 25.03879359,  0.40562132]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 5.127465073007193}
done in step count: 13
reward sum = 0.7718415956699652
running average episode reward sum: 0.49065296863824626
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 20.]), 'previousTarget': array([20., 20.]), 'currentState': array([20.23891234, 20.83443909,  4.59837595]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 0.867967573586982}
episode index:3955
target Thresh 32.0
target distance 19.0
model initialize at round 3955
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.38878158, 12.29958622]), 'previousTarget': array([20.39288577, 12.30234469]), 'currentState': array([ 7.97751034, 27.98270919,  3.96342525]), 'targetState': array([23,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.24413306959671832
running average episode reward sum: 0.4905906531935947
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  9.]), 'previousTarget': array([23.,  9.]), 'currentState': array([22.56012026,  9.75881826,  5.12964343]), 'targetState': array([23,  9], dtype=int32), 'currentDistance': 0.8770971079045944}
episode index:3956
target Thresh 32.0
target distance 11.0
model initialize at round 3956
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([22.        , 19.        ,  5.66224664]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 12.529964086141668}
done in step count: 30
reward sum = 0.5435968870035831
running average episode reward sum: 0.490604048754325
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.27443358,  8.83941333,  4.07817838]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.8831356261239669}
episode index:3957
target Thresh 32.0
target distance 4.0
model initialize at round 3957
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 22.]), 'previousTarget': array([27., 22.]), 'currentState': array([24.07661827, 18.44224922,  1.34088746]), 'targetState': array([27, 22], dtype=int32), 'currentDistance': 4.604753126281243}
done in step count: 8
reward sum = 0.8460191891039017
running average episode reward sum: 0.49069384540423644
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 22.]), 'previousTarget': array([27., 22.]), 'currentState': array([26.10238936, 21.30999002,  0.41651005]), 'targetState': array([27, 22], dtype=int32), 'currentDistance': 1.1321742927862868}
episode index:3958
target Thresh 32.0
target distance 13.0
model initialize at round 3958
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 22.]), 'previousTarget': array([23., 22.]), 'currentState': array([27.07139034,  9.05909579,  0.8140063 ]), 'targetState': array([23, 22], dtype=int32), 'currentDistance': 13.566253025601268}
done in step count: 33
reward sum = 0.5316662919923061
running average episode reward sum: 0.4907041945950897
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 22.]), 'previousTarget': array([23., 22.]), 'currentState': array([23.54969789, 21.12047462,  1.93340129]), 'targetState': array([23, 22], dtype=int32), 'currentDistance': 1.0371753315768364}
episode index:3959
target Thresh 32.0
target distance 18.0
model initialize at round 3959
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.77824354,  5.72678488]), 'previousTarget': array([18.78704435,  5.72118773]), 'currentState': array([ 2.98690178, 18.0000699 ,  3.36440173]), 'targetState': array([21,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 70
reward sum = 0.23701828554437104
running average episode reward sum: 0.49064013249684457
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  4.]), 'previousTarget': array([21.,  4.]), 'currentState': array([20.2155351 ,  4.06758714,  5.65884861]), 'targetState': array([21,  4], dtype=int32), 'currentDistance': 0.7873710724831393}
episode index:3960
target Thresh 32.0
target distance 19.0
model initialize at round 3960
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.67985983,  4.09022194]), 'previousTarget': array([20.67985983,  4.09022194]), 'currentState': array([10.        , 21.        ,  6.06526403]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.28524232948432876
running average episode reward sum: 0.49058827745947714
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  2.]), 'previousTarget': array([22.,  2.]), 'currentState': array([21.16268206,  2.4620502 ,  5.23158975]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 0.9563428907371859}
episode index:3961
target Thresh 32.0
target distance 19.0
model initialize at round 3961
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  8.]), 'previousTarget': array([11.,  8.]), 'currentState': array([13.05334648, 26.99867311,  6.04013108]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 19.109312168020175}
done in step count: 46
reward sum = 0.36106095675315514
running average episode reward sum: 0.49055558505142405
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  8.]), 'previousTarget': array([11.,  8.]), 'currentState': array([11.21985297,  8.92005377,  4.52196317]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 0.9459567986230463}
episode index:3962
target Thresh 32.0
target distance 21.0
model initialize at round 3962
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4.9491083 , 8.05786148]), 'previousTarget': array([4.95130299, 8.02263725]), 'currentState': array([ 3.98805779, 28.03475768,  2.15424454]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.27295781000893965
running average episode reward sum: 0.4905006777147996
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 7.]), 'previousTarget': array([5., 7.]), 'currentState': array([4.87718495, 7.67172294, 4.81482504]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 0.6828581465820645}
episode index:3963
target Thresh 32.0
target distance 23.0
model initialize at round 3963
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.58901132, 21.62762278]), 'previousTarget': array([15.5731765, 21.7042351]), 'currentState': array([19.03007239,  1.92586908,  5.35027027]), 'targetState': array([15, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 68
reward sum = 0.22277739367371996
running average episode reward sum: 0.4904331390457681
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 25.]), 'previousTarget': array([15., 25.]), 'currentState': array([15.60739935, 24.17241895,  1.7264913 ]), 'targetState': array([15, 25], dtype=int32), 'currentDistance': 1.0265594854482596}
episode index:3964
target Thresh 32.0
target distance 23.0
model initialize at round 3964
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.13139909, 24.98364298]), 'previousTarget': array([10.13125551, 24.98112317]), 'currentState': array([11.00181723,  5.00259265,  1.05898678]), 'targetState': array([10, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.27741946334842144
running average episode reward sum: 0.4903794155462227
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 28.]), 'previousTarget': array([10., 28.]), 'currentState': array([ 9.85951735, 27.09120052,  1.37835471]), 'targetState': array([10, 28], dtype=int32), 'currentDistance': 0.9195933213620474}
episode index:3965
target Thresh 32.0
target distance 8.0
model initialize at round 3965
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 20.]), 'previousTarget': array([15., 20.]), 'currentState': array([ 7.02015645, 19.93045568,  5.23834234]), 'targetState': array([15, 20], dtype=int32), 'currentDistance': 7.980146588327398}
done in step count: 16
reward sum = 0.7002761719758432
running average episode reward sum: 0.49043233958969973
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 20.]), 'previousTarget': array([15., 20.]), 'currentState': array([14.20236042, 20.0047408 ,  6.19063653]), 'targetState': array([15, 20], dtype=int32), 'currentDistance': 0.7976536721026303}
episode index:3966
target Thresh 32.0
target distance 19.0
model initialize at round 3966
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.30245047, 16.86292184]), 'previousTarget': array([22.29822396, 16.88271492]), 'currentState': array([7.05178638, 3.92394024, 5.50684035]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.21581586895824306
running average episode reward sum: 0.49036311436392926
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([25.61207584, 19.11652462,  0.82313554]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 0.9648906149849894}
episode index:3967
target Thresh 32.0
target distance 15.0
model initialize at round 3967
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([26.        , 22.        ,  2.24441296]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 19.209372712298542}
done in step count: 51
reward sum = 0.3626118214157765
running average episode reward sum: 0.4903309189776016
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([11.7856697 , 10.92670764,  4.01822147]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 1.214933712475778}
episode index:3968
target Thresh 32.0
target distance 15.0
model initialize at round 3968
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([23.        , 19.        ,  0.06087676]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 17.4928556845359}
done in step count: 46
reward sum = 0.38985448358804
running average episode reward sum: 0.4903056036751603
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.71439529,  4.85648381,  4.05443996]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.1153139300393629}
episode index:3969
target Thresh 32.0
target distance 4.0
model initialize at round 3969
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([25.13251959, 20.86376262,  5.32343644]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 3.866034536026768}
done in step count: 7
reward sum = 0.8620671712666061
running average episode reward sum: 0.49039924638739996
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([25.44777605, 17.7296398 ,  4.37364019]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 0.8560827255910218}
episode index:3970
target Thresh 32.0
target distance 12.0
model initialize at round 3970
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 13.]), 'previousTarget': array([17., 13.]), 'currentState': array([ 4.97237516, 11.93632562,  4.5555501 ]), 'targetState': array([17, 13], dtype=int32), 'currentDistance': 12.074566760530612}
done in step count: 28
reward sum = 0.5481622316838797
running average episode reward sum: 0.49041379259371987
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 13.]), 'previousTarget': array([17., 13.]), 'currentState': array([16.10180666, 12.68429962,  0.18666727]), 'targetState': array([17, 13], dtype=int32), 'currentDistance': 0.9520598745272899}
episode index:3971
target Thresh 32.0
target distance 7.0
model initialize at round 3971
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  7.]), 'previousTarget': array([11.,  7.]), 'currentState': array([ 8.87015631, 14.05360854,  3.00254107]), 'targetState': array([11,  7], dtype=int32), 'currentDistance': 7.3681495386995595}
done in step count: 18
reward sum = 0.6861540292416977
running average episode reward sum: 0.4904630726130169
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  7.]), 'previousTarget': array([11.,  7.]), 'currentState': array([10.65010609,  7.80541817,  4.9706691 ]), 'targetState': array([11,  7], dtype=int32), 'currentDistance': 0.8781367643966859}
episode index:3972
target Thresh 32.0
target distance 9.0
model initialize at round 3972
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 21.]), 'previousTarget': array([20., 21.]), 'currentState': array([11.94697512, 12.0251708 ,  2.46442318]), 'targetState': array([20, 21], dtype=int32), 'currentDistance': 12.058141183018193}
done in step count: 29
reward sum = 0.5543195481084315
running average episode reward sum: 0.4904791452220014
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 21.]), 'previousTarget': array([20., 21.]), 'currentState': array([19.42556548, 20.02226506,  0.83894945]), 'targetState': array([20, 21], dtype=int32), 'currentDistance': 1.133993228141472}
episode index:3973
target Thresh 32.0
target distance 9.0
model initialize at round 3973
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([20.88992611, 18.9425848 ,  3.69191563]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 10.192212728057287}
done in step count: 24
reward sum = 0.6419580248233314
running average episode reward sum: 0.49051726270554474
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.15349798, 10.7880773 ,  4.4130994 ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.8028869496365525}
episode index:3974
target Thresh 32.0
target distance 21.0
model initialize at round 3974
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9.49442256, 27.23047895]), 'previousTarget': array([ 9.49442256, 27.23047895]), 'currentState': array([4.        , 8.        , 5.43921077]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.26804348243087617
running average episode reward sum: 0.49046129445893477
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([10.36017424, 28.04809117,  1.48485043]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 1.0177700605528779}
episode index:3975
target Thresh 32.0
target distance 8.0
model initialize at round 3975
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 26.]), 'previousTarget': array([ 6., 26.]), 'currentState': array([10.93852004, 18.31287686,  1.8082038 ]), 'targetState': array([ 6, 26], dtype=int32), 'currentDistance': 9.136785121860319}
done in step count: 20
reward sum = 0.6975747507792172
running average episode reward sum: 0.49051338536847205
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 26.]), 'previousTarget': array([ 6., 26.]), 'currentState': array([ 6.70203251, 25.00370982,  2.25458646]), 'targetState': array([ 6, 26], dtype=int32), 'currentDistance': 1.2187878278325552}
episode index:3976
target Thresh 32.0
target distance 12.0
model initialize at round 3976
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 15.]), 'previousTarget': array([21., 15.]), 'currentState': array([23.86225493,  3.07033047,  2.53556052]), 'targetState': array([21, 15], dtype=int32), 'currentDistance': 12.268232080162823}
done in step count: 30
reward sum = 0.5780855860041069
running average episode reward sum: 0.4905354050316945
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 15.]), 'previousTarget': array([21., 15.]), 'currentState': array([21.46314121, 14.02472053,  1.82388687]), 'targetState': array([21, 15], dtype=int32), 'currentDistance': 1.0796619069144784}
episode index:3977
target Thresh 32.0
target distance 12.0
model initialize at round 3977
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([19.62001124,  9.03031354,  2.90246841]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 14.090442278013343}
done in step count: 35
reward sum = 0.5142122159051707
running average episode reward sum: 0.490541356970074
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([ 8.86268041, 17.02966166,  2.62766717]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 0.8631901900687248}
episode index:3978
target Thresh 32.0
target distance 20.0
model initialize at round 3978
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.74695771,  5.8434743 ]), 'previousTarget': array([25.74695771,  5.8434743 ]), 'currentState': array([20.        , 25.        ,  5.70502752]), 'targetState': array([26,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.32971076613234773
running average episode reward sum: 0.4905009371181419
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  5.]), 'previousTarget': array([26.,  5.]), 'currentState': array([25.15990951,  5.61487755,  5.25778558]), 'targetState': array([26,  5], dtype=int32), 'currentDistance': 1.0410698542448706}
episode index:3979
target Thresh 32.0
target distance 8.0
model initialize at round 3979
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 29.]), 'previousTarget': array([ 4., 29.]), 'currentState': array([ 4.        , 21.        ,  4.50367811]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 7.999999999999999}
done in step count: 21
reward sum = 0.6442058877553901
running average episode reward sum: 0.4905395564524729
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 29.]), 'previousTarget': array([ 4., 29.]), 'currentState': array([ 3.85387993, 28.1531847 ,  1.55906647]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 0.8593295228772372}
episode index:3980
target Thresh 32.0
target distance 22.0
model initialize at round 3980
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2.0028443 , 23.94812505]), 'previousTarget': array([ 2., 24.]), 'currentState': array([2.03056821, 3.94814427, 5.49754143]), 'targetState': array([ 2, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.249211674053467
running average episode reward sum: 0.4904789365372759
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 26.]), 'previousTarget': array([ 2., 26.]), 'currentState': array([ 1.99720032, 25.23484709,  1.65436474]), 'targetState': array([ 2, 26], dtype=int32), 'currentDistance': 0.7651580319810746}
episode index:3981
target Thresh 32.0
target distance 17.0
model initialize at round 3981
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 25.]), 'previousTarget': array([14., 25.]), 'currentState': array([14.88513232,  8.37504383,  1.80637707]), 'targetState': array([14, 25], dtype=int32), 'currentDistance': 16.648502236398723}
done in step count: 38
reward sum = 0.4540937665595625
running average episode reward sum: 0.4904697991264327
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 25.]), 'previousTarget': array([14., 25.]), 'currentState': array([14.28662918, 24.18367366,  1.57407472]), 'targetState': array([14, 25], dtype=int32), 'currentDistance': 0.865184939645611}
episode index:3982
target Thresh 32.0
target distance 14.0
model initialize at round 3982
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 29.]), 'previousTarget': array([19., 29.]), 'currentState': array([12.01103437, 14.9890587 ,  5.74869385]), 'targetState': array([19, 29], dtype=int32), 'currentDistance': 15.657334276415703}
done in step count: 43
reward sum = 0.4428448544281964
running average episode reward sum: 0.4904578420727801
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 29.]), 'previousTarget': array([19., 29.]), 'currentState': array([18.73016468, 28.0200454 ,  1.00452315]), 'targetState': array([19, 29], dtype=int32), 'currentDistance': 1.0164261459599855}
episode index:3983
target Thresh 32.0
target distance 22.0
model initialize at round 3983
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 23.]), 'previousTarget': array([13., 23.]), 'currentState': array([13.        ,  3.        ,  3.91611862]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.25837780968419805
running average episode reward sum: 0.4903995890526022
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([13.07285365, 24.17866957,  1.53358532]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 0.8245552327312314}
episode index:3984
target Thresh 32.0
target distance 18.0
model initialize at round 3984
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 25.]), 'previousTarget': array([ 5., 25.]), 'currentState': array([12.90784027,  7.33793923,  1.87304942]), 'targetState': array([ 5, 25], dtype=int32), 'currentDistance': 19.351545891233172}
done in step count: 47
reward sum = 0.3814664238039775
running average episode reward sum: 0.4903722532520379
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 25.]), 'previousTarget': array([ 5., 25.]), 'currentState': array([ 5.9188118 , 24.27590222,  2.3928764 ]), 'targetState': array([ 5, 25], dtype=int32), 'currentDistance': 1.1698430307093228}
episode index:3985
target Thresh 32.0
target distance 10.0
model initialize at round 3985
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 14.]), 'previousTarget': array([18., 14.]), 'currentState': array([16.12386396,  4.34781424,  1.20791623]), 'targetState': array([18, 14], dtype=int32), 'currentDistance': 9.832831552027901}
done in step count: 23
reward sum = 0.6610526612864108
running average episode reward sum: 0.49041507322394823
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 14.]), 'previousTarget': array([18., 14.]), 'currentState': array([17.8057705 , 13.03555193,  1.42546303]), 'targetState': array([18, 14], dtype=int32), 'currentDistance': 0.9838115593938336}
episode index:3986
target Thresh 32.0
target distance 22.0
model initialize at round 3986
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.28951719, 21.84170742]), 'previousTarget': array([15.26249016, 21.87322975]), 'currentState': array([26.04323389,  4.97880207,  6.07982111]), 'targetState': array([12, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 76
reward sum = 0.17370509601078632
running average episode reward sum: 0.49033563756374926
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 27.]), 'previousTarget': array([12., 27.]), 'currentState': array([12.85193081, 26.10844507,  2.4291964 ]), 'targetState': array([12, 27], dtype=int32), 'currentDistance': 1.233148936986151}
episode index:3987
target Thresh 32.0
target distance 15.0
model initialize at round 3987
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 26.]), 'previousTarget': array([ 7., 26.]), 'currentState': array([14.00051317, 10.99087688,  4.53608155]), 'targetState': array([ 7, 26], dtype=int32), 'currentDistance': 16.56142993832531}
done in step count: 45
reward sum = 0.3911628844271104
running average episode reward sum: 0.49031076977209015
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 26.]), 'previousTarget': array([ 7., 26.]), 'currentState': array([ 7.85171319, 25.40846604,  2.22742141]), 'targetState': array([ 7, 26], dtype=int32), 'currentDistance': 1.036980125854135}
episode index:3988
target Thresh 32.0
target distance 21.0
model initialize at round 3988
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.51118562,  8.89720523]), 'previousTarget': array([12.71986011,  9.28336929]), 'currentState': array([21.76058263, 26.62990539,  4.15025919]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.2839613993163949
running average episode reward sum: 0.4902590401730789
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  6.]), 'previousTarget': array([11.,  6.]), 'currentState': array([11.3352652 ,  6.88727048,  4.22022406]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 0.9484996887519628}
episode index:3989
target Thresh 32.0
target distance 18.0
model initialize at round 3989
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.06261825, 25.42632466]), 'previousTarget': array([23.06563667, 25.42900019]), 'currentState': array([ 6.00371352, 14.98631655,  4.99155365]), 'targetState': array([24, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.30414904945253346
running average episode reward sum: 0.4902123960651289
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 26.]), 'previousTarget': array([24., 26.]), 'currentState': array([23.07004578, 25.24444781,  0.73965923]), 'targetState': array([24, 26], dtype=int32), 'currentDistance': 1.1981961266601338}
episode index:3990
target Thresh 32.0
target distance 7.0
model initialize at round 3990
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 29.]), 'previousTarget': array([14., 29.]), 'currentState': array([21.        , 29.        ,  4.89958942]), 'targetState': array([14, 29], dtype=int32), 'currentDistance': 6.999999999999999}
done in step count: 17
reward sum = 0.7099954931866144
running average episode reward sum: 0.4902674657461917
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 29.]), 'previousTarget': array([14., 29.]), 'currentState': array([14.88656162, 29.03910813,  3.01451082]), 'targetState': array([14, 29], dtype=int32), 'currentDistance': 0.8874237774457863}
episode index:3991
target Thresh 32.0
target distance 2.0
model initialize at round 3991
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 22.]), 'previousTarget': array([18., 22.]), 'currentState': array([19.        , 20.        ,  5.56114569]), 'targetState': array([18, 22], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 15
reward sum = 0.8053778660961861
running average episode reward sum: 0.49034640121722123
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 22.]), 'previousTarget': array([18., 22.]), 'currentState': array([18.97022491, 22.04072699,  2.32676662]), 'targetState': array([18, 22], dtype=int32), 'currentDistance': 0.9710793289377706}
episode index:3992
target Thresh 32.0
target distance 21.0
model initialize at round 3992
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.90599608,  6.35899411]), 'previousTarget': array([14.90599608,  6.35899411]), 'currentState': array([26.        , 23.        ,  5.96468728]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 71
reward sum = 0.205826382545996
running average episode reward sum: 0.4902751465168278
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  2.]), 'previousTarget': array([12.,  2.]), 'currentState': array([12.64990863,  2.90256263,  4.11209494]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 1.112205250439289}
episode index:3993
target Thresh 32.0
target distance 17.0
model initialize at round 3993
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 26.]), 'previousTarget': array([24., 26.]), 'currentState': array([23.68956358,  9.25342734,  2.390762  ]), 'targetState': array([24, 26], dtype=int32), 'currentDistance': 16.749449736630908}
done in step count: 38
reward sum = 0.4284386946398684
running average episode reward sum: 0.4902596641803538
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 26.]), 'previousTarget': array([24., 26.]), 'currentState': array([23.94633778, 25.20909607,  1.64602278]), 'targetState': array([24, 26], dtype=int32), 'currentDistance': 0.7927223124952346}
episode index:3994
target Thresh 32.0
target distance 19.0
model initialize at round 3994
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 29.]), 'previousTarget': array([ 4., 29.]), 'currentState': array([3.06052137, 9.97086447, 6.08702087]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 19.052312696005018}
done in step count: 54
reward sum = 0.3453858319899267
running average episode reward sum: 0.4902234003925715
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 29.]), 'previousTarget': array([ 4., 29.]), 'currentState': array([ 4.26240849, 28.12953049,  1.72671996]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 0.9091619109903463}
episode index:3995
target Thresh 32.0
target distance 6.0
model initialize at round 3995
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 19.]), 'previousTarget': array([ 3., 19.]), 'currentState': array([ 6.02820899, 13.37250459,  1.5861105 ]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 6.390520660187558}
done in step count: 12
reward sum = 0.7841449236279535
running average episode reward sum: 0.4902969543273151
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 19.]), 'previousTarget': array([ 3., 19.]), 'currentState': array([ 3.76818041, 18.2334065 ,  2.06789129]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 1.0852496188608003}
episode index:3996
target Thresh 32.0
target distance 10.0
model initialize at round 3996
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 28.]), 'previousTarget': array([27., 28.]), 'currentState': array([17.       , 25.       ,  5.0561544]), 'targetState': array([27, 28], dtype=int32), 'currentDistance': 10.44030650891055}
done in step count: 25
reward sum = 0.6092180678344735
running average episode reward sum: 0.4903267069201365
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 28.]), 'previousTarget': array([27., 28.]), 'currentState': array([26.21670833, 27.25428426,  0.51122818]), 'targetState': array([27, 28], dtype=int32), 'currentDistance': 1.081497944380088}
episode index:3997
target Thresh 32.0
target distance 7.0
model initialize at round 3997
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 19.]), 'previousTarget': array([ 7., 19.]), 'currentState': array([11.        , 26.        ,  2.40986964]), 'targetState': array([ 7, 19], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 19
reward sum = 0.6723195742567303
running average episode reward sum: 0.4903722278974593
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 19.]), 'previousTarget': array([ 7., 19.]), 'currentState': array([ 7.05798577, 19.8287834 ,  4.18761225]), 'targetState': array([ 7, 19], dtype=int32), 'currentDistance': 0.8308094114686081}
episode index:3998
target Thresh 32.0
target distance 3.0
model initialize at round 3998
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 10.]), 'previousTarget': array([18., 10.]), 'currentState': array([20.84905978, 12.52616499,  4.22188485]), 'targetState': array([18, 10], dtype=int32), 'currentDistance': 3.8077094418032496}
done in step count: 6
reward sum = 0.8698331722946516
running average episode reward sum: 0.4904671168557982
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 10.]), 'previousTarget': array([18., 10.]), 'currentState': array([18.67425733, 10.74610828,  3.54986413]), 'targetState': array([18, 10], dtype=int32), 'currentDistance': 1.0056343837633868}
episode index:3999
target Thresh 32.0
target distance 7.0
model initialize at round 3999
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 20.]), 'previousTarget': array([22., 20.]), 'currentState': array([22.85622168, 13.45934159,  1.75144199]), 'targetState': array([22, 20], dtype=int32), 'currentDistance': 6.596463294194939}
done in step count: 13
reward sum = 0.7650874095274951
running average episode reward sum: 0.49053577192896614
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 20.]), 'previousTarget': array([22., 20.]), 'currentState': array([22.37482959, 19.18446285,  1.74147635]), 'targetState': array([22, 20], dtype=int32), 'currentDistance': 0.8975511467401053}
episode index:4000
target Thresh 32.0
target distance 20.0
model initialize at round 4000
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6.76783447, 20.94980808]), 'previousTarget': array([ 6.76121364, 20.9529684 ]), 'currentState': array([22.01051269,  8.00141975,  0.38673848]), 'targetState': array([ 2, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 75
reward sum = 0.17738672343293133
running average episode reward sum: 0.49045750423376594
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 25.]), 'previousTarget': array([ 2., 25.]), 'currentState': array([ 2.84359608, 24.5268336 ,  2.64876135]), 'targetState': array([ 2, 25], dtype=int32), 'currentDistance': 0.9672335715136836}
episode index:4001
target Thresh 32.0
target distance 6.0
model initialize at round 4001
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 13.]), 'previousTarget': array([18., 13.]), 'currentState': array([19.09475388,  7.2543725 ,  1.3446312 ]), 'targetState': array([18, 13], dtype=int32), 'currentDistance': 5.848993195150614}
done in step count: 12
reward sum = 0.794841208871918
running average episode reward sum: 0.4905335621309769
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 13.]), 'previousTarget': array([18., 13.]), 'currentState': array([18.23809379, 12.1850926 ,  1.55447111]), 'targetState': array([18, 13], dtype=int32), 'currentDistance': 0.8489774584953578}
episode index:4002
target Thresh 32.0
target distance 9.0
model initialize at round 4002
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  2.]), 'previousTarget': array([22.,  2.]), 'currentState': array([19.        , 11.        ,  0.79687944]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 9.486832980505138}
done in step count: 22
reward sum = 0.6165651718035307
running average episode reward sum: 0.4905650464201781
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  2.]), 'previousTarget': array([22.,  2.]), 'currentState': array([21.64987394,  2.76856297,  4.98283388]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 0.8445574590841003}
episode index:4003
target Thresh 32.0
target distance 4.0
model initialize at round 4003
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 28.]), 'previousTarget': array([ 7., 28.]), 'currentState': array([11.06639379, 24.06739476,  1.04537976]), 'targetState': array([ 7, 28], dtype=int32), 'currentDistance': 5.656937548338452}
done in step count: 12
reward sum = 0.7728154017936257
running average episode reward sum: 0.4906355385169247
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 28.]), 'previousTarget': array([ 7., 28.]), 'currentState': array([ 7.83472435, 27.58443507,  2.46830048]), 'targetState': array([ 7, 28], dtype=int32), 'currentDistance': 0.9324478298146376}
episode index:4004
target Thresh 32.0
target distance 5.0
model initialize at round 4004
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 14.]), 'previousTarget': array([ 3., 14.]), 'currentState': array([ 8.00001656, 16.99967834,  4.51132631]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 5.830800608347771}
done in step count: 13
reward sum = 0.7768308516677986
running average episode reward sum: 0.49070699802083256
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 14.]), 'previousTarget': array([ 3., 14.]), 'currentState': array([ 3.92979554, 14.25567799,  3.40071205]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 0.9643085521806576}
episode index:4005
target Thresh 32.0
target distance 9.0
model initialize at round 4005
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 23.]), 'previousTarget': array([17., 23.]), 'currentState': array([19.        , 14.        ,  0.18770328]), 'targetState': array([17, 23], dtype=int32), 'currentDistance': 9.219544457292889}
done in step count: 21
reward sum = 0.6424962027327964
running average episode reward sum: 0.49074488848631237
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 23.]), 'previousTarget': array([17., 23.]), 'currentState': array([16.92726643, 22.07226415,  1.59295669]), 'targetState': array([17, 23], dtype=int32), 'currentDistance': 0.9305825986525189}
episode index:4006
target Thresh 32.0
target distance 21.0
model initialize at round 4006
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.49656946, 26.21237531]), 'previousTarget': array([12.49442256, 26.23047895]), 'currentState': array([7.07506068, 6.9612132 , 5.94848078]), 'targetState': array([13, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.2843349843824975
running average episode reward sum: 0.490693376156863
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 28.]), 'previousTarget': array([13., 28.]), 'currentState': array([13.25637707, 27.15615317,  1.40624209]), 'targetState': array([13, 28], dtype=int32), 'currentDistance': 0.8819334871318505}
episode index:4007
target Thresh 32.0
target distance 18.0
model initialize at round 4007
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'previousTarget': array([26., 11.]), 'currentState': array([ 8.34973215, 12.00392283,  6.26005284]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 17.67879567120997}
done in step count: 40
reward sum = 0.4112107877946253
running average episode reward sum: 0.4906735451717426
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'previousTarget': array([26., 11.]), 'currentState': array([25.1967478 , 10.67546276,  0.27836787]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 0.8663362615408962}
episode index:4008
target Thresh 32.0
target distance 7.0
model initialize at round 4008
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([18.        , 22.        ,  1.59779507]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 8.602325267042627}
done in step count: 20
reward sum = 0.6434711383856672
running average episode reward sum: 0.4907116588143502
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([24.22182164, 17.48864175,  5.59851946]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 0.9188755781435972}
episode index:4009
target Thresh 32.0
target distance 2.0
model initialize at round 4009
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.01898512,  4.99549298,  5.82617137]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.9955832912009586}
done in step count: 5
reward sum = 0.9096229465605883
running average episode reward sum: 0.49081612546964853
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.10937491,  3.77426367,  4.41906452]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.7819508261909908}
episode index:4010
target Thresh 32.0
target distance 24.0
model initialize at round 4010
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2.34618112, 8.05534543]), 'previousTarget': array([2.3390904 , 8.06908484]), 'currentState': array([ 4.04727744, 27.9828709 ,  5.83046433]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.2182081448676082
running average episode reward sum: 0.49074816037849867
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.51202367, 4.76687196, 4.94319316]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.9089628710569226}
episode index:4011
target Thresh 32.0
target distance 4.0
model initialize at round 4011
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.92203146, 14.54043224,  4.50016918]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 3.6585246326719076}
done in step count: 8
reward sum = 0.8731542048352305
running average episode reward sum: 0.49084347594291955
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.16394377, 11.86381589,  4.40059004]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.8792357190938437}
episode index:4012
target Thresh 32.0
target distance 8.0
model initialize at round 4012
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 5.        , 13.        ,  5.76081291]), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.0}
done in step count: 17
reward sum = 0.7010776651001127
running average episode reward sum: 0.4908958642282814
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.12737445, 5.8704224 , 4.43755625]), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.879692793666632}
episode index:4013
target Thresh 32.0
target distance 10.0
model initialize at round 4013
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 16.]), 'previousTarget': array([ 6., 16.]), 'currentState': array([15.8920424 ,  8.94151809,  3.43663535]), 'targetState': array([ 6, 16], dtype=int32), 'currentDistance': 12.152146709691733}
done in step count: 25
reward sum = 0.5711718031756113
running average episode reward sum: 0.4909158632165592
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 16.]), 'previousTarget': array([ 6., 16.]), 'currentState': array([ 6.99606245, 15.75444405,  2.48140394]), 'targetState': array([ 6, 16], dtype=int32), 'currentDistance': 1.025884073656228}
episode index:4014
target Thresh 32.0
target distance 13.0
model initialize at round 4014
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([ 9.        , 21.        ,  2.90286309]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 15.811388300841896}
done in step count: 42
reward sum = 0.4223984730190201
running average episode reward sum: 0.49089879786408164
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([17.59403849,  8.94274172,  5.14700552]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 1.0264339734207697}
episode index:4015
target Thresh 32.0
target distance 12.0
model initialize at round 4015
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 19.]), 'previousTarget': array([16., 19.]), 'currentState': array([ 4.05766453, 27.80224934,  5.05749861]), 'targetState': array([16, 19], dtype=int32), 'currentDistance': 14.835732874630086}
done in step count: 33
reward sum = 0.502452786303695
running average episode reward sum: 0.4909016748532349
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 19.]), 'previousTarget': array([16., 19.]), 'currentState': array([15.00288589, 19.45493738,  5.79764788]), 'targetState': array([16, 19], dtype=int32), 'currentDistance': 1.095994785133714}
episode index:4016
target Thresh 32.0
target distance 15.0
model initialize at round 4016
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 13.]), 'previousTarget': array([27., 13.]), 'currentState': array([12.16995128,  7.29430654,  0.91262183]), 'targetState': array([27, 13], dtype=int32), 'currentDistance': 15.889785490008142}
done in step count: 39
reward sum = 0.4656356615754361
running average episode reward sum: 0.49089538508144553
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 13.]), 'previousTarget': array([27., 13.]), 'currentState': array([26.22890706, 12.24165709,  0.53897737]), 'targetState': array([27, 13], dtype=int32), 'currentDistance': 1.0815120403990752}
episode index:4017
target Thresh 32.0
target distance 16.0
model initialize at round 4017
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.59074408, 23.67882258]), 'previousTarget': array([17.59074408, 23.67882258]), 'currentState': array([ 3.        , 10.        ,  2.93789816]), 'targetState': array([19, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.2381403819896844
running average episode reward sum: 0.49083247940621116
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 25.]), 'previousTarget': array([19., 25.]), 'currentState': array([18.2017751 , 24.29573371,  0.86386729]), 'targetState': array([19, 25], dtype=int32), 'currentDistance': 1.0644970691460018}
episode index:4018
target Thresh 32.0
target distance 8.0
model initialize at round 4018
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 23.]), 'previousTarget': array([ 5., 23.]), 'currentState': array([ 9.03039621, 15.0258032 ,  0.94737625]), 'targetState': array([ 5, 23], dtype=int32), 'currentDistance': 8.934870356298006}
done in step count: 18
reward sum = 0.665446941988521
running average episode reward sum: 0.49087592664746077
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 23.]), 'previousTarget': array([ 5., 23.]), 'currentState': array([ 5.20651729, 22.09323814,  1.88677803]), 'targetState': array([ 5, 23], dtype=int32), 'currentDistance': 0.9299819702844703}
episode index:4019
target Thresh 32.0
target distance 12.0
model initialize at round 4019
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 22.]), 'previousTarget': array([ 9., 22.]), 'currentState': array([5.00297215, 9.97530153, 5.08465052]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 12.671606245067183}
done in step count: 32
reward sum = 0.5088157674640185
running average episode reward sum: 0.49088038929443006
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 22.]), 'previousTarget': array([ 9., 22.]), 'currentState': array([ 8.75833007, 21.06544905,  1.23156137]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 0.9652926184273799}
episode index:4020
target Thresh 32.0
target distance 16.0
model initialize at round 4020
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([7.15000477, 7.23438732, 1.10696951]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 15.87378421896925}
done in step count: 36
reward sum = 0.46714802178338777
running average episode reward sum: 0.49087448718860793
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([ 9.01337158, 22.14820652,  1.3853317 ]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 0.8518984313207125}
episode index:4021
target Thresh 32.0
target distance 17.0
model initialize at round 4021
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8.20859686, 13.86502556]), 'previousTarget': array([ 8.20859686, 13.86502556]), 'currentState': array([25.        ,  3.        ,  1.47754028]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.33300587542304416
running average episode reward sum: 0.4908352359176567
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([ 8.83775215, 13.69231748,  2.618991  ]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 0.8924669156923518}
episode index:4022
target Thresh 32.0
target distance 17.0
model initialize at round 4022
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6.56345002, 14.28805145]), 'previousTarget': array([ 6.56139529, 14.28585494]), 'currentState': array([21.99961078, 27.00516358,  1.8842918 ]), 'targetState': array([ 5, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.2709567372145644
running average episode reward sum: 0.49078058056128016
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 13.]), 'previousTarget': array([ 5., 13.]), 'currentState': array([ 5.62236426, 13.91015312,  3.88424348]), 'targetState': array([ 5, 13], dtype=int32), 'currentDistance': 1.102595112157561}
episode index:4023
target Thresh 32.0
target distance 9.0
model initialize at round 4023
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([17.03605964, 11.18198023,  1.5176098 ]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 10.74707996860991}
done in step count: 26
reward sum = 0.6072586439888966
running average episode reward sum: 0.4908095264020922
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([ 8.93187403, 16.42625237,  2.74179707]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 1.0943379513282285}
episode index:4024
target Thresh 32.0
target distance 16.0
model initialize at round 4024
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 19.]), 'previousTarget': array([14., 19.]), 'currentState': array([24.        ,  3.        ,  4.03522277]), 'targetState': array([14, 19], dtype=int32), 'currentDistance': 18.867962264113206}
done in step count: 53
reward sum = 0.3478746645348145
running average episode reward sum: 0.4907740146351686
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 19.]), 'previousTarget': array([14., 19.]), 'currentState': array([14.6079644 , 18.10944045,  2.01436331]), 'targetState': array([14, 19], dtype=int32), 'currentDistance': 1.0782935732167758}
episode index:4025
target Thresh 32.0
target distance 4.0
model initialize at round 4025
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 13.]), 'previousTarget': array([ 4., 13.]), 'currentState': array([ 8.02284777, 14.98781241,  5.54181883]), 'targetState': array([ 4, 13], dtype=int32), 'currentDistance': 4.487170864271305}
done in step count: 11
reward sum = 0.7925514471833247
running average episode reward sum: 0.49084897177191683
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 13.]), 'previousTarget': array([ 4., 13.]), 'currentState': array([ 4.9616811 , 13.23310803,  3.33453039]), 'targetState': array([ 4, 13], dtype=int32), 'currentDistance': 0.9895301372340017}
episode index:4026
target Thresh 32.0
target distance 23.0
model initialize at round 4026
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7.02896263, 21.33936061]), 'previousTarget': array([ 7.02547777, 21.34140113]), 'currentState': array([15.01207983,  3.00169883,  0.10621859]), 'targetState': array([ 5, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.29637289520406096
running average episode reward sum: 0.4906534858352453
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 26.]), 'previousTarget': array([ 5., 26.]), 'currentState': array([ 5.83631581, 24.26518924,  2.24656766]), 'targetState': array([ 5, 26], dtype=int32), 'currentDistance': 1.9258744802722911}
episode index:4027
target Thresh 32.0
target distance 1.0
model initialize at round 4027
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 16.]), 'previousTarget': array([10., 16.]), 'currentState': array([10.05203182, 15.01170391,  0.47375506]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 0.9896648241945765}
done in step count: 0
reward sum = 0.9898838288858017
running average episode reward sum: 0.4907774258409679
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 16.]), 'previousTarget': array([10., 16.]), 'currentState': array([10.05203182, 15.01170391,  0.47375506]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 0.9896648241945765}
episode index:4028
target Thresh 32.0
target distance 22.0
model initialize at round 4028
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.30841466, 26.77569561]), 'previousTarget': array([19.29773591, 26.81660336]), 'currentState': array([22.05526892,  6.96522384,  5.9740591 ]), 'targetState': array([19, 29], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 67
reward sum = 0.21332717864293987
running average episode reward sum: 0.49070856253811407
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 29.]), 'previousTarget': array([19., 29.]), 'currentState': array([18.83921816, 28.02493247,  1.63145994]), 'targetState': array([19, 29], dtype=int32), 'currentDistance': 0.9882345323530652}
episode index:4029
target Thresh 32.0
target distance 6.0
model initialize at round 4029
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 24.]), 'previousTarget': array([ 7., 24.]), 'currentState': array([12.99933066, 18.99989829,  3.05392569]), 'targetState': array([ 7, 24], dtype=int32), 'currentDistance': 7.809800609143161}
done in step count: 17
reward sum = 0.7014984141884122
running average episode reward sum: 0.4907608677122208
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 24.]), 'previousTarget': array([ 7., 24.]), 'currentState': array([ 7.25608082, 23.14955434,  2.10451304]), 'targetState': array([ 7, 24], dtype=int32), 'currentDistance': 0.8881639517688747}
episode index:4030
target Thresh 32.0
target distance 14.0
model initialize at round 4030
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 29.]), 'previousTarget': array([ 5., 29.]), 'currentState': array([19.        , 20.        ,  0.74692672]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 16.64331697709324}
done in step count: 39
reward sum = 0.4099046605904042
running average episode reward sum: 0.49074080911457213
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 29.]), 'previousTarget': array([ 5., 29.]), 'currentState': array([ 5.79322082, 28.50790563,  2.63139325]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 0.9334645923386904}
episode index:4031
target Thresh 32.0
target distance 13.0
model initialize at round 4031
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 6.]), 'previousTarget': array([6., 6.]), 'currentState': array([18.78489102,  8.16543804,  2.63824502]), 'targetState': array([6, 6], dtype=int32), 'currentDistance': 12.966979616496717}
done in step count: 33
reward sum = 0.538229479017958
running average episode reward sum: 0.4907525870584966
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 6.]), 'previousTarget': array([6., 6.]), 'currentState': array([6.76145898, 5.84214285, 3.14903774]), 'targetState': array([6, 6], dtype=int32), 'currentDistance': 0.7776494478737074}
episode index:4032
target Thresh 32.0
target distance 14.0
model initialize at round 4032
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 27.]), 'previousTarget': array([ 2., 27.]), 'currentState': array([ 3.01359313, 12.99259675,  6.03522843]), 'targetState': array([ 2, 27], dtype=int32), 'currentDistance': 14.044027801572415}
done in step count: 35
reward sum = 0.48562502608411134
running average episode reward sum: 0.4907513156573128
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 27.]), 'previousTarget': array([ 2., 27.]), 'currentState': array([ 2.19839731, 26.04424502,  1.790552  ]), 'targetState': array([ 2, 27], dtype=int32), 'currentDistance': 0.976129640327571}
episode index:4033
target Thresh 32.0
target distance 16.0
model initialize at round 4033
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([ 4.0704604 , 21.9503834 ,  5.60486397]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 19.33570703640723}
done in step count: 47
reward sum = 0.3789065356037822
running average episode reward sum: 0.4907235901292876
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.14511391,  6.55411457,  5.33358054]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 1.0187606151913344}
episode index:4034
target Thresh 32.0
target distance 11.0
model initialize at round 4034
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 26.]), 'previousTarget': array([ 7., 26.]), 'currentState': array([ 1.96259993, 15.04288203,  2.06606981]), 'targetState': array([ 7, 26], dtype=int32), 'currentDistance': 12.059595089862155}
done in step count: 26
reward sum = 0.5601978207857049
running average episode reward sum: 0.4907408080303177
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 26.]), 'previousTarget': array([ 7., 26.]), 'currentState': array([ 6.55601104, 25.20277512,  1.01170817]), 'targetState': array([ 7, 26], dtype=int32), 'currentDistance': 0.9125205239119615}
episode index:4035
target Thresh 32.0
target distance 7.0
model initialize at round 4035
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 8.]), 'previousTarget': array([9., 8.]), 'currentState': array([2.00761705, 8.02233853, 0.98967731]), 'targetState': array([9, 8], dtype=int32), 'currentDistance': 6.992418633045025}
done in step count: 15
reward sum = 0.7295608891270042
running average episode reward sum: 0.49079998049837936
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 8.]), 'previousTarget': array([9., 8.]), 'currentState': array([8.11468517, 8.1051714 , 6.09970725]), 'targetState': array([9, 8], dtype=int32), 'currentDistance': 0.8915398883415623}
episode index:4036
target Thresh 32.0
target distance 17.0
model initialize at round 4036
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 13.]), 'previousTarget': array([ 2., 13.]), 'currentState': array([18.91233162,  8.06973374,  2.5310877 ]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 17.616313068539377}
done in step count: 49
reward sum = 0.4239335118309975
running average episode reward sum: 0.4907834170927149
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 13.]), 'previousTarget': array([ 2., 13.]), 'currentState': array([ 2.93794444, 12.50447558,  2.8807719 ]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 1.06079415031981}
episode index:4037
target Thresh 32.0
target distance 2.0
model initialize at round 4037
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([17.16179196,  4.88020808,  5.43219993]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 2.0585857106664114}
done in step count: 4
reward sum = 0.9349306977265539
running average episode reward sum: 0.4908934089898505
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([17.72606249,  3.79830956,  5.01622169]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 0.8440023192600646}
episode index:4038
target Thresh 32.0
target distance 10.0
model initialize at round 4038
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 24.]), 'previousTarget': array([19., 24.]), 'currentState': array([ 9.00065603, 25.99494119,  5.05391467]), 'targetState': array([19, 24], dtype=int32), 'currentDistance': 10.19640476981916}
done in step count: 23
reward sum = 0.6345390295369824
running average episode reward sum: 0.4909289736396517
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 24.]), 'previousTarget': array([19., 24.]), 'currentState': array([18.07572788, 24.05536768,  5.97738765]), 'targetState': array([19, 24], dtype=int32), 'currentDistance': 0.9259290140483667}
episode index:4039
target Thresh 32.0
target distance 12.0
model initialize at round 4039
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([ 5.        , 29.        ,  4.39801636]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 15.000000000000002}
done in step count: 34
reward sum = 0.48480399517562767
running average episode reward sum: 0.49092745755587347
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([16.14649426, 20.62214697,  5.56676066]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 1.0561907538529542}
episode index:4040
target Thresh 32.0
target distance 12.0
model initialize at round 4040
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([24.        ,  9.        ,  0.44726085]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 12.041594578792296}
done in step count: 32
reward sum = 0.517179982232528
running average episode reward sum: 0.4909339540974911
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([12.79134261,  9.56682271,  2.86769897]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.9021450455375496}
episode index:4041
target Thresh 32.0
target distance 10.0
model initialize at round 4041
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 15.]), 'previousTarget': array([12., 15.]), 'currentState': array([11.01951982, 24.98449433,  5.36781836]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 10.032520538879975}
done in step count: 24
reward sum = 0.6332931303168108
running average episode reward sum: 0.4909691740817116
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 15.]), 'previousTarget': array([12., 15.]), 'currentState': array([12.54755337, 15.88351105,  4.49661212]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 1.0394260316503894}
episode index:4042
target Thresh 32.0
target distance 13.0
model initialize at round 4042
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([ 7.05435108, 16.8734693 ,  5.34754345]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 13.5127195793372}
done in step count: 32
reward sum = 0.5376671199410836
running average episode reward sum: 0.49098072440223084
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([19.00337278, 13.81815562,  5.99942199]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 1.2894357039076474}
episode index:4043
target Thresh 32.0
target distance 8.0
model initialize at round 4043
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 21.]), 'previousTarget': array([ 3., 21.]), 'currentState': array([ 5.91484383, 13.0370504 ,  2.47870874]), 'targetState': array([ 3, 21], dtype=int32), 'currentDistance': 8.479674571673456}
done in step count: 16
reward sum = 0.6910316592938369
running average episode reward sum: 0.4910301929815809
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 21.]), 'previousTarget': array([ 3., 21.]), 'currentState': array([ 2.92252238, 20.17098952,  1.78773107]), 'targetState': array([ 3, 21], dtype=int32), 'currentDistance': 0.8326230607656645}
episode index:4044
target Thresh 32.0
target distance 10.0
model initialize at round 4044
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([26.08434425,  3.98377881,  0.04091769]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 14.213327758000549}
done in step count: 35
reward sum = 0.46041248156664094
running average episode reward sum: 0.49102262370805433
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([16.40766942, 13.23069781,  2.19422462]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 0.8706435641844165}
episode index:4045
target Thresh 32.0
target distance 16.0
model initialize at round 4045
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([24.95987526, 10.1075145 ,  1.97355424]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 19.322625566237612}
done in step count: 42
reward sum = 0.37422411347987106
running average episode reward sum: 0.49099375605846757
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([ 9.70106432, 20.23674248,  2.96547306]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 1.0363653902234846}
episode index:4046
target Thresh 32.0
target distance 3.0
model initialize at round 4046
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 23.]), 'previousTarget': array([25., 23.]), 'currentState': array([25.99033565, 19.98961207,  3.71257192]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 3.169100850418311}
done in step count: 8
reward sum = 0.8432419526117244
running average episode reward sum: 0.49108079539539695
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 23.]), 'previousTarget': array([25., 23.]), 'currentState': array([24.51993405, 22.0999871 ,  1.80917625]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 1.0200424218529465}
episode index:4047
target Thresh 32.0
target distance 13.0
model initialize at round 4047
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 12.]), 'previousTarget': array([14., 12.]), 'currentState': array([13.95397231, 25.00187735,  3.35332775]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 13.001958824715066}
done in step count: 30
reward sum = 0.5293812669371476
running average episode reward sum: 0.49109025697433517
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 12.]), 'previousTarget': array([14., 12.]), 'currentState': array([14.62053926, 12.99772371,  4.77816836]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 1.1749559861609713}
episode index:4048
target Thresh 32.0
target distance 14.0
model initialize at round 4048
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 29.]), 'previousTarget': array([20., 29.]), 'currentState': array([18.13386232, 15.38806902,  1.35242343]), 'targetState': array([20, 29], dtype=int32), 'currentDistance': 13.739255249706549}
done in step count: 32
reward sum = 0.5227639661094295
running average episode reward sum: 0.49109807957476365
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 29.]), 'previousTarget': array([20., 29.]), 'currentState': array([20.34679226, 28.05504516,  1.27904807]), 'targetState': array([20, 29], dtype=int32), 'currentDistance': 1.0065806108180406}
episode index:4049
target Thresh 32.0
target distance 15.0
model initialize at round 4049
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 28.]), 'previousTarget': array([27., 28.]), 'currentState': array([12.        , 15.        ,  2.67427802]), 'targetState': array([27, 28], dtype=int32), 'currentDistance': 19.849433241279208}
done in step count: 51
reward sum = 0.32074113678044286
running average episode reward sum: 0.4910560161320984
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 28.]), 'previousTarget': array([27., 28.]), 'currentState': array([26.02656486, 27.48798748,  0.70943877]), 'targetState': array([27, 28], dtype=int32), 'currentDistance': 1.0998785322395546}
episode index:4050
target Thresh 32.0
target distance 13.0
model initialize at round 4050
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'previousTarget': array([26., 11.]), 'currentState': array([13.13151822, 23.92852711,  5.61814892]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 18.2412893327741}
done in step count: 41
reward sum = 0.40467086364766874
running average episode reward sum: 0.4910346917301027
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'previousTarget': array([26., 11.]), 'currentState': array([25.29896813, 11.19290921,  5.73488574]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 0.7270898518136255}
episode index:4051
target Thresh 32.0
target distance 12.0
model initialize at round 4051
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([20.10786945, 20.9930049 ,  6.00605305]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 13.941089433464462}
done in step count: 37
reward sum = 0.4708013267652462
running average episode reward sum: 0.49102969830340853
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([12.78807945,  9.78842942,  4.37485294]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.8164136609400526}
episode index:4052
target Thresh 32.0
target distance 20.0
model initialize at round 4052
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8.63452377, 22.84697923]), 'previousTarget': array([ 8.62070537, 22.86588292]), 'currentState': array([22.00872379,  7.97651274,  4.81552315]), 'targetState': array([ 4, 28], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 71
reward sum = 0.14739639707254432
running average episode reward sum: 0.4909449133783577
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 28.]), 'previousTarget': array([ 4., 28.]), 'currentState': array([ 4.42438788, 27.00879527,  2.30992095]), 'targetState': array([ 4, 28], dtype=int32), 'currentDistance': 1.0782355463016275}
episode index:4053
target Thresh 32.0
target distance 19.0
model initialize at round 4053
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 24.]), 'previousTarget': array([ 3., 24.]), 'currentState': array([22.       , 20.       ,  3.9612999]), 'targetState': array([ 3, 24], dtype=int32), 'currentDistance': 19.416487838947596}
done in step count: 47
reward sum = 0.3526764368467073
running average episode reward sum: 0.49091080669939086
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 24.]), 'previousTarget': array([ 3., 24.]), 'currentState': array([ 3.90470397, 24.00420944,  3.20018956]), 'targetState': array([ 3, 24], dtype=int32), 'currentDistance': 0.9047137590304847}
episode index:4054
target Thresh 32.0
target distance 3.0
model initialize at round 4054
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([8.93473235, 8.7974649 , 4.61735865]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 4.1499006792866675}
done in step count: 7
reward sum = 0.8603776251295606
running average episode reward sum: 0.4910019205880296
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([11.00765699,  6.96573979,  5.83936112]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 1.3847014086896934}
episode index:4055
target Thresh 32.0
target distance 13.0
model initialize at round 4055
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 15.]), 'previousTarget': array([13., 15.]), 'currentState': array([ 6.        , 28.        ,  4.02391243]), 'targetState': array([13, 15], dtype=int32), 'currentDistance': 14.764823060233402}
done in step count: 37
reward sum = 0.4800743866317335
running average episode reward sum: 0.49099922642285304
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 15.]), 'previousTarget': array([13., 15.]), 'currentState': array([12.37766647, 15.79176161,  5.34678701]), 'targetState': array([13, 15], dtype=int32), 'currentDistance': 1.0070677573282636}
episode index:4056
target Thresh 32.0
target distance 11.0
model initialize at round 4056
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 29.]), 'previousTarget': array([15., 29.]), 'currentState': array([ 9.96218237, 18.00971064,  2.63774729]), 'targetState': array([15, 29], dtype=int32), 'currentDistance': 12.089915908191077}
done in step count: 29
reward sum = 0.5337121937425671
running average episode reward sum: 0.4910097546376225
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 29.]), 'previousTarget': array([15., 29.]), 'currentState': array([15.27401627, 28.0789686 ,  1.37193508]), 'targetState': array([15, 29], dtype=int32), 'currentDistance': 0.9609285907941123}
episode index:4057
target Thresh 32.0
target distance 11.0
model initialize at round 4057
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([24.62372858, 13.05240175,  3.14360398]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 12.751469692991051}
done in step count: 28
reward sum = 0.5542838450917498
running average episode reward sum: 0.49102534706996703
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.24436378,  6.9373709 ,  3.94989737]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.9686990585340238}
episode index:4058
target Thresh 32.0
target distance 3.0
model initialize at round 4058
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 16.]), 'previousTarget': array([23., 16.]), 'currentState': array([25.97674649, 12.98803881,  3.36417961]), 'targetState': array([23, 16], dtype=int32), 'currentDistance': 4.234729020550263}
done in step count: 8
reward sum = 0.8358014011471071
running average episode reward sum: 0.49111028820179187
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 16.]), 'previousTarget': array([23., 16.]), 'currentState': array([23.52129683, 15.26160865,  2.10595249]), 'targetState': array([23, 16], dtype=int32), 'currentDistance': 0.9038651315568961}
episode index:4059
target Thresh 32.0
target distance 16.0
model initialize at round 4059
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'previousTarget': array([13., 22.]), 'currentState': array([20.07985957,  6.2684459 ,  1.44473612]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 17.251266794809933}
done in step count: 45
reward sum = 0.4324325032516564
running average episode reward sum: 0.4910958355454002
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'previousTarget': array([13., 22.]), 'currentState': array([13.93323334, 21.04702453,  2.38778754]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 1.333824095957089}
episode index:4060
target Thresh 32.0
target distance 20.0
model initialize at round 4060
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.20729355,  3.7615699 ]), 'previousTarget': array([11.20729355,  3.7615699 ]), 'currentState': array([ 3.        , 22.        ,  0.52384427]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.25474279015991186
running average episode reward sum: 0.49103763484473895
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  2.]), 'previousTarget': array([12.,  2.]), 'currentState': array([11.25963524,  2.52353773,  5.45070636]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 0.9067699478640776}
episode index:4061
target Thresh 32.0
target distance 9.0
model initialize at round 4061
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 21.]), 'previousTarget': array([23., 21.]), 'currentState': array([14.4931997 , 21.92464013,  6.0167443 ]), 'targetState': array([23, 21], dtype=int32), 'currentDistance': 8.55690427575433}
done in step count: 17
reward sum = 0.6962318123560776
running average episode reward sum: 0.4910881503980406
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 21.]), 'previousTarget': array([23., 21.]), 'currentState': array([22.13364036, 21.02427525,  0.07601106]), 'targetState': array([23, 21], dtype=int32), 'currentDistance': 0.8666996698115395}
episode index:4062
target Thresh 32.0
target distance 6.0
model initialize at round 4062
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'previousTarget': array([11., 21.]), 'currentState': array([ 8.        , 27.        ,  3.10953116]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 6.708203932499369}
done in step count: 16
reward sum = 0.7127587906565759
running average episode reward sum: 0.49114270876384386
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'previousTarget': array([11., 21.]), 'currentState': array([10.63203579, 21.83971732,  5.24351411]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 0.9168003305575038}
episode index:4063
target Thresh 32.0
target distance 16.0
model initialize at round 4063
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 28.]), 'previousTarget': array([ 9., 28.]), 'currentState': array([25.03505782, 24.03030265,  0.95934606]), 'targetState': array([ 9, 28], dtype=int32), 'currentDistance': 16.519127590694552}
done in step count: 42
reward sum = 0.39804967535632996
running average episode reward sum: 0.49111980201349753
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 28.]), 'previousTarget': array([ 9., 28.]), 'currentState': array([ 9.94572621, 28.82626303,  3.32077247]), 'targetState': array([ 9, 28], dtype=int32), 'currentDistance': 1.2558298706181452}
episode index:4064
target Thresh 32.0
target distance 10.0
model initialize at round 4064
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  6.]), 'previousTarget': array([13.,  6.]), 'currentState': array([ 6.        , 16.        ,  1.01718068]), 'targetState': array([13,  6], dtype=int32), 'currentDistance': 12.206555615733704}
done in step count: 28
reward sum = 0.5240100500676339
running average episode reward sum: 0.49112789309542965
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  6.]), 'previousTarget': array([13.,  6.]), 'currentState': array([12.3760673 ,  6.73732383,  5.53436925]), 'targetState': array([13,  6], dtype=int32), 'currentDistance': 0.9658873920224497}
episode index:4065
target Thresh 32.0
target distance 22.0
model initialize at round 4065
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.20116627, 16.13389861]), 'previousTarget': array([21.0585156 , 16.06407315]), 'currentState': array([ 2.0950546 , 10.22149668,  0.96119457]), 'targetState': array([24, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.27208220235458674
running average episode reward sum: 0.4910740205694235
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 17.]), 'previousTarget': array([24., 17.]), 'currentState': array([23.1232128 , 16.36140697,  0.71723446]), 'targetState': array([24, 17], dtype=int32), 'currentDistance': 1.0846920556503787}
episode index:4066
target Thresh 32.0
target distance 20.0
model initialize at round 4066
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5.03319094, 26.77872706]), 'previousTarget': array([ 5.03319094, 26.77872706]), 'currentState': array([8.        , 7.        , 0.50623562]), 'targetState': array([ 5, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.32685569006570486
running average episode reward sum: 0.4910336423224346
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 27.]), 'previousTarget': array([ 5., 27.]), 'currentState': array([ 5.68675462, 26.13558267,  1.98693549]), 'targetState': array([ 5, 27], dtype=int32), 'currentDistance': 1.104015043800426}
episode index:4067
target Thresh 32.0
target distance 8.0
model initialize at round 4067
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 21.]), 'previousTarget': array([22., 21.]), 'currentState': array([19.       , 29.       ,  0.8187772]), 'targetState': array([22, 21], dtype=int32), 'currentDistance': 8.54400374531753}
done in step count: 23
reward sum = 0.6378077768223104
running average episode reward sum: 0.49106972249315733
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 21.]), 'previousTarget': array([22., 21.]), 'currentState': array([21.59350996, 21.8827701 ,  4.96636986]), 'targetState': array([22, 21], dtype=int32), 'currentDistance': 0.9718627429305692}
episode index:4068
target Thresh 32.0
target distance 13.0
model initialize at round 4068
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  2.]), 'previousTarget': array([26.,  2.]), 'currentState': array([13.        , 13.        ,  3.57540461]), 'targetState': array([26,  2], dtype=int32), 'currentDistance': 17.029386365926403}
done in step count: 44
reward sum = 0.39210719808996064
running average episode reward sum: 0.491045401400898
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  2.]), 'previousTarget': array([26.,  2.]), 'currentState': array([25.21033551,  2.15879793,  5.8062328 ]), 'targetState': array([26,  2], dtype=int32), 'currentDistance': 0.8054730249624814}
episode index:4069
target Thresh 32.0
target distance 14.0
model initialize at round 4069
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  7.]), 'previousTarget': array([11.,  7.]), 'currentState': array([25.01119096,  1.98999945,  5.30381912]), 'targetState': array([11,  7], dtype=int32), 'currentDistance': 14.8799723639576}
done in step count: 36
reward sum = 0.42970904489963613
running average episode reward sum: 0.4910303310430353
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  7.]), 'previousTarget': array([11.,  7.]), 'currentState': array([11.69149201,  6.44110531,  2.89971168]), 'targetState': array([11,  7], dtype=int32), 'currentDistance': 0.8891144329502415}
episode index:4070
target Thresh 32.0
target distance 4.0
model initialize at round 4070
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 16.]), 'previousTarget': array([25., 16.]), 'currentState': array([23.90506948, 12.48963996,  1.63892166]), 'targetState': array([25, 16], dtype=int32), 'currentDistance': 3.677159287435124}
done in step count: 6
reward sum = 0.877847674381297
running average episode reward sum: 0.49112534881344505
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 16.]), 'previousTarget': array([25., 16.]), 'currentState': array([24.32449193, 15.19551525,  1.23381333]), 'targetState': array([25, 16], dtype=int32), 'currentDistance': 1.050479350902875}
episode index:4071
target Thresh 32.0
target distance 13.0
model initialize at round 4071
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([20.79194308, 20.69832306,  4.33460939]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 12.755657958742663}
done in step count: 28
reward sum = 0.5585312730167822
running average episode reward sum: 0.4911419023311767
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([21.38568668,  8.8444414 ,  4.97676955]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 1.0442519529714356}
episode index:4072
target Thresh 32.0
target distance 9.0
model initialize at round 4072
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 18.]), 'previousTarget': array([ 8., 18.]), 'currentState': array([ 8.01130284, 26.50153825,  4.60361516]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 8.501545763845593}
done in step count: 16
reward sum = 0.7000190779409188
running average episode reward sum: 0.4911931857035336
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 18.]), 'previousTarget': array([ 8., 18.]), 'currentState': array([ 7.79880297, 18.83097499,  4.73973996]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 0.854985189865212}
episode index:4073
target Thresh 32.0
target distance 25.0
model initialize at round 4073
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.83649506, 13.84665909]), 'previousTarget': array([21.74881264, 13.84018998]), 'currentState': array([ 2.10005336, 17.08284033,  0.48421869]), 'targetState': array([27, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.22353691178484897
running average episode reward sum: 0.49112748705996
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 13.]), 'previousTarget': array([27., 13.]), 'currentState': array([26.25267097, 12.5118818 ,  0.10123506]), 'targetState': array([27, 13], dtype=int32), 'currentDistance': 0.8926141696870166}
episode index:4074
target Thresh 32.0
target distance 17.0
model initialize at round 4074
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([23.04753024, 28.03786798,  0.42023373]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 17.16045819330851}
done in step count: 49
reward sum = 0.3777848729475842
running average episode reward sum: 0.49109967292152756
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([20.65220753, 11.77034039,  4.71894329]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 0.8452123486883675}
episode index:4075
target Thresh 32.0
target distance 13.0
model initialize at round 4075
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  6.]), 'previousTarget': array([13.,  6.]), 'currentState': array([26.       ,  3.       ,  4.8742218]), 'targetState': array([13,  6], dtype=int32), 'currentDistance': 13.341664064126334}
done in step count: 35
reward sum = 0.4924433181101424
running average episode reward sum: 0.491100002569513
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  6.]), 'previousTarget': array([13.,  6.]), 'currentState': array([13.89315656,  6.20298316,  2.99145712]), 'targetState': array([13,  6], dtype=int32), 'currentDistance': 0.9159316553387352}
episode index:4076
target Thresh 32.0
target distance 19.0
model initialize at round 4076
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.67950336,  8.63639806]), 'previousTarget': array([17.69765531,  8.60711423]), 'currentState': array([ 2.01621383, 21.07269015,  1.09883487]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.2164081611837022
running average episode reward sum: 0.4910326265966442
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([20.02945626,  5.98423044,  5.9552634 ]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 0.9706718454467056}
episode index:4077
target Thresh 32.0
target distance 10.0
model initialize at round 4077
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 26.]), 'previousTarget': array([12., 26.]), 'currentState': array([ 7.05295227, 15.9922387 ,  0.10497605]), 'targetState': array([12, 26], dtype=int32), 'currentDistance': 11.163716556777379}
done in step count: 23
reward sum = 0.5992748552843943
running average episode reward sum: 0.4910591695659154
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 26.]), 'previousTarget': array([12., 26.]), 'currentState': array([11.8448835 , 25.00497455,  1.35365954]), 'targetState': array([12, 26], dtype=int32), 'currentDistance': 1.0070435860652316}
episode index:4078
target Thresh 32.0
target distance 11.0
model initialize at round 4078
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  2.]), 'previousTarget': array([26.,  2.]), 'currentState': array([24.98468598, 12.711416  ,  4.71040267]), 'targetState': array([26,  2], dtype=int32), 'currentDistance': 10.759428209609421}
done in step count: 23
reward sum = 0.6389908621797062
running average episode reward sum: 0.4910954362225993
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  2.]), 'previousTarget': array([26.,  2.]), 'currentState': array([25.69702881,  2.92143919,  4.85951906]), 'targetState': array([26,  2], dtype=int32), 'currentDistance': 0.9699699586947756}
episode index:4079
target Thresh 32.0
target distance 3.0
model initialize at round 4079
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 6.]), 'previousTarget': array([8., 6.]), 'currentState': array([5.28109642, 8.99401535, 6.05241672]), 'targetState': array([8, 6], dtype=int32), 'currentDistance': 4.044324984497097}
done in step count: 8
reward sum = 0.8475977547744395
running average episode reward sum: 0.49118281424185223
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 6.]), 'previousTarget': array([8., 6.]), 'currentState': array([7.91542801, 6.78359385, 5.31309729]), 'targetState': array([8, 6], dtype=int32), 'currentDistance': 0.7881444921151133}
episode index:4080
target Thresh 32.0
target distance 19.0
model initialize at round 4080
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9.52067942, 5.7205326 ]), 'previousTarget': array([9.49385478, 5.70632169]), 'currentState': array([26.03946675, 16.99571177,  5.92952975]), 'targetState': array([7, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 94
reward sum = 0.18825720143828492
running average episode reward sum: 0.4911085859613319
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 4.]), 'previousTarget': array([7., 4.]), 'currentState': array([7.75843322, 4.45785887, 3.869752  ]), 'targetState': array([7, 4], dtype=int32), 'currentDistance': 0.8859208165563713}
episode index:4081
target Thresh 32.0
target distance 6.0
model initialize at round 4081
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 28.]), 'previousTarget': array([ 9., 28.]), 'currentState': array([14.9972277 , 25.95678624,  4.40314811]), 'targetState': array([ 9, 28], dtype=int32), 'currentDistance': 6.335729049924097}
done in step count: 13
reward sum = 0.7530419187959553
running average episode reward sum: 0.49117275385276615
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 28.]), 'previousTarget': array([ 9., 28.]), 'currentState': array([ 9.89123328, 27.07613487,  2.63794586]), 'targetState': array([ 9, 28], dtype=int32), 'currentDistance': 1.2836757908642031}
episode index:4082
target Thresh 32.0
target distance 16.0
model initialize at round 4082
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 12.]), 'previousTarget': array([ 3., 12.]), 'currentState': array([18.88370525,  2.9182864 ,  3.60041538]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 18.296710480388846}
done in step count: 47
reward sum = 0.3916041566877787
running average episode reward sum: 0.49114836771581666
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 12.]), 'previousTarget': array([ 3., 12.]), 'currentState': array([ 3.88558935, 12.03830335,  2.8057153 ]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 0.8864173033200534}
episode index:4083
target Thresh 32.0
target distance 15.0
model initialize at round 4083
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  6.]), 'previousTarget': array([24.,  6.]), 'currentState': array([ 9.19079505, 10.7055598 ,  5.43273824]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 15.5388173352708}
done in step count: 35
reward sum = 0.47227254672573116
running average episode reward sum: 0.4911437458203734
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  6.]), 'previousTarget': array([24.,  6.]), 'currentState': array([2.30382289e+01, 5.60150505e+00, 3.81620018e-03]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 1.0410580446716922}
episode index:4084
target Thresh 32.0
target distance 3.0
model initialize at round 4084
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 19.]), 'previousTarget': array([26., 19.]), 'currentState': array([24.32350428, 16.17633949,  0.6543735 ]), 'targetState': array([26, 19], dtype=int32), 'currentDistance': 3.2838539196708996}
done in step count: 6
reward sum = 0.8943339555366693
running average episode reward sum: 0.4912424459941106
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 19.]), 'previousTarget': array([26., 19.]), 'currentState': array([25.58913743, 18.05968608,  1.11536622]), 'targetState': array([26, 19], dtype=int32), 'currentDistance': 1.0261570663199822}
episode index:4085
target Thresh 32.0
target distance 12.0
model initialize at round 4085
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 21.]), 'previousTarget': array([23., 21.]), 'currentState': array([17.        ,  9.        ,  6.07078362]), 'targetState': array([23, 21], dtype=int32), 'currentDistance': 13.416407864998739}
done in step count: 32
reward sum = 0.515085118017442
running average episode reward sum: 0.4912482812050805
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 21.]), 'previousTarget': array([23., 21.]), 'currentState': array([22.6298172 , 20.32479797,  1.46647547]), 'targetState': array([23, 21], dtype=int32), 'currentDistance': 0.7700214861458231}
episode index:4086
target Thresh 32.0
target distance 4.0
model initialize at round 4086
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 19.]), 'previousTarget': array([10., 19.]), 'currentState': array([14.        , 18.        ,  0.84213924]), 'targetState': array([10, 19], dtype=int32), 'currentDistance': 4.123105625617661}
done in step count: 10
reward sum = 0.8036293415790754
running average episode reward sum: 0.4913247140556737
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 19.]), 'previousTarget': array([10., 19.]), 'currentState': array([10.92783206, 19.09521091,  3.08557039]), 'targetState': array([10, 19], dtype=int32), 'currentDistance': 0.9327043706084945}
episode index:4087
target Thresh 32.0
target distance 4.0
model initialize at round 4087
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([19.04251609, 21.72540304,  4.67061028]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 4.809940949295165}
done in step count: 8
reward sum = 0.8365796412733488
running average episode reward sum: 0.4914091697619402
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([16.92036348, 18.63069254,  3.75541157]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 1.1157248869498624}
episode index:4088
target Thresh 32.0
target distance 18.0
model initialize at round 4088
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  6.]), 'previousTarget': array([22.,  6.]), 'currentState': array([ 4.        , 13.        ,  4.48518467]), 'targetState': array([22,  6], dtype=int32), 'currentDistance': 19.313207915827967}
done in step count: 48
reward sum = 0.34669663715589066
running average episode reward sum: 0.4913737790716477
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  6.]), 'previousTarget': array([22.,  6.]), 'currentState': array([21.24768445,  5.65525167,  6.23138224]), 'targetState': array([22,  6], dtype=int32), 'currentDistance': 0.8275446241578642}
episode index:4089
target Thresh 32.0
target distance 3.0
model initialize at round 4089
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 2.]), 'previousTarget': array([9., 2.]), 'currentState': array([6.31396405, 4.61086698, 5.38625021]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 3.7458531072212007}
done in step count: 6
reward sum = 0.8760262455601169
running average episode reward sum: 0.4914678261294688
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 2.]), 'previousTarget': array([9., 2.]), 'currentState': array([8.28234776, 2.45628954, 5.53384746]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 0.8504262945633025}
episode index:4090
target Thresh 32.0
target distance 7.0
model initialize at round 4090
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([15.05340013,  7.9889406 ,  0.0482828 ]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 7.074673503531108}
done in step count: 16
reward sum = 0.7196136149716159
running average episode reward sum: 0.4915235938607918
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([15.76039608, 14.12782815,  1.50720801]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 0.9044853661122777}
episode index:4091
target Thresh 32.0
target distance 19.0
model initialize at round 4091
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 29.]), 'previousTarget': array([21., 29.]), 'currentState': array([18.        , 10.        ,  4.67499769]), 'targetState': array([21, 29], dtype=int32), 'currentDistance': 19.235384061671347}
done in step count: 53
reward sum = 0.31984140628992946
running average episode reward sum: 0.4914816382919817
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 29.]), 'previousTarget': array([21., 29.]), 'currentState': array([21.16121967, 28.18747294,  1.67291123]), 'targetState': array([21, 29], dtype=int32), 'currentDistance': 0.8283670759628765}
episode index:4092
target Thresh 32.0
target distance 8.0
model initialize at round 4092
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 21.]), 'previousTarget': array([10., 21.]), 'currentState': array([15.02269112, 12.99951767,  0.23124665]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 9.446435499654447}
done in step count: 24
reward sum = 0.6070171530482396
running average episode reward sum: 0.4915098658792664
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 21.]), 'previousTarget': array([10., 21.]), 'currentState': array([10.0265516 , 20.26278189,  1.82124905]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 0.7376960942464562}
episode index:4093
target Thresh 32.0
target distance 11.0
model initialize at round 4093
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([20.       , 11.       ,  4.5715366]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 12.529964086141666}
done in step count: 31
reward sum = 0.516595882793883
running average episode reward sum: 0.49151599338706187
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([14.49721322, 21.1166734 ,  1.96805545]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 1.0136502723336167}
episode index:4094
target Thresh 32.0
target distance 5.0
model initialize at round 4094
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.28309014, 9.6062789 , 5.15328729]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 4.614969696362552}
done in step count: 8
reward sum = 0.8309807627085828
running average episode reward sum: 0.49159889076662755
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.1459178 , 5.81135395, 4.32713735]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.8243708077881434}
episode index:4095
target Thresh 32.0
target distance 6.0
model initialize at round 4095
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 20.]), 'previousTarget': array([11., 20.]), 'currentState': array([16.89519891, 24.71635719,  4.11936259]), 'targetState': array([11, 20], dtype=int32), 'currentDistance': 7.5496619357329156}
done in step count: 15
reward sum = 0.7244427807033025
running average episode reward sum: 0.4916557374194441
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 20.]), 'previousTarget': array([11., 20.]), 'currentState': array([11.25310517, 20.89126698,  3.84674773]), 'targetState': array([11, 20], dtype=int32), 'currentDistance': 0.9265090697335566}
episode index:4096
target Thresh 32.0
target distance 6.0
model initialize at round 4096
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 20.]), 'previousTarget': array([ 8., 20.]), 'currentState': array([ 1.9646341 , 16.95040354,  4.33321065]), 'targetState': array([ 8, 20], dtype=int32), 'currentDistance': 6.762076615801954}
done in step count: 15
reward sum = 0.711634296865706
running average episode reward sum: 0.49170943001389034
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 20.]), 'previousTarget': array([ 8., 20.]), 'currentState': array([ 7.12725651, 19.04092635,  0.55375234]), 'targetState': array([ 8, 20], dtype=int32), 'currentDistance': 1.2967279847695505}
episode index:4097
target Thresh 32.0
target distance 5.0
model initialize at round 4097
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 20.]), 'previousTarget': array([21., 20.]), 'currentState': array([16.28532239, 15.87665868,  6.1158739 ]), 'targetState': array([21, 20], dtype=int32), 'currentDistance': 6.263395930281206}
done in step count: 12
reward sum = 0.7688457913748146
running average episode reward sum: 0.49177705723725806
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 20.]), 'previousTarget': array([21., 20.]), 'currentState': array([20.49919385, 19.15310286,  0.851387  ]), 'targetState': array([21, 20], dtype=int32), 'currentDistance': 0.9838910346786035}
episode index:4098
target Thresh 32.0
target distance 7.0
model initialize at round 4098
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([25.98982279, 13.02641288,  2.18297279]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 8.617935335432552}
done in step count: 18
reward sum = 0.6636214584351083
running average episode reward sum: 0.4918189807310853
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([21.80882324,  6.92428177,  4.23473512]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 1.2282067515745227}
episode index:4099
target Thresh 32.0
target distance 15.0
model initialize at round 4099
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([12.        , 18.        ,  4.41075659]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 17.492855684535904}
done in step count: 47
reward sum = 0.39023265403299606
running average episode reward sum: 0.4917942035782321
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([26.06178437,  8.7204568 ,  6.11299683]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 0.9789754724075143}
episode index:4100
target Thresh 32.0
target distance 22.0
model initialize at round 4100
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.66475581,  8.52085402]), 'previousTarget': array([18.66475581,  8.52085402]), 'currentState': array([ 6.        , 24.        ,  1.10905111]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 72
reward sum = 0.12194406430813848
running average episode reward sum: 0.4917040182236186
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([23.21926213,  2.33307899,  5.66494295]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 0.8488187342151319}
episode index:4101
target Thresh 32.0
target distance 4.0
model initialize at round 4101
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 14.]), 'previousTarget': array([ 3., 14.]), 'currentState': array([ 3.        , 18.        ,  3.11747646]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 4.0}
done in step count: 8
reward sum = 0.8307411631097696
running average episode reward sum: 0.49178666989228903
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 14.]), 'previousTarget': array([ 3., 14.]), 'currentState': array([ 2.53355947, 14.99278816,  4.83134215]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 1.0969024965800716}
episode index:4102
target Thresh 32.0
target distance 22.0
model initialize at round 4102
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9.47545314, 7.73326351]), 'previousTarget': array([9.47545314, 7.73326351]), 'currentState': array([26.        , 19.        ,  5.81242964]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 71
reward sum = 0.17106811122846
running average episode reward sum: 0.4917085030488418
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.62196754, 4.84799367, 4.01816976]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.0516353393206248}
episode index:4103
target Thresh 32.0
target distance 8.0
model initialize at round 4103
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([ 3.2590743 , 28.84056405,  5.62025952]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 9.717647481875165}
done in step count: 19
reward sum = 0.6750065825778124
running average episode reward sum: 0.49175316632358085
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([ 8.28296438, 21.87477984,  5.18298787]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 1.131096748111274}
episode index:4104
target Thresh 32.0
target distance 6.0
model initialize at round 4104
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 27.]), 'previousTarget': array([19., 27.]), 'currentState': array([13.        , 26.        ,  1.20645225]), 'targetState': array([19, 27], dtype=int32), 'currentDistance': 6.082762530298219}
done in step count: 12
reward sum = 0.7738683950493563
running average episode reward sum: 0.4918218911052436
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 27.]), 'previousTarget': array([19., 27.]), 'currentState': array([18.03269224, 27.21534008,  0.03940108]), 'targetState': array([19, 27], dtype=int32), 'currentDistance': 0.99098721404084}
episode index:4105
target Thresh 32.0
target distance 10.0
model initialize at round 4105
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([17.05954087, 21.02920688,  0.24716443]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 14.2049059506762}
done in step count: 35
reward sum = 0.46411400850498247
running average episode reward sum: 0.4918151429604311
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.53427267, 11.93096821,  3.8646434 ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.0733820815775217}
episode index:4106
target Thresh 32.0
target distance 16.0
model initialize at round 4106
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.02624101, 21.03425617,  0.66461921]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 16.063797109414104}
done in step count: 39
reward sum = 0.41955547313380054
running average episode reward sum: 0.4917975486897161
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.87077373,  5.85566337,  4.88934252]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.8653665339979169}
episode index:4107
target Thresh 32.0
target distance 11.0
model initialize at round 4107
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 11.]), 'previousTarget': array([25., 11.]), 'currentState': array([14.36186145, 12.92960937,  6.050745  ]), 'targetState': array([25, 11], dtype=int32), 'currentDistance': 10.811724384087327}
done in step count: 22
reward sum = 0.6267974650770518
running average episode reward sum: 0.4918304113762758
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 11.]), 'previousTarget': array([25., 11.]), 'currentState': array([24.14204123, 10.47741545,  6.08997246]), 'targetState': array([25, 11], dtype=int32), 'currentDistance': 1.004583430168822}
episode index:4108
target Thresh 32.0
target distance 9.0
model initialize at round 4108
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 12.]), 'previousTarget': array([16., 12.]), 'currentState': array([9.99710727, 2.97570087, 4.80531156]), 'targetState': array([16, 12], dtype=int32), 'currentDistance': 10.838482181745372}
done in step count: 26
reward sum = 0.5542707043270962
running average episode reward sum: 0.4918456073589847
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 12.]), 'previousTarget': array([16., 12.]), 'currentState': array([16.36469926, 11.17224516,  1.34148766]), 'targetState': array([16, 12], dtype=int32), 'currentDistance': 0.9045350349788891}
episode index:4109
target Thresh 32.0
target distance 6.0
model initialize at round 4109
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 11.]), 'previousTarget': array([20., 11.]), 'currentState': array([25.68659071, 17.01766528,  3.27168766]), 'targetState': array([20, 11], dtype=int32), 'currentDistance': 8.279469144558261}
done in step count: 20
reward sum = 0.6833323207714443
running average episode reward sum: 0.4918921978002042
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 11.]), 'previousTarget': array([20., 11.]), 'currentState': array([19.79204231, 11.83980465,  4.35264549]), 'targetState': array([20, 11], dtype=int32), 'currentDistance': 0.8651694903423157}
episode index:4110
target Thresh 32.0
target distance 10.0
model initialize at round 4110
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 13.]), 'previousTarget': array([12., 13.]), 'currentState': array([22.       ,  6.       ,  4.1886896]), 'targetState': array([12, 13], dtype=int32), 'currentDistance': 12.206555615733702}
done in step count: 28
reward sum = 0.5579989756415391
running average episode reward sum: 0.4919082782618538
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 13.]), 'previousTarget': array([12., 13.]), 'currentState': array([12.78290268, 12.43081225,  2.66745601]), 'targetState': array([12, 13], dtype=int32), 'currentDistance': 0.9679417846543176}
episode index:4111
target Thresh 32.0
target distance 4.0
model initialize at round 4111
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'previousTarget': array([10., 17.]), 'currentState': array([ 6.37794118, 19.67305656,  5.62735078]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 4.501615430253787}
done in step count: 7
reward sum = 0.8530131822776663
running average episode reward sum: 0.4919960956023246
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'previousTarget': array([10., 17.]), 'currentState': array([ 9.3032615 , 17.76935606,  5.53188251]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 1.0379563025252407}
episode index:4112
target Thresh 32.0
target distance 3.0
model initialize at round 4112
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  4.]), 'previousTarget': array([27.,  4.]), 'currentState': array([24.02949801,  7.05708151,  0.85651565]), 'targetState': array([27,  4], dtype=int32), 'currentDistance': 4.26258483075996}
done in step count: 10
reward sum = 0.8030143467771025
running average episode reward sum: 0.4920717139468845
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  4.]), 'previousTarget': array([27.,  4.]), 'currentState': array([26.85577494,  4.77453947,  5.00396676]), 'targetState': array([27,  4], dtype=int32), 'currentDistance': 0.7878529421695118}
episode index:4113
target Thresh 32.0
target distance 15.0
model initialize at round 4113
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.85786438, 27.14213562]), 'previousTarget': array([12.85786438, 27.14213562]), 'currentState': array([27.        , 13.        ,  3.81926437]), 'targetState': array([12, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.31268785889563183
running average episode reward sum: 0.49202811067633245
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 28.]), 'previousTarget': array([12., 28.]), 'currentState': array([12.90544659, 27.49624704,  2.42988919]), 'targetState': array([12, 28], dtype=int32), 'currentDistance': 1.0361469816204447}
episode index:4114
target Thresh 32.0
target distance 4.0
model initialize at round 4114
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([15.65034945,  9.64289568,  3.88551322]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 5.157105778324736}
done in step count: 9
reward sum = 0.8204133322352388
running average episode reward sum: 0.4921079126742811
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([12.29915204,  6.82138204,  3.97677216]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 0.8741626806878362}
episode index:4115
target Thresh 32.0
target distance 4.0
model initialize at round 4115
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  9.]), 'previousTarget': array([22.,  9.]), 'currentState': array([25.99148689, 12.03706587,  2.0417197 ]), 'targetState': array([22,  9], dtype=int32), 'currentDistance': 5.015549487281618}
done in step count: 12
reward sum = 0.7814093136989315
running average episode reward sum: 0.49217819970076915
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  9.]), 'previousTarget': array([22.,  9.]), 'currentState': array([22.28433536,  9.88325579,  3.90795206]), 'targetState': array([22,  9], dtype=int32), 'currentDistance': 0.92789406115083}
episode index:4116
target Thresh 32.0
target distance 9.0
model initialize at round 4116
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 17.]), 'previousTarget': array([ 9., 17.]), 'currentState': array([11.99479952,  7.99641831,  3.49221063]), 'targetState': array([ 9, 17], dtype=int32), 'currentDistance': 9.488588276580291}
done in step count: 22
reward sum = 0.634357928119302
running average episode reward sum: 0.4922127344902805
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 17.]), 'previousTarget': array([ 9., 17.]), 'currentState': array([ 8.95926407, 16.05662398,  1.86543376]), 'targetState': array([ 9, 17], dtype=int32), 'currentDistance': 0.9442551196986615}
episode index:4117
target Thresh 32.0
target distance 9.0
model initialize at round 4117
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([25.99975602, 20.94092791,  4.5260359 ]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 9.06185643627585}
done in step count: 21
reward sum = 0.6533244424579716
running average episode reward sum: 0.4922518582658919
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([17.83712882, 22.0089743 ,  2.65638826]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 0.8371769175098107}
episode index:4118
target Thresh 32.0
target distance 15.0
model initialize at round 4118
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 12.]), 'previousTarget': array([16., 12.]), 'currentState': array([ 9.29111449, 26.8668615 ,  5.67221612]), 'targetState': array([16, 12], dtype=int32), 'currentDistance': 16.31050935617105}
done in step count: 38
reward sum = 0.45819658347304704
running average episode reward sum: 0.49224359041573584
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 12.]), 'previousTarget': array([16., 12.]), 'currentState': array([15.40212105, 12.87579393,  5.23642549]), 'targetState': array([16, 12], dtype=int32), 'currentDistance': 1.0604122995130538}
episode index:4119
target Thresh 32.0
target distance 19.0
model initialize at round 4119
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9.23560066, 23.44400689]), 'previousTarget': array([ 9.23886  , 23.4327075]), 'currentState': array([17.03887237,  5.0290958 ,  0.88993219]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.33453331113723417
running average episode reward sum: 0.49220531122173616
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([ 9.67613161, 23.07987053,  2.07205589]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 1.1418372050383545}
episode index:4120
target Thresh 32.0
target distance 5.0
model initialize at round 4120
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([ 8.03756006, 25.72404808,  5.00872657]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 6.165838186447873}
done in step count: 11
reward sum = 0.7917915458187199
running average episode reward sum: 0.4922780086822062
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([11.33216061, 21.76856646,  5.3694332 ]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 1.0181865460547423}
episode index:4121
target Thresh 32.0
target distance 6.0
model initialize at round 4121
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 22.]), 'previousTarget': array([20., 22.]), 'currentState': array([26.01234929, 25.04322535,  1.54501402]), 'targetState': array([20, 22], dtype=int32), 'currentDistance': 6.738661921298246}
done in step count: 15
reward sum = 0.7205938653476129
running average episode reward sum: 0.4923333982641241
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 22.]), 'previousTarget': array([20., 22.]), 'currentState': array([20.66766874, 22.99705215,  3.70344792]), 'targetState': array([20, 22], dtype=int32), 'currentDistance': 1.1999560538278116}
episode index:4122
target Thresh 32.0
target distance 10.0
model initialize at round 4122
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  3.]), 'previousTarget': array([17.,  3.]), 'currentState': array([6.99740336, 8.84437054, 4.91765967]), 'targetState': array([17,  3], dtype=int32), 'currentDistance': 11.584843826311838}
done in step count: 26
reward sum = 0.5907771264702906
running average episode reward sum: 0.49235727498694876
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  3.]), 'previousTarget': array([17.,  3.]), 'currentState': array([16.21551979,  3.22461897,  5.81126176]), 'targetState': array([17,  3], dtype=int32), 'currentDistance': 0.8160042130156819}
episode index:4123
target Thresh 32.0
target distance 11.0
model initialize at round 4123
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 4.]), 'previousTarget': array([5., 4.]), 'currentState': array([14.65632738, 14.99366628,  3.30602947]), 'targetState': array([5, 4], dtype=int32), 'currentDistance': 14.632339416217452}
done in step count: 32
reward sum = 0.5080261881126692
running average episode reward sum: 0.49236107443242055
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 4.]), 'previousTarget': array([5., 4.]), 'currentState': array([5.58450827, 4.87862567, 4.0330989 ]), 'targetState': array([5, 4], dtype=int32), 'currentDistance': 1.0552881080027943}
episode index:4124
target Thresh 32.0
target distance 20.0
model initialize at round 4124
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.9223227 , 26.61161351]), 'previousTarget': array([13.9223227 , 26.61161351]), 'currentState': array([10.        ,  7.        ,  3.31875107]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.31397877379895633
running average episode reward sum: 0.4923178302383276
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 27.]), 'previousTarget': array([14., 27.]), 'currentState': array([14.44604857, 26.00040103,  1.39712508]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 1.094603772504984}
episode index:4125
target Thresh 32.0
target distance 14.0
model initialize at round 4125
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 24.]), 'previousTarget': array([12., 24.]), 'currentState': array([11.01396782, 10.00283473,  0.41917473]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 14.03185287348527}
done in step count: 35
reward sum = 0.5055228316645327
running average episode reward sum: 0.49232103067493116
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 24.]), 'previousTarget': array([12., 24.]), 'currentState': array([12.40171703, 23.2593023 ,  1.74265359]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 0.8426207081776645}
episode index:4126
target Thresh 32.0
target distance 22.0
model initialize at round 4126
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7.18488375, 21.69822065]), 'previousTarget': array([ 7.18339664, 21.70226409]), 'currentState': array([26.9967953 , 18.96177031,  4.46873957]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 75
reward sum = 0.26095405053450305
running average episode reward sum: 0.4922649688915194
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'previousTarget': array([ 5., 22.]), 'currentState': array([ 5.91728252, 22.05619943,  3.03928622]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 0.9190025032429462}
episode index:4127
target Thresh 32.0
target distance 7.0
model initialize at round 4127
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 5.]), 'previousTarget': array([9., 5.]), 'currentState': array([2.        , 6.        , 2.44461715]), 'targetState': array([9, 5], dtype=int32), 'currentDistance': 7.0710678118654755}
done in step count: 18
reward sum = 0.6720476321108422
running average episode reward sum: 0.49230852089326826
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 5.]), 'previousTarget': array([9., 5.]), 'currentState': array([8.28745297, 4.87186968, 6.01765668]), 'targetState': array([9, 5], dtype=int32), 'currentDistance': 0.7239755827802012}
episode index:4128
target Thresh 32.0
target distance 22.0
model initialize at round 4128
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9.28688301, 22.3892708 ]), 'previousTarget': array([ 9.26234812, 22.29527642]), 'currentState': array([4.01696958, 3.09605924, 1.34280826]), 'targetState': array([10, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.2990674413401058
running average episode reward sum: 0.4922617199536817
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 25.]), 'previousTarget': array([10., 25.]), 'currentState': array([10.10213578, 24.01273466,  1.37072006]), 'targetState': array([10, 25], dtype=int32), 'currentDistance': 0.9925344197554906}
episode index:4129
target Thresh 32.0
target distance 10.0
model initialize at round 4129
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 20.]), 'previousTarget': array([ 5., 20.]), 'currentState': array([15.02386836, 11.06407427,  1.45803541]), 'targetState': array([ 5, 20], dtype=int32), 'currentDistance': 13.42865240974481}
done in step count: 30
reward sum = 0.5184505359044109
running average episode reward sum: 0.49226806107134524
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 20.]), 'previousTarget': array([ 5., 20.]), 'currentState': array([ 5.84566268, 20.09187591,  2.75844917]), 'targetState': array([ 5, 20], dtype=int32), 'currentDistance': 0.8506389112203253}
episode index:4130
target Thresh 32.0
target distance 10.0
model initialize at round 4130
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 24.]), 'previousTarget': array([10., 24.]), 'currentState': array([20.        , 17.        ,  4.96116394]), 'targetState': array([10, 24], dtype=int32), 'currentDistance': 12.206555615733702}
done in step count: 31
reward sum = 0.5113998492152051
running average episode reward sum: 0.49227269234419535
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 24.]), 'previousTarget': array([10., 24.]), 'currentState': array([10.78034381, 24.15561314,  2.63252276]), 'targetState': array([10, 24], dtype=int32), 'currentDistance': 0.7957084291864186}
episode index:4131
target Thresh 32.0
target distance 16.0
model initialize at round 4131
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([24.00310534, 18.07422901,  1.76277575]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 16.039119320462483}
done in step count: 38
reward sum = 0.44478284200132695
running average episode reward sum: 0.4922611991567939
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([ 8.99278753, 17.27971333,  3.01880554]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 1.0314391087096144}
episode index:4132
target Thresh 32.0
target distance 18.0
model initialize at round 4132
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 29.]), 'previousTarget': array([ 2., 29.]), 'currentState': array([ 8.       , 11.       ,  2.6992321]), 'targetState': array([ 2, 29], dtype=int32), 'currentDistance': 18.973665961010276}
done in step count: 46
reward sum = 0.3853371300270447
running average episode reward sum: 0.49223532834403566
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 29.]), 'previousTarget': array([ 2., 29.]), 'currentState': array([ 2.58400529, 28.15243325,  1.69206763]), 'targetState': array([ 2, 29], dtype=int32), 'currentDistance': 1.029286927053896}
episode index:4133
target Thresh 32.0
target distance 9.0
model initialize at round 4133
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([11.        , 14.        ,  0.51960576]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 9.486832980505138}
done in step count: 21
reward sum = 0.6282844518925037
running average episode reward sum: 0.4922682381465389
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.49945405,  5.77679447,  4.91946102]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.9240973388093815}
episode index:4134
target Thresh 32.0
target distance 6.0
model initialize at round 4134
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 9.00138306, 15.00471716,  1.53809142]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 7.814332507547524}
done in step count: 18
reward sum = 0.6684480438960272
running average episode reward sum: 0.4923108451128629
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.76382811, 10.65096484,  3.75334756]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.0035878597663104}
episode index:4135
target Thresh 32.0
target distance 16.0
model initialize at round 4135
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  2.]), 'previousTarget': array([18.,  2.]), 'currentState': array([ 6.97120332, 17.95275359,  4.41751361]), 'targetState': array([18,  2], dtype=int32), 'currentDistance': 19.393934702317562}
done in step count: 45
reward sum = 0.37157097387052074
running average episode reward sum: 0.49228165268751417
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  2.]), 'previousTarget': array([18.,  2.]), 'currentState': array([17.27547052,  2.99283152,  5.35684978]), 'targetState': array([18,  2], dtype=int32), 'currentDistance': 1.2290880318898392}
episode index:4136
target Thresh 32.0
target distance 7.0
model initialize at round 4136
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([5.97982634, 7.97859575, 4.2090795 ]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 7.020206286398203}
done in step count: 17
reward sum = 0.6984291820019313
running average episode reward sum: 0.4923314828855597
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([12.09499261,  7.84145192,  6.13589381]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 0.918790438113321}
episode index:4137
target Thresh 32.0
target distance 15.0
model initialize at round 4137
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7.37889464, 23.64636501]), 'previousTarget': array([ 7.37889464, 23.64636501]), 'currentState': array([22.       , 10.       ,  2.4440558]), 'targetState': array([ 7, 24], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 50
reward sum = 0.3577542274184984
running average episode reward sum: 0.4922989605908601
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 24.]), 'previousTarget': array([ 7., 24.]), 'currentState': array([ 7.99102438, 23.49547896,  2.40547965]), 'targetState': array([ 7, 24], dtype=int32), 'currentDistance': 1.112057015484257}
episode index:4138
target Thresh 32.0
target distance 24.0
model initialize at round 4138
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7.93124113, 19.63062098]), 'previousTarget': array([ 8.02633404, 19.67544468]), 'currentState': array([26.92001595, 25.90966792,  3.80213648]), 'targetState': array([ 3, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 77
reward sum = 0.2341334231748938
running average episode reward sum: 0.4922365866992399
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 18.]), 'previousTarget': array([ 3., 18.]), 'currentState': array([ 3.92738429, 18.56829927,  3.46327991]), 'targetState': array([ 3, 18], dtype=int32), 'currentDistance': 1.0876606481185038}
episode index:4139
target Thresh 32.0
target distance 13.0
model initialize at round 4139
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 24.]), 'previousTarget': array([12., 24.]), 'currentState': array([24.88990662, 29.08211856,  2.75322127]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 13.855598929895114}
done in step count: 29
reward sum = 0.522791605334824
running average episode reward sum: 0.49224396713852386
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 24.]), 'previousTarget': array([12., 24.]), 'currentState': array([12.81259599, 24.49798856,  3.47213175]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 0.9530502879994608}
episode index:4140
target Thresh 32.0
target distance 16.0
model initialize at round 4140
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  2.]), 'previousTarget': array([23.,  2.]), 'currentState': array([16.06215919, 17.94026324,  5.32940239]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 17.384637683349197}
done in step count: 40
reward sum = 0.4406391062965028
running average episode reward sum: 0.49223150520642
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  2.]), 'previousTarget': array([23.,  2.]), 'currentState': array([22.2926662 ,  2.88875705,  5.15950776]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 1.1358741958186962}
episode index:4141
target Thresh 32.0
target distance 7.0
model initialize at round 4141
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 19.]), 'previousTarget': array([ 8., 19.]), 'currentState': array([ 5.        , 12.        ,  5.17780638]), 'targetState': array([ 8, 19], dtype=int32), 'currentDistance': 7.615773105863908}
done in step count: 21
reward sum = 0.6611673580758971
running average episode reward sum: 0.49227229126457295
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 19.]), 'previousTarget': array([ 8., 19.]), 'currentState': array([ 7.98748153, 18.09976694,  1.12138414]), 'targetState': array([ 8, 19], dtype=int32), 'currentDistance': 0.9003200974307569}
episode index:4142
target Thresh 32.0
target distance 1.0
model initialize at round 4142
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([ 4.93644371, 13.97654895,  3.36384952]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 0.9367373006163966}
done in step count: 0
reward sum = 0.9971144263546474
running average episode reward sum: 0.49239414550910354
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([ 4.93644371, 13.97654895,  3.36384952]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 0.9367373006163966}
episode index:4143
target Thresh 32.0
target distance 1.0
model initialize at round 4143
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 26.]), 'previousTarget': array([ 5., 26.]), 'currentState': array([ 5.73362656, 26.76568091,  3.76811408]), 'targetState': array([ 5, 26], dtype=int32), 'currentDistance': 1.0604127438057276}
done in step count: 0
reward sum = 0.9935368575040542
running average episode reward sum: 0.49251507763072394
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 26.]), 'previousTarget': array([ 5., 26.]), 'currentState': array([ 5.73362656, 26.76568091,  3.76811408]), 'targetState': array([ 5, 26], dtype=int32), 'currentDistance': 1.0604127438057276}
episode index:4144
target Thresh 32.0
target distance 6.0
model initialize at round 4144
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  3.]), 'previousTarget': array([20.,  3.]), 'currentState': array([26.        ,  8.        ,  1.60691354]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 7.810249675906655}
done in step count: 21
reward sum = 0.6685230791920724
running average episode reward sum: 0.4925575403572767
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  3.]), 'previousTarget': array([20.,  3.]), 'currentState': array([20.26354715,  3.91551508,  3.90619396]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 0.9526935305443128}
episode index:4145
target Thresh 32.0
target distance 3.0
model initialize at round 4145
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([15.98263428,  9.01912694,  2.05546975]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 3.172791808982227}
done in step count: 8
reward sum = 0.8416557819795314
running average episode reward sum: 0.49264174157329754
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([18.21554306,  9.77111369,  6.13386448]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 0.8171668343076647}
episode index:4146
target Thresh 32.0
target distance 15.0
model initialize at round 4146
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 19.]), 'previousTarget': array([13., 19.]), 'currentState': array([10.2470127 ,  4.20027198,  0.83803061]), 'targetState': array([13, 19], dtype=int32), 'currentDistance': 15.053600519880844}
done in step count: 32
reward sum = 0.4932276540431325
running average episode reward sum: 0.49264188285915955
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 19.]), 'previousTarget': array([13., 19.]), 'currentState': array([13.35703862, 18.06093143,  1.44824592]), 'targetState': array([13, 19], dtype=int32), 'currentDistance': 1.0046523506476273}
episode index:4147
target Thresh 32.0
target distance 9.0
model initialize at round 4147
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 22.]), 'previousTarget': array([25., 22.]), 'currentState': array([15.97752064, 12.92210677,  4.68393087]), 'targetState': array([25, 22], dtype=int32), 'currentDistance': 12.79895617611275}
done in step count: 28
reward sum = 0.5129123818218966
running average episode reward sum: 0.4926467696718314
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 22.]), 'previousTarget': array([25., 22.]), 'currentState': array([24.42947423, 21.02770286,  0.77850567]), 'targetState': array([25, 22], dtype=int32), 'currentDistance': 1.1273248766645596}
episode index:4148
target Thresh 32.0
target distance 13.0
model initialize at round 4148
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([10.98211536, 14.99013414,  3.89819312]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 13.356879223752088}
done in step count: 35
reward sum = 0.4947691839844369
running average episode reward sum: 0.49264728122023166
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([23.02334467, 12.53377165,  5.9934819 ]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 1.112999462453988}
episode index:4149
target Thresh 32.0
target distance 2.0
model initialize at round 4149
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  2.]), 'previousTarget': array([17.,  2.]), 'currentState': array([15.24644078,  1.84882899,  5.86266059]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 1.760063244849306}
done in step count: 3
reward sum = 0.9459146375120535
running average episode reward sum: 0.4927565022699405
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  2.]), 'previousTarget': array([17.,  2.]), 'currentState': array([16.28569875,  1.68608446,  0.08711193]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 0.7802366603006583}
episode index:4150
target Thresh 32.0
target distance 10.0
model initialize at round 4150
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 28.]), 'previousTarget': array([17., 28.]), 'currentState': array([ 6.94093072, 18.17836596,  1.68475413]), 'targetState': array([17, 28], dtype=int32), 'currentDistance': 14.058782668411954}
done in step count: 30
reward sum = 0.5165807565206485
running average episode reward sum: 0.4927622416711091
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 28.]), 'previousTarget': array([17., 28.]), 'currentState': array([16.53705406, 27.28953145,  0.73682315]), 'targetState': array([17, 28], dtype=int32), 'currentDistance': 0.8479885037637391}
episode index:4151
target Thresh 32.0
target distance 21.0
model initialize at round 4151
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.37435405, 12.86259254]), 'previousTarget': array([22.3829006 , 12.87838597]), 'currentState': array([4.02229192, 4.91263558, 5.21471834]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.2724587624171024
running average episode reward sum: 0.4927091820662791
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 14.]), 'previousTarget': array([25., 14.]), 'currentState': array([24.17713855, 13.15718222,  0.44441526]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 1.1778976133593781}
episode index:4152
target Thresh 32.0
target distance 6.0
model initialize at round 4152
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 10.]), 'previousTarget': array([25., 10.]), 'currentState': array([19.1305956 , 14.74145823,  5.24708262]), 'targetState': array([25, 10], dtype=int32), 'currentDistance': 7.545285559560551}
done in step count: 15
reward sum = 0.7392926679955353
running average episode reward sum: 0.49276855685219995
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 10.]), 'previousTarget': array([25., 10.]), 'currentState': array([24.16844066, 10.24853659,  5.5917691 ]), 'targetState': array([25, 10], dtype=int32), 'currentDistance': 0.867906316054421}
episode index:4153
target Thresh 32.0
target distance 7.0
model initialize at round 4153
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 24.]), 'previousTarget': array([11., 24.]), 'currentState': array([17.67137954, 20.21330947,  2.52307412]), 'targetState': array([11, 24], dtype=int32), 'currentDistance': 7.671136171559427}
done in step count: 16
reward sum = 0.7284359326979931
running average episode reward sum: 0.49282528948962073
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 24.]), 'previousTarget': array([11., 24.]), 'currentState': array([11.72310037, 24.02039731,  2.71726727]), 'targetState': array([11, 24], dtype=int32), 'currentDistance': 0.7233879971474809}
episode index:4154
target Thresh 32.0
target distance 20.0
model initialize at round 4154
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9.12936002, 23.92706571]), 'previousTarget': array([ 9.11145618, 23.94427191]), 'currentState': array([26.9901238 , 14.92744811,  4.34156525]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.2730734934585924
running average episode reward sum: 0.4927724009707204
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([ 7.96994058, 24.81829798,  2.54695083]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 0.9868132306749268}
episode index:4155
target Thresh 32.0
target distance 15.0
model initialize at round 4155
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 8.]), 'previousTarget': array([9., 8.]), 'currentState': array([23.6745992 , 13.04133773,  3.14091308]), 'targetState': array([9, 8], dtype=int32), 'currentDistance': 15.516408982623682}
done in step count: 39
reward sum = 0.4909696892391465
running average episode reward sum: 0.492771967209476
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 8.]), 'previousTarget': array([9., 8.]), 'currentState': array([9.99900421, 8.67504834, 3.49493679]), 'targetState': array([9, 8], dtype=int32), 'currentDistance': 1.205694683007311}
episode index:4156
target Thresh 32.0
target distance 7.0
model initialize at round 4156
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 20.]), 'previousTarget': array([ 9., 20.]), 'currentState': array([14.        , 13.        ,  6.04778069]), 'targetState': array([ 9, 20], dtype=int32), 'currentDistance': 8.602325267042627}
done in step count: 22
reward sum = 0.6402050504589661
running average episode reward sum: 0.49280743343109007
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 20.]), 'previousTarget': array([ 9., 20.]), 'currentState': array([ 9.85616314, 19.4483489 ,  2.24189659]), 'targetState': array([ 9, 20], dtype=int32), 'currentDistance': 1.0184960737364959}
episode index:4157
target Thresh 32.0
target distance 16.0
model initialize at round 4157
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 14.]), 'previousTarget': array([19., 14.]), 'currentState': array([ 3.        , 17.        ,  3.74051023]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 16.278820596099706}
done in step count: 44
reward sum = 0.4109713657401096
running average episode reward sum: 0.49278775183712875
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 14.]), 'previousTarget': array([19., 14.]), 'currentState': array([18.03604961, 14.10982977,  6.18594121]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 0.9701870590565573}
episode index:4158
target Thresh 32.0
target distance 18.0
model initialize at round 4158
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 21.]), 'previousTarget': array([23., 21.]), 'currentState': array([ 5.00738801, 17.95490009,  5.12726116]), 'targetState': array([23, 21], dtype=int32), 'currentDistance': 18.24847170541698}
done in step count: 42
reward sum = 0.3856602590352432
running average episode reward sum: 0.49276199384414926
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 21.]), 'previousTarget': array([23., 21.]), 'currentState': array([22.02628965, 20.70492662,  0.22906449]), 'targetState': array([23, 21], dtype=int32), 'currentDistance': 1.017438026261891}
episode index:4159
target Thresh 32.0
target distance 11.0
model initialize at round 4159
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 16.]), 'previousTarget': array([19., 16.]), 'currentState': array([ 8.30871789, 21.07123562,  0.10567343]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 11.833044575378734}
done in step count: 28
reward sum = 0.5859746263038655
running average episode reward sum: 0.4927844007269521
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 16.]), 'previousTarget': array([19., 16.]), 'currentState': array([18.09120076, 15.77475591,  5.85262337]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 0.9362964066210863}
episode index:4160
target Thresh 32.0
target distance 3.0
model initialize at round 4160
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 29.]), 'previousTarget': array([16., 29.]), 'currentState': array([18.84903946, 26.31478639,  2.08395994]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 3.9150220961781566}
done in step count: 8
reward sum = 0.8657740580454749
running average episode reward sum: 0.49287404015432984
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 29.]), 'previousTarget': array([16., 29.]), 'currentState': array([16.84759762, 28.76667069,  2.43756051]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 0.879127004591865}
episode index:4161
target Thresh 32.0
target distance 21.0
model initialize at round 4161
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8.21992184, 15.31512286]), 'previousTarget': array([ 8.28336929, 15.28013989]), 'currentState': array([25.94050636,  6.04253509,  2.56778893]), 'targetState': array([ 5, 17], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 56
reward sum = 0.2755525249072815
running average episode reward sum: 0.4928218245091479
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 17.]), 'previousTarget': array([ 5., 17.]), 'currentState': array([ 5.7998094 , 16.8835658 ,  2.71586071]), 'targetState': array([ 5, 17], dtype=int32), 'currentDistance': 0.8082400651847584}
episode index:4162
target Thresh 32.0
target distance 5.0
model initialize at round 4162
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 22.]), 'previousTarget': array([24., 22.]), 'currentState': array([19.        , 24.        ,  4.79692221]), 'targetState': array([24, 22], dtype=int32), 'currentDistance': 5.385164807134504}
done in step count: 11
reward sum = 0.789225785398013
running average episode reward sum: 0.4928930241154148
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 22.]), 'previousTarget': array([24., 22.]), 'currentState': array([23.19248676, 21.99660365,  5.96712413]), 'targetState': array([24, 22], dtype=int32), 'currentDistance': 0.8075203864679221}
episode index:4163
target Thresh 32.0
target distance 12.0
model initialize at round 4163
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([21.00943967, 21.95789005,  4.6842491 ]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 12.047580442931269}
done in step count: 31
reward sum = 0.5545006925920878
running average episode reward sum: 0.4929078194248473
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([ 9.81340414, 21.39205131,  3.33908675]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 0.9029565438827233}
episode index:4164
target Thresh 32.0
target distance 7.0
model initialize at round 4164
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 12.]), 'previousTarget': array([11., 12.]), 'currentState': array([ 4.24171422, 10.82980952,  5.85320598]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 6.858846291424833}
done in step count: 16
reward sum = 0.7558492087315104
running average episode reward sum: 0.4929709506107552
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 12.]), 'previousTarget': array([11., 12.]), 'currentState': array([10.05041561, 11.58314701,  0.10573688]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 1.037052039753499}
episode index:4165
target Thresh 32.0
target distance 9.0
model initialize at round 4165
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 18.]), 'previousTarget': array([23., 18.]), 'currentState': array([18.28776708,  9.14455449,  0.60438266]), 'targetState': array([23, 18], dtype=int32), 'currentDistance': 10.031154184840663}
done in step count: 22
reward sum = 0.6569311593657178
running average episode reward sum: 0.49301030735793594
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 18.]), 'previousTarget': array([23., 18.]), 'currentState': array([22.43820832, 17.06839595,  1.08517095]), 'targetState': array([23, 18], dtype=int32), 'currentDistance': 1.0878860245542339}
episode index:4166
target Thresh 32.0
target distance 1.0
model initialize at round 4166
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.96809833,  2.94808183,  4.01605341]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.0524017986853194}
done in step count: 5
reward sum = 0.9175908146614288
running average episode reward sum: 0.4931121985283951
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.646086  ,  3.0430491 ,  2.10229215]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.02029904487761}
episode index:4167
target Thresh 32.0
target distance 20.0
model initialize at round 4167
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.09938325,  6.48395872]), 'previousTarget': array([19.86588292,  6.62070537]), 'currentState': array([ 5.34389843, 19.98490934,  6.00504108]), 'targetState': array([25,  2], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 85
reward sum = 0.18548655799607439
running average episode reward sum: 0.4930383919927588
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  2.]), 'previousTarget': array([25.,  2.]), 'currentState': array([24.04363667,  2.60723591,  5.37860644]), 'targetState': array([25,  2], dtype=int32), 'currentDistance': 1.1328575717396978}
episode index:4168
target Thresh 32.0
target distance 10.0
model initialize at round 4168
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  6.]), 'previousTarget': array([17.,  6.]), 'currentState': array([23.        , 16.        ,  6.05804521]), 'targetState': array([17,  6], dtype=int32), 'currentDistance': 11.6619037896906}
done in step count: 28
reward sum = 0.5603693809150424
running average episode reward sum: 0.49305454238588
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  6.]), 'previousTarget': array([17.,  6.]), 'currentState': array([17.54789093,  6.86652285,  4.04256849]), 'targetState': array([17,  6], dtype=int32), 'currentDistance': 1.0252055043635213}
episode index:4169
target Thresh 32.0
target distance 20.0
model initialize at round 4169
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.79270645,  5.7615699 ]), 'previousTarget': array([16.79270645,  5.7615699 ]), 'currentState': array([25.        , 24.        ,  5.31852388]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 72
reward sum = 0.24826160052860913
running average episode reward sum: 0.49299583904250893
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.56935168,  4.75821107,  3.96918356]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.9481800233812443}
episode index:4170
target Thresh 32.0
target distance 12.0
model initialize at round 4170
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 25.]), 'previousTarget': array([19., 25.]), 'currentState': array([ 7.18401806, 16.92193042,  6.08008063]), 'targetState': array([19, 25], dtype=int32), 'currentDistance': 14.313372675628486}
done in step count: 36
reward sum = 0.5071830240415095
running average episode reward sum: 0.4929992404294662
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 25.]), 'previousTarget': array([19., 25.]), 'currentState': array([18.56297931, 24.12838488,  0.89353615]), 'targetState': array([19, 25], dtype=int32), 'currentDistance': 0.975038460022682}
episode index:4171
target Thresh 32.0
target distance 22.0
model initialize at round 4171
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8.48906088, 12.57265691]), 'previousTarget': array([ 8.48906088, 12.57265691]), 'currentState': array([27.      ,  5.      ,  5.139595]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 72
reward sum = 0.2153569964279941
running average episode reward sum: 0.4929326914735694
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 14.]), 'previousTarget': array([ 5., 14.]), 'currentState': array([ 5.86198656, 13.95388284,  2.85130597]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 0.8632193353936869}
episode index:4172
target Thresh 32.0
target distance 8.0
model initialize at round 4172
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([ 8.01954156, 14.67671212,  4.99810556]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 9.793506898916696}
done in step count: 22
reward sum = 0.6502199928857547
running average episode reward sum: 0.4929703831345836
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.15826794,  9.36645466,  5.58780899]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.918042420098272}
episode index:4173
target Thresh 32.0
target distance 12.0
model initialize at round 4173
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([19.83790471,  2.16207411,  2.26627061]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 12.444516945280558}
done in step count: 32
reward sum = 0.5711703406198354
running average episode reward sum: 0.49298911815075164
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([16.37989616, 13.19721001,  1.76257728]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 0.8881401180336738}
episode index:4174
target Thresh 32.0
target distance 8.0
model initialize at round 4174
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 16.]), 'previousTarget': array([23., 16.]), 'currentState': array([15.        , 11.        ,  4.16486683]), 'targetState': array([23, 16], dtype=int32), 'currentDistance': 9.433981132056603}
done in step count: 22
reward sum = 0.6087564186200898
running average episode reward sum: 0.4930168468454748
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 16.]), 'previousTarget': array([23., 16.]), 'currentState': array([22.32274429, 15.34618906,  0.68326735]), 'targetState': array([23, 16], dtype=int32), 'currentDistance': 0.9413522375218366}
episode index:4175
target Thresh 32.0
target distance 24.0
model initialize at round 4175
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.15661389,  9.01421231]), 'previousTarget': array([26.16738911,  9.01733854]), 'currentState': array([26.9363177 , 28.99900808,  3.40966749]), 'targetState': array([26,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 92
reward sum = 0.20394778536014957
running average episode reward sum: 0.4929476253269199
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  5.]), 'previousTarget': array([26.,  5.]), 'currentState': array([25.43572459,  5.88187734,  5.01785464]), 'targetState': array([26,  5], dtype=int32), 'currentDistance': 1.0469548103641597}
episode index:4176
target Thresh 32.0
target distance 10.0
model initialize at round 4176
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 27.]), 'previousTarget': array([19., 27.]), 'currentState': array([22.80588375, 17.28877153,  2.10926561]), 'targetState': array([19, 27], dtype=int32), 'currentDistance': 10.430374369711442}
done in step count: 22
reward sum = 0.6445645816380767
running average episode reward sum: 0.49298392337726965
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 27.]), 'previousTarget': array([19., 27.]), 'currentState': array([19.36031855, 26.15459528,  1.95961292]), 'targetState': array([19, 27], dtype=int32), 'currentDistance': 0.918987811015839}
episode index:4177
target Thresh 32.0
target distance 18.0
model initialize at round 4177
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8.80205666, 8.81412187]), 'previousTarget': array([8.85786438, 8.85786438]), 'currentState': array([22.92177123, 22.97864305,  3.50413662]), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 20.0}
done in step count: 83
reward sum = 0.21676309095780494
running average episode reward sum: 0.4929178102053168
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.52900126, 5.86438001, 4.51610706]), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.9843742230467374}
episode index:4178
target Thresh 32.0
target distance 8.0
model initialize at round 4178
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 12.]), 'previousTarget': array([22., 12.]), 'currentState': array([14.30697508,  6.93041968,  6.20936093]), 'targetState': array([22, 12], dtype=int32), 'currentDistance': 9.213212091218088}
done in step count: 20
reward sum = 0.6725302684024699
running average episode reward sum: 0.49296078997516535
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 12.]), 'previousTarget': array([22., 12.]), 'currentState': array([21.69950701, 11.03101078,  0.70559126]), 'targetState': array([22, 12], dtype=int32), 'currentDistance': 1.0145127601500272}
episode index:4179
target Thresh 32.0
target distance 15.0
model initialize at round 4179
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([ 4.9751249 , 22.10167181,  1.60512984]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 17.592863954417577}
done in step count: 52
reward sum = 0.34180188977448867
running average episode reward sum: 0.4929246275588494
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.04771493,  6.85556144,  5.73768847]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.9631767016186011}
episode index:4180
target Thresh 32.0
target distance 21.0
model initialize at round 4180
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7.09009055, 28.89618185]), 'previousTarget': array([ 7.09009055, 28.89618185]), 'currentState': array([27.        , 27.        ,  5.54464844]), 'targetState': array([ 6, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 85
reward sum = 0.21517029239206295
running average episode reward sum: 0.4928581950462527
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 29.]), 'previousTarget': array([ 6., 29.]), 'currentState': array([ 6.90837399, 29.3458615 ,  3.30115344]), 'targetState': array([ 6, 29], dtype=int32), 'currentDistance': 0.9719894430696551}
episode index:4181
target Thresh 32.0
target distance 12.0
model initialize at round 4181
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([12.85058392, 22.91951472,  3.69060719]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 16.11862285079279}
done in step count: 45
reward sum = 0.45550781958509734
running average episode reward sum: 0.4928492638230434
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.1434167 , 11.79870846,  4.29323144]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.8114823190875506}
episode index:4182
target Thresh 32.0
target distance 12.0
model initialize at round 4182
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 28.]), 'previousTarget': array([ 4., 28.]), 'currentState': array([ 3.        , 16.        ,  3.59605646]), 'targetState': array([ 4, 28], dtype=int32), 'currentDistance': 12.041594578792296}
done in step count: 29
reward sum = 0.5359218659758316
running average episode reward sum: 0.49285956088308475
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 28.]), 'previousTarget': array([ 4., 28.]), 'currentState': array([ 4.23672577, 27.12682085,  1.62068649]), 'targetState': array([ 4, 28], dtype=int32), 'currentDistance': 0.9046993523611293}
episode index:4183
target Thresh 32.0
target distance 14.0
model initialize at round 4183
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  9.]), 'previousTarget': array([20.,  9.]), 'currentState': array([25.74729811, 22.88782912,  3.74123898]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 15.030077618969973}
done in step count: 35
reward sum = 0.4889301881626973
running average episode reward sum: 0.49285862174046513
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  9.]), 'previousTarget': array([20.,  9.]), 'currentState': array([19.97454617,  9.87174019,  4.53915452]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 0.8721117241266005}
episode index:4184
target Thresh 32.0
target distance 12.0
model initialize at round 4184
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 23.]), 'previousTarget': array([25., 23.]), 'currentState': array([26.11753045, 11.06495939,  0.72913316]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 11.987246082998048}
done in step count: 29
reward sum = 0.5756560639040325
running average episode reward sum: 0.49287840607551014
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 23.]), 'previousTarget': array([25., 23.]), 'currentState': array([25.64348589, 22.07359959,  1.9271116 ]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 1.1279591333492924}
episode index:4185
target Thresh 32.0
target distance 17.0
model initialize at round 4185
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 22.]), 'previousTarget': array([ 2., 22.]), 'currentState': array([12.01643617,  5.00652812,  0.62135693]), 'targetState': array([ 2, 22], dtype=int32), 'currentDistance': 19.72579732783989}
done in step count: 55
reward sum = 0.3105534741751902
running average episode reward sum: 0.4928348501911575
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 22.]), 'previousTarget': array([ 2., 22.]), 'currentState': array([ 2.95125837, 21.78049798,  2.69172215]), 'targetState': array([ 2, 22], dtype=int32), 'currentDistance': 0.9762548944286096}
episode index:4186
target Thresh 32.0
target distance 19.0
model initialize at round 4186
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 25.]), 'previousTarget': array([22., 25.]), 'currentState': array([17.9486702 ,  5.99434498,  3.02230296]), 'targetState': array([22, 25], dtype=int32), 'currentDistance': 19.43265797020519}
done in step count: 53
reward sum = 0.32850063844244404
running average episode reward sum: 0.49279560151388296
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 25.]), 'previousTarget': array([22., 25.]), 'currentState': array([21.70530309, 24.17615053,  1.45109994]), 'targetState': array([22, 25], dtype=int32), 'currentDistance': 0.8749709775878968}
episode index:4187
target Thresh 32.0
target distance 22.0
model initialize at round 4187
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.21964896, 13.94488862]), 'previousTarget': array([19.88854382, 14.05572809]), 'currentState': array([ 2.43524792, 23.09448314,  0.04138485]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.15987485119700987
running average episode reward sum: 0.4926397585213541
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([21.48544823, 13.46861376,  5.75941269]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 2.912009097838294}
episode index:4188
target Thresh 32.0
target distance 20.0
model initialize at round 4188
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([ 2.04874483, 16.08050087,  0.85929954]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 19.95141757689161}
done in step count: 54
reward sum = 0.3286487335303071
running average episode reward sum: 0.4926006105087041
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([21.11454161, 15.64844716,  0.21868555]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 0.9526940555201537}
episode index:4189
target Thresh 32.0
target distance 8.0
model initialize at round 4189
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 16.]), 'previousTarget': array([ 7., 16.]), 'currentState': array([12.88414643, 23.65468685,  4.23327196]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 9.654916365704768}
done in step count: 19
reward sum = 0.6579004786846356
running average episode reward sum: 0.49264006155122814
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 16.]), 'previousTarget': array([ 7., 16.]), 'currentState': array([ 7.09070046, 16.9159289 ,  4.17251263]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 0.9204087757976284}
episode index:4190
target Thresh 32.0
target distance 15.0
model initialize at round 4190
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 19.]), 'previousTarget': array([26., 19.]), 'currentState': array([10.92359119, 11.02154724,  2.6196219 ]), 'targetState': array([26, 19], dtype=int32), 'currentDistance': 17.05736823684006}
done in step count: 41
reward sum = 0.39325760516579805
running average episode reward sum: 0.49261634824739
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 19.]), 'previousTarget': array([26., 19.]), 'currentState': array([25.0592093 , 18.17626589,  0.50801548]), 'targetState': array([26, 19], dtype=int32), 'currentDistance': 1.2504499258242534}
episode index:4191
target Thresh 32.0
target distance 21.0
model initialize at round 4191
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.87838597,  8.6170994 ]), 'previousTarget': array([22.87838597,  8.6170994 ]), 'currentState': array([15.        , 27.        ,  2.63451499]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.20008397585447563
running average episode reward sum: 0.49254656476160924
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  6.]), 'previousTarget': array([24.,  6.]), 'currentState': array([23.44470671,  6.80376614,  5.56310306]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 0.9769291880264614}
episode index:4192
target Thresh 32.0
target distance 12.0
model initialize at round 4192
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 22.]), 'previousTarget': array([27., 22.]), 'currentState': array([24.99822514,  9.99903988,  3.39382324]), 'targetState': array([27, 22], dtype=int32), 'currentDistance': 12.166764011286356}
done in step count: 35
reward sum = 0.5234073784398108
running average episode reward sum: 0.4925539248411891
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 22.]), 'previousTarget': array([27., 22.]), 'currentState': array([27.0473407 , 21.07694461,  1.47710947]), 'targetState': array([27, 22], dtype=int32), 'currentDistance': 0.9242685769314177}
episode index:4193
target Thresh 32.0
target distance 23.0
model initialize at round 4193
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.57529518,  8.13267868]), 'previousTarget': array([22.35234545,  8.04843794]), 'currentState': array([3.18738185, 3.22260568, 0.74882416]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.27225360946372873
running average episode reward sum: 0.49250139734586784
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([25.29366491,  8.44719266,  0.46073299]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 0.8969421472870328}
episode index:4194
target Thresh 32.0
target distance 10.0
model initialize at round 4194
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 26.]), 'previousTarget': array([19., 26.]), 'currentState': array([14.29169492, 16.02016327,  0.31157705]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 11.034730534525357}
done in step count: 24
reward sum = 0.5936290618513722
running average episode reward sum: 0.49252550405969514
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 26.]), 'previousTarget': array([19., 26.]), 'currentState': array([19.22767947, 25.14854938,  1.3504368 ]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 0.8813660472529783}
episode index:4195
target Thresh 32.0
target distance 18.0
model initialize at round 4195
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 17.]), 'previousTarget': array([21., 17.]), 'currentState': array([ 3.        , 18.        ,  4.39725602]), 'targetState': array([21, 17], dtype=int32), 'currentDistance': 18.027756377319943}
done in step count: 50
reward sum = 0.3701651787499898
running average episode reward sum: 0.4924963428763516
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 17.]), 'previousTarget': array([21., 17.]), 'currentState': array([20.20209243, 16.19913625,  0.279373  ]), 'targetState': array([21, 17], dtype=int32), 'currentDistance': 1.1305039761759244}
episode index:4196
target Thresh 32.0
target distance 10.0
model initialize at round 4196
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 8.]), 'previousTarget': array([9., 8.]), 'currentState': array([11.0854939 , 17.68662677,  4.79945081]), 'targetState': array([9, 8], dtype=int32), 'currentDistance': 9.908583303049351}
done in step count: 20
reward sum = 0.6472407876808166
running average episode reward sum: 0.49253321312767506
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 8.]), 'previousTarget': array([9., 8.]), 'currentState': array([8.71562357, 8.79301377, 4.64335499]), 'targetState': array([9, 8], dtype=int32), 'currentDistance': 0.8424611514765201}
episode index:4197
target Thresh 32.0
target distance 14.0
model initialize at round 4197
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 19.]), 'previousTarget': array([16., 19.]), 'currentState': array([1.90000000e+01, 5.00000000e+00, 7.84080824e-03]), 'targetState': array([16, 19], dtype=int32), 'currentDistance': 14.317821063276353}
done in step count: 39
reward sum = 0.477703483091333
running average episode reward sum: 0.4925296805573948
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 19.]), 'previousTarget': array([16., 19.]), 'currentState': array([16.7498236 , 18.04855211,  1.87403294]), 'targetState': array([16, 19], dtype=int32), 'currentDistance': 1.2113994048228975}
episode index:4198
target Thresh 32.0
target distance 15.0
model initialize at round 4198
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 21.]), 'previousTarget': array([ 5., 21.]), 'currentState': array([19.96406831, 21.09224225,  2.1900439 ]), 'targetState': array([ 5, 21], dtype=int32), 'currentDistance': 14.964352614294917}
done in step count: 34
reward sum = 0.48188117416529297
running average episode reward sum: 0.4925271445949294
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 21.]), 'previousTarget': array([ 5., 21.]), 'currentState': array([ 5.87293811, 21.33669783,  3.42572696]), 'targetState': array([ 5, 21], dtype=int32), 'currentDistance': 0.9356208540916007}
episode index:4199
target Thresh 32.0
target distance 4.0
model initialize at round 4199
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([14.        , 18.        ,  5.89324045]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 4.0}
done in step count: 10
reward sum = 0.8044387409400396
running average episode reward sum: 0.4926014092607259
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([14.26423739, 21.24244844,  1.63456537]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 0.802312755727467}
episode index:4200
target Thresh 32.0
target distance 22.0
model initialize at round 4200
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9.12677025, 12.26249016]), 'previousTarget': array([ 9.12677025, 12.26249016]), 'currentState': array([26.        , 23.        ,  2.11815006]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 20.0}
done in step count: 83
reward sum = 0.18742902216296242
running average episode reward sum: 0.49252876646446364
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.05862305, 9.95090353, 3.92507012]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.9527088659655152}
episode index:4201
target Thresh 32.0
target distance 13.0
model initialize at round 4201
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 20.]), 'previousTarget': array([24., 20.]), 'currentState': array([23.        ,  7.        ,  5.06628019]), 'targetState': array([24, 20], dtype=int32), 'currentDistance': 13.038404810405298}
done in step count: 39
reward sum = 0.48265407417355405
running average episode reward sum: 0.4925264164662983
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 20.]), 'previousTarget': array([24., 20.]), 'currentState': array([24.71792607, 19.18012033,  1.66766521]), 'targetState': array([24, 20], dtype=int32), 'currentDistance': 1.0897800309376777}
episode index:4202
target Thresh 32.0
target distance 18.0
model initialize at round 4202
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 27.]), 'previousTarget': array([20., 27.]), 'currentState': array([ 2.12112356, 24.86360652,  5.61326259]), 'targetState': array([20, 27], dtype=int32), 'currentDistance': 18.0060656431937}
done in step count: 42
reward sum = 0.4049652945585894
running average episode reward sum: 0.492505583460848
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 27.]), 'previousTarget': array([20., 27.]), 'currentState': array([1.91679821e+01, 2.66176555e+01, 1.96895387e-02]), 'targetState': array([20, 27], dtype=int32), 'currentDistance': 0.9156642898698173}
episode index:4203
target Thresh 32.0
target distance 13.0
model initialize at round 4203
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 23.]), 'previousTarget': array([15., 23.]), 'currentState': array([ 2.00217067, 26.01125005,  1.12769103]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 13.342083576290433}
done in step count: 30
reward sum = 0.5171465657629818
running average episode reward sum: 0.49251144477918823
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 23.]), 'previousTarget': array([15., 23.]), 'currentState': array([14.13383522, 22.95746912,  5.79368913]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 0.867208340650763}
episode index:4204
target Thresh 32.0
target distance 4.0
model initialize at round 4204
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([26.05772793,  3.9915896 ,  0.06193509]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 5.0415329056927005}
done in step count: 12
reward sum = 0.7647376658729521
running average episode reward sum: 0.4925761834762379
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([23.78752687,  7.5321624 ,  2.37832905]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 0.9160079673948851}
episode index:4205
target Thresh 32.0
target distance 11.0
model initialize at round 4205
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  7.]), 'previousTarget': array([17.,  7.]), 'currentState': array([10.99937479, 18.01723707,  1.38420209]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 12.545398186276243}
done in step count: 32
reward sum = 0.5122465594675583
running average episode reward sum: 0.49258086021803327
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  7.]), 'previousTarget': array([17.,  7.]), 'currentState': array([16.62832649,  7.83535924,  5.48790989]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 0.9143119074118721}
episode index:4206
target Thresh 32.0
target distance 7.0
model initialize at round 4206
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 19.]), 'previousTarget': array([ 9., 19.]), 'currentState': array([13.        , 26.        ,  5.86740264]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 18
reward sum = 0.6843602960565004
running average episode reward sum: 0.4926264460121475
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 19.]), 'previousTarget': array([ 9., 19.]), 'currentState': array([ 9.9074028 , 19.48792452,  4.03317827]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 1.0302670414502302}
episode index:4207
target Thresh 32.0
target distance 16.0
model initialize at round 4207
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 28.]), 'previousTarget': array([ 6., 28.]), 'currentState': array([15.04763077, 12.02030614,  0.65549158]), 'targetState': array([ 6, 28], dtype=int32), 'currentDistance': 18.363285064406742}
done in step count: 52
reward sum = 0.3690260488768293
running average episode reward sum: 0.4925970732941971
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 28.]), 'previousTarget': array([ 6., 28.]), 'currentState': array([ 6.91944939, 27.50620092,  2.50533476]), 'targetState': array([ 6, 28], dtype=int32), 'currentDistance': 1.0436592867539638}
episode index:4208
target Thresh 32.0
target distance 7.0
model initialize at round 4208
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([26.82871351,  7.24968255,  2.31405282]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 8.927344454152006}
done in step count: 16
reward sum = 0.7085989688081648
running average episode reward sum: 0.4926483923475385
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([20.90958703, 12.10217236,  2.32173008]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 1.2780622198839315}
episode index:4209
target Thresh 32.0
target distance 4.0
model initialize at round 4209
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([ 7.06556058, 28.95570301,  5.47374237]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 3.956246261484064}
done in step count: 7
reward sum = 0.8595407457820401
running average episode reward sum: 0.4927355401749576
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([ 7.96020991, 25.95882582,  4.46474909]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 1.3569635308634298}
episode index:4210
target Thresh 32.0
target distance 10.0
model initialize at round 4210
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'previousTarget': array([ 2., 17.]), 'currentState': array([8.02525491, 7.00747623, 0.5403114 ]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 11.668514386093278}
done in step count: 27
reward sum = 0.5657549672312665
running average episode reward sum: 0.49275288033811515
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'previousTarget': array([ 2., 17.]), 'currentState': array([ 2.39121183, 16.02661536,  2.20330883]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 1.049058791314841}
episode index:4211
target Thresh 32.0
target distance 16.0
model initialize at round 4211
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([17.        , 27.        ,  0.82024306]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 16.492422502470642}
done in step count: 54
reward sum = 0.35330758260842765
running average episode reward sum: 0.4927197736672392
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([20.95279127, 11.9637821 ,  5.00092233]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 0.9649376103004282}
episode index:4212
target Thresh 32.0
target distance 13.0
model initialize at round 4212
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 20.]), 'previousTarget': array([21., 20.]), 'currentState': array([ 7.98506201, 13.96472776,  4.5642848 ]), 'targetState': array([21, 20], dtype=int32), 'currentDistance': 14.346188409904977}
done in step count: 41
reward sum = 0.44597468693111275
running average episode reward sum: 0.4927086782277101
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 20.]), 'previousTarget': array([21., 20.]), 'currentState': array([20.99391717, 19.07255239,  0.90437369]), 'targetState': array([21, 20], dtype=int32), 'currentDistance': 0.9274675591375233}
episode index:4213
target Thresh 32.0
target distance 9.0
model initialize at round 4213
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([25.70013977, 13.96897269,  3.08657557]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 10.586109882308959}
done in step count: 26
reward sum = 0.610225912177968
running average episode reward sum: 0.4927365655637211
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([17.89138866, 20.26793406,  2.89196939]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 0.9307859098352281}
episode index:4214
target Thresh 32.0
target distance 15.0
model initialize at round 4214
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 12.]), 'previousTarget': array([12., 12.]), 'currentState': array([ 5.9237577 , 26.70348187,  4.58896342]), 'targetState': array([12, 12], dtype=int32), 'currentDistance': 15.909528578900092}
done in step count: 44
reward sum = 0.45080892633575875
running average episode reward sum: 0.4927266183183527
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 12.]), 'previousTarget': array([12., 12.]), 'currentState': array([11.133419  , 12.14373532,  5.67466057]), 'targetState': array([12, 12], dtype=int32), 'currentDistance': 0.8784204446939441}
episode index:4215
target Thresh 32.0
target distance 19.0
model initialize at round 4215
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.51500271, 21.76185931]), 'previousTarget': array([22.51905368, 21.75489296]), 'currentState': array([7.97847747, 8.02543204, 2.21221424]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.12727884479788062
running average episode reward sum: 0.4925795581990177
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.28568142, 22.14916575]), 'previousTarget': array([22.28568142, 22.14916575]), 'currentState': array([6.79633861, 9.49688298, 2.06369917]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 20.0}
episode index:4216
target Thresh 32.0
target distance 1.0
model initialize at round 4216
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 12.]), 'previousTarget': array([13., 12.]), 'currentState': array([13.        , 13.        ,  2.20568317]), 'targetState': array([13, 12], dtype=int32), 'currentDistance': 1.0}
done in step count: 3
reward sum = 0.9475707343803128
running average episode reward sum: 0.49268745271554165
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 12.]), 'previousTarget': array([13., 12.]), 'currentState': array([12.99925112, 12.99997689,  3.37392768]), 'targetState': array([13, 12], dtype=int32), 'currentDistance': 0.9999771749001507}
episode index:4217
target Thresh 32.0
target distance 13.0
model initialize at round 4217
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 16.]), 'previousTarget': array([ 7., 16.]), 'currentState': array([ 9.99400695, 28.50329062,  4.6195671 ]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 12.856762965322824}
done in step count: 29
reward sum = 0.5547341238041601
running average episode reward sum: 0.49270216268972106
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 16.]), 'previousTarget': array([ 7., 16.]), 'currentState': array([ 7.19067062, 16.82921797,  4.63742055]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 0.8508570515453252}
episode index:4218
target Thresh 32.0
target distance 17.0
model initialize at round 4218
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.86502556,  5.20859686]), 'previousTarget': array([17.86502556,  5.20859686]), 'currentState': array([ 7.        , 22.        ,  0.45497771]), 'targetState': array([18,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.02074717795410684
running average episode reward sum: 0.4925804633911565
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.96275687,  5.07479225]), 'previousTarget': array([17.96275687,  5.07479225]), 'currentState': array([ 9.04779442, 22.97796081,  0.35194814]), 'targetState': array([18,  5], dtype=int32), 'currentDistance': 20.0}
episode index:4219
target Thresh 32.0
target distance 15.0
model initialize at round 4219
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 18.]), 'previousTarget': array([18., 18.]), 'currentState': array([ 3.        , 11.        ,  2.14363709]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 16.55294535724685}
done in step count: 48
reward sum = 0.36855937064224975
running average episode reward sum: 0.4925510745066189
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 18.]), 'previousTarget': array([18., 18.]), 'currentState': array([17.02939143, 17.61348195,  0.36813993]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 1.0447378640421976}
episode index:4220
target Thresh 32.0
target distance 5.0
model initialize at round 4220
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([11.        ,  5.        ,  3.28088385]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 7.071067811865475}
done in step count: 15
reward sum = 0.7454786069650543
running average episode reward sum: 0.492610995741506
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([6.63888467, 9.19637963, 2.03668336]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 1.0266349511302333}
episode index:4221
target Thresh 32.0
target distance 23.0
model initialize at round 4221
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.94951698, 20.76576138]), 'previousTarget': array([18.92535561, 20.75221194]), 'currentState': array([8.05310144, 3.99471107, 0.13710075]), 'targetState': array([23, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 88
reward sum = 0.13466292812003428
running average episode reward sum: 0.49252621410540437
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 27.]), 'previousTarget': array([23., 27.]), 'currentState': array([22.81366236, 26.05117053,  1.45603552]), 'targetState': array([23, 27], dtype=int32), 'currentDistance': 0.9669535073049753}
episode index:4222
target Thresh 32.0
target distance 17.0
model initialize at round 4222
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  2.]), 'previousTarget': array([27.,  2.]), 'currentState': array([ 9.99848156, 10.9993711 ,  3.76398587]), 'targetState': array([27,  2], dtype=int32), 'currentDistance': 19.2364318262403}
done in step count: 51
reward sum = 0.34767003799760887
running average episode reward sum: 0.492491912382433
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  2.]), 'previousTarget': array([27.,  2.]), 'currentState': array([26.04005142,  2.93952652,  5.91229731]), 'targetState': array([27,  2], dtype=int32), 'currentDistance': 1.3432093504073417}
episode index:4223
target Thresh 32.0
target distance 9.0
model initialize at round 4223
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 19.]), 'previousTarget': array([14., 19.]), 'currentState': array([22.76645808, 19.19436287,  2.63738346]), 'targetState': array([14, 19], dtype=int32), 'currentDistance': 8.76861244452259}
done in step count: 18
reward sum = 0.6934377430712515
running average episode reward sum: 0.49253948478553167
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 19.]), 'previousTarget': array([14., 19.]), 'currentState': array([14.91507463, 19.47671091,  3.06107469]), 'targetState': array([14, 19], dtype=int32), 'currentDistance': 1.031801757439565}
episode index:4224
target Thresh 32.0
target distance 23.0
model initialize at round 4224
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.80458732,  9.4122993 ]), 'previousTarget': array([14.73762894,  9.58076497]), 'currentState': array([ 1.97831389, 24.75787553,  4.62417361]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 82
reward sum = 0.13632257763904504
running average episode reward sum: 0.49245517309153247
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  2.]), 'previousTarget': array([21.,  2.]), 'currentState': array([20.51390285,  2.96994381,  5.16026899]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 1.0849338398389}
episode index:4225
target Thresh 32.0
target distance 15.0
model initialize at round 4225
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 17.]), 'previousTarget': array([ 9., 17.]), 'currentState': array([5.27652728, 2.29677242, 0.94779503]), 'targetState': array([ 9, 17], dtype=int32), 'currentDistance': 15.167371237341213}
done in step count: 37
reward sum = 0.4847432085080211
running average episode reward sum: 0.4924533482063968
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 17.]), 'previousTarget': array([ 9., 17.]), 'currentState': array([ 8.91294185, 16.16209447,  1.55395179]), 'targetState': array([ 9, 17], dtype=int32), 'currentDistance': 0.8424160457264223}
episode index:4226
target Thresh 32.0
target distance 4.0
model initialize at round 4226
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  2.]), 'previousTarget': array([13.,  2.]), 'currentState': array([9.        , 4.        , 0.40675634]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 4.47213595499958}
done in step count: 12
reward sum = 0.8273471295079754
running average episode reward sum: 0.4925325755026593
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  2.]), 'previousTarget': array([13.,  2.]), 'currentState': array([12.29576234,  2.91761247,  5.57379843]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 1.156703648743542}
episode index:4227
target Thresh 32.0
target distance 13.0
model initialize at round 4227
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 24.]), 'previousTarget': array([16., 24.]), 'currentState': array([ 3.        , 26.        ,  3.87009025]), 'targetState': array([16, 24], dtype=int32), 'currentDistance': 13.152946437965905}
done in step count: 30
reward sum = 0.4940188279795434
running average episode reward sum: 0.4925329270287891
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 24.]), 'previousTarget': array([16., 24.]), 'currentState': array([15.25298894, 23.58800804,  6.26495825]), 'targetState': array([16, 24], dtype=int32), 'currentDistance': 0.8530902095739736}
episode index:4228
target Thresh 32.0
target distance 8.0
model initialize at round 4228
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 17.]), 'previousTarget': array([ 9., 17.]), 'currentState': array([14.94010125,  9.28628454,  1.84444535]), 'targetState': array([ 9, 17], dtype=int32), 'currentDistance': 9.735820918645166}
done in step count: 20
reward sum = 0.6679832311315864
running average episode reward sum: 0.49257441444995315
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 17.]), 'previousTarget': array([ 9., 17.]), 'currentState': array([ 9.90710742, 16.49089125,  2.48963782]), 'targetState': array([ 9, 17], dtype=int32), 'currentDistance': 1.0402094030230535}
episode index:4229
target Thresh 32.0
target distance 22.0
model initialize at round 4229
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4.42459507, 25.65876092]), 'previousTarget': array([ 4.42229124, 25.6773982 ]), 'currentState': array([7.99347744, 5.97975996, 4.17107844]), 'targetState': array([ 4, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.14606487378838404
running average episode reward sum: 0.4924234358948141
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 28.]), 'previousTarget': array([ 4., 28.]), 'currentState': array([ 5.6323437 , 20.12642246,  1.54050405]), 'targetState': array([ 4, 28], dtype=int32), 'currentDistance': 8.041005485470684}
episode index:4230
target Thresh 32.0
target distance 22.0
model initialize at round 4230
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([25.        , 29.        ,  0.58525443]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 61
reward sum = 0.25945146784276324
running average episode reward sum: 0.4923683727967162
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([24.81632549,  7.9029196 ,  5.10850974]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 0.9214120287890035}
episode index:4231
target Thresh 32.0
target distance 20.0
model initialize at round 4231
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.60724649,  4.12256814]), 'previousTarget': array([16.60700849,  4.12283287]), 'currentState': array([10.00241542, 23.00049723,  6.2337046 ]), 'targetState': array([17,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.30424835460992394
running average episode reward sum: 0.49232392099657757
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  3.]), 'previousTarget': array([17.,  3.]), 'currentState': array([16.14164669,  3.57305353,  5.17580114]), 'targetState': array([17,  3], dtype=int32), 'currentDistance': 1.0320662567268133}
episode index:4232
target Thresh 32.0
target distance 8.0
model initialize at round 4232
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([8.93219899, 9.01221326, 2.71087003]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 8.041998006955767}
done in step count: 18
reward sum = 0.697990390277083
running average episode reward sum: 0.49237250745282146
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([ 8.02931957, 16.00875893,  1.60263933]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 0.9916745914407009}
episode index:4233
target Thresh 32.0
target distance 8.0
model initialize at round 4233
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 18.]), 'previousTarget': array([18., 18.]), 'currentState': array([10.        , 24.        ,  4.16059351]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 9.999999999999998}
done in step count: 24
reward sum = 0.6148092971669948
running average episode reward sum: 0.4924014249751914
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 18.]), 'previousTarget': array([18., 18.]), 'currentState': array([17.19296658, 18.19335459,  5.82531487]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 0.8298728412652906}
episode index:4234
target Thresh 32.0
target distance 16.0
model initialize at round 4234
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 15.]), 'previousTarget': array([ 4., 15.]), 'currentState': array([20.        , 14.        ,  1.06657577]), 'targetState': array([ 4, 15], dtype=int32), 'currentDistance': 16.0312195418814}
done in step count: 49
reward sum = 0.4041491604436419
running average episode reward sum: 0.4923805861878167
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 15.]), 'previousTarget': array([ 4., 15.]), 'currentState': array([ 4.91067378, 15.39590676,  3.3640835 ]), 'targetState': array([ 4, 15], dtype=int32), 'currentDistance': 0.9930100144645907}
episode index:4235
target Thresh 32.0
target distance 19.0
model initialize at round 4235
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 24.]), 'previousTarget': array([ 2., 24.]), 'currentState': array([5.85278559, 5.04493654, 2.73127496]), 'targetState': array([ 2, 24], dtype=int32), 'currentDistance': 19.342657196476445}
done in step count: 44
reward sum = 0.3731138352155794
running average episode reward sum: 0.4923524306753114
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 24.]), 'previousTarget': array([ 2., 24.]), 'currentState': array([ 2.50460407, 23.01720747,  1.97015861]), 'targetState': array([ 2, 24], dtype=int32), 'currentDistance': 1.1047653262564707}
episode index:4236
target Thresh 32.0
target distance 12.0
model initialize at round 4236
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 3.]), 'previousTarget': array([9., 3.]), 'currentState': array([20.65541461,  4.85168924,  3.54988041]), 'targetState': array([9, 3], dtype=int32), 'currentDistance': 11.801586455621855}
done in step count: 27
reward sum = 0.5985897767312132
running average episode reward sum: 0.4923775043939935
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 3.]), 'previousTarget': array([9., 3.]), 'currentState': array([9.94818325, 3.12040861, 3.36421925]), 'targetState': array([9, 3], dtype=int32), 'currentDistance': 0.9557979462302452}
episode index:4237
target Thresh 32.0
target distance 12.0
model initialize at round 4237
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 17.]), 'previousTarget': array([20., 17.]), 'currentState': array([8.24834925, 8.01423984, 0.27348426]), 'targetState': array([20, 17], dtype=int32), 'currentDistance': 14.793416815168987}
done in step count: 37
reward sum = 0.49741890784129095
running average episode reward sum: 0.4923786939653591
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 17.]), 'previousTarget': array([20., 17.]), 'currentState': array([19.06694088, 16.05271783,  0.4461044 ]), 'targetState': array([20, 17], dtype=int32), 'currentDistance': 1.3296401121771857}
episode index:4238
target Thresh 32.0
target distance 15.0
model initialize at round 4238
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 15.]), 'previousTarget': array([12., 15.]), 'currentState': array([27.        , 21.        ,  1.99488395]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 16.15549442140351}
done in step count: 43
reward sum = 0.40618073752417533
running average episode reward sum: 0.492358359462778
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 15.]), 'previousTarget': array([12., 15.]), 'currentState': array([12.75900425, 15.43530921,  3.09742954]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 0.8749751810641158}
episode index:4239
target Thresh 32.0
target distance 21.0
model initialize at round 4239
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3.23707087, 10.82062865]), 'previousTarget': array([ 3.20101013, 10.82842712]), 'currentState': array([23.03008813,  7.95070395,  5.00788832]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.27874581752947436
running average episode reward sum: 0.49230797914628427
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.76553405, 10.91263514,  3.05176638]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.7705030855486085}
episode index:4240
target Thresh 32.0
target distance 11.0
model initialize at round 4240
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([2.95045175, 7.00301201, 2.85278398]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 11.448775195788699}
done in step count: 34
reward sum = 0.5311306447648018
running average episode reward sum: 0.4923171332763523
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.12135238,  9.54095586,  0.08871661]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.9913340344623929}
episode index:4241
target Thresh 32.0
target distance 16.0
model initialize at round 4241
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 7.]), 'previousTarget': array([6., 7.]), 'currentState': array([ 6.95603388, 22.99876578,  3.42215729]), 'targetState': array([6, 7], dtype=int32), 'currentDistance': 16.0273050558153}
done in step count: 44
reward sum = 0.4398570845113481
running average episode reward sum: 0.4923047664567472
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 7.]), 'previousTarget': array([6., 7.]), 'currentState': array([5.5709092 , 7.77901608, 4.9035651 ]), 'targetState': array([6, 7], dtype=int32), 'currentDistance': 0.8893733564414714}
episode index:4242
target Thresh 32.0
target distance 12.0
model initialize at round 4242
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([ 4.       , 28.       ,  4.0310185]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 13.892443989449806}
done in step count: 31
reward sum = 0.5003216712911229
running average episode reward sum: 0.49230665589931955
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([15.01741273, 21.47411435,  5.5839541 ]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 1.0909913656015138}
episode index:4243
target Thresh 32.0
target distance 12.0
model initialize at round 4243
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 17.]), 'previousTarget': array([18., 17.]), 'currentState': array([22.68307583,  5.31319269,  2.31420843]), 'targetState': array([18, 17], dtype=int32), 'currentDistance': 12.590181264960476}
done in step count: 25
reward sum = 0.5805103486424483
running average episode reward sum: 0.49232743905029575
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 17.]), 'previousTarget': array([18., 17.]), 'currentState': array([18.73170442, 16.01319751,  1.94492734]), 'targetState': array([18, 17], dtype=int32), 'currentDistance': 1.228483017806206}
episode index:4244
target Thresh 32.0
target distance 5.0
model initialize at round 4244
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([18.02965136, 15.04113242,  0.69369686]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 5.133672427542913}
done in step count: 13
reward sum = 0.7627672312025531
running average episode reward sum: 0.49239114689297
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([18.94366049, 10.92725588,  4.65011166]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 0.928965880612021}
episode index:4245
target Thresh 32.0
target distance 3.0
model initialize at round 4245
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 29.]), 'previousTarget': array([ 9., 29.]), 'currentState': array([11.00115252, 26.32201902,  1.77207682]), 'targetState': array([ 9, 29], dtype=int32), 'currentDistance': 3.3430814410782648}
done in step count: 5
reward sum = 0.8969787424715263
running average episode reward sum: 0.49248643365594186
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 29.]), 'previousTarget': array([ 9., 29.]), 'currentState': array([ 9.84846064, 28.26108845,  2.29516482]), 'targetState': array([ 9, 29], dtype=int32), 'currentDistance': 1.1251114321003162}
episode index:4246
target Thresh 32.0
target distance 2.0
model initialize at round 4246
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([16.99221222, 10.03838356,  2.0024451 ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.9925819456749525}
done in step count: 5
reward sum = 0.9113202922394202
running average episode reward sum: 0.4925850524123778
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.70700156, 10.42749342,  3.28507198]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.826197211357077}
episode index:4247
target Thresh 32.0
target distance 12.0
model initialize at round 4247
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 18.]), 'previousTarget': array([ 8., 18.]), 'currentState': array([19.94572797, 27.74080303,  4.3365761 ]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 15.413749069871004}
done in step count: 31
reward sum = 0.4840850820382785
running average episode reward sum: 0.4925830514777323
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 18.]), 'previousTarget': array([ 8., 18.]), 'currentState': array([ 8.65447467, 18.92143953,  3.64971463]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 1.130215864651711}
episode index:4248
target Thresh 32.0
target distance 6.0
model initialize at round 4248
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  5.]), 'previousTarget': array([23.,  5.]), 'currentState': array([17.05879796,  6.73985906,  5.10424507]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 6.19071815404877}
done in step count: 15
reward sum = 0.7695169987877569
running average episode reward sum: 0.49264822774210276
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  5.]), 'previousTarget': array([23.,  5.]), 'currentState': array([22.03868102,  4.82370287,  6.115498  ]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 0.9773509428352982}
episode index:4249
target Thresh 32.0
target distance 25.0
model initialize at round 4249
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.30482361,  9.51439767]), 'previousTarget': array([12.33254095,  9.55225396]), 'currentState': array([16.91008089, 28.97696702,  3.62086117]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 77
reward sum = 0.19886492947894613
running average episode reward sum: 0.4925791022601585
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([10.86050623,  4.80414165,  4.49847299]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 0.8161509076396183}
episode index:4250
target Thresh 32.0
target distance 5.0
model initialize at round 4250
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 24.]), 'previousTarget': array([12., 24.]), 'currentState': array([11.        , 29.        ,  2.94524449]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 5.0990195135927845}
done in step count: 14
reward sum = 0.7753190184175369
running average episode reward sum: 0.4926456136495157
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 24.]), 'previousTarget': array([12., 24.]), 'currentState': array([11.28561638, 24.78626495,  5.14876426]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 1.0623354150059123}
episode index:4251
target Thresh 32.0
target distance 15.0
model initialize at round 4251
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([15.        , 21.        ,  5.44526339]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 19.849433241279208}
done in step count: 49
reward sum = 0.3400986473417618
running average episode reward sum: 0.49260973712874717
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.25064796, 6.9625133 , 4.0076317 ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.9946136201822752}
episode index:4252
target Thresh 32.0
target distance 18.0
model initialize at round 4252
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([27.       ,  4.       ,  1.3662481]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 18.973665961010276}
done in step count: 46
reward sum = 0.3626686576701597
running average episode reward sum: 0.4925791843237957
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 9.78543143, 10.06010293,  2.98870588]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.7877276733796739}
episode index:4253
target Thresh 32.0
target distance 9.0
model initialize at round 4253
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 23.]), 'previousTarget': array([12., 23.]), 'currentState': array([20.99684822, 27.07896894,  1.86318684]), 'targetState': array([12, 23], dtype=int32), 'currentDistance': 9.87832300930169}
done in step count: 23
reward sum = 0.6240571175215692
running average episode reward sum: 0.4926100912192348
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 23.]), 'previousTarget': array([12., 23.]), 'currentState': array([12.689819  , 23.91423472,  3.75614989]), 'targetState': array([12, 23], dtype=int32), 'currentDistance': 1.1452839692245542}
episode index:4254
target Thresh 32.0
target distance 3.0
model initialize at round 4254
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 17.]), 'previousTarget': array([12., 17.]), 'currentState': array([10.        , 14.        ,  4.98823404]), 'targetState': array([12, 17], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 10
reward sum = 0.8131298928017726
running average episode reward sum: 0.492685419022192
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 17.]), 'previousTarget': array([12., 17.]), 'currentState': array([11.99387892, 16.03885334,  1.19940888]), 'targetState': array([12, 17], dtype=int32), 'currentDistance': 0.9611661553217722}
episode index:4255
target Thresh 32.0
target distance 20.0
model initialize at round 4255
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5.59715  , 7.1492875]), 'previousTarget': array([5.59715  , 7.1492875]), 'currentState': array([25.        , 12.        ,  0.82127097]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.2827940782865528
running average episode reward sum: 0.49263610244777106
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 7.]), 'previousTarget': array([5., 7.]), 'currentState': array([5.97119412, 7.57131337, 3.23278512]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 1.1267728241422907}
episode index:4256
target Thresh 32.0
target distance 8.0
model initialize at round 4256
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 27.]), 'previousTarget': array([ 9., 27.]), 'currentState': array([17.00021619, 26.99441974,  4.50311941]), 'targetState': array([ 9, 27], dtype=int32), 'currentDistance': 8.00021813518888}
done in step count: 18
reward sum = 0.6898541898150584
running average episode reward sum: 0.4926824303987617
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 27.]), 'previousTarget': array([ 9., 27.]), 'currentState': array([ 9.95679817, 27.45381988,  3.16090188]), 'targetState': array([ 9, 27], dtype=int32), 'currentDistance': 1.0589689417536485}
episode index:4257
target Thresh 32.0
target distance 16.0
model initialize at round 4257
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.86186482, 14.11337551]), 'previousTarget': array([26.52228  , 14.3881475]), 'currentState': array([11.40225371, 26.80196978,  5.78204153]), 'targetState': array([27, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.35264386166738265
running average episode reward sum: 0.4926495420547665
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 14.]), 'previousTarget': array([27., 14.]), 'currentState': array([26.07721915, 14.13300325,  5.69266409]), 'targetState': array([27, 14], dtype=int32), 'currentDistance': 0.932316665066019}
episode index:4258
target Thresh 32.0
target distance 10.0
model initialize at round 4258
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 14.]), 'previousTarget': array([21., 14.]), 'currentState': array([10.96252725,  9.01599707,  2.48561478]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 11.206745488194917}
done in step count: 26
reward sum = 0.5672788492323363
running average episode reward sum: 0.49266706478479183
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 14.]), 'previousTarget': array([21., 14.]), 'currentState': array([20.25505585, 13.2621903 ,  0.48650842]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 1.0484774396253405}
episode index:4259
target Thresh 32.0
target distance 17.0
model initialize at round 4259
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 28.]), 'previousTarget': array([17., 28.]), 'currentState': array([20.02615396, 10.99972602,  0.22943303]), 'targetState': array([17, 28], dtype=int32), 'currentDistance': 17.267510626206377}
done in step count: 45
reward sum = 0.4061932805142602
running average episode reward sum: 0.4926467657743997
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 28.]), 'previousTarget': array([17., 28.]), 'currentState': array([17.34683326, 27.15628083,  1.67678127]), 'targetState': array([17, 28], dtype=int32), 'currentDistance': 0.9122254924122614}
episode index:4260
target Thresh 32.0
target distance 16.0
model initialize at round 4260
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8.01217651, 17.98375439]), 'previousTarget': array([ 8., 18.]), 'currentState': array([20.00732135,  1.98011417,  4.82581428]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.3094926003131253
running average episode reward sum: 0.49260378192895
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 18.]), 'previousTarget': array([ 8., 18.]), 'currentState': array([ 8.9464835 , 17.3350043 ,  2.20591462]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 1.1567412383887623}
episode index:4261
target Thresh 32.0
target distance 7.0
model initialize at round 4261
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 19.]), 'previousTarget': array([21., 19.]), 'currentState': array([15.96846537, 25.9616745 ,  4.27213076]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 8.589601424854498}
done in step count: 19
reward sum = 0.6860438424347568
running average episode reward sum: 0.4926491690853333
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 19.]), 'previousTarget': array([21., 19.]), 'currentState': array([20.17252955, 19.56613172,  5.31610999]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 1.002602848214569}
episode index:4262
target Thresh 32.0
target distance 22.0
model initialize at round 4262
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4.05572809, 10.11145618]), 'previousTarget': array([ 4.05572809, 10.11145618]), 'currentState': array([13.       , 28.       ,  6.0130241]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.21801011955876626
running average episode reward sum: 0.49258474519381873
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.99434531, 6.92192934, 4.35908564]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.9219466863027217}
episode index:4263
target Thresh 32.0
target distance 11.0
model initialize at round 4263
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([21.91354399, 15.71473165,  4.2991268 ]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 14.59739116095251}
done in step count: 35
reward sum = 0.4990957194629252
running average episode reward sum: 0.49258627215776546
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([12.11787717,  5.82780886,  4.08804524]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 0.8361593974957449}
episode index:4264
target Thresh 32.0
target distance 21.0
model initialize at round 4264
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.04731786,  6.86828089]), 'previousTarget': array([24.04869701,  7.02263725]), 'currentState': array([25.13562356, 26.83864871,  5.22050795]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.3212222764248711
running average episode reward sum: 0.4925460930262924
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  6.]), 'previousTarget': array([24.,  6.]), 'currentState': array([24.05019942,  6.99654752,  4.39524229]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 0.9978110773150624}
episode index:4265
target Thresh 32.0
target distance 12.0
model initialize at round 4265
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 26.]), 'previousTarget': array([17., 26.]), 'currentState': array([18.99210106, 13.96489233,  4.25199443]), 'targetState': array([17, 26], dtype=int32), 'currentDistance': 12.198864015727983}
done in step count: 27
reward sum = 0.5224019399157881
running average episode reward sum: 0.4925530915839317
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 26.]), 'previousTarget': array([17., 26.]), 'currentState': array([17.29803354, 25.07974796,  1.72185655]), 'targetState': array([17, 26], dtype=int32), 'currentDistance': 0.9673095745109052}
episode index:4266
target Thresh 32.0
target distance 4.0
model initialize at round 4266
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 19.]), 'previousTarget': array([ 4., 19.]), 'currentState': array([ 7.99944481, 14.96725367,  4.44293642]), 'targetState': array([ 4, 19], dtype=int32), 'currentDistance': 5.679665642278251}
done in step count: 13
reward sum = 0.746136559408584
running average episode reward sum: 0.4926125205663138
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 19.]), 'previousTarget': array([ 4., 19.]), 'currentState': array([ 4.87865416, 18.52765323,  2.09880399]), 'targetState': array([ 4, 19], dtype=int32), 'currentDistance': 0.9975693476740415}
episode index:4267
target Thresh 32.0
target distance 11.0
model initialize at round 4267
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 16.]), 'previousTarget': array([13., 16.]), 'currentState': array([16.10635423, 26.75985064,  4.90579134]), 'targetState': array([13, 16], dtype=int32), 'currentDistance': 11.199277760674113}
done in step count: 22
reward sum = 0.6127674090265609
running average episode reward sum: 0.49264067307063913
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 16.]), 'previousTarget': array([13., 16.]), 'currentState': array([13.22186636, 16.91828332,  4.3073038 ]), 'targetState': array([13, 16], dtype=int32), 'currentDistance': 0.9447057425643863}
episode index:4268
target Thresh 32.0
target distance 11.0
model initialize at round 4268
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([15.09159904, 16.89899359,  5.69803458]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 11.340610757029179}
done in step count: 23
reward sum = 0.5971036181016847
running average episode reward sum: 0.49266514319128357
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([25.28146717, 19.52017068,  0.26032965]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 0.8640171287628927}
episode index:4269
target Thresh 32.0
target distance 14.0
model initialize at round 4269
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  9.]), 'previousTarget': array([17.,  9.]), 'currentState': array([3.10933862, 8.14598485, 0.72538942]), 'targetState': array([17,  9], dtype=int32), 'currentDistance': 13.916889574236539}
done in step count: 30
reward sum = 0.5227819606057966
running average episode reward sum: 0.492672196310116
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  9.]), 'previousTarget': array([17.,  9.]), 'currentState': array([16.23146597,  8.65333395,  0.03260761]), 'targetState': array([17,  9], dtype=int32), 'currentDistance': 0.8431025442887808}
episode index:4270
target Thresh 32.0
target distance 18.0
model initialize at round 4270
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([ 2.        , 11.        ,  1.50286984]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 19.313207915827967}
done in step count: 43
reward sum = 0.349522909975612
running average episode reward sum: 0.4926386797364016
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([19.21558586, 17.61606507,  0.2061124 ]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 0.8733335996324504}
episode index:4271
target Thresh 32.0
target distance 11.0
model initialize at round 4271
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 26.]), 'previousTarget': array([ 7., 26.]), 'currentState': array([17.61877364, 23.00128682,  3.10382517]), 'targetState': array([ 7, 26], dtype=int32), 'currentDistance': 11.034066989082259}
done in step count: 26
reward sum = 0.6183952465786602
running average episode reward sum: 0.4926681171350069
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 26.]), 'previousTarget': array([ 7., 26.]), 'currentState': array([ 7.86761744, 25.76706978,  2.79804576]), 'targetState': array([ 7, 26], dtype=int32), 'currentDistance': 0.8983409780267297}
episode index:4272
target Thresh 32.0
target distance 7.0
model initialize at round 4272
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 17.]), 'previousTarget': array([27., 17.]), 'currentState': array([20.        , 23.        ,  0.52105531]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 21
reward sum = 0.6515378857480856
running average episode reward sum: 0.49270529704809213
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 17.]), 'previousTarget': array([27., 17.]), 'currentState': array([26.08719558, 17.19332089,  5.53869415]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 0.9330513769559518}
episode index:4273
target Thresh 32.0
target distance 12.0
model initialize at round 4273
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 8.]), 'previousTarget': array([7., 8.]), 'currentState': array([13.01316233, 20.01476965,  0.59037888]), 'targetState': array([7, 8], dtype=int32), 'currentDistance': 13.435505608658428}
done in step count: 32
reward sum = 0.4742373545552777
running average episode reward sum: 0.4927009760507845
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 8.]), 'previousTarget': array([7., 8.]), 'currentState': array([7.13113411, 8.84981044, 4.4225343 ]), 'targetState': array([7, 8], dtype=int32), 'currentDistance': 0.8598685548270498}
episode index:4274
target Thresh 32.0
target distance 16.0
model initialize at round 4274
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([26.        , 17.        ,  5.82208043]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 16.278820596099706}
done in step count: 40
reward sum = 0.410416573757273
running average episode reward sum: 0.4926817282373825
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([10.93122831, 14.07465953,  3.31448356]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 0.9342163635799156}
episode index:4275
target Thresh 32.0
target distance 17.0
model initialize at round 4275
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 21.]), 'previousTarget': array([24., 21.]), 'currentState': array([ 7.07331737, 13.25047648,  1.14719993]), 'targetState': array([24, 21], dtype=int32), 'currentDistance': 18.616328848193806}
done in step count: 43
reward sum = 0.3942693940770275
running average episode reward sum: 0.4926587131919755
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 21.]), 'previousTarget': array([24., 21.]), 'currentState': array([23.04340522, 20.59461226,  0.3218726 ]), 'targetState': array([24, 21], dtype=int32), 'currentDistance': 1.0389479223306122}
episode index:4276
target Thresh 32.0
target distance 19.0
model initialize at round 4276
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 17.]), 'previousTarget': array([24., 17.]), 'currentState': array([ 5.11665624, 19.84753666,  5.5290615 ]), 'targetState': array([24, 17], dtype=int32), 'currentDistance': 19.09683577377814}
done in step count: 48
reward sum = 0.3794833060686754
running average episode reward sum: 0.49263225179213377
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 17.]), 'previousTarget': array([24., 17.]), 'currentState': array([23.10578486, 16.77486467,  6.15818651]), 'targetState': array([24, 17], dtype=int32), 'currentDistance': 0.9221207285458423}
episode index:4277
target Thresh 32.0
target distance 4.0
model initialize at round 4277
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([20.        , 25.        ,  5.51942921]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 5.0}
done in step count: 11
reward sum = 0.7893400077645658
running average episode reward sum: 0.49270160844383376
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([16.87533223, 22.65713907,  3.67726799]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 1.0945493473420789}
episode index:4278
target Thresh 32.0
target distance 14.0
model initialize at round 4278
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 18.]), 'previousTarget': array([ 3., 18.]), 'currentState': array([16.98137089,  5.98249574,  3.64803112]), 'targetState': array([ 3, 18], dtype=int32), 'currentDistance': 18.436353779423527}
done in step count: 39
reward sum = 0.3709265918640864
running average episode reward sum: 0.4926731496879142
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 18.]), 'previousTarget': array([ 3., 18.]), 'currentState': array([ 3.78376203, 17.47090139,  2.28219553]), 'targetState': array([ 3, 18], dtype=int32), 'currentDistance': 0.9456364374084076}
episode index:4279
target Thresh 32.0
target distance 5.0
model initialize at round 4279
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 12.]), 'previousTarget': array([ 8., 12.]), 'currentState': array([ 6.0350369 , 16.97905274,  5.55266675]), 'targetState': array([ 8, 12], dtype=int32), 'currentDistance': 5.3527606119211315}
done in step count: 10
reward sum = 0.8123067222745769
running average episode reward sum: 0.4927478304291728
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 12.]), 'previousTarget': array([ 8., 12.]), 'currentState': array([ 7.45964218, 12.65813994,  5.08203723]), 'targetState': array([ 8, 12], dtype=int32), 'currentDistance': 0.8515484488998432}
episode index:4280
target Thresh 32.0
target distance 14.0
model initialize at round 4280
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 26.]), 'previousTarget': array([27., 26.]), 'currentState': array([13.00434123, 16.98254118,  5.20229343]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 16.649114931658303}
done in step count: 42
reward sum = 0.422302354673777
running average episode reward sum: 0.49273137505058007
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 26.]), 'previousTarget': array([27., 26.]), 'currentState': array([26.12425761, 25.19789553,  0.64232326]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 1.1875589740448123}
episode index:4281
target Thresh 32.0
target distance 1.0
model initialize at round 4281
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 23.]), 'previousTarget': array([27., 23.]), 'currentState': array([26.9757581 , 23.74554296,  4.67477955]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 0.745936973699919}
done in step count: 0
reward sum = 0.9968674570431048
running average episode reward sum: 0.49284910883899496
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 23.]), 'previousTarget': array([27., 23.]), 'currentState': array([26.9757581 , 23.74554296,  4.67477955]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 0.745936973699919}
episode index:4282
target Thresh 32.0
target distance 20.0
model initialize at round 4282
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5.62253696, 22.61445347]), 'previousTarget': array([ 5.79270645, 22.2384301 ]), 'currentState': array([13.81932886,  4.37130126,  2.05739772]), 'targetState': array([ 5, 24], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.32321700102119166
running average episode reward sum: 0.4928095029300952
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 24.]), 'previousTarget': array([ 5., 24.]), 'currentState': array([ 5.68526608, 23.07589211,  2.15414134]), 'targetState': array([ 5, 24], dtype=int32), 'currentDistance': 1.1504629511380964}
episode index:4283
target Thresh 32.0
target distance 5.0
model initialize at round 4283
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 7.]), 'previousTarget': array([7., 7.]), 'currentState': array([11.64255958,  8.90335843,  3.44677634]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 5.017582363115362}
done in step count: 9
reward sum = 0.8413855123813767
running average episode reward sum: 0.49289086987908004
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 7.]), 'previousTarget': array([7., 7.]), 'currentState': array([7.91343861, 7.65837546, 3.52375222]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 1.1259788387519134}
episode index:4284
target Thresh 32.0
target distance 17.0
model initialize at round 4284
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 19.]), 'previousTarget': array([ 6., 19.]), 'currentState': array([12.        ,  2.        ,  5.79469883]), 'targetState': array([ 6, 19], dtype=int32), 'currentDistance': 18.02775637731995}
done in step count: 45
reward sum = 0.3552669181927849
running average episode reward sum: 0.4928587522707518
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 19.]), 'previousTarget': array([ 6., 19.]), 'currentState': array([ 6.57700997, 18.2051523 ,  1.9185555 ]), 'targetState': array([ 6, 19], dtype=int32), 'currentDistance': 0.9822033262781211}
episode index:4285
target Thresh 32.0
target distance 18.0
model initialize at round 4285
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'previousTarget': array([19.,  3.]), 'currentState': array([1.40000000e+01, 2.10000000e+01, 7.81726837e-03]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 18.681541692269402}
done in step count: 43
reward sum = 0.3605981691003502
running average episode reward sum: 0.49282789352526174
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'previousTarget': array([19.,  3.]), 'currentState': array([18.98494759,  3.79561396,  4.62187947]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 0.7957563352884781}
episode index:4286
target Thresh 32.0
target distance 8.0
model initialize at round 4286
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 27.]), 'previousTarget': array([ 4., 27.]), 'currentState': array([11.88804211, 24.15973234,  2.25477263]), 'targetState': array([ 4, 27], dtype=int32), 'currentDistance': 8.383813492179703}
done in step count: 19
reward sum = 0.6983290696819264
running average episode reward sum: 0.4928758294189302
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 27.]), 'previousTarget': array([ 4., 27.]), 'currentState': array([ 4.75688333, 27.26138867,  3.12482837]), 'targetState': array([ 4, 27], dtype=int32), 'currentDistance': 0.8007474101610267}
episode index:4287
target Thresh 32.0
target distance 22.0
model initialize at round 4287
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.1078428 ,  6.00345246]), 'previousTarget': array([19.88854382,  6.05572809]), 'currentState': array([ 2.3253799 , 15.15681321,  0.26764321]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.2370305095285889
running average episode reward sum: 0.4928161639991797
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  4.]), 'previousTarget': array([24.,  4.]), 'currentState': array([23.02231117,  4.3436539 ,  5.92848488]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 1.0363269023211013}
episode index:4288
target Thresh 32.0
target distance 9.0
model initialize at round 4288
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  4.]), 'previousTarget': array([26.,  4.]), 'currentState': array([17.41402054,  7.77082418,  5.76541563]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 9.377534765757405}
done in step count: 20
reward sum = 0.6740081249008052
running average episode reward sum: 0.4928584097349927
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  4.]), 'previousTarget': array([26.,  4.]), 'currentState': array([25.12419556,  4.34085465,  6.04770793]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 0.9397953570906806}
episode index:4289
target Thresh 32.0
target distance 2.0
model initialize at round 4289
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 17.]), 'previousTarget': array([22., 17.]), 'currentState': array([22.99167388, 18.86316163,  4.5596984 ]), 'targetState': array([22, 17], dtype=int32), 'currentDistance': 2.1106369519216353}
done in step count: 5
reward sum = 0.9382904901858561
running average episode reward sum: 0.4929622400567761
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 17.]), 'previousTarget': array([22., 17.]), 'currentState': array([22.61619156, 17.85245899,  4.33445333]), 'targetState': array([22, 17], dtype=int32), 'currentDistance': 1.0518452160974867}
episode index:4290
target Thresh 32.0
target distance 25.0
model initialize at round 4290
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.42781353, 10.43046618]), 'previousTarget': array([21.42781353, 10.43046618]), 'currentState': array([14.        , 29.        ,  2.45859593]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 68
reward sum = 0.14370226263971747
running average episode reward sum: 0.49288084644749686
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  4.]), 'previousTarget': array([24.,  4.]), 'currentState': array([23.47054518,  4.81541708,  5.06054642]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 0.9722280759323749}
episode index:4291
target Thresh 32.0
target distance 8.0
model initialize at round 4291
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  7.]), 'previousTarget': array([13.,  7.]), 'currentState': array([ 5.        , 10.        ,  0.22285515]), 'targetState': array([13,  7], dtype=int32), 'currentDistance': 8.54400374531753}
done in step count: 19
reward sum = 0.6860951223942845
running average episode reward sum: 0.4929258637531695
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  7.]), 'previousTarget': array([13.,  7.]), 'currentState': array([12.25252279,  7.11918406,  6.10497087]), 'targetState': array([13,  7], dtype=int32), 'currentDistance': 0.7569194301131817}
episode index:4292
target Thresh 32.0
target distance 14.0
model initialize at round 4292
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([ 2.        , 17.        ,  0.89529073]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 14.317821063276353}
done in step count: 31
reward sum = 0.49910319866914543
running average episode reward sum: 0.49292730268513224
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([15.27079559, 14.12540751,  6.01111899]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 0.7399095316959163}
episode index:4293
target Thresh 32.0
target distance 20.0
model initialize at round 4293
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.5805057 ,  5.44403605]), 'previousTarget': array([19.57218647,  5.43046618]), 'currentState': array([27.040338  , 24.00073031,  6.04878819]), 'targetState': array([19,  4], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 48
reward sum = 0.26907154726156707
running average episode reward sum: 0.4928751704644933
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  4.]), 'previousTarget': array([19.,  4.]), 'currentState': array([18.99681968,  4.73619444,  4.63272444]), 'targetState': array([19,  4], dtype=int32), 'currentDistance': 0.7362013133534581}
episode index:4294
target Thresh 32.0
target distance 10.0
model initialize at round 4294
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([25.76989878, 20.82268348,  3.94709268]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 10.521275951205205}
done in step count: 22
reward sum = 0.635304488154351
running average episode reward sum: 0.49290833212169705
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([22.42138196, 11.65381539,  4.33362198]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 0.7778414499571927}
episode index:4295
target Thresh 32.0
target distance 12.0
model initialize at round 4295
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 14.]), 'previousTarget': array([15., 14.]), 'currentState': array([24.03786661, 25.72110021,  4.66580436]), 'targetState': array([15, 14], dtype=int32), 'currentDistance': 14.80091967241221}
done in step count: 36
reward sum = 0.4912560944850426
running average episode reward sum: 0.49290794752261957
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 14.]), 'previousTarget': array([15., 14.]), 'currentState': array([15.89615358, 14.52154043,  4.13148823]), 'targetState': array([15, 14], dtype=int32), 'currentDistance': 1.0368681975823395}
episode index:4296
target Thresh 32.0
target distance 2.0
model initialize at round 4296
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 18.]), 'previousTarget': array([18., 18.]), 'currentState': array([18.989574  , 20.01124624,  2.57086492]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 2.2415102382865966}
done in step count: 6
reward sum = 0.8866142920502166
running average episode reward sum: 0.4929995710610249
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 18.]), 'previousTarget': array([18., 18.]), 'currentState': array([18.07277848, 18.95374555,  4.41824375]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 0.9565183162112456}
episode index:4297
target Thresh 32.0
target distance 13.0
model initialize at round 4297
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([3.        , 7.        , 4.85131073]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 13.341664064126334}
done in step count: 33
reward sum = 0.513520302381991
running average episode reward sum: 0.49300434554481287
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.03557674,  9.6279739 ,  0.51446634]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 1.0336903000329574}
episode index:4298
target Thresh 32.0
target distance 3.0
model initialize at round 4298
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  3.]), 'previousTarget': array([13.,  3.]), 'currentState': array([11.        ,  6.        ,  3.35175228]), 'targetState': array([13,  3], dtype=int32), 'currentDistance': 3.6055512754639896}
done in step count: 10
reward sum = 0.8215146475747988
running average episode reward sum: 0.49308076106052123
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  3.]), 'previousTarget': array([13.,  3.]), 'currentState': array([12.00706486,  3.28891488,  5.65758144]), 'targetState': array([13,  3], dtype=int32), 'currentDistance': 1.03411411177468}
episode index:4299
target Thresh 32.0
target distance 12.0
model initialize at round 4299
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([22.00301921, 28.00430008,  0.7061429 ]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 12.004300461144476}
done in step count: 29
reward sum = 0.539405859408258
running average episode reward sum: 0.49309153433920677
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([22.05483721, 16.99543687,  4.61872466]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 0.9969461814889325}
episode index:4300
target Thresh 32.0
target distance 14.0
model initialize at round 4300
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 17.]), 'previousTarget': array([ 5., 17.]), 'currentState': array([18.67371712,  3.95089967,  3.07318044]), 'targetState': array([ 5, 17], dtype=int32), 'currentDistance': 18.901046512410602}
done in step count: 52
reward sum = 0.37358838197803834
running average episode reward sum: 0.4930637493700459
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 17.]), 'previousTarget': array([ 5., 17.]), 'currentState': array([ 5.97806197, 16.41475891,  2.43395031]), 'targetState': array([ 5, 17], dtype=int32), 'currentDistance': 1.139786101003585}
episode index:4301
target Thresh 32.0
target distance 11.0
model initialize at round 4301
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 13.]), 'previousTarget': array([16., 13.]), 'currentState': array([5.10653959, 7.90772532, 5.75946289]), 'targetState': array([16, 13], dtype=int32), 'currentDistance': 12.02492166920389}
done in step count: 25
reward sum = 0.5812492216083217
running average episode reward sum: 0.49308424808511747
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 13.]), 'previousTarget': array([16., 13.]), 'currentState': array([15.08711965, 12.40459587,  0.4680369 ]), 'targetState': array([16, 13], dtype=int32), 'currentDistance': 1.089888349395765}
episode index:4302
target Thresh 32.0
target distance 23.0
model initialize at round 4302
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.8839166 , 23.44862101]), 'previousTarget': array([21.86874449, 22.98112317]), 'currentState': array([20.97489102,  3.46928988,  1.6305631 ]), 'targetState': array([22, 26], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 50
reward sum = 0.26991522805715285
running average episode reward sum: 0.49303238449691666
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 26.]), 'previousTarget': array([22., 26.]), 'currentState': array([22.21650237, 25.19852062,  1.20520144]), 'targetState': array([22, 26], dtype=int32), 'currentDistance': 0.8302062850761702}
episode index:4303
target Thresh 32.0
target distance 11.0
model initialize at round 4303
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([11.01716337,  7.28713931,  1.31124961]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 13.980584327856052}
done in step count: 30
reward sum = 0.528068074461851
running average episode reward sum: 0.493040524759455
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([19.42286654, 17.04637778,  0.78586437]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 1.11466513774415}
episode index:4304
target Thresh 32.0
target distance 13.0
model initialize at round 4304
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 16.]), 'previousTarget': array([16., 16.]), 'currentState': array([ 3.        , 17.        ,  1.64401078]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 13.038404810405298}
done in step count: 33
reward sum = 0.5191455570882784
running average episode reward sum: 0.49304658864617484
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 16.]), 'previousTarget': array([16., 16.]), 'currentState': array([15.11901293, 16.10870947,  6.17518544]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 0.8876688387274566}
episode index:4305
target Thresh 32.0
target distance 21.0
model initialize at round 4305
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2.95130299, 21.97736275]), 'previousTarget': array([ 2.95130299, 21.97736275]), 'currentState': array([2.       , 2.       , 0.3810395]), 'targetState': array([ 3, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.3135454512347112
running average episode reward sum: 0.4930049023625215
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 23.]), 'previousTarget': array([ 3., 23.]), 'currentState': array([ 3.18093806, 22.13655592,  1.78823748]), 'targetState': array([ 3, 23], dtype=int32), 'currentDistance': 0.8821985360376796}
episode index:4306
target Thresh 32.0
target distance 13.0
model initialize at round 4306
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([ 8.46524659, 18.81844224,  5.82664326]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 15.325957280029309}
done in step count: 35
reward sum = 0.48562637520137875
running average episode reward sum: 0.49300318921481745
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([20.19137567, 10.51499108,  5.7953771 ]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 0.9586913585719501}
episode index:4307
target Thresh 32.0
target distance 23.0
model initialize at round 4307
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.86874449, 24.98112317]), 'previousTarget': array([24.86874449, 24.98112317]), 'currentState': array([24.        ,  5.        ,  2.44384027]), 'targetState': array([25, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.2651139979427157
running average episode reward sum: 0.4929502901453485
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 28.]), 'previousTarget': array([25., 28.]), 'currentState': array([24.78092401, 27.28375832,  1.60130358]), 'targetState': array([25, 28], dtype=int32), 'currentDistance': 0.748996952120811}
episode index:4308
target Thresh 32.0
target distance 13.0
model initialize at round 4308
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 15.]), 'previousTarget': array([22., 15.]), 'currentState': array([10.92044489,  2.32980141,  1.6439696 ]), 'targetState': array([22, 15], dtype=int32), 'currentDistance': 16.831235056786817}
done in step count: 37
reward sum = 0.4344585157604597
running average episode reward sum: 0.49293671581850124
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 15.]), 'previousTarget': array([22., 15.]), 'currentState': array([21.01543209, 14.39076875,  0.81575196]), 'targetState': array([22, 15], dtype=int32), 'currentDistance': 1.1578154767801465}
episode index:4309
target Thresh 32.0
target distance 13.0
model initialize at round 4309
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([ 3.28472914, 11.09623942,  0.33269991]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 14.468587463994371}
done in step count: 33
reward sum = 0.5273049870502271
running average episode reward sum: 0.49294468989535317
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([15.14486476, 17.36694912,  0.71273278]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 1.0639594447186693}
episode index:4310
target Thresh 32.0
target distance 14.0
model initialize at round 4310
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([12.21427541,  7.11014082,  0.52860276]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 19.569731485973083}
done in step count: 44
reward sum = 0.3861470542942066
running average episode reward sum: 0.4929199166094332
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([25.25792003, 20.1178161 ,  0.8706704 ]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 1.1527927440600059}
episode index:4311
target Thresh 32.0
target distance 9.0
model initialize at round 4311
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([22.56505456, 16.85030906,  3.53743702]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 11.618412621292736}
done in step count: 24
reward sum = 0.6015378329498929
running average episode reward sum: 0.49294510629318555
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.47092026,  9.88641194,  3.9397416 ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.0037390233827022}
episode index:4312
target Thresh 32.0
target distance 16.0
model initialize at round 4312
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 13.]), 'previousTarget': array([23., 13.]), 'currentState': array([25.        , 29.        ,  2.80532447]), 'targetState': array([23, 13], dtype=int32), 'currentDistance': 16.1245154965971}
done in step count: 39
reward sum = 0.4268747727122228
running average episode reward sum: 0.4929297874122255
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 13.]), 'previousTarget': array([23., 13.]), 'currentState': array([22.45067834, 13.92516892,  4.87602353]), 'targetState': array([23, 13], dtype=int32), 'currentDistance': 1.0759608810439936}
episode index:4313
target Thresh 32.0
target distance 18.0
model initialize at round 4313
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.07757937, 24.43884599]), 'previousTarget': array([26.06563667, 24.42900019]), 'currentState': array([ 8.9909809 , 14.04422466,  1.52241388]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.31543983083307814
running average episode reward sum: 0.49288864463137727
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 25.]), 'previousTarget': array([27., 25.]), 'currentState': array([26.48174804, 24.22002295,  0.56729654]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 0.9364557145481857}
episode index:4314
target Thresh 32.0
target distance 20.0
model initialize at round 4314
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5.39217009, 5.11781791]), 'previousTarget': array([5.39299151, 5.12283287]), 'currentState': array([12.01322207, 23.99006395,  5.38891992]), 'targetState': array([5, 4], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 54
reward sum = 0.3124023365650069
running average episode reward sum: 0.4928468169817674
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 4.]), 'previousTarget': array([5., 4.]), 'currentState': array([4.6391274 , 4.89132699, 4.64351565]), 'targetState': array([5, 4], dtype=int32), 'currentDistance': 0.9616095000884702}
episode index:4315
target Thresh 32.0
target distance 7.0
model initialize at round 4315
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 12.]), 'previousTarget': array([16., 12.]), 'currentState': array([15.90824127,  5.1232367 ,  2.07908845]), 'targetState': array([16, 12], dtype=int32), 'currentDistance': 6.877375451889524}
done in step count: 15
reward sum = 0.759249979626252
running average episode reward sum: 0.49290854153288993
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 12.]), 'previousTarget': array([16., 12.]), 'currentState': array([15.76487973, 11.155334  ,  1.25851629]), 'targetState': array([16, 12], dtype=int32), 'currentDistance': 0.8767794434256485}
episode index:4316
target Thresh 32.0
target distance 10.0
model initialize at round 4316
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([27.02037462,  9.99027297,  5.58527327]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 11.674385545472028}
done in step count: 28
reward sum = 0.5627750447932964
running average episode reward sum: 0.49292472557348765
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([17.81050709,  4.6983683 ,  3.6494703 ]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 1.0698785133601283}
episode index:4317
target Thresh 32.0
target distance 8.0
model initialize at round 4317
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([25.62419607,  6.90671449,  3.40769226]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 8.566842118714623}
done in step count: 24
reward sum = 0.6841980679969595
running average episode reward sum: 0.4929690223179118
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([18.41287037,  3.98735263,  4.05382825]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 1.0701995859407951}
episode index:4318
target Thresh 32.0
target distance 14.0
model initialize at round 4318
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([ 4.88011576, 20.78010853,  4.39876315]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 14.700496782491705}
done in step count: 36
reward sum = 0.5039398934218071
running average episode reward sum: 0.4929715624594038
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([9.56577808, 7.92142728, 5.26214636]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 1.018615192732886}
episode index:4319
target Thresh 32.0
target distance 2.0
model initialize at round 4319
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([12.        , 23.        ,  2.31800297]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 7
reward sum = 0.8967608400488791
running average episode reward sum: 0.4930650321995866
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([12.42945441, 24.19310956,  1.05331381]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 0.9882279394220301}
episode index:4320
target Thresh 32.0
target distance 18.0
model initialize at round 4320
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([17.        ,  5.        ,  3.85109806]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 19.697715603592208}
done in step count: 50
reward sum = 0.3307705315179625
running average episode reward sum: 0.4930274727224559
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([ 9.83601539, 22.21800662,  2.34029091]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 1.1447424934533008}
episode index:4321
target Thresh 32.0
target distance 20.0
model initialize at round 4321
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 16.]), 'previousTarget': array([11., 16.]), 'currentState': array([27.        ,  4.        ,  0.39223879]), 'targetState': array([ 7, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 71
reward sum = 0.20147547437275903
running average episode reward sum: 0.4929600150643463
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 19.]), 'previousTarget': array([ 7., 19.]), 'currentState': array([ 7.96484924, 18.85419714,  2.90972543]), 'targetState': array([ 7, 19], dtype=int32), 'currentDistance': 0.9758035344639845}
episode index:4322
target Thresh 32.0
target distance 5.0
model initialize at round 4322
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 22.]), 'previousTarget': array([10., 22.]), 'currentState': array([ 5.38436054, 19.87810551,  6.06775686]), 'targetState': array([10, 22], dtype=int32), 'currentDistance': 5.080016125113417}
done in step count: 10
reward sum = 0.8229112498834757
running average episode reward sum: 0.49303633966180616
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 22.]), 'previousTarget': array([10., 22.]), 'currentState': array([ 9.32988792, 21.12777111,  0.6486368 ]), 'targetState': array([10, 22], dtype=int32), 'currentDistance': 1.0999242868260135}
episode index:4323
target Thresh 32.0
target distance 10.0
model initialize at round 4323
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 27.]), 'previousTarget': array([20., 27.]), 'currentState': array([10.00232139, 22.02754626,  1.23597378]), 'targetState': array([20, 27], dtype=int32), 'currentDistance': 11.165969449198974}
done in step count: 25
reward sum = 0.6005186056159614
running average episode reward sum: 0.4930611968000934
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 27.]), 'previousTarget': array([20., 27.]), 'currentState': array([19.4202201 , 26.01940964,  0.57006326]), 'targetState': array([20, 27], dtype=int32), 'currentDistance': 1.1391673222981615}
episode index:4324
target Thresh 32.0
target distance 13.0
model initialize at round 4324
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 11.]), 'previousTarget': array([24., 11.]), 'currentState': array([12.        , 24.        ,  2.50285508]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 17.69180601295413}
done in step count: 60
reward sum = 0.3097396877255188
running average episode reward sum: 0.4930188103240068
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 11.]), 'previousTarget': array([24., 11.]), 'currentState': array([23.24604609, 11.45114339,  5.73708088]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 0.8786221365872345}
episode index:4325
target Thresh 32.0
target distance 23.0
model initialize at round 4325
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.83527011, 11.88816918]), 'previousTarget': array([16.07464439, 12.24778806]), 'currentState': array([26.75095883, 28.64668164,  4.12461519]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.1801192737720162
running average episode reward sum: 0.4929464803340503
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([12.89605273,  6.99290639,  4.18213916]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 1.3374504077238916}
episode index:4326
target Thresh 32.0
target distance 12.0
model initialize at round 4326
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 19.]), 'previousTarget': array([ 9., 19.]), 'currentState': array([4.        , 7.        , 5.89026403]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 13.0}
done in step count: 32
reward sum = 0.5268679787462365
running average episode reward sum: 0.4929543198298701
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 19.]), 'previousTarget': array([ 9., 19.]), 'currentState': array([ 8.49410684, 18.1963725 ,  1.15373094]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 0.9496025771252}
episode index:4327
target Thresh 32.0
target distance 1.0
model initialize at round 4327
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 25.]), 'previousTarget': array([ 9., 25.]), 'currentState': array([ 9.91364043, 24.25194844,  2.01797208]), 'targetState': array([ 9, 25], dtype=int32), 'currentDistance': 1.1808132714859259}
done in step count: 0
reward sum = 0.9950042577818947
running average episode reward sum: 0.49307032027764097
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 25.]), 'previousTarget': array([ 9., 25.]), 'currentState': array([ 9.91364043, 24.25194844,  2.01797208]), 'targetState': array([ 9, 25], dtype=int32), 'currentDistance': 1.1808132714859259}
episode index:4328
target Thresh 32.0
target distance 13.0
model initialize at round 4328
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([12.7670849 , 11.37738354,  2.14449776]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 13.172751224358498}
done in step count: 29
reward sum = 0.5574832942476395
running average episode reward sum: 0.49308519968950737
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([ 9.78212798, 23.21282543,  1.79120985]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 1.1096702111702663}
episode index:4329
target Thresh 32.0
target distance 3.0
model initialize at round 4329
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  8.]), 'previousTarget': array([12.,  8.]), 'currentState': array([10.34725786,  5.34590494,  0.77634649]), 'targetState': array([12,  8], dtype=int32), 'currentDistance': 3.1266239244462426}
done in step count: 6
reward sum = 0.8916296542288322
running average episode reward sum: 0.4931772422887082
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  8.]), 'previousTarget': array([12.,  8.]), 'currentState': array([12.00306076,  7.04642895,  1.07104268]), 'targetState': array([12,  8], dtype=int32), 'currentDistance': 0.953575963318603}
episode index:4330
target Thresh 32.0
target distance 2.0
model initialize at round 4330
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([15.01730373, 11.99218206,  6.10222167]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 2.2354026088764147}
done in step count: 6
reward sum = 0.8920166452774984
running average episode reward sum: 0.4932693317375627
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([16.09218444, 13.22746813,  1.3606552 ]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 0.7780125040722513}
episode index:4331
target Thresh 32.0
target distance 15.0
model initialize at round 4331
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([21.        ,  7.        ,  5.91425037]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 16.55294535724685}
done in step count: 45
reward sum = 0.40038430809616465
running average episode reward sum: 0.4932478901346908
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([14.6960914 , 21.16933211,  2.19733571]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 1.0837676743070874}
episode index:4332
target Thresh 32.0
target distance 20.0
model initialize at round 4332
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5.99875234, 9.02495322]), 'previousTarget': array([5.99875234, 9.02495322]), 'currentState': array([ 5.        , 29.        ,  2.42068714]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.28686559439728465
running average episode reward sum: 0.4932002597871861
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([5.50415092, 9.80850692, 5.26617309]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.9484459675198161}
episode index:4333
target Thresh 32.0
target distance 15.0
model initialize at round 4333
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  3.]), 'previousTarget': array([13.,  3.]), 'currentState': array([12.92660583, 17.64536602,  4.52790303]), 'targetState': array([13,  3], dtype=int32), 'currentDistance': 14.645549921522074}
done in step count: 37
reward sum = 0.5053113513004861
running average episode reward sum: 0.49320305422454497
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  3.]), 'previousTarget': array([13.,  3.]), 'currentState': array([12.36462977,  3.76408237,  4.97083121]), 'targetState': array([13,  3], dtype=int32), 'currentDistance': 0.9937389983828083}
episode index:4334
target Thresh 32.0
target distance 10.0
model initialize at round 4334
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 23.]), 'previousTarget': array([22., 23.]), 'currentState': array([12.4138305 , 12.98428782,  0.08803607]), 'targetState': array([22, 23], dtype=int32), 'currentDistance': 13.86395095566821}
done in step count: 35
reward sum = 0.5068730077818421
running average episode reward sum: 0.493206207616369
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 23.]), 'previousTarget': array([22., 23.]), 'currentState': array([21.93583033, 22.05690409,  1.03064057]), 'targetState': array([22, 23], dtype=int32), 'currentDistance': 0.9452764918052629}
episode index:4335
target Thresh 32.0
target distance 15.0
model initialize at round 4335
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([24.        , 16.        ,  1.94397441]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 15.811388300841898}
done in step count: 40
reward sum = 0.4485639188277083
running average episode reward sum: 0.4931959118855599
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.84184733, 11.56606291,  3.59674007]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.0144624900810542}
episode index:4336
target Thresh 32.0
target distance 20.0
model initialize at round 4336
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.98114735,  3.09212629]), 'previousTarget': array([21.9223227 ,  3.38838649]), 'currentState': array([17.97145862, 22.68606403,  4.61700961]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 48
reward sum = 0.3706065824288035
running average episode reward sum: 0.4931676459576243
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  3.]), 'previousTarget': array([22.,  3.]), 'currentState': array([21.37727623,  3.95725852,  4.99494396]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 1.1419845762722183}
episode index:4337
target Thresh 32.0
target distance 12.0
model initialize at round 4337
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 25.]), 'previousTarget': array([24., 25.]), 'currentState': array([14.06940245, 13.19991754,  1.13187656]), 'targetState': array([24, 25], dtype=int32), 'currentDistance': 15.42266882698916}
done in step count: 33
reward sum = 0.4941285895048154
running average episode reward sum: 0.49316786747527
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 25.]), 'previousTarget': array([24., 25.]), 'currentState': array([23.10147546, 24.32534041,  0.84870674]), 'targetState': array([24, 25], dtype=int32), 'currentDistance': 1.1236155532341723}
episode index:4338
target Thresh 32.0
target distance 17.0
model initialize at round 4338
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 22.]), 'previousTarget': array([ 8., 22.]), 'currentState': array([2.01153326, 4.9989336 , 0.14304852]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 18.024926978232426}
done in step count: 40
reward sum = 0.39745183095301717
running average episode reward sum: 0.49314580800614755
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 22.]), 'previousTarget': array([ 8., 22.]), 'currentState': array([ 8.13191171, 21.00101316,  1.7230484 ]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 1.0076583796143295}
episode index:4339
target Thresh 32.0
target distance 14.0
model initialize at round 4339
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 20.]), 'previousTarget': array([20., 20.]), 'currentState': array([20.04600148,  6.00029809,  0.25897997]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 13.999777485222431}
done in step count: 34
reward sum = 0.4945223171426111
running average episode reward sum: 0.49314612517415135
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 20.]), 'previousTarget': array([20., 20.]), 'currentState': array([20.42789575, 19.22671553,  1.90702312]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 0.8837780496576474}
episode index:4340
target Thresh 32.0
target distance 16.0
model initialize at round 4340
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([24.08293715, 24.84720013,  5.09430791]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 15.87371274501895}
done in step count: 43
reward sum = 0.4597594463516782
running average episode reward sum: 0.49313843416313485
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([25.34479944,  9.87633577,  4.63063764]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 0.941727685439746}
episode index:4341
target Thresh 32.0
target distance 10.0
model initialize at round 4341
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 18.]), 'previousTarget': array([18., 18.]), 'currentState': array([ 8.13378624, 23.9819075 ,  6.00549475]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 11.537997717632885}
done in step count: 32
reward sum = 0.5691862685430196
running average episode reward sum: 0.4931559486344338
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 18.]), 'previousTarget': array([18., 18.]), 'currentState': array([17.05810737, 17.62631111,  6.15482516]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 1.013313922338689}
episode index:4342
target Thresh 32.0
target distance 21.0
model initialize at round 4342
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.28013989, 11.28336929]), 'previousTarget': array([16.28013989, 11.28336929]), 'currentState': array([ 7.        , 29.        ,  0.40730503]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.22909533261838183
running average episode reward sum: 0.49309514720316133
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([17.08986862,  8.17178078,  5.46476766]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 0.9262007194796459}
episode index:4343
target Thresh 32.0
target distance 21.0
model initialize at round 4343
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.88526795,  8.60813592]), 'previousTarget': array([21.87838597,  8.6170994 ]), 'currentState': array([14.02499829, 26.99879009,  6.00895229]), 'targetState': array([23,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.2770000184578454
running average episode reward sum: 0.4930454015473728
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  6.]), 'previousTarget': array([23.,  6.]), 'currentState': array([22.68769728,  6.94438816,  5.02268132]), 'targetState': array([23,  6], dtype=int32), 'currentDistance': 0.9946868757960524}
episode index:4344
target Thresh 32.0
target distance 15.0
model initialize at round 4344
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  5.]), 'previousTarget': array([21.,  5.]), 'currentState': array([ 5.99667456, 14.0150707 ,  1.53547239]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 17.503464626918174}
done in step count: 43
reward sum = 0.384088708134153
running average episode reward sum: 0.4930203252082674
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  5.]), 'previousTarget': array([21.,  5.]), 'currentState': array([20.05928113,  5.37150517,  5.79891579]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 1.0114188509629365}
episode index:4345
target Thresh 32.0
target distance 13.0
model initialize at round 4345
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 29.]), 'previousTarget': array([14., 29.]), 'currentState': array([12.29057535, 16.22498962,  0.78085099]), 'targetState': array([14, 29], dtype=int32), 'currentDistance': 12.888872055585562}
done in step count: 32
reward sum = 0.5514447024693081
running average episode reward sum: 0.4930337684612036
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 29.]), 'previousTarget': array([14., 29.]), 'currentState': array([14.16993851, 28.14682866,  1.34266058]), 'targetState': array([14, 29], dtype=int32), 'currentDistance': 0.8699312776481583}
episode index:4346
target Thresh 32.0
target distance 15.0
model initialize at round 4346
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 22.]), 'previousTarget': array([11., 22.]), 'currentState': array([10.0101194 ,  6.97619029,  5.36676931]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 15.056384754359186}
done in step count: 39
reward sum = 0.4332110196463371
running average episode reward sum: 0.49302000661422535
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 22.]), 'previousTarget': array([11., 22.]), 'currentState': array([11.69086516, 21.1953702 ,  1.73064841]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 1.0605299516341125}
episode index:4347
target Thresh 32.0
target distance 14.0
model initialize at round 4347
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 12.]), 'previousTarget': array([11., 12.]), 'currentState': array([15.97285627, 26.04014468,  2.41783392]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 14.894796479294547}
done in step count: 39
reward sum = 0.45716150671230027
running average episode reward sum: 0.49301175948913295
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 12.]), 'previousTarget': array([11., 12.]), 'currentState': array([11.76165682, 12.87819302,  4.39234409]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 1.1624732701529943}
episode index:4348
target Thresh 32.0
target distance 22.0
model initialize at round 4348
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.74582612,  8.71811917]), 'previousTarget': array([16.73765188,  8.70472358]), 'currentState': array([22.03802619, 28.00522933,  6.17850495]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.25101070768109823
running average episode reward sum: 0.4929561142714258
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.95672496,  6.88854776,  4.35652717]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.3056951273206865}
episode index:4349
target Thresh 32.0
target distance 14.0
model initialize at round 4349
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 23.]), 'previousTarget': array([25., 23.]), 'currentState': array([11.29902263, 27.99524233,  6.16161158]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 14.583183015525227}
done in step count: 30
reward sum = 0.5080663444879409
running average episode reward sum: 0.49295958788756755
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 23.]), 'previousTarget': array([25., 23.]), 'currentState': array([24.20543104, 23.21953933,  6.16308642]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 0.8243405574797699}
episode index:4350
target Thresh 32.0
target distance 16.0
model initialize at round 4350
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([20.        , 14.        ,  1.61223614]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 19.4164878389476}
done in step count: 62
reward sum = 0.3222855472550038
running average episode reward sum: 0.4929203614934897
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.4918514 , 3.98639543, 3.79674997]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.1022221890246213}
episode index:4351
target Thresh 32.0
target distance 8.0
model initialize at round 4351
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([12.        , 11.        ,  1.74659872]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 10.0}
done in step count: 27
reward sum = 0.5785948038946273
running average episode reward sum: 0.49294004771646793
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([19.00379579,  4.96129785,  5.79839929]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 0.9969557070653252}
episode index:4352
target Thresh 32.0
target distance 2.0
model initialize at round 4352
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.       ,  9.       ,  5.4754706]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.9999999999999998}
done in step count: 8
reward sum = 0.8646877570303848
running average episode reward sum: 0.49302544806319754
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.39710276, 10.18970673,  1.69083997]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.9023667666676979}
episode index:4353
target Thresh 32.0
target distance 13.0
model initialize at round 4353
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([ 8.        , 16.        ,  3.61078787]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 13.152946437965904}
done in step count: 35
reward sum = 0.497985710481324
running average episode reward sum: 0.49302658730582916
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([ 9.9470507 , 28.20823659,  1.45737475]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 0.7935319344768188}
episode index:4354
target Thresh 32.0
target distance 1.0
model initialize at round 4354
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 24.]), 'previousTarget': array([24., 24.]), 'currentState': array([24.0141496 , 22.97288674,  5.31393932]), 'targetState': array([24, 24], dtype=int32), 'currentDistance': 1.0272107171314364}
done in step count: 5
reward sum = 0.9202003335476547
running average episode reward sum: 0.493124675422073
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 24.]), 'previousTarget': array([24., 24.]), 'currentState': array([24.14586287, 23.06826383,  0.90815965]), 'targetState': array([24, 24], dtype=int32), 'currentDistance': 0.9430844463454584}
episode index:4355
target Thresh 32.0
target distance 11.0
model initialize at round 4355
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 20.]), 'previousTarget': array([24., 20.]), 'currentState': array([19.95799234,  9.27136831,  1.60291117]), 'targetState': array([24, 20], dtype=int32), 'currentDistance': 11.46478799761484}
done in step count: 24
reward sum = 0.6168005213828648
running average episode reward sum: 0.4931530674895572
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 20.]), 'previousTarget': array([24., 20.]), 'currentState': array([23.51052359, 19.03341327,  1.09942138]), 'targetState': array([24, 20], dtype=int32), 'currentDistance': 1.0834560777932005}
episode index:4356
target Thresh 32.0
target distance 12.0
model initialize at round 4356
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 13.]), 'previousTarget': array([10., 13.]), 'currentState': array([21.91545771, 11.89912328,  3.76235867]), 'targetState': array([10, 13], dtype=int32), 'currentDistance': 11.96620499623303}
done in step count: 27
reward sum = 0.5794327362039617
running average episode reward sum: 0.49317287003000115
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 13.]), 'previousTarget': array([10., 13.]), 'currentState': array([10.90837237, 12.5608443 ,  3.04749825]), 'targetState': array([10, 13], dtype=int32), 'currentDistance': 1.0089589166496518}
episode index:4357
target Thresh 32.0
target distance 14.0
model initialize at round 4357
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 18.]), 'previousTarget': array([11., 18.]), 'currentState': array([24.80411028,  9.92949656,  3.30750453]), 'targetState': array([11, 18], dtype=int32), 'currentDistance': 15.990199699770324}
done in step count: 41
reward sum = 0.4642149438475198
running average episode reward sum: 0.4931662252557509
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 18.]), 'previousTarget': array([11., 18.]), 'currentState': array([11.86626572, 17.55092909,  2.47122667]), 'targetState': array([11, 18], dtype=int32), 'currentDistance': 0.975746368904608}
episode index:4358
target Thresh 32.0
target distance 11.0
model initialize at round 4358
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([15.        , 15.        ,  0.90171289]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 12.083045973594574}
done in step count: 29
reward sum = 0.523242761080302
running average episode reward sum: 0.49317312512632316
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.83830828, 10.27269838,  3.68116191]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.8815470359110805}
episode index:4359
target Thresh 32.0
target distance 5.0
model initialize at round 4359
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 13.]), 'previousTarget': array([18., 13.]), 'currentState': array([20.78455276,  8.07132764,  2.64357287]), 'targetState': array([18, 13], dtype=int32), 'currentDistance': 5.660878486636687}
done in step count: 10
reward sum = 0.8022478198244845
running average episode reward sum: 0.4932440138177677
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 13.]), 'previousTarget': array([18., 13.]), 'currentState': array([18.20999559, 12.14435005,  1.96206059]), 'targetState': array([18, 13], dtype=int32), 'currentDistance': 0.8810419886883467}
episode index:4360
target Thresh 32.0
target distance 3.0
model initialize at round 4360
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([23.0131752 ,  5.89506794,  5.08979464]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 3.686200742980113}
done in step count: 9
reward sum = 0.8030993375063002
running average episode reward sum: 0.4933150652563572
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([24.97424966,  8.17936975,  1.35698268]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 0.8210341536053207}
episode index:4361
target Thresh 32.0
target distance 8.0
model initialize at round 4361
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 14.]), 'previousTarget': array([24., 14.]), 'currentState': array([21.05386406,  5.9886968 ,  0.04565447]), 'targetState': array([24, 14], dtype=int32), 'currentDistance': 8.535847703035952}
done in step count: 19
reward sum = 0.6725363018225329
running average episode reward sum: 0.49335615219733986
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 14.]), 'previousTarget': array([24., 14.]), 'currentState': array([23.77030426, 13.20463429,  1.30462154]), 'targetState': array([24, 14], dtype=int32), 'currentDistance': 0.8278688012201161}
episode index:4362
target Thresh 32.0
target distance 21.0
model initialize at round 4362
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4.55464716, 16.11146002]), 'previousTarget': array([ 4.6170994 , 16.12161403]), 'currentState': array([22.89409508, 24.09047311,  2.5623768 ]), 'targetState': array([ 2, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.2808290602711254
running average episode reward sum: 0.49330744096838586
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 15.]), 'previousTarget': array([ 2., 15.]), 'currentState': array([ 2.82036893, 14.86389446,  3.33328923]), 'targetState': array([ 2, 15], dtype=int32), 'currentDistance': 0.8315827688734427}
episode index:4363
target Thresh 32.0
target distance 18.0
model initialize at round 4363
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  6.]), 'previousTarget': array([11.,  6.]), 'currentState': array([12.        , 24.        ,  2.06311786]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 18.027756377319946}
done in step count: 47
reward sum = 0.34985371868243464
running average episode reward sum: 0.493274568896368
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  6.]), 'previousTarget': array([11.,  6.]), 'currentState': array([10.82496503,  6.76802229,  4.66463625]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 0.7877153548505361}
episode index:4364
target Thresh 32.0
target distance 18.0
model initialize at round 4364
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 23.]), 'previousTarget': array([ 3., 23.]), 'currentState': array([6.19499584, 5.13839118, 0.73944841]), 'targetState': array([ 3, 23], dtype=int32), 'currentDistance': 18.145111406450486}
done in step count: 46
reward sum = 0.39051425352115865
running average episode reward sum: 0.49325102701426604
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 23.]), 'previousTarget': array([ 3., 23.]), 'currentState': array([ 3.83608812, 22.37602609,  2.06127975]), 'targetState': array([ 3, 23], dtype=int32), 'currentDistance': 1.0432577761530637}
episode index:4365
target Thresh 32.0
target distance 8.0
model initialize at round 4365
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 18.]), 'previousTarget': array([13., 18.]), 'currentState': array([ 4.98969212, 21.99608761,  3.75684929]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 8.951745553895988}
done in step count: 22
reward sum = 0.6275265769677771
running average episode reward sum: 0.4932817818356022
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 18.]), 'previousTarget': array([13., 18.]), 'currentState': array([12.10234639, 17.69130088,  5.94507093]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 0.9492508360529747}
episode index:4366
target Thresh 32.0
target distance 9.0
model initialize at round 4366
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 13.]), 'previousTarget': array([ 9., 13.]), 'currentState': array([11.        ,  4.        ,  5.95053267]), 'targetState': array([ 9, 13], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 31
reward sum = 0.5780689615940104
running average episode reward sum: 0.49330119726490335
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 13.]), 'previousTarget': array([ 9., 13.]), 'currentState': array([ 9.95054103, 13.00030841,  2.39696761]), 'targetState': array([ 9, 13], dtype=int32), 'currentDistance': 0.9505410815513139}
episode index:4367
target Thresh 32.0
target distance 8.0
model initialize at round 4367
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 12.]), 'previousTarget': array([ 3., 12.]), 'currentState': array([5.00358991, 3.99611615, 5.69538862]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 8.250850208352213}
done in step count: 22
reward sum = 0.6467237198102805
running average episode reward sum: 0.4933363214687828
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 12.]), 'previousTarget': array([ 3., 12.]), 'currentState': array([ 3.56724255, 11.27644872,  1.83295402]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 0.9193968484485038}
episode index:4368
target Thresh 32.0
target distance 5.0
model initialize at round 4368
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 6.]), 'previousTarget': array([8., 6.]), 'currentState': array([3.        , 5.        , 4.52146602]), 'targetState': array([8, 6], dtype=int32), 'currentDistance': 5.099019513592785}
done in step count: 12
reward sum = 0.7725980567605312
running average episode reward sum: 0.49340024038278874
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 6.]), 'previousTarget': array([8., 6.]), 'currentState': array([7.22170776, 5.45763705, 0.21607024]), 'targetState': array([8, 6], dtype=int32), 'currentDistance': 0.9486286806721638}
episode index:4369
target Thresh 32.0
target distance 13.0
model initialize at round 4369
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([14.00232913, 11.02713582,  1.23267376]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 15.276595085460608}
done in step count: 35
reward sum = 0.4516890942918338
running average episode reward sum: 0.4933906954980997
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([26.13621109,  3.28926853,  5.4368648 ]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 0.9109377413209997}
episode index:4370
target Thresh 32.0
target distance 18.0
model initialize at round 4370
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([26.77059464, 22.82555723,  3.6936657 ]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 17.771450816188622}
done in step count: 43
reward sum = 0.41524189649451704
running average episode reward sum: 0.49337281656902093
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([ 9.73440331, 23.19515079,  3.02802194]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 0.7598894988566337}
episode index:4371
target Thresh 32.0
target distance 5.0
model initialize at round 4371
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([10.03000755, 12.03953953,  0.66910744]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 6.415434415001487}
done in step count: 14
reward sum = 0.7351046910830132
running average episode reward sum: 0.4934281074826792
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.58684578,  7.7349888 ,  4.9671339 ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.8431517899449975}
episode index:4372
target Thresh 32.0
target distance 4.0
model initialize at round 4372
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'previousTarget': array([10., 17.]), 'currentState': array([ 8.46180774, 13.08052859,  0.34486499]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 4.2104977776408905}
done in step count: 9
reward sum = 0.8324405792176668
running average episode reward sum: 0.4935056314871921
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'previousTarget': array([10., 17.]), 'currentState': array([10.22937266, 16.08774899,  1.45187524]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 0.9406453749477682}
episode index:4373
target Thresh 32.0
target distance 17.0
model initialize at round 4373
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.96132727,  5.79098405]), 'previousTarget': array([24.88715666,  5.85099785]), 'currentState': array([ 9.04984541, 17.90811015,  5.25743337]), 'targetState': array([26,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 71
reward sum = 0.29688861406800876
running average episode reward sum: 0.4934606801800547
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  5.]), 'previousTarget': array([26.,  5.]), 'currentState': array([25.06867418,  4.66107797,  5.99554896]), 'targetState': array([26,  5], dtype=int32), 'currentDistance': 0.9910781627399365}
episode index:4374
target Thresh 32.0
target distance 16.0
model initialize at round 4374
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 24.]), 'previousTarget': array([ 8., 24.]), 'currentState': array([17.68046169,  8.29746639,  2.3589986 ]), 'targetState': array([ 8, 24], dtype=int32), 'currentDistance': 18.44670432137635}
done in step count: 50
reward sum = 0.39138343046276036
running average episode reward sum: 0.4934373482372621
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 24.]), 'previousTarget': array([ 8., 24.]), 'currentState': array([ 8.13585737, 23.10867527,  2.15753149]), 'targetState': array([ 8, 24], dtype=int32), 'currentDistance': 0.901619096283209}
episode index:4375
target Thresh 32.0
target distance 21.0
model initialize at round 4375
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.37055368,  4.0771564 ]), 'previousTarget': array([21.36486284,  4.07722123]), 'currentState': array([ 4.01226069, 14.01142359,  0.56396091]), 'targetState': array([25,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 78
reward sum = 0.2103287468915237
running average episode reward sum: 0.4933726524874117
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  2.]), 'previousTarget': array([25.,  2.]), 'currentState': array([24.19478411,  1.85841561,  6.12745924]), 'targetState': array([25,  2], dtype=int32), 'currentDistance': 0.8175688194568727}
episode index:4376
target Thresh 32.0
target distance 21.0
model initialize at round 4376
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.98986325,  8.48976965]), 'previousTarget': array([12.92277877,  8.63513716]), 'currentState': array([ 3.00730941, 25.82033856,  4.72653301]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.27089428081184214
running average episode reward sum: 0.49332182352426895
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.17155538,  5.82278088,  5.16469627]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.167599615033337}
episode index:4377
target Thresh 32.0
target distance 9.0
model initialize at round 4377
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 16.]), 'previousTarget': array([25., 16.]), 'currentState': array([18.01502203,  7.38801467,  1.40973464]), 'targetState': array([25, 16], dtype=int32), 'currentDistance': 11.088562056763879}
done in step count: 21
reward sum = 0.6271660273980917
running average episode reward sum: 0.4933523955214992
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 16.]), 'previousTarget': array([25., 16.]), 'currentState': array([24.17217142, 15.1727366 ,  0.84187981]), 'targetState': array([25, 16], dtype=int32), 'currentDistance': 1.1703268308925103}
episode index:4378
target Thresh 32.0
target distance 20.0
model initialize at round 4378
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6.48919624, 17.48390008]), 'previousTarget': array([ 6.61536159, 17.53075311]), 'currentState': array([22.78235957, 29.08272885,  2.78818228]), 'targetState': array([ 3, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.17521177172875635
running average episode reward sum: 0.49319972044334204
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 15.]), 'previousTarget': array([ 3., 15.]), 'currentState': array([ 4.68982055, 19.7351795 ,  4.4386211 ]), 'targetState': array([ 3, 15], dtype=int32), 'currentDistance': 5.027665300677271}
episode index:4379
target Thresh 32.0
target distance 9.0
model initialize at round 4379
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  6.]), 'previousTarget': array([10.,  6.]), 'currentState': array([ 3.        , 15.        ,  0.94860122]), 'targetState': array([10,  6], dtype=int32), 'currentDistance': 11.40175425099138}
done in step count: 28
reward sum = 0.5683818404414993
running average episode reward sum: 0.4932168853109215
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  6.]), 'previousTarget': array([10.,  6.]), 'currentState': array([9.0627363 , 6.55899901, 5.21277176]), 'targetState': array([10,  6], dtype=int32), 'currentDistance': 1.0913034136779989}
episode index:4380
target Thresh 32.0
target distance 18.0
model initialize at round 4380
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 20.]), 'previousTarget': array([ 7., 20.]), 'currentState': array([24.5947109 , 27.92493481,  3.29946513]), 'targetState': array([ 7, 20], dtype=int32), 'currentDistance': 19.29710971236845}
done in step count: 46
reward sum = 0.37113802552882796
running average episode reward sum: 0.49318901978711827
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 20.]), 'previousTarget': array([ 7., 20.]), 'currentState': array([ 7.21532464, 20.96363051,  3.9469395 ]), 'targetState': array([ 7, 20], dtype=int32), 'currentDistance': 0.9873947825813246}
episode index:4381
target Thresh 32.0
target distance 12.0
model initialize at round 4381
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 14.]), 'previousTarget': array([25., 14.]), 'currentState': array([21.0133535 , 26.00169373,  6.1851801 ]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 12.646501600917615}
done in step count: 32
reward sum = 0.5448591746868892
running average episode reward sum: 0.4932008112419105
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 14.]), 'previousTarget': array([25., 14.]), 'currentState': array([25.48919041, 14.92954977,  5.02519431]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 1.050414221847218}
episode index:4382
target Thresh 32.0
target distance 22.0
model initialize at round 4382
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.1833587 , 19.25602145]), 'previousTarget': array([20.20732955, 19.27605889]), 'currentState': array([ 1.99247856, 10.94386892,  4.83168459]), 'targetState': array([24, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 72
reward sum = 0.21245938879460496
running average episode reward sum: 0.4931367588982082
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 21.]), 'previousTarget': array([24., 21.]), 'currentState': array([23.02696227, 20.70534346,  0.70121782]), 'targetState': array([24, 21], dtype=int32), 'currentDistance': 1.0166734519117764}
episode index:4383
target Thresh 32.0
target distance 12.0
model initialize at round 4383
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  9.]), 'previousTarget': array([17.,  9.]), 'currentState': array([5.02616094, 4.22094722, 1.20044148]), 'targetState': array([17,  9], dtype=int32), 'currentDistance': 12.892329787629093}
done in step count: 30
reward sum = 0.5333175916879078
running average episode reward sum: 0.4931459242341548
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  9.]), 'previousTarget': array([17.,  9.]), 'currentState': array([16.1203632 ,  9.24506564,  0.24806113]), 'targetState': array([17,  9], dtype=int32), 'currentDistance': 0.9131363909007993}
episode index:4384
target Thresh 32.0
target distance 8.0
model initialize at round 4384
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  4.]), 'previousTarget': array([22.,  4.]), 'currentState': array([14.        , 11.        ,  0.95744598]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 10.63014581273465}
done in step count: 28
reward sum = 0.5881722790044268
running average episode reward sum: 0.49316759501061325
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  4.]), 'previousTarget': array([22.,  4.]), 'currentState': array([21.61451612,  4.80099583,  5.37595084]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 0.8889275268811153}
episode index:4385
target Thresh 32.0
target distance 9.0
model initialize at round 4385
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 22.]), 'previousTarget': array([15., 22.]), 'currentState': array([ 6.12083806, 17.8661965 ,  5.69899917]), 'targetState': array([15, 22], dtype=int32), 'currentDistance': 9.79427628910423}
done in step count: 27
reward sum = 0.623412664190027
running average episode reward sum: 0.493197290648821
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 22.]), 'previousTarget': array([15., 22.]), 'currentState': array([14.77324507, 21.02358629,  0.63469781]), 'targetState': array([15, 22], dtype=int32), 'currentDistance': 1.0023978912105196}
episode index:4386
target Thresh 32.0
target distance 8.0
model initialize at round 4386
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 18.]), 'previousTarget': array([24., 18.]), 'currentState': array([15.99125157, 16.01064953,  2.00600147]), 'targetState': array([24, 18], dtype=int32), 'currentDistance': 8.252124983217975}
done in step count: 21
reward sum = 0.6578654390466907
running average episode reward sum: 0.4932348261282826
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 18.]), 'previousTarget': array([24., 18.]), 'currentState': array([23.03316486, 18.65107781,  6.10995002]), 'targetState': array([24, 18], dtype=int32), 'currentDistance': 1.1656210813892613}
episode index:4387
target Thresh 32.0
target distance 15.0
model initialize at round 4387
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 14.]), 'previousTarget': array([ 7., 14.]), 'currentState': array([22.03205459, 24.99048301,  5.74207544]), 'targetState': array([ 7, 14], dtype=int32), 'currentDistance': 18.62131525766564}
done in step count: 48
reward sum = 0.35075343378013946
running average episode reward sum: 0.49320235543722785
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 14.]), 'previousTarget': array([ 7., 14.]), 'currentState': array([ 7.82815178, 14.52526358,  3.56991564]), 'targetState': array([ 7, 14], dtype=int32), 'currentDistance': 0.9806820086043179}
episode index:4388
target Thresh 32.0
target distance 15.0
model initialize at round 4388
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 5.]), 'previousTarget': array([8., 5.]), 'currentState': array([ 6.81738757, 19.60523351,  4.40088958]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 14.653034432653637}
done in step count: 33
reward sum = 0.5099092281135386
running average episode reward sum: 0.49320616197007733
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 5.]), 'previousTarget': array([8., 5.]), 'currentState': array([7.72769917, 5.96550575, 4.56009879]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 1.0031695301073533}
episode index:4389
target Thresh 32.0
target distance 5.0
model initialize at round 4389
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 21.]), 'previousTarget': array([20., 21.]), 'currentState': array([20.72902665, 16.31495268,  2.1759952 ]), 'targetState': array([20, 21], dtype=int32), 'currentDistance': 4.741428928212849}
done in step count: 10
reward sum = 0.8319843209402745
running average episode reward sum: 0.4932833323935329
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 21.]), 'previousTarget': array([20., 21.]), 'currentState': array([19.95035349, 20.06049992,  1.44972012]), 'targetState': array([20, 21], dtype=int32), 'currentDistance': 0.9408109102818297}
episode index:4390
target Thresh 32.0
target distance 17.0
model initialize at round 4390
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 20.]), 'previousTarget': array([22., 20.]), 'currentState': array([20.        ,  3.        ,  5.63578045]), 'targetState': array([22, 20], dtype=int32), 'currentDistance': 17.11724276862369}
done in step count: 51
reward sum = 0.38514044602948017
running average episode reward sum: 0.49325870408873573
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 20.]), 'previousTarget': array([22., 20.]), 'currentState': array([21.56052615, 19.02655196,  1.5171703 ]), 'targetState': array([22, 20], dtype=int32), 'currentDistance': 1.0680535332502004}
episode index:4391
target Thresh 32.0
target distance 12.0
model initialize at round 4391
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 24.]), 'previousTarget': array([ 3., 24.]), 'currentState': array([14.79400356, 12.92923813,  3.32361221]), 'targetState': array([ 3, 24], dtype=int32), 'currentDistance': 16.175916918332852}
done in step count: 40
reward sum = 0.4617802811377938
running average episode reward sum: 0.49325153687039536
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 24.]), 'previousTarget': array([ 3., 24.]), 'currentState': array([ 3.99688606, 23.04198709,  2.18599305]), 'targetState': array([ 3, 24], dtype=int32), 'currentDistance': 1.38259558484078}
episode index:4392
target Thresh 32.0
target distance 9.0
model initialize at round 4392
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 23.]), 'previousTarget': array([10., 23.]), 'currentState': array([19.        , 25.        ,  5.16832981]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 9.219544457292885}
done in step count: 25
reward sum = 0.6375402270033192
running average episode reward sum: 0.4932843820081447
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 23.]), 'previousTarget': array([10., 23.]), 'currentState': array([10.9947013 , 23.3051399 ,  3.16824835]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 1.0404523207522882}
episode index:4393
target Thresh 32.0
target distance 16.0
model initialize at round 4393
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([ 9.14312363, 25.78396337,  5.23122576]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 15.89281251371557}
done in step count: 44
reward sum = 0.447344078505862
running average episode reward sum: 0.4932739267729371
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([11.42738734, 10.93530767,  4.89158351]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 1.0283289264314313}
episode index:4394
target Thresh 32.0
target distance 10.0
model initialize at round 4394
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  6.]), 'previousTarget': array([24.,  6.]), 'currentState': array([14.14025476,  5.64801089,  5.34406996]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 9.866026176733845}
done in step count: 21
reward sum = 0.6352808859228835
running average episode reward sum: 0.49330623779890975
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  6.]), 'previousTarget': array([24.,  6.]), 'currentState': array([23.15039484,  5.09914402,  0.11577843]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 1.2382933567994356}
episode index:4395
target Thresh 32.0
target distance 16.0
model initialize at round 4395
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([3.        , 6.        , 2.78218555]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 19.4164878389476}
done in step count: 50
reward sum = 0.3270077014261861
running average episode reward sum: 0.49326840828654106
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([14.2187255 , 21.06207964,  1.1461068 ]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 0.9630864207571276}
episode index:4396
target Thresh 32.0
target distance 14.0
model initialize at round 4396
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([19.76709481,  9.70554221,  3.79068279]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 13.77024345494687}
done in step count: 33
reward sum = 0.5028604058217945
running average episode reward sum: 0.49327058977335825
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([6.98466006, 9.72936346, 3.37552153]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 1.021175586173432}
episode index:4397
target Thresh 32.0
target distance 20.0
model initialize at round 4397
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 26.]), 'previousTarget': array([ 2., 26.]), 'currentState': array([1.99646406, 6.38450493, 1.62311822]), 'targetState': array([ 2, 26], dtype=int32), 'currentDistance': 19.615495392546887}
done in step count: 47
reward sum = 0.35837205744310907
running average episode reward sum: 0.49323991707387427
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 26.]), 'previousTarget': array([ 2., 26.]), 'currentState': array([ 2.44278973, 25.05474498,  1.34105041]), 'targetState': array([ 2, 26], dtype=int32), 'currentDistance': 1.0438246063312475}
episode index:4398
target Thresh 32.0
target distance 2.0
model initialize at round 4398
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 25.]), 'previousTarget': array([22., 25.]), 'currentState': array([21.94849577, 26.96227042,  3.94567826]), 'targetState': array([22, 25], dtype=int32), 'currentDistance': 1.9629462235092694}
done in step count: 5
reward sum = 0.9307370929843859
running average episode reward sum: 0.49333937085334933
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 25.]), 'previousTarget': array([22., 25.]), 'currentState': array([21.47825725, 25.87851086,  4.55940853]), 'targetState': array([22, 25], dtype=int32), 'currentDistance': 1.0217616332888018}
episode index:4399
target Thresh 32.0
target distance 12.0
model initialize at round 4399
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  2.]), 'previousTarget': array([11.,  2.]), 'currentState': array([22.60845112,  8.06742844,  3.05462667]), 'targetState': array([11,  2], dtype=int32), 'currentDistance': 13.098466520030039}
done in step count: 29
reward sum = 0.555372714115575
running average episode reward sum: 0.4933534693404544
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  2.]), 'previousTarget': array([11.,  2.]), 'currentState': array([11.77041666,  2.62889935,  3.6643124 ]), 'targetState': array([11,  2], dtype=int32), 'currentDistance': 0.9945130627709176}
episode index:4400
target Thresh 32.0
target distance 12.0
model initialize at round 4400
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([13.02192339, 25.04169895,  0.83714506]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 14.427170246521127}
done in step count: 39
reward sum = 0.4833933571657594
running average episode reward sum: 0.4933512061929482
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([24.00746053, 17.38471046,  5.57381738]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 1.0644889524298362}
episode index:4401
target Thresh 32.0
target distance 11.0
model initialize at round 4401
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([17.        ,  7.        ,  0.64881842]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 11.40175425099138}
done in step count: 27
reward sum = 0.5537982542487929
running average episode reward sum: 0.4933649379167228
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 6.88007228, 10.13512126,  3.17181498]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.8903847373053458}
episode index:4402
target Thresh 32.0
target distance 22.0
model initialize at round 4402
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8.23596274, 9.83404191]), 'previousTarget': array([8.23656605, 9.82527831]), 'currentState': array([19.98673447, 26.01796491,  2.45934057]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 79
reward sum = 0.15327249041229518
running average episode reward sum: 0.49328769684302204
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.74055981, 4.92712377, 4.52438876]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.9627396837279765}
episode index:4403
target Thresh 32.0
target distance 21.0
model initialize at round 4403
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9.35899411, 14.90599608]), 'previousTarget': array([ 9.35899411, 14.90599608]), 'currentState': array([26.       , 26.       ,  6.1264208]), 'targetState': array([ 5, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.2164150036378428
running average episode reward sum: 0.4931265472743388
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 12.]), 'previousTarget': array([ 5., 12.]), 'currentState': array([ 7.54401328, 12.69179946,  3.44189679]), 'targetState': array([ 5, 12], dtype=int32), 'currentDistance': 2.636397168859938}
episode index:4404
target Thresh 32.0
target distance 8.0
model initialize at round 4404
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 12.]), 'previousTarget': array([20., 12.]), 'currentState': array([13.        ,  4.        ,  3.13460776]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 10.630145812734648}
done in step count: 29
reward sum = 0.5715217168068143
running average episode reward sum: 0.4931443441346186
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 12.]), 'previousTarget': array([20., 12.]), 'currentState': array([19.62582698, 11.04142575,  0.6990873 ]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 1.0290141102228243}
episode index:4405
target Thresh 32.0
target distance 25.0
model initialize at round 4405
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7.44887314, 23.65243265]), 'previousTarget': array([ 7.55225396, 23.66745905]), 'currentState': array([26.86394406, 18.85086818,  3.7503722 ]), 'targetState': array([ 2, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 79
reward sum = 0.18853287562182014
running average episode reward sum: 0.4930752085312339
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 25.]), 'previousTarget': array([ 2., 25.]), 'currentState': array([ 2.96138484, 25.18089476,  2.96406611]), 'targetState': array([ 2, 25], dtype=int32), 'currentDistance': 0.9782554512403708}
episode index:4406
target Thresh 32.0
target distance 18.0
model initialize at round 4406
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.18901873, 16.17996241]), 'previousTarget': array([22.14213562, 16.14213562]), 'currentState': array([8.06367664, 2.02105317, 0.27723102]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 73
reward sum = 0.2046929054271439
running average episode reward sum: 0.4930097712035498
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([26.15935469, 19.1362137 ,  1.23162678]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 0.8783625075832722}
episode index:4407
target Thresh 32.0
target distance 16.0
model initialize at round 4407
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 14.]), 'previousTarget': array([26., 14.]), 'currentState': array([10.        , 14.        ,  4.93239021]), 'targetState': array([26, 14], dtype=int32), 'currentDistance': 16.0}
done in step count: 41
reward sum = 0.44115590361444035
running average episode reward sum: 0.49299800762197327
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 14.]), 'previousTarget': array([26., 14.]), 'currentState': array([25.07956255, 13.44623757,  0.23235588]), 'targetState': array([26, 14], dtype=int32), 'currentDistance': 1.0741777950006954}
episode index:4408
target Thresh 32.0
target distance 22.0
model initialize at round 4408
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7.45316396, 15.65072835]), 'previousTarget': array([ 7.78146944, 15.82541376]), 'currentState': array([24.6389614 , 25.88050675,  3.41064274]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 78
reward sum = 0.2167513064412973
running average episode reward sum: 0.4929353524391244
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 13.]), 'previousTarget': array([ 3., 13.]), 'currentState': array([ 3.22124693, 13.86136924,  4.11897603]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 0.8893296246944835}
episode index:4409
target Thresh 32.0
target distance 11.0
model initialize at round 4409
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([16.        , 25.        ,  0.57649133]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 12.52996408614167}
done in step count: 30
reward sum = 0.547226409474369
running average episode reward sum: 0.49294766333641127
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([21.15684159, 14.99096661,  5.25290506]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 1.3011267908922717}
episode index:4410
target Thresh 32.0
target distance 6.0
model initialize at round 4410
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 8.]), 'previousTarget': array([9., 8.]), 'currentState': array([3.29377759, 3.18004185, 0.59902863]), 'targetState': array([9, 8], dtype=int32), 'currentDistance': 7.469469237199589}
done in step count: 14
reward sum = 0.7582220127193834
running average episode reward sum: 0.49300780261307936
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 8.]), 'previousTarget': array([9., 8.]), 'currentState': array([8.07008616, 7.13267379, 0.58427077]), 'targetState': array([9, 8], dtype=int32), 'currentDistance': 1.271610988226972}
episode index:4411
target Thresh 32.0
target distance 10.0
model initialize at round 4411
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 22.]), 'previousTarget': array([ 3., 22.]), 'currentState': array([12.99767928, 26.00417214,  2.32043856]), 'targetState': array([ 3, 22], dtype=int32), 'currentDistance': 10.769725415608091}
done in step count: 25
reward sum = 0.6110170301585652
running average episode reward sum: 0.4930345499447986
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 22.]), 'previousTarget': array([ 3., 22.]), 'currentState': array([ 3.89353103, 22.84568392,  3.59069602]), 'targetState': array([ 3, 22], dtype=int32), 'currentDistance': 1.230275983225431}
episode index:4412
target Thresh 32.0
target distance 5.0
model initialize at round 4412
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 24.]), 'previousTarget': array([ 6., 24.]), 'currentState': array([ 6.6558716 , 28.72546442,  4.0186294 ]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 4.770763206181422}
done in step count: 10
reward sum = 0.8181921741872785
running average episode reward sum: 0.49310823170873297
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 24.]), 'previousTarget': array([ 6., 24.]), 'currentState': array([ 5.70747874, 24.81020357,  4.58209533]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 0.8613933572516224}
episode index:4413
target Thresh 32.0
target distance 9.0
model initialize at round 4413
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  8.]), 'previousTarget': array([21.,  8.]), 'currentState': array([14.00340901, 17.05735988,  1.25893426]), 'targetState': array([21,  8], dtype=int32), 'currentDistance': 11.445001244832529}
done in step count: 29
reward sum = 0.5568789977939876
running average episode reward sum: 0.49312267909570284
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  8.]), 'previousTarget': array([21.,  8.]), 'currentState': array([20.17467779,  8.87380903,  5.31567591]), 'targetState': array([21,  8], dtype=int32), 'currentDistance': 1.2019563084324358}
episode index:4414
target Thresh 32.0
target distance 7.0
model initialize at round 4414
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 19.]), 'previousTarget': array([ 9., 19.]), 'currentState': array([ 5.        , 12.        ,  5.48677254]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 19
reward sum = 0.6667677501368504
running average episode reward sum: 0.4931620098026204
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 19.]), 'previousTarget': array([ 9., 19.]), 'currentState': array([ 8.85387817, 18.11695017,  1.09674738]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 0.8950578675247642}
episode index:4415
target Thresh 32.0
target distance 10.0
model initialize at round 4415
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'previousTarget': array([27.,  8.]), 'currentState': array([17.24577963, 16.90377375,  5.82301556]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 13.206892220908841}
done in step count: 31
reward sum = 0.5519941937394207
running average episode reward sum: 0.49317533230804095
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'previousTarget': array([27.,  8.]), 'currentState': array([26.12790152,  8.32993256,  5.55613898]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 0.9324222512173471}
episode index:4416
target Thresh 32.0
target distance 7.0
model initialize at round 4416
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 17.]), 'previousTarget': array([21., 17.]), 'currentState': array([24.        , 10.        ,  6.02399015]), 'targetState': array([21, 17], dtype=int32), 'currentDistance': 7.615773105863908}
done in step count: 19
reward sum = 0.6782367764059261
running average episode reward sum: 0.4932172298502863
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 17.]), 'previousTarget': array([21., 17.]), 'currentState': array([21.63120514, 16.17633386,  1.95507984]), 'targetState': array([21, 17], dtype=int32), 'currentDistance': 1.0377118276900443}
episode index:4417
target Thresh 32.0
target distance 5.0
model initialize at round 4417
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 29.]), 'previousTarget': array([22., 29.]), 'currentState': array([24.        , 24.        ,  3.49128604]), 'targetState': array([22, 29], dtype=int32), 'currentDistance': 5.385164807134504}
done in step count: 12
reward sum = 0.7785566565455198
running average episode reward sum: 0.49328181550594385
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 29.]), 'previousTarget': array([22., 29.]), 'currentState': array([22.5424945 , 28.08804481,  1.72352996]), 'targetState': array([22, 29], dtype=int32), 'currentDistance': 1.0611138280434864}
episode index:4418
target Thresh 32.0
target distance 3.0
model initialize at round 4418
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 28.]), 'previousTarget': array([27., 28.]), 'currentState': array([23.91616379, 25.04447216,  2.40137124]), 'targetState': array([27, 28], dtype=int32), 'currentDistance': 4.271438934639412}
done in step count: 11
reward sum = 0.8144455475658419
running average episode reward sum: 0.49335449342675397
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 28.]), 'previousTarget': array([27., 28.]), 'currentState': array([26.19148725, 27.18351302,  0.60185684]), 'targetState': array([27, 28], dtype=int32), 'currentDistance': 1.1490621665246383}
episode index:4419
target Thresh 32.0
target distance 16.0
model initialize at round 4419
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'previousTarget': array([11., 19.]), 'currentState': array([27.        , 16.        ,  4.74495286]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 16.278820596099706}
done in step count: 42
reward sum = 0.4225818107757081
running average episode reward sum: 0.4933384815076021
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'previousTarget': array([11., 19.]), 'currentState': array([11.79645105, 18.73266315,  2.8231012 ]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 0.840120988577406}
episode index:4420
target Thresh 32.0
target distance 8.0
model initialize at round 4420
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 13.]), 'previousTarget': array([ 4., 13.]), 'currentState': array([2.        , 5.        , 6.21541357]), 'targetState': array([ 4, 13], dtype=int32), 'currentDistance': 8.246211251235321}
done in step count: 19
reward sum = 0.6749379443012592
running average episode reward sum: 0.493379558065574
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 13.]), 'previousTarget': array([ 4., 13.]), 'currentState': array([ 3.91125074, 12.16083755,  1.30414377]), 'targetState': array([ 4, 13], dtype=int32), 'currentDistance': 0.8438424310376386}
episode index:4421
target Thresh 32.0
target distance 9.0
model initialize at round 4421
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 28.]), 'previousTarget': array([16., 28.]), 'currentState': array([25.07062457, 22.03384167,  0.68207635]), 'targetState': array([16, 28], dtype=int32), 'currentDistance': 10.856853833821166}
done in step count: 27
reward sum = 0.5802447921395949
running average episode reward sum: 0.49339920194483095
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 28.]), 'previousTarget': array([16., 28.]), 'currentState': array([16.94087322, 27.39378382,  2.66438964]), 'targetState': array([16, 28], dtype=int32), 'currentDistance': 1.1192588961708618}
episode index:4422
target Thresh 32.0
target distance 8.0
model initialize at round 4422
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 27.]), 'previousTarget': array([20., 27.]), 'currentState': array([21.20749134, 19.12822434,  0.80603126]), 'targetState': array([20, 27], dtype=int32), 'currentDistance': 7.9638487759667385}
done in step count: 17
reward sum = 0.7018350187763982
running average episode reward sum: 0.49344632738386135
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 27.]), 'previousTarget': array([20., 27.]), 'currentState': array([20.3435241 , 26.12215902,  1.61025889]), 'targetState': array([20, 27], dtype=int32), 'currentDistance': 0.9426630298652082}
episode index:4423
target Thresh 32.0
target distance 16.0
model initialize at round 4423
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([10.14210117,  6.10299859,  0.84445092]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 17.042302088320298}
done in step count: 44
reward sum = 0.430712894536154
running average episode reward sum: 0.4934321471323135
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([ 4.93329046, 21.06827984,  1.98870682]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 1.3187621179708415}
episode index:4424
target Thresh 32.0
target distance 4.0
model initialize at round 4424
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 8.]), 'previousTarget': array([7., 8.]), 'currentState': array([3.       , 6.       , 4.3452698]), 'targetState': array([7, 8], dtype=int32), 'currentDistance': 4.47213595499958}
done in step count: 13
reward sum = 0.7659705821801605
running average episode reward sum: 0.49349373773910393
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 8.]), 'previousTarget': array([7., 8.]), 'currentState': array([6.28986358, 7.71343116, 0.53165705]), 'targetState': array([7, 8], dtype=int32), 'currentDistance': 0.7657776656373373}
episode index:4425
target Thresh 32.0
target distance 8.0
model initialize at round 4425
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 12.]), 'previousTarget': array([17., 12.]), 'currentState': array([10.04935839, 20.04965274,  0.54406652]), 'targetState': array([17, 12], dtype=int32), 'currentDistance': 10.635239913367279}
done in step count: 24
reward sum = 0.6084871121051532
running average episode reward sum: 0.4935197190708631
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 12.]), 'previousTarget': array([17., 12.]), 'currentState': array([16.00200802, 12.59633758,  5.23340707]), 'targetState': array([17, 12], dtype=int32), 'currentDistance': 1.1625861310502037}
episode index:4426
target Thresh 32.0
target distance 16.0
model initialize at round 4426
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 29.]), 'previousTarget': array([17., 29.]), 'currentState': array([21.13178747, 13.13820034,  0.99518859]), 'targetState': array([17, 29], dtype=int32), 'currentDistance': 16.39110601153495}
done in step count: 37
reward sum = 0.4473615187963112
running average episode reward sum: 0.49350929255171366
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 29.]), 'previousTarget': array([17., 29.]), 'currentState': array([17.6106947 , 28.25414085,  1.9658336 ]), 'targetState': array([17, 29], dtype=int32), 'currentDistance': 0.9639781596673378}
episode index:4427
target Thresh 32.0
target distance 13.0
model initialize at round 4427
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([3.18322927, 4.88830371, 5.91853514]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 12.847517090154225}
done in step count: 30
reward sum = 0.5610466838632746
running average episode reward sum: 0.4935245448984417
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.0501594 ,  3.49963284,  0.08816364]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.0735755479455604}
episode index:4428
target Thresh 32.0
target distance 14.0
model initialize at round 4428
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 20.]), 'previousTarget': array([23., 20.]), 'currentState': array([19.        ,  6.        ,  2.63414812]), 'targetState': array([23, 20], dtype=int32), 'currentDistance': 14.560219778561036}
done in step count: 38
reward sum = 0.4803389818174022
running average episode reward sum: 0.49352156780133605
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 20.]), 'previousTarget': array([23., 20.]), 'currentState': array([22.72356131, 19.11755587,  1.39558496]), 'targetState': array([23, 20], dtype=int32), 'currentDistance': 0.9247302295338475}
episode index:4429
target Thresh 32.0
target distance 1.0
model initialize at round 4429
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  5.]), 'previousTarget': array([23.,  5.]), 'currentState': array([23.        ,  4.        ,  5.53004366]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 1.0}
done in step count: 5
reward sum = 0.9123708667751304
running average episode reward sum: 0.4936161161758223
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  5.]), 'previousTarget': array([23.,  5.]), 'currentState': array([23.02386256,  4.04117335,  1.14784011]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 0.9591235369239245}
episode index:4430
target Thresh 32.0
target distance 11.0
model initialize at round 4430
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 13.]), 'previousTarget': array([12., 13.]), 'currentState': array([22.94013005, 11.13713349,  2.21265903]), 'targetState': array([12, 13], dtype=int32), 'currentDistance': 11.097599607544328}
done in step count: 25
reward sum = 0.6022431183911503
running average episode reward sum: 0.4936406314099038
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 13.]), 'previousTarget': array([12., 13.]), 'currentState': array([12.90397791, 13.27620059,  3.09965737]), 'targetState': array([12, 13], dtype=int32), 'currentDistance': 0.9452316294981437}
episode index:4431
target Thresh 32.0
target distance 2.0
model initialize at round 4431
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 15.]), 'previousTarget': array([13., 15.]), 'currentState': array([15.02495056, 14.98117866,  5.38440895]), 'targetState': array([13, 15], dtype=int32), 'currentDistance': 2.025038025865194}
done in step count: 7
reward sum = 0.8725107213223495
running average episode reward sum: 0.49372611653849413
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 15.]), 'previousTarget': array([13., 15.]), 'currentState': array([13.98245091, 14.75959246,  3.06790943]), 'targetState': array([13, 15], dtype=int32), 'currentDistance': 1.0114373826771252}
episode index:4432
target Thresh 32.0
target distance 13.0
model initialize at round 4432
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 5.]), 'previousTarget': array([6., 5.]), 'currentState': array([19.03167653, 16.08785737,  1.47725838]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 17.11038206506519}
done in step count: 47
reward sum = 0.39115815863256304
running average episode reward sum: 0.49370297916923944
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 5.]), 'previousTarget': array([6., 5.]), 'currentState': array([6.83384656, 5.96194333, 4.01896185]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 1.2730416582938069}
episode index:4433
target Thresh 32.0
target distance 10.0
model initialize at round 4433
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 13.]), 'previousTarget': array([11., 13.]), 'currentState': array([21.        ,  3.        ,  0.55473544]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 14.142135623730951}
done in step count: 33
reward sum = 0.48330817728357445
running average episode reward sum: 0.49370063482961707
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 13.]), 'previousTarget': array([11., 13.]), 'currentState': array([11.90768119, 12.66791746,  2.48517407]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 0.9665215803737649}
episode index:4434
target Thresh 32.0
target distance 16.0
model initialize at round 4434
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 17.]), 'previousTarget': array([26., 17.]), 'currentState': array([ 9.98853058, 15.98855862,  4.16805777]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 16.043383893614774}
done in step count: 40
reward sum = 0.420236986260504
running average episode reward sum: 0.4936840703090829
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 17.]), 'previousTarget': array([26., 17.]), 'currentState': array([25.0533324 , 16.41296262,  0.2856151 ]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 1.1139086269133627}
episode index:4435
target Thresh 32.0
target distance 14.0
model initialize at round 4435
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  6.]), 'previousTarget': array([26.,  6.]), 'currentState': array([20.92640251, 19.70401758,  4.6939913 ]), 'targetState': array([26,  6], dtype=int32), 'currentDistance': 14.61305886367348}
done in step count: 34
reward sum = 0.5123738730338481
running average episode reward sum: 0.4936882835197964
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  6.]), 'previousTarget': array([26.,  6.]), 'currentState': array([25.15847433,  6.89305319,  5.23206953]), 'targetState': array([26,  6], dtype=int32), 'currentDistance': 1.2270735285280003}
episode index:4436
target Thresh 32.0
target distance 5.0
model initialize at round 4436
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 14.]), 'previousTarget': array([12., 14.]), 'currentState': array([8.        , 9.        , 2.65761614]), 'targetState': array([12, 14], dtype=int32), 'currentDistance': 6.4031242374328485}
done in step count: 16
reward sum = 0.727318758805336
running average episode reward sum: 0.4937409385739513
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 14.]), 'previousTarget': array([12., 14.]), 'currentState': array([11.75807222, 13.05758642,  0.87394696]), 'targetState': array([12, 14], dtype=int32), 'currentDistance': 0.9729709176507939}
episode index:4437
target Thresh 32.0
target distance 5.0
model initialize at round 4437
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([11.03372375, 11.05626719,  0.78044802]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 6.42628860085091}
done in step count: 15
reward sum = 0.7338099699521529
running average episode reward sum: 0.4937950325422655
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.42384058,  6.70059305,  5.1547748 ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.9070778887146878}
episode index:4438
target Thresh 32.0
target distance 18.0
model initialize at round 4438
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  6.]), 'previousTarget': array([27.,  6.]), 'currentState': array([27.01785214, 24.11751514,  1.36957251]), 'targetState': array([27,  6], dtype=int32), 'currentDistance': 18.117523935703257}
done in step count: 43
reward sum = 0.33931523925916596
running average episode reward sum: 0.4937602319580611
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  6.]), 'previousTarget': array([27.,  6.]), 'currentState': array([26.42303979,  6.82383818,  4.82968485]), 'targetState': array([27,  6], dtype=int32), 'currentDistance': 1.0057795189190695}
episode index:4439
target Thresh 32.0
target distance 6.0
model initialize at round 4439
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 16.]), 'previousTarget': array([19., 16.]), 'currentState': array([24.82733031, 11.03065714,  2.78374359]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 7.658468971647106}
done in step count: 15
reward sum = 0.7363074864545573
running average episode reward sum: 0.4938148597180829
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 16.]), 'previousTarget': array([19., 16.]), 'currentState': array([19.88477169, 15.513097  ,  2.39058777]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 1.0098987448012158}
episode index:4440
target Thresh 32.0
target distance 17.0
model initialize at round 4440
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 18.]), 'previousTarget': array([ 5., 18.]), 'currentState': array([21.5979931 , 21.02362824,  3.04433387]), 'targetState': array([ 5, 18], dtype=int32), 'currentDistance': 16.871150007647138}
done in step count: 44
reward sum = 0.4330017275039975
running average episode reward sum: 0.49380116615082004
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 18.]), 'previousTarget': array([ 5., 18.]), 'currentState': array([ 5.80419929, 18.32162796,  3.48012118]), 'targetState': array([ 5, 18], dtype=int32), 'currentDistance': 0.8661299235325337}
episode index:4441
target Thresh 32.0
target distance 3.0
model initialize at round 4441
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.        ,  7.        ,  3.27971506]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 3.0}
done in step count: 9
reward sum = 0.8492699611622497
running average episode reward sum: 0.4938811906431685
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([9.90442595, 9.12700248, 1.44398278]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.8782135646095011}
episode index:4442
target Thresh 32.0
target distance 13.0
model initialize at round 4442
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([ 9.07877422, 18.09391546,  0.67375106]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 12.952955830723322}
done in step count: 32
reward sum = 0.5500144695518021
running average episode reward sum: 0.49389382473700344
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([21.25393489, 18.99564508,  0.09572441]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 0.7460778194642419}
episode index:4443
target Thresh 32.0
target distance 15.0
model initialize at round 4443
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 14.]), 'previousTarget': array([25., 14.]), 'currentState': array([10.18607586, 11.95596871,  6.27897125]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 14.954277393235976}
done in step count: 34
reward sum = 0.4976812058246473
running average episode reward sum: 0.4938946769829728
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 14.]), 'previousTarget': array([25., 14.]), 'currentState': array([24.09486515, 13.75666023,  0.13698597]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 0.9372744171311497}
episode index:4444
target Thresh 32.0
target distance 14.0
model initialize at round 4444
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([18.        , 12.        ,  4.04703796]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 14.142135623730951}
done in step count: 34
reward sum = 0.5126057739108049
running average episode reward sum: 0.49389888645359775
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([ 4.88526693, 13.66602167,  2.90463799]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 0.9461707379183899}
episode index:4445
target Thresh 32.0
target distance 20.0
model initialize at round 4445
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6.73231401, 9.77447047]), 'previousTarget': array([6.8434743 , 9.74695771]), 'currentState': array([25.84641056,  3.88793385,  3.71131813]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.33614378205975937
running average episode reward sum: 0.49386340397397704
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([6.87691757, 9.56616499, 2.54621317]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.9783645699386468}
episode index:4446
target Thresh 32.0
target distance 17.0
model initialize at round 4446
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.85927803, 16.14050559]), 'previousTarget': array([12.85786438, 16.14213562]), 'currentState': array([27.00087857,  1.99783491,  4.84649917]), 'targetState': array([10, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 72
reward sum = 0.20443627907457373
running average episode reward sum: 0.49379832029399073
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 19.]), 'previousTarget': array([10., 19.]), 'currentState': array([10.98042665, 18.42712416,  2.57470571]), 'targetState': array([10, 19], dtype=int32), 'currentDistance': 1.1355276932455864}
episode index:4447
target Thresh 32.0
target distance 6.0
model initialize at round 4447
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([21.        ,  4.        ,  4.79874894]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 6.708203932499369}
done in step count: 16
reward sum = 0.7157228186155785
running average episode reward sum: 0.493848213391635
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.81207588,  6.45717346,  2.42359455]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.9767946994400064}
episode index:4448
target Thresh 32.0
target distance 7.0
model initialize at round 4448
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'previousTarget': array([11., 19.]), 'currentState': array([ 5.89650153, 25.94975387,  3.84605455]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 8.622341644960025}
done in step count: 18
reward sum = 0.6532584069458414
running average episode reward sum: 0.49388404395885327
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'previousTarget': array([11., 19.]), 'currentState': array([10.67328819, 19.84937986,  5.09606168]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 0.9100476622145203}
episode index:4449
target Thresh 32.0
target distance 23.0
model initialize at round 4449
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.35439725, 19.02845995]), 'previousTarget': array([20.39893894, 19.08397111]), 'currentState': array([9.93304015, 1.95815485, 3.4476409 ]), 'targetState': array([24, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.21896144023032282
running average episode reward sum: 0.49372385396240626
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 25.]), 'previousTarget': array([24., 25.]), 'currentState': array([22.70014138, 22.82053432,  0.8248359 ]), 'targetState': array([24, 25], dtype=int32), 'currentDistance': 2.537657004592282}
episode index:4450
target Thresh 32.0
target distance 11.0
model initialize at round 4450
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 11.]), 'previousTarget': array([25., 11.]), 'currentState': array([14.00072255, 12.02906371,  1.29344058]), 'targetState': array([25, 11], dtype=int32), 'currentDistance': 11.047310829737166}
done in step count: 27
reward sum = 0.5781138969163406
running average episode reward sum: 0.4937428137563748
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 11.]), 'previousTarget': array([25., 11.]), 'currentState': array([24.12525265, 10.5884061 ,  0.03981749]), 'targetState': array([25, 11], dtype=int32), 'currentDistance': 0.9667432283850298}
episode index:4451
target Thresh 32.0
target distance 12.0
model initialize at round 4451
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([ 9.13795974, 22.58513014,  4.9705137 ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 11.73381584365266}
done in step count: 23
reward sum = 0.5994114094833524
running average episode reward sum: 0.49376654884076987
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.62000145, 11.92631786,  4.89877244]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.0012310757179341}
episode index:4452
target Thresh 32.0
target distance 9.0
model initialize at round 4452
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([10.        , 15.        ,  5.43072701]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 9.055385138137416}
done in step count: 21
reward sum = 0.6145699201591768
running average episode reward sum: 0.4937936773768845
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([ 9.35978177, 23.08802057,  1.73315685]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 0.9803822764276061}
episode index:4453
target Thresh 32.0
target distance 20.0
model initialize at round 4453
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.60529367,  5.1189049 ]), 'previousTarget': array([21.60700849,  5.12283287]), 'currentState': array([14.95190855, 23.97977621,  3.74912441]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 85
reward sum = 0.25392535038669917
running average episode reward sum: 0.49373982279067213
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  4.]), 'previousTarget': array([22.,  4.]), 'currentState': array([21.08108749,  4.92041312,  5.18172532]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 1.3006000607221175}
episode index:4454
target Thresh 32.0
target distance 16.0
model initialize at round 4454
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  2.]), 'previousTarget': array([27.,  2.]), 'currentState': array([10.97879171,  3.06723311,  1.62386107]), 'targetState': array([27,  2], dtype=int32), 'currentDistance': 16.056715152400223}
done in step count: 43
reward sum = 0.41707609498913656
running average episode reward sum: 0.4937226143220298
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  2.]), 'previousTarget': array([27.,  2.]), 'currentState': array([26.15836056,  1.70002817,  0.10566064]), 'targetState': array([27,  2], dtype=int32), 'currentDistance': 0.8934987663853466}
episode index:4455
target Thresh 32.0
target distance 17.0
model initialize at round 4455
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([10.        , 20.        ,  1.62766242]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 17.0}
done in step count: 43
reward sum = 0.3987285274477506
running average episode reward sum: 0.49370129607991264
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([26.16563776, 20.11222787,  6.18594782]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 0.8418761396851516}
episode index:4456
target Thresh 32.0
target distance 20.0
model initialize at round 4456
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.00728486, 25.9290429 ]), 'previousTarget': array([16.00992562, 25.9007438 ]), 'currentState': array([18.04986196,  6.03361933,  0.81124714]), 'targetState': array([16, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 72
reward sum = 0.3001054931276466
running average episode reward sum: 0.4936578597319315
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 26.]), 'previousTarget': array([16., 26.]), 'currentState': array([16.13801559, 25.19599091,  1.71801868]), 'targetState': array([16, 26], dtype=int32), 'currentDistance': 0.8157689151034253}
episode index:4457
target Thresh 32.0
target distance 12.0
model initialize at round 4457
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 22.]), 'previousTarget': array([ 3., 22.]), 'currentState': array([15.00288358, 14.12987321,  1.79671383]), 'targetState': array([ 3, 22], dtype=int32), 'currentDistance': 14.352982616270975}
done in step count: 32
reward sum = 0.4966913675649621
running average episode reward sum: 0.49365854019577915
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 22.]), 'previousTarget': array([ 3., 22.]), 'currentState': array([ 3.85577225, 21.67285591,  2.69249479]), 'targetState': array([ 3, 22], dtype=int32), 'currentDistance': 0.916171053628859}
episode index:4458
target Thresh 32.0
target distance 17.0
model initialize at round 4458
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  5.]), 'previousTarget': array([21.,  5.]), 'currentState': array([11.        , 22.        ,  2.69845247]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 19.72308292331602}
done in step count: 53
reward sum = 0.3155978448219031
running average episode reward sum: 0.4936186073194898
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  5.]), 'previousTarget': array([21.,  5.]), 'currentState': array([20.08181227,  5.76833277,  5.33251617]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 1.197248493823276}
episode index:4459
target Thresh 32.0
target distance 21.0
model initialize at round 4459
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5.99469707, 12.47290771]), 'previousTarget': array([ 5.99469707, 12.47290771]), 'currentState': array([23.        , 23.        ,  5.09283304]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.22073621554420336
running average episode reward sum: 0.49355742292671506
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.91557079, 10.56917328,  3.65079185]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.0780668334578738}
episode index:4460
target Thresh 32.0
target distance 10.0
model initialize at round 4460
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 28.]), 'previousTarget': array([26., 28.]), 'currentState': array([23.07269002, 18.07980697,  1.04455504]), 'targetState': array([26, 28], dtype=int32), 'currentDistance': 10.3430833599369}
done in step count: 22
reward sum = 0.6353617309059272
running average episode reward sum: 0.49358921048734705
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 28.]), 'previousTarget': array([26., 28.]), 'currentState': array([25.74372451, 27.19243038,  1.28924959]), 'targetState': array([26, 28], dtype=int32), 'currentDistance': 0.8472578275438826}
episode index:4461
target Thresh 32.0
target distance 21.0
model initialize at round 4461
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.53541143, 11.99710475]), 'previousTarget': array([12.58173352, 12.16928442]), 'currentState': array([26.10051389, 26.6936348 ,  4.92557694]), 'targetState': array([7, 6], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.11570023319533938
running average episode reward sum: 0.49345265973797847
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 6.]), 'previousTarget': array([7., 6.]), 'currentState': array([16.7523605 , 12.47768242,  3.65729332]), 'targetState': array([7, 6], dtype=int32), 'currentDistance': 11.707643013395298}
episode index:4462
target Thresh 32.0
target distance 10.0
model initialize at round 4462
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 12.]), 'previousTarget': array([13., 12.]), 'currentState': array([3.14599699, 3.36242551, 1.11615831]), 'targetState': array([13, 12], dtype=int32), 'currentDistance': 13.10378069310505}
done in step count: 25
reward sum = 0.5650344633243269
running average episode reward sum: 0.4934686986811975
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 12.]), 'previousTarget': array([13., 12.]), 'currentState': array([12.16190801, 11.04546305,  0.6530272 ]), 'targetState': array([13, 12], dtype=int32), 'currentDistance': 1.2702515378634718}
episode index:4463
target Thresh 32.0
target distance 8.0
model initialize at round 4463
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 19.]), 'previousTarget': array([ 3., 19.]), 'currentState': array([10.66930831, 26.62534883,  3.93072407]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 10.815000452144407}
done in step count: 22
reward sum = 0.6217508946724699
running average episode reward sum: 0.49349743573227084
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 19.]), 'previousTarget': array([ 3., 19.]), 'currentState': array([ 3.49473628, 19.76629243,  4.09146839]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 0.9121228447218996}
episode index:4464
target Thresh 32.0
target distance 12.0
model initialize at round 4464
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 10.]), 'previousTarget': array([22., 10.]), 'currentState': array([17.93300358, 21.94149743,  4.1119132 ]), 'targetState': array([22, 10], dtype=int32), 'currentDistance': 12.615063251364342}
done in step count: 31
reward sum = 0.5423866026405056
running average episode reward sum: 0.49350838515375084
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 10.]), 'previousTarget': array([22., 10.]), 'currentState': array([21.34197719, 10.88206135,  5.13227431]), 'targetState': array([22, 10], dtype=int32), 'currentDistance': 1.1004663766317297}
episode index:4465
target Thresh 32.0
target distance 20.0
model initialize at round 4465
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.58314485, 24.94368636]), 'previousTarget': array([20.42781353, 24.56953382]), 'currentState': array([13.24150194,  6.33991578,  0.92193137]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.335811060625545
running average episode reward sum: 0.4934730745123428
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([20.70910983, 25.04724191,  1.22305423]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 0.9961752239233118}
episode index:4466
target Thresh 32.0
target distance 14.0
model initialize at round 4466
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 18.]), 'previousTarget': array([23., 18.]), 'currentState': array([ 9.229121  , 20.92512907,  6.0141545 ]), 'targetState': array([23, 18], dtype=int32), 'currentDistance': 14.078120914644606}
done in step count: 33
reward sum = 0.5176586333915549
running average episode reward sum: 0.49347848878565354
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 18.]), 'previousTarget': array([23., 18.]), 'currentState': array([22.12522721, 17.93698241,  5.94960706]), 'targetState': array([23, 18], dtype=int32), 'currentDistance': 0.8770397055661873}
episode index:4467
target Thresh 32.0
target distance 17.0
model initialize at round 4467
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 27.]), 'previousTarget': array([22., 27.]), 'currentState': array([ 5.31891346, 29.18986033,  0.31570011]), 'targetState': array([22, 27], dtype=int32), 'currentDistance': 16.82421280408322}
done in step count: 42
reward sum = 0.4215662462269123
running average episode reward sum: 0.4934623938343199
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 27.]), 'previousTarget': array([22., 27.]), 'currentState': array([2.11848696e+01, 2.70252804e+01, 1.19639027e-02]), 'targetState': array([22, 27], dtype=int32), 'currentDistance': 0.815522304483798}
episode index:4468
target Thresh 32.0
target distance 19.0
model initialize at round 4468
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.51432132, 14.24876447]), 'previousTarget': array([22.51905368, 14.24510704]), 'currentState': array([ 7.99393298, 28.00224849,  3.03917789]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 93
reward sum = 0.12482281618686125
running average episode reward sum: 0.4933799056764216
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 10.]), 'previousTarget': array([27., 10.]), 'currentState': array([26.03842158,  9.98604543,  5.72124842]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 0.9616796745208201}
episode index:4469
target Thresh 32.0
target distance 19.0
model initialize at round 4469
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.33589719,  9.09517373]), 'previousTarget': array([15.33589719,  9.09517373]), 'currentState': array([ 2.        , 24.        ,  2.65056211]), 'targetState': array([19,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.1945024550369804
running average episode reward sum: 0.4932260170051211
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  5.]), 'previousTarget': array([19.,  5.]), 'currentState': array([15.97590824,  7.57723284,  5.55861894]), 'targetState': array([19,  5], dtype=int32), 'currentDistance': 3.9733185252375423}
episode index:4470
target Thresh 32.0
target distance 11.0
model initialize at round 4470
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([ 9.07944403, 13.8870828 ,  5.56684959]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 11.669388609535847}
done in step count: 27
reward sum = 0.5711515133983833
running average episode reward sum: 0.49324344610295007
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([19.2155439 , 17.56441371,  0.48188516]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 0.8972774315288286}
episode index:4471
target Thresh 32.0
target distance 4.0
model initialize at round 4471
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 24.]), 'previousTarget': array([12., 24.]), 'currentState': array([13.764964  , 27.69627945,  4.28072679]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 4.09604439591857}
done in step count: 9
reward sum = 0.8521057887676433
running average episode reward sum: 0.49332369260175707
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 24.]), 'previousTarget': array([12., 24.]), 'currentState': array([12.18985014, 24.86018028,  4.30217036]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 0.8808820538174826}
episode index:4472
target Thresh 32.0
target distance 14.0
model initialize at round 4472
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 16.]), 'previousTarget': array([ 8., 16.]), 'currentState': array([21.77934704, 18.09265769,  2.96405104]), 'targetState': array([ 8, 16], dtype=int32), 'currentDistance': 13.937346269293915}
done in step count: 30
reward sum = 0.5195659251086158
running average episode reward sum: 0.4933295594098292
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 16.]), 'previousTarget': array([ 8., 16.]), 'currentState': array([ 8.81319774, 16.26191671,  3.4605673 ]), 'targetState': array([ 8, 16], dtype=int32), 'currentDistance': 0.8543365439463707}
episode index:4473
target Thresh 32.0
target distance 10.0
model initialize at round 4473
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 16.]), 'previousTarget': array([16., 16.]), 'currentState': array([10.24218373,  6.05401417,  0.47193831]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 11.492392366719397}
done in step count: 24
reward sum = 0.5959123700505442
running average episode reward sum: 0.49335248806665544
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 16.]), 'previousTarget': array([16., 16.]), 'currentState': array([15.49142085, 15.02023038,  1.02396484]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 1.1039027413799443}
episode index:4474
target Thresh 32.0
target distance 17.0
model initialize at round 4474
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([12.93195496,  5.36257688,  1.71257044]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 16.917941608771855}
done in step count: 35
reward sum = 0.44603062340349836
running average episode reward sum: 0.49334191334829497
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([15.81264517, 21.15523693,  1.41499442]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 0.8652898246102122}
episode index:4475
target Thresh 32.0
target distance 19.0
model initialize at round 4475
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8.32882486, 4.6158266 ]), 'previousTarget': array([8.30234469, 4.60711423]), 'currentState': array([24.05445411, 16.97319626,  5.59732801]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 74
reward sum = 0.22811453353595723
running average episode reward sum: 0.4932826579015094
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 2.]), 'previousTarget': array([5., 2.]), 'currentState': array([5.8397133 , 2.96157445, 3.79357577]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 1.2766142090114296}
episode index:4476
target Thresh 32.0
target distance 16.0
model initialize at round 4476
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 27.]), 'previousTarget': array([ 7., 27.]), 'currentState': array([15.        , 11.        ,  3.73568529]), 'targetState': array([ 7, 27], dtype=int32), 'currentDistance': 17.88854381999832}
done in step count: 44
reward sum = 0.38626773149994653
running average episode reward sum: 0.4932587546344999
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 27.]), 'previousTarget': array([ 7., 27.]), 'currentState': array([ 7.77970356, 26.10875044,  2.10349325]), 'targetState': array([ 7, 27], dtype=int32), 'currentDistance': 1.184172039485824}
episode index:4477
target Thresh 32.0
target distance 14.0
model initialize at round 4477
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 26.]), 'previousTarget': array([27., 26.]), 'currentState': array([20.99791689, 11.97822088,  4.86011207]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 15.25238641979652}
done in step count: 37
reward sum = 0.4258785530461846
running average episode reward sum: 0.4932437076935467
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 26.]), 'previousTarget': array([27., 26.]), 'currentState': array([26.70197784, 25.15595869,  1.36445621]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 0.8951105746324548}
episode index:4478
target Thresh 32.0
target distance 17.0
model initialize at round 4478
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 20.]), 'previousTarget': array([ 8., 20.]), 'currentState': array([25.        , 13.        ,  4.27802494]), 'targetState': array([ 8, 20], dtype=int32), 'currentDistance': 18.38477631085024}
done in step count: 41
reward sum = 0.37751442148904485
running average episode reward sum: 0.4932178694961356
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 20.]), 'previousTarget': array([ 8., 20.]), 'currentState': array([ 8.82856892, 19.48815118,  2.79908836]), 'targetState': array([ 8, 20], dtype=int32), 'currentDistance': 0.9739176900280925}
episode index:4479
target Thresh 32.0
target distance 5.0
model initialize at round 4479
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 16.]), 'previousTarget': array([ 6., 16.]), 'currentState': array([ 6.12034535, 20.51472641,  4.93745429]), 'targetState': array([ 6, 16], dtype=int32), 'currentDistance': 4.516330097043661}
done in step count: 9
reward sum = 0.8429720995910863
running average episode reward sum: 0.49329593963678176
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 16.]), 'previousTarget': array([ 6., 16.]), 'currentState': array([ 6.22436865, 16.95987861,  4.47658987]), 'targetState': array([ 6, 16], dtype=int32), 'currentDistance': 0.9857526248350714}
episode index:4480
target Thresh 32.0
target distance 7.0
model initialize at round 4480
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 28.]), 'previousTarget': array([13., 28.]), 'currentState': array([17.        , 21.        ,  0.46880662]), 'targetState': array([13, 28], dtype=int32), 'currentDistance': 8.062257748298547}
done in step count: 21
reward sum = 0.6664906912483716
running average episode reward sum: 0.49333459055211576
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 28.]), 'previousTarget': array([13., 28.]), 'currentState': array([13.43136366, 27.12397165,  2.25878777]), 'targetState': array([13, 28], dtype=int32), 'currentDistance': 0.9764733865194752}
episode index:4481
target Thresh 32.0
target distance 5.0
model initialize at round 4481
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([20.13644368,  3.44932006,  1.29478309]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 4.91747192017873}
done in step count: 9
reward sum = 0.8348910619283729
running average episode reward sum: 0.4934107968152519
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([21.45217369,  7.1975562 ,  1.10094842]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 0.9716119148451065}
episode index:4482
target Thresh 32.0
target distance 4.0
model initialize at round 4482
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([19.50028843,  6.01356513,  3.07871179]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 3.636629321015435}
done in step count: 9
reward sum = 0.8628036907736679
running average episode reward sum: 0.49349319540859526
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.87221873,  6.82756663,  2.6596932 ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.8890999837911286}
episode index:4483
target Thresh 32.0
target distance 6.0
model initialize at round 4483
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 21.]), 'previousTarget': array([ 2., 21.]), 'currentState': array([ 5.05870851, 26.80927805,  4.77841279]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 6.565318670232315}
done in step count: 13
reward sum = 0.7641806537727877
running average episode reward sum: 0.4935535628167942
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 21.]), 'previousTarget': array([ 2., 21.]), 'currentState': array([ 2.42728972, 21.88180894,  4.16063841]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 0.9798793327902212}
episode index:4484
target Thresh 32.0
target distance 7.0
model initialize at round 4484
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 3.]), 'previousTarget': array([6., 3.]), 'currentState': array([12.03295592,  9.97091842,  5.30765772]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 9.219016256965288}
done in step count: 19
reward sum = 0.6677213699472087
running average episode reward sum: 0.493592396218607
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 3.]), 'previousTarget': array([6., 3.]), 'currentState': array([6.99147901, 3.97303627, 4.05514703]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 1.389183287599545}
episode index:4485
target Thresh 32.0
target distance 16.0
model initialize at round 4485
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 16.]), 'previousTarget': array([25., 16.]), 'currentState': array([ 9.21790127, 10.81398276,  5.8278532 ]), 'targetState': array([25, 16], dtype=int32), 'currentDistance': 16.612327205878252}
done in step count: 41
reward sum = 0.4355579995177157
running average episode reward sum: 0.49357945943824566
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 16.]), 'previousTarget': array([25., 16.]), 'currentState': array([24.01941424, 15.75116987,  0.31631963]), 'targetState': array([25, 16], dtype=int32), 'currentDistance': 1.0116644025793546}
episode index:4486
target Thresh 32.0
target distance 8.0
model initialize at round 4486
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 13.]), 'previousTarget': array([13., 13.]), 'currentState': array([15.       , 21.       ,  0.4808839]), 'targetState': array([13, 13], dtype=int32), 'currentDistance': 8.246211251235321}
done in step count: 22
reward sum = 0.6421981852371486
running average episode reward sum: 0.49361258150773507
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 13.]), 'previousTarget': array([13., 13.]), 'currentState': array([13.20765239, 13.86075053,  4.63776113]), 'targetState': array([13, 13], dtype=int32), 'currentDistance': 0.8854439528889021}
episode index:4487
target Thresh 32.0
target distance 21.0
model initialize at round 4487
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.48763018,  8.78454174]), 'previousTarget': array([26.49442256,  8.76952105]), 'currentState': array([20.96830688, 28.00788875,  3.15013981]), 'targetState': array([27,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.277617760710316
running average episode reward sum: 0.4935644543195003
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  7.]), 'previousTarget': array([27.,  7.]), 'currentState': array([26.52845998,  7.91302639,  5.18788964]), 'targetState': array([27,  7], dtype=int32), 'currentDistance': 1.0276026346830018}
episode index:4488
target Thresh 32.0
target distance 6.0
model initialize at round 4488
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 18.]), 'previousTarget': array([ 7., 18.]), 'currentState': array([ 2.26506427, 23.83340978,  5.6074734 ]), 'targetState': array([ 7, 18], dtype=int32), 'currentDistance': 7.5132074360308}
done in step count: 14
reward sum = 0.7469073974820477
running average episode reward sum: 0.49362089070692794
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 18.]), 'previousTarget': array([ 7., 18.]), 'currentState': array([ 6.4129932 , 18.94386603,  5.22782436]), 'targetState': array([ 7, 18], dtype=int32), 'currentDistance': 1.1115125147606206}
episode index:4489
target Thresh 32.0
target distance 7.0
model initialize at round 4489
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 20.]), 'previousTarget': array([18., 20.]), 'currentState': array([22.08784961, 13.14203864,  1.20661837]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 7.983868013541412}
done in step count: 20
reward sum = 0.6965634538804597
running average episode reward sum: 0.4936660894960535
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 20.]), 'previousTarget': array([18., 20.]), 'currentState': array([18.45390954, 19.16129661,  2.01068909]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 0.9536546824496114}
episode index:4490
target Thresh 32.0
target distance 5.0
model initialize at round 4490
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 2.]), 'previousTarget': array([9., 2.]), 'currentState': array([4.        , 6.        , 0.47556931]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 6.4031242374328485}
done in step count: 15
reward sum = 0.7592497617761249
running average episode reward sum: 0.49372522636362864
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 2.]), 'previousTarget': array([9., 2.]), 'currentState': array([8.00185345, 2.77455385, 5.58969048]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 1.2634200411377123}
episode index:4491
target Thresh 32.0
target distance 14.0
model initialize at round 4491
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  6.]), 'previousTarget': array([23.,  6.]), 'currentState': array([18.16914802, 20.00150846,  6.07171569]), 'targetState': array([23,  6], dtype=int32), 'currentDistance': 14.811460764563794}
done in step count: 34
reward sum = 0.4779655120127686
running average episode reward sum: 0.4937217179677358
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  6.]), 'previousTarget': array([23.,  6.]), 'currentState': array([22.82713679,  6.89196244,  4.94392487]), 'targetState': array([23,  6], dtype=int32), 'currentDistance': 0.9085585790834874}
episode index:4492
target Thresh 32.0
target distance 10.0
model initialize at round 4492
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 16.]), 'previousTarget': array([19., 16.]), 'currentState': array([9.11137656, 9.04765274, 0.59380363]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 12.088010833470742}
done in step count: 27
reward sum = 0.5939185049618836
running average episode reward sum: 0.49374401861028955
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 16.]), 'previousTarget': array([19., 16.]), 'currentState': array([18.13975556, 15.22997223,  0.71344165]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 1.1545402812333923}
episode index:4493
target Thresh 32.0
target distance 4.0
model initialize at round 4493
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 10.]), 'previousTarget': array([27., 10.]), 'currentState': array([24.32450129,  6.10298117,  0.53721511]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 4.727055015878667}
done in step count: 10
reward sum = 0.8269138246076871
running average episode reward sum: 0.4938181551937336
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 10.]), 'previousTarget': array([27., 10.]), 'currentState': array([26.32537509,  9.11497808,  0.99059108]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 1.1128263855695706}
episode index:4494
target Thresh 32.0
target distance 19.0
model initialize at round 4494
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 10.]), 'previousTarget': array([25., 10.]), 'currentState': array([6.        , 4.        , 5.01663971]), 'targetState': array([25, 10], dtype=int32), 'currentDistance': 19.924858845171272}
done in step count: 57
reward sum = 0.31309914298070746
running average episode reward sum: 0.4937779507416284
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 10.]), 'previousTarget': array([25., 10.]), 'currentState': array([24.15288973,  9.70614922,  0.27710711]), 'targetState': array([25, 10], dtype=int32), 'currentDistance': 0.896629295067941}
episode index:4495
target Thresh 32.0
target distance 13.0
model initialize at round 4495
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([ 4.01035407, 22.02107419,  0.87814209]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 16.411623895584082}
done in step count: 41
reward sum = 0.42267115130267097
running average episode reward sum: 0.4937621351723582
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.40409444,  9.97614637,  5.35166688]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.1436630466160052}
episode index:4496
target Thresh 32.0
target distance 6.0
model initialize at round 4496
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([ 6.        , 16.        ,  0.62239289]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 7.810249675906655}
done in step count: 20
reward sum = 0.6900623182823893
running average episode reward sum: 0.49380578653618074
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([10.13164453, 10.983311  ,  5.32010185]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 1.311846689362802}
episode index:4497
target Thresh 32.0
target distance 3.0
model initialize at round 4497
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([11.04343984, 26.06139142,  1.16910371]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 3.1183628822195892}
done in step count: 11
reward sum = 0.859814934827496
running average episode reward sum: 0.4938871580675928
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([10.48595623, 28.04769119,  1.90808356]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 1.0691330726047876}
episode index:4498
target Thresh 32.0
target distance 4.0
model initialize at round 4498
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 16.]), 'previousTarget': array([ 6., 16.]), 'currentState': array([ 2.00048438, 12.14266605,  1.37221026]), 'targetState': array([ 6, 16], dtype=int32), 'currentDistance': 5.556541224856599}
done in step count: 13
reward sum = 0.8061870342543469
running average episode reward sum: 0.4939565734657228
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 16.]), 'previousTarget': array([ 6., 16.]), 'currentState': array([ 5.00324924, 15.11615084,  0.70675744]), 'targetState': array([ 6, 16], dtype=int32), 'currentDistance': 1.3321791949037407}
episode index:4499
target Thresh 32.0
target distance 21.0
model initialize at round 4499
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.64100589,  9.90599608]), 'previousTarget': array([19.64100589,  9.90599608]), 'currentState': array([ 3.        , 21.        ,  1.74529696]), 'targetState': array([24,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 88
reward sum = 0.18281143431784674
running average episode reward sum: 0.49388743010146774
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  7.]), 'previousTarget': array([24.,  7.]), 'currentState': array([23.1024545 ,  7.5277734 ,  5.62720571]), 'targetState': array([24,  7], dtype=int32), 'currentDistance': 1.041216925949471}
episode index:4500
target Thresh 32.0
target distance 6.0
model initialize at round 4500
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 13.]), 'previousTarget': array([25., 13.]), 'currentState': array([20.08525685,  7.06380234,  0.82194415]), 'targetState': array([25, 13], dtype=int32), 'currentDistance': 7.706694683445529}
done in step count: 20
reward sum = 0.7255964400409286
running average episode reward sum: 0.4939389095526874
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 13.]), 'previousTarget': array([25., 13.]), 'currentState': array([24.15720007, 12.00844961,  0.90300494]), 'targetState': array([25, 13], dtype=int32), 'currentDistance': 1.3013392717299794}
episode index:4501
target Thresh 32.0
target distance 13.0
model initialize at round 4501
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 12.]), 'previousTarget': array([16., 12.]), 'currentState': array([11.        , 25.        ,  0.57668889]), 'targetState': array([16, 12], dtype=int32), 'currentDistance': 13.92838827718412}
done in step count: 38
reward sum = 0.4712445623226582
running average episode reward sum: 0.4939338686048353
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 12.]), 'previousTarget': array([16., 12.]), 'currentState': array([15.4202039 , 12.98280495,  5.20791113]), 'targetState': array([16, 12], dtype=int32), 'currentDistance': 1.1410824235868686}
episode index:4502
target Thresh 32.0
target distance 16.0
model initialize at round 4502
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([14.15026552,  6.23057826,  1.13399592]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 18.753734352542136}
done in step count: 51
reward sum = 0.3761094413948778
running average episode reward sum: 0.49390770284263014
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([ 4.4354526 , 21.05666667,  2.36717032]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 1.038988327830011}
episode index:4503
target Thresh 32.0
target distance 11.0
model initialize at round 4503
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([25.        , 26.        ,  0.07115769]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 12.08304597359457}
done in step count: 34
reward sum = 0.5175328414174765
running average episode reward sum: 0.493912948210875
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([14.83271247, 20.8256023 ,  3.5039441 ]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 0.8507788315735557}
episode index:4504
target Thresh 32.0
target distance 14.0
model initialize at round 4504
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  9.]), 'previousTarget': array([23.,  9.]), 'currentState': array([ 8.99136126, 14.01029985,  2.05776754]), 'targetState': array([23,  9], dtype=int32), 'currentDistance': 14.877669975532568}
done in step count: 44
reward sum = 0.43021382446266976
running average episode reward sum: 0.49389880856076435
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  9.]), 'previousTarget': array([23.,  9.]), 'currentState': array([22.15461182,  9.23209762,  6.07664755]), 'targetState': array([23,  9], dtype=int32), 'currentDistance': 0.8766701056667242}
episode index:4505
target Thresh 32.0
target distance 21.0
model initialize at round 4505
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8.99469707, 14.52709229]), 'previousTarget': array([ 8.99469707, 14.52709229]), 'currentState': array([26.       ,  4.       ,  1.2576009]), 'targetState': array([ 5, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 90
reward sum = 0.1988476622358308
running average episode reward sum: 0.4938333289455125
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 17.]), 'previousTarget': array([ 5., 17.]), 'currentState': array([ 5.85568559, 16.55462864,  3.04871585]), 'targetState': array([ 5, 17], dtype=int32), 'currentDistance': 0.9646519980353726}
episode index:4506
target Thresh 32.0
target distance 18.0
model initialize at round 4506
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.07306832, 19.61318782]), 'previousTarget': array([26.09400392, 19.64100589]), 'currentState': array([14.95923166,  2.98542084,  3.73753071]), 'targetState': array([27, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.24010298269600744
running average episode reward sum: 0.4937770319971545
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 21.]), 'previousTarget': array([27., 21.]), 'currentState': array([26.59897625, 20.17754421,  1.16395783]), 'targetState': array([27, 21], dtype=int32), 'currentDistance': 0.9150156195270183}
episode index:4507
target Thresh 32.0
target distance 1.0
model initialize at round 4507
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 25.]), 'previousTarget': array([18., 25.]), 'currentState': array([19.03398162, 25.07704884,  1.34859851]), 'targetState': array([18, 25], dtype=int32), 'currentDistance': 1.036848352976658}
done in step count: 3
reward sum = 0.9427324892331898
running average episode reward sum: 0.49387662282617756
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 25.]), 'previousTarget': array([18., 25.]), 'currentState': array([18.97710618, 25.22943938,  2.6141181 ]), 'targetState': array([18, 25], dtype=int32), 'currentDistance': 1.0036826754097439}
episode index:4508
target Thresh 32.0
target distance 19.0
model initialize at round 4508
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 16.]), 'previousTarget': array([ 4., 16.]), 'currentState': array([23.        , 20.        ,  5.96063483]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 19.4164878389476}
done in step count: 58
reward sum = 0.2913036717070459
running average episode reward sum: 0.493831696467535
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 16.]), 'previousTarget': array([ 4., 16.]), 'currentState': array([ 4.94033736, 16.41275405,  3.64016776]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 1.026937319764358}
episode index:4509
target Thresh 32.0
target distance 21.0
model initialize at round 4509
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9.62180659, 19.7085141 ]), 'previousTarget': array([ 9.72533058, 19.62476387]), 'currentState': array([25.91280545,  8.10664544,  2.27159322]), 'targetState': array([ 5, 23], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 89
reward sum = 0.21563395950088557
running average episode reward sum: 0.49377001182519203
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 23.]), 'previousTarget': array([ 5., 23.]), 'currentState': array([ 5.51035078, 22.007746  ,  2.67582841]), 'targetState': array([ 5, 23], dtype=int32), 'currentDistance': 1.1158072964511432}
episode index:4510
target Thresh 32.0
target distance 11.0
model initialize at round 4510
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 18.]), 'previousTarget': array([25., 18.]), 'currentState': array([22.08783028,  7.19355199,  1.27002271]), 'targetState': array([25, 18], dtype=int32), 'currentDistance': 11.191963678150607}
done in step count: 24
reward sum = 0.6151966455808093
running average episode reward sum: 0.49379692972227823
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 18.]), 'previousTarget': array([25., 18.]), 'currentState': array([24.75207273, 17.2518055 ,  1.41831949]), 'targetState': array([25, 18], dtype=int32), 'currentDistance': 0.7882023507239733}
episode index:4511
target Thresh 32.0
target distance 19.0
model initialize at round 4511
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([1.99268512, 8.01545812, 2.03014351]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 19.24020554202845}
done in step count: 66
reward sum = 0.28314119988822367
running average episode reward sum: 0.493750241838893
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([20.17897972, 10.59038096,  0.55891647]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 0.9175304086727345}
episode index:4512
target Thresh 32.0
target distance 16.0
model initialize at round 4512
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 13.]), 'previousTarget': array([24., 13.]), 'currentState': array([ 7.98603137, 15.10258921,  1.51987267]), 'targetState': array([24, 13], dtype=int32), 'currentDistance': 16.151410857966027}
done in step count: 40
reward sum = 0.4278372899167928
running average episode reward sum: 0.4937356367088415
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 13.]), 'previousTarget': array([24., 13.]), 'currentState': array([23.1396155 , 13.36336865,  6.1073412 ]), 'targetState': array([24, 13], dtype=int32), 'currentDistance': 0.933969087849961}
episode index:4513
target Thresh 32.0
target distance 8.0
model initialize at round 4513
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 19.]), 'previousTarget': array([21., 19.]), 'currentState': array([21.        , 27.        ,  0.11773697]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 8.0}
done in step count: 18
reward sum = 0.6757409054398076
running average episode reward sum: 0.4937759568835715
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 19.]), 'previousTarget': array([21., 19.]), 'currentState': array([21.23230139, 19.73479214,  4.49844395]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 0.7706383232293669}
episode index:4514
target Thresh 32.0
target distance 10.0
model initialize at round 4514
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 14.]), 'previousTarget': array([20., 14.]), 'currentState': array([12.03303994, 24.07120925,  0.88387442]), 'targetState': array([20, 14], dtype=int32), 'currentDistance': 12.841406005078618}
done in step count: 27
reward sum = 0.5248889564338147
running average episode reward sum: 0.4937828479133722
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 14.]), 'previousTarget': array([20., 14.]), 'currentState': array([19.44979121, 14.99170018,  5.22161944]), 'targetState': array([20, 14], dtype=int32), 'currentDistance': 1.134107113621384}
episode index:4515
target Thresh 32.0
target distance 13.0
model initialize at round 4515
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 24.]), 'previousTarget': array([24., 24.]), 'currentState': array([16.92299477, 11.01783832,  2.66145706]), 'targetState': array([24, 24], dtype=int32), 'currentDistance': 14.785821757578605}
done in step count: 32
reward sum = 0.4689914774060224
running average episode reward sum: 0.49377735823876917
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 24.]), 'previousTarget': array([24., 24.]), 'currentState': array([23.21109227, 23.04450829,  0.80291998]), 'targetState': array([24, 24], dtype=int32), 'currentDistance': 1.2390882946881934}
episode index:4516
target Thresh 32.0
target distance 15.0
model initialize at round 4516
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.62553124, 24.64953785]), 'previousTarget': array([24.62110536, 24.64636501]), 'currentState': array([10.02307317, 10.98322092,  5.8231833 ]), 'targetState': array([25, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.3214082474343791
running average episode reward sum: 0.4937391981522506
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 25.]), 'previousTarget': array([25., 25.]), 'currentState': array([24.0573313 , 24.97549656,  0.64433178]), 'targetState': array([25, 25], dtype=int32), 'currentDistance': 0.9429871096434361}
episode index:4517
target Thresh 32.0
target distance 6.0
model initialize at round 4517
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 22.]), 'previousTarget': array([19., 22.]), 'currentState': array([25.        , 18.        ,  4.76144314]), 'targetState': array([19, 22], dtype=int32), 'currentDistance': 7.211102550927978}
done in step count: 18
reward sum = 0.6781127888485462
running average episode reward sum: 0.4937800068265968
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 22.]), 'previousTarget': array([19., 22.]), 'currentState': array([19.10148321, 21.25869512,  2.226573  ]), 'targetState': array([19, 22], dtype=int32), 'currentDistance': 0.7482190678011035}
episode index:4518
target Thresh 32.0
target distance 9.0
model initialize at round 4518
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 28.]), 'previousTarget': array([ 3., 28.]), 'currentState': array([ 1.95517659, 18.9621041 ,  3.60637632]), 'targetState': array([ 3, 28], dtype=int32), 'currentDistance': 9.098088714219315}
done in step count: 22
reward sum = 0.6193752426073429
running average episode reward sum: 0.4938077995320141
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 28.]), 'previousTarget': array([ 3., 28.]), 'currentState': array([ 2.01293364, 27.49388131,  1.21244896]), 'targetState': array([ 3, 28], dtype=int32), 'currentDistance': 1.1092592680655868}
episode index:4519
target Thresh 32.0
target distance 17.0
model initialize at round 4519
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 29.]), 'previousTarget': array([19., 29.]), 'currentState': array([16.        , 12.        ,  5.99336529]), 'targetState': array([19, 29], dtype=int32), 'currentDistance': 17.262676501632072}
done in step count: 43
reward sum = 0.3884898538356121
running average episode reward sum: 0.49378449910155037
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 29.]), 'previousTarget': array([19., 29.]), 'currentState': array([18.63576591, 28.20426535,  1.27240874]), 'targetState': array([19, 29], dtype=int32), 'currentDistance': 0.8751343320127535}
episode index:4520
target Thresh 32.0
target distance 11.0
model initialize at round 4520
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 12.]), 'previousTarget': array([ 9., 12.]), 'currentState': array([19.93639277, 12.09057718,  2.37397608]), 'targetState': array([ 9, 12], dtype=int32), 'currentDistance': 10.9367678503814}
done in step count: 27
reward sum = 0.6115982864063516
running average episode reward sum: 0.49381055833342485
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 12.]), 'previousTarget': array([ 9., 12.]), 'currentState': array([ 9.98681115, 11.77519185,  3.10246614]), 'targetState': array([ 9, 12], dtype=int32), 'currentDistance': 1.0120943363390744}
episode index:4521
target Thresh 32.0
target distance 11.0
model initialize at round 4521
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([16.04782334, 14.81295598,  4.80666754]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 12.907045777827154}
done in step count: 27
reward sum = 0.5627520670121176
running average episode reward sum: 0.49382580413366345
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([9.8200637 , 4.65392836, 4.24641335]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 1.04886928104634}
episode index:4522
target Thresh 32.0
target distance 3.0
model initialize at round 4522
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([20.15641859,  3.22851734,  0.82991585]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 2.852748748060826}
done in step count: 6
reward sum = 0.8976200903715059
running average episode reward sum: 0.49391507989891614
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([22.20690357,  3.79952427,  6.1712488 ]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 1.1261620696480656}
episode index:4523
target Thresh 32.0
target distance 24.0
model initialize at round 4523
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4.99053505, 22.09884943]), 'previousTarget': array([ 5., 22.]), 'currentState': array([4.9420113 , 2.0989083 , 1.97978128]), 'targetState': array([ 5, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 81
reward sum = 0.2433982853103558
running average episode reward sum: 0.4938597048337993
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 26.]), 'previousTarget': array([ 5., 26.]), 'currentState': array([ 4.7378859, 25.1050686,  1.5269257]), 'targetState': array([ 5, 26], dtype=int32), 'currentDistance': 0.9325266831714158}
episode index:4524
target Thresh 32.0
target distance 17.0
model initialize at round 4524
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 21.]), 'previousTarget': array([25., 21.]), 'currentState': array([ 7.99969699, 22.99976072,  4.0446879 ]), 'targetState': array([25, 21], dtype=int32), 'currentDistance': 17.11751574776863}
done in step count: 40
reward sum = 0.38487294167687
running average episode reward sum: 0.49383561936127846
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 21.]), 'previousTarget': array([25., 21.]), 'currentState': array([24.22664088, 21.14408567,  6.11783932]), 'targetState': array([25, 21], dtype=int32), 'currentDistance': 0.7866670245393975}
episode index:4525
target Thresh 32.0
target distance 17.0
model initialize at round 4525
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 20.]), 'previousTarget': array([25., 20.]), 'currentState': array([ 8.12184447, 22.25207161,  0.97742453]), 'targetState': array([25, 20], dtype=int32), 'currentDistance': 17.027740912446696}
done in step count: 35
reward sum = 0.42618919753440426
running average episode reward sum: 0.49382067317881556
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 20.]), 'previousTarget': array([25., 20.]), 'currentState': array([24.05775419, 20.99936706,  6.01071358]), 'targetState': array([25, 20], dtype=int32), 'currentDistance': 1.3735216326658337}
episode index:4526
target Thresh 32.0
target distance 9.0
model initialize at round 4526
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  3.]), 'previousTarget': array([11.,  3.]), 'currentState': array([20.        ,  7.        ,  5.41472101]), 'targetState': array([11,  3], dtype=int32), 'currentDistance': 9.848857801796104}
done in step count: 22
reward sum = 0.6059101743646137
running average episode reward sum: 0.49384543339555637
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  3.]), 'previousTarget': array([11.,  3.]), 'currentState': array([11.87847099,  2.4602112 ,  3.45187261]), 'targetState': array([11,  3], dtype=int32), 'currentDistance': 1.0310592755549068}
episode index:4527
target Thresh 32.0
target distance 7.0
model initialize at round 4527
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([21.04088192, 22.93238609,  5.03610843]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 7.301241961738834}
done in step count: 17
reward sum = 0.6887521458055377
running average episode reward sum: 0.4938884781641982
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([14.7312552 , 20.21639882,  2.85178159]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 1.0718045440004513}
episode index:4528
target Thresh 32.0
target distance 19.0
model initialize at round 4528
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.79907671, 22.52570366]), 'previousTarget': array([24.76114  , 22.4327075]), 'currentState': array([16.9977334 ,  4.10997553,  1.51334907]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.3622406154525685
running average episode reward sum: 0.4938594104091283
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 23.]), 'previousTarget': array([25., 23.]), 'currentState': array([24.15107576, 22.16213623,  1.09676175]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 1.1927648779063729}
episode index:4529
target Thresh 32.0
target distance 13.0
model initialize at round 4529
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  6.]), 'previousTarget': array([24.,  6.]), 'currentState': array([18.        , 19.        ,  0.99448335]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 14.317821063276353}
done in step count: 40
reward sum = 0.46112925963586404
running average episode reward sum: 0.4938521852102821
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  6.]), 'previousTarget': array([24.,  6.]), 'currentState': array([23.87302502,  6.90725107,  4.90915674]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 0.9160934125901427}
episode index:4530
target Thresh 32.0
target distance 14.0
model initialize at round 4530
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([26.87597196, 19.81728169,  4.12349652]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 15.014803354883528}
done in step count: 34
reward sum = 0.5082425330914653
running average episode reward sum: 0.49385536118642015
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([21.78237418,  6.78156983,  4.50214649]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 1.1058755623168857}
episode index:4531
target Thresh 32.0
target distance 10.0
model initialize at round 4531
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  6.]), 'previousTarget': array([17.,  6.]), 'currentState': array([ 8.15552336, 15.74905986,  5.40462607]), 'targetState': array([17,  6], dtype=int32), 'currentDistance': 13.16316585195587}
done in step count: 28
reward sum = 0.5306756268323409
running average episode reward sum: 0.49386348569340294
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  6.]), 'previousTarget': array([17.,  6.]), 'currentState': array([17.43853547,  6.76889328,  4.99077467]), 'targetState': array([17,  6], dtype=int32), 'currentDistance': 0.8851611304756859}
episode index:4532
target Thresh 32.0
target distance 12.0
model initialize at round 4532
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 20.]), 'previousTarget': array([ 6., 20.]), 'currentState': array([9.06387738, 8.10128824, 1.26064479]), 'targetState': array([ 6, 20], dtype=int32), 'currentDistance': 12.286850127341072}
done in step count: 28
reward sum = 0.5731695906355514
running average episode reward sum: 0.4938809809735578
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 20.]), 'previousTarget': array([ 6., 20.]), 'currentState': array([ 6.18787186, 19.03854273,  1.84082754]), 'targetState': array([ 6, 20], dtype=int32), 'currentDistance': 0.97964071223119}
episode index:4533
target Thresh 32.0
target distance 15.0
model initialize at round 4533
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 14.]), 'previousTarget': array([ 9., 14.]), 'currentState': array([24.00418183,  8.96442272,  4.57689428]), 'targetState': array([ 9, 14], dtype=int32), 'currentDistance': 15.826639283918349}
done in step count: 35
reward sum = 0.4386008316007884
running average episode reward sum: 0.4938687886159547
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 14.]), 'previousTarget': array([ 9., 14.]), 'currentState': array([ 9.73673646, 13.31021211,  2.68220177]), 'targetState': array([ 9, 14], dtype=int32), 'currentDistance': 1.0092511806178115}
episode index:4534
target Thresh 32.0
target distance 14.0
model initialize at round 4534
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 16.]), 'previousTarget': array([18., 16.]), 'currentState': array([16.67196501,  2.21108704,  2.44915813]), 'targetState': array([18, 16], dtype=int32), 'currentDistance': 13.852718058971805}
done in step count: 28
reward sum = 0.5222842459708583
running average episode reward sum: 0.49387505442794033
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 16.]), 'previousTarget': array([18., 16.]), 'currentState': array([17.58856835, 15.04273316,  1.29376464]), 'targetState': array([18, 16], dtype=int32), 'currentDistance': 1.0419384853634586}
episode index:4535
target Thresh 32.0
target distance 13.0
model initialize at round 4535
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 22.]), 'previousTarget': array([ 3., 22.]), 'currentState': array([15.51689122, 13.12420726,  2.7697157 ]), 'targetState': array([ 3, 22], dtype=int32), 'currentDistance': 15.344453810830858}
done in step count: 31
reward sum = 0.49057200958134517
running average episode reward sum: 0.4938743262434503
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 22.]), 'previousTarget': array([ 3., 22.]), 'currentState': array([ 3.73561165, 21.10606677,  2.6221203 ]), 'targetState': array([ 3, 22], dtype=int32), 'currentDistance': 1.157687832712221}
episode index:4536
target Thresh 32.0
target distance 8.0
model initialize at round 4536
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 17.]), 'previousTarget': array([26., 17.]), 'currentState': array([18.03486778,  8.9687452 ,  5.80487347]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 11.311250372076497}
done in step count: 26
reward sum = 0.5865255647692644
running average episode reward sum: 0.4938947474994621
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 17.]), 'previousTarget': array([26., 17.]), 'currentState': array([25.13204333, 16.33486384,  0.86162866]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 1.0935057750557249}
episode index:4537
target Thresh 32.0
target distance 16.0
model initialize at round 4537
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([13.95429702, 25.60701614,  4.49651048]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 17.086286892874114}
done in step count: 40
reward sum = 0.4458448527702761
running average episode reward sum: 0.49388415915774125
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 7.4419928 , 10.92201369,  4.3828041 ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 1.022480750265998}
episode index:4538
target Thresh 32.0
target distance 7.0
model initialize at round 4538
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([ 5.84867326, 20.96336201,  3.57914233]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 7.2045821037896225}
done in step count: 15
reward sum = 0.7324388815167239
running average episode reward sum: 0.49393671582713083
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([ 4.53927218, 14.68365783,  4.53916873]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 0.8707482512775458}
episode index:4539
target Thresh 32.0
target distance 12.0
model initialize at round 4539
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([26.99843216,  6.02604483,  1.85384712]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 12.361504103057394}
done in step count: 27
reward sum = 0.5634038877879928
running average episode reward sum: 0.49395201696632923
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.95826459,  8.49985642,  2.97153585]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.0809322981711327}
episode index:4540
target Thresh 32.0
target distance 13.0
model initialize at round 4540
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([15.14852536, 23.58958881,  5.0826988 ]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 14.333194029438742}
done in step count: 34
reward sum = 0.5168769616384024
running average episode reward sum: 0.49395706540162365
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([22.04311924, 11.905055  ,  5.19645982]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 0.9060815736384303}
episode index:4541
target Thresh 32.0
target distance 18.0
model initialize at round 4541
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.83708639, 19.41655612]), 'previousTarget': array([14.70981108, 19.21358457]), 'currentState': array([2.99842014, 3.29681725, 1.48494665]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.30483165424953573
running average episode reward sum: 0.4939154261653506
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([15.15034016, 20.8248103 ,  0.78264297]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 0.867532866626581}
episode index:4542
target Thresh 32.0
target distance 19.0
model initialize at round 4542
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  3.]), 'previousTarget': array([12.,  3.]), 'currentState': array([ 8.11406587, 21.75168764,  5.07919276]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 19.15009852337524}
done in step count: 46
reward sum = 0.3921103982291483
running average episode reward sum: 0.49389301695823284
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  3.]), 'previousTarget': array([12.,  3.]), 'currentState': array([11.86475007,  3.83410285,  4.98053834]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 0.8449971005258975}
episode index:4543
target Thresh 32.0
target distance 2.0
model initialize at round 4543
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 24.]), 'previousTarget': array([13., 24.]), 'currentState': array([14.86647793, 22.9145793 ,  3.53655833]), 'targetState': array([13, 24], dtype=int32), 'currentDistance': 2.1591382458451474}
done in step count: 4
reward sum = 0.9247764831231838
running average episode reward sum: 0.49398784166469517
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 24.]), 'previousTarget': array([13., 24.]), 'currentState': array([13.5245701 , 23.1508413 ,  2.54313676]), 'targetState': array([13, 24], dtype=int32), 'currentDistance': 0.9981203754430917}
episode index:4544
target Thresh 32.0
target distance 2.0
model initialize at round 4544
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.62893739, 6.66541236, 3.94208893]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 1.780213625893257}
done in step count: 3
reward sum = 0.9431261412255381
running average episode reward sum: 0.49408666197262935
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.0886822 , 5.83844023, 4.37191104]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.8431171602929657}
episode index:4545
target Thresh 32.0
target distance 2.0
model initialize at round 4545
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 21.]), 'previousTarget': array([10., 21.]), 'currentState': array([11.77654031, 22.55618905,  4.1561449 ]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 2.3617408511684483}
done in step count: 3
reward sum = 0.9352375506424546
running average episode reward sum: 0.4941837035231506
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 21.]), 'previousTarget': array([10., 21.]), 'currentState': array([10.92670037, 21.58979432,  3.84095965]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 1.0984675326116897}
episode index:4546
target Thresh 32.0
target distance 9.0
model initialize at round 4546
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  6.]), 'previousTarget': array([13.,  6.]), 'currentState': array([21.80746361,  5.94909153,  3.29861236]), 'targetState': array([13,  6], dtype=int32), 'currentDistance': 8.807610740662613}
done in step count: 20
reward sum = 0.69552364859582
running average episode reward sum: 0.4942279832559575
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  6.]), 'previousTarget': array([13.,  6.]), 'currentState': array([13.76048835,  5.68381239,  3.09816927]), 'targetState': array([13,  6], dtype=int32), 'currentDistance': 0.8236001048737064}
episode index:4547
target Thresh 32.0
target distance 8.0
model initialize at round 4547
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 26.]), 'previousTarget': array([ 4., 26.]), 'currentState': array([ 7.05613549, 18.17783572,  1.49295792]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 8.397988934251082}
done in step count: 17
reward sum = 0.7043382324049771
running average episode reward sum: 0.4942741816396754
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 26.]), 'previousTarget': array([ 4., 26.]), 'currentState': array([ 4.21702412, 25.05149394,  1.84480102]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 0.9730175804084787}
episode index:4548
target Thresh 32.0
target distance 10.0
model initialize at round 4548
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([5.        , 5.        , 4.64435744]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 11.180339887498947}
done in step count: 26
reward sum = 0.5682062538103253
running average episode reward sum: 0.4942904340186973
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.29196144,  9.39991678,  0.52373074]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.9281263265335368}
episode index:4549
target Thresh 32.0
target distance 16.0
model initialize at round 4549
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 12.]), 'previousTarget': array([ 5., 12.]), 'currentState': array([21.0042641 , 22.00029683,  6.10018477]), 'targetState': array([ 5, 12], dtype=int32), 'currentDistance': 18.871735640477812}
done in step count: 59
reward sum = 0.3258245487208724
running average episode reward sum: 0.4942534085494011
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 12.]), 'previousTarget': array([ 5., 12.]), 'currentState': array([ 5.95036402, 12.86559131,  3.89839411]), 'targetState': array([ 5, 12], dtype=int32), 'currentDistance': 1.285472707396468}
episode index:4550
target Thresh 32.0
target distance 4.0
model initialize at round 4550
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 2.]), 'previousTarget': array([5., 2.]), 'currentState': array([9.        , 3.        , 4.34196854]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 4.123105625617661}
done in step count: 8
reward sum = 0.8451607699504525
running average episode reward sum: 0.4943305141001374
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 2.]), 'previousTarget': array([5., 2.]), 'currentState': array([5.95641375, 2.04105964, 3.09589103]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 0.9572947108627293}
episode index:4551
target Thresh 32.0
target distance 11.0
model initialize at round 4551
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 14.]), 'previousTarget': array([ 5., 14.]), 'currentState': array([2.        , 3.        , 2.57662916]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 11.40175425099138}
done in step count: 25
reward sum = 0.5910227848308103
running average episode reward sum: 0.4943517558116336
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 14.]), 'previousTarget': array([ 5., 14.]), 'currentState': array([ 4.48896789, 13.09842207,  1.19998149]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 1.0363380675878713}
episode index:4552
target Thresh 32.0
target distance 9.0
model initialize at round 4552
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 29.]), 'previousTarget': array([18., 29.]), 'currentState': array([21.02129113, 20.00148236,  0.32201117]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 9.492182036584978}
done in step count: 21
reward sum = 0.6359501779141312
running average episode reward sum: 0.49438285583845165
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 29.]), 'previousTarget': array([18., 29.]), 'currentState': array([18.41461927, 28.03417006,  1.82228899]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 1.0510645078826646}
episode index:4553
target Thresh 32.0
target distance 1.0
model initialize at round 4553
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 17.]), 'previousTarget': array([ 4., 17.]), 'currentState': array([ 5.        , 16.        ,  5.85163052]), 'targetState': array([ 4, 17], dtype=int32), 'currentDistance': 1.414213562373095}
done in step count: 9
reward sum = 0.8684372357919783
running average episode reward sum: 0.49446499338345684
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 17.]), 'previousTarget': array([ 4., 17.]), 'currentState': array([ 4.87885188, 16.03404796,  2.71610884]), 'targetState': array([ 4, 17], dtype=int32), 'currentDistance': 1.305926479164741}
episode index:4554
target Thresh 32.0
target distance 13.0
model initialize at round 4554
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([10.        ,  8.        ,  3.76358044]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 13.038404810405298}
done in step count: 35
reward sum = 0.48777875516051067
running average episode reward sum: 0.49446352549361644
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([22.10449812,  6.8686015 ,  0.03641031]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 0.9050906996350001}
episode index:4555
target Thresh 32.0
target distance 8.0
model initialize at round 4555
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 16.]), 'previousTarget': array([16., 16.]), 'currentState': array([24.        , 22.        ,  1.59100756]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 10.0}
done in step count: 27
reward sum = 0.5922624453788986
running average episode reward sum: 0.49448499145496083
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 16.]), 'previousTarget': array([16., 16.]), 'currentState': array([16.69095591, 16.70189316,  3.77727779]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 0.9849233902571686}
episode index:4556
target Thresh 32.0
target distance 12.0
model initialize at round 4556
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 24.]), 'previousTarget': array([11., 24.]), 'currentState': array([23.00139826, 17.93211016,  4.4804821 ]), 'targetState': array([11, 24], dtype=int32), 'currentDistance': 13.448154050256273}
done in step count: 30
reward sum = 0.5027673021642417
running average episode reward sum: 0.49448680894688735
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 24.]), 'previousTarget': array([11., 24.]), 'currentState': array([11.80680424, 23.48860601,  2.72571846]), 'targetState': array([11, 24], dtype=int32), 'currentDistance': 0.9552260927032955}
episode index:4557
target Thresh 32.0
target distance 19.0
model initialize at round 4557
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.74393058,  9.60723587]), 'previousTarget': array([16.76114  ,  9.5672925]), 'currentState': array([ 8.97270834, 28.03569497,  2.47606361]), 'targetState': array([17,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 78
reward sum = 0.22376885048061937
running average episode reward sum: 0.49442741492352926
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  9.]), 'previousTarget': array([17.,  9.]), 'currentState': array([16.04662791,  9.94698885,  5.37589003]), 'targetState': array([17,  9], dtype=int32), 'currentDistance': 1.3437656874045663}
episode index:4558
target Thresh 32.0
target distance 14.0
model initialize at round 4558
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 27.]), 'previousTarget': array([12., 27.]), 'currentState': array([26.        , 13.        ,  5.93306491]), 'targetState': array([12, 27], dtype=int32), 'currentDistance': 19.79898987322333}
done in step count: 94
reward sum = 0.2035101586661006
running average episode reward sum: 0.49436360328583295
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 27.]), 'previousTarget': array([12., 27.]), 'currentState': array([12.77838925, 26.29371522,  2.40577349]), 'targetState': array([12, 27], dtype=int32), 'currentDistance': 1.0510604201638394}
episode index:4559
target Thresh 32.0
target distance 12.0
model initialize at round 4559
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 16.]), 'previousTarget': array([13., 16.]), 'currentState': array([25.00583518, 13.94610029,  4.567729  ]), 'targetState': array([13, 16], dtype=int32), 'currentDistance': 12.180253793693872}
done in step count: 28
reward sum = 0.5450667550659978
running average episode reward sum: 0.4943747223980654
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 16.]), 'previousTarget': array([13., 16.]), 'currentState': array([13.6607283 , 15.73703111,  2.82942653]), 'targetState': array([13, 16], dtype=int32), 'currentDistance': 0.7111360783804783}
episode index:4560
target Thresh 32.0
target distance 8.0
model initialize at round 4560
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([26.0194563 ,  8.93200794,  4.73859644]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 9.974988594808343}
done in step count: 21
reward sum = 0.6436307094205141
running average episode reward sum: 0.4944074467977634
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([18.98354269,  3.50479996,  3.8030917 ]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 1.1055221512169853}
episode index:4561
target Thresh 32.0
target distance 18.0
model initialize at round 4561
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7.11870566, 5.21322265]), 'previousTarget': array([7.28714138, 5.51685448]), 'currentState': array([16.84713149, 22.68771065,  4.22497888]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.36568870113605495
running average episode reward sum: 0.49437923137784634
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 5.]), 'previousTarget': array([7., 5.]), 'currentState': array([7.59018141, 5.86028582, 4.26963353]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 1.043266883901496}
episode index:4562
target Thresh 32.0
target distance 12.0
model initialize at round 4562
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 24.]), 'previousTarget': array([21., 24.]), 'currentState': array([26.       , 12.       ,  5.8705945]), 'targetState': array([21, 24], dtype=int32), 'currentDistance': 13.000000000000002}
done in step count: 32
reward sum = 0.49784200188320105
running average episode reward sum: 0.4943799902580798
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 24.]), 'previousTarget': array([21., 24.]), 'currentState': array([21.22915442, 23.18836694,  2.00130447]), 'targetState': array([21, 24], dtype=int32), 'currentDistance': 0.8433622991602618}
episode index:4563
target Thresh 32.0
target distance 8.0
model initialize at round 4563
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([20.21924081, 15.85802292,  5.58704108]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 8.05727171504358}
done in step count: 17
reward sum = 0.7128013121532966
running average episode reward sum: 0.4944278476905722
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([21.93036994,  8.88684258,  4.67567214]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 0.8895718711006871}
episode index:4564
target Thresh 32.0
target distance 9.0
model initialize at round 4564
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([13.27552224, 10.91994848,  6.1336962 ]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 8.724845011749197}
done in step count: 17
reward sum = 0.7016924500767492
running average episode reward sum: 0.4944732506702844
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([21.14112346, 10.9665332 ,  6.27182354]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 0.859528319705821}
episode index:4565
target Thresh 32.0
target distance 3.0
model initialize at round 4565
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 15.]), 'previousTarget': array([23., 15.]), 'currentState': array([25.52104798, 14.14300253,  2.87183211]), 'targetState': array([23, 15], dtype=int32), 'currentDistance': 2.6627293522548006}
done in step count: 4
reward sum = 0.9173952825716003
running average episode reward sum: 0.49456587485598336
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 15.]), 'previousTarget': array([23., 15.]), 'currentState': array([23.81039269, 14.75550666,  2.70383092]), 'targetState': array([23, 15], dtype=int32), 'currentDistance': 0.8464710937460691}
episode index:4566
target Thresh 32.0
target distance 14.0
model initialize at round 4566
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 25.]), 'previousTarget': array([ 2., 25.]), 'currentState': array([15.700741  , 26.06809218,  2.9909059 ]), 'targetState': array([ 2, 25], dtype=int32), 'currentDistance': 13.74231147701918}
done in step count: 30
reward sum = 0.5412574358397939
running average episode reward sum: 0.49457609853914164
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 25.]), 'previousTarget': array([ 2., 25.]), 'currentState': array([ 2.98630553, 25.20996456,  3.26716099]), 'targetState': array([ 2, 25], dtype=int32), 'currentDistance': 1.0084065270874578}
episode index:4567
target Thresh 32.0
target distance 17.0
model initialize at round 4567
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.71414506, 10.56139529]), 'previousTarget': array([16.71414506, 10.56139529]), 'currentState': array([ 4.        , 26.        ,  2.68823862]), 'targetState': array([18,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.2638833115627962
running average episode reward sum: 0.4945255966155479
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  9.]), 'previousTarget': array([18.,  9.]), 'currentState': array([17.27121089,  9.90981921,  5.35170527]), 'targetState': array([18,  9], dtype=int32), 'currentDistance': 1.165720619146811}
episode index:4568
target Thresh 32.0
target distance 5.0
model initialize at round 4568
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  7.]), 'previousTarget': array([19.,  7.]), 'currentState': array([23.99381888,  1.96713005,  4.2780166 ]), 'targetState': array([19,  7], dtype=int32), 'currentDistance': 7.089993431809421}
done in step count: 16
reward sum = 0.7100271813635186
running average episode reward sum: 0.49457276264416417
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  7.]), 'previousTarget': array([19.,  7.]), 'currentState': array([19.73414601,  6.2286656 ,  2.17889605]), 'targetState': array([19,  7], dtype=int32), 'currentDistance': 1.0648601428678397}
episode index:4569
target Thresh 32.0
target distance 20.0
model initialize at round 4569
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 24.]), 'previousTarget': array([ 6., 24.]), 'currentState': array([25.95368515, 24.01720804,  2.93186897]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 19.953692569511514}
done in step count: 52
reward sum = 0.36347674480193226
running average episode reward sum: 0.49454407642581794
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 24.]), 'previousTarget': array([ 6., 24.]), 'currentState': array([ 6.84208073, 23.70269393,  2.94579223]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 0.8930234296954264}
episode index:4570
target Thresh 32.0
target distance 14.0
model initialize at round 4570
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 20.]), 'previousTarget': array([16., 20.]), 'currentState': array([5.        , 6.        , 5.34207726]), 'targetState': array([16, 20], dtype=int32), 'currentDistance': 17.804493814764854}
done in step count: 45
reward sum = 0.3763707292668864
running average episode reward sum: 0.49451822358242287
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 20.]), 'previousTarget': array([16., 20.]), 'currentState': array([15.07755641, 19.56159022,  0.78795456]), 'targetState': array([16, 20], dtype=int32), 'currentDistance': 1.02132527144725}
episode index:4571
target Thresh 32.0
target distance 11.0
model initialize at round 4571
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 24.]), 'previousTarget': array([21., 24.]), 'currentState': array([10.0999402 , 26.78861369,  5.33138293]), 'targetState': array([21, 24], dtype=int32), 'currentDistance': 11.251118608579432}
done in step count: 25
reward sum = 0.6111561644972341
running average episode reward sum: 0.49454373494307785
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 24.]), 'previousTarget': array([21., 24.]), 'currentState': array([20.05178595, 24.39729673,  6.01969977]), 'targetState': array([21, 24], dtype=int32), 'currentDistance': 1.0280829607162933}
episode index:4572
target Thresh 32.0
target distance 10.0
model initialize at round 4572
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([24.81930818,  7.26056585,  2.30385655]), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 12.502705869267336}
done in step count: 26
reward sum = 0.5860002186970089
running average episode reward sum: 0.4945637341741633
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.86728654, 14.22976358,  2.54695294]), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1599353805957653}
episode index:4573
target Thresh 32.0
target distance 5.0
model initialize at round 4573
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 19.]), 'previousTarget': array([ 2., 19.]), 'currentState': array([ 2.        , 24.        ,  5.56591797]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 5.0}
done in step count: 9
reward sum = 0.8174538363070032
running average episode reward sum: 0.4946343266757227
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 19.]), 'previousTarget': array([ 2., 19.]), 'currentState': array([ 2.13658222, 19.97359701,  4.35947861]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 0.9831306360261447}
episode index:4574
target Thresh 32.0
target distance 18.0
model initialize at round 4574
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  4.]), 'previousTarget': array([26.,  4.]), 'currentState': array([26.10563835, 21.83177586,  5.0754447 ]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 17.8320887634927}
done in step count: 42
reward sum = 0.4156863999751461
running average episode reward sum: 0.4946170702983018
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  4.]), 'previousTarget': array([26.,  4.]), 'currentState': array([26.20144853,  4.88156806,  4.65131427]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 0.9042918529227996}
episode index:4575
target Thresh 32.0
target distance 4.0
model initialize at round 4575
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 13.]), 'previousTarget': array([21., 13.]), 'currentState': array([19.        ,  9.        ,  3.35935423]), 'targetState': array([21, 13], dtype=int32), 'currentDistance': 4.47213595499958}
done in step count: 11
reward sum = 0.7840331904331204
running average episode reward sum: 0.4946803168280516
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 13.]), 'previousTarget': array([21., 13.]), 'currentState': array([20.22338452, 12.3722846 ,  0.78044154]), 'targetState': array([21, 13], dtype=int32), 'currentDistance': 0.9985781008195213}
episode index:4576
target Thresh 32.0
target distance 23.0
model initialize at round 4576
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6.16497099, 8.58635915]), 'previousTarget': array([6.15885487, 8.58874323]), 'currentState': array([16.02373101, 25.98764792,  5.55091074]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 20.0}
done in step count: 76
reward sum = 0.20022647280166178
running average episode reward sum: 0.494615983455968
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.79198858, 3.8109207 , 4.25706891]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1335070727345324}
episode index:4577
target Thresh 32.0
target distance 19.0
model initialize at round 4577
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8.66410281, 23.90482627]), 'previousTarget': array([ 8.66410281, 23.90482627]), 'currentState': array([22.        ,  9.        ,  1.16773224]), 'targetState': array([ 5, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.22747238970864625
running average episode reward sum: 0.49455762967839106
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 28.]), 'previousTarget': array([ 5., 28.]), 'currentState': array([ 5.88523546, 27.42891832,  2.35304928]), 'targetState': array([ 5, 28], dtype=int32), 'currentDistance': 1.0534591106821876}
episode index:4578
target Thresh 32.0
target distance 10.0
model initialize at round 4578
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 17.]), 'previousTarget': array([12., 17.]), 'currentState': array([22.        , 13.        ,  0.30113571]), 'targetState': array([12, 17], dtype=int32), 'currentDistance': 10.770329614269007}
done in step count: 26
reward sum = 0.5655327124328237
running average episode reward sum: 0.49457312980565776
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 17.]), 'previousTarget': array([12., 17.]), 'currentState': array([12.81678325, 16.99893699,  2.93779445]), 'targetState': array([12, 17], dtype=int32), 'currentDistance': 0.81678393866419}
episode index:4579
target Thresh 32.0
target distance 8.0
model initialize at round 4579
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  8.]), 'previousTarget': array([11.,  8.]), 'currentState': array([2.93839513, 7.92119693, 4.3013711 ]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 8.061990013502546}
done in step count: 18
reward sum = 0.6716820252563284
running average episode reward sum: 0.4946117998701667
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  8.]), 'previousTarget': array([11.,  8.]), 'currentState': array([10.02604435,  7.83942805,  0.12667106]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 0.987103312630219}
episode index:4580
target Thresh 32.0
target distance 6.0
model initialize at round 4580
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 15.]), 'previousTarget': array([12., 15.]), 'currentState': array([12.26498141,  9.28710385,  0.97490206]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 5.7190381701763275}
done in step count: 11
reward sum = 0.7935183466803004
running average episode reward sum: 0.4946770490617864
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 15.]), 'previousTarget': array([12., 15.]), 'currentState': array([12.21204338, 14.12746528,  1.60452987]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 0.8979305307336334}
episode index:4581
target Thresh 32.0
target distance 16.0
model initialize at round 4581
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 13.]), 'previousTarget': array([ 8., 13.]), 'currentState': array([24.        , 10.        ,  1.51877868]), 'targetState': array([ 8, 13], dtype=int32), 'currentDistance': 16.278820596099706}
done in step count: 39
reward sum = 0.4317403716254088
running average episode reward sum: 0.49466331342725206
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 13.]), 'previousTarget': array([ 8., 13.]), 'currentState': array([ 8.92132395, 12.91269229,  3.03570339]), 'targetState': array([ 8, 13], dtype=int32), 'currentDistance': 0.9254514898324412}
episode index:4582
target Thresh 32.0
target distance 17.0
model initialize at round 4582
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 20.]), 'previousTarget': array([ 3., 20.]), 'currentState': array([7.10317037, 3.10466256, 0.91096297]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 17.386443981110776}
done in step count: 38
reward sum = 0.42630567753144993
running average episode reward sum: 0.49464839794920373
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 20.]), 'previousTarget': array([ 3., 20.]), 'currentState': array([ 3.37150568, 19.13344499,  1.89888054]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 0.9428329960291992}
episode index:4583
target Thresh 32.0
target distance 15.0
model initialize at round 4583
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  5.]), 'previousTarget': array([21.,  5.]), 'currentState': array([ 5.99380224, 17.02800452,  1.59038752]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 19.231715056940708}
done in step count: 56
reward sum = 0.3337191410607403
running average episode reward sum: 0.49461329121777076
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  5.]), 'previousTarget': array([21.,  5.]), 'currentState': array([20.05583683,  5.34765262,  5.57255038]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 1.0061344044973786}
episode index:4584
target Thresh 32.0
target distance 24.0
model initialize at round 4584
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.69112683,  8.17372591]), 'previousTarget': array([13.71202025,  8.27212152]), 'currentState': array([16.95843283, 27.90503888,  4.43410504]), 'targetState': array([13,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.27373619991665993
running average episode reward sum: 0.49456511737015874
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  4.]), 'previousTarget': array([13.,  4.]), 'currentState': array([13.06973774,  4.91688244,  4.68401133]), 'targetState': array([13,  4], dtype=int32), 'currentDistance': 0.9195307273627445}
episode index:4585
target Thresh 32.0
target distance 6.0
model initialize at round 4585
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 21.]), 'previousTarget': array([23., 21.]), 'currentState': array([17.00851387, 20.96008912,  5.17506075]), 'targetState': array([23, 21], dtype=int32), 'currentDistance': 5.991619058113088}
done in step count: 13
reward sum = 0.770746188974105
running average episode reward sum: 0.4946253400198761
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 21.]), 'previousTarget': array([23., 21.]), 'currentState': array([2.20273329e+01, 2.08206866e+01, 3.94733645e-03]), 'targetState': array([23, 21], dtype=int32), 'currentDistance': 0.9890574438320858}
episode index:4586
target Thresh 32.0
target distance 2.0
model initialize at round 4586
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 12.]), 'previousTarget': array([27., 12.]), 'currentState': array([25.07626522, 13.66019952,  5.02518898]), 'targetState': array([27, 12], dtype=int32), 'currentDistance': 2.541066302249329}
done in step count: 4
reward sum = 0.9271265323113689
running average episode reward sum: 0.49471962848560347
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 12.]), 'previousTarget': array([27., 12.]), 'currentState': array([26.0262859 , 12.4739034 ,  5.58194651]), 'targetState': array([27, 12], dtype=int32), 'currentDistance': 1.0829143950787736}
episode index:4587
target Thresh 32.0
target distance 5.0
model initialize at round 4587
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  7.]), 'previousTarget': array([20.,  7.]), 'currentState': array([15.29439937,  9.6656446 ,  5.55756734]), 'targetState': array([20,  7], dtype=int32), 'currentDistance': 5.4081732956284085}
done in step count: 10
reward sum = 0.8123560944110554
running average episode reward sum: 0.4947888604964852
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  7.]), 'previousTarget': array([20.,  7.]), 'currentState': array([19.12531459,  7.194814  ,  5.65470978]), 'targetState': array([20,  7], dtype=int32), 'currentDistance': 0.8961177707726345}
episode index:4588
target Thresh 32.0
target distance 5.0
model initialize at round 4588
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([12.99049485, 11.04787741,  1.51427889]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 5.876919904425867}
done in step count: 15
reward sum = 0.7178239941636179
running average episode reward sum: 0.49483746261757194
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.5424874 ,  6.79924425,  5.14294184]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.9209284207015295}
episode index:4589
target Thresh 32.0
target distance 18.0
model initialize at round 4589
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2.84714121, 8.27510248]), 'previousTarget': array([2.90599608, 8.35899411]), 'currentState': array([13.91465628, 24.93373719,  3.95899391]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 47
reward sum = 0.32289704653946766
running average episode reward sum: 0.494800002831934
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.25454806, 7.73082784, 4.27794292]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.7738889075246395}
episode index:4590
target Thresh 32.0
target distance 21.0
model initialize at round 4590
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 26.]), 'previousTarget': array([ 3., 26.]), 'currentState': array([3.        , 6.        , 4.01713067]), 'targetState': array([ 3, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.28286680358330946
running average episode reward sum: 0.4947538400788848
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 27.]), 'previousTarget': array([ 3., 27.]), 'currentState': array([ 2.96940708, 26.02841701,  1.50913654]), 'targetState': array([ 3, 27], dtype=int32), 'currentDistance': 0.9720645225398904}
episode index:4591
target Thresh 32.0
target distance 9.0
model initialize at round 4591
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 21.]), 'previousTarget': array([ 4., 21.]), 'currentState': array([12.9977143 , 13.16729043,  1.79612488]), 'targetState': array([ 4, 21], dtype=int32), 'currentDistance': 11.929383965298047}
done in step count: 28
reward sum = 0.5849975797668157
running average episode reward sum: 0.49477349246122104
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 21.]), 'previousTarget': array([ 4., 21.]), 'currentState': array([ 4.87292401, 20.42455671,  2.60483228]), 'targetState': array([ 4, 21], dtype=int32), 'currentDistance': 1.0455291979353887}
episode index:4592
target Thresh 32.0
target distance 5.0
model initialize at round 4592
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 21.]), 'previousTarget': array([20., 21.]), 'currentState': array([15.        , 23.        ,  1.92182866]), 'targetState': array([20, 21], dtype=int32), 'currentDistance': 5.385164807134504}
done in step count: 14
reward sum = 0.7504923901761819
running average episode reward sum: 0.49482916824996803
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 21.]), 'previousTarget': array([20., 21.]), 'currentState': array([19.20898672, 21.63358911,  5.5397634 ]), 'targetState': array([20, 21], dtype=int32), 'currentDistance': 1.0134777641892165}
episode index:4593
target Thresh 32.0
target distance 14.0
model initialize at round 4593
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 20.]), 'previousTarget': array([25., 20.]), 'currentState': array([10.98402446,  6.024362  ,  2.09457493]), 'targetState': array([25, 20], dtype=int32), 'currentDistance': 19.793080304668546}
done in step count: 53
reward sum = 0.3373714121400858
running average episode reward sum: 0.49479489359691836
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 20.]), 'previousTarget': array([25., 20.]), 'currentState': array([24.28544104, 19.53905551,  0.60350307]), 'targetState': array([25, 20], dtype=int32), 'currentDistance': 0.8503318951105406}
episode index:4594
target Thresh 32.0
target distance 16.0
model initialize at round 4594
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 25.]), 'previousTarget': array([21., 25.]), 'currentState': array([ 5.42342711, 21.97798036,  0.08224248]), 'targetState': array([21, 25], dtype=int32), 'currentDistance': 15.867016908339348}
done in step count: 34
reward sum = 0.47117128852934087
running average episode reward sum: 0.4947897524423878
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 25.]), 'previousTarget': array([21., 25.]), 'currentState': array([20.2833775 , 24.44012505,  0.27292507]), 'targetState': array([21, 25], dtype=int32), 'currentDistance': 0.909399676905788}
episode index:4595
target Thresh 32.0
target distance 4.0
model initialize at round 4595
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 25.]), 'previousTarget': array([12., 25.]), 'currentState': array([10.0023798 , 29.01961408,  1.19755507]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 4.488628271867299}
done in step count: 14
reward sum = 0.77595665571004
running average episode reward sum: 0.4948509288791302
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 25.]), 'previousTarget': array([12., 25.]), 'currentState': array([11.63922475, 25.98609079,  5.03468245]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 1.0500161104762695}
episode index:4596
target Thresh 32.0
target distance 6.0
model initialize at round 4596
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 12.]), 'previousTarget': array([15., 12.]), 'currentState': array([11.1111061 , 17.77351033,  5.27149105]), 'targetState': array([15, 12], dtype=int32), 'currentDistance': 6.961100291182449}
done in step count: 14
reward sum = 0.7735557126120458
running average episode reward sum: 0.49491155641529133
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 12.]), 'previousTarget': array([15., 12.]), 'currentState': array([14.34415809, 12.98596047,  5.28863781]), 'targetState': array([15, 12], dtype=int32), 'currentDistance': 1.1841649642030447}
episode index:4597
target Thresh 32.0
target distance 12.0
model initialize at round 4597
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([ 8.81451856, 16.82740625,  4.03203177]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 13.650098963417639}
done in step count: 31
reward sum = 0.5482454773738494
running average episode reward sum: 0.49492315578914053
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.42143121, 5.89347534, 4.2310935 ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.9878777531985684}
episode index:4598
target Thresh 32.0
target distance 12.0
model initialize at round 4598
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 27.]), 'previousTarget': array([10., 27.]), 'currentState': array([22.05887915, 15.00246772,  0.28260335]), 'targetState': array([10, 27], dtype=int32), 'currentDistance': 17.010506963125774}
done in step count: 40
reward sum = 0.3935446514170094
running average episode reward sum: 0.4949011121917558
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 27.]), 'previousTarget': array([10., 27.]), 'currentState': array([10.76458727, 26.63257152,  2.575838  ]), 'targetState': array([10, 27], dtype=int32), 'currentDistance': 0.8482908600365886}
episode index:4599
target Thresh 32.0
target distance 14.0
model initialize at round 4599
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  4.]), 'previousTarget': array([19.,  4.]), 'currentState': array([ 5.00325677, 11.02089297,  1.19086665]), 'targetState': array([19,  4], dtype=int32), 'currentDistance': 15.658919478410638}
done in step count: 41
reward sum = 0.44807507733315655
running average episode reward sum: 0.4948909326189605
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  4.]), 'previousTarget': array([19.,  4.]), 'currentState': array([18.09154747,  4.25452692,  5.80304289]), 'targetState': array([19,  4], dtype=int32), 'currentDistance': 0.9434351862155351}
episode index:4600
target Thresh 32.0
target distance 22.0
model initialize at round 4600
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.89477488, 15.88244106]), 'previousTarget': array([22.79586847, 15.83486126]), 'currentState': array([4.07640463, 9.10977976, 0.79161093]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.27522035121738225
running average episode reward sum: 0.4948431885238938
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 17.]), 'previousTarget': array([26., 17.]), 'currentState': array([25.2128491 , 16.12627007,  0.5143972 ]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 1.1760146769849191}
episode index:4601
target Thresh 32.0
target distance 18.0
model initialize at round 4601
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 6.]), 'previousTarget': array([9., 6.]), 'currentState': array([27.        ,  4.        ,  1.81727958]), 'targetState': array([9, 6], dtype=int32), 'currentDistance': 18.110770276274835}
done in step count: 45
reward sum = 0.390733553259684
running average episode reward sum: 0.4948205658304423
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 6.]), 'previousTarget': array([9., 6.]), 'currentState': array([9.88866215, 5.91257463, 2.84650434]), 'targetState': array([9, 6], dtype=int32), 'currentDistance': 0.8929521912554977}
episode index:4602
target Thresh 32.0
target distance 15.0
model initialize at round 4602
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([10.69509107, 21.97048983,  3.3769055 ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 16.39938444657623}
done in step count: 38
reward sum = 0.43984699771145663
running average episode reward sum: 0.49480862284366867
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.94445572, 7.8345183 , 4.48231058]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.8363647258236798}
episode index:4603
target Thresh 32.0
target distance 18.0
model initialize at round 4603
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 16.]), 'previousTarget': array([21., 16.]), 'currentState': array([ 3.        , 18.        ,  4.65814734]), 'targetState': array([21, 16], dtype=int32), 'currentDistance': 18.11077027627483}
done in step count: 42
reward sum = 0.3821559700966723
running average episode reward sum: 0.49478415441344564
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 16.]), 'previousTarget': array([21., 16.]), 'currentState': array([20.057433  , 15.47153062,  0.11923116]), 'targetState': array([21, 16], dtype=int32), 'currentDistance': 1.0806074377339534}
episode index:4604
target Thresh 32.0
target distance 13.0
model initialize at round 4604
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 6.]), 'previousTarget': array([5., 6.]), 'currentState': array([14.9924524 , 19.08008057,  1.91726875]), 'targetState': array([5, 6], dtype=int32), 'currentDistance': 16.460182645560163}
done in step count: 45
reward sum = 0.4012667389649591
running average episode reward sum: 0.4947638466142168
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 6.]), 'previousTarget': array([5., 6.]), 'currentState': array([5.21154705, 6.91845522, 4.09263979]), 'targetState': array([5, 6], dtype=int32), 'currentDistance': 0.9425031264170135}
episode index:4605
target Thresh 32.0
target distance 6.0
model initialize at round 4605
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'previousTarget': array([10., 17.]), 'currentState': array([ 5.05556148, 22.7281495 ,  5.02411583]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 7.566978846891066}
done in step count: 14
reward sum = 0.7522400599026469
running average episode reward sum: 0.49481974679078833
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'previousTarget': array([10., 17.]), 'currentState': array([ 9.08693006, 17.90920589,  5.23286036]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 1.2885464938918718}
episode index:4606
target Thresh 32.0
target distance 6.0
model initialize at round 4606
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 12.]), 'previousTarget': array([20., 12.]), 'currentState': array([26.00121608, 12.99893407,  5.34245622]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 6.083786954537502}
done in step count: 19
reward sum = 0.7224082599184456
running average episode reward sum: 0.49486914737970256
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 12.]), 'previousTarget': array([20., 12.]), 'currentState': array([20.87773303, 11.72720814,  2.76147144]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 0.9191467097298022}
episode index:4607
target Thresh 32.0
target distance 11.0
model initialize at round 4607
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 15.]), 'previousTarget': array([21., 15.]), 'currentState': array([13.99663037,  4.27042669,  1.53205556]), 'targetState': array([21, 15], dtype=int32), 'currentDistance': 12.81292041353297}
done in step count: 36
reward sum = 0.5476797205442807
running average episode reward sum: 0.49488060800755945
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 15.]), 'previousTarget': array([21., 15.]), 'currentState': array([20.94839699, 14.04293358,  0.99476747]), 'targetState': array([21, 15], dtype=int32), 'currentDistance': 0.9584565742724005}
episode index:4608
target Thresh 32.0
target distance 22.0
model initialize at round 4608
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 22.]), 'previousTarget': array([23., 22.]), 'currentState': array([ 3.        , 22.        ,  1.19520994]), 'targetState': array([25, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.2788330100488635
running average episode reward sum: 0.4948337328507014
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 22.]), 'previousTarget': array([25., 22.]), 'currentState': array([24.16048853, 21.63781194,  0.16433253]), 'targetState': array([25, 22], dtype=int32), 'currentDistance': 0.9143083154006693}
episode index:4609
target Thresh 32.0
target distance 11.0
model initialize at round 4609
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 12.]), 'previousTarget': array([23., 12.]), 'currentState': array([22.14142378, 22.90271588,  5.58103228]), 'targetState': array([23, 12], dtype=int32), 'currentDistance': 10.936469571380453}
done in step count: 27
reward sum = 0.6098269155182255
running average episode reward sum: 0.49485867714195253
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 12.]), 'previousTarget': array([23., 12.]), 'currentState': array([22.50132326, 12.86056483,  4.7058173 ]), 'targetState': array([23, 12], dtype=int32), 'currentDistance': 0.9946106387897017}
episode index:4610
target Thresh 32.0
target distance 17.0
model initialize at round 4610
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 29.]), 'previousTarget': array([16., 29.]), 'currentState': array([17.02512928, 12.42650277,  1.5240966 ]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 16.60517089241993}
done in step count: 35
reward sum = 0.4564587629248832
running average episode reward sum: 0.49485034924904053
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 29.]), 'previousTarget': array([16., 29.]), 'currentState': array([16.44653771, 28.10803689,  1.65574132]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 0.9974939147589752}
episode index:4611
target Thresh 32.0
target distance 3.0
model initialize at round 4611
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 12.]), 'previousTarget': array([ 6., 12.]), 'currentState': array([8.75984808, 9.97541969, 3.08327341]), 'targetState': array([ 6, 12], dtype=int32), 'currentDistance': 3.4228185544069043}
done in step count: 7
reward sum = 0.8818646148867654
running average episode reward sum: 0.4949342638773227
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 12.]), 'previousTarget': array([ 6., 12.]), 'currentState': array([ 6.77903327, 11.5399045 ,  2.29577142]), 'targetState': array([ 6, 12], dtype=int32), 'currentDistance': 0.9047545022806947}
episode index:4612
target Thresh 32.0
target distance 20.0
model initialize at round 4612
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5.24043397, 11.10242291]), 'previousTarget': array([ 5.23112767, 11.10023299]), 'currentState': array([22.01843085, 21.98813927,  5.58820397]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 20.0}
done in step count: 87
reward sum = 0.18616662595997718
running average episode reward sum: 0.4948673296397513
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.44906568, 9.8488558 , 4.08059013]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.9603208591761856}
episode index:4613
target Thresh 32.0
target distance 25.0
model initialize at round 4613
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9.90448546, 9.95151706]), 'previousTarget': array([9.90448546, 9.95151706]), 'currentState': array([16.        , 29.        ,  0.35496074]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 77
reward sum = 0.18068389282975328
running average episode reward sum: 0.4947992361337239
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 4.]), 'previousTarget': array([8., 4.]), 'currentState': array([8.43417839, 4.66451149, 4.26051253]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 0.793779813442704}
episode index:4614
target Thresh 32.0
target distance 16.0
model initialize at round 4614
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.57541363,  8.34071288]), 'previousTarget': array([18.52228  ,  8.3881475]), 'currentState': array([ 2.97676079, 20.85798213,  4.70104051]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.3316776823884213
running average episode reward sum: 0.49476389018491673
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  8.]), 'previousTarget': array([19.,  8.]), 'currentState': array([18.01629983,  8.23617345,  5.52988706]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 1.0116540586217544}
episode index:4615
target Thresh 32.0
target distance 9.0
model initialize at round 4615
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 12.]), 'previousTarget': array([26., 12.]), 'currentState': array([21.06660628, 21.03192066,  0.19440538]), 'targetState': array([26, 12], dtype=int32), 'currentDistance': 10.2914510363106}
done in step count: 24
reward sum = 0.6247786388869766
running average episode reward sum: 0.4947920562916546
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 12.]), 'previousTarget': array([26., 12.]), 'currentState': array([25.3567123 , 12.92626024,  5.02371575]), 'targetState': array([26, 12], dtype=int32), 'currentDistance': 1.1277309478288313}
episode index:4616
target Thresh 32.0
target distance 12.0
model initialize at round 4616
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 16.]), 'previousTarget': array([23., 16.]), 'currentState': array([11.44220526, 14.02331834,  0.09063829]), 'targetState': array([23, 16], dtype=int32), 'currentDistance': 11.725608286700567}
done in step count: 24
reward sum = 0.6041121874848583
running average episode reward sum: 0.49481573403287044
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 16.]), 'previousTarget': array([23., 16.]), 'currentState': array([22.06218729, 15.90513007,  0.07830747]), 'targetState': array([23, 16], dtype=int32), 'currentDistance': 0.9425990530723451}
episode index:4617
target Thresh 32.0
target distance 7.0
model initialize at round 4617
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 20.]), 'previousTarget': array([16., 20.]), 'currentState': array([22.99716289, 20.993816  ,  4.10511118]), 'targetState': array([16, 20], dtype=int32), 'currentDistance': 7.06738698418541}
done in step count: 18
reward sum = 0.7400206128552581
running average episode reward sum: 0.4948688316679555
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 20.]), 'previousTarget': array([16., 20.]), 'currentState': array([16.96054646, 20.27893463,  3.03834941]), 'targetState': array([16, 20], dtype=int32), 'currentDistance': 1.0002269872449399}
episode index:4618
target Thresh 32.0
target distance 15.0
model initialize at round 4618
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 16.]), 'previousTarget': array([10., 16.]), 'currentState': array([25.        , 22.        ,  5.34707212]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 16.15549442140351}
done in step count: 48
reward sum = 0.3597787660477452
running average episode reward sum: 0.4948395850635779
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 16.]), 'previousTarget': array([10., 16.]), 'currentState': array([10.79705444, 16.35402491,  3.15256845]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 0.872140712649285}
episode index:4619
target Thresh 32.0
target distance 16.0
model initialize at round 4619
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.47620023, 15.61338659]), 'previousTarget': array([10.47772  , 15.6118525]), 'currentState': array([26.00325491,  3.00741289,  1.33643797]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.32194206313755835
running average episode reward sum: 0.4948021613575333
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 16.]), 'previousTarget': array([10., 16.]), 'currentState': array([10.99492539, 15.30043639,  2.36400841]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 1.2162507093323824}
episode index:4620
target Thresh 32.0
target distance 9.0
model initialize at round 4620
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([16.2539984 , 13.32838492,  1.03651665]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 8.959766567735828}
done in step count: 19
reward sum = 0.6755865826329936
running average episode reward sum: 0.49484128371660613
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([14.54697023, 21.13585332,  2.0955747 ]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 1.0227051975095502}
episode index:4621
target Thresh 32.0
target distance 24.0
model initialize at round 4621
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.36714677, 12.17621366]), 'previousTarget': array([18.36442559, 12.19631201]), 'currentState': array([ 2.98710968, 24.96114459,  4.63568884]), 'targetState': array([27,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.1664134732864627
running average episode reward sum: 0.49469821691500443
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  5.]), 'previousTarget': array([27.,  5.]), 'currentState': array([20.17793189,  9.38843499,  5.65891621]), 'targetState': array([27,  5], dtype=int32), 'currentDistance': 8.111656734071946}
episode index:4622
target Thresh 32.0
target distance 6.0
model initialize at round 4622
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 19.]), 'previousTarget': array([14., 19.]), 'currentState': array([ 8.        , 17.        ,  1.62843251]), 'targetState': array([14, 19], dtype=int32), 'currentDistance': 6.324555320336758}
done in step count: 17
reward sum = 0.7431330338523718
running average episode reward sum: 0.4947519557895312
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 19.]), 'previousTarget': array([14., 19.]), 'currentState': array([13.15997919, 18.58772858,  0.09931639]), 'targetState': array([14, 19], dtype=int32), 'currentDistance': 0.9357364366235833}
episode index:4623
target Thresh 32.0
target distance 25.0
model initialize at round 4623
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.06137547, 22.65060074]), 'previousTarget': array([14.0776773 , 22.61161351]), 'currentState': array([17.95370539,  3.03301252,  2.43903755]), 'targetState': array([13, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 81
reward sum = 0.22702400056844965
running average episode reward sum: 0.4946940561452359
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 28.]), 'previousTarget': array([13., 28.]), 'currentState': array([13.53337958, 27.14978933,  1.79409711]), 'targetState': array([13, 28], dtype=int32), 'currentDistance': 1.0036692524354873}
episode index:4624
target Thresh 32.0
target distance 11.0
model initialize at round 4624
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  8.]), 'previousTarget': array([26.,  8.]), 'currentState': array([15.31170534, 16.90024663,  5.93094681]), 'targetState': array([26,  8], dtype=int32), 'currentDistance': 13.90877538468565}
done in step count: 31
reward sum = 0.5338897478799769
running average episode reward sum: 0.4947025308893948
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  8.]), 'previousTarget': array([26.,  8.]), 'currentState': array([25.09272799,  8.39308616,  5.6239708 ]), 'targetState': array([26,  8], dtype=int32), 'currentDistance': 0.9887665160663772}
episode index:4625
target Thresh 32.0
target distance 8.0
model initialize at round 4625
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 11.]), 'previousTarget': array([24., 11.]), 'currentState': array([21.        , 19.        ,  0.24003616]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 8.544003745317532}
done in step count: 22
reward sum = 0.6620141852908983
running average episode reward sum: 0.494738698562201
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 11.]), 'previousTarget': array([24., 11.]), 'currentState': array([23.52946199, 11.82857087,  4.77887582]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 0.9528566002816635}
episode index:4626
target Thresh 32.0
target distance 23.0
model initialize at round 4626
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9.90147065, 17.60486105]), 'previousTarget': array([ 9.91602889, 17.60106106]), 'currentState': array([26.96915721, 28.03050614,  2.56002903]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.1380388440628558
running average episode reward sum: 0.49460194093466153
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([ 8.56383584, 18.53030008,  3.76390162]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 6.430568898138652}
episode index:4627
target Thresh 32.0
target distance 9.0
model initialize at round 4627
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  4.]), 'previousTarget': array([12.,  4.]), 'currentState': array([21.        ,  8.        ,  1.36857682]), 'targetState': array([12,  4], dtype=int32), 'currentDistance': 9.848857801796104}
done in step count: 29
reward sum = 0.5666359147780814
running average episode reward sum: 0.49461750575182734
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  4.]), 'previousTarget': array([12.,  4.]), 'currentState': array([12.50012384,  4.92512146,  4.02890965]), 'targetState': array([12,  4], dtype=int32), 'currentDistance': 1.05165278586996}
episode index:4628
target Thresh 32.0
target distance 2.0
model initialize at round 4628
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 12.]), 'previousTarget': array([11., 12.]), 'currentState': array([13.12908548, 11.97985532,  6.0724126 ]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 2.1291807826161055}
done in step count: 10
reward sum = 0.825308176862866
running average episode reward sum: 0.49468894465247787
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 12.]), 'previousTarget': array([11., 12.]), 'currentState': array([11.80965971, 11.65295937,  2.47624844]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 0.8809006969391201}
episode index:4629
target Thresh 32.0
target distance 9.0
model initialize at round 4629
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([12.21319863, 21.80771738,  5.41747576]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 8.98713216885022}
done in step count: 18
reward sum = 0.6890497053968379
running average episode reward sum: 0.49473092321851336
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([13.5107829 , 13.89761824,  4.69864963]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 1.0222777831126282}
episode index:4630
target Thresh 32.0
target distance 6.0
model initialize at round 4630
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 4.08774737, 15.98241863,  5.83294021]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 6.280608469660389}
done in step count: 12
reward sum = 0.7723875500938036
running average episode reward sum: 0.49479087930291743
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 5.53874803, 10.91534068,  4.72273233]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 1.024988756173647}
episode index:4631
target Thresh 32.0
target distance 17.0
model initialize at round 4631
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  5.]), 'previousTarget': array([24.,  5.]), 'currentState': array([24.97901542, 22.03544135,  2.34879991]), 'targetState': array([24,  5], dtype=int32), 'currentDistance': 17.063549844833705}
done in step count: 44
reward sum = 0.3867833179534699
running average episode reward sum: 0.4947675616083256
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  5.]), 'previousTarget': array([24.,  5.]), 'currentState': array([23.99669454,  5.78438229,  4.44265951]), 'targetState': array([24,  5], dtype=int32), 'currentDistance': 0.7843892566272751}
episode index:4632
target Thresh 32.0
target distance 12.0
model initialize at round 4632
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 2.]), 'previousTarget': array([5., 2.]), 'currentState': array([ 5.98363226, 14.00488942,  3.08918703]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 12.045119441128653}
done in step count: 38
reward sum = 0.5009836907495007
running average episode reward sum: 0.4947689033154572
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 2.]), 'previousTarget': array([5., 2.]), 'currentState': array([4.13169182, 2.76002881, 5.07336417]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 1.15395098932267}
episode index:4633
target Thresh 32.0
target distance 7.0
model initialize at round 4633
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([1.99305694, 6.97419773, 4.66657159]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 7.61203673035093}
done in step count: 19
reward sum = 0.6936122764467382
running average episode reward sum: 0.4948118129773328
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([8.05713242, 3.86596486, 6.22167934]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 0.9523469419535422}
episode index:4634
target Thresh 32.0
target distance 11.0
model initialize at round 4634
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([ 8.99347139, 25.05297837,  1.90612581]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 11.09753649068515}
done in step count: 27
reward sum = 0.5445043271252948
running average episode reward sum: 0.49482253412385874
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([ 7.56309963, 14.8836168 ,  4.77731033]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 0.9857284516938151}
episode index:4635
target Thresh 32.0
target distance 16.0
model initialize at round 4635
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 22.]), 'previousTarget': array([24., 22.]), 'currentState': array([ 8.        , 24.        ,  3.55548644]), 'targetState': array([24, 22], dtype=int32), 'currentDistance': 16.1245154965971}
done in step count: 42
reward sum = 0.40584747494229306
running average episode reward sum: 0.49480334191954867
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 22.]), 'previousTarget': array([24., 22.]), 'currentState': array([23.00486256, 21.79705348,  5.98591771]), 'targetState': array([24, 22], dtype=int32), 'currentDistance': 1.0156209045814069}
episode index:4636
target Thresh 32.0
target distance 10.0
model initialize at round 4636
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([21.00410262, 18.00240205,  0.27718365]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 10.199590834593199}
done in step count: 23
reward sum = 0.6086640978030077
running average episode reward sum: 0.49482789675152694
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([22.22516595,  8.91090947,  4.92362279]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 1.195877862715859}
episode index:4637
target Thresh 32.0
target distance 1.0
model initialize at round 4637
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 20.]), 'previousTarget': array([12., 20.]), 'currentState': array([10.95833948, 21.0065966 ,  3.07601812]), 'targetState': array([12, 20], dtype=int32), 'currentDistance': 1.4485487009775768}
done in step count: 9
reward sum = 0.8865751451954597
running average episode reward sum: 0.494912361445025
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 20.]), 'previousTarget': array([12., 20.]), 'currentState': array([11.06020589, 20.57511109,  5.32897003]), 'targetState': array([12, 20], dtype=int32), 'currentDistance': 1.1018011322288173}
episode index:4638
target Thresh 32.0
target distance 4.0
model initialize at round 4638
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([4.        , 4.        , 0.60334462]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 4.123105625617661}
done in step count: 10
reward sum = 0.8354431884057893
running average episode reward sum: 0.49498576752973306
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([7.04391511, 2.89302163, 5.81881825]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 0.9620512868738768}
episode index:4639
target Thresh 32.0
target distance 20.0
model initialize at round 4639
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  9.]), 'previousTarget': array([18.,  9.]), 'currentState': array([ 6.        , 25.        ,  4.34486005]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 91
reward sum = 0.20021359297341565
running average episode reward sum: 0.4949222390438373
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  5.]), 'previousTarget': array([21.,  5.]), 'currentState': array([20.11199665,  5.26117978,  5.4902684 ]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 0.9256159182821322}
episode index:4640
target Thresh 32.0
target distance 22.0
model initialize at round 4640
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.84744364, 22.19405941]), 'previousTarget': array([13.82541376, 22.21853056]), 'currentState': array([24.04211048,  4.98741045,  6.2384842 ]), 'targetState': array([11, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 91
reward sum = 0.17221476770224523
running average episode reward sum: 0.4948527050056254
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 27.]), 'previousTarget': array([11., 27.]), 'currentState': array([11.83420334, 26.0703323 ,  1.98219406]), 'targetState': array([11, 27], dtype=int32), 'currentDistance': 1.2490705521628565}
episode index:4641
target Thresh 32.0
target distance 8.0
model initialize at round 4641
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'previousTarget': array([10., 17.]), 'currentState': array([9.93885298, 9.02654478, 2.47952771]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 7.973689678823118}
done in step count: 17
reward sum = 0.7046059381101157
running average episode reward sum: 0.4948978909670869
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'previousTarget': array([10., 17.]), 'currentState': array([10.14845161, 16.04910163,  1.36214623]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 0.9624165372724565}
episode index:4642
target Thresh 32.0
target distance 5.0
model initialize at round 4642
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([5.00803738, 8.00722841, 0.4799549 ]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 5.092563187736333}
done in step count: 11
reward sum = 0.806874038060363
running average episode reward sum: 0.49496508376206716
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([9.09815605, 6.85127172, 5.75070189]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 0.9140254953201206}
episode index:4643
target Thresh 32.0
target distance 14.0
model initialize at round 4643
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 15.]), 'previousTarget': array([11., 15.]), 'currentState': array([20.00909775, 29.00071073,  6.10864887]), 'targetState': array([11, 15], dtype=int32), 'currentDistance': 16.648836095475477}
done in step count: 50
reward sum = 0.38354227674636476
running average episode reward sum: 0.49494109090956595
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 15.]), 'previousTarget': array([11., 15.]), 'currentState': array([11.10155169, 15.92349719,  3.84409469]), 'targetState': array([11, 15], dtype=int32), 'currentDistance': 0.9290639435803688}
episode index:4644
target Thresh 32.0
target distance 14.0
model initialize at round 4644
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 13.]), 'previousTarget': array([24., 13.]), 'currentState': array([10.22032707, 26.04158045,  0.16416306]), 'targetState': array([24, 13], dtype=int32), 'currentDistance': 18.972669992480228}
done in step count: 48
reward sum = 0.37702684872938114
running average episode reward sum: 0.4949157057121106
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 13.]), 'previousTarget': array([24., 13.]), 'currentState': array([23.01012114, 13.23786173,  5.3044743 ]), 'targetState': array([24, 13], dtype=int32), 'currentDistance': 1.0180561701729665}
episode index:4645
target Thresh 32.0
target distance 24.0
model initialize at round 4645
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.35100996, 11.99015373]), 'previousTarget': array([12.4000212 , 12.04003392]), 'currentState': array([22.91981598, 28.969563  ,  3.68290231]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 20.0}
done in step count: 76
reward sum = 0.17629594072855814
running average episode reward sum: 0.4948471263395356
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 5.]), 'previousTarget': array([8., 5.]), 'currentState': array([7.94366844, 5.90260475, 4.34102749]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 0.9043608639965002}
episode index:4646
target Thresh 32.0
target distance 15.0
model initialize at round 4646
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 18.]), 'previousTarget': array([25., 18.]), 'currentState': array([10.31618548, 27.06554056,  0.15329482]), 'targetState': array([25, 18], dtype=int32), 'currentDistance': 17.256837329378236}
done in step count: 45
reward sum = 0.3992687045644313
running average episode reward sum: 0.4948265585707008
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 18.]), 'previousTarget': array([25., 18.]), 'currentState': array([24.00165845, 18.0422346 ,  5.45457878]), 'targetState': array([25, 18], dtype=int32), 'currentDistance': 0.9992345097231105}
episode index:4647
target Thresh 32.0
target distance 14.0
model initialize at round 4647
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  7.]), 'previousTarget': array([17.,  7.]), 'currentState': array([21.03941156, 20.83192174,  4.86133943]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 14.409680942935854}
done in step count: 41
reward sum = 0.4851890659977245
running average episode reward sum: 0.49482448509983745
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  7.]), 'previousTarget': array([17.,  7.]), 'currentState': array([16.5724427 ,  7.98707176,  4.31770359]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 1.0756932197923883}
episode index:4648
target Thresh 32.0
target distance 9.0
model initialize at round 4648
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 16.]), 'previousTarget': array([ 6., 16.]), 'currentState': array([15.        , 24.        ,  1.44110423]), 'targetState': array([ 6, 16], dtype=int32), 'currentDistance': 12.041594578792296}
done in step count: 29
reward sum = 0.5263812017708562
running average episode reward sum: 0.49483127295027224
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 16.]), 'previousTarget': array([ 6., 16.]), 'currentState': array([ 6.43027573, 16.8522909 ,  3.90773863]), 'targetState': array([ 6, 16], dtype=int32), 'currentDistance': 0.9547444587874655}
episode index:4649
target Thresh 32.0
target distance 15.0
model initialize at round 4649
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 25.]), 'previousTarget': array([17., 25.]), 'currentState': array([20.        , 10.        ,  3.17578313]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 15.297058540778355}
done in step count: 42
reward sum = 0.4626446671892557
running average episode reward sum: 0.4948243510995709
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 25.]), 'previousTarget': array([17., 25.]), 'currentState': array([17.7653365 , 24.10264032,  1.47597723]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 1.1794042320514866}
episode index:4650
target Thresh 32.0
target distance 18.0
model initialize at round 4650
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 22.]), 'previousTarget': array([ 2., 22.]), 'currentState': array([19.97104396, 13.96757039,  3.82993618]), 'targetState': array([ 2, 22], dtype=int32), 'currentDistance': 19.68446967297673}
done in step count: 54
reward sum = 0.35450023426095806
running average episode reward sum: 0.4947941803584746
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 22.]), 'previousTarget': array([ 2., 22.]), 'currentState': array([ 2.97157456, 21.91501216,  2.38700549]), 'targetState': array([ 2, 22], dtype=int32), 'currentDistance': 0.9752846071007781}
episode index:4651
target Thresh 32.0
target distance 12.0
model initialize at round 4651
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([4.44960972, 6.90675954, 6.14105186]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 11.585928039408012}
done in step count: 24
reward sum = 0.5988021694572738
running average episode reward sum: 0.49481653805174614
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.18525798,  5.95908906,  6.12480716]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.8157685159816773}
episode index:4652
target Thresh 32.0
target distance 15.0
model initialize at round 4652
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  4.]), 'previousTarget': array([26.,  4.]), 'currentState': array([23.9971721 , 19.00261388,  2.64800429]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 15.135710846419823}
done in step count: 35
reward sum = 0.44534390671810503
running average episode reward sum: 0.4948059056358136
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  4.]), 'previousTarget': array([26.,  4.]), 'currentState': array([25.24269866,  4.86044134,  4.71067362]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 1.146239339204126}
episode index:4653
target Thresh 32.0
target distance 9.0
model initialize at round 4653
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 15.]), 'previousTarget': array([ 4., 15.]), 'currentState': array([4.88788574, 6.41238235, 1.78384062]), 'targetState': array([ 4, 15], dtype=int32), 'currentDistance': 8.633395505201282}
done in step count: 17
reward sum = 0.7045700632758024
running average episode reward sum: 0.49485097743590817
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 15.]), 'previousTarget': array([ 4., 15.]), 'currentState': array([ 4.48499715, 14.02435005,  1.54811854]), 'targetState': array([ 4, 15], dtype=int32), 'currentDistance': 1.0895480978035097}
episode index:4654
target Thresh 32.0
target distance 14.0
model initialize at round 4654
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 26.]), 'previousTarget': array([12., 26.]), 'currentState': array([26.        , 23.        ,  1.57014132]), 'targetState': array([12, 26], dtype=int32), 'currentDistance': 14.317821063276353}
done in step count: 31
reward sum = 0.49620001792129487
running average episode reward sum: 0.4948512672405237
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 26.]), 'previousTarget': array([12., 26.]), 'currentState': array([12.96924688, 26.37985441,  2.87951588]), 'targetState': array([12, 26], dtype=int32), 'currentDistance': 1.0410229997258702}
episode index:4655
target Thresh 32.0
target distance 8.0
model initialize at round 4655
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 28.]), 'previousTarget': array([27., 28.]), 'currentState': array([22.33880712, 20.36614593,  0.94276272]), 'targetState': array([27, 28], dtype=int32), 'currentDistance': 8.944408707138123}
done in step count: 21
reward sum = 0.662135541676731
running average episode reward sum: 0.4948871959936243
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 28.]), 'previousTarget': array([27., 28.]), 'currentState': array([27.26889557, 27.02832666,  0.8181698 ]), 'targetState': array([27, 28], dtype=int32), 'currentDistance': 1.0081933879767024}
episode index:4656
target Thresh 32.0
target distance 15.0
model initialize at round 4656
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 13.]), 'previousTarget': array([17., 13.]), 'currentState': array([ 2.10999416, 15.15663734,  0.84878851]), 'targetState': array([17, 13], dtype=int32), 'currentDistance': 15.045376644544358}
done in step count: 34
reward sum = 0.48340872199817275
running average episode reward sum: 0.4948847312150124
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 13.]), 'previousTarget': array([17., 13.]), 'currentState': array([16.04790281, 12.66838258,  6.081901  ]), 'targetState': array([17, 13], dtype=int32), 'currentDistance': 1.0081960011279725}
episode index:4657
target Thresh 32.0
target distance 4.0
model initialize at round 4657
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 19.]), 'previousTarget': array([18., 19.]), 'currentState': array([22.0113267 , 21.98473575,  5.10342363]), 'targetState': array([18, 19], dtype=int32), 'currentDistance': 4.999938935439938}
done in step count: 12
reward sum = 0.7964237407572434
running average episode reward sum: 0.4949494669405475
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 19.]), 'previousTarget': array([18., 19.]), 'currentState': array([18.81034514, 19.59307175,  3.38443683]), 'targetState': array([18, 19], dtype=int32), 'currentDistance': 1.0041879049281481}
episode index:4658
target Thresh 32.0
target distance 13.0
model initialize at round 4658
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 19.]), 'previousTarget': array([ 5., 19.]), 'currentState': array([15.72028439,  6.14871603,  2.61246162]), 'targetState': array([ 5, 19], dtype=int32), 'currentDistance': 16.73559073414963}
done in step count: 38
reward sum = 0.44874673820328986
running average episode reward sum: 0.4949395500638063
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 19.]), 'previousTarget': array([ 5., 19.]), 'currentState': array([ 5.91788708, 18.78325189,  2.16736543]), 'targetState': array([ 5, 19], dtype=int32), 'currentDistance': 0.943131189985158}
episode index:4659
target Thresh 32.0
target distance 11.0
model initialize at round 4659
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 16.]), 'previousTarget': array([18., 16.]), 'currentState': array([ 7.00914929, 16.04903805,  1.13384175]), 'targetState': array([18, 16], dtype=int32), 'currentDistance': 10.9909601044699}
done in step count: 23
reward sum = 0.596906616202489
running average episode reward sum: 0.4949614314084712
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 16.]), 'previousTarget': array([18., 16.]), 'currentState': array([17.17886565, 15.3610525 ,  6.17803219]), 'targetState': array([18, 16], dtype=int32), 'currentDistance': 1.040440063160864}
episode index:4660
target Thresh 32.0
target distance 11.0
model initialize at round 4660
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'previousTarget': array([22., 18.]), 'currentState': array([11.91125094, 28.96443095,  3.77364486]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 14.899718242973902}
done in step count: 32
reward sum = 0.4600702024117103
running average episode reward sum: 0.49495394562666545
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'previousTarget': array([22., 18.]), 'currentState': array([21.06425457, 18.25321823,  5.43668861]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 0.9694013491416901}
episode index:4661
target Thresh 32.0
target distance 8.0
model initialize at round 4661
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  5.]), 'previousTarget': array([21.,  5.]), 'currentState': array([12.95591856, 10.970647  ,  3.98155117]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 10.017777791430023}
done in step count: 25
reward sum = 0.6020585587711406
running average episode reward sum: 0.4949769195891589
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  5.]), 'previousTarget': array([21.,  5.]), 'currentState': array([20.48571682,  5.81542785,  5.36656312]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 0.9640590036586988}
episode index:4662
target Thresh 32.0
target distance 14.0
model initialize at round 4662
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 22.]), 'previousTarget': array([19., 22.]), 'currentState': array([15.99930243,  7.92976309,  4.87157056]), 'targetState': array([19, 22], dtype=int32), 'currentDistance': 14.386651885813867}
done in step count: 32
reward sum = 0.44436089632407494
running average episode reward sum: 0.4949660647696726
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 22.]), 'previousTarget': array([19., 22.]), 'currentState': array([19.2449405 , 21.02220524,  1.29862699]), 'targetState': array([19, 22], dtype=int32), 'currentDistance': 1.008007166193976}
episode index:4663
target Thresh 32.0
target distance 4.0
model initialize at round 4663
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 18.]), 'previousTarget': array([ 5., 18.]), 'currentState': array([ 3.04351788, 22.01238511,  0.0423311 ]), 'targetState': array([ 5, 18], dtype=int32), 'currentDistance': 4.463973178340543}
done in step count: 10
reward sum = 0.8191243141219389
running average episode reward sum: 0.49503556696721807
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 18.]), 'previousTarget': array([ 5., 18.]), 'currentState': array([ 4.89381577, 18.74899627,  4.75791967]), 'targetState': array([ 5, 18], dtype=int32), 'currentDistance': 0.756485621727926}
episode index:4664
target Thresh 32.0
target distance 14.0
model initialize at round 4664
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 27.]), 'previousTarget': array([27., 27.]), 'currentState': array([23.01083746, 12.94699314,  5.16656351]), 'targetState': array([27, 27], dtype=int32), 'currentDistance': 14.608231229830022}
done in step count: 37
reward sum = 0.42621048646598564
running average episode reward sum: 0.4950208134665747
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 27.]), 'previousTarget': array([27., 27.]), 'currentState': array([27.77323989, 26.05349656,  1.58185371]), 'targetState': array([27, 27], dtype=int32), 'currentDistance': 1.2221983047934921}
episode index:4665
target Thresh 32.0
target distance 12.0
model initialize at round 4665
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 15.]), 'previousTarget': array([13., 15.]), 'currentState': array([24.62956699, 24.85006329,  3.71394345]), 'targetState': array([13, 15], dtype=int32), 'currentDistance': 15.240425692383615}
done in step count: 36
reward sum = 0.4890491667805351
running average episode reward sum: 0.49501953364516754
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 15.]), 'previousTarget': array([13., 15.]), 'currentState': array([13.80867675, 15.60871838,  3.5607314 ]), 'targetState': array([13, 15], dtype=int32), 'currentDistance': 1.012173972799721}
episode index:4666
target Thresh 32.0
target distance 10.0
model initialize at round 4666
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 19.]), 'previousTarget': array([25., 19.]), 'currentState': array([26.        ,  9.        ,  5.99152175]), 'targetState': array([25, 19], dtype=int32), 'currentDistance': 10.04987562112089}
done in step count: 24
reward sum = 0.6050207927052784
running average episode reward sum: 0.495043103659965
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 19.]), 'previousTarget': array([25., 19.]), 'currentState': array([25.55726404, 18.05198936,  1.69867942]), 'targetState': array([25, 19], dtype=int32), 'currentDistance': 1.099666938008574}
episode index:4667
target Thresh 32.0
target distance 9.0
model initialize at round 4667
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 26.]), 'previousTarget': array([22., 26.]), 'currentState': array([13.34713141, 27.7879053 ,  5.93404576]), 'targetState': array([22, 26], dtype=int32), 'currentDistance': 8.835651657939431}
done in step count: 19
reward sum = 0.6859289064821807
running average episode reward sum: 0.4950839960770221
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 26.]), 'previousTarget': array([22., 26.]), 'currentState': array([2.11017777e+01, 2.56911913e+01, 1.35963106e-03]), 'targetState': array([22, 26], dtype=int32), 'currentDistance': 0.9498242234582577}
episode index:4668
target Thresh 32.0
target distance 19.0
model initialize at round 4668
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.22453733, 27.65942008]), 'previousTarget': array([12.02072541, 27.30852571]), 'currentState': array([ 2.21022076, 10.34718557,  1.03908494]), 'targetState': array([13, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.3127652405693455
running average episode reward sum: 0.4950449472966606
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 29.]), 'previousTarget': array([13., 29.]), 'currentState': array([13.31690502, 28.09281628,  1.01463631]), 'targetState': array([13, 29], dtype=int32), 'currentDistance': 0.9609428167764262}
episode index:4669
target Thresh 32.0
target distance 8.0
model initialize at round 4669
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 12.]), 'previousTarget': array([12., 12.]), 'currentState': array([18.98613634,  3.95444895,  4.16444206]), 'targetState': array([12, 12], dtype=int32), 'currentDistance': 10.655373880381546}
done in step count: 25
reward sum = 0.5850139382096797
running average episode reward sum: 0.4950642126052073
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 12.]), 'previousTarget': array([12., 12.]), 'currentState': array([12.72886133, 11.51598865,  2.38263302]), 'targetState': array([12, 12], dtype=int32), 'currentDistance': 0.8749318977341699}
episode index:4670
target Thresh 32.0
target distance 9.0
model initialize at round 4670
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  6.]), 'previousTarget': array([19.,  6.]), 'currentState': array([10.0013142 ,  8.92666341,  4.98280716]), 'targetState': array([19,  6], dtype=int32), 'currentDistance': 9.462647877113966}
done in step count: 20
reward sum = 0.6448914961570921
running average episode reward sum: 0.49509628866676836
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  6.]), 'previousTarget': array([19.,  6.]), 'currentState': array([18.30619617,  6.80280848,  5.85196134]), 'targetState': array([19,  6], dtype=int32), 'currentDistance': 1.061067954023293}
episode index:4671
target Thresh 32.0
target distance 3.0
model initialize at round 4671
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 10.]), 'previousTarget': array([18., 10.]), 'currentState': array([20.67823851, 13.02913714,  3.30378389]), 'targetState': array([18, 10], dtype=int32), 'currentDistance': 4.043344321633612}
done in step count: 9
reward sum = 0.8386429661850696
running average episode reward sum: 0.4951698217741139
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 10.]), 'previousTarget': array([18., 10.]), 'currentState': array([18.23714951, 10.89937715,  4.07313555]), 'targetState': array([18, 10], dtype=int32), 'currentDistance': 0.9301178124072}
episode index:4672
target Thresh 32.0
target distance 10.0
model initialize at round 4672
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 16.]), 'previousTarget': array([19., 16.]), 'currentState': array([12.        , 26.        ,  3.65615904]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 12.206555615733702}
done in step count: 34
reward sum = 0.4904971379160026
running average episode reward sum: 0.49516882184176675
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 16.]), 'previousTarget': array([19., 16.]), 'currentState': array([18.00359403, 15.97689473,  5.66708879]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 0.9966738231521878}
episode index:4673
target Thresh 32.0
target distance 10.0
model initialize at round 4673
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 17.]), 'previousTarget': array([15., 17.]), 'currentState': array([10.94577026,  7.18462401,  1.6766859 ]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 10.619716788998597}
done in step count: 24
reward sum = 0.6219662203403469
running average episode reward sum: 0.49519595008278056
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 17.]), 'previousTarget': array([15., 17.]), 'currentState': array([15.05621268, 16.27288352,  1.41945748]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 0.729286119128013}
episode index:4674
target Thresh 32.0
target distance 13.0
model initialize at round 4674
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  2.]), 'previousTarget': array([10.,  2.]), 'currentState': array([13.0464413 , 14.98920345,  5.86782426]), 'targetState': array([10,  2], dtype=int32), 'currentDistance': 13.34167196889425}
done in step count: 36
reward sum = 0.5129481813163151
running average episode reward sum: 0.49519974735149364
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  2.]), 'previousTarget': array([10.,  2.]), 'currentState': array([9.68061027, 2.92047494, 4.35840867]), 'targetState': array([10,  2], dtype=int32), 'currentDistance': 0.9743120221431214}
episode index:4675
target Thresh 32.0
target distance 9.0
model initialize at round 4675
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  7.]), 'previousTarget': array([22.,  7.]), 'currentState': array([13.0442742 ,  8.04576938,  0.54950166]), 'targetState': array([22,  7], dtype=int32), 'currentDistance': 9.016576855504828}
done in step count: 21
reward sum = 0.6705738706964539
running average episode reward sum: 0.4952372525104639
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  7.]), 'previousTarget': array([22.,  7.]), 'currentState': array([21.0494078 ,  6.54081198,  6.1001371 ]), 'targetState': array([22,  7], dtype=int32), 'currentDistance': 1.0556889502860196}
episode index:4676
target Thresh 32.0
target distance 9.0
model initialize at round 4676
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.8997731 , 19.93826939,  3.93799907]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 8.938831309557456}
done in step count: 24
reward sum = 0.6474359599776095
running average episode reward sum: 0.49526979446202835
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.00751255, 11.79982835,  4.8204251 ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.2746594534731726}
episode index:4677
target Thresh 32.0
target distance 23.0
model initialize at round 4677
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.93094569, 21.68932075]), 'previousTarget': array([18.73762894, 21.41923503]), 'currentState': array([6.1560735 , 6.30092771, 1.0410358 ]), 'targetState': array([25, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.19618638117898832
running average episode reward sum: 0.4951219842491936
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 29.]), 'previousTarget': array([25., 29.]), 'currentState': array([23.8162876 , 26.34604228,  0.90499929]), 'targetState': array([25, 29], dtype=int32), 'currentDistance': 2.905970859729559}
episode index:4678
target Thresh 32.0
target distance 14.0
model initialize at round 4678
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([20.0093936 , 11.9968149 ,  6.20877981]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 19.11402447681084}
done in step count: 60
reward sum = 0.3024801760709931
running average episode reward sum: 0.4950808126723229
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([ 6.85264591, 24.92574573,  2.65611116]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 0.8558730896557121}
episode index:4679
target Thresh 32.0
target distance 12.0
model initialize at round 4679
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 7.]), 'previousTarget': array([8., 7.]), 'currentState': array([19.86422047,  6.86200555,  3.81455688]), 'targetState': array([8, 7], dtype=int32), 'currentDistance': 11.865022955238926}
done in step count: 31
reward sum = 0.5752151865174446
running average episode reward sum: 0.49509793540177705
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 7.]), 'previousTarget': array([8., 7.]), 'currentState': array([8.88489497, 7.29352194, 3.22670927]), 'targetState': array([8, 7], dtype=int32), 'currentDistance': 0.9323058746416397}
episode index:4680
target Thresh 32.0
target distance 10.0
model initialize at round 4680
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'previousTarget': array([13., 22.]), 'currentState': array([22.72615796, 17.94091226,  3.27296557]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 10.539181272155618}
done in step count: 26
reward sum = 0.6207918477635102
running average episode reward sum: 0.49512478733776544
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'previousTarget': array([13., 22.]), 'currentState': array([13.88070637, 22.29295131,  2.82590837]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 0.928150941403577}
episode index:4681
target Thresh 32.0
target distance 16.0
model initialize at round 4681
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'previousTarget': array([19.,  3.]), 'currentState': array([ 6.96749696, 18.93883254,  4.47645521]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 19.970666295111826}
done in step count: 67
reward sum = 0.2971906417092909
running average episode reward sum: 0.49508251178338086
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'previousTarget': array([19.,  3.]), 'currentState': array([18.00660401,  2.8772969 ,  5.70518756]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 1.0009453759764655}
episode index:4682
target Thresh 32.0
target distance 20.0
model initialize at round 4682
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.20549671, 23.32118454]), 'previousTarget': array([23.14985851, 23.28991511]), 'currentState': array([ 6.06136121, 13.02173709,  0.2915089 ]), 'targetState': array([26, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 73
reward sum = 0.26865372392615794
running average episode reward sum: 0.4950341605581284
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 25.]), 'previousTarget': array([26., 25.]), 'currentState': array([25.7616341 , 24.06664264,  0.60446244]), 'targetState': array([26, 25], dtype=int32), 'currentDistance': 0.9633142117884281}
episode index:4683
target Thresh 32.0
target distance 9.0
model initialize at round 4683
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([ 7.        , 12.        ,  5.20705512]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 28
reward sum = 0.5923887985257871
running average episode reward sum: 0.4950549450666612
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([ 9.39594017, 20.10663751,  1.3337836 ]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 0.9771720203092619}
episode index:4684
target Thresh 32.0
target distance 20.0
model initialize at round 4684
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.53075311, 18.38463841]), 'previousTarget': array([11.53075311, 18.38463841]), 'currentState': array([23.        ,  2.        ,  3.59329346]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 83
reward sum = 0.2121959220701478
running average episode reward sum: 0.4949945696081775
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 22.]), 'previousTarget': array([ 9., 22.]), 'currentState': array([ 9.97637022, 21.72863453,  2.17410836]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 1.0133795077848207}
episode index:4685
target Thresh 32.0
target distance 13.0
model initialize at round 4685
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 16.]), 'previousTarget': array([ 4., 16.]), 'currentState': array([16.97977063, 14.93619422,  4.17061502]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 13.023291766879288}
done in step count: 35
reward sum = 0.5252622522495601
running average episode reward sum: 0.4950010287807428
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 16.]), 'previousTarget': array([ 4., 16.]), 'currentState': array([ 4.89261333, 16.45256278,  2.98779277]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 1.0007855048767271}
episode index:4686
target Thresh 32.0
target distance 21.0
model initialize at round 4686
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2.17157288, 8.20101013]), 'previousTarget': array([2.17157288, 8.20101013]), 'currentState': array([5.00000000e+00, 2.80000000e+01, 6.87718391e-04]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.27375060955007846
running average episode reward sum: 0.49495382365609364
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.92613161, 7.87418824, 4.53816815]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.877303609207337}
episode index:4687
target Thresh 32.0
target distance 9.0
model initialize at round 4687
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([ 3.00046474, 15.00112719,  0.94236201]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 10.81689245632862}
done in step count: 29
reward sum = 0.578687646683548
running average episode reward sum: 0.49497168496646643
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([11.20598222,  9.04948553,  5.64605781]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.7955583276784534}
episode index:4688
target Thresh 32.0
target distance 16.0
model initialize at round 4688
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'previousTarget': array([11., 19.]), 'currentState': array([5.04452792, 3.00501375, 0.36462563]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 17.067724885423424}
done in step count: 40
reward sum = 0.41808150348314754
running average episode reward sum: 0.49495528697510727
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'previousTarget': array([11., 19.]), 'currentState': array([11.13620081, 18.16038166,  1.33443751]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 0.8505936847511627}
episode index:4689
target Thresh 32.0
target distance 6.0
model initialize at round 4689
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([ 9.99300823, 20.0155206 ,  1.76909274]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 7.825560493283613}
done in step count: 20
reward sum = 0.6623646504394783
running average episode reward sum: 0.4949909819353342
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([15.14955706, 15.51951223,  5.39477541]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 0.996567183290555}
episode index:4690
target Thresh 32.0
target distance 18.0
model initialize at round 4690
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.18114972, 11.31521931]), 'previousTarget': array([25.21358457, 11.29018892]), 'currentState': array([ 8.97435684, 23.03442852,  1.96285069]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.2505796774757529
running average episode reward sum: 0.49493887976000706
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 10.]), 'previousTarget': array([27., 10.]), 'currentState': array([26.06673472, 10.27359115,  5.52046496]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 0.9725411053960826}
episode index:4691
target Thresh 32.0
target distance 8.0
model initialize at round 4691
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 18.]), 'previousTarget': array([13., 18.]), 'currentState': array([16.93259481, 10.0165532 ,  2.64828062]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 8.899478900938902}
done in step count: 18
reward sum = 0.6946630096814175
running average episode reward sum: 0.4949814467101182
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 18.]), 'previousTarget': array([13., 18.]), 'currentState': array([13.93180396, 17.08913472,  1.87148844]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 1.3030480338386512}
episode index:4692
target Thresh 32.0
target distance 9.0
model initialize at round 4692
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 18.]), 'previousTarget': array([13., 18.]), 'currentState': array([22.        , 13.        ,  5.08054733]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 10.295630140986999}
done in step count: 24
reward sum = 0.583807788473689
running average episode reward sum: 0.4950003741215317
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 18.]), 'previousTarget': array([13., 18.]), 'currentState': array([13.74495437, 17.33800944,  2.43411462]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 0.9965884324756882}
episode index:4693
target Thresh 32.0
target distance 15.0
model initialize at round 4693
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 25.]), 'previousTarget': array([16., 25.]), 'currentState': array([23.94449606, 10.35542006,  1.95643547]), 'targetState': array([16, 25], dtype=int32), 'currentDistance': 16.66069443883746}
done in step count: 37
reward sum = 0.4500370554849706
running average episode reward sum: 0.49499079522961936
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 25.]), 'previousTarget': array([16., 25.]), 'currentState': array([16.31968196, 24.04001546,  1.73795284]), 'targetState': array([16, 25], dtype=int32), 'currentDistance': 1.0118136566673064}
episode index:4694
target Thresh 32.0
target distance 25.0
model initialize at round 4694
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7.96281485, 8.89914618]), 'previousTarget': array([7.95151706, 8.90448546]), 'currentState': array([27.01958729, 14.9686951 ,  5.07661048]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 20.0}
done in step count: 68
reward sum = 0.16870657011845092
running average episode reward sum: 0.4949212991220345
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.77483303, 7.23924464, 3.02839305]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.8109280026095633}
episode index:4695
target Thresh 32.0
target distance 14.0
model initialize at round 4695
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([20.95774339, 18.59214125,  4.64073148]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 13.625842213473673}
done in step count: 30
reward sum = 0.5429808686420048
running average episode reward sum: 0.494931533272273
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([19.75976988,  5.96824834,  4.6714688 ]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 0.9976048126371441}
episode index:4696
target Thresh 32.0
target distance 15.0
model initialize at round 4696
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 29.]), 'previousTarget': array([24., 29.]), 'currentState': array([ 8.99788997, 27.99156204,  4.71985054]), 'targetState': array([24, 29], dtype=int32), 'currentDistance': 15.03596529644958}
done in step count: 34
reward sum = 0.46566397732004516
running average episode reward sum: 0.4949253021554001
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 29.]), 'previousTarget': array([24., 29.]), 'currentState': array([23.01323232, 28.6320862 ,  0.10183336]), 'targetState': array([24, 29], dtype=int32), 'currentDistance': 1.0531244072603176}
episode index:4697
target Thresh 32.0
target distance 12.0
model initialize at round 4697
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'previousTarget': array([7., 3.]), 'currentState': array([17.62133916, 14.95638753,  3.48533535]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 15.992749869282944}
done in step count: 39
reward sum = 0.4692129299856278
running average episode reward sum: 0.4949198291089612
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'previousTarget': array([7., 3.]), 'currentState': array([7.72033436, 3.91656835, 3.87440274]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 1.165752603189528}
episode index:4698
target Thresh 32.0
target distance 21.0
model initialize at round 4698
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9.32455532, 23.97366596]), 'previousTarget': array([ 9.32455532, 23.97366596]), 'currentState': array([3.        , 5.        , 2.93706286]), 'targetState': array([10, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.28202217424077086
running average episode reward sum: 0.494874522095795
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 26.]), 'previousTarget': array([10., 26.]), 'currentState': array([ 9.79915657, 25.08915236,  1.27492863]), 'targetState': array([10, 26], dtype=int32), 'currentDistance': 0.9327279914543876}
episode index:4699
target Thresh 32.0
target distance 15.0
model initialize at round 4699
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([ 6.95122751, 22.96291404,  4.04420829]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 16.99033652155785}
done in step count: 42
reward sum = 0.4292548066129199
running average episode reward sum: 0.49486056045420285
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.15564557,  8.80486342,  5.27427622]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.166507408744508}
episode index:4700
target Thresh 32.0
target distance 11.0
model initialize at round 4700
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([25.98332054, 15.07674961,  2.03729188]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 13.065745867582987}
done in step count: 30
reward sum = 0.5252886035354136
running average episode reward sum: 0.4948670331287575
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.78476624,  8.80902991,  3.76682756]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.1271146570094284}
episode index:4701
target Thresh 32.0
target distance 18.0
model initialize at round 4701
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.03363389, 29.02786321,  0.4617601 ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 18.053745187186603}
done in step count: 58
reward sum = 0.35482974735858464
running average episode reward sum: 0.49483725063497397
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.85619625, 11.92997125,  4.68146758]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.9410239364417464}
episode index:4702
target Thresh 32.0
target distance 11.0
model initialize at round 4702
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([ 7.        , 12.        ,  5.86970234]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 13.038404810405298}
done in step count: 35
reward sum = 0.5282113351297126
running average episode reward sum: 0.49484434697443697
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([13.11917664, 22.10951637,  0.73275271]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 1.2525218087080505}
episode index:4703
target Thresh 32.0
target distance 15.0
model initialize at round 4703
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 10.]), 'previousTarget': array([20., 10.]), 'currentState': array([15.00282291, 25.03743672,  1.24303389]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 15.84601785824247}
done in step count: 42
reward sum = 0.40948793967073005
running average episode reward sum: 0.49482620147968703
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 10.]), 'previousTarget': array([20., 10.]), 'currentState': array([20.19568176, 10.78726735,  4.79663447]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 0.8112220577791384}
episode index:4704
target Thresh 32.0
target distance 22.0
model initialize at round 4704
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.76086706, 10.8276638 ]), 'previousTarget': array([21.76343395, 10.82527831]), 'currentState': array([ 9.99592291, 27.00128706,  3.08074006]), 'targetState': array([26,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 82
reward sum = 0.1522218783467194
running average episode reward sum: 0.4947533844078203
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  5.]), 'previousTarget': array([26.,  5.]), 'currentState': array([25.13741707,  5.25657351,  5.36918324]), 'targetState': array([26,  5], dtype=int32), 'currentDistance': 0.899932926536825}
episode index:4705
target Thresh 32.0
target distance 5.0
model initialize at round 4705
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([18.61191767,  3.94334164,  3.12538686]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 5.049715652882741}
done in step count: 10
reward sum = 0.8212624671030108
running average episode reward sum: 0.4948227658533569
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.85116131,  5.75557964,  2.82812771]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.8855602130505442}
episode index:4706
target Thresh 32.0
target distance 10.0
model initialize at round 4706
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 29.]), 'previousTarget': array([ 5., 29.]), 'currentState': array([15.        , 26.        ,  0.38987511]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 10.44030650891055}
done in step count: 28
reward sum = 0.5693902672539559
running average episode reward sum: 0.4948386076849695
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 29.]), 'previousTarget': array([ 5., 29.]), 'currentState': array([ 5.95734334, 28.94941536,  2.62031395]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 0.9586788165999838}
episode index:4707
target Thresh 32.0
target distance 1.0
model initialize at round 4707
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 21.]), 'previousTarget': array([ 2., 21.]), 'currentState': array([ 2.97604039, 21.84753485,  4.40718874]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 1.2926601171746037}
done in step count: 0
reward sum = 0.9955427748270096
running average episode reward sum: 0.49494495946218736
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 21.]), 'previousTarget': array([ 2., 21.]), 'currentState': array([ 2.97604039, 21.84753485,  4.40718874]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 1.2926601171746037}
episode index:4708
target Thresh 32.0
target distance 9.0
model initialize at round 4708
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 3.]), 'previousTarget': array([6., 3.]), 'currentState': array([10.        , 12.        ,  0.26474148]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 9.848857801796104}
done in step count: 23
reward sum = 0.6018528258878804
running average episode reward sum: 0.4949676623431442
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 3.]), 'previousTarget': array([6., 3.]), 'currentState': array([6.2730358 , 3.85168786, 4.28848866]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 0.894382890347923}
episode index:4709
target Thresh 32.0
target distance 4.0
model initialize at round 4709
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 26.]), 'previousTarget': array([18., 26.]), 'currentState': array([19.70388411, 22.40288177,  2.20012192]), 'targetState': array([18, 26], dtype=int32), 'currentDistance': 3.9802613725402307}
done in step count: 7
reward sum = 0.8729931127170424
running average episode reward sum: 0.49504792252369073
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 26.]), 'previousTarget': array([18., 26.]), 'currentState': array([18.49728133, 25.0107532 ,  1.79860638]), 'targetState': array([18, 26], dtype=int32), 'currentDistance': 1.1072027582190984}
episode index:4710
target Thresh 32.0
target distance 17.0
model initialize at round 4710
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([ 5.12669021, 20.94493987,  5.88262608]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 17.582975109571386}
done in step count: 47
reward sum = 0.4235637844123643
running average episode reward sum: 0.4950327486459341
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([21.24940323, 16.12148988,  6.11782633]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 0.7603652429351235}
episode index:4711
target Thresh 32.0
target distance 8.0
model initialize at round 4711
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 17.]), 'previousTarget': array([17., 17.]), 'currentState': array([25.0264185 , 13.96624973,  5.145641  ]), 'targetState': array([17, 17], dtype=int32), 'currentDistance': 8.580619706863747}
done in step count: 21
reward sum = 0.6317962398513808
running average episode reward sum: 0.4950617731559522
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 17.]), 'previousTarget': array([17., 17.]), 'currentState': array([17.76128569, 16.84307019,  2.7306662 ]), 'targetState': array([17, 17], dtype=int32), 'currentDistance': 0.7772920108209271}
episode index:4712
target Thresh 32.0
target distance 20.0
model initialize at round 4712
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.09297124, 21.77231113]), 'previousTarget': array([13.10023299, 21.76887233]), 'currentState': array([23.97436053,  4.9915076 ,  3.27497014]), 'targetState': array([11, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 79
reward sum = 0.23725954895395712
running average episode reward sum: 0.4950070729174201
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 25.]), 'previousTarget': array([11., 25.]), 'currentState': array([11.88805472, 24.65340504,  2.28710314]), 'targetState': array([11, 25], dtype=int32), 'currentDistance': 0.9532938940179432}
episode index:4713
target Thresh 32.0
target distance 19.0
model initialize at round 4713
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([ 3.97475421, 27.00591601,  3.16390944]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 19.00593277949653}
done in step count: 52
reward sum = 0.35774820945037655
running average episode reward sum: 0.4949779556362433
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.38072166, 8.71577608, 4.94775397]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.9464888013395936}
episode index:4714
target Thresh 32.0
target distance 16.0
model initialize at round 4714
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  2.]), 'previousTarget': array([13.,  2.]), 'currentState': array([14.01675906, 17.99707655,  5.87975081]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 16.029356100796807}
done in step count: 36
reward sum = 0.45410684736278134
running average episode reward sum: 0.49496928732059675
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  2.]), 'previousTarget': array([13.,  2.]), 'currentState': array([13.05386533,  2.95924192,  4.62050425]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 0.9607531038117759}
episode index:4715
target Thresh 32.0
target distance 10.0
model initialize at round 4715
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 25.]), 'previousTarget': array([10., 25.]), 'currentState': array([19.84645476, 23.07743939,  2.75187087]), 'targetState': array([10, 25], dtype=int32), 'currentDistance': 10.032393065691798}
done in step count: 23
reward sum = 0.660385261165003
running average episode reward sum: 0.49500436280275206
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 25.]), 'previousTarget': array([10., 25.]), 'currentState': array([10.98409708, 24.90897963,  3.06016817]), 'targetState': array([10, 25], dtype=int32), 'currentDistance': 0.9882974053301212}
episode index:4716
target Thresh 32.0
target distance 2.0
model initialize at round 4716
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 27.]), 'previousTarget': array([ 2., 27.]), 'currentState': array([ 2.03261278, 24.88980672,  5.11698329]), 'targetState': array([ 2, 27], dtype=int32), 'currentDistance': 2.1104452816350148}
done in step count: 9
reward sum = 0.850280422926727
running average episode reward sum: 0.49507968102622546
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 27.]), 'previousTarget': array([ 2., 27.]), 'currentState': array([ 2.575534  , 26.16916134,  1.69474038]), 'targetState': array([ 2, 27], dtype=int32), 'currentDistance': 1.0107087966887882}
episode index:4717
target Thresh 32.0
target distance 15.0
model initialize at round 4717
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 27.]), 'previousTarget': array([ 9., 27.]), 'currentState': array([ 8.        , 12.        ,  2.83852428]), 'targetState': array([ 9, 27], dtype=int32), 'currentDistance': 15.03329637837291}
done in step count: 37
reward sum = 0.47754635419477987
running average episode reward sum: 0.49507596476364985
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 27.]), 'previousTarget': array([ 9., 27.]), 'currentState': array([ 8.98903057, 26.10236371,  1.3389541 ]), 'targetState': array([ 9, 27], dtype=int32), 'currentDistance': 0.8977033161354722}
episode index:4718
target Thresh 32.0
target distance 9.0
model initialize at round 4718
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 13.]), 'previousTarget': array([18., 13.]), 'currentState': array([15.       ,  4.       ,  0.1206229]), 'targetState': array([18, 13], dtype=int32), 'currentDistance': 9.486832980505138}
done in step count: 23
reward sum = 0.6490571968952861
running average episode reward sum: 0.49510859481919794
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 13.]), 'previousTarget': array([18., 13.]), 'currentState': array([17.89354125, 12.21680348,  1.37505207]), 'targetState': array([18, 13], dtype=int32), 'currentDistance': 0.7903987966154031}
episode index:4719
target Thresh 32.0
target distance 4.0
model initialize at round 4719
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 19.]), 'previousTarget': array([18., 19.]), 'currentState': array([21.59317098, 19.16837768,  2.90024388]), 'targetState': array([18, 19], dtype=int32), 'currentDistance': 3.5971139445655362}
done in step count: 6
reward sum = 0.8816740204362773
running average episode reward sum: 0.4951904942737778
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 19.]), 'previousTarget': array([18., 19.]), 'currentState': array([18.91886495, 19.11518483,  3.25018661]), 'targetState': array([18, 19], dtype=int32), 'currentDistance': 0.9260563415512792}
episode index:4720
target Thresh 32.0
target distance 27.0
model initialize at round 4720
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6.67303857, 10.01605026]), 'previousTarget': array([ 6.67544468, 10.02633404]), 'currentState': array([12.9997705 , 28.98899054,  4.68488331]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 96
reward sum = 0.15958124989106304
running average episode reward sum: 0.4951194056814494
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.27256675, 2.93705967, 4.42717485]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.9758962291594804}
episode index:4721
target Thresh 32.0
target distance 16.0
model initialize at round 4721
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.60781539, 20.70121762]), 'previousTarget': array([16.59074408, 20.67882258]), 'currentState': array([1.98361933, 7.05816483, 1.66785413]), 'targetState': array([18, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.3044508862468226
running average episode reward sum: 0.49507902691833316
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 22.]), 'previousTarget': array([18., 22.]), 'currentState': array([17.38163887, 21.05423712,  0.67113408]), 'targetState': array([18, 22], dtype=int32), 'currentDistance': 1.1299725304071493}
episode index:4722
target Thresh 32.0
target distance 16.0
model initialize at round 4722
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 12.]), 'previousTarget': array([27., 12.]), 'currentState': array([18.09535068, 27.88202914,  5.33439042]), 'targetState': array([27, 12], dtype=int32), 'currentDistance': 18.208010022188454}
done in step count: 45
reward sum = 0.422891991331363
running average episode reward sum: 0.4950637427693628
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 12.]), 'previousTarget': array([27., 12.]), 'currentState': array([26.31795303, 12.79958558,  5.31516691]), 'targetState': array([27, 12], dtype=int32), 'currentDistance': 1.0509639190137952}
episode index:4723
target Thresh 32.0
target distance 21.0
model initialize at round 4723
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.60748542, 21.32959752]), 'previousTarget': array([21.45612429, 21.36758945]), 'currentState': array([ 2.14522613, 25.93616496,  5.89210011]), 'targetState': array([23, 21], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 62
reward sum = 0.33184042146827564
running average episode reward sum: 0.4950291908385201
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 21.]), 'previousTarget': array([23., 21.]), 'currentState': array([22.03291575, 21.25969318,  6.18445898]), 'targetState': array([23, 21], dtype=int32), 'currentDistance': 1.0013453435866624}
episode index:4724
target Thresh 32.0
target distance 8.0
model initialize at round 4724
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 20.]), 'previousTarget': array([ 4., 20.]), 'currentState': array([12.02209949, 19.08083046,  1.55641317]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 8.074586852072782}
done in step count: 18
reward sum = 0.6792975851229809
running average episode reward sum: 0.4950681894404851
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 20.]), 'previousTarget': array([ 4., 20.]), 'currentState': array([ 4.69846654, 19.92180321,  3.17148869]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 0.7028301710399235}
episode index:4725
target Thresh 32.0
target distance 25.0
model initialize at round 4725
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9.58245803, 9.06839256]), 'previousTarget': array([9.59490445, 9.06369443]), 'currentState': array([ 7.94038997, 29.00086888,  3.37951756]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 75
reward sum = 0.21873889212686332
running average episode reward sum: 0.4950097194241259
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  4.]), 'previousTarget': array([10.,  4.]), 'currentState': array([9.62449186, 4.7580841 , 4.95833458]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 0.8459892790160685}
episode index:4726
target Thresh 32.0
target distance 16.0
model initialize at round 4726
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 16.]), 'previousTarget': array([27., 16.]), 'currentState': array([11.24065657, 10.18206484,  0.59188791]), 'targetState': array([27, 16], dtype=int32), 'currentDistance': 16.79896647937369}
done in step count: 46
reward sum = 0.4386693467727869
running average episode reward sum: 0.4949978005807471
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 16.]), 'previousTarget': array([27., 16.]), 'currentState': array([26.46134745, 15.04147331,  0.51066239]), 'targetState': array([27, 16], dtype=int32), 'currentDistance': 1.0995089722235354}
episode index:4727
target Thresh 32.0
target distance 6.0
model initialize at round 4727
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([4.99993755, 4.99407534, 4.44934893]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 6.330156489316253}
done in step count: 15
reward sum = 0.7066172837529692
running average episode reward sum: 0.4950425593546837
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.10720087, 10.11452101,  1.7742994 ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.8919445387294764}
episode index:4728
target Thresh 32.0
target distance 17.0
model initialize at round 4728
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 20.]), 'previousTarget': array([ 7., 20.]), 'currentState': array([15.94124154,  3.24275512,  1.82464896]), 'targetState': array([ 7, 20], dtype=int32), 'currentDistance': 18.993447718734874}
done in step count: 53
reward sum = 0.38379994082717217
running average episode reward sum: 0.4950190358574269
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 20.]), 'previousTarget': array([ 7., 20.]), 'currentState': array([ 7.42968832, 19.198774  ,  2.28126122]), 'targetState': array([ 7, 20], dtype=int32), 'currentDistance': 0.9091727813995154}
episode index:4729
target Thresh 32.0
target distance 16.0
model initialize at round 4729
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([22.00558459, 24.99914606,  5.90583293]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 16.00558461318996}
done in step count: 42
reward sum = 0.40429258642211297
running average episode reward sum: 0.49499985478989295
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([ 6.77524208, 24.88807513,  3.03360597]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 0.7832799383692638}
episode index:4730
target Thresh 32.0
target distance 10.0
model initialize at round 4730
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([23.00062306, 12.00030628,  0.70939195]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 12.206662016469556}
done in step count: 33
reward sum = 0.5362872669910692
running average episode reward sum: 0.4950085817846512
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([16.10327726, 21.01611767,  2.0164341 ]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 0.9892879447568376}
episode index:4731
target Thresh 32.0
target distance 7.0
model initialize at round 4731
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  8.]), 'previousTarget': array([10.,  8.]), 'currentState': array([ 3.        , 11.        ,  4.94206131]), 'targetState': array([10,  8], dtype=int32), 'currentDistance': 7.615773105863908}
done in step count: 16
reward sum = 0.7219338425621142
running average episode reward sum: 0.4950565372497352
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  8.]), 'previousTarget': array([10.,  8.]), 'currentState': array([9.1419469 , 8.34911874, 5.85690182]), 'targetState': array([10,  8], dtype=int32), 'currentDistance': 0.9263579308838663}
episode index:4732
target Thresh 32.0
target distance 19.0
model initialize at round 4732
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.99740272,  9.00712998]), 'previousTarget': array([13.91410718,  9.23313766]), 'currentState': array([ 7.15192616, 27.79913495,  5.32857855]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.363886602050418
running average episode reward sum: 0.4950288233399107
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.86024202,  9.8266421 ,  5.19962306]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.8383730995949351}
episode index:4733
target Thresh 32.0
target distance 9.0
model initialize at round 4733
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 28.]), 'previousTarget': array([17., 28.]), 'currentState': array([20.80797834, 19.20836917,  2.28985643]), 'targetState': array([17, 28], dtype=int32), 'currentDistance': 9.580890966869287}
done in step count: 21
reward sum = 0.673123185677813
running average episode reward sum: 0.49506644361078905
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 28.]), 'previousTarget': array([17., 28.]), 'currentState': array([17.26373254, 27.08759755,  2.03859467]), 'targetState': array([17, 28], dtype=int32), 'currentDistance': 0.9497542248539834}
episode index:4734
target Thresh 32.0
target distance 6.0
model initialize at round 4734
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 14.]), 'previousTarget': array([ 6., 14.]), 'currentState': array([11.71574603, 14.83014276,  3.63982289]), 'targetState': array([ 6, 14], dtype=int32), 'currentDistance': 5.775715513728283}
done in step count: 11
reward sum = 0.8043331600256207
running average episode reward sum: 0.49513175865121456
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 14.]), 'previousTarget': array([ 6., 14.]), 'currentState': array([ 6.87702049, 14.01222395,  3.2871794 ]), 'targetState': array([ 6, 14], dtype=int32), 'currentDistance': 0.8771056787217466}
episode index:4735
target Thresh 32.0
target distance 23.0
model initialize at round 4735
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9.47778389, 19.21097258]), 'previousTarget': array([ 9.60106106, 19.08397111]), 'currentState': array([19.77720782,  2.06682295,  2.76665857]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 74
reward sum = 0.2168962622715288
running average episode reward sum: 0.4950730096021479
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([ 6.35008951, 24.09782096,  2.09933566]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 0.9677239737168943}
episode index:4736
target Thresh 32.0
target distance 8.0
model initialize at round 4736
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 27.]), 'previousTarget': array([16., 27.]), 'currentState': array([ 7.98528444, 27.96497539,  4.55394816]), 'targetState': array([16, 27], dtype=int32), 'currentDistance': 8.07259828422061}
done in step count: 20
reward sum = 0.6748228597283612
running average episode reward sum: 0.4951109555278659
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 27.]), 'previousTarget': array([16., 27.]), 'currentState': array([15.22194902, 27.29008284,  6.02654816]), 'targetState': array([16, 27], dtype=int32), 'currentDistance': 0.8303682165201446}
episode index:4737
target Thresh 32.0
target distance 9.0
model initialize at round 4737
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([10.        , 13.        ,  2.43303359]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 9.055385138137417}
done in step count: 25
reward sum = 0.6221091325937096
running average episode reward sum: 0.4951377597020039
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([10.5217295 ,  4.89315628,  5.06438226]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 1.0131489593105518}
episode index:4738
target Thresh 32.0
target distance 17.0
model initialize at round 4738
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([22.        ,  5.        ,  0.54033348]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 17.46424919657298}
done in step count: 50
reward sum = 0.37375017097487395
running average episode reward sum: 0.49511214510214585
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([5.97459085, 8.74267364, 2.99653664]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 1.007990165692375}
episode index:4739
target Thresh 32.0
target distance 17.0
model initialize at round 4739
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 2.02997871, 28.03385333,  0.61125356]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 18.04974552902646}
done in step count: 45
reward sum = 0.38296567047348684
running average episode reward sum: 0.49508848550834234
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.73586486, 11.85052012,  4.97254702]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.8905907235915715}
episode index:4740
target Thresh 32.0
target distance 12.0
model initialize at round 4740
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([18.00746713, 16.89717373,  4.56449255]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 13.377440892552148}
done in step count: 30
reward sum = 0.5283151580638261
running average episode reward sum: 0.4950954938763144
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.67392774, 11.32469255,  3.67163653]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.7480667449848258}
episode index:4741
target Thresh 32.0
target distance 15.0
model initialize at round 4741
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 26.]), 'previousTarget': array([12., 26.]), 'currentState': array([17.73707206, 11.06188297,  2.77849302]), 'targetState': array([12, 26], dtype=int32), 'currentDistance': 16.001916645134663}
done in step count: 40
reward sum = 0.4664396234168144
running average episode reward sum: 0.4950894508838092
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 26.]), 'previousTarget': array([12., 26.]), 'currentState': array([12.65443514, 25.01152069,  2.07315777]), 'targetState': array([12, 26], dtype=int32), 'currentDistance': 1.1854858463722133}
episode index:4742
target Thresh 32.0
target distance 19.0
model initialize at round 4742
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.51898837, 15.2488731 ]), 'previousTarget': array([16.51905368, 15.24510704]), 'currentState': array([ 2.00592585, 29.0100873 ,  0.78715086]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 81
reward sum = 0.1793351294187719
running average episode reward sum: 0.495022878182678
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([20.26020897, 11.4514754 ,  5.78149543]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 0.866672256661345}
episode index:4743
target Thresh 32.0
target distance 17.0
model initialize at round 4743
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'previousTarget': array([27.,  8.]), 'currentState': array([10.       ,  2.       ,  1.8774209]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 18.027756377319946}
done in step count: 49
reward sum = 0.3846956326772394
running average episode reward sum: 0.4949996220179425
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'previousTarget': array([27.,  8.]), 'currentState': array([26.24567666,  7.64001112,  0.28680335]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 0.8358203691308808}
episode index:4744
target Thresh 32.0
target distance 20.0
model initialize at round 4744
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'previousTarget': array([ 2.0992562 , 17.00992562]), 'currentState': array([21.72154466, 18.96940001,  3.20947736]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 19.819633197780703}
done in step count: 52
reward sum = 0.3432490281979854
running average episode reward sum: 0.49496764086013006
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'previousTarget': array([ 2., 17.]), 'currentState': array([ 2.78639145, 16.66238981,  3.21534612]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 0.8557991317917256}
episode index:4745
target Thresh 32.0
target distance 16.0
model initialize at round 4745
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 7.75863666, 18.76899816,  4.04171905]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 16.471366844656938}
done in step count: 38
reward sum = 0.45712522639548214
running average episode reward sum: 0.49495966732147334
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.38943606, 3.95486501, 4.35779132]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0312262756746302}
episode index:4746
target Thresh 32.0
target distance 2.0
model initialize at round 4746
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 24.]), 'previousTarget': array([ 5., 24.]), 'currentState': array([ 4.        , 26.        ,  4.22638422]), 'targetState': array([ 5, 24], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 5
reward sum = 0.9232740729799651
running average episode reward sum: 0.49504989576167946
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 24.]), 'previousTarget': array([ 5., 24.]), 'currentState': array([ 4.16183188, 24.75837252,  5.21939106]), 'targetState': array([ 5, 24], dtype=int32), 'currentDistance': 1.130333883340732}
episode index:4747
target Thresh 32.0
target distance 6.0
model initialize at round 4747
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  9.]), 'previousTarget': array([24.,  9.]), 'currentState': array([19.15965525, 14.85776598,  5.46788554]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 7.598839357391726}
done in step count: 16
reward sum = 0.7483330583624574
running average episode reward sum: 0.4951032409939037
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  9.]), 'previousTarget': array([24.,  9.]), 'currentState': array([23.30132185,  9.81156827,  5.45302377]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 1.070884775521775}
episode index:4748
target Thresh 32.0
target distance 24.0
model initialize at round 4748
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.84555753,  4.51930531]), 'previousTarget': array([22.84555753,  4.51930531]), 'currentState': array([3.        , 7.        , 2.09598613]), 'targetState': array([27,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 82
reward sum = 0.1959627825438229
running average episode reward sum: 0.495040250794188
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  4.]), 'previousTarget': array([27.,  4.]), 'currentState': array([26.18306779,  4.13319699,  0.0763588 ]), 'targetState': array([27,  4], dtype=int32), 'currentDistance': 0.8277195619570912}
episode index:4749
target Thresh 32.0
target distance 16.0
model initialize at round 4749
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 28.]), 'previousTarget': array([ 8., 28.]), 'currentState': array([18.07834246, 11.98322957,  0.04161709]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 18.923792479420115}
done in step count: 56
reward sum = 0.32374594703119236
running average episode reward sum: 0.495004188835501
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 28.]), 'previousTarget': array([ 8., 28.]), 'currentState': array([ 8.58001639, 27.25416888,  2.34119366]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 0.9448190681059607}
episode index:4750
target Thresh 32.0
target distance 10.0
model initialize at round 4750
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 16.]), 'previousTarget': array([16., 16.]), 'currentState': array([ 6.11122416, 22.67351501,  5.15702721]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 11.929949290874786}
done in step count: 26
reward sum = 0.5921896559032844
running average episode reward sum: 0.49502464462734863
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 16.]), 'previousTarget': array([16., 16.]), 'currentState': array([15.09026124, 16.38671139,  5.90985422]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 0.9885192540524524}
episode index:4751
target Thresh 32.0
target distance 19.0
model initialize at round 4751
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([8.        , 6.        , 5.29087335]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 19.02629759044045}
done in step count: 54
reward sum = 0.31916801011345514
running average episode reward sum: 0.49498763775981625
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([ 7.43081232, 24.23391066,  2.08346937]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 0.8789153144747959}
episode index:4752
target Thresh 32.0
target distance 9.0
model initialize at round 4752
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.       ,  2.       ,  0.5570654]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 9.0}
done in step count: 21
reward sum = 0.6660807637834453
running average episode reward sum: 0.4950236346304293
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.17585392, 10.12671753,  1.76256961]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.8908124782042252}
episode index:4753
target Thresh 32.0
target distance 10.0
model initialize at round 4753
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([23.97224217,  5.89927728,  4.21286717]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 10.385153952112132}
done in step count: 23
reward sum = 0.6314083876232794
running average episode reward sum: 0.4950523230513365
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.78560002,  3.414325  ,  3.49549432]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.8881624805595858}
episode index:4754
target Thresh 32.0
target distance 10.0
model initialize at round 4754
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([ 1.96524398, 12.99284151,  3.59047094]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 12.828945546744983}
done in step count: 35
reward sum = 0.5060516492983438
running average episode reward sum: 0.49505463626400675
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([11.62264687,  5.95628215,  5.4824406 ]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 1.0280422823713031}
episode index:4755
target Thresh 32.0
target distance 22.0
model initialize at round 4755
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 23.]), 'previousTarget': array([ 5., 23.]), 'currentState': array([5.        , 3.        , 6.04664898]), 'targetState': array([ 5, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 71
reward sum = 0.24058680292870863
running average episode reward sum: 0.4950011316733139
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 25.]), 'previousTarget': array([ 5., 25.]), 'currentState': array([ 5.36417063, 24.1046613 ,  1.95897467]), 'targetState': array([ 5, 25], dtype=int32), 'currentDistance': 0.9665669314296039}
episode index:4756
target Thresh 32.0
target distance 13.0
model initialize at round 4756
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 27.]), 'previousTarget': array([ 8., 27.]), 'currentState': array([ 9.       , 14.       ,  3.7505722]), 'targetState': array([ 8, 27], dtype=int32), 'currentDistance': 13.038404810405297}
done in step count: 31
reward sum = 0.5029675918394796
running average episode reward sum: 0.4950028063548708
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 27.]), 'previousTarget': array([ 8., 27.]), 'currentState': array([ 7.8116498 , 26.25196159,  1.67585433]), 'targetState': array([ 8, 27], dtype=int32), 'currentDistance': 0.771386579373451}
episode index:4757
target Thresh 32.0
target distance 11.0
model initialize at round 4757
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'previousTarget': array([7., 3.]), 'currentState': array([18.0244823 ,  4.01010427,  0.61478263]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 11.070660354137086}
done in step count: 26
reward sum = 0.5490892587589682
running average episode reward sum: 0.4950141738312063
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'previousTarget': array([7., 3.]), 'currentState': array([7.84024053, 3.01191694, 3.32111868]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 0.8403250336849294}
episode index:4758
target Thresh 32.0
target distance 8.0
model initialize at round 4758
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 20.]), 'previousTarget': array([11., 20.]), 'currentState': array([11.22724925, 27.7099197 ,  5.28581396]), 'targetState': array([11, 20], dtype=int32), 'currentDistance': 7.713268046072012}
done in step count: 16
reward sum = 0.729448132640774
running average episode reward sum: 0.4950634350118765
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 20.]), 'previousTarget': array([11., 20.]), 'currentState': array([10.9831957 , 20.99687736,  4.92106334]), 'targetState': array([11, 20], dtype=int32), 'currentDistance': 0.9970189859380812}
episode index:4759
target Thresh 32.0
target distance 25.0
model initialize at round 4759
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.90799356, 16.66528948]), 'previousTarget': array([19.74433602, 16.77294527]), 'currentState': array([ 2.14058523, 25.84783823,  5.46708906]), 'targetState': array([27, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.1661684587956753
running average episode reward sum: 0.49492452074847154
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 13.]), 'previousTarget': array([27., 13.]), 'currentState': array([22.60955215, 13.33163669,  6.14261792]), 'targetState': array([27, 13], dtype=int32), 'currentDistance': 4.402955282893415}
episode index:4760
target Thresh 32.0
target distance 12.0
model initialize at round 4760
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 28.]), 'previousTarget': array([27., 28.]), 'currentState': array([24.16020092, 16.080104  ,  0.71616527]), 'targetState': array([27, 28], dtype=int32), 'currentDistance': 12.253504787619066}
done in step count: 28
reward sum = 0.5647383866024878
running average episode reward sum: 0.4949391844464035
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 28.]), 'previousTarget': array([27., 28.]), 'currentState': array([27.02456458, 27.21657855,  1.6219852 ]), 'targetState': array([27, 28], dtype=int32), 'currentDistance': 0.7838064773549187}
episode index:4761
target Thresh 32.0
target distance 5.0
model initialize at round 4761
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'previousTarget': array([16., 23.]), 'currentState': array([20.64600028, 26.69235578,  3.77008215]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 5.934543780587926}
done in step count: 10
reward sum = 0.8071262741403107
running average episode reward sum: 0.4950047424240797
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'previousTarget': array([16., 23.]), 'currentState': array([16.88143109, 23.81725688,  3.95204139]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 1.2020106357966547}
episode index:4762
target Thresh 32.0
target distance 18.0
model initialize at round 4762
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.70829809, 28.47515495]), 'previousTarget': array([12.71285862, 28.48314552]), 'currentState': array([ 2.99235148, 10.99372533,  3.60984442]), 'targetState': array([13, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.28108702240212213
running average episode reward sum: 0.4949598300327251
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 29.]), 'previousTarget': array([13., 29.]), 'currentState': array([12.50880153, 28.08611526,  1.3832673 ]), 'targetState': array([13, 29], dtype=int32), 'currentDistance': 1.0375265104776952}
episode index:4763
target Thresh 32.0
target distance 5.0
model initialize at round 4763
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 17.]), 'previousTarget': array([13., 17.]), 'currentState': array([ 8.        , 21.        ,  0.49832362]), 'targetState': array([13, 17], dtype=int32), 'currentDistance': 6.4031242374328485}
done in step count: 16
reward sum = 0.7502708622136584
running average episode reward sum: 0.4950134217691191
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 17.]), 'previousTarget': array([13., 17.]), 'currentState': array([12.70310611, 17.92043995,  5.39961463]), 'targetState': array([13, 17], dtype=int32), 'currentDistance': 0.9671378809044121}
episode index:4764
target Thresh 32.0
target distance 8.0
model initialize at round 4764
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 28.]), 'previousTarget': array([18., 28.]), 'currentState': array([12.        , 20.        ,  2.38656348]), 'targetState': array([18, 28], dtype=int32), 'currentDistance': 10.0}
done in step count: 23
reward sum = 0.6251980020818074
running average episode reward sum: 0.49504074277233273
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 28.]), 'previousTarget': array([18., 28.]), 'currentState': array([17.45817591, 27.28431065,  0.8065365 ]), 'targetState': array([18, 28], dtype=int32), 'currentDistance': 0.897655055587901}
episode index:4765
target Thresh 32.0
target distance 11.0
model initialize at round 4765
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 27.]), 'previousTarget': array([ 3., 27.]), 'currentState': array([13.87979098, 21.15664581,  2.31697084]), 'targetState': array([ 3, 27], dtype=int32), 'currentDistance': 12.349681778926293}
done in step count: 28
reward sum = 0.5951068874818048
running average episode reward sum: 0.49506173860630454
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 27.]), 'previousTarget': array([ 3., 27.]), 'currentState': array([ 3.99930341, 26.21761934,  2.8001826 ]), 'targetState': array([ 3, 27], dtype=int32), 'currentDistance': 1.2691441167265585}
episode index:4766
target Thresh 32.0
target distance 15.0
model initialize at round 4766
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  6.]), 'previousTarget': array([23.,  6.]), 'currentState': array([23.20043052, 20.73072912,  5.24037912]), 'targetState': array([23,  6], dtype=int32), 'currentDistance': 14.73209261125636}
done in step count: 35
reward sum = 0.49947834489232096
running average episode reward sum: 0.4950626651022739
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  6.]), 'previousTarget': array([23.,  6.]), 'currentState': array([23.67320104,  6.78686895,  4.38348774]), 'targetState': array([23,  6], dtype=int32), 'currentDistance': 1.0355493161551879}
episode index:4767
target Thresh 32.0
target distance 11.0
model initialize at round 4767
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([14.96582252, 18.0721161 ,  2.26308599]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 13.04852825865161}
done in step count: 28
reward sum = 0.5392541573116835
running average episode reward sum: 0.49507193345215
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.95510457, 11.9054867 ,  3.84423004]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.3161044469017706}
episode index:4768
target Thresh 32.0
target distance 1.0
model initialize at round 4768
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 26.]), 'previousTarget': array([ 6., 26.]), 'currentState': array([ 6.      , 25.      ,  4.156394]), 'targetState': array([ 6, 26], dtype=int32), 'currentDistance': 1.0}
done in step count: 6
reward sum = 0.900935213859419
running average episode reward sum: 0.4951570379353556
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 26.]), 'previousTarget': array([ 6., 26.]), 'currentState': array([ 5.97641634, 25.05599456,  1.87213227]), 'targetState': array([ 6, 26], dtype=int32), 'currentDistance': 0.9442999861016126}
episode index:4769
target Thresh 32.0
target distance 13.0
model initialize at round 4769
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 18.]), 'previousTarget': array([ 3., 18.]), 'currentState': array([16.        , 28.        ,  6.22247106]), 'targetState': array([ 3, 18], dtype=int32), 'currentDistance': 16.401219466856727}
done in step count: 41
reward sum = 0.4024871006420501
running average episode reward sum: 0.4951376102755457
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 18.]), 'previousTarget': array([ 3., 18.]), 'currentState': array([ 3.81843928, 18.33041128,  3.91211065]), 'targetState': array([ 3, 18], dtype=int32), 'currentDistance': 0.8826179596176937}
episode index:4770
target Thresh 32.0
target distance 5.0
model initialize at round 4770
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 19.]), 'previousTarget': array([19., 19.]), 'currentState': array([23.99583866, 18.01654277,  2.06973374]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 5.0917179814399605}
done in step count: 10
reward sum = 0.8121235539642271
running average episode reward sum: 0.4952040504230386
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 19.]), 'previousTarget': array([19., 19.]), 'currentState': array([19.96033266, 19.13369459,  3.05653076]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 0.9695942813607258}
episode index:4771
target Thresh 32.0
target distance 20.0
model initialize at round 4771
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.60700849,  4.12283287]), 'previousTarget': array([15.60700849,  4.12283287]), 'currentState': array([ 9.        , 23.        ,  0.80041468]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.25932929126856435
running average episode reward sum: 0.4951546215129056
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.55227345,  3.97944923,  5.41773602]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.0769307616668784}
episode index:4772
target Thresh 32.0
target distance 10.0
model initialize at round 4772
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 11.]), 'previousTarget': array([20., 11.]), 'currentState': array([10.28306234,  8.27371468,  0.68464589]), 'targetState': array([20, 11], dtype=int32), 'currentDistance': 10.092150870896717}
done in step count: 20
reward sum = 0.6551490777684593
running average episode reward sum: 0.4951881422454125
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 11.]), 'previousTarget': array([20., 11.]), 'currentState': array([19.11426488, 10.61831613,  0.3464947 ]), 'targetState': array([20, 11], dtype=int32), 'currentDistance': 0.9644735806730028}
episode index:4773
target Thresh 32.0
target distance 9.0
model initialize at round 4773
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 23.]), 'previousTarget': array([ 7., 23.]), 'currentState': array([ 7.97367146, 13.96694778,  3.80033788]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 9.085376632066756}
done in step count: 21
reward sum = 0.6269664965303091
running average episode reward sum: 0.4952157455873239
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 23.]), 'previousTarget': array([ 7., 23.]), 'currentState': array([ 7.07826455, 22.11910842,  1.76652317]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 0.8843615250876287}
episode index:4774
target Thresh 32.0
target distance 12.0
model initialize at round 4774
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 27.]), 'previousTarget': array([ 8., 27.]), 'currentState': array([ 2.19948419, 15.20220773,  0.86956489]), 'targetState': array([ 8, 27], dtype=int32), 'currentDistance': 13.146630222657052}
done in step count: 29
reward sum = 0.5502128929421424
running average episode reward sum: 0.4952272633145186
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 27.]), 'previousTarget': array([ 8., 27.]), 'currentState': array([ 8.11785305, 26.29793664,  1.35486545]), 'targetState': array([ 8, 27], dtype=int32), 'currentDistance': 0.7118864365196316}
episode index:4775
target Thresh 32.0
target distance 10.0
model initialize at round 4775
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 17.]), 'previousTarget': array([21., 17.]), 'currentState': array([10.95400596, 27.0153234 ,  3.03447822]), 'targetState': array([21, 17], dtype=int32), 'currentDistance': 14.18551018308327}
done in step count: 36
reward sum = 0.46882655333788376
running average episode reward sum: 0.4952217355276725
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 17.]), 'previousTarget': array([21., 17.]), 'currentState': array([20.19984283, 17.44778503,  5.63543088]), 'targetState': array([21, 17], dtype=int32), 'currentDistance': 0.916931254827454}
episode index:4776
target Thresh 32.0
target distance 8.0
model initialize at round 4776
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([19.82886031,  4.24217091,  2.22865893]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 10.342113312610483}
done in step count: 19
reward sum = 0.6572166843587737
running average episode reward sum: 0.49525564696766233
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.93740613, 10.42323282,  2.54785035]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.1006319265587838}
episode index:4777
target Thresh 32.0
target distance 8.0
model initialize at round 4777
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 11.]), 'previousTarget': array([20., 11.]), 'currentState': array([11.91625962, 14.99866424,  3.41004252]), 'targetState': array([20, 11], dtype=int32), 'currentDistance': 9.01865701164565}
done in step count: 21
reward sum = 0.6179486164648401
running average episode reward sum: 0.495281325697151
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 11.]), 'previousTarget': array([20., 11.]), 'currentState': array([19.21383311, 11.23429849,  6.07456878]), 'targetState': array([20, 11], dtype=int32), 'currentDistance': 0.820337829764331}
episode index:4778
target Thresh 32.0
target distance 7.0
model initialize at round 4778
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 28.]), 'previousTarget': array([ 4., 28.]), 'currentState': array([11.05413202, 24.99996745,  0.24700808]), 'targetState': array([ 4, 28], dtype=int32), 'currentDistance': 7.6655706760047835}
done in step count: 22
reward sum = 0.6569852953609381
running average episode reward sum: 0.49531516205824405
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 28.]), 'previousTarget': array([ 4., 28.]), 'currentState': array([ 4.82223378, 28.03581742,  2.89556315]), 'targetState': array([ 4, 28], dtype=int32), 'currentDistance': 0.8230135347973153}
episode index:4779
target Thresh 32.0
target distance 8.0
model initialize at round 4779
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 28.]), 'previousTarget': array([ 8., 28.]), 'currentState': array([ 9.        , 20.        ,  3.70561743]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 19
reward sum = 0.6675822308200463
running average episode reward sum: 0.4953512011939683
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 28.]), 'previousTarget': array([ 8., 28.]), 'currentState': array([ 8.17385757, 27.17337351,  1.68585059]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 0.8447116702384005}
episode index:4780
target Thresh 32.0
target distance 7.0
model initialize at round 4780
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([ 9.0547082 , 22.41091221,  1.37623481]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 6.6565497437542245}
done in step count: 14
reward sum = 0.7671980850909816
running average episode reward sum: 0.49540806103163765
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([ 9.95628184, 28.15507484,  1.58213722]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 0.8460554406109932}
episode index:4781
target Thresh 32.0
target distance 3.0
model initialize at round 4781
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 24.]), 'previousTarget': array([11., 24.]), 'currentState': array([ 8.15867527, 22.27565765,  0.96473989]), 'targetState': array([11, 24], dtype=int32), 'currentDistance': 3.3236249457070026}
done in step count: 8
reward sum = 0.8858519137530293
running average episode reward sum: 0.49548970968339867
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 24.]), 'previousTarget': array([11., 24.]), 'currentState': array([10.10855506, 23.73710329,  0.45773044]), 'targetState': array([11, 24], dtype=int32), 'currentDistance': 0.9294023684748355}
episode index:4782
target Thresh 32.0
target distance 7.0
model initialize at round 4782
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 19.]), 'previousTarget': array([ 2., 19.]), 'currentState': array([ 3.        , 12.        ,  3.14137682]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 7.071067811865475}
done in step count: 17
reward sum = 0.7228457600323406
running average episode reward sum: 0.4955372438774921
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 19.]), 'previousTarget': array([ 2., 19.]), 'currentState': array([ 2.20692443, 18.1161136 ,  1.7267906 ]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 0.9077846048188107}
episode index:4783
target Thresh 32.0
target distance 10.0
model initialize at round 4783
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([11.15316051,  8.07330399,  0.59301677]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 12.653544313045332}
done in step count: 27
reward sum = 0.5682585214301454
running average episode reward sum: 0.49555244481343536
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([18.75083103, 17.1915205 ,  0.99197408]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 0.8460048905471105}
episode index:4784
target Thresh 32.0
target distance 19.0
model initialize at round 4784
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.60024095, 24.16025981]), 'previousTarget': array([16.56172689, 24.07475678]), 'currentState': array([8.00362442, 6.10207097, 1.28280246]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.3056774648218779
running average episode reward sum: 0.495512763521901
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 25.]), 'previousTarget': array([17., 25.]), 'currentState': array([16.78331995, 24.26847406,  1.22506603]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 0.7629419698686337}
episode index:4785
target Thresh 32.0
target distance 12.0
model initialize at round 4785
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  8.]), 'previousTarget': array([26.,  8.]), 'currentState': array([14.2292554 , 18.79994786,  5.36811203]), 'targetState': array([26,  8], dtype=int32), 'currentDistance': 15.974645603917565}
done in step count: 39
reward sum = 0.47453600993470235
running average episode reward sum: 0.49550838058132696
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.,  8.]), 'previousTarget': array([26.,  8.]), 'currentState': array([25.00002099,  8.61987047,  5.59358518]), 'targetState': array([26,  8], dtype=int32), 'currentDistance': 1.1765191977804517}
episode index:4786
target Thresh 32.0
target distance 21.0
model initialize at round 4786
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9.1499527 , 21.95559337]), 'previousTarget': array([ 9.17157288, 21.79898987]), 'currentState': array([11.99234378,  2.15860337,  1.64001673]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.3396224124619025
running average episode reward sum: 0.4954758161426139
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([ 9.07909358, 22.21537457,  1.53546807]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 0.78860183859774}
episode index:4787
target Thresh 32.0
target distance 6.0
model initialize at round 4787
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 19.]), 'previousTarget': array([ 7., 19.]), 'currentState': array([10.93587309, 24.61582779,  4.44711559]), 'targetState': array([ 7, 19], dtype=int32), 'currentDistance': 6.857741522079002}
done in step count: 14
reward sum = 0.765128077503867
running average episode reward sum: 0.49553213449294
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 19.]), 'previousTarget': array([ 7., 19.]), 'currentState': array([ 7.46412639, 19.82608188,  4.05708032]), 'targetState': array([ 7, 19], dtype=int32), 'currentDistance': 0.9475360516398775}
episode index:4788
target Thresh 32.0
target distance 7.0
model initialize at round 4788
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([12.0547881 ,  6.94307152,  5.22613049]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 7.688618008998137}
done in step count: 22
reward sum = 0.6591821560878146
running average episode reward sum: 0.4955663065584223
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([5.86409765, 9.66256236, 2.62139725]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.9276469790539588}
episode index:4789
target Thresh 32.0
target distance 5.0
model initialize at round 4789
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([12.69959142, 16.86071294,  3.50386944]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 4.701655066873526}
done in step count: 10
reward sum = 0.8411420332416019
running average episode reward sum: 0.49563845180407645
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([ 8.91555851, 16.68447323,  2.95257574]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 0.968403081958504}
episode index:4790
target Thresh 32.0
target distance 13.0
model initialize at round 4790
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 16.]), 'previousTarget': array([23., 16.]), 'currentState': array([26.        , 29.        ,  6.18823415]), 'targetState': array([23, 16], dtype=int32), 'currentDistance': 13.341664064126334}
done in step count: 32
reward sum = 0.5076388080348095
running average episode reward sum: 0.49564095657473617
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 16.]), 'previousTarget': array([23., 16.]), 'currentState': array([22.93106826, 16.96090845,  4.59581499]), 'targetState': array([23, 16], dtype=int32), 'currentDistance': 0.9633777206440934}
episode index:4791
target Thresh 32.0
target distance 22.0
model initialize at round 4791
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.27479181,  6.71474571]), 'previousTarget': array([20.20732955,  6.72394111]), 'currentState': array([ 2.10711259, 15.07748639,  0.41376725]), 'targetState': array([24,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.24663352749158513
running average episode reward sum: 0.49558899342175555
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  5.]), 'previousTarget': array([24.,  5.]), 'currentState': array([23.15701101,  5.21548366,  5.93612156]), 'targetState': array([24,  5], dtype=int32), 'currentDistance': 0.8700940433482404}
episode index:4792
target Thresh 32.0
target distance 8.0
model initialize at round 4792
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([24.00272715, 21.04637928,  1.76456296]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 8.002861540572885}
done in step count: 19
reward sum = 0.6872432249748506
running average episode reward sum: 0.4956289796999848
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([16.86244793, 21.14640592,  3.10935792]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 0.8747863284572917}
episode index:4793
target Thresh 32.0
target distance 19.0
model initialize at round 4793
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  7.]), 'previousTarget': array([13.,  7.]), 'currentState': array([ 6.98840215, 26.00288734,  3.15009761]), 'targetState': array([13,  7], dtype=int32), 'currentDistance': 19.931107247036817}
done in step count: 58
reward sum = 0.3187723940753846
running average episode reward sum: 0.49559208846393465
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  7.]), 'previousTarget': array([13.,  7.]), 'currentState': array([12.67752898,  7.88805764,  4.80229287]), 'targetState': array([13,  7], dtype=int32), 'currentDistance': 0.9447930586764156}
episode index:4794
target Thresh 32.0
target distance 17.0
model initialize at round 4794
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 20.]), 'previousTarget': array([ 4., 20.]), 'currentState': array([20.99904009, 25.94442583,  4.45260409]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 18.008430316934547}
done in step count: 47
reward sum = 0.3957465366812495
running average episode reward sum: 0.49557126561684756
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 20.]), 'previousTarget': array([ 4., 20.]), 'currentState': array([ 4.9475729 , 20.41223187,  3.54701996]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 1.0333583701952713}
episode index:4795
target Thresh 32.0
target distance 8.0
model initialize at round 4795
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 27.]), 'previousTarget': array([10., 27.]), 'currentState': array([13.       , 19.       ,  4.0062024]), 'targetState': array([10, 27], dtype=int32), 'currentDistance': 8.54400374531753}
done in step count: 27
reward sum = 0.6347772636396327
running average episode reward sum: 0.49560029105430015
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 27.]), 'previousTarget': array([10., 27.]), 'currentState': array([ 9.97496844, 26.02771441,  1.30569101]), 'targetState': array([10, 27], dtype=int32), 'currentDistance': 0.9726077590834706}
episode index:4796
target Thresh 32.0
target distance 7.0
model initialize at round 4796
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 12.]), 'previousTarget': array([19., 12.]), 'currentState': array([12.2377216 , 12.21216397,  0.59056252]), 'targetState': array([19, 12], dtype=int32), 'currentDistance': 6.765605861051181}
done in step count: 14
reward sum = 0.762487601760563
running average episode reward sum: 0.4956559273500488
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 12.]), 'previousTarget': array([19., 12.]), 'currentState': array([18.04019135, 11.88086048,  6.22559305]), 'targetState': array([19, 12], dtype=int32), 'currentDistance': 0.9671746797001078}
episode index:4797
target Thresh 32.0
target distance 8.0
model initialize at round 4797
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 26.]), 'previousTarget': array([ 2., 26.]), 'currentState': array([10.        , 22.        ,  0.83889878]), 'targetState': array([ 2, 26], dtype=int32), 'currentDistance': 8.944271909999161}
done in step count: 22
reward sum = 0.6438012626465739
running average episode reward sum: 0.4956868038267676
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 26.]), 'previousTarget': array([ 2., 26.]), 'currentState': array([ 2.84516757, 25.92598918,  2.69202114]), 'targetState': array([ 2, 26], dtype=int32), 'currentDistance': 0.8484019184144711}
episode index:4798
target Thresh 32.0
target distance 14.0
model initialize at round 4798
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 19.]), 'previousTarget': array([ 3., 19.]), 'currentState': array([12.86864447,  5.2112062 ,  2.15428885]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 16.956443555887162}
done in step count: 38
reward sum = 0.44771371582940195
running average episode reward sum: 0.49567680735083564
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 19.]), 'previousTarget': array([ 3., 19.]), 'currentState': array([ 3.15297101, 18.13546249,  1.99604296]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 0.8779665335557866}
episode index:4799
target Thresh 32.0
target distance 2.0
model initialize at round 4799
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 15.]), 'previousTarget': array([10., 15.]), 'currentState': array([ 8.        , 14.        ,  5.35256624]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 2.2360679774997894}
done in step count: 5
reward sum = 0.905647670946366
running average episode reward sum: 0.4957622179474181
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 15.]), 'previousTarget': array([10., 15.]), 'currentState': array([ 9.41787576, 14.01694391,  0.38461539]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 1.142483218783072}
episode index:4800
target Thresh 32.0
target distance 3.0
model initialize at round 4800
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 10.]), 'previousTarget': array([22., 10.]), 'currentState': array([19.92243446,  7.20737374,  1.68876141]), 'targetState': array([22, 10], dtype=int32), 'currentDistance': 3.4806666049548483}
done in step count: 6
reward sum = 0.8763023977248832
running average episode reward sum: 0.4958414806384777
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 10.]), 'previousTarget': array([22., 10.]), 'currentState': array([21.33340773,  9.25486998,  0.6251411 ]), 'targetState': array([22, 10], dtype=int32), 'currentDistance': 0.9997819741176612}
episode index:4801
target Thresh 32.0
target distance 18.0
model initialize at round 4801
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.02930853,  5.6240896 ]), 'previousTarget': array([23.94818637,  5.71272322]), 'currentState': array([ 9.03979121, 18.8647229 ,  5.17041659]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 92
reward sum = 0.223487283170952
running average episode reward sum: 0.49578476381268277
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([26.17078834,  3.25757171,  5.5238077 ]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 0.8682944007341424}
episode index:4802
target Thresh 32.0
target distance 9.0
model initialize at round 4802
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 13.]), 'previousTarget': array([26., 13.]), 'currentState': array([16.99935658,  8.02126301,  1.34854734]), 'targetState': array([26, 13], dtype=int32), 'currentDistance': 10.285883725858636}
done in step count: 23
reward sum = 0.6398472793614451
running average episode reward sum: 0.4958147580903318
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 13.]), 'previousTarget': array([26., 13.]), 'currentState': array([25.07888827, 12.39015534,  0.37960003]), 'targetState': array([26, 13], dtype=int32), 'currentDistance': 1.104697845270436}
episode index:4803
target Thresh 32.0
target distance 10.0
model initialize at round 4803
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 14.]), 'previousTarget': array([25., 14.]), 'currentState': array([18.        , 24.        ,  4.10127798]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 12.206555615733702}
done in step count: 28
reward sum = 0.5592516754953796
running average episode reward sum: 0.4958279631106077
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 14.]), 'previousTarget': array([25., 14.]), 'currentState': array([24.66641055, 14.84537257,  5.01676384]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 0.9088105952917941}
episode index:4804
target Thresh 32.0
target distance 16.0
model initialize at round 4804
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([16.21198219, 23.85581689,  5.63969167]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 15.956313390974898}
done in step count: 43
reward sum = 0.45808206076548463
running average episode reward sum: 0.4958201075638136
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([18.03656531,  8.86196361,  4.48632211]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 0.8627388331980019}
episode index:4805
target Thresh 32.0
target distance 11.0
model initialize at round 4805
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  2.]), 'previousTarget': array([19.,  2.]), 'currentState': array([19.790299  , 12.64341186,  4.25213499]), 'targetState': array([19,  2], dtype=int32), 'currentDistance': 10.672712328659186}
done in step count: 22
reward sum = 0.6295734892169401
running average episode reward sum: 0.4958479380635334
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  2.]), 'previousTarget': array([19.,  2.]), 'currentState': array([18.95663198,  2.86633421,  4.52521409]), 'targetState': array([19,  2], dtype=int32), 'currentDistance': 0.8674190202850975}
episode index:4806
target Thresh 32.0
target distance 10.0
model initialize at round 4806
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 13.]), 'previousTarget': array([ 2., 13.]), 'currentState': array([4.2194317 , 3.21899992, 1.02940497]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 10.02964803111417}
done in step count: 22
reward sum = 0.6357962186104632
running average episode reward sum: 0.4958770514982217
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 13.]), 'previousTarget': array([ 2., 13.]), 'currentState': array([ 2.32165071, 12.30861259,  1.62564547]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 0.7625455587368216}
episode index:4807
target Thresh 32.0
target distance 17.0
model initialize at round 4807
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.52008242, 28.32542705]), 'previousTarget': array([17.53366395, 28.33935727]), 'currentState': array([ 5.92605686, 12.02884546,  2.51791903]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.29527482225375346
running average episode reward sum: 0.4958353289047848
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 29.]), 'previousTarget': array([18., 29.]), 'currentState': array([17.05823846, 28.33081055,  0.53291934]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 1.1553048567434197}
episode index:4808
target Thresh 32.0
target distance 13.0
model initialize at round 4808
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 15.]), 'previousTarget': array([10., 15.]), 'currentState': array([2.       , 2.       , 2.2394416]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 15.264337522473747}
done in step count: 42
reward sum = 0.4633564479951494
running average episode reward sum: 0.4958285751345811
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 15.]), 'previousTarget': array([10., 15.]), 'currentState': array([ 9.59235954, 14.1565717 ,  0.87046834]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 0.9367721439687559}
episode index:4809
target Thresh 32.0
target distance 21.0
model initialize at round 4809
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.33390171, 20.98269613]), 'previousTarget': array([13.32455532, 20.97366596]), 'currentState': array([7.06305861, 1.99121049, 0.0898939 ]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.2943786290209153
running average episode reward sum: 0.4957866936489026
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([14.03951543, 22.06140382,  1.2392348 ]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 0.9394276197520385}
episode index:4810
target Thresh 32.0
target distance 20.0
model initialize at round 4810
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7.39666902, 13.91901502]), 'previousTarget': array([ 7.38838649, 13.9223227 ]), 'currentState': array([26.9924393 ,  9.91829169,  4.40570849]), 'targetState': array([ 7, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.3238218246764364
running average episode reward sum: 0.49575094954809773
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 14.]), 'previousTarget': array([ 7., 14.]), 'currentState': array([ 7.9233147, 14.0848582,  2.8003611]), 'targetState': array([ 7, 14], dtype=int32), 'currentDistance': 0.9272059953307594}
episode index:4811
target Thresh 32.0
target distance 9.0
model initialize at round 4811
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 19.]), 'previousTarget': array([24., 19.]), 'currentState': array([20.01770378, 27.68567901,  4.86334828]), 'targetState': array([24, 19], dtype=int32), 'currentDistance': 9.55508781289563}
done in step count: 20
reward sum = 0.6719288184083149
running average episode reward sum: 0.4957875617402964
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 19.]), 'previousTarget': array([24., 19.]), 'currentState': array([23.50984862, 19.67182373,  4.98549513]), 'targetState': array([24, 19], dtype=int32), 'currentDistance': 0.8316222142004979}
episode index:4812
target Thresh 32.0
target distance 3.0
model initialize at round 4812
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  8.]), 'previousTarget': array([19.,  8.]), 'currentState': array([15.91574187,  8.01671542,  2.75519893]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 3.0843034293420315}
done in step count: 10
reward sum = 0.8217073849350802
running average episode reward sum: 0.4958552783044341
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  8.]), 'previousTarget': array([19.,  8.]), 'currentState': array([18.07325583,  8.72502479,  6.14048288]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 1.1766544545702988}
episode index:4813
target Thresh 32.0
target distance 25.0
model initialize at round 4813
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.31689859, 19.2435212 ]), 'previousTarget': array([11.31390548, 19.23068683]), 'currentState': array([23.01791256,  3.0235868 ,  1.17378616]), 'targetState': array([ 5, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.170189942775583
running average episode reward sum: 0.495716922421368
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 28.]), 'previousTarget': array([ 5., 28.]), 'currentState': array([ 9.13558465, 21.68181819,  2.12957654]), 'targetState': array([ 5, 28], dtype=int32), 'currentDistance': 7.551323178701329}
episode index:4814
target Thresh 32.0
target distance 24.0
model initialize at round 4814
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6.90721487, 10.35792055]), 'previousTarget': array([ 6.90599608, 10.35899411]), 'currentState': array([18.0042472 , 26.9969071 ,  5.41894436]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 20.0}
done in step count: 85
reward sum = 0.16383218973026176
running average episode reward sum: 0.4956479951663958
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.76431158, 3.97623382, 3.86223259]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.2398405779768102}
episode index:4815
target Thresh 32.0
target distance 16.0
model initialize at round 4815
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 22.]), 'previousTarget': array([18., 22.]), 'currentState': array([16.        ,  6.        ,  4.09562528]), 'targetState': array([18, 22], dtype=int32), 'currentDistance': 16.1245154965971}
done in step count: 44
reward sum = 0.3993526055561041
running average episode reward sum: 0.49562800027652654
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 22.]), 'previousTarget': array([18., 22.]), 'currentState': array([18.42110553, 21.24924228,  1.48384525]), 'targetState': array([18, 22], dtype=int32), 'currentDistance': 0.8607944099692668}
episode index:4816
target Thresh 32.0
target distance 11.0
model initialize at round 4816
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 27.]), 'previousTarget': array([ 5., 27.]), 'currentState': array([16.04497358, 23.03540833,  0.90863723]), 'targetState': array([ 5, 27], dtype=int32), 'currentDistance': 11.73496606433322}
done in step count: 29
reward sum = 0.5520109155159889
running average episode reward sum: 0.49563970526204437
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 27.]), 'previousTarget': array([ 5., 27.]), 'currentState': array([ 5.83340088, 26.96301086,  2.77848183]), 'targetState': array([ 5, 27], dtype=int32), 'currentDistance': 0.8342213303551044}
episode index:4817
target Thresh 32.0
target distance 16.0
model initialize at round 4817
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 20.]), 'previousTarget': array([ 6., 20.]), 'currentState': array([21.85008753, 17.83010522,  3.89667884]), 'targetState': array([ 6, 20], dtype=int32), 'currentDistance': 15.997928553119658}
done in step count: 42
reward sum = 0.45742916184256477
running average episode reward sum: 0.49563177447262563
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 20.]), 'previousTarget': array([ 6., 20.]), 'currentState': array([ 6.919417  , 20.0784237 ,  3.02075142]), 'targetState': array([ 6, 20], dtype=int32), 'currentDistance': 0.9227556022866128}
episode index:4818
target Thresh 32.0
target distance 14.0
model initialize at round 4818
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 5.        , 24.        ,  0.21256176]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 17.20465053408525}
done in step count: 46
reward sum = 0.4180824360024186
running average episode reward sum: 0.4956156820595793
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.00368253, 10.4353859 ,  5.15926846]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.0872945241159655}
episode index:4819
target Thresh 32.0
target distance 20.0
model initialize at round 4819
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26.43626003, 21.58546213]), 'previousTarget': array([26.42781353, 21.56953382]), 'currentState': array([19.031949  ,  3.00654456,  0.36624333]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.3189105953615212
running average episode reward sum: 0.4955790212532104
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 23.]), 'previousTarget': array([27., 23.]), 'currentState': array([27.10435158, 22.07401309,  1.19173362]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 0.9318481638621318}
episode index:4820
target Thresh 32.0
target distance 18.0
model initialize at round 4820
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 25.]), 'previousTarget': array([24., 25.]), 'currentState': array([18.        ,  7.        ,  6.12066031]), 'targetState': array([24, 25], dtype=int32), 'currentDistance': 18.973665961010276}
done in step count: 51
reward sum = 0.36201671315306105
running average episode reward sum: 0.4955513169785578
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 25.]), 'previousTarget': array([24., 25.]), 'currentState': array([24.10740263, 24.06758562,  1.29908635]), 'targetState': array([24, 25], dtype=int32), 'currentDistance': 0.9385797298298845}
episode index:4821
target Thresh 32.0
target distance 11.0
model initialize at round 4821
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 26.]), 'previousTarget': array([ 9., 26.]), 'currentState': array([ 9.87137344, 15.21845398,  2.00568353]), 'targetState': array([ 9, 26], dtype=int32), 'currentDistance': 10.816701263379738}
done in step count: 23
reward sum = 0.6338933695054054
running average episode reward sum: 0.49558000674473923
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 26.]), 'previousTarget': array([ 9., 26.]), 'currentState': array([ 9.46537249, 25.0693299 ,  1.63468491]), 'targetState': array([ 9, 26], dtype=int32), 'currentDistance': 1.0405375479390073}
episode index:4822
target Thresh 32.0
target distance 12.0
model initialize at round 4822
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  2.]), 'previousTarget': array([27.,  2.]), 'currentState': array([15.99187213, 13.98866287,  4.33018935]), 'targetState': array([27,  2], dtype=int32), 'currentDistance': 16.27596131240071}
done in step count: 45
reward sum = 0.44077613510708485
running average episode reward sum: 0.4955686437193116
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  2.]), 'previousTarget': array([27.,  2.]), 'currentState': array([26.21627646,  2.90254241,  5.29236092]), 'targetState': array([27,  2], dtype=int32), 'currentDistance': 1.1953264785120505}
episode index:4823
target Thresh 32.0
target distance 6.0
model initialize at round 4823
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 19.]), 'previousTarget': array([21., 19.]), 'currentState': array([26.054453  , 24.82957744,  4.88883579]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 7.7156638231434735}
done in step count: 21
reward sum = 0.698856615154877
running average episode reward sum: 0.49561078467524766
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 19.]), 'previousTarget': array([21., 19.]), 'currentState': array([21.92638892, 19.00068126,  3.38961964]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 0.9263891694199211}
episode index:4824
target Thresh 32.0
target distance 5.0
model initialize at round 4824
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 13.]), 'previousTarget': array([27., 13.]), 'currentState': array([25.17027358, 17.98817582,  6.02054102]), 'targetState': array([27, 13], dtype=int32), 'currentDistance': 5.313172010012292}
done in step count: 13
reward sum = 0.7971546959056452
running average episode reward sum: 0.49567328082265294
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 13.]), 'previousTarget': array([27., 13.]), 'currentState': array([26.80637128, 13.89115781,  4.72478166]), 'targetState': array([27, 13], dtype=int32), 'currentDistance': 0.9119508372465225}
episode index:4825
target Thresh 32.0
target distance 11.0
model initialize at round 4825
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([25.60410799,  4.00354501,  3.17856576]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 10.650822914416533}
done in step count: 25
reward sum = 0.6340326812474296
running average episode reward sum: 0.49570195040417486
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.90224134,  4.96849205,  2.89485651]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.9027913283797566}
episode index:4826
target Thresh 32.0
target distance 9.0
model initialize at round 4826
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([ 4.96384702, 28.78705523,  4.79671621]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 10.73042725135914}
done in step count: 24
reward sum = 0.6008810500597361
running average episode reward sum: 0.49572374014928683
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([13.65571943, 23.93203931,  5.57713535]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 0.9935926694932599}
episode index:4827
target Thresh 32.0
target distance 5.0
model initialize at round 4827
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 13.]), 'previousTarget': array([13., 13.]), 'currentState': array([ 9.        , 18.        ,  3.01729465]), 'targetState': array([13, 13], dtype=int32), 'currentDistance': 6.403124237432848}
done in step count: 16
reward sum = 0.7100734346256291
running average episode reward sum: 0.4957681373519538
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 13.]), 'previousTarget': array([13., 13.]), 'currentState': array([12.10869686, 13.35290804,  5.60530324]), 'targetState': array([13, 13], dtype=int32), 'currentDistance': 0.9586268159263016}
episode index:4828
target Thresh 32.0
target distance 8.0
model initialize at round 4828
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 12.]), 'previousTarget': array([16., 12.]), 'currentState': array([19.54822545, 19.96957404,  3.46133876]), 'targetState': array([16, 12], dtype=int32), 'currentDistance': 8.723761476974987}
done in step count: 20
reward sum = 0.6692621171003279
running average episode reward sum: 0.49580406486898604
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 12.]), 'previousTarget': array([16., 12.]), 'currentState': array([15.78817706, 12.86741979,  4.3635808 ]), 'targetState': array([16, 12], dtype=int32), 'currentDistance': 0.8929087555195099}
episode index:4829
target Thresh 32.0
target distance 14.0
model initialize at round 4829
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 23.]), 'previousTarget': array([17., 23.]), 'currentState': array([14.19614218,  9.03649961,  0.43648308]), 'targetState': array([17, 23], dtype=int32), 'currentDistance': 14.242224608599306}
done in step count: 34
reward sum = 0.5066013155522676
running average episode reward sum: 0.4958063003246141
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 23.]), 'previousTarget': array([17., 23.]), 'currentState': array([16.93584848, 22.04872392,  1.21325515]), 'targetState': array([17, 23], dtype=int32), 'currentDistance': 0.9534367347074375}
episode index:4830
target Thresh 32.0
target distance 6.0
model initialize at round 4830
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.82995637, 8.63074189, 4.40488207]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.751022194745578}
done in step count: 11
reward sum = 0.7951683221549007
running average episode reward sum: 0.4958682672096959
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.66976713, 3.77266673, 4.84844166]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.840278303645008}
episode index:4831
target Thresh 32.0
target distance 11.0
model initialize at round 4831
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 16.]), 'previousTarget': array([20., 16.]), 'currentState': array([ 8.98440002, 10.01003364,  2.35261998]), 'targetState': array([20, 16], dtype=int32), 'currentDistance': 12.538865183203724}
done in step count: 35
reward sum = 0.5295311960083652
running average episode reward sum: 0.49587523387542415
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 16.]), 'previousTarget': array([20., 16.]), 'currentState': array([19.11883789, 15.55262456,  0.22219722]), 'targetState': array([20, 16], dtype=int32), 'currentDistance': 0.9882264109893204}
episode index:4832
target Thresh 32.0
target distance 18.0
model initialize at round 4832
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.78704435, 21.27881227]), 'previousTarget': array([20.78704435, 21.27881227]), 'currentState': array([5.        , 9.        , 2.97740054]), 'targetState': array([23, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.2506973744606762
running average episode reward sum: 0.4958245039231347
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 23.]), 'previousTarget': array([23., 23.]), 'currentState': array([22.28123739, 22.41384269,  0.49164057]), 'targetState': array([23, 23], dtype=int32), 'currentDistance': 0.9274697217504227}
episode index:4833
target Thresh 32.0
target distance 21.0
model initialize at round 4833
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 25.]), 'previousTarget': array([25., 25.]), 'currentState': array([25.        ,  5.        ,  3.75095987]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 57
reward sum = 0.2893460032043058
running average episode reward sum: 0.4957817901248892
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 26.]), 'previousTarget': array([25., 26.]), 'currentState': array([25.13750124, 25.10812914,  1.48296774]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 0.9024080096631789}
episode index:4834
target Thresh 32.0
target distance 16.0
model initialize at round 4834
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 12.]), 'previousTarget': array([ 9., 12.]), 'currentState': array([25.00190452, 11.00775592,  1.58250391]), 'targetState': array([ 9, 12], dtype=int32), 'currentDistance': 16.032638476193096}
done in step count: 50
reward sum = 0.4162537245405424
running average episode reward sum: 0.49576534171422026
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 12.]), 'previousTarget': array([ 9., 12.]), 'currentState': array([ 9.97434137, 12.40389278,  2.95863289]), 'targetState': array([ 9, 12], dtype=int32), 'currentDistance': 1.0547371608100364}
episode index:4835
target Thresh 32.0
target distance 14.0
model initialize at round 4835
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  3.]), 'previousTarget': array([24.,  3.]), 'currentState': array([16.87727279, 16.68314437,  4.40402241]), 'targetState': array([24,  3], dtype=int32), 'currentDistance': 15.426006698964041}
done in step count: 34
reward sum = 0.47876091864161097
running average episode reward sum: 0.495761825497704
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  3.]), 'previousTarget': array([24.,  3.]), 'currentState': array([23.18584678,  3.70133175,  5.14236657]), 'targetState': array([24,  3], dtype=int32), 'currentDistance': 1.0745751212426964}
episode index:4836
target Thresh 32.0
target distance 19.0
model initialize at round 4836
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 29.]), 'previousTarget': array([23., 29.]), 'currentState': array([23.07023868, 10.41815385,  1.46300998]), 'targetState': array([23, 29], dtype=int32), 'currentDistance': 18.58197890392829}
done in step count: 45
reward sum = 0.40737868178608994
running average episode reward sum: 0.49574355319178887
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 29.]), 'previousTarget': array([23., 29.]), 'currentState': array([23.14569372, 28.00979147,  1.50658856]), 'targetState': array([23, 29], dtype=int32), 'currentDistance': 1.0008694138405443}
episode index:4837
target Thresh 32.0
target distance 10.0
model initialize at round 4837
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  7.]), 'previousTarget': array([19.,  7.]), 'currentState': array([13.91595998, 16.60892268,  4.61610158]), 'targetState': array([19,  7], dtype=int32), 'currentDistance': 10.871009982371026}
done in step count: 25
reward sum = 0.6157824131415395
running average episode reward sum: 0.4957683648618901
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  7.]), 'previousTarget': array([19.,  7.]), 'currentState': array([18.03133432,  7.43096224,  5.19345504]), 'targetState': array([19,  7], dtype=int32), 'currentDistance': 1.0602083044519308}
episode index:4838
target Thresh 32.0
target distance 5.0
model initialize at round 4838
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 26.]), 'previousTarget': array([ 4., 26.]), 'currentState': array([ 7.        , 21.        ,  3.24317908]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 5.830951894845301}
done in step count: 14
reward sum = 0.7734032791729064
running average episode reward sum: 0.4958257393017146
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 26.]), 'previousTarget': array([ 4., 26.]), 'currentState': array([ 4.7661924 , 25.34545015,  2.04082653]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 1.0077134065797657}
episode index:4839
target Thresh 32.0
target distance 20.0
model initialize at round 4839
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 29.]), 'previousTarget': array([ 5.38838649, 28.9223227 ]), 'currentState': array([24.54522378, 24.99662212,  3.16154853]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 19.951010176408438}
done in step count: 47
reward sum = 0.3561677423837604
running average episode reward sum: 0.49579688434367364
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 29.]), 'previousTarget': array([ 5., 29.]), 'currentState': array([ 5.97296457, 29.02356258,  2.67863078]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 0.9732498344833191}
episode index:4840
target Thresh 32.0
target distance 16.0
model initialize at round 4840
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([26.98191717, 21.01202861,  2.76478571]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 19.998784951185133}
done in step count: 61
reward sum = 0.32232545496179266
running average episode reward sum: 0.4957610505429337
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.14853389,  5.9266877 ,  4.00742428]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.9385160685718665}
episode index:4841
target Thresh 32.0
target distance 9.0
model initialize at round 4841
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([13.        , 10.        ,  2.15646267]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 9.486832980505138}
done in step count: 21
reward sum = 0.631946500983018
running average episode reward sum: 0.49578917641043474
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([21.03236741, 12.55671065,  0.04644001]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 1.0643393591807238}
episode index:4842
target Thresh 32.0
target distance 10.0
model initialize at round 4842
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([13.        , 12.        ,  3.07660851]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 10.44030650891055}
done in step count: 24
reward sum = 0.602903881434407
running average episode reward sum: 0.49581129383868666
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.24979137,  2.97482116,  5.11172438]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.2300769433089458}
episode index:4843
target Thresh 32.0
target distance 12.0
model initialize at round 4843
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 10.]), 'previousTarget': array([17., 10.]), 'currentState': array([ 5.32332074, 13.13163327,  0.31194174]), 'targetState': array([17, 10], dtype=int32), 'currentDistance': 12.089332718575317}
done in step count: 29
reward sum = 0.5814779483772223
running average episode reward sum: 0.49582897894490846
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 10.]), 'previousTarget': array([17., 10.]), 'currentState': array([16.14272667,  9.99558122,  5.88618455]), 'targetState': array([17, 10], dtype=int32), 'currentDistance': 0.8572847146686616}
episode index:4844
target Thresh 32.0
target distance 18.0
model initialize at round 4844
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.7060726 , 18.94838661]), 'previousTarget': array([12.71272322, 18.94818637]), 'currentState': array([25.97560979,  3.98445055,  3.45958108]), 'targetState': array([10, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.2387493876926119
running average episode reward sum: 0.4957759181417604
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 22.]), 'previousTarget': array([10., 22.]), 'currentState': array([10.84350806, 21.13765013,  2.23868426]), 'targetState': array([10, 22], dtype=int32), 'currentDistance': 1.2062972918787076}
episode index:4845
target Thresh 32.0
target distance 18.0
model initialize at round 4845
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 28.]), 'previousTarget': array([20., 28.]), 'currentState': array([27.        , 10.        ,  0.63951735]), 'targetState': array([20, 28], dtype=int32), 'currentDistance': 19.313207915827967}
done in step count: 51
reward sum = 0.3601690776675439
running average episode reward sum: 0.49574793488949587
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 28.]), 'previousTarget': array([20., 28.]), 'currentState': array([20.78436955, 27.18768103,  1.87665939]), 'targetState': array([20, 28], dtype=int32), 'currentDistance': 1.1292022418805612}
episode index:4846
target Thresh 32.0
target distance 6.0
model initialize at round 4846
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 22.]), 'previousTarget': array([11., 22.]), 'currentState': array([ 5.35708282, 17.30478853,  0.62066054]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 7.34081229222965}
done in step count: 13
reward sum = 0.7515109607205402
running average episode reward sum: 0.49580070217355426
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 22.]), 'previousTarget': array([11., 22.]), 'currentState': array([10.35783432, 21.19193619,  0.70344566]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 1.0321549694950205}
episode index:4847
target Thresh 32.0
target distance 17.0
model initialize at round 4847
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 12.]), 'previousTarget': array([20., 12.]), 'currentState': array([21.98952048, 28.99982549,  3.40093708]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 17.115848170362188}
done in step count: 36
reward sum = 0.4178257818039868
running average episode reward sum: 0.4957846182378345
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 12.]), 'previousTarget': array([20., 12.]), 'currentState': array([19.98233584, 12.97536164,  4.49178849]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 0.9755215751809294}
episode index:4848
target Thresh 32.0
target distance 18.0
model initialize at round 4848
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.27134306, 27.74103725]), 'previousTarget': array([22.21358457, 27.70981108]), 'currentState': array([ 6.10444439, 15.96685417,  6.19344488]), 'targetState': array([24, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.29110558894322786
running average episode reward sum: 0.49574240767291494
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 29.]), 'previousTarget': array([24., 29.]), 'currentState': array([23.40191047, 28.11592045,  0.54081242]), 'targetState': array([24, 29], dtype=int32), 'currentDistance': 1.0673835912233938}
episode index:4849
target Thresh 32.0
target distance 6.0
model initialize at round 4849
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 23.]), 'previousTarget': array([17., 23.]), 'currentState': array([20.09207474, 17.28318484,  1.29159442]), 'targetState': array([17, 23], dtype=int32), 'currentDistance': 6.499453961873862}
done in step count: 18
reward sum = 0.7464996556312491
running average episode reward sum: 0.4957941101982672
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 23.]), 'previousTarget': array([17., 23.]), 'currentState': array([17.95260164, 22.72535306,  2.41738388]), 'targetState': array([17, 23], dtype=int32), 'currentDistance': 0.9914034679690316}
episode index:4850
target Thresh 32.0
target distance 17.0
model initialize at round 4850
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([19.       , 26.       ,  5.6112285]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 18.384776310850235}
done in step count: 61
reward sum = 0.360064234870793
running average episode reward sum: 0.49576613042598777
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([12.27901045,  9.96881354,  4.27322166]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 1.008189719885641}
episode index:4851
target Thresh 32.0
target distance 19.0
model initialize at round 4851
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 25.]), 'previousTarget': array([25., 25.]), 'currentState': array([ 5.9893048 , 21.00390399,  2.54537064]), 'targetState': array([25, 25], dtype=int32), 'currentDistance': 19.426150291595533}
done in step count: 50
reward sum = 0.32558040094595536
running average episode reward sum: 0.49573105504893084
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 25.]), 'previousTarget': array([25., 25.]), 'currentState': array([24.20813835, 24.82820806,  6.27111505]), 'targetState': array([25, 25], dtype=int32), 'currentDistance': 0.8102822590626505}
episode index:4852
target Thresh 32.0
target distance 10.0
model initialize at round 4852
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([23.97524642, 13.02952866,  2.5209527 ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 13.455033592784973}
done in step count: 29
reward sum = 0.5110630707321081
running average episode reward sum: 0.4957342143350803
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.33287708,  4.79504173,  3.91134149]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.8619155992310207}
episode index:4853
target Thresh 32.0
target distance 9.0
model initialize at round 4853
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 23.]), 'previousTarget': array([26., 23.]), 'currentState': array([23.1579512 , 14.0624214 ,  0.48261741]), 'targetState': array([26, 23], dtype=int32), 'currentDistance': 9.378568790180585}
done in step count: 21
reward sum = 0.6647147272776848
running average episode reward sum: 0.49576902696650654
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 23.]), 'previousTarget': array([26., 23.]), 'currentState': array([26.0573073 , 22.21288299,  1.2393165 ]), 'targetState': array([26, 23], dtype=int32), 'currentDistance': 0.7892004292470014}
episode index:4854
target Thresh 32.0
target distance 21.0
model initialize at round 4854
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.19707278,  5.04456904]), 'previousTarget': array([21.97736275,  5.04869701]), 'currentState': array([2.22781336, 6.15302663, 0.47104952]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.3385249887964187
running average episode reward sum: 0.4957366389050915
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  5.]), 'previousTarget': array([23.,  5.]), 'currentState': array([22.09630232,  5.34812626,  6.20924869]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 0.9684324366013708}
episode index:4855
target Thresh 32.0
target distance 16.0
model initialize at round 4855
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 24.]), 'previousTarget': array([ 6., 24.]), 'currentState': array([21.91751041, 20.07598994,  2.55227047]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 16.394053574592313}
done in step count: 48
reward sum = 0.4352134513981646
running average episode reward sum: 0.495724175316231
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 24.]), 'previousTarget': array([ 6., 24.]), 'currentState': array([ 6.9525287 , 23.88858915,  2.92916644]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 0.9590220539526657}
episode index:4856
target Thresh 32.0
target distance 6.0
model initialize at round 4856
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'previousTarget': array([26., 11.]), 'currentState': array([20.        ,  6.        ,  5.87220478]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 7.810249675906654}
done in step count: 19
reward sum = 0.7097429942194701
running average episode reward sum: 0.4957682393102403
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'previousTarget': array([26., 11.]), 'currentState': array([25.36771851, 10.07460391,  0.7686363 ]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 1.1207755412927214}
episode index:4857
target Thresh 32.0
target distance 13.0
model initialize at round 4857
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 27.]), 'previousTarget': array([16., 27.]), 'currentState': array([ 3.0320727 , 28.08518631,  1.01251981]), 'targetState': array([16, 27], dtype=int32), 'currentDistance': 13.013253548323139}
done in step count: 31
reward sum = 0.5348079544235476
running average episode reward sum: 0.4957762754804983
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 27.]), 'previousTarget': array([16., 27.]), 'currentState': array([15.16218443, 26.6696291 ,  6.23922417]), 'targetState': array([16, 27], dtype=int32), 'currentDistance': 0.9005997192628564}
episode index:4858
target Thresh 32.0
target distance 7.0
model initialize at round 4858
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'previousTarget': array([10., 17.]), 'currentState': array([17.        , 14.        ,  1.43842608]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 7.6157731058639095}
done in step count: 18
reward sum = 0.7076940989300577
running average episode reward sum: 0.4958198889448839
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'previousTarget': array([10., 17.]), 'currentState': array([10.98527896, 16.73686384,  2.79819835]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 1.019811389085469}
episode index:4859
target Thresh 32.0
target distance 13.0
model initialize at round 4859
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  4.]), 'previousTarget': array([24.,  4.]), 'currentState': array([11.24758835, 14.03728041,  0.05421694]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 16.228709154978045}
done in step count: 33
reward sum = 0.4683345825920546
running average episode reward sum: 0.49581423353205406
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  4.]), 'previousTarget': array([24.,  4.]), 'currentState': array([23.06888297,  4.6880053 ,  5.54998145]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 1.1577263144785828}
episode index:4860
target Thresh 32.0
target distance 6.0
model initialize at round 4860
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([12.        , 29.        ,  2.07219464]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 6.708203932499369}
done in step count: 19
reward sum = 0.6765864147594547
running average episode reward sum: 0.4958514218022099
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([ 8.33721632, 23.84301721,  5.1108413 ]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 1.072361980441746}
episode index:4861
target Thresh 32.0
target distance 20.0
model initialize at round 4861
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.76156733, 26.2157367 ]), 'previousTarget': array([19.74695771, 26.1565257 ]), 'currentState': array([13.9440554 ,  7.08051904,  2.01074034]), 'targetState': array([20, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.32514113826361474
running average episode reward sum: 0.4958163106784874
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 27.]), 'previousTarget': array([20., 27.]), 'currentState': array([19.6128692 , 26.08708272,  1.13020087]), 'targetState': array([20, 27], dtype=int32), 'currentDistance': 0.9916089045930865}
episode index:4862
target Thresh 32.0
target distance 14.0
model initialize at round 4862
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'previousTarget': array([22., 18.]), 'currentState': array([ 8.18114525, 11.02624639,  0.23096364]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 15.478823794457426}
done in step count: 39
reward sum = 0.48926816785044075
running average episode reward sum: 0.4958149641551833
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'previousTarget': array([22., 18.]), 'currentState': array([21.17481376, 17.67263285,  0.49822778]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 0.8877508591691997}
episode index:4863
target Thresh 32.0
target distance 9.0
model initialize at round 4863
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([18.63656052, 13.84759711,  3.63934474]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 11.021785871272407}
done in step count: 21
reward sum = 0.6251212726873833
running average episode reward sum: 0.49584154851137824
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([10.5989701 ,  7.86360725,  3.96551486]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 1.050991278282524}
episode index:4864
target Thresh 32.0
target distance 8.0
model initialize at round 4864
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 12.]), 'previousTarget': array([23., 12.]), 'currentState': array([23.92442747,  4.01244899,  2.81417307]), 'targetState': array([23, 12], dtype=int32), 'currentDistance': 8.040866700356355}
done in step count: 20
reward sum = 0.6921247171825046
running average episode reward sum: 0.4958818944864391
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 12.]), 'previousTarget': array([23., 12.]), 'currentState': array([22.64334183, 11.01448747,  1.30670116]), 'targetState': array([23, 12], dtype=int32), 'currentDistance': 1.048064879833244}
episode index:4865
target Thresh 32.0
target distance 21.0
model initialize at round 4865
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.49678415, 22.24464071]), 'previousTarget': array([12.49442256, 22.23047895]), 'currentState': array([6.98530545, 3.01904312, 2.06218946]), 'targetState': array([13, 24], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.312584939925714
running average episode reward sum: 0.4958442255685269
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 24.]), 'previousTarget': array([13., 24.]), 'currentState': array([12.8093613 , 23.03037099,  1.14757815]), 'targetState': array([13, 24], dtype=int32), 'currentDistance': 0.9881920517539173}
episode index:4866
target Thresh 32.0
target distance 9.0
model initialize at round 4866
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 26.]), 'previousTarget': array([15., 26.]), 'currentState': array([23.00066629, 16.99985412,  0.03695696]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 12.042146275873051}
done in step count: 28
reward sum = 0.5349483392209565
running average episode reward sum: 0.4958522601100623
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 26.]), 'previousTarget': array([15., 26.]), 'currentState': array([15.88715708, 25.56552331,  2.44554533]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 0.987834840979051}
episode index:4867
target Thresh 32.0
target distance 9.0
model initialize at round 4867
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([11.8843896 ,  9.11067661,  2.54896542]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 9.789281932860389}
done in step count: 22
reward sum = 0.6502605076897849
running average episode reward sum: 0.495883979142022
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.89497851, 5.68159285, 3.79919321]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.1249690405449246}
episode index:4868
target Thresh 32.0
target distance 2.0
model initialize at round 4868
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 23.]), 'previousTarget': array([17., 23.]), 'currentState': array([15.31561987, 23.76786895,  5.70964767]), 'targetState': array([17, 23], dtype=int32), 'currentDistance': 1.8511507574338366}
done in step count: 3
reward sum = 0.9500644160875541
running average episode reward sum: 0.49597725916604035
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 23.]), 'previousTarget': array([17., 23.]), 'currentState': array([16.18999737, 23.30383225,  5.83333222]), 'targetState': array([17, 23], dtype=int32), 'currentDistance': 0.865111724824771}
episode index:4869
target Thresh 32.0
target distance 19.0
model initialize at round 4869
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'previousTarget': array([26., 11.]), 'currentState': array([ 7.06084837, 11.93358411,  5.66411209]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 18.962147661582804}
done in step count: 43
reward sum = 0.3769948852486866
running average episode reward sum: 0.495952827467084
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'previousTarget': array([26., 11.]), 'currentState': array([25.01849653, 10.614188  ,  6.25805557]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 1.0546089164627561}
episode index:4870
target Thresh 32.0
target distance 12.0
model initialize at round 4870
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([16.        , 18.        ,  3.13797572]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 13.000000000000002}
done in step count: 33
reward sum = 0.5148359865393002
running average episode reward sum: 0.49595670411645215
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([20.42433176,  6.84632612,  5.16109707]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 1.0235535254412726}
episode index:4871
target Thresh 32.0
target distance 7.0
model initialize at round 4871
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 25.]), 'previousTarget': array([15., 25.]), 'currentState': array([21.09363309, 18.25615438,  1.35252297]), 'targetState': array([15, 25], dtype=int32), 'currentDistance': 9.089104353756362}
done in step count: 20
reward sum = 0.6681910773190356
running average episode reward sum: 0.4959920559992934
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 25.]), 'previousTarget': array([15., 25.]), 'currentState': array([15.86263983, 24.50010091,  2.63489798]), 'targetState': array([15, 25], dtype=int32), 'currentDistance': 0.997018842227649}
episode index:4872
target Thresh 32.0
target distance 7.0
model initialize at round 4872
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 20.]), 'previousTarget': array([19., 20.]), 'currentState': array([25.72838526, 27.07898283,  3.0517025 ]), 'targetState': array([19, 20], dtype=int32), 'currentDistance': 9.766430574640465}
done in step count: 22
reward sum = 0.6436369972428398
running average episode reward sum: 0.49602235457127036
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 20.]), 'previousTarget': array([19., 20.]), 'currentState': array([19.35849993, 20.98572558,  3.95691744]), 'targetState': array([19, 20], dtype=int32), 'currentDistance': 1.0488932821930765}
episode index:4873
target Thresh 32.0
target distance 25.0
model initialize at round 4873
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8.40509555, 23.93630557]), 'previousTarget': array([ 8.40509555, 23.93630557]), 'currentState': array([10.        ,  4.        ,  5.29815382]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 71
reward sum = 0.1833370436290534
running average episode reward sum: 0.49595820083492603
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 29.]), 'previousTarget': array([ 8., 29.]), 'currentState': array([ 8.31280256, 28.204152  ,  1.90645553]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 0.8551137220989097}
episode index:4874
target Thresh 32.0
target distance 6.0
model initialize at round 4874
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  5.]), 'previousTarget': array([21.,  5.]), 'currentState': array([26.81635284,  1.86252526,  3.62275934]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 6.608608632678561}
done in step count: 14
reward sum = 0.7606620305810649
running average episode reward sum: 0.49601249905641237
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21.,  5.]), 'previousTarget': array([21.,  5.]), 'currentState': array([21.80483347,  4.38606792,  2.50956076]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 1.012259609549288}
episode index:4875
target Thresh 32.0
target distance 7.0
model initialize at round 4875
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 15.]), 'previousTarget': array([ 7., 15.]), 'currentState': array([13.86992576, 11.35568704,  2.0108393 ]), 'targetState': array([ 7, 15], dtype=int32), 'currentDistance': 7.776689331910074}
done in step count: 16
reward sum = 0.7258388153528912
running average episode reward sum: 0.49605963324761343
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 15.]), 'previousTarget': array([ 7., 15.]), 'currentState': array([ 7.9391688 , 14.57765589,  2.71585328]), 'targetState': array([ 7, 15], dtype=int32), 'currentDistance': 1.029763361976264}
episode index:4876
target Thresh 32.0
target distance 18.0
model initialize at round 4876
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'previousTarget': array([27.,  8.]), 'currentState': array([ 9.        , 16.        ,  3.63825312]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 19.697715603592208}
done in step count: 56
reward sum = 0.3158541087998554
running average episode reward sum: 0.4960226831708352
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'previousTarget': array([27.,  8.]), 'currentState': array([26.15905656,  8.30461749,  5.90357572]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 0.894414712500091}
episode index:4877
target Thresh 32.0
target distance 10.0
model initialize at round 4877
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 16.]), 'previousTarget': array([ 9., 16.]), 'currentState': array([17.       ,  6.       ,  4.0092206]), 'targetState': array([ 9, 16], dtype=int32), 'currentDistance': 12.806248474865697}
done in step count: 30
reward sum = 0.5341494038903566
running average episode reward sum: 0.49603049922674325
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 16.]), 'previousTarget': array([ 9., 16.]), 'currentState': array([ 9.6898011 , 15.10860516,  2.32358074]), 'targetState': array([ 9, 16], dtype=int32), 'currentDistance': 1.1271248047038265}
episode index:4878
target Thresh 32.0
target distance 10.0
model initialize at round 4878
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 12.]), 'previousTarget': array([13., 12.]), 'currentState': array([2.99390835, 2.98416326, 4.58398032]), 'targetState': array([13, 12], dtype=int32), 'currentDistance': 13.46874835290502}
done in step count: 35
reward sum = 0.47964934290884664
running average episode reward sum: 0.49602714174440715
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 12.]), 'previousTarget': array([13., 12.]), 'currentState': array([12.19773692, 11.1334703 ,  0.82308526]), 'targetState': array([13, 12], dtype=int32), 'currentDistance': 1.1808893970975864}
episode index:4879
target Thresh 32.0
target distance 14.0
model initialize at round 4879
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 28.]), 'previousTarget': array([ 8., 28.]), 'currentState': array([10.        , 14.        ,  3.05671689]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 14.142135623730951}
done in step count: 34
reward sum = 0.49347644266150115
running average episode reward sum: 0.49602661906016887
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 28.]), 'previousTarget': array([ 8., 28.]), 'currentState': array([ 8.39687436, 27.08477766,  1.72419465]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 0.99756763852512}
episode index:4880
target Thresh 32.0
target distance 5.0
model initialize at round 4880
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([18.59931799,  5.13947317,  2.87563846]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 4.738367347949466}
done in step count: 9
reward sum = 0.839290247823705
running average episode reward sum: 0.49609694555653505
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.89282022,  4.51984471,  3.47752211]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.0331342961124572}
episode index:4881
target Thresh 32.0
target distance 5.0
model initialize at round 4881
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 13.]), 'previousTarget': array([13., 13.]), 'currentState': array([17.56800529, 15.92165911,  3.42841715]), 'targetState': array([13, 13], dtype=int32), 'currentDistance': 5.42243158647729}
done in step count: 9
reward sum = 0.8290917795668257
running average episode reward sum: 0.49616515424846663
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 13.]), 'previousTarget': array([13., 13.]), 'currentState': array([13.91214274, 13.96766477,  3.68740734]), 'targetState': array([13, 13], dtype=int32), 'currentDistance': 1.329804301493728}
episode index:4882
target Thresh 32.0
target distance 13.0
model initialize at round 4882
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([10.01375556, 16.01282326,  0.49783552]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 13.935429762832298}
done in step count: 36
reward sum = 0.49492710994857103
running average episode reward sum: 0.49616490070673
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.31906493,  3.87407972,  5.04752412]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.10801079325897}
episode index:4883
target Thresh 32.0
target distance 11.0
model initialize at round 4883
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 27.]), 'previousTarget': array([ 5., 27.]), 'currentState': array([14.84351298, 16.21874789,  2.26872538]), 'targetState': array([ 5, 27], dtype=int32), 'currentDistance': 14.59897752522192}
done in step count: 31
reward sum = 0.5290119024951402
running average episode reward sum: 0.49617162613707166
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5., 27.]), 'previousTarget': array([ 5., 27.]), 'currentState': array([ 5.98496111, 26.19225595,  2.21631417]), 'targetState': array([ 5, 27], dtype=int32), 'currentDistance': 1.2738127187661774}
episode index:4884
target Thresh 32.0
target distance 5.0
model initialize at round 4884
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'previousTarget': array([11., 19.]), 'currentState': array([ 9.8054027 , 14.25531209,  2.01895753]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 4.892762625183001}
done in step count: 10
reward sum = 0.8120545934938675
running average episode reward sum: 0.4962362899993761
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'previousTarget': array([11., 19.]), 'currentState': array([10.97253748, 18.20265388,  1.20433399]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 0.7978189203095125}
episode index:4885
target Thresh 32.0
target distance 16.0
model initialize at round 4885
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  8.]), 'previousTarget': array([17.,  8.]), 'currentState': array([25.7880359 , 23.85822514,  3.77258565]), 'targetState': array([17,  8], dtype=int32), 'currentDistance': 18.13044068307408}
done in step count: 49
reward sum = 0.4081421294363112
running average episode reward sum: 0.4962182600852207
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  8.]), 'previousTarget': array([17.,  8.]), 'currentState': array([17.37342824,  8.92006191,  4.43112662]), 'targetState': array([17,  8], dtype=int32), 'currentDistance': 0.9929564827185403}
episode index:4886
target Thresh 32.0
target distance 2.0
model initialize at round 4886
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 29.]), 'previousTarget': array([ 6., 29.]), 'currentState': array([ 4.06609938, 28.24982827,  1.18359226]), 'targetState': array([ 6, 29], dtype=int32), 'currentDistance': 2.0743021095402607}
done in step count: 5
reward sum = 0.9242563795811457
running average episode reward sum: 0.49630584717740317
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 29.]), 'previousTarget': array([ 6., 29.]), 'currentState': array([ 5.00617736, 29.04219622,  0.2120451 ]), 'targetState': array([ 6, 29], dtype=int32), 'currentDistance': 0.994718026760394}
episode index:4887
target Thresh 32.0
target distance 16.0
model initialize at round 4887
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 28.]), 'previousTarget': array([19., 28.]), 'currentState': array([ 7.27030174, 12.42053629,  1.03201742]), 'targetState': array([19, 28], dtype=int32), 'currentDistance': 19.501423295370294}
done in step count: 49
reward sum = 0.37170694012894584
running average episode reward sum: 0.4962803564026388
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 28.]), 'previousTarget': array([19., 28.]), 'currentState': array([18.41050021, 27.0545008 ,  0.90270709]), 'targetState': array([19, 28], dtype=int32), 'currentDistance': 1.1142166486386393}
episode index:4888
target Thresh 32.0
target distance 11.0
model initialize at round 4888
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 5.]), 'previousTarget': array([8., 5.]), 'currentState': array([10.        , 16.        ,  3.41658592]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 11.180339887498949}
done in step count: 27
reward sum = 0.5852662347117035
running average episode reward sum: 0.49629855764590103
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 5.]), 'previousTarget': array([8., 5.]), 'currentState': array([8.01778031, 5.92742756, 4.52231494]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 0.927597986687054}
episode index:4889
target Thresh 32.0
target distance 24.0
model initialize at round 4889
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.82879796, 11.58958267]), 'previousTarget': array([15.80368799, 11.63557441]), 'currentState': array([ 3.01117391, 26.94238417,  4.96766353]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.3423764792605152
running average episode reward sum: 0.4961270494583946
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([21.98427253,  3.27099912,  5.74790906]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 1.0512577323376138}
episode index:4890
target Thresh 32.0
target distance 21.0
model initialize at round 4890
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23.82842712, 22.79898987]), 'previousTarget': array([23.82842712, 22.79898987]), 'currentState': array([21.        ,  3.        ,  5.46765864]), 'targetState': array([24, 24], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.28547379742688944
running average episode reward sum: 0.4960839798914284
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 24.]), 'previousTarget': array([24., 24.]), 'currentState': array([24.10896803, 23.2193909 ,  1.41388385]), 'targetState': array([24, 24], dtype=int32), 'currentDistance': 0.7881780198014374}
episode index:4891
target Thresh 32.0
target distance 9.0
model initialize at round 4891
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 22.]), 'previousTarget': array([ 8., 22.]), 'currentState': array([13.76592492, 13.37485477,  2.11783647]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 10.374922670358133}
done in step count: 21
reward sum = 0.6454942447767796
running average episode reward sum: 0.49611452164631087
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 22.]), 'previousTarget': array([ 8., 22.]), 'currentState': array([ 8.4316592 , 21.22899704,  2.19821473]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 0.8836148632683128}
episode index:4892
target Thresh 32.0
target distance 15.0
model initialize at round 4892
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 19.]), 'previousTarget': array([ 2., 19.]), 'currentState': array([17.        , 10.        ,  1.01254582]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 17.492855684535897}
done in step count: 39
reward sum = 0.3970727925986143
running average episode reward sum: 0.49609428013209717
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 19.]), 'previousTarget': array([ 2., 19.]), 'currentState': array([ 2.82496511, 18.3003547 ,  2.36459068]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 1.0816981885647008}
episode index:4893
target Thresh 32.0
target distance 6.0
model initialize at round 4893
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 14.]), 'previousTarget': array([18., 14.]), 'currentState': array([24.00035231, 18.00081285,  1.41430575]), 'targetState': array([18, 14], dtype=int32), 'currentDistance': 7.2118465957674225}
done in step count: 17
reward sum = 0.6808252124445813
running average episode reward sum: 0.49613202654245936
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 14.]), 'previousTarget': array([18., 14.]), 'currentState': array([18.60010409, 14.85467118,  4.00641175]), 'targetState': array([18, 14], dtype=int32), 'currentDistance': 1.0443120890751467}
episode index:4894
target Thresh 32.0
target distance 13.0
model initialize at round 4894
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 27.]), 'previousTarget': array([11., 27.]), 'currentState': array([ 4.26551426, 14.09619346,  0.50041276]), 'targetState': array([11, 27], dtype=int32), 'currentDistance': 14.555463628195238}
done in step count: 30
reward sum = 0.5109634653922217
running average episode reward sum: 0.49613505645846545
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 27.]), 'previousTarget': array([11., 27.]), 'currentState': array([10.6770898 , 26.08612177,  1.1430068 ]), 'targetState': array([11, 27], dtype=int32), 'currentDistance': 0.9692494090118391}
episode index:4895
target Thresh 32.0
target distance 14.0
model initialize at round 4895
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 14.]), 'previousTarget': array([ 2., 14.]), 'currentState': array([ 7.        , 28.        ,  0.48999223]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 14.866068747318506}
done in step count: 38
reward sum = 0.44441247654187893
running average episode reward sum: 0.4961244922060315
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 14.]), 'previousTarget': array([ 2., 14.]), 'currentState': array([ 2.08455276, 14.74905031,  4.39864645]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 0.7538073552505316}
episode index:4896
target Thresh 32.0
target distance 3.0
model initialize at round 4896
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 10.]), 'previousTarget': array([23., 10.]), 'currentState': array([22.        ,  7.        ,  0.16482454]), 'targetState': array([23, 10], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 9
reward sum = 0.8639612836507273
running average episode reward sum: 0.49619960692758447
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 10.]), 'previousTarget': array([23., 10.]), 'currentState': array([23.21745156,  9.07371581,  1.80395533]), 'targetState': array([23, 10], dtype=int32), 'currentDistance': 0.9514660186911212}
episode index:4897
target Thresh 32.0
target distance 13.0
model initialize at round 4897
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 22.]), 'previousTarget': array([18., 22.]), 'currentState': array([ 5.        , 13.        ,  3.10043502]), 'targetState': array([18, 22], dtype=int32), 'currentDistance': 15.811388300841896}
done in step count: 42
reward sum = 0.4313524012902883
running average episode reward sum: 0.4961863674000963
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 22.]), 'previousTarget': array([18., 22.]), 'currentState': array([17.03450764, 21.00085369,  0.55668014]), 'targetState': array([18, 22], dtype=int32), 'currentDistance': 1.3894131268983172}
episode index:4898
target Thresh 32.0
target distance 23.0
model initialize at round 4898
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2.75488788, 7.39621665]), 'previousTarget': array([2.75140711, 7.45647272]), 'currentState': array([ 7.09444512, 26.91974714,  5.35141882]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.253426149274339
running average episode reward sum: 0.49613681438557783
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.00433424, 4.7253081 , 4.46699074]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.7253210486604448}
episode index:4899
target Thresh 32.0
target distance 16.0
model initialize at round 4899
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 26.]), 'previousTarget': array([10., 26.]), 'currentState': array([18.00374314, 10.00305076,  0.93633685]), 'targetState': array([10, 26], dtype=int32), 'currentDistance': 17.887489741673072}
done in step count: 40
reward sum = 0.40595422669233006
running average episode reward sum: 0.4961184097758445
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 26.]), 'previousTarget': array([10., 26.]), 'currentState': array([10.79125378, 25.17457218,  2.22024954]), 'targetState': array([10, 26], dtype=int32), 'currentDistance': 1.1434218964098013}
episode index:4900
target Thresh 32.0
target distance 23.0
model initialize at round 4900
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.67156076,  8.83060467]), 'previousTarget': array([11.62910995,  8.95731557]), 'currentState': array([ 2.98739974, 26.84685698,  4.74638   ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.23637925243279995
running average episode reward sum: 0.4960654126003001
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.09423492,  4.97976508,  4.95271032]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.334297567695924}
episode index:4901
target Thresh 32.0
target distance 4.0
model initialize at round 4901
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([19.01024201,  3.98897025,  5.65741614]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 4.136296477178498}
done in step count: 13
reward sum = 0.7683240867204688
running average episode reward sum: 0.49612095292549807
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([18.85076978,  7.71120818,  2.40427785]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 0.8984486253545871}
episode index:4902
target Thresh 32.0
target distance 14.0
model initialize at round 4902
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 29.]), 'previousTarget': array([ 4., 29.]), 'currentState': array([ 2.99469151, 14.99350238,  3.77487326]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 14.042528998387004}
done in step count: 35
reward sum = 0.4778649015319383
running average episode reward sum: 0.49611722948038417
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 29.]), 'previousTarget': array([ 4., 29.]), 'currentState': array([ 4.047363  , 28.00476785,  1.45198865]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 0.9963585082076909}
episode index:4903
target Thresh 32.0
target distance 14.0
model initialize at round 4903
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 26.]), 'previousTarget': array([17., 26.]), 'currentState': array([ 6.14648627, 12.37745625,  1.18325181]), 'targetState': array([17, 26], dtype=int32), 'currentDistance': 17.417590490924255}
done in step count: 38
reward sum = 0.43083796076551395
running average episode reward sum: 0.49610391804712256
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 26.]), 'previousTarget': array([17., 26.]), 'currentState': array([16.52564442, 25.23096348,  1.00674675]), 'targetState': array([17, 26], dtype=int32), 'currentDistance': 0.903565371427216}
episode index:4904
target Thresh 32.0
target distance 4.0
model initialize at round 4904
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  4.]), 'previousTarget': array([20.,  4.]), 'currentState': array([16.28867895,  2.3419168 ,  0.78632143]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 4.064867010781385}
done in step count: 8
reward sum = 0.8612430031762609
running average episode reward sum: 0.496178360266313
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.,  4.]), 'previousTarget': array([20.,  4.]), 'currentState': array([19.19296107,  3.54535733,  0.36290589]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 0.9262892605144344}
episode index:4905
target Thresh 32.0
target distance 11.0
model initialize at round 4905
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  2.]), 'previousTarget': array([13.,  2.]), 'currentState': array([24.02062708,  9.98763689,  5.49930367]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 13.610898731270755}
done in step count: 33
reward sum = 0.5127769649933694
running average episode reward sum: 0.49618174359381545
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  2.]), 'previousTarget': array([13.,  2.]), 'currentState': array([13.87770853,  2.87737844,  3.68001169]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 1.2410339141448095}
episode index:4906
target Thresh 32.0
target distance 15.0
model initialize at round 4906
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 8.]), 'previousTarget': array([7., 8.]), 'currentState': array([12.00944594, 22.80018984,  4.65586969]), 'targetState': array([7, 8], dtype=int32), 'currentDistance': 15.624985376544547}
done in step count: 33
reward sum = 0.48704344087285334
running average episode reward sum: 0.496179881294504
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7., 8.]), 'previousTarget': array([7., 8.]), 'currentState': array([7.18497406, 8.82592694, 4.46879805]), 'targetState': array([7, 8], dtype=int32), 'currentDistance': 0.8463868611690165}
episode index:4907
target Thresh 32.0
target distance 6.0
model initialize at round 4907
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 24.]), 'previousTarget': array([18., 24.]), 'currentState': array([16.00641524, 17.99723652,  6.12893867]), 'targetState': array([18, 24], dtype=int32), 'currentDistance': 6.325152144202123}
done in step count: 17
reward sum = 0.7379997862895454
running average episode reward sum: 0.49622915185379396
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18., 24.]), 'previousTarget': array([18., 24.]), 'currentState': array([18.13975757, 23.011545  ,  1.53113724]), 'targetState': array([18, 24], dtype=int32), 'currentDistance': 0.998286263064347}
episode index:4908
target Thresh 32.0
target distance 19.0
model initialize at round 4908
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 14.]), 'previousTarget': array([21., 14.]), 'currentState': array([ 2.        , 20.        ,  1.60728955]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 19.92485884517128}
done in step count: 52
reward sum = 0.3258750463532634
running average episode reward sum: 0.49619444944892527
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 14.]), 'previousTarget': array([21., 14.]), 'currentState': array([20.01246038, 14.23868662,  5.93175975]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 1.0159752965305293}
episode index:4909
target Thresh 32.0
target distance 16.0
model initialize at round 4909
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([11.28637114, 27.59013792,  5.31470007]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 18.368641377556138}
done in step count: 38
reward sum = 0.40684452505677726
running average episode reward sum: 0.4961762519083159
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([20.23678424, 12.9064076 ,  5.25044457]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 1.1849358748729584}
episode index:4910
target Thresh 32.0
target distance 22.0
model initialize at round 4910
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.29527642,  7.26234812]), 'previousTarget': array([24.29527642,  7.26234812]), 'currentState': array([5.        , 2.        , 4.49495485]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.2510437578928185
running average episode reward sum: 0.49612633692277003
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'previousTarget': array([27.,  8.]), 'currentState': array([26.14656765,  7.69917096,  0.39529713]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 0.904900481864375}
episode index:4911
target Thresh 32.0
target distance 18.0
model initialize at round 4911
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3.29018892, 26.21358457]), 'previousTarget': array([ 3.29018892, 26.21358457]), 'currentState': array([15.        , 10.        ,  3.91570571]), 'targetState': array([ 2, 28], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 55
reward sum = 0.27584519896179066
running average episode reward sum: 0.49608149141422747
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 28.]), 'previousTarget': array([ 2., 28.]), 'currentState': array([ 2.66393739, 27.19141573,  2.29900345]), 'targetState': array([ 2, 28], dtype=int32), 'currentDistance': 1.0462415491783743}
episode index:4912
target Thresh 32.0
target distance 16.0
model initialize at round 4912
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  3.]), 'previousTarget': array([10.,  3.]), 'currentState': array([25.99083587, 12.01009361,  2.54161561]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 18.354525834664265}
done in step count: 45
reward sum = 0.3868686303348024
running average episode reward sum: 0.496059262051093
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10.,  3.]), 'previousTarget': array([10.,  3.]), 'currentState': array([10.84313217,  3.48323537,  3.79903033]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 0.9717964169443609}
episode index:4913
target Thresh 32.0
target distance 2.0
model initialize at round 4913
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 24.]), 'previousTarget': array([11., 24.]), 'currentState': array([ 9.        , 25.        ,  4.58323097]), 'targetState': array([11, 24], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 5
reward sum = 0.9002907032286209
running average episode reward sum: 0.49614152323163385
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 24.]), 'previousTarget': array([11., 24.]), 'currentState': array([10.15561054, 24.07348491,  6.0044686 ]), 'targetState': array([11, 24], dtype=int32), 'currentDistance': 0.8475810220061927}
episode index:4914
target Thresh 32.0
target distance 4.0
model initialize at round 4914
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 14.]), 'previousTarget': array([ 6., 14.]), 'currentState': array([10.09030909, 14.96315454,  5.8419951 ]), 'targetState': array([ 6, 14], dtype=int32), 'currentDistance': 4.202177427045734}
done in step count: 17
reward sum = 0.7601833978663245
running average episode reward sum: 0.49619524487448935
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 14.]), 'previousTarget': array([ 6., 14.]), 'currentState': array([ 6.84716968, 13.74255743,  2.95036916]), 'targetState': array([ 6, 14], dtype=int32), 'currentDistance': 0.8854225835525887}
episode index:4915
target Thresh 32.0
target distance 18.0
model initialize at round 4915
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 10.]), 'previousTarget': array([23., 10.]), 'currentState': array([ 5.        , 16.        ,  3.84828779]), 'targetState': array([23, 10], dtype=int32), 'currentDistance': 18.973665961010276}
done in step count: 45
reward sum = 0.3422788154147173
running average episode reward sum: 0.49616393559266275
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([23., 10.]), 'previousTarget': array([23., 10.]), 'currentState': array([22.22115217, 10.08004686,  6.04082301]), 'targetState': array([23, 10], dtype=int32), 'currentDistance': 0.7829504701164254}
episode index:4916
target Thresh 32.0
target distance 19.0
model initialize at round 4916
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 7.90632547, 28.52072359,  4.50612897]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 18.928195399077374}
done in step count: 45
reward sum = 0.39402800347499756
running average episode reward sum: 0.4961431635910118
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.23667247, 10.9827465 ,  4.51265041]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.0108434826064914}
episode index:4917
target Thresh 32.0
target distance 9.0
model initialize at round 4917
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.28142519, 12.73811669,  5.42407095]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 8.767612732805974}
done in step count: 17
reward sum = 0.6889036952092835
running average episode reward sum: 0.4961823584937402
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.03145898,  4.80871723,  4.79534772]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.809328875007566}
episode index:4918
target Thresh 32.0
target distance 5.0
model initialize at round 4918
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 15.]), 'previousTarget': array([ 7., 15.]), 'currentState': array([11.98465057, 16.05869879,  2.07906401]), 'targetState': array([ 7, 15], dtype=int32), 'currentDistance': 5.095839917021727}
done in step count: 12
reward sum = 0.7887558514903797
running average episode reward sum: 0.4962418367399278
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 15.]), 'previousTarget': array([ 7., 15.]), 'currentState': array([ 7.71224325, 15.30076267,  3.56873451]), 'targetState': array([ 7, 15], dtype=int32), 'currentDistance': 0.7731420487510653}
episode index:4919
target Thresh 32.0
target distance 2.0
model initialize at round 4919
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([22.92947477,  9.95152563,  3.66273214]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 2.3113354381779914}
done in step count: 13
reward sum = 0.8118660230403958
running average episode reward sum: 0.49630598799730585
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([23.24109792, 12.11266426,  0.36636592]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 0.7672193974998736}
episode index:4920
target Thresh 32.0
target distance 2.0
model initialize at round 4920
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  9.]), 'previousTarget': array([19.,  9.]), 'currentState': array([16.93182316,  9.88974002,  4.37796697]), 'targetState': array([19,  9], dtype=int32), 'currentDistance': 2.251442368086044}
done in step count: 10
reward sum = 0.8707221852114942
running average episode reward sum: 0.4963820733858883
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  9.]), 'previousTarget': array([19.,  9.]), 'currentState': array([18.08414852,  8.73678856,  6.04607154]), 'targetState': array([19,  9], dtype=int32), 'currentDistance': 0.9529240210209451}
episode index:4921
target Thresh 32.0
target distance 19.0
model initialize at round 4921
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.33585963, 18.90487265]), 'previousTarget': array([16.33589719, 18.90482627]), 'currentState': array([2.99980264, 4.00018935, 2.15413564]), 'targetState': array([20, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.21176612685528168
running average episode reward sum: 0.4963242481224729
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 23.]), 'previousTarget': array([20., 23.]), 'currentState': array([19.10357765, 22.18165145,  0.76784516]), 'targetState': array([20, 23], dtype=int32), 'currentDistance': 1.2137822614579334}
episode index:4922
target Thresh 32.0
target distance 16.0
model initialize at round 4922
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 13.]), 'previousTarget': array([11., 13.]), 'currentState': array([16.03079341, 28.81749029,  4.82995978]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 16.5982493431443}
done in step count: 41
reward sum = 0.45255130019067025
running average episode reward sum: 0.4963153566034943
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 13.]), 'previousTarget': array([11., 13.]), 'currentState': array([11.22688913, 13.87306733,  4.20582365]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 0.9020671989583913}
episode index:4923
target Thresh 32.0
target distance 8.0
model initialize at round 4923
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 12.]), 'previousTarget': array([ 2., 12.]), 'currentState': array([10.        , 16.        ,  1.88563383]), 'targetState': array([ 2, 12], dtype=int32), 'currentDistance': 8.94427190999916}
done in step count: 23
reward sum = 0.6547206863173711
running average episode reward sum: 0.49634752665420795
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 12.]), 'previousTarget': array([ 2., 12.]), 'currentState': array([ 2.9359196 , 12.70927815,  3.79762883]), 'targetState': array([ 2, 12], dtype=int32), 'currentDistance': 1.1743172464090113}
episode index:4924
target Thresh 32.0
target distance 14.0
model initialize at round 4924
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 18.]), 'previousTarget': array([ 4., 18.]), 'currentState': array([17.        ,  4.        ,  5.98630905]), 'targetState': array([ 4, 18], dtype=int32), 'currentDistance': 19.104973174542796}
done in step count: 53
reward sum = 0.31913849437413344
running average episode reward sum: 0.496311545124811
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 18.]), 'previousTarget': array([ 4., 18.]), 'currentState': array([ 4.90922604, 17.65339226,  2.16174425]), 'targetState': array([ 4, 18], dtype=int32), 'currentDistance': 0.9730513415571346}
episode index:4925
target Thresh 32.0
target distance 14.0
model initialize at round 4925
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 21.]), 'previousTarget': array([19., 21.]), 'currentState': array([ 4.99987873, 21.01122928,  1.44979638]), 'targetState': array([19, 21], dtype=int32), 'currentDistance': 14.00012577404505}
done in step count: 36
reward sum = 0.5025244202809513
running average episode reward sum: 0.49631280636621505
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 21.]), 'previousTarget': array([19., 21.]), 'currentState': array([18.16965781, 20.8361132 ,  6.25934263]), 'targetState': array([19, 21], dtype=int32), 'currentDistance': 0.8463610515194085}
episode index:4926
target Thresh 32.0
target distance 10.0
model initialize at round 4926
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'previousTarget': array([25.,  3.]), 'currentState': array([16.25525469, 13.00405895,  6.21161965]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 13.287278350823522}
done in step count: 30
reward sum = 0.5462824808837012
running average episode reward sum: 0.49632294837443863
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'previousTarget': array([25.,  3.]), 'currentState': array([24.41393358,  3.74347639,  5.36327409]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 0.9466947692552713}
episode index:4927
target Thresh 32.0
target distance 9.0
model initialize at round 4927
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 21.]), 'previousTarget': array([20., 21.]), 'currentState': array([26.00573061, 12.00497044,  0.96698025]), 'targetState': array([20, 21], dtype=int32), 'currentDistance': 10.815699559901693}
done in step count: 24
reward sum = 0.6145192432007516
running average episode reward sum: 0.4963469330121875
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20., 21.]), 'previousTarget': array([20., 21.]), 'currentState': array([20.86198301, 20.26830606,  2.22009316]), 'targetState': array([20, 21], dtype=int32), 'currentDistance': 1.1306594274929977}
episode index:4928
target Thresh 32.0
target distance 16.0
model initialize at round 4928
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 19.]), 'previousTarget': array([24., 19.]), 'currentState': array([ 8.2318269 , 18.81856115,  5.79598558]), 'targetState': array([24, 19], dtype=int32), 'currentDistance': 15.769216942773241}
done in step count: 35
reward sum = 0.4799207500984737
running average episode reward sum: 0.49634360045326814
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24., 19.]), 'previousTarget': array([24., 19.]), 'currentState': array([23.04022044, 18.85103946,  0.14574361]), 'targetState': array([24, 19], dtype=int32), 'currentDistance': 0.9712703223007492}
episode index:4929
target Thresh 32.0
target distance 6.0
model initialize at round 4929
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([ 2.01352536, 15.02068062,  1.2194775 ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 6.020695815648623}
done in step count: 19
reward sum = 0.6861530944793316
running average episode reward sum: 0.4963821013648353
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.66543906, 9.95899589, 4.89825455]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.0156791528636848}
episode index:4930
target Thresh 32.0
target distance 16.0
model initialize at round 4930
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9.85786438, 4.85786438]), 'previousTarget': array([9.85786438, 4.85786438]), 'currentState': array([24.       , 19.       ,  6.0391981]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 20.0}
done in step count: 72
reward sum = 0.25138455308437796
running average episode reward sum: 0.49633241619990315
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([8.46681392, 3.81052074, 3.91734547]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 0.9353390326055637}
episode index:4931
target Thresh 32.0
target distance 15.0
model initialize at round 4931
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 26.]), 'previousTarget': array([ 4., 26.]), 'currentState': array([13.        , 11.        ,  4.54074192]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 17.4928556845359}
done in step count: 63
reward sum = 0.2810875580907024
running average episode reward sum: 0.4962887736901487
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 26.]), 'previousTarget': array([ 4., 26.]), 'currentState': array([ 4.64430005, 25.36663583,  2.12009028]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 0.9034781280027514}
episode index:4932
target Thresh 32.0
target distance 23.0
model initialize at round 4932
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([7.25906793, 8.42303259]), 'previousTarget': array([7.2957649, 8.4268235]), 'currentState': array([26.9528356 ,  4.93655656,  3.85047552]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 20.0}
done in step count: 71
reward sum = 0.2568031451975973
running average episode reward sum: 0.49624022602574713
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.7778429 , 9.01080186, 2.96851601]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.7779179012893583}
episode index:4933
target Thresh 32.0
target distance 13.0
model initialize at round 4933
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 10.]), 'previousTarget': array([27., 10.]), 'currentState': array([14.29507753, 17.05732846,  0.09854804]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 14.533442124624179}
done in step count: 32
reward sum = 0.504344342453318
running average episode reward sum: 0.49624186853009
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 10.]), 'previousTarget': array([27., 10.]), 'currentState': array([26.16607024,  9.88231564,  5.90078376]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 0.8421926493760802}
episode index:4934
target Thresh 32.0
target distance 4.0
model initialize at round 4934
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 27.]), 'previousTarget': array([21., 27.]), 'currentState': array([22.30661408, 23.37388393,  0.92288289]), 'targetState': array([21, 27], dtype=int32), 'currentDistance': 3.854342761080471}
done in step count: 17
reward sum = 0.7834655216001934
running average episode reward sum: 0.49630006987822983
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 27.]), 'previousTarget': array([21., 27.]), 'currentState': array([21.9999098 , 27.02968501,  2.64725845]), 'targetState': array([21, 27], dtype=int32), 'currentDistance': 1.0003503455604055}
episode index:4935
target Thresh 32.0
target distance 10.0
model initialize at round 4935
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([26.        ,  6.        ,  5.45383623]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 10.770329614269007}
done in step count: 28
reward sum = 0.5766684338788581
running average episode reward sum: 0.4963163519616983
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.83534755,  2.46835215,  3.42714942]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.957684322529824}
episode index:4936
target Thresh 32.0
target distance 11.0
model initialize at round 4936
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([23.97317218, 22.0265261 ,  2.6143496 ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 13.607194873951233}
done in step count: 36
reward sum = 0.5022596596888365
running average episode reward sum: 0.49631755579149917
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.96029399, 11.80265271,  4.1227291 ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.8036342046176023}
episode index:4937
target Thresh 32.0
target distance 20.0
model initialize at round 4937
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.06694926, 25.87573861]), 'previousTarget': array([14.05572809, 25.88854382]), 'currentState': array([23.04363891,  8.00344041,  0.29870331]), 'targetState': array([13, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.2720924680332656
running average episode reward sum: 0.4962721477137839
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 28.]), 'previousTarget': array([13., 28.]), 'currentState': array([13.77810934, 27.3792058 ,  2.09703867]), 'targetState': array([13, 28], dtype=int32), 'currentDistance': 0.9954092556953813}
episode index:4938
target Thresh 32.0
target distance 14.0
model initialize at round 4938
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 23.]), 'previousTarget': array([ 6., 23.]), 'currentState': array([10.01889316,  9.002856  ,  0.40117082]), 'targetState': array([ 6, 23], dtype=int32), 'currentDistance': 14.562676341345506}
done in step count: 32
reward sum = 0.48866933534284257
running average episode reward sum: 0.4962706083713318
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 23.]), 'previousTarget': array([ 6., 23.]), 'currentState': array([ 6.5025509 , 22.13462365,  1.87895766]), 'targetState': array([ 6, 23], dtype=int32), 'currentDistance': 1.000716560045015}
episode index:4939
target Thresh 32.0
target distance 19.0
model initialize at round 4939
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.06532747,  8.17622929]), 'previousTarget': array([19.08589282,  8.23313766]), 'currentState': array([26.01698161, 26.92921585,  4.89871488]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.35269787677269315
running average episode reward sum: 0.4962415450653402
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19.,  8.]), 'previousTarget': array([19.,  8.]), 'currentState': array([19.02742939,  8.87928373,  4.40492343]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 0.8797114606859745}
episode index:4940
target Thresh 32.0
target distance 9.0
model initialize at round 4940
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([10.       , 28.       ,  2.9879303]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 10.295630140986999}
done in step count: 28
reward sum = 0.5784960346261385
running average episode reward sum: 0.4962581924018228
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([14.29320793, 19.994492  ,  5.49452112]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 1.2200694147695983}
episode index:4941
target Thresh 32.0
target distance 11.0
model initialize at round 4941
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([ 5.       , 21.       ,  4.1689899]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 11.704699910719626}
done in step count: 28
reward sum = 0.5590321218685997
running average episode reward sum: 0.49627089453243123
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([15.05869087, 17.16583939,  5.90533279]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 0.9558062500073262}
episode index:4942
target Thresh 32.0
target distance 21.0
model initialize at round 4942
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5.94278962, 26.59867161]), 'previousTarget': array([ 5.94278962, 26.59867161]), 'currentState': array([24.        , 18.        ,  1.62084687]), 'targetState': array([ 3, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.24523456507167607
running average episode reward sum: 0.49622010830352964
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 28.]), 'previousTarget': array([ 3., 28.]), 'currentState': array([ 3.8996796 , 27.88343663,  2.9414684 ]), 'targetState': array([ 3, 28], dtype=int32), 'currentDistance': 0.9071992079679089}
episode index:4943
target Thresh 32.0
target distance 17.0
model initialize at round 4943
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 18.]), 'previousTarget': array([25., 18.]), 'currentState': array([ 8.01106543, 11.02942935,  1.02790594]), 'targetState': array([25, 18], dtype=int32), 'currentDistance': 18.36335353374826}
done in step count: 46
reward sum = 0.39519724274465323
running average episode reward sum: 0.4961996748760299
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 18.]), 'previousTarget': array([25., 18.]), 'currentState': array([24.13355818, 17.55260472,  0.2669007 ]), 'targetState': array([25, 18], dtype=int32), 'currentDistance': 0.9751327970236864}
episode index:4944
target Thresh 32.0
target distance 13.0
model initialize at round 4944
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 21.]), 'previousTarget': array([ 6., 21.]), 'currentState': array([18.72212559, 28.91731875,  3.42816854]), 'targetState': array([ 6, 21], dtype=int32), 'currentDistance': 14.984539223215718}
done in step count: 33
reward sum = 0.5137145514005171
running average episode reward sum: 0.49620321681263746
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 21.]), 'previousTarget': array([ 6., 21.]), 'currentState': array([ 6.91992301, 21.57882087,  3.74175156]), 'targetState': array([ 6, 21], dtype=int32), 'currentDistance': 1.0868725554147824}
episode index:4945
target Thresh 32.0
target distance 22.0
model initialize at round 4945
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2.18975657, 4.10464666]), 'previousTarget': array([2.18928508, 4.08213587]), 'currentState': array([ 3.98568728, 24.02384927,  2.36381304]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.2538340938314826
running average episode reward sum: 0.4961542137550189
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.52451186, 2.8613075 , 4.69356798]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.9838392088550477}
episode index:4946
target Thresh 32.0
target distance 8.0
model initialize at round 4946
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 25.]), 'previousTarget': array([21., 25.]), 'currentState': array([13.18570794, 28.11595736,  0.35642609]), 'targetState': array([21, 25], dtype=int32), 'currentDistance': 8.412630423990167}
done in step count: 17
reward sum = 0.6968597564029402
running average episode reward sum: 0.4961947849178747
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 25.]), 'previousTarget': array([21., 25.]), 'currentState': array([20.05065373, 24.99611803,  5.87085546]), 'targetState': array([21, 25], dtype=int32), 'currentDistance': 0.9493542036398468}
episode index:4947
target Thresh 32.0
target distance 15.0
model initialize at round 4947
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 14.]), 'previousTarget': array([27., 14.]), 'currentState': array([12.02084089, 24.01023347,  0.2078383 ]), 'targetState': array([27, 14], dtype=int32), 'currentDistance': 18.01610340185973}
done in step count: 48
reward sum = 0.39718509665552115
running average episode reward sum: 0.4961747748757845
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([27., 14.]), 'previousTarget': array([27., 14.]), 'currentState': array([26.15518867, 14.22044305,  5.61049021]), 'targetState': array([27, 14], dtype=int32), 'currentDistance': 0.8730986882710174}
episode index:4948
target Thresh 32.0
target distance 19.0
model initialize at round 4948
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.04947719, 23.55255388]), 'previousTarget': array([20.07475678, 23.56172689]), 'currentState': array([ 1.95413935, 15.03441155,  2.27492753]), 'targetState': array([21, 24], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.28800856791952156
running average episode reward sum: 0.4961327125991718
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 24.]), 'previousTarget': array([21., 24.]), 'currentState': array([20.12936659, 23.52369774,  0.35463256]), 'targetState': array([21, 24], dtype=int32), 'currentDistance': 0.9924043409781769}
episode index:4949
target Thresh 32.0
target distance 2.0
model initialize at round 4949
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 21.]), 'previousTarget': array([21., 21.]), 'currentState': array([22.04134654, 23.01211507,  0.14572984]), 'targetState': array([21, 21], dtype=int32), 'currentDistance': 2.2656146323440027}
done in step count: 9
reward sum = 0.8644677545659072
running average episode reward sum: 0.49620712371876097
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 21.]), 'previousTarget': array([21., 21.]), 'currentState': array([21.85847354, 21.66556675,  3.8138024 ]), 'targetState': array([21, 21], dtype=int32), 'currentDistance': 1.086257763667902}
episode index:4950
target Thresh 32.0
target distance 11.0
model initialize at round 4950
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 20.]), 'previousTarget': array([14., 20.]), 'currentState': array([ 3.        , 18.        ,  5.00533083]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 11.180339887498949}
done in step count: 29
reward sum = 0.5823753439368586
running average episode reward sum: 0.4962245279240161
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14., 20.]), 'previousTarget': array([14., 20.]), 'currentState': array([13.071783  , 19.46204396,  0.31322229]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 1.0728389919662626}
episode index:4951
target Thresh 32.0
target distance 2.0
model initialize at round 4951
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 15.]), 'previousTarget': array([ 6., 15.]), 'currentState': array([ 4.        , 13.        ,  5.09123325]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 2.8284271247461903}
done in step count: 8
reward sum = 0.8548130763607203
running average episode reward sum: 0.4962969407972868
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 6., 15.]), 'previousTarget': array([ 6., 15.]), 'currentState': array([ 5.63889949, 14.08754472,  0.90101842]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 0.9813094384696732}
episode index:4952
target Thresh 32.0
target distance 9.0
model initialize at round 4952
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([24.8178187 , 22.10716274,  2.74267611]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 8.818469846732913}
done in step count: 18
reward sum = 0.6887956895044798
running average episode reward sum: 0.4963358058787944
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([16.72003109, 22.21312455,  3.05842918]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 0.7509106785862911}
episode index:4953
target Thresh 32.0
target distance 11.0
model initialize at round 4953
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 18.]), 'previousTarget': array([ 8., 18.]), 'currentState': array([18.77757564, 26.9531472 ,  3.4884713 ]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 14.011244821045585}
done in step count: 30
reward sum = 0.5377710198276455
running average episode reward sum: 0.4963441698703061
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 18.]), 'previousTarget': array([ 8., 18.]), 'currentState': array([ 8.79662219, 18.73908988,  3.81023376]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 1.086674174999897}
episode index:4954
target Thresh 32.0
target distance 13.0
model initialize at round 4954
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 5.]), 'previousTarget': array([6., 5.]), 'currentState': array([12.97246474, 17.78795072,  4.54677488]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 14.565265121914345}
done in step count: 33
reward sum = 0.519596564348335
running average episode reward sum: 0.49634886258362154
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([6., 5.]), 'previousTarget': array([6., 5.]), 'currentState': array([6.29434189, 5.93914852, 4.05514881]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 0.9841936269775458}
episode index:4955
target Thresh 32.0
target distance 18.0
model initialize at round 4955
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 17.]), 'previousTarget': array([ 3., 17.]), 'currentState': array([21.        , 16.        ,  1.74874851]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 18.027756377319946}
done in step count: 48
reward sum = 0.38194787708162603
running average episode reward sum: 0.49632577925321353
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 17.]), 'previousTarget': array([ 3., 17.]), 'currentState': array([ 3.79895485, 17.24535125,  3.05398033]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 0.835778726976879}
episode index:4956
target Thresh 32.0
target distance 9.0
model initialize at round 4956
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 26.]), 'previousTarget': array([25., 26.]), 'currentState': array([17.00284624, 16.99854479,  6.00255761]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 12.040791672519777}
done in step count: 28
reward sum = 0.5643280781162995
running average episode reward sum: 0.4963394976915559
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 26.]), 'previousTarget': array([25., 26.]), 'currentState': array([24.83378631, 25.24842862,  0.80196425]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 0.76973146072461}
episode index:4957
target Thresh 32.0
target distance 21.0
model initialize at round 4957
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([20.28964042, 21.07981535]), 'previousTarget': array([20.18513205, 21.01582747]), 'currentState': array([5.13810986, 8.02488824, 0.21512364]), 'targetState': array([26, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 94
reward sum = 0.16966742315789649
running average episode reward sum: 0.49627360981851565
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 26.]), 'previousTarget': array([26., 26.]), 'currentState': array([25.80163144, 25.06641345,  0.89966254]), 'targetState': array([26, 26], dtype=int32), 'currentDistance': 0.9544285942015894}
episode index:4958
target Thresh 32.0
target distance 13.0
model initialize at round 4958
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  3.]), 'previousTarget': array([17.,  3.]), 'currentState': array([ 9.        , 16.        ,  3.24697518]), 'targetState': array([17,  3], dtype=int32), 'currentDistance': 15.264337522473747}
done in step count: 34
reward sum = 0.4524775965040423
running average episode reward sum: 0.4962647781965527
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  3.]), 'previousTarget': array([17.,  3.]), 'currentState': array([16.25341297,  3.66896349,  5.28196905]), 'targetState': array([17,  3], dtype=int32), 'currentDistance': 1.0024491719093729}
episode index:4959
target Thresh 32.0
target distance 9.0
model initialize at round 4959
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([24.72271644, 16.67284731,  3.99259231]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 12.300571637679688}
done in step count: 25
reward sum = 0.5830530426641534
running average episode reward sum: 0.496282275830518
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.38709711,  8.83622535,  3.84364259]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.9214754499405371}
episode index:4960
target Thresh 32.0
target distance 17.0
model initialize at round 4960
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([4.99463048, 2.99198334, 4.37139869]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 19.73177614552167}
done in step count: 55
reward sum = 0.30209487462758433
running average episode reward sum: 0.4962431330364839
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([21.79229852, 12.18152171,  0.85249631]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 0.8444208709874783}
episode index:4961
target Thresh 32.0
target distance 18.0
model initialize at round 4961
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([ 2.90439448, 15.13514076,  1.94791245]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 18.36518572279914}
done in step count: 42
reward sum = 0.34975235656608966
running average episode reward sum: 0.49621361050998847
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([20.04765595, 12.02308095,  6.17912183]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 0.9526237011272937}
episode index:4962
target Thresh 32.0
target distance 14.0
model initialize at round 4962
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([26.        , 11.        ,  1.99743605]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 14.0}
done in step count: 33
reward sum = 0.5052546070601771
running average episode reward sum: 0.4962154321897286
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.83351826, 11.27241942,  3.13516294]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.8769065114041426}
episode index:4963
target Thresh 32.0
target distance 16.0
model initialize at round 4963
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 18.]), 'previousTarget': array([25., 18.]), 'currentState': array([9.26264899, 9.0369522 , 0.32703825]), 'targetState': array([25, 18], dtype=int32), 'currentDistance': 18.110782502387877}
done in step count: 43
reward sum = 0.4112594329256598
running average episode reward sum: 0.49619831776602513
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 18.]), 'previousTarget': array([25., 18.]), 'currentState': array([24.26471724, 17.15291308,  0.62366526]), 'targetState': array([25, 18], dtype=int32), 'currentDistance': 1.1216938051492134}
episode index:4964
target Thresh 32.0
target distance 21.0
model initialize at round 4964
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.11990655,  4.3102453 ]), 'previousTarget': array([12.11990655,  4.3102453 ]), 'currentState': array([ 5.        , 23.        ,  5.73279428]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 95
reward sum = 0.22507766655070569
running average episode reward sum: 0.4961437113911579
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13.,  2.]), 'previousTarget': array([13.,  2.]), 'currentState': array([12.65262979,  2.97927828,  4.92328549]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 1.03906304564439}
episode index:4965
target Thresh 32.0
target distance 14.0
model initialize at round 4965
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 4.]), 'previousTarget': array([5., 4.]), 'currentState': array([14.        , 18.        ,  2.37325585]), 'targetState': array([5, 4], dtype=int32), 'currentDistance': 16.643316977093235}
done in step count: 41
reward sum = 0.4045467064106983
running average episode reward sum: 0.49612526656534633
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([5., 4.]), 'previousTarget': array([5., 4.]), 'currentState': array([5.34389814, 4.74539684, 4.17171687]), 'targetState': array([5, 4], dtype=int32), 'currentDistance': 0.8209033982117485}
episode index:4966
target Thresh 32.0
target distance 4.0
model initialize at round 4966
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([13.89801165, 18.27079319,  1.73975262]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 4.280810488323145}
done in step count: 8
reward sum = 0.8454911710339882
running average episode reward sum: 0.49619560397313145
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([15.6369659 , 21.08349817,  0.98238482]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 0.9857836301738441}
episode index:4967
target Thresh 32.0
target distance 16.0
model initialize at round 4967
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([7.98644095, 3.94717855, 4.71361828]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 16.13150884961314}
done in step count: 38
reward sum = 0.4408684192525197
running average episode reward sum: 0.4961844672612312
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([23.06684818,  1.78743392,  6.1799122 ]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 0.9570562426161071}
episode index:4968
target Thresh 32.0
target distance 8.0
model initialize at round 4968
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([ 5.96201668, 10.95877082,  4.21940041]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 11.311552006988434}
done in step count: 23
reward sum = 0.5963072290539958
running average episode reward sum: 0.4962046167403604
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.079305  ,  3.79671368,  5.64791107]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.2175516323878346}
episode index:4969
target Thresh 32.0
target distance 9.0
model initialize at round 4969
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([8.99758135, 4.93266966, 4.92898226]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 9.207537868243751}
done in step count: 21
reward sum = 0.6510305834145225
running average episode reward sum: 0.496235768846331
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([17.18147223,  3.05558265,  6.13399941]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 0.820412782553892}
episode index:4970
target Thresh 32.0
target distance 15.0
model initialize at round 4970
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 26.]), 'previousTarget': array([ 7., 26.]), 'currentState': array([ 2.91590081, 11.08407348,  2.11898911]), 'targetState': array([ 7, 26], dtype=int32), 'currentDistance': 15.464951670819472}
done in step count: 36
reward sum = 0.4705344030254945
running average episode reward sum: 0.49623059858565494
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7., 26.]), 'previousTarget': array([ 7., 26.]), 'currentState': array([ 6.97366958, 25.13452271,  1.35708717]), 'targetState': array([ 7, 26], dtype=int32), 'currentDistance': 0.8658777195061631}
episode index:4971
target Thresh 32.0
target distance 19.0
model initialize at round 4971
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([2.18627029, 9.09136156, 0.43840118]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 18.835658982309635}
done in step count: 48
reward sum = 0.38649890839097534
running average episode reward sum: 0.49620852865600995
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([20.20774597,  9.9046815 ,  0.1501009 ]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 0.797967455995474}
episode index:4972
target Thresh 32.0
target distance 8.0
model initialize at round 4972
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([11.09585185, 11.78327847,  4.93371162]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 8.060522233277256}
done in step count: 17
reward sum = 0.7106750026029751
running average episode reward sum: 0.4962516548321505
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([9.24915091, 4.75338104, 4.61544527]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 0.793510657265338}
episode index:4973
target Thresh 32.0
target distance 7.0
model initialize at round 4973
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([ 8.94772619, 10.10336415,  1.86808294]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 9.863982553928572}
done in step count: 22
reward sum = 0.6480175676804627
running average episode reward sum: 0.4962821666763098
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([15.28004303, 16.24684311,  0.54727307]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 1.0419133105450633}
episode index:4974
target Thresh 32.0
target distance 16.0
model initialize at round 4974
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  9.]), 'previousTarget': array([17.,  9.]), 'currentState': array([ 6.27380976, 24.92882291,  5.78223726]), 'targetState': array([17,  9], dtype=int32), 'currentDistance': 19.20360789729829}
done in step count: 45
reward sum = 0.3821099958995763
running average episode reward sum: 0.4962592174962542
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17.,  9.]), 'previousTarget': array([17.,  9.]), 'currentState': array([16.17526294,  9.77810918,  5.2463851 ]), 'targetState': array([17,  9], dtype=int32), 'currentDistance': 1.1338629150871025}
episode index:4975
target Thresh 32.0
target distance 4.0
model initialize at round 4975
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 18.]), 'previousTarget': array([ 4., 18.]), 'currentState': array([ 4.        , 22.        ,  0.70664465]), 'targetState': array([ 4, 18], dtype=int32), 'currentDistance': 4.0}
done in step count: 13
reward sum = 0.7825790240755457
running average episode reward sum: 0.49631675765030947
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 18.]), 'previousTarget': array([ 4., 18.]), 'currentState': array([ 3.88692325, 18.64402397,  4.73531965]), 'targetState': array([ 4, 18], dtype=int32), 'currentDistance': 0.653875540176795}
episode index:4976
target Thresh 32.0
target distance 4.0
model initialize at round 4976
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 21.]), 'previousTarget': array([13., 21.]), 'currentState': array([13.01273565, 16.90275601,  4.95217279]), 'targetState': array([13, 21], dtype=int32), 'currentDistance': 4.097263787095377}
done in step count: 12
reward sum = 0.7581249055674386
running average episode reward sum: 0.4963693612564813
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 21.]), 'previousTarget': array([13., 21.]), 'currentState': array([12.97541066, 20.30289552,  2.00325573]), 'targetState': array([13, 21], dtype=int32), 'currentDistance': 0.6975380199154092}
episode index:4977
target Thresh 32.0
target distance 6.0
model initialize at round 4977
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([ 7.95540894, 11.11020552,  1.73180223]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 6.79782755821912}
done in step count: 20
reward sum = 0.7002943206794091
running average episode reward sum: 0.4964103264954172
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.14062761,  8.48706878,  5.74001367]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.9878040806643384}
episode index:4978
target Thresh 32.0
target distance 3.0
model initialize at round 4978
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  7.]), 'previousTarget': array([22.,  7.]), 'currentState': array([24.73277102,  4.24727985,  2.30088775]), 'targetState': array([22,  7], dtype=int32), 'currentDistance': 3.8788536564340865}
done in step count: 7
reward sum = 0.8845676835328162
running average episode reward sum: 0.49648828539419954
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([22.,  7.]), 'previousTarget': array([22.,  7.]), 'currentState': array([22.8461296 ,  6.20003182,  2.36652638]), 'targetState': array([22,  7], dtype=int32), 'currentDistance': 1.1644244894258304}
episode index:4979
target Thresh 32.0
target distance 24.0
model initialize at round 4979
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 7.04088399, 10.17126849]), 'previousTarget': array([ 7.01733854, 10.16738911]), 'currentState': array([27.02294419, 11.01818646,  0.9227365 ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 77
reward sum = 0.20496828250257304
running average episode reward sum: 0.49642974724100847
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.73615945, 10.29364489,  3.38308429]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.7925642286370697}
episode index:4980
target Thresh 32.0
target distance 12.0
model initialize at round 4980
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([27.        , 10.        ,  0.73859474]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 14.999999999999998}
done in step count: 38
reward sum = 0.4632357692731863
running average episode reward sum: 0.49642308312176175
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([15.8294553 , 18.65305034,  2.69525633]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 0.8990940802096107}
episode index:4981
target Thresh 32.0
target distance 15.0
model initialize at round 4981
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 25.]), 'previousTarget': array([26., 25.]), 'currentState': array([10.99130822, 18.96408431,  4.70883512]), 'targetState': array([26, 25], dtype=int32), 'currentDistance': 16.176931326944676}
done in step count: 40
reward sum = 0.41980784880648936
running average episode reward sum: 0.4964077047126258
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([26., 25.]), 'previousTarget': array([26., 25.]), 'currentState': array([25.10636729, 24.62038946,  0.75160801]), 'targetState': array([26, 25], dtype=int32), 'currentDistance': 0.9709189362906567}
episode index:4982
target Thresh 32.0
target distance 12.0
model initialize at round 4982
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([14.991318  , 29.07022255,  1.94630754]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 13.948792058528445}
done in step count: 35
reward sum = 0.4723325966963215
running average episode reward sum: 0.49640287326409754
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([ 8.42713364, 17.76503147,  4.17449955]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 0.8761942098789137}
episode index:4983
target Thresh 32.0
target distance 3.0
model initialize at round 4983
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 22.]), 'previousTarget': array([11., 22.]), 'currentState': array([ 7.99270627, 25.00569798,  2.49987247]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 4.2518273598089795}
done in step count: 15
reward sum = 0.769597503094882
running average episode reward sum: 0.4964576875959255
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11., 22.]), 'previousTarget': array([11., 22.]), 'currentState': array([10.06028882, 22.22270433,  5.66018407]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 0.965740300120245}
episode index:4984
target Thresh 32.0
target distance 13.0
model initialize at round 4984
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([12.       , 18.       ,  0.4941994]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 13.341664064126334}
done in step count: 34
reward sum = 0.5082613247838088
running average episode reward sum: 0.49646005542685584
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.52674448,  5.94184904,  5.17720487]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.0540637559837869}
episode index:4985
target Thresh 32.0
target distance 19.0
model initialize at round 4985
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 5.93902677, 24.76457776]), 'previousTarget': array([ 6.09022194, 24.67985983]), 'currentState': array([22.80635918, 14.01780627,  2.96931021]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.29884343865228913
running average episode reward sum: 0.49642042112746265
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 26.]), 'previousTarget': array([ 4., 26.]), 'currentState': array([ 4.9298564 , 25.53498517,  2.34065933]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 1.0396498077981988}
episode index:4986
target Thresh 32.0
target distance 10.0
model initialize at round 4986
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 18.]), 'previousTarget': array([ 8., 18.]), 'currentState': array([18.        , 24.        ,  1.22708809]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 11.661903789690601}
done in step count: 34
reward sum = 0.5424478860803992
running average episode reward sum: 0.4964296506171264
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 8., 18.]), 'previousTarget': array([ 8., 18.]), 'currentState': array([ 8.65474745, 18.92097114,  3.93455391]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 1.1299920661583227}
episode index:4987
target Thresh 32.0
target distance 20.0
model initialize at round 4987
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9.00992562, 23.9007438 ]), 'previousTarget': array([ 9.00992562, 23.9007438 ]), 'currentState': array([11.        ,  4.        ,  6.17725223]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.3158215586566661
running average episode reward sum: 0.4963934420982891
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([ 9.30029863, 23.30984823,  1.83590504]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 0.7526544566963812}
episode index:4988
target Thresh 32.0
target distance 5.0
model initialize at round 4988
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'previousTarget': array([10., 17.]), 'currentState': array([14.86028811, 13.89146141,  3.61941937]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 5.769351148206291}
done in step count: 14
reward sum = 0.7796030774897151
running average episode reward sum: 0.49645020891235836
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'previousTarget': array([10., 17.]), 'currentState': array([10.86036208, 16.59238779,  2.48559318]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 0.9520349929741728}
episode index:4989
target Thresh 32.0
target distance 16.0
model initialize at round 4989
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  8.]), 'previousTarget': array([11.,  8.]), 'currentState': array([26.80243385, 14.86274255,  3.74088071]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 17.228295070804627}
done in step count: 42
reward sum = 0.4440224446545729
running average episode reward sum: 0.4964397023463748
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([11.,  8.]), 'previousTarget': array([11.,  8.]), 'currentState': array([11.73220999,  8.83813374,  3.66942762]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 1.112923915036503}
episode index:4990
target Thresh 32.0
target distance 23.0
model initialize at round 4990
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([12.28798697,  7.37514441]), 'previousTarget': array([12.28798697,  7.37514441]), 'currentState': array([ 5.        , 26.        ,  3.01399469]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 92
reward sum = 0.18002695082609158
running average episode reward sum: 0.496376305682075
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.74177265,  3.79153064,  5.32557606]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.8325876061602311}
episode index:4991
target Thresh 32.0
target distance 13.0
model initialize at round 4991
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 13.]), 'previousTarget': array([17., 13.]), 'currentState': array([ 4.25891013, 20.99434711,  6.12840547]), 'targetState': array([17, 13], dtype=int32), 'currentDistance': 15.041441314110035}
done in step count: 33
reward sum = 0.5018086942276889
running average episode reward sum: 0.4963773939009343
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 13.]), 'previousTarget': array([17., 13.]), 'currentState': array([16.02898351, 13.31702901,  5.72289478]), 'targetState': array([17, 13], dtype=int32), 'currentDistance': 1.0214599443468377}
episode index:4992
target Thresh 32.0
target distance 6.0
model initialize at round 4992
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 16.]), 'previousTarget': array([17., 16.]), 'currentState': array([11.24413777, 21.83832087,  5.55545807]), 'targetState': array([17, 16], dtype=int32), 'currentDistance': 8.198532830848901}
done in step count: 19
reward sum = 0.7239016157057319
running average episode reward sum: 0.496422962541392
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([17., 16.]), 'previousTarget': array([17., 16.]), 'currentState': array([16.01866946, 16.7915223 ,  5.60021348]), 'targetState': array([17, 16], dtype=int32), 'currentDistance': 1.2607605533487876}
episode index:4993
target Thresh 32.0
target distance 12.0
model initialize at round 4993
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 12.]), 'previousTarget': array([ 4., 12.]), 'currentState': array([16.        ,  4.        ,  0.02685001]), 'targetState': array([ 4, 12], dtype=int32), 'currentDistance': 14.422205101855958}
done in step count: 36
reward sum = 0.4595333810539086
running average episode reward sum: 0.49641557576095796
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 4., 12.]), 'previousTarget': array([ 4., 12.]), 'currentState': array([ 4.92651547, 11.49572909,  2.67219412]), 'targetState': array([ 4, 12], dtype=int32), 'currentDistance': 1.0548554721185979}
episode index:4994
target Thresh 32.0
target distance 11.0
model initialize at round 4994
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 20.]), 'previousTarget': array([ 9., 20.]), 'currentState': array([20.        , 19.        ,  1.53726333]), 'targetState': array([ 9, 20], dtype=int32), 'currentDistance': 11.04536101718726}
done in step count: 26
reward sum = 0.5827115559042876
running average episode reward sum: 0.4964328522334591
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 9., 20.]), 'previousTarget': array([ 9., 20.]), 'currentState': array([ 9.92097257, 20.4564074 ,  3.40009452]), 'targetState': array([ 9, 20], dtype=int32), 'currentDistance': 1.0278609764269457}
episode index:4995
target Thresh 32.0
target distance 12.0
model initialize at round 4995
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([13.29140894, 26.89681292,  5.81917594]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 15.330949435857136}
done in step count: 38
reward sum = 0.48578243079747485
running average episode reward sum: 0.4964307204437401
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([24.199304  , 17.46336168,  5.77162119]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 0.9251043900677419}
episode index:4996
target Thresh 32.0
target distance 16.0
model initialize at round 4996
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 28.]), 'previousTarget': array([ 2., 28.]), 'currentState': array([18.01032365, 18.12245807,  1.73919165]), 'targetState': array([ 2, 28], dtype=int32), 'currentDistance': 18.81213166794122}
done in step count: 50
reward sum = 0.37957385833181406
running average episode reward sum: 0.4964073350400755
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([ 2., 28.]), 'previousTarget': array([ 2., 28.]), 'currentState': array([ 2.80022799, 27.66742113,  2.85315604]), 'targetState': array([ 2, 28], dtype=int32), 'currentDistance': 0.8665872943986077}
episode index:4997
target Thresh 32.0
target distance 13.0
model initialize at round 4997
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 24.]), 'previousTarget': array([19., 24.]), 'currentState': array([ 5.97529214, 22.92841886,  4.62822109]), 'targetState': array([19, 24], dtype=int32), 'currentDistance': 13.068714584895154}
done in step count: 29
reward sum = 0.5166172809753965
running average episode reward sum: 0.4964113786467052
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([19., 24.]), 'previousTarget': array([19., 24.]), 'currentState': array([18.00467538, 23.57777055,  0.14029964]), 'targetState': array([19, 24], dtype=int32), 'currentDistance': 1.081179356229048}
episode index:4998
target Thresh 32.0
target distance 17.0
model initialize at round 4998
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 26.]), 'previousTarget': array([16., 26.]), 'currentState': array([15.88219492,  9.07335928,  2.33213639]), 'targetState': array([16, 26], dtype=int32), 'currentDistance': 16.927050663063394}
done in step count: 39
reward sum = 0.4286315211594935
running average episode reward sum: 0.4963978199634711
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([16., 26.]), 'previousTarget': array([16., 26.]), 'currentState': array([16.32277785, 25.21173268,  1.71194558]), 'targetState': array([16, 26], dtype=int32), 'currentDistance': 0.8517927654964528}
episode index:4999
target Thresh 32.0
target distance 9.0
model initialize at round 4999
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 18.]), 'previousTarget': array([13., 18.]), 'currentState': array([ 4.        , 12.        ,  2.52740675]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 10.816653826391969}
done in step count: 28
reward sum = 0.5886320864289908
running average episode reward sum: 0.49641626681676415
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([13., 18.]), 'previousTarget': array([13., 18.]), 'currentState': array([12.09333394, 17.11787562,  0.56221271]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 1.264984888045574}

Process finished with exit code 0
