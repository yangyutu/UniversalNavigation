/home/yangyutu/anaconda3/bin/python /home/yangyutu/Dropbox/UniversalNavigationProject/activeParticleModel/Navigation/FreeSpace/Slider/DDPGHER_MLP.py
episode index:0
target Thresh 6.399999999999999
target distance 3.0
model initialize at round 0
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([15.153659  ,  5.00593348,  4.79748426]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 4.131112610681785}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 20, 'currentTarget': array([18.76265555,  7.81712085]), 'previousTarget': array([18.,  8.]), 'currentState': array([47.93564716,  0.82165368,  0.20052641]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 29.999999999999996}
episode index:1
target Thresh 6.6547242560212965
target distance 2.0
model initialize at round 1
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 25.]), 'previousTarget': array([17., 25.]), 'currentState': array([18.63540274, 23.32421054,  5.45325899]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 2.3415406109306387}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 20, 'currentTarget': array([17., 25.]), 'previousTarget': array([17., 25.]), 'currentState': array([17.06541211, 19.68020064,  4.43606269]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 5.320201494242074}
episode index:2
target Thresh 6.9069139633470655
target distance 6.0
model initialize at round 2
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 29.]), 'previousTarget': array([17., 29.]), 'currentState': array([19.28938234, 23.05349325,  4.07162249]), 'targetState': array([17, 29], dtype=int32), 'currentDistance': 6.371986661648262}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 20, 'currentTarget': array([17., 29.]), 'previousTarget': array([17., 29.]), 'currentState': array([16.94100295, 19.58721928,  2.70449626]), 'targetState': array([17, 29], dtype=int32), 'currentDistance': 9.412965603391001}
episode index:3
target Thresh 7.15659434115819
target distance 3.0
model initialize at round 3
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 28.]), 'previousTarget': array([14., 28.]), 'currentState': array([ 9.30316901, 28.21094273,  3.98192382]), 'targetState': array([14, 28], dtype=int32), 'currentDistance': 4.701565507153935}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 20, 'currentTarget': array([14., 28.]), 'previousTarget': array([14., 28.]), 'currentState': array([21.42513181, 34.11286391,  3.04858047]), 'targetState': array([14, 28], dtype=int32), 'currentDistance': 9.61767579105473}
episode index:4
target Thresh 7.403790357700526
target distance 3.0
model initialize at round 4
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([7.75453147, 7.18102428, 0.96875184]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 4.899019074059679}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.1234580281884576
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.90056494, 6.91648409, 0.54853589]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.9218624679069416}
episode index:5
target Thresh 7.648526732781718
target distance 3.0
model initialize at round 5
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 25.]), 'previousTarget': array([ 8., 25.]), 'currentState': array([ 7.51757923, 23.71478848,  2.09011731]), 'targetState': array([ 8, 25], dtype=int32), 'currentDistance': 1.3727703569071432}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.26298102515704797
{'scaleFactor': 20, 'currentTarget': array([ 8., 25.]), 'previousTarget': array([ 8., 25.]), 'currentState': array([ 7.40637147, 24.67490257,  1.43770942]), 'targetState': array([ 8, 25], dtype=int32), 'currentDistance': 0.6768184136862836}
episode index:6
target Thresh 7.890827940243231
target distance 5.0
model initialize at round 6
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  9.]), 'previousTarget': array([18.,  9.]), 'currentState': array([14.9818932 ,  8.09719539,  0.26864236]), 'targetState': array([18,  9], dtype=int32), 'currentDistance': 3.1502420280047416}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.36126802869175545
{'scaleFactor': 20, 'currentTarget': array([18.,  9.]), 'previousTarget': array([18.,  9.]), 'currentState': array([17.74315565,  9.47750339,  0.30571525]), 'targetState': array([18,  9], dtype=int32), 'currentDistance': 0.5421978493518839}
episode index:7
target Thresh 8.130718210407721
target distance 4.0
model initialize at round 7
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 23.]), 'previousTarget': array([13., 23.]), 'currentState': array([10.59631243, 21.76713824,  1.30006247]), 'targetState': array([13, 23], dtype=int32), 'currentDistance': 2.70141852452331}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.4104494360057401
{'scaleFactor': 20, 'currentTarget': array([13., 23.]), 'previousTarget': array([13., 23.]), 'currentState': array([13.46179588, 22.86329477,  3.6547198 ]), 'targetState': array([13, 23], dtype=int32), 'currentDistance': 0.48160539120828194}
episode index:8
target Thresh 8.368221532502123
target distance 7.0
model initialize at round 8
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 20.]), 'previousTarget': array([19., 20.]), 'currentState': array([24.87368752, 22.35120193,  4.01373102]), 'targetState': array([19, 20], dtype=int32), 'currentDistance': 6.326796630050407}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.45664045687572297
{'scaleFactor': 20, 'currentTarget': array([19., 20.]), 'previousTarget': array([19., 20.]), 'currentState': array([18.30572416, 19.43946045,  1.11987863]), 'targetState': array([19, 20], dtype=int32), 'currentDistance': 0.8923135831544097}
episode index:9
target Thresh 8.603361657056556
target distance 2.0
model initialize at round 9
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  5.]), 'previousTarget': array([22.,  5.]), 'currentState': array([18.37813348,  5.8695613 ,  3.35728383]), 'targetState': array([22,  5], dtype=int32), 'currentDistance': 3.724789112290381}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.48061773299310806
{'scaleFactor': 20, 'currentTarget': array([22.,  5.]), 'previousTarget': array([22.,  5.]), 'currentState': array([21.70300573,  4.10531749,  2.81781208]), 'targetState': array([22,  5], dtype=int32), 'currentDistance': 0.9426889107118187}
episode index:10
target Thresh 8.836162098279434
target distance 3.0
model initialize at round 10
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 16.]), 'previousTarget': array([20., 16.]), 'currentState': array([21.05400752, 15.55367398,  3.47027424]), 'targetState': array([20, 16], dtype=int32), 'currentDistance': 1.1446129333663841}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5242521218119164
{'scaleFactor': 20, 'currentTarget': array([20., 16.]), 'previousTarget': array([20., 16.]), 'currentState': array([20.2797708 , 15.76899034,  2.45387378]), 'targetState': array([20, 16], dtype=int32), 'currentDistance': 0.3628183585699436}
episode index:11
target Thresh 9.066646136408878
target distance 2.0
model initialize at round 11
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([10.04210254,  5.38838724,  3.17835033]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 1.735971439568373}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5630644449942567
{'scaleFactor': 20, 'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([8.98062714, 4.00154802, 4.95766211]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 0.019434608131935184}
episode index:12
target Thresh 9.294836820040768
target distance 8.0
model initialize at round 12
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 6.]), 'previousTarget': array([8., 6.]), 'currentState': array([13.74650095, 15.12299965,  3.42105186]), 'targetState': array([8, 6], dtype=int32), 'currentDistance': 10.781994054724683}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5820385544732567
{'scaleFactor': 20, 'currentTarget': array([8., 6.]), 'previousTarget': array([8., 6.]), 'currentState': array([8.37886622, 5.15346964, 2.01820256]), 'targetState': array([8, 6], dtype=int32), 'currentDistance': 0.9274444781438885}
episode index:13
target Thresh 9.52075696843363
target distance 6.0
model initialize at round 13
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 26.]), 'previousTarget': array([ 9., 26.]), 'currentState': array([16.49814707, 26.76719753,  1.48295563]), 'targetState': array([ 9, 26], dtype=int32), 'currentDistance': 7.537294043972264}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6063747073271611
{'scaleFactor': 20, 'currentTarget': array([ 9., 26.]), 'previousTarget': array([ 9., 26.]), 'currentState': array([ 9.09569059, 25.61340997,  3.14750279]), 'targetState': array([ 9, 26], dtype=int32), 'currentDistance': 0.3982568789716646}
episode index:14
target Thresh 9.744429173790568
target distance 3.0
model initialize at round 14
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  5.]), 'previousTarget': array([19.,  5.]), 'currentState': array([17.44646246,  4.38120084,  0.75748268]), 'targetState': array([19,  5], dtype=int32), 'currentDistance': 1.6722414021284666}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6319497268386838
{'scaleFactor': 20, 'currentTarget': array([19.,  5.]), 'previousTarget': array([19.,  5.]), 'currentState': array([18.65726035,  5.96214426,  1.08017472]), 'targetState': array([19,  5], dtype=int32), 'currentDistance': 1.0213677292230543}
episode index:15
target Thresh 9.965875803518518
target distance 5.0
model initialize at round 15
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 15.]), 'previousTarget': array([11., 15.]), 'currentState': array([14.43656227,  9.12313603,  0.46186906]), 'targetState': array([11, 15], dtype=int32), 'currentDistance': 6.807899119422117}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.652490119536266
{'scaleFactor': 20, 'currentTarget': array([11., 15.]), 'previousTarget': array([11., 15.]), 'currentState': array([11.04102966, 14.82056606,  3.64516827]), 'targetState': array([11, 15], dtype=int32), 'currentDistance': 0.184065133085518}
episode index:16
target Thresh 10.185119002464987
target distance 3.0
model initialize at round 16
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 24.]), 'previousTarget': array([14., 24.]), 'currentState': array([13.58456867, 26.08790762,  4.72253501]), 'targetState': array([14, 24], dtype=int32), 'currentDistance': 2.1288356968391233}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6723436419164857
{'scaleFactor': 20, 'currentTarget': array([14., 24.]), 'previousTarget': array([14., 24.]), 'currentState': array([13.82620747, 24.10679407,  4.9472333 ]), 'targetState': array([14, 24], dtype=int32), 'currentDistance': 0.20398239635931428}
episode index:17
target Thresh 10.402180695132575
target distance 9.0
model initialize at round 17
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 24.]), 'previousTarget': array([ 4., 24.]), 'currentState': array([13.79775521, 13.51038345,  6.20258689]), 'targetState': array([ 4, 24], dtype=int32), 'currentDistance': 14.353677668867329}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.6606146599999084
{'scaleFactor': 20, 'currentTarget': array([ 4., 24.]), 'previousTarget': array([ 4., 24.]), 'currentState': array([ 3.9826372 , 23.66893035,  0.18430062]), 'targetState': array([ 4, 24], dtype=int32), 'currentDistance': 0.3315246294232717}
episode index:18
target Thresh 10.617082587871437
target distance 4.0
model initialize at round 18
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([4.83187438, 7.53695086, 6.23932266]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 3.4896321775363064}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6774296778946501
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([8.00859097, 8.8479161 , 1.68664303]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.1523263473657629}
episode index:19
target Thresh 10.829846171049923
target distance 2.0
model initialize at round 19
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  6.]), 'previousTarget': array([23.,  6.]), 'currentState': array([21.99149751,  6.72500922,  0.87104613]), 'targetState': array([23,  6], dtype=int32), 'currentDistance': 1.2420610433161479}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6915879944999176
{'scaleFactor': 20, 'currentTarget': array([23.,  6.]), 'previousTarget': array([23.,  6.]), 'currentState': array([22.2540685 ,  6.20040844,  5.16627225]), 'targetState': array([23,  6], dtype=int32), 'currentDistance': 0.7723841925400834}
episode index:20
target Thresh 11.040492721203663
target distance 7.0
model initialize at round 20
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([14.84380559, 18.23493786,  1.32800412]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 7.540283880784845}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.7021560541658092
{'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([ 9.00197597, 23.69870219,  2.5323056 ]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 0.6987049834805013}
episode index:21
target Thresh 11.249043303163209
target distance 3.0
model initialize at round 21
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 23.]), 'previousTarget': array([ 6., 23.]), 'currentState': array([ 8.8029198 , 23.08021618,  4.14433795]), 'targetState': array([ 6, 23], dtype=int32), 'currentDistance': 2.8040674073713974}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.7130344221310451
{'scaleFactor': 20, 'currentTarget': array([ 6., 23.]), 'previousTarget': array([ 6., 23.]), 'currentState': array([ 5.3472339 , 23.44247578,  2.15152455]), 'targetState': array([ 6, 23], dtype=int32), 'currentDistance': 0.7885990135612249}
episode index:22
target Thresh 11.455518772160548
target distance 11.0
model initialize at round 22
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 12.]), 'previousTarget': array([16., 12.]), 'currentState': array([24.73957792, 21.88459738,  2.85622549]), 'targetState': array([16, 12], dtype=int32), 'currentDistance': 13.194145958738398}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.7221522600569962
{'scaleFactor': 20, 'currentTarget': array([16., 12.]), 'previousTarget': array([16., 12.]), 'currentState': array([16.75337027, 11.74314493,  4.90111387]), 'targetState': array([16, 12], dtype=int32), 'currentDistance': 0.7959530712103978}
episode index:23
target Thresh 11.659939775914648
target distance 2.0
model initialize at round 23
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 19.]), 'previousTarget': array([ 2., 19.]), 'currentState': array([ 2.04211876, 19.63295354,  3.48358637]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 0.6343533522119691}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.733729249221288
{'scaleFactor': 20, 'currentTarget': array([ 2., 19.]), 'previousTarget': array([ 2., 19.]), 'currentState': array([ 2.04211876, 19.63295354,  3.48358637]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 0.6343533522119691}
episode index:24
target Thresh 11.86232675669623
target distance 9.0
model initialize at round 24
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 19.]), 'previousTarget': array([26., 19.]), 'currentState': array([20.82577046, 26.79435782,  4.95018935]), 'targetState': array([26, 19], dtype=int32), 'currentDistance': 9.355461778838107}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.7409207691517821
{'scaleFactor': 20, 'currentTarget': array([26., 19.]), 'previousTarget': array([26., 19.]), 'currentState': array([25.08237335, 19.29135711,  0.83600974]), 'targetState': array([26, 19], dtype=int32), 'currentDistance': 0.9627708061709361}
episode index:25
target Thresh 12.062699953372032
target distance 10.0
model initialize at round 25
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 18.]), 'previousTarget': array([21., 18.]), 'currentState': array([ 9.7839869 , 12.83651013,  4.91491699]), 'targetState': array([21, 18], dtype=int32), 'currentDistance': 12.347492763800052}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.7468599031943566
{'scaleFactor': 20, 'currentTarget': array([21., 18.]), 'previousTarget': array([21., 18.]), 'currentState': array([21.69423099, 18.16832097,  2.48577246]), 'targetState': array([21, 18], dtype=int32), 'currentDistance': 0.7143448897211504}
episode index:26
target Thresh 12.261079403428703
target distance 10.0
model initialize at round 26
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([12.68283225, 13.0477011 ,  1.45976067]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 12.471376485085013}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.7537193641096392
{'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([ 4.56304137, 22.01967288,  2.86994636]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 0.563384954984329}
episode index:27
target Thresh 12.457484944976557
target distance 6.0
model initialize at round 27
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 12.]), 'previousTarget': array([14., 12.]), 'currentState': array([21.54665282,  9.66351738,  1.41525453]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 7.900070873542536}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.7607647457450092
{'scaleFactor': 20, 'currentTarget': array([14., 12.]), 'previousTarget': array([14., 12.]), 'currentState': array([14.1307385 , 12.97390467,  4.31067711]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 0.982640758310213}
episode index:28
target Thresh 12.651936218733425
target distance 7.0
model initialize at round 28
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 16.]), 'previousTarget': array([ 3., 16.]), 'currentState': array([7.67459414, 9.16769776, 1.10980957]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 8.278416772694948}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.7673242389917331
{'scaleFactor': 20, 'currentTarget': array([ 3., 16.]), 'previousTarget': array([ 3., 16.]), 'currentState': array([ 3.28985077, 16.65399872,  2.81948381]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 0.7153515197727914}
episode index:29
target Thresh 12.84445266998873
target distance 9.0
model initialize at round 29
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 18.]), 'previousTarget': array([26., 18.]), 'currentState': array([20.08356945,  7.31910609,  5.77206528]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 12.210063279826024}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.7715913728339658
{'scaleFactor': 20, 'currentTarget': array([26., 18.]), 'previousTarget': array([26., 18.]), 'currentState': array([25.36878135, 18.88096046,  3.51702593]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 1.0837565746095719}
episode index:30
target Thresh 13.035053550548021
target distance 1.0
model initialize at round 30
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 28.]), 'previousTarget': array([15., 28.]), 'currentState': array([14.09330321, 28.42145316,  3.58739659]), 'targetState': array([15, 28], dtype=int32), 'currentDistance': 0.9998609087148276}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7789593930651283
{'scaleFactor': 20, 'currentTarget': array([15., 28.]), 'previousTarget': array([15., 28.]), 'currentState': array([14.09330321, 28.42145316,  3.58739659]), 'targetState': array([15, 28], dtype=int32), 'currentDistance': 0.9998609087148276}
episode index:31
target Thresh 13.223757920658194
target distance 10.0
model initialize at round 31
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 14.]), 'previousTarget': array([ 5., 14.]), 'currentState': array([14.97505061, 10.80241519,  0.79537034]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 10.47502664648318}
done in step count: 85
reward sum = 0.4255901233886546
running average episode reward sum: 0.7679166033877385
{'scaleFactor': 20, 'currentTarget': array([ 5., 14.]), 'previousTarget': array([ 5., 14.]), 'currentState': array([ 5.96571521, 13.04273454,  0.79565907]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 1.359765798209294}
episode index:32
target Thresh 13.41058465091351
target distance 4.0
model initialize at round 32
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 29.]), 'previousTarget': array([25., 29.]), 'currentState': array([23.26351825, 26.16346578,  1.69472318]), 'targetState': array([25, 29], dtype=int32), 'currentDistance': 3.3258525574797133}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.760256618964589
{'scaleFactor': 20, 'currentTarget': array([25., 29.]), 'previousTarget': array([25., 29.]), 'currentState': array([24.07196725, 29.97593514,  3.91038454]), 'targetState': array([25, 29], dtype=int32), 'currentDistance': 1.3467346425683506}
episode index:33
target Thresh 13.595552424142689
target distance 8.0
model initialize at round 33
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 15.]), 'previousTarget': array([10., 15.]), 'currentState': array([16.24046821, 14.05063645,  3.98556411]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 6.312268584747381}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.765586722800954
{'scaleFactor': 20, 'currentTarget': array([10., 15.]), 'previousTarget': array([10., 15.]), 'currentState': array([ 9.37211696, 14.1416861 ,  6.09342736]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 1.063456563491462}
episode index:34
target Thresh 13.77867973727719
target distance 13.0
model initialize at round 34
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 18.]), 'previousTarget': array([18., 18.]), 'currentState': array([ 3.8951591 , 10.73000042,  5.0058341 ]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 15.868189273376009}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.7690380984842446
{'scaleFactor': 20, 'currentTarget': array([18., 18.]), 'previousTarget': array([18., 18.]), 'currentState': array([17.79053581, 18.48886699,  1.75131324]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 0.5318516495383466}
episode index:35
target Thresh 13.959984903200937
target distance 9.0
model initialize at round 35
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 28.]), 'previousTarget': array([ 2., 28.]), 'currentState': array([ 5.9238894 , 20.29406547,  1.25461042]), 'targetState': array([ 2, 28], dtype=int32), 'currentDistance': 8.647446728968175}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.7720515130540981
{'scaleFactor': 20, 'currentTarget': array([ 2., 28.]), 'previousTarget': array([ 2., 28.]), 'currentState': array([ 2.98245179, 27.05931054,  1.01057731]), 'targetState': array([ 2, 28], dtype=int32), 'currentDistance': 1.3601868176005691}
episode index:36
target Thresh 14.139486052581606
target distance 10.0
model initialize at round 36
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 12.]), 'previousTarget': array([ 4., 12.]), 'currentState': array([14.30438649, 14.65521508,  2.39893278]), 'targetState': array([ 4, 12], dtype=int32), 'currentDistance': 10.640984358562799}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.7766306653877981
{'scaleFactor': 20, 'currentTarget': array([ 4., 12.]), 'previousTarget': array([ 4., 12.]), 'currentState': array([ 4.20779119, 12.24654818,  2.97339079]), 'targetState': array([ 4, 12], dtype=int32), 'currentDistance': 0.32243322427262444}
episode index:37
target Thresh 14.317201135683721
target distance 11.0
model initialize at round 37
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([ 3.50824961, 23.39560902,  6.02917433]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 10.584165091671156}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.7781539047578584
{'scaleFactor': 20, 'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([13.09020515, 21.00141947,  5.5570634 ]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 1.3508848039734276}
episode index:38
target Thresh 14.49314792416369
target distance 7.0
model initialize at round 38
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  7.]), 'previousTarget': array([20.,  7.]), 'currentState': array([14.66945251, 12.2150851 ,  5.40178615]), 'targetState': array([20,  7], dtype=int32), 'currentDistance': 7.457335259734898}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.7828319074563748
{'scaleFactor': 20, 'currentTarget': array([20.,  7.]), 'previousTarget': array([20.,  7.]), 'currentState': array([20.18614615,  6.42642252,  5.4726877 ]), 'targetState': array([20,  7], dtype=int32), 'currentDistance': 0.6030269650528494}
episode index:39
target Thresh 14.667344012846986
target distance 5.0
model initialize at round 39
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 22.]), 'previousTarget': array([10., 22.]), 'currentState': array([13.18718478, 23.96240343,  2.39522874]), 'targetState': array([10, 22], dtype=int32), 'currentDistance': 3.742883118739244}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.7877636097699654
{'scaleFactor': 20, 'currentTarget': array([10., 22.]), 'previousTarget': array([10., 22.]), 'currentState': array([10.10651381, 22.30033859,  3.37978643]), 'targetState': array([10, 22], dtype=int32), 'currentDistance': 0.3186666945108661}
episode index:40
target Thresh 14.839806821487631
target distance 2.0
model initialize at round 40
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 8.]), 'previousTarget': array([7., 8.]), 'currentState': array([6.39822774, 5.06188527, 0.41769927]), 'targetState': array([7, 8], dtype=int32), 'currentDistance': 2.99910786819849}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.7924547412389906
{'scaleFactor': 20, 'currentTarget': array([7., 8.]), 'previousTarget': array([7., 8.]), 'currentState': array([7.50293086, 8.6274612 , 1.57446383]), 'targetState': array([7, 8], dtype=int32), 'currentDistance': 0.8041436489414437}
episode index:41
target Thresh 15.010553596510224
target distance 13.0
model initialize at round 41
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  5.]), 'previousTarget': array([21.,  5.]), 'currentState': array([13.43288384, 17.05162092,  0.36459869]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 14.230348337654775}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.7955568829815841
{'scaleFactor': 20, 'currentTarget': array([21.,  5.]), 'previousTarget': array([21.,  5.]), 'currentState': array([20.05078531,  5.60153727,  5.35768127]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 1.1237684877886904}
episode index:42
target Thresh 15.179601412734545
target distance 10.0
model initialize at round 42
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([4.74993623, 7.49335348, 6.18423653]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 10.28367177759329}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.7989504473169194
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.0805543 ,  2.34753469,  0.43760508]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.1274268849332834}
episode index:43
target Thresh 15.346967175083098
target distance 9.0
model initialize at round 43
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([13.59113221,  4.45165122,  0.67811888]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 9.847517883371093}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8021897587279213
{'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([16.24307081, 14.58517228,  1.48608997]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 0.6336481785718049}
episode index:44
target Thresh 15.512667620271579
target distance 10.0
model initialize at round 44
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 12.]), 'previousTarget': array([22., 12.]), 'currentState': array([13.66290043, 14.74005301,  5.11832917]), 'targetState': array([22, 12], dtype=int32), 'currentDistance': 8.775825876761132}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8048687572990324
{'scaleFactor': 20, 'currentTarget': array([22., 12.]), 'previousTarget': array([22., 12.]), 'currentState': array([21.03753185, 12.87057334,  3.8877126 ]), 'targetState': array([22, 12], dtype=int32), 'currentDistance': 1.297783836830395}
episode index:45
target Thresh 15.6767193184826
target distance 11.0
model initialize at round 45
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 28.]), 'previousTarget': array([ 7., 28.]), 'currentState': array([ 8.01020796, 18.51077457,  0.22323418]), 'targetState': array([ 7, 28], dtype=int32), 'currentDistance': 9.54284650366766}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8078385701708143
{'scaleFactor': 20, 'currentTarget': array([ 7., 28.]), 'previousTarget': array([ 7., 28.]), 'currentState': array([ 6.93742181, 28.41929622,  2.39378359]), 'targetState': array([ 7, 28], dtype=int32), 'currentDistance': 0.42394026878768476}
episode index:46
target Thresh 15.839138675022696
target distance 10.0
model initialize at round 46
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  6.]), 'previousTarget': array([13.,  6.]), 'currentState': array([19.40608684, 17.67611812,  2.27120459]), 'targetState': array([13,  6], dtype=int32), 'currentDistance': 13.318020980657963}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8102833813252208
{'scaleFactor': 20, 'currentTarget': array([13.,  6.]), 'previousTarget': array([13.,  6.]), 'currentState': array([13.12000407,  5.97627865,  4.31549904]), 'targetState': array([13,  6], dtype=int32), 'currentDistance': 0.12232612181455539}
episode index:47
target Thresh 15.999941931962859
target distance 15.0
model initialize at round 47
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 29.]), 'previousTarget': array([13., 29.]), 'currentState': array([18.66008157, 15.70474181,  0.46630049]), 'targetState': array([13, 29], dtype=int32), 'currentDistance': 14.449927811230918}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8122437707769622
{'scaleFactor': 20, 'currentTarget': array([13., 29.]), 'previousTarget': array([13., 29.]), 'currentState': array([12.12511367, 28.91894644,  3.21237537]), 'targetState': array([13, 29], dtype=int32), 'currentDistance': 0.8786328993634196}
episode index:48
target Thresh 16.159145169762795
target distance 10.0
model initialize at round 48
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 8.]), 'previousTarget': array([5., 8.]), 'currentState': array([11.32101394, 18.11573206,  4.08277178]), 'targetState': array([5, 8], dtype=int32), 'currentDistance': 11.928254375434795}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8146891090857382
{'scaleFactor': 20, 'currentTarget': array([5., 8.]), 'previousTarget': array([5., 8.]), 'currentState': array([4.86192682, 7.5761709 , 4.83849961]), 'targetState': array([5, 8], dtype=int32), 'currentDistance': 0.445752521078436}
episode index:49
target Thresh 16.316764308878945
target distance 10.0
model initialize at round 49
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([21.66749114,  9.77226906,  0.87426871]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 11.107525074935193}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8166656718536963
{'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([25.40748525, 20.95257551,  4.81998863]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 1.1218172042035937}
episode index:50
target Thresh 16.47281511135658
target distance 10.0
model initialize at round 50
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 28.]), 'previousTarget': array([22., 28.]), 'currentState': array([22.69582767, 19.6616209 ,  2.75130457]), 'targetState': array([22, 28], dtype=int32), 'currentDistance': 8.367361723637156}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8192994831879375
{'scaleFactor': 20, 'currentTarget': array([22., 28.]), 'previousTarget': array([22., 28.]), 'currentState': array([22.00154562, 28.44055823,  1.83433655]), 'targetState': array([22, 28], dtype=int32), 'currentDistance': 0.44056094518169175}
episode index:51
target Thresh 16.62731318240599
target distance 15.0
model initialize at round 51
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 26.]), 'previousTarget': array([26., 26.]), 'currentState': array([10.90842874, 18.31952305,  5.66795158]), 'targetState': array([26, 26], dtype=int32), 'currentDistance': 16.93355395097044}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8209356868768004
{'scaleFactor': 20, 'currentTarget': array([26., 26.]), 'previousTarget': array([26., 26.]), 'currentState': array([25.94483649, 25.91259193,  0.41293662]), 'targetState': array([26, 26], dtype=int32), 'currentDistance': 0.10335948558929493}
episode index:52
target Thresh 16.780273971963023
target distance 10.0
model initialize at round 52
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  7.]), 'previousTarget': array([24.,  7.]), 'currentState': array([27.47991821, 16.19374604,  4.77824295]), 'targetState': array([24,  7], dtype=int32), 'currentDistance': 9.830299943353197}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8223395089028743
{'scaleFactor': 20, 'currentTarget': array([24.,  7.]), 'previousTarget': array([24.,  7.]), 'currentState': array([23.51872754,  6.05548646,  5.86105791]), 'targetState': array([24,  7], dtype=int32), 'currentDistance': 1.0600608564184506}
episode index:53
target Thresh 16.931712776234107
target distance 12.0
model initialize at round 53
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 27.]), 'previousTarget': array([27., 27.]), 'currentState': array([16.68066068, 21.87591227,  5.20329076]), 'targetState': array([27, 27], dtype=int32), 'currentDistance': 11.52150333577958}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.824371468884432
{'scaleFactor': 20, 'currentTarget': array([27., 27.]), 'previousTarget': array([27., 27.]), 'currentState': array([27.49015502, 26.01127313,  0.98347449]), 'targetState': array([27, 27], dtype=int32), 'currentDistance': 1.1035546069675062}
episode index:54
target Thresh 17.081644739225865
target distance 6.0
model initialize at round 54
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([19.97198212,  5.66559169,  3.04849541]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 5.641452408914129}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8270246967228968
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.77737876,  3.90653997,  3.95618975]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.9334746562437339}
episode index:55
target Thresh 17.23008485425954
target distance 7.0
model initialize at round 55
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 12.]), 'previousTarget': array([ 2., 12.]), 'currentState': array([ 7.74879214, 10.12556861,  3.41898775]), 'targetState': array([ 2, 12], dtype=int32), 'currentDistance': 6.046660579601336}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8295831664242737
{'scaleFactor': 20, 'currentTarget': array([ 2., 12.]), 'previousTarget': array([ 2., 12.]), 'currentState': array([ 2.98320646, 12.16314631,  2.41968369]), 'targetState': array([ 2, 12], dtype=int32), 'currentDistance': 0.9966502209345034}
episode index:56
target Thresh 17.37704796547034
target distance 2.0
model initialize at round 56
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 4.]), 'previousTarget': array([7., 4.]), 'currentState': array([5.89197802, 4.23975953, 5.4676342 ]), 'targetState': array([7, 4], dtype=int32), 'currentDistance': 1.1336654490782834}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8318816373641987
{'scaleFactor': 20, 'currentTarget': array([7., 4.]), 'previousTarget': array([7., 4.]), 'currentState': array([6.16937116, 4.57231206, 3.28293097]), 'targetState': array([7, 4], dtype=int32), 'currentDistance': 1.0087047905494102}
episode index:57
target Thresh 17.522548769291852
target distance 5.0
model initialize at round 57
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([ 2.1829805 , 12.20751944,  4.02321994]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 3.686426563763646}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8344371263751608
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.9079405 , 9.15553306, 4.8683424 ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.18073594915106117}
episode index:58
target Thresh 17.66660181592571
target distance 15.0
model initialize at round 58
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 28.]), 'previousTarget': array([17., 28.]), 'currentState': array([ 3.36421788, 17.98554796,  1.63561612]), 'targetState': array([17, 28], dtype=int32), 'currentDistance': 16.91815012345206}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8348713844813663
{'scaleFactor': 20, 'currentTarget': array([17., 28.]), 'previousTarget': array([17., 28.]), 'currentState': array([17.47354161, 27.27939706,  1.59683173]), 'targetState': array([17, 28], dtype=int32), 'currentDistance': 0.862270407847305}
episode index:59
target Thresh 17.80922151079662
target distance 15.0
model initialize at round 59
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 26.]), 'previousTarget': array([26., 26.]), 'currentState': array([15.53542713, 12.59616632,  2.25612876]), 'targetState': array([26, 26], dtype=int32), 'currentDistance': 17.005000516810703}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8354359582861599
{'scaleFactor': 20, 'currentTarget': array([26., 26.]), 'previousTarget': array([26., 26.]), 'currentState': array([25.05131778, 25.32900206,  4.32524412]), 'targetState': array([26, 26], dtype=int32), 'currentDistance': 1.161996634403199}
episode index:60
target Thresh 17.95042211599292
target distance 15.0
model initialize at round 60
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([19.30403587, 11.46767092,  3.27606368]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 14.311679085188338}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8367159794205448
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.04402977, 11.86215516,  3.06239234]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.2873191517474816}
episode index:61
target Thresh 18.090217751692805
target distance 14.0
model initialize at round 61
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 27.]), 'previousTarget': array([25., 27.]), 'currentState': array([14.78183659, 14.66877003,  0.69079244]), 'targetState': array([25, 27], dtype=int32), 'currentDistance': 16.014683764863033}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8378073680590651
{'scaleFactor': 20, 'currentTarget': array([25., 27.]), 'previousTarget': array([25., 27.]), 'currentState': array([25.96598225, 27.80865169,  1.32559015]), 'targetState': array([25, 27], dtype=int32), 'currentDistance': 1.259777461833727}
episode index:62
target Thresh 18.22862239757633
target distance 8.0
model initialize at round 62
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 14.]), 'previousTarget': array([ 2., 14.]), 'currentState': array([11.42943156, 12.88832129,  1.56604236]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 9.494735860239063}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8394529677629053
{'scaleFactor': 20, 'currentTarget': array([ 2., 14.]), 'previousTarget': array([ 2., 14.]), 'currentState': array([ 1.66423641, 13.63091826,  3.52132856]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 0.498957425929}
episode index:63
target Thresh 18.36564989422343
target distance 17.0
model initialize at round 63
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([21.15011615, 18.54738191,  3.17302632]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 16.71959966779067}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8404674850636226
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.12269369,  3.1045746 ,  4.42722559]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.16121286854094022}
episode index:64
target Thresh 18.50131394449796
target distance 17.0
model initialize at round 64
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([20.79424288, 19.48376851,  2.08931743]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 23.587754596811365}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8406365663871803
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.88615643, 4.34284558, 4.2807288 ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.3612526112880181}
episode index:65
target Thresh 18.63562811491799
target distance 10.0
model initialize at round 65
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([24.29172693,  9.44418764,  4.25805259]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 8.43642623678724}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8423085888646473
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.75738337, 11.23589057,  2.80321413]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.3383891126910827}
episode index:66
target Thresh 18.7686058370125
target distance 17.0
model initialize at round 66
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 2.54915069, 25.37854294,  5.45118761]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 16.00967340644322}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8432350588070974
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([6.17156983, 9.62185959, 6.07900667]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.9106518027088119}
episode index:67
target Thresh 18.900260408664515
target distance 11.0
model initialize at round 67
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([11.5422154 , 19.62045307,  2.85460711]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 9.505489406983763}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8448196910290519
{'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([10.07357378, 28.11116778,  0.81865931]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 0.8918720862463113}
episode index:68
target Thresh 19.030604995440907
target distance 12.0
model initialize at round 68
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  8.]), 'previousTarget': array([25.,  8.]), 'currentState': array([12.89616557,  3.32023618,  5.66065264]), 'targetState': array([25,  8], dtype=int32), 'currentDistance': 12.977018041258116}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.845293623376442
{'scaleFactor': 20, 'currentTarget': array([25.,  8.]), 'previousTarget': array([25.,  8.]), 'currentState': array([25.85855806,  7.35175745,  1.83340985]), 'targetState': array([25,  8], dtype=int32), 'currentDistance': 1.0757975364778753}
episode index:69
target Thresh 19.159652631908976
target distance 7.0
model initialize at round 69
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([14.94548657, 19.7090943 ,  0.63879287]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 5.637251205030289}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8470794144710642
{'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([13.89955387, 24.89847561,  2.52616501]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 0.9052648010359897}
episode index:70
target Thresh 19.287416222939918
target distance 6.0
model initialize at round 70
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([25.60963732, 10.49138151,  1.30628889]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 6.585921289407554}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8486782397602042
{'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([21.99874951, 16.66116391,  2.21206805]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 0.6611650928444467}
episode index:71
target Thresh 19.413908544999295
target distance 14.0
model initialize at round 71
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 29.]), 'previousTarget': array([14., 29.]), 'currentState': array([ 6.88731953, 13.56994681,  6.27773142]), 'targetState': array([14, 29], dtype=int32), 'currentDistance': 16.990490426419505}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.849451904138657
{'scaleFactor': 20, 'currentTarget': array([14., 29.]), 'previousTarget': array([14., 29.]), 'currentState': array([14.03703147, 28.65900046,  1.04041272]), 'targetState': array([14, 29], dtype=int32), 'currentDistance': 0.34300440111094627}
episode index:72
target Thresh 19.539142247424724
target distance 7.0
model initialize at round 72
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 24.]), 'previousTarget': array([20., 24.]), 'currentState': array([16.38744474, 18.5681034 ,  0.93409204]), 'targetState': array([20, 24], dtype=int32), 'currentDistance': 6.523500299397224}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8509744261367577
{'scaleFactor': 20, 'currentTarget': array([20., 24.]), 'previousTarget': array([20., 24.]), 'currentState': array([20.97210557, 24.60999232,  1.18097477]), 'targetState': array([20, 24], dtype=int32), 'currentDistance': 1.1476410063414397}
episode index:73
target Thresh 19.66312985369082
target distance 16.0
model initialize at round 73
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'previousTarget': array([7., 3.]), 'currentState': array([11.25789016, 17.33690573,  3.85623109]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 14.955818078684555}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8514529456716141
{'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'previousTarget': array([7., 3.]), 'currentState': array([7.35182133, 2.28694391, 1.45010432]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 0.7951271855649618}
episode index:74
target Thresh 19.78588376266152
target distance 7.0
model initialize at round 74
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 17.]), 'previousTarget': array([ 9., 17.]), 'currentState': array([ 3.56501713, 10.69563607,  5.74225587]), 'targetState': array([ 9, 17], dtype=int32), 'currentDistance': 8.32370370573387}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8525277777014192
{'scaleFactor': 20, 'currentTarget': array([ 9., 17.]), 'previousTarget': array([ 9., 17.]), 'currentState': array([ 8.40552485, 17.88424333,  3.79609698]), 'targetState': array([ 9, 17], dtype=int32), 'currentDistance': 1.0654984606365436}
episode index:75
target Thresh 19.90741624983002
target distance 2.0
model initialize at round 75
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([16.32876161, 21.22996341,  2.86515284]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 0.4012073850816196}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8544682016790321
{'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([16.32876161, 21.22996341,  2.86515284]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 0.4012073850816196}
episode index:76
target Thresh 20.027739468546322
target distance 16.0
model initialize at round 76
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 25.]), 'previousTarget': array([11., 25.]), 'currentState': array([25.47590795, 20.71381475,  3.71358073]), 'targetState': array([11, 25], dtype=int32), 'currentDistance': 15.097128697076812}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8552350724037673
{'scaleFactor': 20, 'currentTarget': array([11., 25.]), 'previousTarget': array([11., 25.]), 'currentState': array([11.01321151, 24.98836107,  2.76165986]), 'targetState': array([11, 25], dtype=int32), 'currentDistance': 0.017607064058461398}
episode index:77
target Thresh 20.14686545123256
target distance 18.0
model initialize at round 77
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 20.]), 'previousTarget': array([22., 20.]), 'currentState': array([20.37008334,  3.64177538,  2.35908544]), 'targetState': array([22, 20], dtype=int32), 'currentDistance': 16.439225680515467}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8556344288052079
{'scaleFactor': 20, 'currentTarget': array([22., 20.]), 'previousTarget': array([22., 20.]), 'currentState': array([21.25227794, 20.53792179,  4.97638692]), 'targetState': array([22, 20], dtype=int32), 'currentDistance': 0.9211124384779069}
episode index:78
target Thresh 20.264806110586278
target distance 17.0
model initialize at round 78
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 8.]), 'previousTarget': array([9., 8.]), 'currentState': array([25.6959584 , 22.34259392,  5.53744781]), 'targetState': array([9, 8], dtype=int32), 'currentDistance': 22.010566259225044}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.855690427866424
{'scaleFactor': 20, 'currentTarget': array([9., 8.]), 'previousTarget': array([9., 8.]), 'currentState': array([9.09222964, 7.95807036, 2.50020558]), 'targetState': array([9, 8], dtype=int32), 'currentDistance': 0.10131338059807513}
episode index:79
target Thresh 20.38157324077169
target distance 17.0
model initialize at round 79
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 22.]), 'previousTarget': array([ 9., 22.]), 'currentState': array([24.31876336, 10.07636384,  2.08620262]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 19.41230563781485}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8557450269511098
{'scaleFactor': 20, 'currentTarget': array([ 9., 22.]), 'previousTarget': array([ 9., 22.]), 'currentState': array([ 8.09059372, 22.3352035 ,  4.68620491]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 0.9692167849924113}
episode index:80
target Thresh 20.497178518599128
target distance 7.0
model initialize at round 80
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 28.]), 'previousTarget': array([ 4., 28.]), 'currentState': array([10.01537842, 26.63428405,  3.07886839]), 'targetState': array([ 4, 28], dtype=int32), 'currentDistance': 6.1684647676328055}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8571592735319602
{'scaleFactor': 20, 'currentTarget': array([ 4., 28.]), 'previousTarget': array([ 4., 28.]), 'currentState': array([ 4.48462192, 28.34466278,  2.67847997]), 'targetState': array([ 4, 28], dtype=int32), 'currentDistance': 0.5946854912443695}
episode index:81
target Thresh 20.611633504692705
target distance 16.0
model initialize at round 81
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  3.]), 'previousTarget': array([20.,  3.]), 'currentState': array([5.5540297 , 5.35395064, 0.61601132]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 14.636500316059179}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8578465658972245
{'scaleFactor': 20, 'currentTarget': array([20.,  3.]), 'previousTarget': array([20.,  3.]), 'currentState': array([20.3624498 ,  3.22664567,  6.22664106]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 0.42747879218550017}
episode index:82
target Thresh 20.724949644646415
target distance 5.0
model initialize at round 82
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  2.]), 'previousTarget': array([12.,  2.]), 'currentState': array([14.43173557,  6.38931537,  4.52292871]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 5.017910647948517}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8593194988382218
{'scaleFactor': 20, 'currentTarget': array([12.,  2.]), 'previousTarget': array([12.,  2.]), 'currentState': array([12.56323297,  2.88719492,  4.23087399]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 1.0508787794264778}
episode index:83
target Thresh 20.837138270168687
target distance 10.0
model initialize at round 83
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([27.05966715, 21.68191198,  2.54533541]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 10.064694885936797}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.860297601821112
{'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([16.86336401, 21.97542364,  2.6781891 ]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 0.13882863880126348}
episode index:84
target Thresh 20.94821060021556
target distance 17.0
model initialize at round 84
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 15.]), 'previousTarget': array([ 6., 15.]), 'currentState': array([24.42031819, 19.90284276,  1.5762096 ]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 19.06163658539947}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8603969925381457
{'scaleFactor': 20, 'currentTarget': array([ 6., 15.]), 'previousTarget': array([ 6., 15.]), 'currentState': array([ 6.18672472, 14.1751453 ,  0.65696015]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 0.8457253693040573}
episode index:85
target Thresh 21.058177742112598
target distance 11.0
model initialize at round 85
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([14.52623464, 16.18734771,  4.65551949]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 14.396276930888048}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8611219658159338
{'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([26.12514514,  9.21902655,  5.28176308]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 0.9018556771588655}
episode index:86
target Thresh 21.16705069266563
target distance 18.0
model initialize at round 86
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 21.]), 'previousTarget': array([22., 21.]), 'currentState': array([23.47869424,  4.60026028,  2.89561635]), 'targetState': array([22, 21], dtype=int32), 'currentDistance': 16.46626853777997}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8614123440446717
{'scaleFactor': 20, 'currentTarget': array([22., 21.]), 'previousTarget': array([22., 21.]), 'currentState': array([21.25000067, 21.66820212,  4.06748083]), 'targetState': array([22, 21], dtype=int32), 'currentDistance': 1.004486470031538}
episode index:87
target Thresh 21.274840339260443
target distance 20.0
model initialize at round 87
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 19.]), 'previousTarget': array([ 3., 19.]), 'currentState': array([21.1259315 ,  5.1367256 ,  3.68284106]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 22.819723264592028}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8612992238975149
{'scaleFactor': 20, 'currentTarget': array([ 3., 19.]), 'previousTarget': array([ 3., 19.]), 'currentState': array([ 2.00573068, 19.06727887,  4.70397547]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 0.996542986699789}
episode index:88
target Thresh 21.381557460951516
target distance 13.0
model initialize at round 88
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([10.52901922, 20.1485589 ,  2.68454778]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 14.292432041066974}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8616816849128093
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.25730669, 7.1118685 , 1.51554586]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.9246536079935487}
episode index:89
target Thresh 21.487212729539955
target distance 9.0
model initialize at round 89
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([21.3229735 , 18.85868404,  4.23565984]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 8.78821662651199}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8626740000793336
{'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([14.06456101, 14.04341477,  3.2917074 ]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 0.07780081275167569}
episode index:90
target Thresh 21.591816710640664
target distance 13.0
model initialize at round 90
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 17.]), 'previousTarget': array([23., 17.]), 'currentState': array([ 9.91691686, 12.68097542,  0.61027789]), 'targetState': array([23, 17], dtype=int32), 'currentDistance': 13.777555577747659}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.863132330573064
{'scaleFactor': 20, 'currentTarget': array([23., 17.]), 'previousTarget': array([23., 17.]), 'currentState': array([22.90995706, 17.82946325,  3.55941206]), 'targetState': array([23, 17], dtype=int32), 'currentDistance': 0.8343362694286623}
episode index:91
target Thresh 21.695379864738918
target distance 21.0
model initialize at round 91
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 28.]), 'previousTarget': array([ 6., 28.]), 'currentState': array([16.40500823,  7.53701894,  1.80682349]), 'targetState': array([ 6, 28], dtype=int32), 'currentDistance': 22.95643243351539}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8630054331874315
{'scaleFactor': 20, 'currentTarget': array([ 6., 28.]), 'previousTarget': array([ 6., 28.]), 'currentState': array([ 6.20156228, 28.35040312,  1.15623084]), 'targetState': array([ 6, 28], dtype=int32), 'currentDistance': 0.40423965554655855}
episode index:92
target Thresh 21.79791254823644
target distance 5.0
model initialize at round 92
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 20.]), 'previousTarget': array([15., 20.]), 'currentState': array([15.27791287, 16.09513851,  1.71852833]), 'targetState': array([15, 20], dtype=int32), 'currentDistance': 3.9147386622828266}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8642645145510074
{'scaleFactor': 20, 'currentTarget': array([15., 20.]), 'previousTarget': array([15., 20.]), 'currentState': array([14.93968416, 19.96077369,  1.04980804]), 'targetState': array([15, 20], dtype=int32), 'currentDistance': 0.07194931932998094}
episode index:93
target Thresh 21.89942501448701
target distance 17.0
model initialize at round 93
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 6.]), 'previousTarget': array([5., 6.]), 'currentState': array([12.40925236, 21.36001342,  5.95462382]), 'targetState': array([5, 6], dtype=int32), 'currentDistance': 17.053651600116265}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8645950862500256
{'scaleFactor': 20, 'currentTarget': array([5., 6.]), 'previousTarget': array([5., 6.]), 'currentState': array([5.30408421, 5.92889251, 3.33830005]), 'targetState': array([5, 6], dtype=int32), 'currentDistance': 0.31228749938865563}
episode index:94
target Thresh 21.999927414821858
target distance 21.0
model initialize at round 94
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  2.]), 'previousTarget': array([26.,  2.]), 'currentState': array([3.9874112 , 2.65573366, 5.07680154]), 'targetState': array([26,  2], dtype=int32), 'currentDistance': 22.022353469336117}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8644567987220766
{'scaleFactor': 20, 'currentTarget': array([26.,  2.]), 'previousTarget': array([26.,  2.]), 'currentState': array([25.25476319,  2.2532859 ,  4.80776804]), 'targetState': array([26,  2], dtype=int32), 'currentDistance': 0.7871033296543093}
episode index:95
target Thresh 22.099429799564767
target distance 14.0
model initialize at round 95
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  2.]), 'previousTarget': array([17.,  2.]), 'currentState': array([4.22704205, 8.8475211 , 4.51983563]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 14.492653310982956}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8647784805505833
{'scaleFactor': 20, 'currentTarget': array([17.,  2.]), 'previousTarget': array([17.,  2.]), 'currentState': array([17.89922649,  2.67474241,  3.20035057]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 1.1242266705873258}
episode index:96
target Thresh 22.19794211903713
target distance 7.0
model initialize at round 96
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 12.]), 'previousTarget': array([13., 12.]), 'currentState': array([18.61392222,  9.9545676 ,  3.54849136]), 'targetState': array([13, 12], dtype=int32), 'currentDistance': 5.974940699883675}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8658663209572783
{'scaleFactor': 20, 'currentTarget': array([13., 12.]), 'previousTarget': array([13., 12.]), 'currentState': array([13.63048843, 11.56022046,  2.27076197]), 'targetState': array([13, 12], dtype=int32), 'currentDistance': 0.7687143185811545}
episode index:97
target Thresh 22.29547422455299
target distance 13.0
model initialize at round 97
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 15.]), 'previousTarget': array([27., 15.]), 'currentState': array([17.88813536,  3.26452342,  1.28347528]), 'targetState': array([27, 15], dtype=int32), 'currentDistance': 14.857573416497367}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8663525549014248
{'scaleFactor': 20, 'currentTarget': array([27., 15.]), 'previousTarget': array([27., 15.]), 'currentState': array([27.03571209, 15.50954154,  0.64008725]), 'targetState': array([27, 15], dtype=int32), 'currentDistance': 0.5107914764854901}
episode index:98
target Thresh 22.392035869404168
target distance 10.0
model initialize at round 98
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([22.13228466, 15.32223191,  3.78108025]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 11.635875458442142}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8670163204873396
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.48085125,  7.71331236,  3.16439299]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.8822300968228972}
episode index:99
target Thresh 22.487636709835627
target distance 13.0
model initialize at round 99
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 13.]), 'previousTarget': array([19., 13.]), 'currentState': array([19.33168683, 26.22162861,  4.01952004]), 'targetState': array([19, 13], dtype=int32), 'currentDistance': 13.225788417373675}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8675736042267453
{'scaleFactor': 20, 'currentTarget': array([19., 13.]), 'previousTarget': array([19., 13.]), 'currentState': array([19.25757998, 12.91802128,  4.10493132]), 'targetState': array([19, 13], dtype=int32), 'currentDistance': 0.27031085057298265}
episode index:100
target Thresh 22.582286306011078
target distance 14.0
model initialize at round 100
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 25.]), 'previousTarget': array([21., 25.]), 'currentState': array([18.19206394, 12.67200744,  0.44648182]), 'targetState': array([21, 25], dtype=int32), 'currentDistance': 12.64372988702752}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8679380445315181
{'scaleFactor': 20, 'currentTarget': array([21., 25.]), 'previousTarget': array([21., 25.]), 'currentState': array([20.20221514, 25.90225411,  5.06180266]), 'targetState': array([21, 25], dtype=int32), 'currentDistance': 1.2043766668728177}
episode index:101
target Thresh 22.675994122969
target distance 3.0
model initialize at round 101
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 19.]), 'previousTarget': array([17., 19.]), 'currentState': array([13.39621112, 22.54344735,  3.80733573]), 'targetState': array([17, 19], dtype=int32), 'currentDistance': 5.054039320372465}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8689415833106209
{'scaleFactor': 20, 'currentTarget': array([17., 19.]), 'previousTarget': array([17., 19.]), 'currentState': array([17.08075413, 19.20269721,  5.32568777]), 'targetState': array([17, 19], dtype=int32), 'currentDistance': 0.21819117891965234}
episode index:102
target Thresh 22.768769531569195
target distance 21.0
model initialize at round 102
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'previousTarget': array([19.,  3.]), 'currentState': array([18.03214578, 22.67069072,  4.36260599]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 19.694486920247527}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8690248788415756
{'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'previousTarget': array([19.,  3.]), 'currentState': array([19.23038652,  3.84742152,  3.67694188]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 0.8781806136804851}
episode index:103
target Thresh 22.860621809429826
target distance 4.0
model initialize at round 103
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([25.68535504, 25.05076587,  3.47728646]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 5.060557019500814}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8699986684680989
{'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([25.30794684, 19.61069465,  4.60605106]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 0.7940379264602382}
episode index:104
target Thresh 22.951560141855225
target distance 4.0
model initialize at round 104
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  6.]), 'previousTarget': array([11.,  6.]), 'currentState': array([8.27050647, 4.53208571, 0.70643723]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 3.099178485765999}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8710472525779265
{'scaleFactor': 20, 'currentTarget': array([11.,  6.]), 'previousTarget': array([11.,  6.]), 'currentState': array([11.63337876,  6.33847698,  6.0501153 ]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 0.7181471453494019}
episode index:105
target Thresh 23.041593622754423
target distance 9.0
model initialize at round 105
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([13.42611425, 22.59608679,  3.78952706]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 7.805509053362417}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8718014299111536
{'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([ 5.081017  , 25.41128401,  3.11324666]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 1.0068188993522504}
episode index:106
target Thresh 23.13073125555053
target distance 6.0
model initialize at round 106
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 19.]), 'previousTarget': array([ 9., 19.]), 'currentState': array([ 4.30907161, 21.05878516,  5.95442218]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 5.122831781403867}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8727219679493671
{'scaleFactor': 20, 'currentTarget': array([ 9., 19.]), 'previousTarget': array([ 9., 19.]), 'currentState': array([ 8.9968873 , 18.11618681,  5.34954268]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 0.8838186697207926}
episode index:107
target Thresh 23.21898195408111
target distance 3.0
model initialize at round 107
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 20.]), 'previousTarget': array([14., 20.]), 'currentState': array([17.53664603, 17.68637714,  1.43007987]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 4.226194018389629}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.873716208986873
{'scaleFactor': 20, 'currentTarget': array([14., 20.]), 'previousTarget': array([14., 20.]), 'currentState': array([14.58098551, 19.52908034,  2.03522986]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 0.7478699688237072}
episode index:108
target Thresh 23.306354543489558
target distance 11.0
model initialize at round 108
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([ 7.51646559, 11.73546282,  5.7288304 ]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 12.579348141379223}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8741660116055981
{'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([17.97452805, 20.73309557,  1.17936548]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 1.2194810532005056}
episode index:109
target Thresh 23.392857761107628
target distance 2.0
model initialize at round 109
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 15.]), 'previousTarget': array([14., 15.]), 'currentState': array([15.68137178, 16.65313036,  2.77021915]), 'targetState': array([14, 15], dtype=int32), 'currentDistance': 2.3579336408584903}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.875219047863729
{'scaleFactor': 20, 'currentTarget': array([14., 15.]), 'previousTarget': array([14., 15.]), 'currentState': array([14.17725853, 15.78251488,  4.58047408]), 'targetState': array([14, 15], dtype=int32), 'currentDistance': 0.8023404072893322}
episode index:110
target Thresh 23.478500257329163
target distance 17.0
model initialize at round 110
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 13.]), 'previousTarget': array([25., 13.]), 'currentState': array([8.82875505, 8.49206153, 2.0332103 ]), 'targetState': array([25, 13], dtype=int32), 'currentDistance': 16.787813214503526}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8754003019753955
{'scaleFactor': 20, 'currentTarget': array([25., 13.]), 'previousTarget': array([25., 13.]), 'currentState': array([25.06003687, 13.5521629 ,  5.77603307]), 'targetState': array([25, 13], dtype=int32), 'currentDistance': 0.555417225726613}
episode index:111
target Thresh 23.56329059647516
target distance 9.0
model initialize at round 111
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([12.35740208, 24.6335847 ,  4.37106991]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 12.942893002879725}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8758230197651502
{'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([22.77495148, 15.65087876,  6.07618624]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 0.8499620226415087}
episode index:112
target Thresh 23.64723725765019
target distance 21.0
model initialize at round 112
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 12.]), 'previousTarget': array([27., 12.]), 'currentState': array([7.60790526, 7.49701986, 1.30979365]), 'targetState': array([27, 12], dtype=int32), 'currentDistance': 19.90804280515827}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8759164874815306
{'scaleFactor': 20, 'currentTarget': array([27., 12.]), 'previousTarget': array([27., 12.]), 'currentState': array([27.04845594, 11.04034156,  5.65253897]), 'targetState': array([27, 12], dtype=int32), 'currentDistance': 0.9608809981624628}
episode index:113
target Thresh 23.730348635590325
target distance 23.0
model initialize at round 113
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([ 4.91115566, 24.87379296,  0.44643968]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 26.395390859637544}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.875701937337788
{'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([25.19000054,  8.49499005,  4.64587131]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 0.9545334872941403}
episode index:114
target Thresh 23.812633041502618
target distance 7.0
model initialize at round 114
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 17.]), 'previousTarget': array([15., 17.]), 'currentState': array([14.19075213, 11.82772435,  2.04657788]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 5.235199856093877}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8765245204913724
{'scaleFactor': 20, 'currentTarget': array([15., 17.]), 'previousTarget': array([15., 17.]), 'currentState': array([14.68248379, 17.43062388,  1.65370315]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 0.5350266043917741}
episode index:115
target Thresh 23.894098703896233
target distance 12.0
model initialize at round 115
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([19.47555187,  4.59919164,  0.87772596]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 10.414022296428731}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8770844828095589
{'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([19.55923677, 14.67283688,  0.70492327]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 0.5489152312963359}
episode index:116
target Thresh 23.974753769405304
target distance 12.0
model initialize at round 116
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 26.]), 'previousTarget': array([25., 26.]), 'currentState': array([24.52163253, 13.28095722,  0.56855505]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 12.72803537918944}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8775544047334685
{'scaleFactor': 20, 'currentTarget': array([25., 26.]), 'previousTarget': array([25., 26.]), 'currentState': array([25.55269987, 25.04769058,  1.36653077]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 1.1010769206141144}
episode index:117
target Thresh 24.054606303603585
target distance 6.0
model initialize at round 117
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([ 5.15048214, 15.54717259,  5.19326997]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 7.409024078657377}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8782581471509815
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.89891345, 11.64944906,  4.94034773]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.6572690273019851}
episode index:118
target Thresh 24.13366429181104
target distance 11.0
model initialize at round 118
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 22.]), 'previousTarget': array([23., 22.]), 'currentState': array([19.12437534, 12.32806717,  6.24128098]), 'targetState': array([23, 22], dtype=int32), 'currentDistance': 10.419536991276333}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8787894244808137
{'scaleFactor': 20, 'currentTarget': array([23., 22.]), 'previousTarget': array([23., 22.]), 'currentState': array([22.65293137, 21.89436309,  0.92275417]), 'targetState': array([23, 22], dtype=int32), 'currentDistance': 0.36278890427532884}
episode index:119
target Thresh 24.211935639892374
target distance 15.0
model initialize at round 119
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 17.]), 'previousTarget': array([17., 17.]), 'currentState': array([4.15815575, 3.88690594, 2.06269187]), 'targetState': array([17, 17], dtype=int32), 'currentDistance': 18.353915104539528}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8789273313956295
{'scaleFactor': 20, 'currentTarget': array([17., 17.]), 'previousTarget': array([17., 17.]), 'currentState': array([16.09884398, 17.235582  ,  5.80095303]), 'targetState': array([17, 17], dtype=int32), 'currentDistance': 0.9314403117680607}
episode index:120
target Thresh 24.289428175047625
target distance 12.0
model initialize at round 120
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 4.]), 'previousTarget': array([6., 4.]), 'currentState': array([17.29817839,  4.47034429,  3.27223873]), 'targetState': array([6, 4], dtype=int32), 'currentDistance': 11.307964396260536}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8793664885568804
{'scaleFactor': 20, 'currentTarget': array([6., 4.]), 'previousTarget': array([6., 4.]), 'currentState': array([5.61019271, 4.93157566, 2.46318094]), 'targetState': array([6, 4], dtype=int32), 'currentDistance': 1.0098430251779777}
episode index:121
target Thresh 24.366149646594884
target distance 18.0
model initialize at round 121
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 23.]), 'previousTarget': array([27., 23.]), 'currentState': array([15.64496093,  4.64434339,  0.79706782]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 21.583953341995056}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8791377285776836
{'scaleFactor': 20, 'currentTarget': array([27., 23.]), 'previousTarget': array([27., 23.]), 'currentState': array([26.99832511, 23.43454554,  5.64562309]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 0.43454876439863643}
episode index:122
target Thresh 24.442107726745235
target distance 9.0
model initialize at round 122
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 15.]), 'previousTarget': array([17., 15.]), 'currentState': array([23.7035315 , 22.02435599,  4.64616956]), 'targetState': array([17, 15], dtype=int32), 'currentDistance': 9.709732826911189}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8797218937916861
{'scaleFactor': 20, 'currentTarget': array([17., 15.]), 'previousTarget': array([17., 15.]), 'currentState': array([17.94966345, 15.68777485,  3.3375342 ]), 'targetState': array([17, 15], dtype=int32), 'currentDistance': 1.17255912941508}
episode index:123
target Thresh 24.51731001137
target distance 3.0
model initialize at round 123
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  6.]), 'previousTarget': array([11.,  6.]), 'currentState': array([9.88557192, 7.32049387, 5.65364134]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 1.7279045179912553}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8806112333578822
{'scaleFactor': 20, 'currentTarget': array([11.,  6.]), 'previousTarget': array([11.,  6.]), 'currentState': array([11.46461275,  6.09357833,  5.59098533]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 0.47394294290543676}
episode index:124
target Thresh 24.591764020760305
target distance 11.0
model initialize at round 124
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 17.]), 'previousTarget': array([ 9., 17.]), 'currentState': array([14.90236263, 26.72423345,  5.01190257]), 'targetState': array([ 9, 17], dtype=int32), 'currentDistance': 11.375350579713043}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.881022866274275
{'scaleFactor': 20, 'currentTarget': array([ 9., 17.]), 'previousTarget': array([ 9., 17.]), 'currentState': array([ 9.03686384, 16.14481846,  4.65547475]), 'targetState': array([ 9, 17], dtype=int32), 'currentDistance': 0.8559757034539555}
episode index:125
target Thresh 24.665477200379133
target distance 9.0
model initialize at round 125
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  2.]), 'previousTarget': array([19.,  2.]), 'currentState': array([ 9.3209281 , 11.11447989,  4.08351755]), 'targetState': array([19,  2], dtype=int32), 'currentDistance': 13.29504330312759}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.881353991894542
{'scaleFactor': 20, 'currentTarget': array([19.,  2.]), 'previousTarget': array([19.,  2.]), 'currentState': array([18.46543587,  1.3734189 ,  4.80593726]), 'targetState': array([19,  2], dtype=int32), 'currentDistance': 0.823627758126847}
episode index:126
target Thresh 24.738456921605877
target distance 13.0
model initialize at round 126
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 19.]), 'previousTarget': array([ 4., 19.]), 'currentState': array([16.34783232,  7.55147201,  2.97872531]), 'targetState': array([ 4, 19], dtype=int32), 'currentDistance': 16.838579398075225}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8815353153836306
{'scaleFactor': 20, 'currentTarget': array([ 4., 19.]), 'previousTarget': array([ 4., 19.]), 'currentState': array([ 4.35551515, 19.66963822,  2.16841885]), 'targetState': array([ 4, 19], dtype=int32), 'currentDistance': 0.7581598595867611}
episode index:127
target Thresh 24.81071048247348
target distance 10.0
model initialize at round 127
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([26.97570253, 13.37127432,  1.96237379]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 12.219518559638567}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8819300812627193
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.90629795,  7.85710098,  2.94888639]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.9174944689693615}
episode index:128
target Thresh 24.88224510839823
target distance 4.0
model initialize at round 128
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 7.]), 'previousTarget': array([7., 7.]), 'currentState': array([ 8.43206166, 11.61152061,  3.77972293]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 4.828759980368859}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8826151116405276
{'scaleFactor': 20, 'currentTarget': array([7., 7.]), 'previousTarget': array([7., 7.]), 'currentState': array([6.49336863, 6.95591251, 3.88939713]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 0.5085460145223546}
episode index:129
target Thresh 24.95306795290234
target distance 14.0
model initialize at round 129
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'previousTarget': array([25.,  3.]), 'currentState': array([11.75944964,  1.49812639,  6.19056082]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 13.325456769571026}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8828528203777823
{'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'previousTarget': array([25.,  3.]), 'currentState': array([24.8656974,  2.8415277,  5.3937138]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 0.20772736571802072}
episode index:130
target Thresh 25.023186098329276
target distance 19.0
model initialize at round 130
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([ 5.68249948, 23.03982019,  5.2968492 ]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 19.768423423863236}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8826131635130273
{'scaleFactor': 20, 'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([11.19409818,  4.83417569,  5.81095181]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 0.8564596775430774}
episode index:131
target Thresh 25.092606556552017
target distance 8.0
model initialize at round 131
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 12.]), 'previousTarget': array([19., 12.]), 'currentState': array([25.10491535,  8.78479889,  3.79119229]), 'targetState': array([19, 12], dtype=int32), 'currentDistance': 6.899819531235387}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8832039426530802
{'scaleFactor': 20, 'currentTarget': array([19., 12.]), 'previousTarget': array([19., 12.]), 'currentState': array([19.35209849, 12.34213699,  2.17698577]), 'targetState': array([19, 12], dtype=int32), 'currentDistance': 0.4909491503018238}
episode index:132
target Thresh 25.16133626967423
target distance 14.0
model initialize at round 132
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 26.]), 'previousTarget': array([10., 26.]), 'currentState': array([22.00116954, 17.05675281,  3.14658636]), 'targetState': array([10, 26], dtype=int32), 'currentDistance': 14.966954958971801}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8834318622382724
{'scaleFactor': 20, 'currentTarget': array([10., 26.]), 'previousTarget': array([10., 26.]), 'currentState': array([ 9.34223361, 25.20541168,  1.79717922]), 'targetState': array([10, 26], dtype=int32), 'currentDistance': 1.0315169578824497}
episode index:133
target Thresh 25.229382110724508
target distance 24.0
model initialize at round 133
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.62906004, 20.79242184]), 'previousTarget': array([24.57730084, 21.75513824]), 'currentState': array([1.67293955, 0.34911553, 5.52680922]), 'targetState': array([26, 23], dtype=int32), 'currentDistance': 30.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8826437241573833
{'scaleFactor': 20, 'currentTarget': array([26., 23.]), 'previousTarget': array([26., 23.]), 'currentState': array([26.59389835, 22.53619206,  5.07094388]), 'targetState': array([26, 23], dtype=int32), 'currentDistance': 0.7535469876386643}
episode index:134
target Thresh 25.296750884343655
target distance 16.0
model initialize at round 134
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 19.]), 'previousTarget': array([24., 19.]), 'currentState': array([13.67613999,  4.54118088,  0.14737391]), 'targetState': array([24, 19], dtype=int32), 'currentDistance': 17.766247093871577}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8827377577136895
{'scaleFactor': 20, 'currentTarget': array([24., 19.]), 'previousTarget': array([24., 19.]), 'currentState': array([24.27649869, 18.76820324,  6.23815713]), 'targetState': array([24, 19], dtype=int32), 'currentDistance': 0.36080640285150894}
episode index:135
target Thresh 25.363449327465176
target distance 6.0
model initialize at round 135
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 17.]), 'previousTarget': array([14., 17.]), 'currentState': array([ 9.73958398, 21.88476928,  4.87595272]), 'targetState': array([14, 17], dtype=int32), 'currentDistance': 6.481675370323944}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8833102448628536
{'scaleFactor': 20, 'currentTarget': array([14., 17.]), 'previousTarget': array([14., 17.]), 'currentState': array([14.66919579, 16.86034678,  5.27158633]), 'targetState': array([14, 17], dtype=int32), 'currentDistance': 0.6836124878571361}
episode index:136
target Thresh 25.42948410998897
target distance 19.0
model initialize at round 136
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  5.]), 'previousTarget': array([26.,  5.]), 'currentState': array([ 8.67038601, 19.2054231 ,  5.39555031]), 'targetState': array([26,  5], dtype=int32), 'currentDistance': 22.407801463445825}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8830155948520585
{'scaleFactor': 20, 'currentTarget': array([26.,  5.]), 'previousTarget': array([26.,  5.]), 'currentState': array([25.95656385,  5.36271469,  4.04839814]), 'targetState': array([26,  5], dtype=int32), 'currentDistance': 0.36530623275385615}
episode index:137
target Thresh 25.49486183544831
target distance 6.0
model initialize at round 137
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  3.]), 'previousTarget': array([11.,  3.]), 'currentState': array([7.25709528, 7.68348429, 4.72274208]), 'targetState': array([11,  3], dtype=int32), 'currentDistance': 5.9953616056741295}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8836480832951595
{'scaleFactor': 20, 'currentTarget': array([11.,  3.]), 'previousTarget': array([11.,  3.]), 'currentState': array([10.58660769,  3.83636775,  5.29401475]), 'targetState': array([11,  3], dtype=int32), 'currentDistance': 0.9329545646295669}
episode index:138
target Thresh 25.559589041670232
target distance 9.0
model initialize at round 138
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 12.]), 'previousTarget': array([27., 12.]), 'currentState': array([20.5711311 ,  4.62742466,  0.81849003]), 'targetState': array([27, 12], dtype=int32), 'currentDistance': 9.781882357349714}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.884064141324698
{'scaleFactor': 20, 'currentTarget': array([27., 12.]), 'previousTarget': array([27., 12.]), 'currentState': array([27.02041016, 12.58927999,  0.53415011]), 'targetState': array([27, 12], dtype=int32), 'currentDistance': 0.5896333433123605}
episode index:139
target Thresh 25.623672201429294
target distance 5.0
model initialize at round 139
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([5.16222797, 6.75656253, 2.42357737]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 3.247492033575421}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8847501117438074
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 4.69693198, 10.51688712,  1.3643726 ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.5991848787023438}
episode index:140
target Thresh 25.68711772309487
target distance 14.0
model initialize at round 140
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([ 8.44094156, 16.62418447,  0.29570925]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 13.573417849707232}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8849541339830969
{'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([22.97958073, 15.58921606,  5.00984574]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 1.0622249500596084}
episode index:141
target Thresh 25.74993195127201
target distance 14.0
model initialize at round 141
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 26.]), 'previousTarget': array([13., 26.]), 'currentState': array([25.62966941, 11.57980279,  0.75765436]), 'targetState': array([13, 26], dtype=int32), 'currentDistance': 19.16900198156388}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8849642096009351
{'scaleFactor': 20, 'currentTarget': array([13., 26.]), 'previousTarget': array([13., 26.]), 'currentState': array([12.93491496, 26.73490093,  1.55513636]), 'targetState': array([13, 26], dtype=int32), 'currentDistance': 0.7377773592339529}
episode index:142
target Thresh 25.812121167435865
target distance 22.0
model initialize at round 142
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 25.]), 'previousTarget': array([27., 25.]), 'currentState': array([4.3898781 , 8.56848314, 0.93177414]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 27.95017636793917}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.8846114092642159
{'scaleFactor': 20, 'currentTarget': array([27., 25.]), 'previousTarget': array([27., 25.]), 'currentState': array([27.66889055, 25.42211304,  6.25467416]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 0.7909449907258589}
episode index:143
target Thresh 25.873691590559883
target distance 21.0
model initialize at round 143
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 27.]), 'previousTarget': array([24., 27.]), 'currentState': array([ 3.62739751, 22.43834689,  6.10440493]), 'targetState': array([24, 27], dtype=int32), 'currentDistance': 20.877059451269425}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8844409019404456
{'scaleFactor': 20, 'currentTarget': array([24., 27.]), 'previousTarget': array([24., 27.]), 'currentState': array([23.64883199, 27.29379334,  5.03001466]), 'targetState': array([24, 27], dtype=int32), 'currentDistance': 0.45785751206990416}
episode index:144
target Thresh 25.93464937773768
target distance 11.0
model initialize at round 144
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 21.]), 'previousTarget': array([ 4., 21.]), 'currentState': array([15.27554653, 26.66025968,  2.41632953]), 'targetState': array([ 4, 21], dtype=int32), 'currentDistance': 12.616516520158088}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8847693463953872
{'scaleFactor': 20, 'currentTarget': array([ 4., 21.]), 'previousTarget': array([ 4., 21.]), 'currentState': array([ 4.30790226, 21.87191421,  3.42677888]), 'targetState': array([ 4, 21], dtype=int32), 'currentDistance': 0.9246827525380379}
episode index:145
target Thresh 25.99500062479878
target distance 5.0
model initialize at round 145
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 24.]), 'previousTarget': array([20., 24.]), 'currentState': array([16.94080786, 26.71618254,  0.1992827 ]), 'targetState': array([20, 24], dtype=int32), 'currentDistance': 4.091002827277196}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8854222960776106
{'scaleFactor': 20, 'currentTarget': array([20., 24.]), 'previousTarget': array([20., 24.]), 'currentState': array([19.28493847, 23.93734563,  5.35207921]), 'targetState': array([20, 24], dtype=int32), 'currentDistance': 0.7178011977653174}
episode index:146
target Thresh 26.054751366918175
target distance 8.0
model initialize at round 146
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 29.]), 'previousTarget': array([26., 29.]), 'currentState': array([18.836005  , 22.22127926,  1.32917845]), 'targetState': array([26, 29], dtype=int32), 'currentDistance': 9.862752117576036}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.885803642018586
{'scaleFactor': 20, 'currentTarget': array([26., 29.]), 'previousTarget': array([26., 29.]), 'currentState': array([26.19744892, 29.74622489,  0.31351687]), 'targetState': array([26, 29], dtype=int32), 'currentDistance': 0.7719052161737164}
episode index:147
target Thresh 26.11390757921987
target distance 17.0
model initialize at round 147
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 26.]), 'previousTarget': array([22., 26.]), 'currentState': array([ 5.40549679, 15.36661075,  5.96572447]), 'targetState': array([22, 26], dtype=int32), 'currentDistance': 19.7090462410355}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8858075692462721
{'scaleFactor': 20, 'currentTarget': array([22., 26.]), 'previousTarget': array([22., 26.]), 'currentState': array([22.41573475, 26.46946832,  0.18769957]), 'targetState': array([22, 26], dtype=int32), 'currentDistance': 0.6270852317722193}
episode index:148
target Thresh 26.172475177374395
target distance 19.0
model initialize at round 148
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 24.]), 'previousTarget': array([27., 24.]), 'currentState': array([16.99569212,  6.68296752,  2.58335096]), 'targetState': array([27, 24], dtype=int32), 'currentDistance': 19.999144731865922}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8856930608135387
{'scaleFactor': 20, 'currentTarget': array([27., 24.]), 'previousTarget': array([27., 24.]), 'currentState': array([26.33501875, 23.50267533,  5.9202658 ]), 'targetState': array([27, 24], dtype=int32), 'currentDistance': 0.830380571237572}
episode index:149
target Thresh 26.230460018190367
target distance 19.0
model initialize at round 149
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 25.]), 'previousTarget': array([23., 25.]), 'currentState': array([ 5.16939088, 21.21035504,  1.81258708]), 'targetState': array([23, 25], dtype=int32), 'currentDistance': 18.228879024403284}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8856385805614415
{'scaleFactor': 20, 'currentTarget': array([23., 25.]), 'previousTarget': array([23., 25.]), 'currentState': array([23.57284156, 25.62048557,  5.37787503]), 'targetState': array([23, 25], dtype=int32), 'currentDistance': 0.8444819652177797}
episode index:150
target Thresh 26.287867900200197
target distance 21.0
model initialize at round 150
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  8.]), 'previousTarget': array([11.,  8.]), 'currentState': array([27.34323695, 27.18723127,  5.59658456]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 25.204190876830264}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8852447397884227
{'scaleFactor': 20, 'currentTarget': array([11.,  8.]), 'previousTarget': array([11.,  8.]), 'currentState': array([10.50208888,  7.3145875 ,  2.59787601]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 0.8471751766827988}
episode index:151
target Thresh 26.34470456423992
target distance 24.0
model initialize at round 151
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  2.]), 'previousTarget': array([27.,  2.]), 'currentState': array([ 4.62898464, 17.42284408,  5.52715546]), 'targetState': array([27,  2], dtype=int32), 'currentDistance': 27.1721631044174}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.883866347086264
{'scaleFactor': 20, 'currentTarget': array([27.,  2.]), 'previousTarget': array([27.,  2.]), 'currentState': array([26.223421  ,  2.89179899,  0.55218996]), 'targetState': array([27,  2], dtype=int32), 'currentDistance': 1.1825313461876545}
episode index:152
target Thresh 26.400975694023302
target distance 8.0
model initialize at round 152
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 19.]), 'previousTarget': array([12., 19.]), 'currentState': array([ 2.38077476, 26.54119971,  4.427701  ]), 'targetState': array([12, 19], dtype=int32), 'currentDistance': 12.222896025629831}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.884181373235419
{'scaleFactor': 20, 'currentTarget': array([12., 19.]), 'previousTarget': array([12., 19.]), 'currentState': array([11.51740062, 18.94284132,  5.26237044]), 'targetState': array([12, 19], dtype=int32), 'currentDistance': 0.48597250425826666}
episode index:153
target Thresh 26.45668691671022
target distance 17.0
model initialize at round 153
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 13.]), 'previousTarget': array([ 6., 13.]), 'currentState': array([23.07603013, 27.31874823,  3.74758053]), 'targetState': array([ 6, 13], dtype=int32), 'currentDistance': 22.284913195588757}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.883858856275774
{'scaleFactor': 20, 'currentTarget': array([ 6., 13.]), 'previousTarget': array([ 6., 13.]), 'currentState': array([ 5.30894061, 12.20899055,  2.39444041]), 'targetState': array([ 6, 13], dtype=int32), 'currentDistance': 1.0503613873555067}
episode index:154
target Thresh 26.511843803469365
target distance 4.0
model initialize at round 154
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 18.]), 'previousTarget': array([26., 18.]), 'currentState': array([22.08285004, 23.97603242,  1.26760834]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 7.145420025173088}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8841097326509493
{'scaleFactor': 20, 'currentTarget': array([26., 18.]), 'previousTarget': array([26., 18.]), 'currentState': array([25.00544856, 18.55352106,  0.79618735]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 1.1382082960335766}
episode index:155
target Thresh 26.56645187003538
target distance 15.0
model initialize at round 155
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([25.25787728,  4.51051049,  3.0374645 ]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 14.47796949243291}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8842982423614152
{'scaleFactor': 20, 'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([19.97737251, 18.95694184,  1.65169192]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 0.9572093186336755}
episode index:156
target Thresh 26.620516577260425
target distance 3.0
model initialize at round 156
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'previousTarget': array([ 5., 22.]), 'currentState': array([ 3.76179882, 22.63438765,  5.99809176]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 1.3912547815040026}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8849715019642088
{'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'previousTarget': array([ 5., 22.]), 'currentState': array([ 5.65127425, 21.98110585,  5.90155028]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 0.651548258308273}
episode index:157
target Thresh 26.674043331660275
target distance 9.0
model initialize at round 157
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([10.91066312, 10.28377968,  1.26561606]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 9.146292600848815}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8853893408751949
{'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([ 6.45268896, 17.42107722,  1.84931263]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 0.734900594018517}
episode index:158
target Thresh 26.727037485954984
target distance 7.0
model initialize at round 158
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([15.48783535,  1.21340351,  0.52366846]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 8.532061909246458}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8858019239508227
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([11.10176808,  8.40815438,  2.00552806]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 1.0756866742682378}
episode index:159
target Thresh 26.779504339604138
target distance 22.0
model initialize at round 159
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 25.]), 'previousTarget': array([15., 25.]), 'currentState': array([14.2145609 ,  4.83916959,  1.9540727 ]), 'targetState': array([15, 25], dtype=int32), 'currentDistance': 20.17612444003877}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8849354875157026
{'scaleFactor': 20, 'currentTarget': array([15., 25.]), 'previousTarget': array([15., 25.]), 'currentState': array([15.87004788, 24.18828878,  4.01411225]), 'targetState': array([15, 25], dtype=int32), 'currentDistance': 1.1898984954132694}
episode index:160
target Thresh 26.831449139336822
target distance 16.0
model initialize at round 160
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 12.]), 'previousTarget': array([ 7., 12.]), 'currentState': array([23.2052398 , 15.31796696,  3.84330678]), 'targetState': array([ 7, 12], dtype=int32), 'currentDistance': 16.541423810240072}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8850562737734238
{'scaleFactor': 20, 'currentTarget': array([ 7., 12.]), 'previousTarget': array([ 7., 12.]), 'currentState': array([ 6.17926519, 11.57284454,  3.45141558]), 'targetState': array([ 7, 12], dtype=int32), 'currentDistance': 0.9252391066588908}
episode index:161
target Thresh 26.8828770796763
target distance 15.0
model initialize at round 161
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 14.]), 'previousTarget': array([18., 14.]), 'currentState': array([18.98083375, 27.31713908,  5.71100032]), 'targetState': array([18, 14], dtype=int32), 'currentDistance': 13.35321040945914}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8852889183453652
{'scaleFactor': 20, 'currentTarget': array([18., 14.]), 'previousTarget': array([18., 14.]), 'currentState': array([17.77751301, 13.98723212,  4.05524972]), 'targetState': array([18, 14], dtype=int32), 'currentDistance': 0.22285303979312215}
episode index:162
target Thresh 26.933793303459463
target distance 13.0
model initialize at round 162
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  8.]), 'previousTarget': array([24.,  8.]), 'currentState': array([23.73176645, 19.48443445,  4.15222609]), 'targetState': array([24,  8], dtype=int32), 'currentDistance': 11.487566490586865}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8855758903058659
{'scaleFactor': 20, 'currentTarget': array([24.,  8.]), 'previousTarget': array([24.,  8.]), 'currentState': array([24.15213147,  8.81807989,  3.96935421]), 'targetState': array([24,  8], dtype=int32), 'currentDistance': 0.8321049762458295}
episode index:163
target Thresh 26.98420290235112
target distance 9.0
model initialize at round 163
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 19.]), 'previousTarget': array([ 8., 19.]), 'currentState': array([15.1198153 , 13.61925865,  3.84440768]), 'targetState': array([ 8, 19], dtype=int32), 'currentDistance': 8.924356971555397}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8859167699344948
{'scaleFactor': 20, 'currentTarget': array([ 8., 19.]), 'previousTarget': array([ 8., 19.]), 'currentState': array([ 7.37373006, 19.79840973,  1.56649825]), 'targetState': array([ 8, 19], dtype=int32), 'currentDistance': 1.0147276151947457}
episode index:164
target Thresh 27.034110917353168
target distance 12.0
model initialize at round 164
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 27.]), 'previousTarget': array([23., 27.]), 'currentState': array([24.23779601, 16.50052355,  1.03085923]), 'targetState': array([23, 27], dtype=int32), 'currentDistance': 10.572187320262312}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8861964582858433
{'scaleFactor': 20, 'currentTarget': array([23., 27.]), 'previousTarget': array([23., 27.]), 'currentState': array([23.15948431, 27.7938046 ,  0.71011856]), 'targetState': array([23, 27], dtype=int32), 'currentDistance': 0.8096672072199125}
episode index:165
target Thresh 27.083522339308693
target distance 2.0
model initialize at round 165
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.45616232,  6.15620929,  0.4848172 ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 2.8953246784754194}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8867621422720731
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.64335199,  9.78851229,  1.77318587]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.8654186432123191}
episode index:166
target Thresh 27.132442109401072
target distance 11.0
model initialize at round 166
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 14.]), 'previousTarget': array([ 5., 14.]), 'currentState': array([13.46835773,  2.17761708,  0.49945849]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 14.542414537575961}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8868676508513349
{'scaleFactor': 20, 'currentTarget': array([ 5., 14.]), 'previousTarget': array([ 5., 14.]), 'currentState': array([ 5.1987512 , 13.47559164,  1.07675868]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 0.5608084981312956}
episode index:167
target Thresh 27.180875119648082
target distance 20.0
model initialize at round 167
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 26.]), 'previousTarget': array([19., 26.]), 'currentState': array([15.62596061,  5.56567271,  0.74897402]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 20.711008513747124}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8867597827675114
{'scaleFactor': 20, 'currentTarget': array([19., 26.]), 'previousTarget': array([19., 26.]), 'currentState': array([18.26784643, 26.83177098,  6.13455379]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 1.1081027953928937}
episode index:168
target Thresh 27.228826213391105
target distance 23.0
model initialize at round 168
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.11443927, 25.85119058]), 'previousTarget': array([10., 26.]), 'currentState': array([28.40279896,  2.07019187,  0.42466276]), 'targetState': array([10, 26], dtype=int32), 'currentDistance': 30.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.885889608747516
{'scaleFactor': 20, 'currentTarget': array([10., 26.]), 'previousTarget': array([10., 26.]), 'currentState': array([10.21617037, 26.10590136,  5.58897845]), 'targetState': array([10, 26], dtype=int32), 'currentDistance': 0.24071710971638186}
episode index:169
target Thresh 27.276300185779476
target distance 15.0
model initialize at round 169
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 8.]), 'previousTarget': array([5., 8.]), 'currentState': array([21.27015026,  6.10413152,  1.72558802]), 'targetState': array([5, 8], dtype=int32), 'currentDistance': 16.380235242917433}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8858925220590961
{'scaleFactor': 20, 'currentTarget': array([5., 8.]), 'previousTarget': array([5., 8.]), 'currentState': array([4.09620217, 7.2448014 , 1.26726383]), 'targetState': array([5, 8], dtype=int32), 'currentDistance': 1.1777841238875009}
episode index:170
target Thresh 27.323301784249992
target distance 13.0
model initialize at round 170
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 18.]), 'previousTarget': array([21., 18.]), 'currentState': array([11.14340258,  6.67684951,  0.47548509]), 'targetState': array([21, 18], dtype=int32), 'currentDistance': 15.012203361792588}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8858954012968565
{'scaleFactor': 20, 'currentTarget': array([21., 18.]), 'previousTarget': array([21., 18.]), 'currentState': array([21.93541439, 18.58988801,  5.32074714]), 'targetState': array([21, 18], dtype=int32), 'currentDistance': 1.1058788168503368}
episode index:171
target Thresh 27.369835709001674
target distance 10.0
model initialize at round 171
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  7.]), 'previousTarget': array([27.,  7.]), 'currentState': array([18.67040358, 11.20702063,  1.13294822]), 'targetState': array([27,  7], dtype=int32), 'currentDistance': 9.331730764339392}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8862185684369969
{'scaleFactor': 20, 'currentTarget': array([27.,  7.]), 'previousTarget': array([27.,  7.]), 'currentState': array([26.78341712,  7.85986616,  5.14796484]), 'targetState': array([27,  7], dtype=int32), 'currentDistance': 0.8867231569397236}
episode index:172
target Thresh 27.415906613465772
target distance 9.0
model initialize at round 172
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 21.]), 'previousTarget': array([ 3., 21.]), 'currentState': array([ 3.67125109, 12.19826214,  1.12807911]), 'targetState': array([ 3, 21], dtype=int32), 'currentDistance': 8.827296722038394}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8865379995408351
{'scaleFactor': 20, 'currentTarget': array([ 3., 21.]), 'previousTarget': array([ 3., 21.]), 'currentState': array([ 2.13294493, 21.64960032,  0.46087694]), 'targetState': array([ 3, 21], dtype=int32), 'currentDistance': 1.0834043916675724}
episode index:173
target Thresh 27.461519104771128
target distance 25.0
model initialize at round 173
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  4.]), 'previousTarget': array([24.,  4.]), 'currentState': array([20.00591551, 27.31693121,  3.70608675]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 23.65654226960614}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.885958345180297
{'scaleFactor': 20, 'currentTarget': array([24.,  4.]), 'previousTarget': array([24.,  4.]), 'currentState': array([24.93410498,  3.30910205,  4.43718935]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 1.1618485693311285}
episode index:174
target Thresh 27.50667774420488
target distance 8.0
model initialize at round 174
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([11.33129068, 12.65004078,  2.3826535 ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 10.066447497067117}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8862756126329868
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.95019882, 6.65898841, 3.48746642]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.0095378666115808}
episode index:175
target Thresh 27.551387047668605
target distance 12.0
model initialize at round 175
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([25.33916097,  9.60416735,  4.34397805]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 15.52019956975924}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8861760114973959
{'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([15.6103665 , 22.43453658,  0.2856106 ]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 0.5836405607689428}
episode index:176
target Thresh 27.595651486129906
target distance 7.0
model initialize at round 176
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 21.]), 'previousTarget': array([19., 21.]), 'currentState': array([13.36923375, 15.97856382,  1.63051039]), 'targetState': array([19, 21], dtype=int32), 'currentDistance': 7.54455763390073}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8865964634663371
{'scaleFactor': 20, 'currentTarget': array([19., 21.]), 'previousTarget': array([19., 21.]), 'currentState': array([18.59052162, 20.46025234,  0.58435398]), 'targetState': array([19, 21], dtype=int32), 'currentDistance': 0.6774954487858605}
episode index:177
target Thresh 27.639475486069518
target distance 21.0
model initialize at round 177
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([5.61894233, 4.79149326, 2.50772917]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 26.238298422739756}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8858555804536253
{'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([27.88224401, 19.80774452,  3.99415479]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 0.9029488741545469}
episode index:178
target Thresh 27.682863429923955
target distance 5.0
model initialize at round 178
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 20.]), 'previousTarget': array([ 7., 20.]), 'currentState': array([ 5.56709475, 24.38352737,  4.90010655]), 'targetState': array([ 7, 20], dtype=int32), 'currentDistance': 4.6117816150158975}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8863273314008119
{'scaleFactor': 20, 'currentTarget': array([ 7., 20.]), 'previousTarget': array([ 7., 20.]), 'currentState': array([ 6.81916278, 19.5838568 ,  4.34296277]), 'targetState': array([ 7, 20], dtype=int32), 'currentDistance': 0.4537370012187902}
episode index:179
target Thresh 27.725819656523758
target distance 8.0
model initialize at round 179
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 12.]), 'previousTarget': array([16., 12.]), 'currentState': array([ 9.68191447, 13.06569999,  1.04866176]), 'targetState': array([16, 12], dtype=int32), 'currentDistance': 6.407333391619353}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8867399351708073
{'scaleFactor': 20, 'currentTarget': array([16., 12.]), 'previousTarget': array([16., 12.]), 'currentState': array([16.16423243, 11.90336123,  5.5599568 ]), 'targetState': array([16, 12], dtype=int32), 'currentDistance': 0.190555355778341}
episode index:180
target Thresh 27.768348461527385
target distance 9.0
model initialize at round 180
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 11.]), 'previousTarget': array([27., 11.]), 'currentState': array([18.22202172,  1.3317391 ,  5.85469723]), 'targetState': array([27, 11], dtype=int32), 'currentDistance': 13.058643556487233}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.8848943193807055
{'scaleFactor': 20, 'currentTarget': array([27., 11.]), 'previousTarget': array([27., 11.]), 'currentState': array([26.13292919, 11.97340425,  0.67538349]), 'targetState': array([27, 11], dtype=int32), 'currentDistance': 1.3035826144417804}
episode index:181
target Thresh 27.810454097850776
target distance 2.0
model initialize at round 181
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 12.]), 'previousTarget': array([ 5., 12.]), 'currentState': array([ 5.97169409, 11.374063  ,  3.66279614]), 'targetState': array([ 5, 12], dtype=int32), 'currentDistance': 1.1558488334370254}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8855267681753171
{'scaleFactor': 20, 'currentTarget': array([ 5., 12.]), 'previousTarget': array([ 5., 12.]), 'currentState': array([ 5.97169409, 11.374063  ,  3.66279614]), 'targetState': array([ 5, 12], dtype=int32), 'currentDistance': 1.1558488334370254}
episode index:182
target Thresh 27.852140776092654
target distance 17.0
model initialize at round 182
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([24.31384409, 19.53674271,  3.000736  ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 18.41242746354393}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8854830209339162
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.07186323, 10.53630675,  2.19059855]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.46922889333026596}
episode index:183
target Thresh 27.893412664955576
target distance 10.0
model initialize at round 183
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 15.]), 'previousTarget': array([27., 15.]), 'currentState': array([18.362984  , 13.98741785,  1.63678449]), 'targetState': array([27, 15], dtype=int32), 'currentDistance': 8.69616973055727}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.885787353153846
{'scaleFactor': 20, 'currentTarget': array([27., 15.]), 'previousTarget': array([27., 15.]), 'currentState': array([26.84679094, 14.79556659,  5.27864435]), 'targetState': array([27, 15], dtype=int32), 'currentDistance': 0.2554721818750237}
episode index:184
target Thresh 27.93427389166283
target distance 12.0
model initialize at round 184
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([13.37313902,  6.56905752,  4.41053748]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 11.334826846502486}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8860883952957226
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.86387193, 2.478086  , 3.75480938]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.9873403316787425}
episode index:185
target Thresh 27.974728542371132
target distance 12.0
model initialize at round 185
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([23.36033017,  6.62069662,  2.35892308]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 14.937947406864405}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.885464496104913
{'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([14.12739127, 18.08114252,  2.72315456]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 1.2671799601744103}
episode index:186
target Thresh 28.01478066257927
target distance 20.0
model initialize at round 186
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 22.]), 'previousTarget': array([10., 22.]), 'currentState': array([15.56047443,  3.62457555,  0.82503891]), 'targetState': array([10, 22], dtype=int32), 'currentDistance': 19.198309811286606}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8855172969506553
{'scaleFactor': 20, 'currentTarget': array([10., 22.]), 'previousTarget': array([10., 22.]), 'currentState': array([10.5417669 , 21.63218258,  1.7339918 ]), 'targetState': array([10, 22], dtype=int32), 'currentDistance': 0.6548290092070964}
episode index:187
target Thresh 28.054434257532634
target distance 12.0
model initialize at round 187
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 16.]), 'previousTarget': array([26., 16.]), 'currentState': array([12.3304707 , 21.78772662,  4.27805972]), 'targetState': array([26, 16], dtype=int32), 'currentDistance': 14.844319144070743}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8854747635785718
{'scaleFactor': 20, 'currentTarget': array([26., 16.]), 'previousTarget': array([26., 16.]), 'currentState': array([26.6239581 , 15.04471854,  4.46547255]), 'targetState': array([26, 16], dtype=int32), 'currentDistance': 1.1410023535887526}
episode index:188
target Thresh 28.093693292623772
target distance 7.0
model initialize at round 188
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 16.]), 'previousTarget': array([ 9., 16.]), 'currentState': array([16.10485839, 19.32027353,  3.7647779 ]), 'targetState': array([ 9, 16], dtype=int32), 'currentDistance': 7.842399447800125}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8853402852244063
{'scaleFactor': 20, 'currentTarget': array([ 9., 16.]), 'previousTarget': array([ 9., 16.]), 'currentState': array([ 8.0361455 , 15.20300714,  2.48588333]), 'targetState': array([ 9, 16], dtype=int32), 'currentDistance': 1.2506850545069266}
episode index:189
target Thresh 28.132561693788904
target distance 13.0
model initialize at round 189
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  2.]), 'previousTarget': array([17.,  2.]), 'currentState': array([17.57812264, 14.11141276,  0.25873488]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 12.12520287357361}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.88544050517064
{'scaleFactor': 20, 'currentTarget': array([17.,  2.]), 'previousTarget': array([17.,  2.]), 'currentState': array([17.65669096,  2.02805993,  3.14130855]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 0.657290178938161}
episode index:190
target Thresh 28.171043347900543
target distance 16.0
model initialize at round 190
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([14.37228965, 23.56178028,  2.96256304]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 17.63705098629598}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.8851738730045638
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.3055531 ,  6.75964201,  1.83681139]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.0292290757822349}
episode index:191
target Thresh 28.20914210315616
target distance 17.0
model initialize at round 191
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 10.]), 'previousTarget': array([22., 10.]), 'currentState': array([24.47172056, 26.29519503,  4.58370113]), 'targetState': array([22, 10], dtype=int32), 'currentDistance': 16.481589228651806}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8849539215482064
{'scaleFactor': 20, 'currentTarget': array([22., 10.]), 'previousTarget': array([22., 10.]), 'currentState': array([21.61253516, 10.93153938,  1.49913108]), 'targetState': array([22, 10], dtype=int32), 'currentDistance': 1.008907631335126}
episode index:192
target Thresh 28.246861769463035
target distance 4.0
model initialize at round 192
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([10.33975357,  3.5486002 ,  0.96465731]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 5.265667838625158}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8853961240272312
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.59033517,  5.57604752,  6.26768166]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.5895430203346098}
episode index:193
target Thresh 28.28420611881923
target distance 22.0
model initialize at round 193
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 15.]), 'previousTarget': array([25., 15.]), 'currentState': array([4.45921513, 7.16150177, 0.48844927]), 'targetState': array([25, 15], dtype=int32), 'currentDistance': 21.98558385451486}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8849230217572128
{'scaleFactor': 20, 'currentTarget': array([25., 15.]), 'previousTarget': array([25., 15.]), 'currentState': array([24.65376149, 14.4557991 ,  3.58305775]), 'targetState': array([25, 15], dtype=int32), 'currentDistance': 0.6450083155774755}
episode index:194
target Thresh 28.321178885690806
target distance 9.0
model initialize at round 194
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 28.]), 'previousTarget': array([22., 28.]), 'currentState': array([14.51809988, 27.7267965 ,  5.71992749]), 'targetState': array([22, 28], dtype=int32), 'currentDistance': 7.486886503600342}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.88526182702974
{'scaleFactor': 20, 'currentTarget': array([22., 28.]), 'previousTarget': array([22., 28.]), 'currentState': array([22.36630654, 28.26474624,  5.53562919]), 'targetState': array([22, 28], dtype=int32), 'currentDistance': 0.4519635464777055}
episode index:195
target Thresh 28.35778376738525
target distance 9.0
model initialize at round 195
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 26.]), 'previousTarget': array([ 5., 26.]), 'currentState': array([13.39332775, 28.56982065,  2.94957572]), 'targetState': array([ 5, 26], dtype=int32), 'currentDistance': 8.77792280852753}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8855486552051035
{'scaleFactor': 20, 'currentTarget': array([ 5., 26.]), 'previousTarget': array([ 5., 26.]), 'currentState': array([ 4.72879258, 25.01845097,  2.05509967]), 'targetState': array([ 5, 26], dtype=int32), 'currentDistance': 1.0183280191682498}
episode index:196
target Thresh 28.394024424421247
target distance 11.0
model initialize at round 196
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 16.]), 'previousTarget': array([ 2., 16.]), 'currentState': array([ 5.46252073, 25.40456126,  3.37840247]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 10.021717536884207}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8855528999589667
{'scaleFactor': 20, 'currentTarget': array([ 2., 16.]), 'previousTarget': array([ 2., 16.]), 'currentState': array([ 1.15561448, 15.01582036,  2.87615401]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 1.2967638446061285}
episode index:197
target Thresh 28.429904480894695
target distance 17.0
model initialize at round 197
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 10.]), 'previousTarget': array([17., 10.]), 'currentState': array([26.31308755, 25.94723542,  4.59744159]), 'targetState': array([17, 10], dtype=int32), 'currentDistance': 18.467482699696344}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8854241396290794
{'scaleFactor': 20, 'currentTarget': array([17., 10.]), 'previousTarget': array([17., 10.]), 'currentState': array([17.53254987,  9.34454251,  2.63282422]), 'targetState': array([17, 10], dtype=int32), 'currentDistance': 0.8445317572578737}
episode index:198
target Thresh 28.465427524841143
target distance 14.0
model initialize at round 198
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 17.]), 'previousTarget': array([ 7., 17.]), 'currentState': array([18.33116479,  4.65006605,  2.3827298 ]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 16.760553813980355}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8852966733728594
{'scaleFactor': 20, 'currentTarget': array([ 7., 17.]), 'previousTarget': array([ 7., 17.]), 'currentState': array([ 6.17318149, 16.74394497,  0.53196415]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 0.8655593709787417}
episode index:199
target Thresh 28.50059710859459
target distance 1.0
model initialize at round 199
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 5.02961739, 11.29432884,  3.14747152]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 1.6176926648552783}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8857706900059951
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([5.80131753, 9.45226214, 0.65672162]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.5826589793284934}
episode index:200
target Thresh 28.535416749142716
target distance 22.0
model initialize at round 200
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 19.]), 'previousTarget': array([26., 19.]), 'currentState': array([5.67987414, 3.10203406, 1.07066458]), 'targetState': array([26, 19], dtype=int32), 'currentDistance': 25.800248756576416}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8853122004221028
{'scaleFactor': 20, 'currentTarget': array([26., 19.]), 'previousTarget': array([26., 19.]), 'currentState': array([25.53343133, 18.94604355,  4.09915031]), 'targetState': array([26, 19], dtype=int32), 'currentDistance': 0.4696782084437629}
episode index:201
target Thresh 28.56988992847859
target distance 13.0
model initialize at round 201
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([21.46389854, 22.31240973,  4.5724659 ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 13.470323700393527}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.885361834351987
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.46994818,  9.9340495 ,  2.445081  ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.5341389380867627}
episode index:202
target Thresh 28.604020093948883
target distance 15.0
model initialize at round 202
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'previousTarget': array([10., 17.]), 'currentState': array([23.52627327,  7.8127245 ,  1.62761891]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 16.35133326898154}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8851948192620506
{'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'previousTarget': array([10., 17.]), 'currentState': array([ 9.20252241, 17.94783462,  1.2893276 ]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 1.2386932563902915}
episode index:203
target Thresh 28.637810658598575
target distance 18.0
model initialize at round 203
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 19.]), 'previousTarget': array([23., 19.]), 'currentState': array([ 6.67038809, 27.20540618,  5.39554018]), 'targetState': array([23, 19], dtype=int32), 'currentDistance': 18.275254183940234}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8850716012982234
{'scaleFactor': 20, 'currentTarget': array([23., 19.]), 'previousTarget': array([23., 19.]), 'currentState': array([22.83616206, 19.35428089,  3.70356828]), 'targetState': array([23, 19], dtype=int32), 'currentDistance': 0.3903303970526694}
episode index:204
target Thresh 28.671265001512296
target distance 1.0
model initialize at round 204
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 21.]), 'previousTarget': array([15., 21.]), 'currentState': array([14.02862616, 20.16234629,  2.79979014]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 1.2826655380154992}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8856322276333539
{'scaleFactor': 20, 'currentTarget': array([15., 21.]), 'previousTarget': array([15., 21.]), 'currentState': array([14.02862616, 20.16234629,  2.79979014]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 1.2826655380154992}
episode index:205
target Thresh 28.70438646815221
target distance 23.0
model initialize at round 205
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 15.]), 'previousTarget': array([ 4., 15.]), 'currentState': array([25.36372069,  8.39367259,  3.91548979]), 'targetState': array([ 4, 15], dtype=int32), 'currentDistance': 22.36184526405262}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8855502547456628
{'scaleFactor': 20, 'currentTarget': array([ 4., 15.]), 'previousTarget': array([ 4., 15.]), 'currentState': array([ 3.53308609, 15.54359049,  1.7886411 ]), 'targetState': array([ 4, 15], dtype=int32), 'currentDistance': 0.7165885950482331}
episode index:206
target Thresh 28.73717837069259
target distance 15.0
model initialize at round 206
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([ 9.71363357, 10.34157233,  5.55140162]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 16.6735616428131}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8853855567570118
{'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([25.88368304, 17.76485376,  6.05014742]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 1.168715956373057}
episode index:207
target Thresh 28.769643988351007
target distance 15.0
model initialize at round 207
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 23.]), 'previousTarget': array([12., 23.]), 'currentState': array([7.17359207, 9.6739935 , 2.47746611]), 'targetState': array([12, 23], dtype=int32), 'currentDistance': 14.173096439760265}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8855208052701206
{'scaleFactor': 20, 'currentTarget': array([12., 23.]), 'previousTarget': array([12., 23.]), 'currentState': array([11.31718981, 23.86442239,  0.12251467]), 'targetState': array([12, 23], dtype=int32), 'currentDistance': 1.1015697053303848}
episode index:208
target Thresh 28.80178656771629
target distance 13.0
model initialize at round 208
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.50563395, 13.69022263,  5.68322287]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 11.700671045217781}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8853578242453587
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.93240968,  2.63213413,  6.04876093]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.1264907302996507}
episode index:209
target Thresh 28.833609323073155
target distance 18.0
model initialize at round 209
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 27.]), 'previousTarget': array([ 5., 27.]), 'currentState': array([ 2.31774207, 10.53847723,  0.97820091]), 'targetState': array([ 5, 27], dtype=int32), 'currentDistance': 16.6786162326022}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8853205061441853
{'scaleFactor': 20, 'currentTarget': array([ 5., 27.]), 'previousTarget': array([ 5., 27.]), 'currentState': array([ 4.42344266, 26.55650921,  5.84447509]), 'targetState': array([ 5, 27], dtype=int32), 'currentDistance': 0.7273942920101895}
episode index:210
target Thresh 28.865115436723663
target distance 23.0
model initialize at round 210
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 27.]), 'previousTarget': array([ 2., 27.]), 'currentState': array([24.2606038 , 26.511847  ,  3.03566027]), 'targetState': array([ 2, 27], dtype=int32), 'currentDistance': 22.26595550908736}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8849622471966833
{'scaleFactor': 20, 'currentTarget': array([ 2., 27.]), 'previousTarget': array([ 2., 27.]), 'currentState': array([ 2.53827189, 27.58735484,  0.52086912]), 'targetState': array([ 2, 27], dtype=int32), 'currentDistance': 0.7966946296093763}
episode index:211
target Thresh 28.89630805930543
target distance 12.0
model initialize at round 211
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([20.22885098, 10.66733957,  2.44439051]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 11.835118613474148}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8851404662873967
{'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([26.09500364, 20.46547661,  6.06277413]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 0.5429004977753501}
episode index:212
target Thresh 28.92719031010671
target distance 15.0
model initialize at round 212
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 22.]), 'previousTarget': array([ 2., 22.]), 'currentState': array([5.52921172, 6.29722009, 0.57921618]), 'targetState': array([ 2, 22], dtype=int32), 'currentDistance': 16.094490740312}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8852736906122617
{'scaleFactor': 20, 'currentTarget': array([ 2., 22.]), 'previousTarget': array([ 2., 22.]), 'currentState': array([ 2.41267076, 21.35102957,  2.22408229]), 'targetState': array([ 2, 22], dtype=int32), 'currentDistance': 0.7690642221401613}
episode index:213
target Thresh 28.957765277378314
target distance 8.0
model initialize at round 213
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([ 8.78028093, 12.84039943,  4.9117291 ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 9.631369263446985}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8855363376159474
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.30573592, 5.87457265, 3.40128218]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.705503109816045}
episode index:214
target Thresh 28.988036018642454
target distance 17.0
model initialize at round 214
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  2.]), 'previousTarget': array([10.,  2.]), 'currentState': array([24.16224976, 20.45964469,  3.10183454]), 'targetState': array([10,  2], dtype=int32), 'currentDistance': 23.266452248358874}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8852602087146435
{'scaleFactor': 20, 'currentTarget': array([10.,  2.]), 'previousTarget': array([10.,  2.]), 'currentState': array([10.83603518,  1.0221584 ,  3.04156536]), 'targetState': array([10,  2], dtype=int32), 'currentDistance': 1.2865181783135111}
episode index:215
target Thresh 29.01800556099848
target distance 13.0
model initialize at round 215
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'previousTarget': array([ 5., 22.]), 'currentState': array([10.23817605,  9.09437291,  3.95305824]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 13.928162080188727}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8852654154877985
{'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'previousTarget': array([ 5., 22.]), 'currentState': array([ 5.88980737, 22.30153814,  6.15380485]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 0.939511790780169}
episode index:216
target Thresh 29.0476769014256
target distance 20.0
model initialize at round 216
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 25.]), 'previousTarget': array([17., 25.]), 'currentState': array([17.39728432,  4.06192505,  0.41875809]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 20.94184369254482}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8852705742722609
{'scaleFactor': 20, 'currentTarget': array([17., 25.]), 'previousTarget': array([17., 25.]), 'currentState': array([16.43737869, 24.11924665,  1.29533654]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 1.0451168344083395}
episode index:217
target Thresh 29.077053007082586
target distance 4.0
model initialize at round 217
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.76392969,  5.85784442,  4.89752138]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 2.9581858135815837}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8857055716379845
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.66494416,  2.05772949,  3.92471796]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.1532668599790172}
episode index:218
target Thresh 29.106136815604483
target distance 14.0
model initialize at round 218
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 17.]), 'previousTarget': array([ 6., 17.]), 'currentState': array([20.6303128 , 11.56047873,  2.19691288]), 'targetState': array([ 6, 17], dtype=int32), 'currentDistance': 15.608793808669867}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8857495564901339
{'scaleFactor': 20, 'currentTarget': array([ 6., 17.]), 'previousTarget': array([ 6., 17.]), 'currentState': array([ 6.60667731, 16.8918146 ,  1.21834776]), 'targetState': array([ 6, 17], dtype=int32), 'currentDistance': 0.6162478684518209}
episode index:219
target Thresh 29.13493123539638
target distance 11.0
model initialize at round 219
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([17.91981714, 13.70879606,  3.00654793]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 11.922440608111184}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8859600828147559
{'scaleFactor': 20, 'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([10.49178786,  4.36492731,  3.75141829]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 0.6256608683195093}
episode index:220
target Thresh 29.16343914592425
target distance 14.0
model initialize at round 220
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([ 1.24599299, 17.65654506,  1.24548626]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 15.128100134578103}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.8856521500309662
{'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([16.99750021, 20.32922707,  3.359486  ]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 1.2020578167268043}
episode index:221
target Thresh 29.19166339800291
target distance 12.0
model initialize at round 221
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  2.]), 'previousTarget': array([13.,  2.]), 'currentState': array([25.12169578,  9.31084963,  5.77647889]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 14.155706653878745}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8856957811310913
{'scaleFactor': 20, 'currentTarget': array([13.,  2.]), 'previousTarget': array([13.,  2.]), 'currentState': array([12.26868773,  1.24196564,  2.34072959]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 1.0532966033755948}
episode index:222
target Thresh 29.219606814081075
target distance 5.0
model initialize at round 222
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 25.]), 'previousTarget': array([19., 25.]), 'currentState': array([15.64340464, 27.62113127,  0.77746743]), 'targetState': array([19, 25], dtype=int32), 'currentDistance': 4.2587629143116095}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8861191184354361
{'scaleFactor': 20, 'currentTarget': array([19., 25.]), 'previousTarget': array([19., 25.]), 'currentState': array([18.40574663, 25.63469647,  5.55346999]), 'targetState': array([19, 25], dtype=int32), 'currentDistance': 0.8694691932247576}
episode index:223
target Thresh 29.24727218852365
target distance 6.0
model initialize at round 223
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 17.]), 'previousTarget': array([27., 17.]), 'currentState': array([19.34355057, 21.70240401,  4.32935524]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 8.985200127802036}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8863662658951038
{'scaleFactor': 20, 'currentTarget': array([27., 17.]), 'previousTarget': array([27., 17.]), 'currentState': array([27.19650708, 16.20188931,  4.97425336]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 0.821946294170434}
episode index:224
target Thresh 29.27466228789113
target distance 7.0
model initialize at round 224
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  2.]), 'previousTarget': array([25.,  2.]), 'currentState': array([19.67335045,  6.81486299,  0.89881831]), 'targetState': array([25,  2], dtype=int32), 'currentDistance': 7.180257725726203}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8863269537044542
{'scaleFactor': 20, 'currentTarget': array([25.,  2.]), 'previousTarget': array([25.,  2.]), 'currentState': array([24.18912185,  2.91423476,  6.08847115]), 'targetState': array([25,  2], dtype=int32), 'currentDistance': 1.2220264195760424}
episode index:225
target Thresh 29.301779851216274
target distance 18.0
model initialize at round 225
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 20.]), 'previousTarget': array([14., 20.]), 'currentState': array([10.78784302,  3.16750312,  1.36495709]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 17.136245319564555}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8862491610454478
{'scaleFactor': 20, 'currentTarget': array([14., 20.]), 'previousTarget': array([14., 20.]), 'currentState': array([13.52139237, 20.79244777,  5.79278833]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 0.9257638678998813}
episode index:226
target Thresh 29.328627590278014
target distance 16.0
model initialize at round 226
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 27.]), 'previousTarget': array([ 2., 27.]), 'currentState': array([16.55789008, 25.86758691,  3.6099906 ]), 'targetState': array([ 2, 27], dtype=int32), 'currentDistance': 14.601867102455618}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8863692847742504
{'scaleFactor': 20, 'currentTarget': array([ 2., 27.]), 'previousTarget': array([ 2., 27.]), 'currentState': array([ 1.48255812, 27.51330633,  2.59672   ]), 'targetState': array([ 2, 27], dtype=int32), 'currentDistance': 0.7288549168173808}
episode index:227
target Thresh 29.355208189872634
target distance 8.0
model initialize at round 227
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 23.]), 'previousTarget': array([20., 23.]), 'currentState': array([11.08728673, 27.58601889,  5.14918947]), 'targetState': array([20, 23], dtype=int32), 'currentDistance': 10.023374035507864}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8862538859578778
{'scaleFactor': 20, 'currentTarget': array([20., 23.]), 'previousTarget': array([20., 23.]), 'currentState': array([20.86802738, 23.69228318,  4.97292451]), 'targetState': array([20, 23], dtype=int32), 'currentDistance': 1.110282638056692}
episode index:228
target Thresh 29.38152430808224
target distance 14.0
model initialize at round 228
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  2.]), 'previousTarget': array([23.,  2.]), 'currentState': array([10.53915935,  5.68071769,  5.68958897]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 12.993084022153955}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8863729399383397
{'scaleFactor': 20, 'currentTarget': array([23.,  2.]), 'previousTarget': array([23.,  2.]), 'currentState': array([22.19984494,  2.15536757,  4.87114332]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 0.8150995053161083}
episode index:229
target Thresh 29.407578576540587
target distance 21.0
model initialize at round 229
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  4.]), 'previousTarget': array([24.,  4.]), 'currentState': array([ 4.83300002, 23.78735862,  4.95616841]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 27.548383791375073}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8859696414327105
{'scaleFactor': 20, 'currentTarget': array([24.,  4.]), 'previousTarget': array([24.,  4.]), 'currentState': array([23.21277192,  4.94424349,  0.14957261]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 1.2293591116262603}
episode index:230
target Thresh 29.433373600696225
target distance 12.0
model initialize at round 230
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 13.]), 'previousTarget': array([19., 13.]), 'currentState': array([ 8.67708526, 21.14061692,  5.35683567]), 'targetState': array([19, 13], dtype=int32), 'currentDistance': 13.146566571845124}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8860888951385587
{'scaleFactor': 20, 'currentTarget': array([19., 13.]), 'previousTarget': array([19., 13.]), 'currentState': array([18.18879408, 13.41056667,  4.65708507]), 'targetState': array([19, 13], dtype=int32), 'currentDistance': 0.9091864721490495}
episode index:231
target Thresh 29.45891196007307
target distance 7.0
model initialize at round 231
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([ 3.61494732, 25.56872019,  2.20388247]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 7.248625996993257}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8863686414952892
{'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([10.92768832, 29.35962329,  0.21727794]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 0.9949545370724112}
episode index:232
target Thresh 29.484196208528346
target distance 1.0
model initialize at round 232
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 3.]), 'previousTarget': array([5., 3.]), 'currentState': array([5.30777528, 5.19830734, 6.17803675]), 'targetState': array([5, 3], dtype=int32), 'currentDistance': 2.219747914455934}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.886110272320784
{'scaleFactor': 20, 'currentTarget': array([5., 3.]), 'previousTarget': array([5., 3.]), 'currentState': array([5.8287404 , 3.74721533, 4.97555925]), 'targetState': array([5, 3], dtype=int32), 'currentDistance': 1.115859041295345}
episode index:233
target Thresh 29.50922887450796
target distance 15.0
model initialize at round 233
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 17.]), 'previousTarget': array([17., 17.]), 'currentState': array([12.45872572,  3.61924723,  0.28473556]), 'targetState': array([17, 17], dtype=int32), 'currentDistance': 14.130382758168533}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8862668296802163
{'scaleFactor': 20, 'currentTarget': array([17., 17.]), 'previousTarget': array([17., 17.]), 'currentState': array([16.11900428, 16.72311642,  0.64356124]), 'targetState': array([17, 17], dtype=int32), 'currentDistance': 0.9234814466922134}
episode index:234
target Thresh 29.534012461299376
target distance 18.0
model initialize at round 234
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 29.]), 'previousTarget': array([ 3., 29.]), 'currentState': array([11.71946205, 12.66071824,  0.73028469]), 'targetState': array([ 3, 29], dtype=int32), 'currentDistance': 18.520290141847862}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8863054314869333
{'scaleFactor': 20, 'currentTarget': array([ 3., 29.]), 'previousTarget': array([ 3., 29.]), 'currentState': array([ 3.63951021, 29.39812792,  1.91243216]), 'targetState': array([ 3, 29], dtype=int32), 'currentDistance': 0.753312115987806}
episode index:235
target Thresh 29.55854944728193
target distance 9.0
model initialize at round 235
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 26.]), 'previousTarget': array([ 4., 26.]), 'currentState': array([ 4.32658501, 17.17910698,  2.0249722 ]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 8.826936694296599}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8865795188530905
{'scaleFactor': 20, 'currentTarget': array([ 4., 26.]), 'previousTarget': array([ 4., 26.]), 'currentState': array([ 4.68123695, 25.52981468,  1.00267968]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 0.8277427182979421}
episode index:236
target Thresh 29.58284228617466
target distance 8.0
model initialize at round 236
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 13.]), 'previousTarget': array([17., 13.]), 'currentState': array([ 7.68588471, 10.94857753,  4.82639396]), 'targetState': array([17, 13], dtype=int32), 'currentDistance': 9.53735171896499}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8868111670832504
{'scaleFactor': 20, 'currentTarget': array([17., 13.]), 'previousTarget': array([17., 13.]), 'currentState': array([17.43495292, 12.26267689,  5.9370352 ]), 'targetState': array([17, 13], dtype=int32), 'currentDistance': 0.8560545664054834}
episode index:237
target Thresh 29.606893407281703
target distance 18.0
model initialize at round 237
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 2.]), 'previousTarget': array([9., 2.]), 'currentState': array([17.27228758, 21.51750542,  3.02794659]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 21.198201803284753}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8867721328644089
{'scaleFactor': 20, 'currentTarget': array([9., 2.]), 'previousTarget': array([9., 2.]), 'currentState': array([8.54692737, 1.95186094, 3.98649413]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 0.45562284406895653}
episode index:238
target Thresh 29.63070521573521
target distance 2.0
model initialize at round 238
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 27.]), 'previousTarget': array([15., 27.]), 'currentState': array([11.91877421, 26.30582709,  1.27332377]), 'targetState': array([15, 27], dtype=int32), 'currentDistance': 3.1584534822095547}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8871626260323402
{'scaleFactor': 20, 'currentTarget': array([15., 27.]), 'previousTarget': array([15., 27.]), 'currentState': array([15.38181482, 26.29575376,  5.9606996 ]), 'targetState': array([15, 27], dtype=int32), 'currentDistance': 0.8010900863617443}
episode index:239
target Thresh 29.654280092735878
target distance 11.0
model initialize at round 239
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 13.]), 'previousTarget': array([ 5., 13.]), 'currentState': array([14.38338762, 18.52211338,  2.42371643]), 'targetState': array([ 5, 13], dtype=int32), 'currentDistance': 10.887685676595495}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8873497207068178
{'scaleFactor': 20, 'currentTarget': array([ 5., 13.]), 'previousTarget': array([ 5., 13.]), 'currentState': array([ 4.20770169, 12.06733429,  3.48735087]), 'targetState': array([ 5, 13], dtype=int32), 'currentDistance': 1.2237654815881958}
episode index:240
target Thresh 29.67762039579104
target distance 9.0
model initialize at round 240
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 17.]), 'previousTarget': array([14., 17.]), 'currentState': array([ 9.7322099 , 24.89315929,  4.86931562]), 'targetState': array([14, 17], dtype=int32), 'currentDistance': 8.9730705930607}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8876137884628063
{'scaleFactor': 20, 'currentTarget': array([14., 17.]), 'previousTarget': array([14., 17.]), 'currentState': array([14.82233424, 17.97434431,  4.88981497]), 'targetState': array([14, 17], dtype=int32), 'currentDistance': 1.2749825242075352}
episode index:241
target Thresh 29.70072845895046
target distance 17.0
model initialize at round 241
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 18.]), 'previousTarget': array([ 8., 18.]), 'currentState': array([23.48437349, 20.74956975,  4.16019595]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 15.726600275164719}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8877208275496692
{'scaleFactor': 20, 'currentTarget': array([ 8., 18.]), 'previousTarget': array([ 8., 18.]), 'currentState': array([ 8.84163778, 17.49602182,  2.88638806]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 0.9809934487828621}
episode index:242
target Thresh 29.723606593039708
target distance 8.0
model initialize at round 242
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([8.88813473, 7.34255457, 4.23440675]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 7.6563092314732115}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8879811947198352
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.01491998, 3.87448413, 3.10876006]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.9930442518182451}
episode index:243
target Thresh 29.746257085891262
target distance 5.0
model initialize at round 243
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 15.]), 'previousTarget': array([17., 15.]), 'currentState': array([22.13302936, 18.75406516,  4.4422174 ]), 'targetState': array([17, 15], dtype=int32), 'currentDistance': 6.35932351869687}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8883185627742621
{'scaleFactor': 20, 'currentTarget': array([17., 15.]), 'previousTarget': array([17., 15.]), 'currentState': array([17.95983152, 14.59597535,  3.92388107]), 'targetState': array([17, 15], dtype=int32), 'currentDistance': 1.0413992842267923}
episode index:244
target Thresh 29.768682202573277
target distance 15.0
model initialize at round 244
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 27.]), 'previousTarget': array([ 6., 27.]), 'currentState': array([19.82058545, 13.67337937,  2.6876052 ]), 'targetState': array([ 6, 27], dtype=int32), 'currentDistance': 19.199151015148203}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8883472145762396
{'scaleFactor': 20, 'currentTarget': array([ 6., 27.]), 'previousTarget': array([ 6., 27.]), 'currentState': array([ 5.7064268 , 27.16954293,  1.90764996]), 'targetState': array([ 6, 27], dtype=int32), 'currentDistance': 0.33901331943641067}
episode index:245
target Thresh 29.790884185616115
target distance 19.0
model initialize at round 245
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 25.]), 'previousTarget': array([ 3., 25.]), 'currentState': array([1.9361885 , 7.68175985, 2.61872143]), 'targetState': array([ 3, 25], dtype=int32), 'currentDistance': 17.350882887623097}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8884123969357215
{'scaleFactor': 20, 'currentTarget': array([ 3., 25.]), 'previousTarget': array([ 3., 25.]), 'currentState': array([ 2.75529633, 24.17162541,  0.89984685]), 'targetState': array([ 3, 25], dtype=int32), 'currentDistance': 0.8637617461205362}
episode index:246
target Thresh 29.812865255236577
target distance 3.0
model initialize at round 246
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 16.]), 'previousTarget': array([ 7., 16.]), 'currentState': array([ 8.00764426, 14.68301688,  2.57614666]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 1.6582495249260993}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8888236827780871
{'scaleFactor': 20, 'currentTarget': array([ 7., 16.]), 'previousTarget': array([ 7., 16.]), 'currentState': array([ 6.37202559, 15.83267762,  2.4807583 ]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 0.64988355854345}
episode index:247
target Thresh 29.834627609559945
target distance 5.0
model initialize at round 247
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 24.]), 'previousTarget': array([27., 24.]), 'currentState': array([24.59953272, 29.52342402,  5.58948249]), 'targetState': array([27, 24], dtype=int32), 'currentDistance': 6.022495831716895}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8891130873233367
{'scaleFactor': 20, 'currentTarget': array([27., 24.]), 'previousTarget': array([27., 24.]), 'currentState': array([27.67941244, 23.03972882,  3.93591817]), 'targetState': array([27, 24], dtype=int32), 'currentDistance': 1.1763171396702168}
episode index:248
target Thresh 29.856173424839785
target distance 3.0
model initialize at round 248
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.36033875, 13.84279922,  1.99525386]), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.2120056381146405}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8895182556473393
{'scaleFactor': 20, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.58473981, 15.68603417,  1.94239422]), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.8019251243795821}
episode index:249
target Thresh 29.87750485567558
target distance 1.0
model initialize at round 249
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 25.]), 'previousTarget': array([12., 25.]), 'currentState': array([11.46606402, 23.35094626,  5.94528413]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 1.7333395097016329}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.88992018262475
{'scaleFactor': 20, 'currentTarget': array([12., 25.]), 'previousTarget': array([12., 25.]), 'currentState': array([12.81011889, 24.36811465,  1.65348118]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 1.027410193384415}
episode index:250
target Thresh 29.898624035228192
target distance 16.0
model initialize at round 250
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 10.]), 'previousTarget': array([27., 10.]), 'currentState': array([12.80017412, 18.2274865 ,  5.52217513]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 16.41117269080743}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8898358225854841
{'scaleFactor': 20, 'currentTarget': array([27., 10.]), 'previousTarget': array([27., 10.]), 'currentState': array([27.82776317,  9.68562787,  2.65124159]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 0.8854499980607412}
episode index:251
target Thresh 29.919533075433172
target distance 19.0
model initialize at round 251
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 21.]), 'previousTarget': array([13., 21.]), 'currentState': array([17.78153864,  3.66873087,  2.7109707 ]), 'targetState': array([13, 21], dtype=int32), 'currentDistance': 17.978765291164137}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8897869543331566
{'scaleFactor': 20, 'currentTarget': array([13., 21.]), 'previousTarget': array([13., 21.]), 'currentState': array([12.08293314, 20.91828644,  5.96564623]), 'targetState': array([13, 21], dtype=int32), 'currentDistance': 0.9207001336913188}
episode index:252
target Thresh 29.940234067211968
target distance 12.0
model initialize at round 252
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 15.]), 'previousTarget': array([19., 15.]), 'currentState': array([15.65648283, 28.01358261,  3.50526571]), 'targetState': array([19, 15], dtype=int32), 'currentDistance': 13.436236055210887}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8899172220805668
{'scaleFactor': 20, 'currentTarget': array([19., 15.]), 'previousTarget': array([19., 15.]), 'currentState': array([19.43013375, 14.01218071,  3.66998713]), 'targetState': array([19, 15], dtype=int32), 'currentDistance': 1.0774052128058}
episode index:253
target Thresh 29.96072908068101
target distance 19.0
model initialize at round 253
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 28.]), 'previousTarget': array([24., 28.]), 'currentState': array([ 6.62004126, 19.45591093,  5.54750985]), 'targetState': array([24, 28], dtype=int32), 'currentDistance': 19.366580079415645}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8899385647269374
{'scaleFactor': 20, 'currentTarget': array([24., 28.]), 'previousTarget': array([24., 28.]), 'currentState': array([23.00320678, 27.25695111,  0.3680189 ]), 'targetState': array([24, 28], dtype=int32), 'currentDistance': 1.2432692279012758}
episode index:254
target Thresh 29.98102016535872
target distance 5.0
model initialize at round 254
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 16.]), 'previousTarget': array([25., 16.]), 'currentState': array([21.7705049 , 20.07136364,  5.84761955]), 'targetState': array([25, 16], dtype=int32), 'currentDistance': 5.1966951530764725}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8902537036887926
{'scaleFactor': 20, 'currentTarget': array([25., 16.]), 'previousTarget': array([25., 16.]), 'currentState': array([25.88914673, 16.10583269,  4.72830857]), 'targetState': array([25, 16], dtype=int32), 'currentDistance': 0.8954230669305696}
episode index:255
target Thresh 30.001109350370477
target distance 1.0
model initialize at round 255
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  3.]), 'previousTarget': array([12.,  3.]), 'currentState': array([11.10507017,  3.63970969,  2.81910238]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 1.1000581311682047}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8906824001587583
{'scaleFactor': 20, 'currentTarget': array([12.,  3.]), 'previousTarget': array([12.,  3.]), 'currentState': array([11.10507017,  3.63970969,  2.81910238]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 1.1000581311682047}
episode index:256
target Thresh 30.020998644651527
target distance 20.0
model initialize at round 256
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 22.]), 'previousTarget': array([ 6., 22.]), 'currentState': array([24.44872242, 27.65263003,  3.75336564]), 'targetState': array([ 6, 22], dtype=int32), 'currentDistance': 19.29527365248853}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.890631188574479
{'scaleFactor': 20, 'currentTarget': array([ 6., 22.]), 'previousTarget': array([ 6., 22.]), 'currentState': array([ 5.32850786, 22.39000779,  1.83580966]), 'targetState': array([ 6, 22], dtype=int32), 'currentDistance': 0.7765357529378711}
episode index:257
target Thresh 30.040690037147865
target distance 11.0
model initialize at round 257
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([19.76253133, 26.66613226,  2.72236973]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 13.91537544657047}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8907556595273993
{'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([14.07497127, 13.66332433,  4.39482743]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 0.34492201925776106}
episode index:258
target Thresh 30.060185497015155
target distance 12.0
model initialize at round 258
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 20.]), 'previousTarget': array([22., 20.]), 'currentState': array([12.29707189,  9.6565435 ,  0.38335013]), 'targetState': array([22, 20], dtype=int32), 'currentDistance': 14.182168603157347}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.890843542106381
{'scaleFactor': 20, 'currentTarget': array([22., 20.]), 'previousTarget': array([22., 20.]), 'currentState': array([21.55437759, 20.13669101,  0.42406794]), 'targetState': array([22, 20], dtype=int32), 'currentDistance': 0.46611560787196177}
episode index:259
target Thresh 30.07948697381563
target distance 4.0
model initialize at round 259
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 19.]), 'previousTarget': array([23., 19.]), 'currentState': array([25.14565423, 18.51088926,  2.38585359]), 'targetState': array([23, 19], dtype=int32), 'currentDistance': 2.200695663354682}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8912249130982796
{'scaleFactor': 20, 'currentTarget': array([23., 19.]), 'previousTarget': array([23., 19.]), 'currentState': array([23.53411147, 19.68627113,  2.63955903]), 'targetState': array([23, 19], dtype=int32), 'currentDistance': 0.869622399803543}
episode index:260
target Thresh 30.098596397713052
target distance 16.0
model initialize at round 260
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 19.]), 'previousTarget': array([10., 19.]), 'currentState': array([2.58878912, 4.57661519, 2.22338404]), 'targetState': array([10, 19], dtype=int32), 'currentDistance': 16.216043786000487}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8912753236803123
{'scaleFactor': 20, 'currentTarget': array([10., 19.]), 'previousTarget': array([10., 19.]), 'currentState': array([10.60858619, 18.2542479 ,  6.23004833]), 'targetState': array([10, 19], dtype=int32), 'currentDistance': 0.9625608296905466}
episode index:261
target Thresh 30.11751567966574
target distance 7.0
model initialize at round 261
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  6.]), 'previousTarget': array([13.,  6.]), 'currentState': array([ 8.31084709, 11.34598604,  5.90815711]), 'targetState': array([13,  6], dtype=int32), 'currentDistance': 7.111098493211387}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8915399064525247
{'scaleFactor': 20, 'currentTarget': array([13.,  6.]), 'previousTarget': array([13.,  6.]), 'currentState': array([12.41032669,  5.82574674,  0.03237801]), 'targetState': array([13,  6], dtype=int32), 'currentDistance': 0.6148811313599007}
episode index:262
target Thresh 30.13624671161765
target distance 14.0
model initialize at round 262
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 14.]), 'previousTarget': array([19., 14.]), 'currentState': array([ 6.51235586, 23.73835482,  5.72736055]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 15.835934166574738}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8916234704868635
{'scaleFactor': 20, 'currentTarget': array([19., 14.]), 'previousTarget': array([19., 14.]), 'currentState': array([18.01889315, 13.94884882,  0.42318797]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 0.9824393544728776}
episode index:263
target Thresh 30.154791366687586
target distance 2.0
model initialize at round 263
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 23.]), 'previousTarget': array([21., 23.]), 'currentState': array([20.34380369, 22.03155725,  5.94665629]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 1.169818349674706}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8920339876441102
{'scaleFactor': 20, 'currentTarget': array([21., 23.]), 'previousTarget': array([21., 23.]), 'currentState': array([20.34380369, 22.03155725,  5.94665629]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 1.169818349674706}
episode index:264
target Thresh 30.173151499356518
target distance 8.0
model initialize at round 264
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 14.]), 'previousTarget': array([ 9., 14.]), 'currentState': array([ 3.9637217 , 23.47540698,  2.97158992]), 'targetState': array([ 9, 14], dtype=int32), 'currentDistance': 10.730677359329249}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8921850493809513
{'scaleFactor': 20, 'currentTarget': array([ 9., 14.]), 'previousTarget': array([ 9., 14.]), 'currentState': array([ 8.570195  , 14.88594709,  5.43742877]), 'targetState': array([ 9, 14], dtype=int32), 'currentDistance': 0.9847002455401324}
episode index:265
target Thresh 30.191328945653
target distance 19.0
model initialize at round 265
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  5.]), 'previousTarget': array([10.,  5.]), 'currentState': array([19.76392511, 22.85784985,  2.8775177 ]), 'targetState': array([10,  5], dtype=int32), 'currentDistance': 20.35281393318487}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.892129921462222
{'scaleFactor': 20, 'currentTarget': array([10.,  5.]), 'previousTarget': array([10.,  5.]), 'currentState': array([9.55263178, 4.84130091, 4.12704746]), 'targetState': array([10,  5], dtype=int32), 'currentDistance': 0.4746827636637578}
episode index:266
target Thresh 30.209325523336823
target distance 16.0
model initialize at round 266
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 19.]), 'previousTarget': array([19., 19.]), 'currentState': array([ 3.46251781, 12.38183272,  6.00079393]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 16.888264891978658}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8921419376899242
{'scaleFactor': 20, 'currentTarget': array([19., 19.]), 'previousTarget': array([19., 19.]), 'currentState': array([19.1056228 , 18.1195488 ,  0.33890646]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 0.8867640592566018}
episode index:267
target Thresh 30.227143032080743
target distance 14.0
model initialize at round 267
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 12.]), 'previousTarget': array([18., 12.]), 'currentState': array([16.93719429, 27.3049261 ,  3.26429033]), 'targetState': array([18, 12], dtype=int32), 'currentDistance': 15.341783429225025}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8921876098440992
{'scaleFactor': 20, 'currentTarget': array([18., 12.]), 'previousTarget': array([18., 12.]), 'currentState': array([17.68387084, 11.12749697,  4.6036711 ]), 'targetState': array([18, 12], dtype=int32), 'currentDistance': 0.9280081781766294}
episode index:268
target Thresh 30.244783253650485
target distance 11.0
model initialize at round 268
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 17.]), 'previousTarget': array([21., 17.]), 'currentState': array([11.42286634, 21.10120109,  0.44661015]), 'targetState': array([21, 17], dtype=int32), 'currentDistance': 10.41831750437778}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8923708534855745
{'scaleFactor': 20, 'currentTarget': array([21., 17.]), 'previousTarget': array([21., 17.]), 'currentState': array([20.17901205, 16.02334392,  0.55670493]), 'targetState': array([21, 17], dtype=int32), 'currentDistance': 1.2758833474958442}
episode index:269
target Thresh 30.26224795208291
target distance 9.0
model initialize at round 269
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 23.]), 'previousTarget': array([24., 23.]), 'currentState': array([16.44164886, 16.86835284,  1.55213326]), 'targetState': array([24, 23], dtype=int32), 'currentDistance': 9.732716417919766}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8925527397667429
{'scaleFactor': 20, 'currentTarget': array([24., 23.]), 'previousTarget': array([24., 23.]), 'currentState': array([24.86877648, 22.48908728,  1.68764859]), 'targetState': array([24, 23], dtype=int32), 'currentDistance': 1.0078712093026152}
episode index:270
target Thresh 30.279538873862407
target distance 20.0
model initialize at round 270
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 2.]), 'previousTarget': array([6., 2.]), 'currentState': array([15.35098609, 20.35403619,  3.91248274]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 20.59882485394128}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8924648913276367
{'scaleFactor': 20, 'currentTarget': array([6., 2.]), 'previousTarget': array([6., 2.]), 'currentState': array([5.68044834, 2.00608455, 3.13101831]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 0.31960957972028337}
episode index:271
target Thresh 30.29665774809557
target distance 13.0
model initialize at round 271
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([19.04435393, 21.17155118,  5.46970344]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 14.373722333629782}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8925422896958574
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.72503769, 10.43938594,  3.99367562]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.8477851516811119}
episode index:272
target Thresh 30.313606286684085
target distance 14.0
model initialize at round 272
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([11.6293744 ,  7.42133971,  5.52623207]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 13.50641706592182}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8926191210430654
{'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([23.4504337 ,  1.53154531,  5.8227855 ]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 0.7221308112523137}
episode index:273
target Thresh 30.330386184495936
target distance 10.0
model initialize at round 273
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 28.]), 'previousTarget': array([17., 28.]), 'currentState': array([24.52665839, 19.61525237,  0.84621119]), 'targetState': array([17, 28], dtype=int32), 'currentDistance': 11.267412273064851}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8927630853746856
{'scaleFactor': 20, 'currentTarget': array([17., 28.]), 'previousTarget': array([17., 28.]), 'currentState': array([17.45975441, 28.41370649,  1.88679484]), 'targetState': array([17, 28], dtype=int32), 'currentDistance': 0.6184878207821682}
episode index:274
target Thresh 30.346999119534885
target distance 6.0
model initialize at round 274
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 20.]), 'previousTarget': array([13., 20.]), 'currentState': array([ 8.11272008, 16.737367  ,  0.16157358]), 'targetState': array([13, 20], dtype=int32), 'currentDistance': 5.876247023271985}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8930450341551414
{'scaleFactor': 20, 'currentTarget': array([13., 20.]), 'previousTarget': array([13., 20.]), 'currentState': array([12.14555781, 19.53941471,  0.17332825]), 'targetState': array([13, 20], dtype=int32), 'currentDistance': 0.9706751576862991}
episode index:275
target Thresh 30.363446753108285
target distance 9.0
model initialize at round 275
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([ 6.5587972 , 13.13090703,  4.69423938]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 8.875623174282754}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8932549798643619
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.94612828, 5.33866057, 3.26724923]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.3429185679792739}
episode index:276
target Thresh 30.3797307299932
target distance 9.0
model initialize at round 276
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 27.]), 'previousTarget': array([20., 27.]), 'currentState': array([12.98365925, 29.03062016,  0.23771593]), 'targetState': array([20, 27], dtype=int32), 'currentDistance': 7.3042765382484935}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8934980882764039
{'scaleFactor': 20, 'currentTarget': array([20., 27.]), 'previousTarget': array([20., 27.]), 'currentState': array([19.36372754, 26.82538807,  5.61081285]), 'targetState': array([20, 27], dtype=int32), 'currentDistance': 0.6597969137854971}
episode index:277
target Thresh 30.39585267860088
target distance 14.0
model initialize at round 277
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 28.]), 'previousTarget': array([ 5., 28.]), 'currentState': array([18.14569228, 19.450016  ,  3.11321545]), 'targetState': array([ 5, 28], dtype=int32), 'currentDistance': 15.681564079225483}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8935372393078154
{'scaleFactor': 20, 'currentTarget': array([ 5., 28.]), 'previousTarget': array([ 5., 28.]), 'currentState': array([ 5.34134136, 28.66134957,  1.9609727 ]), 'targetState': array([ 5, 28], dtype=int32), 'currentDistance': 0.7442426858732443}
episode index:278
target Thresh 30.411814211139628
target distance 14.0
model initialize at round 278
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 27.]), 'previousTarget': array([14., 27.]), 'currentState': array([15.6335646 , 13.40478981,  1.25290268]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 13.693000889179807}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8936088522403453
{'scaleFactor': 20, 'currentTarget': array([14., 27.]), 'previousTarget': array([14., 27.]), 'currentState': array([14.59159774, 27.10578257,  1.29203337]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 0.6009807304293785}
episode index:279
target Thresh 30.427616923775997
target distance 16.0
model initialize at round 279
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 27.]), 'previousTarget': array([ 2., 27.]), 'currentState': array([16.31812587, 17.93917888,  4.18773377]), 'targetState': array([ 2, 27], dtype=int32), 'currentDistance': 16.944238187366725}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8935830523099016
{'scaleFactor': 20, 'currentTarget': array([ 2., 27.]), 'previousTarget': array([ 2., 27.]), 'currentState': array([ 2.78197566, 27.2813544 ,  2.46250576]), 'targetState': array([ 2, 27], dtype=int32), 'currentDistance': 0.8310512834164472}
episode index:280
target Thresh 30.44326239679442
target distance 11.0
model initialize at round 280
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([13.32061165, 18.89026011,  4.21684504]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 13.925858950577663}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8936868303957309
{'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([21.08081631,  8.46808865,  6.17422196]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 1.0315064911312968}
episode index:281
target Thresh 30.458752194755238
target distance 16.0
model initialize at round 281
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 4.]), 'previousTarget': array([6., 4.]), 'currentState': array([20.59659971, 12.07109974,  2.71628284]), 'targetState': array([6, 4], dtype=int32), 'currentDistance': 16.679429664502763}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.893724756795068
{'scaleFactor': 20, 'currentTarget': array([6., 4.]), 'previousTarget': array([6., 4.]), 'currentState': array([5.64590411, 4.46157744, 2.31798291]), 'targetState': array([6, 4], dtype=int32), 'currentDistance': 0.5817539223202968}
episode index:282
target Thresh 30.474087866651153
target distance 18.0
model initialize at round 282
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 20.]), 'previousTarget': array([22., 20.]), 'currentState': array([5.47921509, 5.80268973, 5.77036471]), 'targetState': array([22, 20], dtype=int32), 'currentDistance': 21.783019830778276}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.893575403488707
{'scaleFactor': 20, 'currentTarget': array([22., 20.]), 'previousTarget': array([22., 20.]), 'currentState': array([21.38929432, 19.44234165,  5.58303714]), 'targetState': array([22, 20], dtype=int32), 'currentDistance': 0.8270092277465856}
episode index:283
target Thresh 30.489270946062135
target distance 13.0
model initialize at round 283
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 21.]), 'previousTarget': array([ 7., 21.]), 'currentState': array([18.3266163 , 12.17937413,  2.02480793]), 'targetState': array([ 7, 21], dtype=int32), 'currentDistance': 14.35603279068612}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.893613455148989
{'scaleFactor': 20, 'currentTarget': array([ 7., 21.]), 'previousTarget': array([ 7., 21.]), 'currentState': array([ 6.61658676, 21.35605389,  2.9090884 ]), 'targetState': array([ 7, 21], dtype=int32), 'currentDistance': 0.523239989051429}
episode index:284
target Thresh 30.50430295130878
target distance 14.0
model initialize at round 284
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 6.]), 'previousTarget': array([9., 6.]), 'currentState': array([22.15662431, 17.23357344,  4.6290502 ]), 'targetState': array([9, 6], dtype=int32), 'currentDistance': 17.29999813996481}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8935569904747784
{'scaleFactor': 20, 'currentTarget': array([9., 6.]), 'previousTarget': array([9., 6.]), 'currentState': array([9.04777408, 5.06035589, 1.44009101]), 'targetState': array([9, 6], dtype=int32), 'currentDistance': 0.9408578063675889}
episode index:285
target Thresh 30.519185385604136
target distance 6.0
model initialize at round 285
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 22.]), 'previousTarget': array([ 8., 22.]), 'currentState': array([ 7.38895308, 17.56812435,  0.93236578]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 4.473801529045505}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8938595884101813
{'scaleFactor': 20, 'currentTarget': array([ 8., 22.]), 'previousTarget': array([ 8., 22.]), 'currentState': array([ 7.28317516, 21.03694064,  0.48577631]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 1.2005503692013613}
episode index:286
target Thresh 30.533919737204037
target distance 25.0
model initialize at round 286
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  2.]), 'previousTarget': array([10.,  2.]), 'currentState': array([19.95354866, 25.31767115,  3.67478466]), 'targetState': array([10,  2], dtype=int32), 'currentDistance': 25.35324276599298}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8936237313907577
{'scaleFactor': 20, 'currentTarget': array([10.,  2.]), 'previousTarget': array([10.,  2.]), 'currentState': array([9.41747977, 1.35048302, 0.42945147]), 'targetState': array([10,  2], dtype=int32), 'currentDistance': 0.8724689838301847}
episode index:287
target Thresh 30.54850747955592
target distance 18.0
model initialize at round 287
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 13.]), 'previousTarget': array([27., 13.]), 'currentState': array([9.75336835, 7.49506671, 6.18651581]), 'targetState': array([27, 13], dtype=int32), 'currentDistance': 18.103883388457696}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8935678192088417
{'scaleFactor': 20, 'currentTarget': array([27., 13.]), 'previousTarget': array([27., 13.]), 'currentState': array([27.48107824, 12.47338991,  5.22525331]), 'targetState': array([27, 13], dtype=int32), 'currentDistance': 0.7132702586002676}
episode index:288
target Thresh 30.562950071446178
target distance 16.0
model initialize at round 288
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([25.76583082,  9.85579088,  2.87918472]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 14.810097038527243}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8936052387790838
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.83361256, 10.81706681,  3.8702127 ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.2472839094364286}
episode index:289
target Thresh 30.577248957146033
target distance 9.0
model initialize at round 289
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 23.]), 'previousTarget': array([19., 23.]), 'currentState': array([10.77362213, 18.49485922,  2.10287744]), 'targetState': array([19, 23], dtype=int32), 'currentDistance': 9.37921032689699}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8937703246777801
{'scaleFactor': 20, 'currentTarget': array([19., 23.]), 'previousTarget': array([19., 23.]), 'currentState': array([18.5639135 , 22.21761392,  0.74290854]), 'targetState': array([19, 23], dtype=int32), 'currentDistance': 0.8957116791658164}
episode index:290
target Thresh 30.591405566555974
target distance 11.0
model initialize at round 290
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 18.]), 'previousTarget': array([23., 18.]), 'currentState': array([12.49034591, 29.74386331,  3.69377184]), 'targetState': array([23, 18], dtype=int32), 'currentDistance': 15.759795506764789}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.893806791173763
{'scaleFactor': 20, 'currentTarget': array([23., 18.]), 'previousTarget': array([23., 18.]), 'currentState': array([23.13456007, 17.6642492 ,  5.21538651]), 'targetState': array([23, 18], dtype=int32), 'currentDistance': 0.3617112269767503}
episode index:291
target Thresh 30.60542131534874
target distance 10.0
model initialize at round 291
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([ 5.63505434, 14.39872972,  5.51237995]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 11.701120530411194}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8939378136283288
{'scaleFactor': 20, 'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([11.30956003,  3.75724119,  3.83740275]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 0.3933945283101816}
episode index:292
target Thresh 30.619297605110887
target distance 14.0
model initialize at round 292
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 21.]), 'previousTarget': array([ 5., 21.]), 'currentState': array([17.70647361, 11.92334891,  2.8257432 ]), 'targetState': array([ 5, 21], dtype=int32), 'currentDistance': 15.615379173147701}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8939734595716069
{'scaleFactor': 20, 'currentTarget': array([ 5., 21.]), 'previousTarget': array([ 5., 21.]), 'currentState': array([ 4.83694613, 21.51931348,  0.79449081]), 'targetState': array([ 5, 21], dtype=int32), 'currentDistance': 0.5443097089771444}
episode index:293
target Thresh 30.633035823482956
target distance 21.0
model initialize at round 293
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([24.47283859, 27.29277576,  4.58528471]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 21.99058303079381}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8938876512491489
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.33475125,  7.02230555,  5.08239502]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.665622593190032}
episode index:294
target Thresh 30.64663734429823
target distance 14.0
model initialize at round 294
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 24.]), 'previousTarget': array([ 2., 24.]), 'currentState': array([ 3.13260803, 11.44222718,  1.10226321]), 'targetState': array([ 2, 24], dtype=int32), 'currentDistance': 12.608745340352648}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8939854717345008
{'scaleFactor': 20, 'currentTarget': array([ 2., 24.]), 'previousTarget': array([ 2., 24.]), 'currentState': array([ 1.14147487, 23.92434428,  0.22965431]), 'targetState': array([ 2, 24], dtype=int32), 'currentDistance': 0.8618521813305533}
episode index:295
target Thresh 30.660103527720132
target distance 13.0
model initialize at round 295
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([25.52260656,  5.19396234,  2.63103676]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 11.729617754474653}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8941141199648133
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.67899648,  2.15690444,  2.67131063]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.082518520245188}
episode index:296
target Thresh 30.67343572037822
target distance 12.0
model initialize at round 296
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 25.]), 'previousTarget': array([ 2., 25.]), 'currentState': array([ 3.53609884, 12.31240388,  0.58912295]), 'targetState': array([ 2, 25], dtype=int32), 'currentDistance': 12.780246281110458}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8942105192054298
{'scaleFactor': 20, 'currentTarget': array([ 2., 25.]), 'previousTarget': array([ 2., 25.]), 'currentState': array([ 1.69837351, 25.26600593,  2.42878149]), 'targetState': array([ 2, 25], dtype=int32), 'currentDistance': 0.40216624964930675}
episode index:297
target Thresh 30.68663525550287
target distance 10.0
model initialize at round 297
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 16.]), 'previousTarget': array([ 4., 16.]), 'currentState': array([11.21344241,  7.48785592,  3.06710172]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 11.157524286319056}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.894337548831945
{'scaleFactor': 20, 'currentTarget': array([ 4., 16.]), 'previousTarget': array([ 4., 16.]), 'currentState': array([ 4.14791784, 16.00988739,  3.33385912]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 0.1482479231595471}
episode index:298
target Thresh 30.699703453058596
target distance 19.0
model initialize at round 298
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  2.]), 'previousTarget': array([12.,  2.]), 'currentState': array([13.55358109, 20.13961611,  4.68820739]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 18.206023370082214}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8943408956728373
{'scaleFactor': 20, 'currentTarget': array([12.,  2.]), 'previousTarget': array([12.,  2.]), 'currentState': array([12.17778047,  2.83478641,  4.26582897]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 0.8535070296947502}
episode index:299
target Thresh 30.71264161987605
target distance 13.0
model initialize at round 299
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  2.]), 'previousTarget': array([17.,  2.]), 'currentState': array([5.59002373, 5.44844527, 0.67610329]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 11.919703572168268}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8944666438469511
{'scaleFactor': 20, 'currentTarget': array([17.,  2.]), 'previousTarget': array([17.,  2.]), 'currentState': array([16.77886035,  1.73597708,  4.98847795]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 0.344399260932057}
episode index:300
target Thresh 30.725451049782684
target distance 3.0
model initialize at round 300
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  7.]), 'previousTarget': array([20.,  7.]), 'currentState': array([18.84632787,  5.45632531,  2.05198468]), 'targetState': array([20,  7], dtype=int32), 'currentDistance': 1.9271458001646309}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8947840304122436
{'scaleFactor': 20, 'currentTarget': array([20.,  7.]), 'previousTarget': array([20.,  7.]), 'currentState': array([19.66337009,  6.92931627,  0.05679394]), 'targetState': array([20,  7], dtype=int32), 'currentDistance': 0.3439707601795816}
episode index:301
target Thresh 30.738133023732168
target distance 13.0
model initialize at round 301
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 29.]), 'previousTarget': array([15., 29.]), 'currentState': array([ 3.25357987, 16.87709057,  0.27953118]), 'targetState': array([15, 29], dtype=int32), 'currentDistance': 16.880264186006013}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8947858655905433
{'scaleFactor': 20, 'currentTarget': array([15., 29.]), 'previousTarget': array([15., 29.]), 'currentState': array([14.69508664, 28.48504529,  6.15774173]), 'targetState': array([15, 29], dtype=int32), 'currentDistance': 0.598456770985056}
episode index:302
target Thresh 30.75068880993247
target distance 17.0
model initialize at round 302
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  3.]), 'previousTarget': array([26.,  3.]), 'currentState': array([19.00824471, 18.64029065,  5.09221077]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 17.13193899374942}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8948175362486893
{'scaleFactor': 20, 'currentTarget': array([26.,  3.]), 'previousTarget': array([26.,  3.]), 'currentState': array([25.04870314,  3.38668032,  5.7317456 ]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 1.026882363545908}
episode index:303
target Thresh 30.763119663972663
target distance 18.0
model initialize at round 303
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([ 5.02452686, 18.66480983,  4.35688717]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 16.937049608273522}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8948192491368803
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.39713898, 2.24405003, 3.4438372 ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.6503859035732549}
episode index:304
target Thresh 30.77542682894852
target distance 15.0
model initialize at round 304
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  8.]), 'previousTarget': array([26.,  8.]), 'currentState': array([12.88773595, 16.34507975,  6.02519032]), 'targetState': array([26,  8], dtype=int32), 'currentDistance': 15.542581010147517}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8948805540494925
{'scaleFactor': 20, 'currentTarget': array([26.,  8.]), 'previousTarget': array([26.,  8.]), 'currentState': array([26.07839616,  8.74153843,  4.80080125]), 'targetState': array([26,  8], dtype=int32), 'currentDistance': 0.7456709719744625}
episode index:305
target Thresh 30.78761153558679
target distance 12.0
model initialize at round 305
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 15.]), 'previousTarget': array([21., 15.]), 'currentState': array([12.33756477, 25.97857522,  0.35782307]), 'targetState': array([21, 15], dtype=int32), 'currentDistance': 13.984523518685581}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8949414582764015
{'scaleFactor': 20, 'currentTarget': array([21., 15.]), 'previousTarget': array([21., 15.]), 'currentState': array([20.53276343, 15.4065791 ,  3.10751298]), 'targetState': array([21, 15], dtype=int32), 'currentDistance': 0.6193678866246918}
episode index:306
target Thresh 30.799675002368296
target distance 6.0
model initialize at round 306
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 21.]), 'previousTarget': array([20., 21.]), 'currentState': array([25.33697108, 17.54686157,  2.98573631]), 'targetState': array([20, 21], dtype=int32), 'currentDistance': 6.356683509881744}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8951553167510713
{'scaleFactor': 20, 'currentTarget': array([20., 21.]), 'previousTarget': array([20., 21.]), 'currentState': array([19.68809026, 21.41470099,  2.40863678]), 'targetState': array([20, 21], dtype=int32), 'currentDistance': 0.5189071168332144}
episode index:307
target Thresh 30.81161843564976
target distance 10.0
model initialize at round 307
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 13.]), 'previousTarget': array([25., 13.]), 'currentState': array([13.34857124, 16.67569902,  4.34550118]), 'targetState': array([25, 13], dtype=int32), 'currentDistance': 12.217469276223568}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8952751545145645
{'scaleFactor': 20, 'currentTarget': array([25., 13.]), 'previousTarget': array([25., 13.]), 'currentState': array([24.21627797, 12.42194847,  5.01425294]), 'targetState': array([25, 13], dtype=int32), 'currentDistance': 0.9738397148152572}
episode index:308
target Thresh 30.823443029784467
target distance 7.0
model initialize at round 308
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 27.]), 'previousTarget': array([20., 27.]), 'currentState': array([14.43141153, 27.11487338,  0.45619028]), 'targetState': array([20, 27], dtype=int32), 'currentDistance': 5.5697731989018555}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8955179501310223
{'scaleFactor': 20, 'currentTarget': array([20., 27.]), 'previousTarget': array([20., 27.]), 'currentState': array([19.09137605, 26.66192798,  0.65081306]), 'targetState': array([20, 27], dtype=int32), 'currentDistance': 0.9694793276041189}
episode index:309
target Thresh 30.835149967241687
target distance 3.0
model initialize at round 309
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 21.]), 'previousTarget': array([ 2., 21.]), 'currentState': array([ 2.85216959, 22.388994  ,  4.35268784]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 1.6295696843637548}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8958227309370512
{'scaleFactor': 20, 'currentTarget': array([ 2., 21.]), 'previousTarget': array([ 2., 21.]), 'currentState': array([ 1.41579113, 21.11345941,  3.37301564]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 0.5951243948240791}
episode index:310
target Thresh 30.84674041872492
target distance 18.0
model initialize at round 310
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 2.]), 'previousTarget': array([7., 2.]), 'currentState': array([14.45499305, 20.6673392 ,  3.74386454]), 'targetState': array([7, 2], dtype=int32), 'currentDistance': 20.100907296225298}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8957638830015591
{'scaleFactor': 20, 'currentTarget': array([7., 2.]), 'previousTarget': array([7., 2.]), 'currentState': array([7.60380557, 2.49324239, 4.8667317 ]), 'targetState': array([7, 2], dtype=int32), 'currentDistance': 0.7796596810584054}
episode index:311
target Thresh 30.85821554328897
target distance 18.0
model initialize at round 311
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 13.]), 'previousTarget': array([25., 13.]), 'currentState': array([ 8.68092115, 18.91698098,  0.9606511 ]), 'targetState': array([25, 13], dtype=int32), 'currentDistance': 17.358657732753322}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8957625188068704
{'scaleFactor': 20, 'currentTarget': array([25., 13.]), 'previousTarget': array([25., 13.]), 'currentState': array([24.37739985, 12.87147681,  4.26820588]), 'targetState': array([25, 13], dtype=int32), 'currentDistance': 0.6357272651128772}
episode index:312
target Thresh 30.869576488455863
target distance 17.0
model initialize at round 312
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  2.]), 'previousTarget': array([21.,  2.]), 'currentState': array([ 5.68234393, 16.95409814,  5.24590761]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 21.40690633256093}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8956762034521167
{'scaleFactor': 20, 'currentTarget': array([21.,  2.]), 'previousTarget': array([21.,  2.]), 'currentState': array([20.50729919,  1.73493187,  4.94337291]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 0.5594776123631793}
episode index:313
target Thresh 30.880824390329575
target distance 9.0
model initialize at round 313
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([5.41967563, 8.90459484, 1.57662409]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 7.580924726507323}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.895852362198766
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([12.81264056,  8.41544736,  4.56389281]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.6138447282832795}
episode index:314
target Thresh 30.891960373709672
target distance 14.0
model initialize at round 314
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 18.]), 'previousTarget': array([ 2., 18.]), 'currentState': array([14.89095725, 26.73413576,  5.00292778]), 'targetState': array([ 2, 18], dtype=int32), 'currentDistance': 15.57118834240425}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8958507301100674
{'scaleFactor': 20, 'currentTarget': array([ 2., 18.]), 'previousTarget': array([ 2., 18.]), 'currentState': array([ 1.64931858, 17.36021117,  5.11099702]), 'targetState': array([ 2, 18], dtype=int32), 'currentDistance': 0.7295938621134481}
episode index:315
target Thresh 30.90298555220377
target distance 12.0
model initialize at round 315
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 16.]), 'previousTarget': array([ 5., 16.]), 'currentState': array([16.15381321, 28.45477008,  3.10762405]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 16.71911620960048}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8958777280369622
{'scaleFactor': 20, 'currentTarget': array([ 5., 16.]), 'previousTarget': array([ 5., 16.]), 'currentState': array([ 5.24651676, 16.05107967,  2.33785617]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 0.2517531428106228}
episode index:316
target Thresh 30.91390102833891
target distance 17.0
model initialize at round 316
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  5.]), 'previousTarget': array([23.,  5.]), 'currentState': array([16.83369939, 23.21331405,  3.34644032]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 19.228834384059663}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8957921383988928
{'scaleFactor': 20, 'currentTarget': array([23.,  5.]), 'previousTarget': array([23.,  5.]), 'currentState': array([23.64314075,  5.12531564,  2.00425294]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 0.6552358567885243}
episode index:317
target Thresh 30.924707893671794
target distance 17.0
model initialize at round 317
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 23.]), 'previousTarget': array([10., 23.]), 'currentState': array([9.81619685, 7.19624337, 1.34096766]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 15.804825441840446}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8958191507781693
{'scaleFactor': 20, 'currentTarget': array([10., 23.]), 'previousTarget': array([10., 23.]), 'currentState': array([10.57172272, 22.08505534,  2.36679963]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 1.0788839654298832}
episode index:318
target Thresh 30.93540722889797
target distance 2.0
model initialize at round 318
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 19.]), 'previousTarget': array([ 7., 19.]), 'currentState': array([ 6.43603754, 17.62211411,  1.75330806]), 'targetState': array([ 7, 19], dtype=int32), 'currentDistance': 1.4888328258341692}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8961143885500246
{'scaleFactor': 20, 'currentTarget': array([ 7., 19.]), 'previousTarget': array([ 7., 19.]), 'currentState': array([ 7.57606509, 18.93187612,  6.22086686]), 'targetState': array([ 7, 19], dtype=int32), 'currentDistance': 0.5800791716837456}
episode index:319
target Thresh 30.946000103959864
target distance 27.0
model initialize at round 319
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  2.]), 'previousTarget': array([11.,  2.]), 'currentState': array([ 2.94644203, 27.60836946,  4.29963112]), 'targetState': array([11,  2], dtype=int32), 'currentDistance': 26.844894900966015}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8959482285651305
{'scaleFactor': 20, 'currentTarget': array([11.,  2.]), 'previousTarget': array([11.,  2.]), 'currentState': array([11.10559461,  2.5214098 ,  4.661439  ]), 'targetState': array([11,  2], dtype=int32), 'currentDistance': 0.5319947352937343}
episode index:320
target Thresh 30.956487578153826
target distance 18.0
model initialize at round 320
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 28.]), 'previousTarget': array([20., 28.]), 'currentState': array([25.52793166, 10.81572232,  1.6255815 ]), 'targetState': array([20, 28], dtype=int32), 'currentDistance': 18.051521486453677}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8959184361761928
{'scaleFactor': 20, 'currentTarget': array([20., 28.]), 'previousTarget': array([20., 28.]), 'currentState': array([19.86561079, 27.06250916,  1.75912596]), 'targetState': array([20, 28], dtype=int32), 'currentDistance': 0.9470742005888472}
episode index:321
target Thresh 30.966870700236
target distance 19.0
model initialize at round 321
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 28.]), 'previousTarget': array([ 6., 28.]), 'currentState': array([20.59671582,  9.92907567,  3.56677747]), 'targetState': array([ 6, 28], dtype=int32), 'currentDistance': 23.22977440400045}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8957803595765614
{'scaleFactor': 20, 'currentTarget': array([ 6., 28.]), 'previousTarget': array([ 6., 28.]), 'currentState': array([ 6.90946718, 28.34211485,  2.01177573]), 'targetState': array([ 6, 28], dtype=int32), 'currentDistance': 0.9716857150602048}
episode index:322
target Thresh 30.97715050852726
target distance 16.0
model initialize at round 322
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([22.4414586 , 25.13131823,  4.73083768]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 16.862149843433343}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8957789908294473
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.76210797,  9.41691992,  4.04346863]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.9595785166537403}
episode index:323
target Thresh 30.987328031016993
target distance 10.0
model initialize at round 323
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([ 5.59282294, 11.45656117,  0.68119317]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 12.653346404424866}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8958337385351701
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.19071998,  1.70564423,  3.27978694]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.8611500893522384}
episode index:324
target Thresh 30.99740428546593
target distance 8.0
model initialize at round 324
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 4.]), 'previousTarget': array([8., 4.]), 'currentState': array([13.16363484, 10.7841291 ,  4.46583818]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 8.525698348258176}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8960034502624465
{'scaleFactor': 20, 'currentTarget': array([8., 4.]), 'previousTarget': array([8., 4.]), 'currentState': array([8.06441785, 4.79618146, 4.95245836]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 0.7987831851451465}
episode index:325
target Thresh 31.007380279507917
target distance 12.0
model initialize at round 325
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 26.]), 'previousTarget': array([12., 26.]), 'currentState': array([23.25166803, 15.50744398,  3.04157937]), 'targetState': array([12, 26], dtype=int32), 'currentDistance': 15.384855063342634}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8960291515653495
{'scaleFactor': 20, 'currentTarget': array([12., 26.]), 'previousTarget': array([12., 26.]), 'currentState': array([12.04098147, 26.65271311,  1.83200108]), 'targetState': array([12, 26], dtype=int32), 'currentDistance': 0.6539983863590143}
episode index:326
target Thresh 31.01725701075067
target distance 24.0
model initialize at round 326
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 14.]), 'previousTarget': array([ 2., 14.]), 'currentState': array([24.94042696,  7.30755231,  3.26181555]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 23.896695274125758}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8959457162785104
{'scaleFactor': 20, 'currentTarget': array([ 2., 14.]), 'previousTarget': array([ 2., 14.]), 'currentState': array([ 2.97791801, 14.22283392,  3.95794713]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 1.002984843701802}
episode index:327
target Thresh 31.02703546687554
target distance 14.0
model initialize at round 327
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([ 2.99586668, 10.35670212,  1.94758993]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 14.602769941772774}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8959714368843954
{'scaleFactor': 20, 'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([16.30356839, 16.34668125,  5.2072338 ]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 0.7204020821177897}
episode index:328
target Thresh 31.03671662573629
target distance 18.0
model initialize at round 328
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([ 5.99306278, 24.3102526 ,  3.70972419]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 20.25316966075382}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8958886842275097
{'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([18.20214063,  7.71084153,  5.8067584 ]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 0.35280795871515325}
episode index:329
target Thresh 31.046301455456874
target distance 13.0
model initialize at round 329
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([20.50905486,  5.74507819,  1.46862572]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 14.869673462020236}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8959144217753318
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([5.17347221, 8.9924311 , 3.20782688]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.82656244490436}
episode index:330
target Thresh 31.055790914528256
target distance 7.0
model initialize at round 330
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 15.]), 'previousTarget': array([18., 15.]), 'currentState': array([26.06747126, 14.30111229,  1.89372176]), 'targetState': array([18, 15], dtype=int32), 'currentDistance': 8.097687107310238}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8960520825838685
{'scaleFactor': 20, 'currentTarget': array([18., 15.]), 'previousTarget': array([18., 15.]), 'currentState': array([17.78470255, 14.33466613,  5.39570742]), 'targetState': array([18, 15], dtype=int32), 'currentDistance': 0.6993011885884465}
episode index:331
target Thresh 31.065185951904247
target distance 14.0
model initialize at round 331
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([5.82048933, 8.46941667, 0.05154395]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 16.79276562914362}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8960771729224979
{'scaleFactor': 20, 'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([16.48898073, 20.55473881,  5.94292465]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 0.6777892191214719}
episode index:332
target Thresh 31.07448750709641
target distance 11.0
model initialize at round 332
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 19.]), 'previousTarget': array([25., 19.]), 'currentState': array([19.67140747,  7.80306056,  0.89271229]), 'targetState': array([25, 19], dtype=int32), 'currentDistance': 12.400215771883694}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8961572555696613
{'scaleFactor': 20, 'currentTarget': array([25., 19.]), 'previousTarget': array([25., 19.]), 'currentState': array([25.03117935, 19.14758832,  2.14422819]), 'targetState': array([25, 19], dtype=int32), 'currentDistance': 0.15084583053701145}
episode index:333
target Thresh 31.083696510268023
target distance 26.0
model initialize at round 333
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'previousTarget': array([19.,  3.]), 'currentState': array([ 7.46746097, 28.17601798,  4.76155442]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 27.69171965722287}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.8959229731805223
{'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'previousTarget': array([19.,  3.]), 'currentState': array([19.02726263,  2.7359549 ,  5.74915758]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 0.2654488020504935}
episode index:334
target Thresh 31.092813882327075
target distance 6.0
model initialize at round 334
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([19.61505572,  3.47326853,  1.29505509]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 8.619960726274332}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8960589647513297
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([12.25618888,  8.844763  ,  3.92903804]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.7598378130914613}
episode index:335
target Thresh 31.10184053501837
target distance 22.0
model initialize at round 335
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([27.47497402,  6.81045649,  1.51243752]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 27.960626556367004}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.8958757945034093
{'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([ 3.74403576, 21.4199016 ,  5.4169473 ]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 0.6340598124219499}
episode index:336
target Thresh 31.11077737101469
target distance 2.0
model initialize at round 336
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([13.23715814,  8.66620601,  0.41946077]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.884526962129362}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8961550948164556
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.6696005 ,  7.76973484,  4.72555596]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.40272306954580955}
episode index:337
target Thresh 31.1196252840071
target distance 14.0
model initialize at round 337
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([ 6.47567881, 27.28667469,  4.58928347]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 13.746147246217692}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8962064621320389
{'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([ 9.68189319, 14.15129375,  4.85678953]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 0.3522523848539546}
episode index:338
target Thresh 31.128385158794256
target distance 5.0
model initialize at round 338
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 8.]), 'previousTarget': array([8., 8.]), 'currentState': array([ 1.59912292, 12.06729882,  4.73899364]), 'targetState': array([8, 8], dtype=int32), 'currentDistance': 7.583808213999927}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.896396401801266
{'scaleFactor': 20, 'currentTarget': array([8., 8.]), 'previousTarget': array([8., 8.]), 'currentState': array([7.35809854, 8.58119304, 4.73904705]), 'targetState': array([8, 8], dtype=int32), 'currentDistance': 0.8659231111709773}
episode index:339
target Thresh 31.137057871370946
target distance 5.0
model initialize at round 339
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  5.]), 'previousTarget': array([11.,  5.]), 'currentState': array([6.87256157, 5.43910541, 0.01574111]), 'targetState': array([11,  5], dtype=int32), 'currentDistance': 4.150730246072374}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8966425888547918
{'scaleFactor': 20, 'currentTarget': array([11.,  5.]), 'previousTarget': array([11.,  5.]), 'currentState': array([10.14396206,  4.41452413,  4.62970575]), 'targetState': array([11,  5], dtype=int32), 'currentDistance': 1.0371031481497643}
episode index:340
target Thresh 31.145644289015653
target distance 10.0
model initialize at round 340
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([18.3397773 , 19.72423078,  4.31619358]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 12.183308049491416}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8967464679135958
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([10.38952109, 10.09882993,  4.80344503]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.6184269232672068}
episode index:341
target Thresh 31.154145270377295
target distance 21.0
model initialize at round 341
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 29.]), 'previousTarget': array([15., 29.]), 'currentState': array([8.80221208, 9.47947418, 2.08393981]), 'targetState': array([15, 29], dtype=int32), 'currentDistance': 20.48080816841521}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.896664594652939
{'scaleFactor': 20, 'currentTarget': array([15., 29.]), 'previousTarget': array([15., 29.]), 'currentState': array([14.62409914, 29.23115067,  4.08215576]), 'targetState': array([15, 29], dtype=int32), 'currentDistance': 0.44128459471401354}
episode index:342
target Thresh 31.162561665561093
target distance 22.0
model initialize at round 342
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 13.]), 'previousTarget': array([25., 13.]), 'currentState': array([ 4.61906355, 16.45937055,  5.54964585]), 'targetState': array([25, 13], dtype=int32), 'currentDistance': 20.672440955775016}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.896583198787388
{'scaleFactor': 20, 'currentTarget': array([25., 13.]), 'previousTarget': array([25., 13.]), 'currentState': array([25.06285228, 13.1208779 ,  4.9016608 ]), 'targetState': array([25, 13], dtype=int32), 'currentDistance': 0.1362419702848264}
episode index:343
target Thresh 31.17089431621358
target distance 10.0
model initialize at round 343
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([26.64830389, 15.44690948,  4.09782887]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 11.539557168628187}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8966863445697124
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.3424584 , 10.67207889,  2.83360195]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.4741413362629015}
episode index:344
target Thresh 31.179144055606763
target distance 20.0
model initialize at round 344
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 23.]), 'previousTarget': array([ 6., 23.]), 'currentState': array([27.05480506, 17.31140168,  1.90341824]), 'targetState': array([ 6, 23], dtype=int32), 'currentDistance': 21.809744766808553}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8966307929129856
{'scaleFactor': 20, 'currentTarget': array([ 6., 23.]), 'previousTarget': array([ 6., 23.]), 'currentState': array([ 5.81772758, 22.8784302 ,  3.47254341]), 'targetState': array([ 6, 23], dtype=int32), 'currentDistance': 0.2190946201644516}
episode index:345
target Thresh 31.18731170872146
target distance 22.0
model initialize at round 345
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 25.]), 'previousTarget': array([20., 25.]), 'currentState': array([10.41871404,  3.57612742,  1.78219938]), 'targetState': array([20, 25], dtype=int32), 'currentDistance': 23.46877407894279}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8965250922243391
{'scaleFactor': 20, 'currentTarget': array([20., 25.]), 'previousTarget': array([20., 25.]), 'currentState': array([20.63280554, 25.66568062,  1.13533282]), 'targetState': array([20, 25], dtype=int32), 'currentDistance': 0.9184625919541968}
episode index:346
target Thresh 31.195398092329786
target distance 23.0
model initialize at round 346
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  6.]), 'previousTarget': array([23.,  6.]), 'currentState': array([15.45225355, 27.11223716,  4.52134413]), 'targetState': array([23,  6], dtype=int32), 'currentDistance': 22.42086159028007}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8964200007615638
{'scaleFactor': 20, 'currentTarget': array([23.,  6.]), 'previousTarget': array([23.,  6.]), 'currentState': array([22.88270643,  6.24373751,  5.71556854]), 'targetState': array([23,  6], dtype=int32), 'currentDistance': 0.2704916921789041}
episode index:347
target Thresh 31.203404015076842
target distance 11.0
model initialize at round 347
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([18.52395011, 25.39931692,  4.02088022]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 11.530866405087828}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8965224299200276
{'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([ 7.14835077, 25.11921413,  3.91608101]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 0.1903154177938132}
episode index:348
target Thresh 31.211330277561572
target distance 16.0
model initialize at round 348
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 29.]), 'previousTarget': array([ 9., 29.]), 'currentState': array([20.39306258, 13.94433295,  1.60572737]), 'targetState': array([ 9, 29], dtype=int32), 'currentDistance': 18.88054515364965}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8964679846279902
{'scaleFactor': 20, 'currentTarget': array([ 9., 29.]), 'previousTarget': array([ 9., 29.]), 'currentState': array([ 9.73619266, 28.64112147,  5.92872068]), 'targetState': array([ 9, 29], dtype=int32), 'currentDistance': 0.8190075860404221}
episode index:349
target Thresh 31.219177672416834
target distance 11.0
model initialize at round 349
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 22.]), 'previousTarget': array([11., 22.]), 'currentState': array([21.49829979, 22.60645105,  2.88350213]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 10.515801505523935}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8965696913802159
{'scaleFactor': 20, 'currentTarget': array([11., 22.]), 'previousTarget': array([11., 22.]), 'currentState': array([10.50159681, 21.80016516,  4.14718959]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 0.5369727205366595}
episode index:350
target Thresh 31.226946984388647
target distance 21.0
model initialize at round 350
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 27.]), 'previousTarget': array([ 7., 27.]), 'currentState': array([23.01333144,  7.34370662,  1.93465632]), 'targetState': array([ 7, 27], dtype=int32), 'currentDistance': 25.35343474708624}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8964169093346424
{'scaleFactor': 20, 'currentTarget': array([ 7., 27.]), 'previousTarget': array([ 7., 27.]), 'currentState': array([ 6.98741124, 26.6251181 ,  1.2450971 ]), 'targetState': array([ 7, 27], dtype=int32), 'currentDistance': 0.3750932116962325}
episode index:351
target Thresh 31.234638990414687
target distance 13.0
model initialize at round 351
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 29.]), 'previousTarget': array([19., 29.]), 'currentState': array([17.66552601, 16.2416841 ,  1.15410393]), 'targetState': array([19, 29], dtype=int32), 'currentDistance': 12.827916646753684}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8964917041786574
{'scaleFactor': 20, 'currentTarget': array([19., 29.]), 'previousTarget': array([19., 29.]), 'currentState': array([18.54035843, 28.84727822,  0.21232444]), 'targetState': array([19, 29], dtype=int32), 'currentDistance': 0.48434937279633433}
episode index:352
target Thresh 31.242254459701964
target distance 8.0
model initialize at round 352
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 17.]), 'previousTarget': array([20., 17.]), 'currentState': array([22.85532095, 10.23373334,  1.30876923]), 'targetState': array([20, 17], dtype=int32), 'currentDistance': 7.344060335811226}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8966733027787179
{'scaleFactor': 20, 'currentTarget': array([20., 17.]), 'previousTarget': array([20., 17.]), 'currentState': array([19.51581346, 16.04074006,  0.7337532 ]), 'targetState': array([20, 17], dtype=int32), 'currentDistance': 1.0745307041575443}
episode index:353
target Thresh 31.249794153803755
target distance 13.0
model initialize at round 353
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 13.]), 'previousTarget': array([ 5., 13.]), 'currentState': array([17.41825153, 23.42077335,  3.34943867]), 'targetState': array([ 5, 13], dtype=int32), 'currentDistance': 16.21127658076136}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8966442394141344
{'scaleFactor': 20, 'currentTarget': array([ 5., 13.]), 'previousTarget': array([ 5., 13.]), 'currentState': array([ 5.03195777, 12.36936127,  5.51394761]), 'targetState': array([ 5, 13], dtype=int32), 'currentDistance': 0.63144794133781}
episode index:354
target Thresh 31.257258826695754
target distance 9.0
model initialize at round 354
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([8.3662722 , 8.01730556, 0.38646954]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 8.20840623867148}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8967973262042354
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.74030066,  4.93118117,  0.38509287]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.26866294977776084}
episode index:355
target Thresh 31.26464922485147
target distance 1.0
model initialize at round 355
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 12.]), 'previousTarget': array([23., 12.]), 'currentState': array([23.11519735, 11.35955092,  5.5336338 ]), 'targetState': array([23, 12], dtype=int32), 'currentDistance': 0.6507268647683639}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.897087221355347
{'scaleFactor': 20, 'currentTarget': array([23., 12.]), 'previousTarget': array([23., 12.]), 'currentState': array([23.11519735, 11.35955092,  5.5336338 ]), 'targetState': array([23, 12], dtype=int32), 'currentDistance': 0.6507268647683639}
episode index:356
target Thresh 31.271966087316876
target distance 15.0
model initialize at round 356
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([18.57754461, 28.10055086,  4.71543956]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 16.504538956940404}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8971076551190823
{'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([10.52466617, 14.706916  ,  3.74099448]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 0.8803435856804707}
episode index:357
target Thresh 31.27921014578432
target distance 10.0
model initialize at round 357
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([ 3.8493679 , 12.80828363,  1.05012158]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 9.676426407369178}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8972581646017105
{'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([ 9.56928304, 20.17668072,  5.83242043]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 1.0009684390446028}
episode index:358
target Thresh 31.286382124665675
target distance 21.0
model initialize at round 358
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 27.]), 'previousTarget': array([26., 27.]), 'currentState': array([ 5.03476968, 23.68261081,  0.54013515]), 'targetState': array([26, 27], dtype=int32), 'currentDistance': 21.22606778404313}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8971787430088616
{'scaleFactor': 20, 'currentTarget': array([26., 27.]), 'previousTarget': array([26., 27.]), 'currentState': array([26.46730983, 26.25977979,  5.38786794]), 'targetState': array([26, 27], dtype=int32), 'currentDistance': 0.8753881627264621}
episode index:359
target Thresh 31.29348274116482
target distance 7.0
model initialize at round 359
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([14.37052339, 17.97675673,  5.89237661]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 6.368052095015988}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8973549020838371
{'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([20.34573011, 14.95578052,  0.11591767]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 0.3485465087139178}
episode index:360
target Thresh 31.30051270534931
target distance 8.0
model initialize at round 360
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 13.]), 'previousTarget': array([10., 13.]), 'currentState': array([12.5310458 ,  5.83232675,  3.62699676]), 'targetState': array([10, 13], dtype=int32), 'currentDistance': 7.601429639677149}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8975034759005023
{'scaleFactor': 20, 'currentTarget': array([10., 13.]), 'previousTarget': array([10., 13.]), 'currentState': array([ 9.8008152 , 13.21293838,  2.41747042]), 'targetState': array([10, 13], dtype=int32), 'currentDistance': 0.291577329848469}
episode index:361
target Thresh 31.30747272022143
target distance 16.0
model initialize at round 361
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([18.40005065, 15.52206337,  3.83618546]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 15.093393120454948}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8975477128385773
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.19928029, 11.00704867,  2.64375794]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.19940490861347443}
episode index:362
target Thresh 31.31436348178846
target distance 10.0
model initialize at round 362
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 18.]), 'previousTarget': array([24., 18.]), 'currentState': array([24.66537194,  8.24274349,  1.15474003]), 'targetState': array([24, 18], dtype=int32), 'currentDistance': 9.779916890734437}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.897668738834617
{'scaleFactor': 20, 'currentTarget': array([24., 18.]), 'previousTarget': array([24., 18.]), 'currentState': array([23.21066278, 17.76157354,  0.10834795]), 'targetState': array([24, 18], dtype=int32), 'currentDistance': 0.8245607452415583}
episode index:363
target Thresh 31.32118567913231
target distance 9.0
model initialize at round 363
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  2.]), 'previousTarget': array([18.,  2.]), 'currentState': array([13.54169974, 10.15991165,  4.67423344]), 'targetState': array([18,  2], dtype=int32), 'currentDistance': 9.298419180326663}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8977890998526564
{'scaleFactor': 20, 'currentTarget': array([18.,  2.]), 'previousTarget': array([18.,  2.]), 'currentState': array([17.87043266,  1.58298773,  4.50364578]), 'targetState': array([18,  2], dtype=int32), 'currentDistance': 0.43667713990089885}
episode index:364
target Thresh 31.32793999447839
target distance 21.0
model initialize at round 364
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 20.]), 'previousTarget': array([ 6., 20.]), 'currentState': array([25.33060063, 10.78670635,  4.27867067]), 'targetState': array([ 6, 20], dtype=int32), 'currentDistance': 21.413941730776507}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8976857279479679
{'scaleFactor': 20, 'currentTarget': array([ 6., 20.]), 'previousTarget': array([ 6., 20.]), 'currentState': array([ 6.19852488, 19.7399846 ,  0.89402497]), 'targetState': array([ 6, 20], dtype=int32), 'currentDistance': 0.32713932168810084}
episode index:365
target Thresh 31.334627103263863
target distance 12.0
model initialize at round 365
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([24.19574649, 17.52163456,  3.20415211]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 14.069932302074712}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8977289834658249
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.39488822,  9.63586414,  1.69838624]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.7485051119119828}
episode index:366
target Thresh 31.341247674205185
target distance 12.0
model initialize at round 366
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([26.49727252, 16.76848102,  1.48419445]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 15.809309605513855}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8977471117806559
{'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([12.61795326, 25.0222899 ,  3.66286029]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 0.382696418182507}
episode index:367
target Thresh 31.347802369364967
target distance 10.0
model initialize at round 367
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  6.]), 'previousTarget': array([17.,  6.]), 'currentState': array([12.62330086, 15.03196717,  4.76443291]), 'targetState': array([17,  6], dtype=int32), 'currentDistance': 10.036529591043081}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8978659515567982
{'scaleFactor': 20, 'currentTarget': array([17.,  6.]), 'previousTarget': array([17.,  6.]), 'currentState': array([16.15483421,  6.51362971,  3.85737902]), 'targetState': array([17,  6], dtype=int32), 'currentDistance': 0.9889998417049377}
episode index:368
target Thresh 31.35429184421818
target distance 5.0
model initialize at round 368
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 7.]), 'previousTarget': array([9., 7.]), 'currentState': array([4.30645494, 2.34516664, 5.90550232]), 'targetState': array([9, 7], dtype=int32), 'currentDistance': 6.610358452400583}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8980359517151808
{'scaleFactor': 20, 'currentTarget': array([9., 7.]), 'previousTarget': array([9., 7.]), 'currentState': array([9.54876187, 6.71286239, 0.16671818]), 'targetState': array([9, 7], dtype=int32), 'currentDistance': 0.6193444867187272}
episode index:369
target Thresh 31.36071674771773
target distance 11.0
model initialize at round 369
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 16.]), 'previousTarget': array([ 9., 16.]), 'currentState': array([18.32141814, 14.87854474,  4.22382247]), 'targetState': array([ 9, 16], dtype=int32), 'currentDistance': 9.388636642836746}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.898153368465683
{'scaleFactor': 20, 'currentTarget': array([ 9., 16.]), 'previousTarget': array([ 9., 16.]), 'currentState': array([ 8.91408274, 16.09483195,  2.02814568]), 'targetState': array([ 9, 16], dtype=int32), 'currentDistance': 0.12796434609771012}
episode index:370
target Thresh 31.367077722359312
target distance 16.0
model initialize at round 370
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 19.]), 'previousTarget': array([18., 19.]), 'currentState': array([ 3.57230511, 28.3997961 ,  4.90852309]), 'targetState': array([18, 19], dtype=int32), 'currentDistance': 17.219597745026093}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8981701574321066
{'scaleFactor': 20, 'currentTarget': array([18., 19.]), 'previousTarget': array([18., 19.]), 'currentState': array([17.91777174, 19.92080278,  5.28429723]), 'targetState': array([18, 19], dtype=int32), 'currentDistance': 0.9244670078846865}
episode index:371
target Thresh 31.373375404245692
target distance 15.0
model initialize at round 371
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 12.]), 'previousTarget': array([ 4., 12.]), 'currentState': array([20.13371454, 25.75618352,  4.44151085]), 'targetState': array([ 4, 12], dtype=int32), 'currentDistance': 21.20210673177058}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8980910597313992
{'scaleFactor': 20, 'currentTarget': array([ 4., 12.]), 'previousTarget': array([ 4., 12.]), 'currentState': array([ 3.32513677, 11.92334924,  4.1219755 ]), 'targetState': array([ 4, 12], dtype=int32), 'currentDistance': 0.6792022622746392}
episode index:372
target Thresh 31.379610423150307
target distance 21.0
model initialize at round 372
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([19.74844282, 24.33593658,  5.57235456]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 25.581023830177383}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.8979206112105377
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.52288614, 4.75492525, 2.28196297]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.5774699602324529}
episode index:373
target Thresh 31.385783402580245
target distance 18.0
model initialize at round 373
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 15.]), 'previousTarget': array([ 2., 15.]), 'currentState': array([18.48396681, 25.69600069,  3.82127414]), 'targetState': array([ 2, 15], dtype=int32), 'currentDistance': 19.650078690695587}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8979378878517096
{'scaleFactor': 20, 'currentTarget': array([ 2., 15.]), 'previousTarget': array([ 2., 15.]), 'currentState': array([ 2.41340388, 15.20916985,  2.30112124]), 'targetState': array([ 2, 15], dtype=int32), 'currentDistance': 0.4633085302571043}
episode index:374
target Thresh 31.39189495983859
target distance 6.0
model initialize at round 374
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([13.46030865,  9.5940895 ,  2.90724146]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 5.4925324619051725}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8981308508174384
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([8.34748735, 8.6261893 , 3.79726875]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.5103742774509125}
episode index:375
target Thresh 31.397945706086166
target distance 13.0
model initialize at round 375
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 17.]), 'previousTarget': array([24., 17.]), 'currentState': array([12.64074145, 18.3746453 ,  1.23448854]), 'targetState': array([24, 17], dtype=int32), 'currentDistance': 11.442132868865666}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.898221102139485
{'scaleFactor': 20, 'currentTarget': array([24., 17.]), 'previousTarget': array([24., 17.]), 'currentState': array([23.66068097, 17.00349193,  5.53489088]), 'targetState': array([24, 17], dtype=int32), 'currentDistance': 0.3393369947239476}
episode index:376
target Thresh 31.403936246402637
target distance 20.0
model initialize at round 376
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([ 8.35137321, 27.33825697,  3.94922614]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 21.108095156288908}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8981897063028184
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.1261552 ,  7.70438605,  3.98988855]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.1223922846411307}
episode index:377
target Thresh 31.409867179847026
target distance 6.0
model initialize at round 377
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 2.]), 'previousTarget': array([9., 2.]), 'currentState': array([13.49808659,  5.75937092,  3.68347323]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 5.862222506820788}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8983804716300596
{'scaleFactor': 20, 'currentTarget': array([9., 2.]), 'previousTarget': array([9., 2.]), 'currentState': array([9.11623417, 2.68602827, 2.29763719]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 0.6958054088160023}
episode index:378
target Thresh 31.41573909951762
target distance 17.0
model initialize at round 378
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([26.01006142, 16.53184478,  4.97190285]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 16.209019239464006}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.898396306995175
{'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([10.74965607, 13.46744528,  3.90119135]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 0.9195644384738499}
episode index:379
target Thresh 31.42155259261128
target distance 21.0
model initialize at round 379
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([24.51477652,  2.73337588,  1.4608857 ]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 22.831920168756323}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8982954176468754
{'scaleFactor': 20, 'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([13.71020155, 22.52678766,  0.73248301]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 0.5548991435737503}
episode index:380
target Thresh 31.427308240482162
target distance 6.0
model initialize at round 380
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 13.]), 'previousTarget': array([ 5., 13.]), 'currentState': array([9.32502907, 9.21550204, 4.01385939]), 'targetState': array([ 5, 13], dtype=int32), 'currentDistance': 5.74702542742019}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8984844034273297
{'scaleFactor': 20, 'currentTarget': array([ 5., 13.]), 'previousTarget': array([ 5., 13.]), 'currentState': array([ 5.41913226, 12.71799276,  2.2585203 ]), 'targetState': array([ 5, 13], dtype=int32), 'currentDistance': 0.505173178076249}
episode index:381
target Thresh 31.433006618699846
target distance 16.0
model initialize at round 381
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 26.]), 'previousTarget': array([22., 26.]), 'currentState': array([ 7.58183321, 20.42537683,  0.66155785]), 'targetState': array([22, 26], dtype=int32), 'currentDistance': 15.458329700773852}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8985479120425146
{'scaleFactor': 20, 'currentTarget': array([22., 26.]), 'previousTarget': array([22., 26.]), 'currentState': array([21.59660383, 25.77421743,  1.05958   ]), 'targetState': array([22, 26], dtype=int32), 'currentDistance': 0.46228371891063846}
episode index:382
target Thresh 31.4386482971069
target distance 4.0
model initialize at round 382
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 22.]), 'previousTarget': array([23., 22.]), 'currentState': array([25.38917095, 21.48789136,  1.83770311]), 'targetState': array([23, 22], dtype=int32), 'currentDistance': 2.443438782573576}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8987866903400538
{'scaleFactor': 20, 'currentTarget': array([23., 22.]), 'previousTarget': array([23., 22.]), 'currentState': array([23.78823244, 22.01404151,  3.83034742]), 'targetState': array([23, 22], dtype=int32), 'currentDistance': 0.7883574971228052}
episode index:383
target Thresh 31.444233839875878
target distance 3.0
model initialize at round 383
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 28.]), 'previousTarget': array([12., 28.]), 'currentState': array([10.22320563, 23.84407783,  0.25287503]), 'targetState': array([12, 28], dtype=int32), 'currentDistance': 4.519810540594739}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8989984437506265
{'scaleFactor': 20, 'currentTarget': array([12., 28.]), 'previousTarget': array([12., 28.]), 'currentState': array([11.27637732, 27.04112352,  0.25272244]), 'targetState': array([12, 28], dtype=int32), 'currentDistance': 1.2012801067976888}
episode index:384
target Thresh 31.4497638055657
target distance 16.0
model initialize at round 384
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 28.]), 'previousTarget': array([13., 28.]), 'currentState': array([ 9.21011134, 13.98691055,  1.38697657]), 'targetState': array([13, 28], dtype=int32), 'currentDistance': 14.516539943446796}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8990601223238143
{'scaleFactor': 20, 'currentTarget': array([13., 28.]), 'previousTarget': array([13., 28.]), 'currentState': array([13.98866637, 28.02589502,  1.80323418]), 'targetState': array([13, 28], dtype=int32), 'currentDistance': 0.989005426570089}
episode index:385
target Thresh 31.455238747177546
target distance 14.0
model initialize at round 385
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([21.33405884,  6.54563738,  2.98757124]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 12.47215401615068}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8991214813189027
{'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([22.48598329, 18.91203334,  0.96990377]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 0.4938804419581126}
episode index:386
target Thresh 31.46065921221014
target distance 27.0
model initialize at round 386
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 29.]), 'previousTarget': array([ 4., 29.]), 'currentState': array([5.82249287, 3.97210126, 1.41323903]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 25.094166568299915}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.8989545363063217
{'scaleFactor': 20, 'currentTarget': array([ 4., 29.]), 'previousTarget': array([ 4., 29.]), 'currentState': array([ 3.75341742, 28.98955263,  5.42827043]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 0.2468037999731644}
episode index:387
target Thresh 31.466025742714503
target distance 7.0
model initialize at round 387
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([25.75282988, 20.43665354,  4.05885975]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 7.915317640523803}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8991134060838828
{'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([20.37243788, 15.03363737,  4.75229966]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 0.37395379908496057}
episode index:388
target Thresh 31.47133887534816
target distance 25.0
model initialize at round 388
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 27.]), 'previousTarget': array([13., 27.]), 'currentState': array([20.85850409,  3.67701133,  2.66497082]), 'targetState': array([13, 27], dtype=int32), 'currentDistance': 24.611336557457445}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8990130074940561
{'scaleFactor': 20, 'currentTarget': array([13., 27.]), 'previousTarget': array([13., 27.]), 'currentState': array([13.78735654, 27.43000876,  1.17808136]), 'targetState': array([13, 27], dtype=int32), 'currentDistance': 0.8971275613819911}
episode index:389
target Thresh 31.476599141428796
target distance 14.0
model initialize at round 389
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 15.]), 'previousTarget': array([12., 15.]), 'currentState': array([24.86605841, 27.36447132,  4.27974007]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 17.84420382297584}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8990035850498629
{'scaleFactor': 20, 'currentTarget': array([12., 15.]), 'previousTarget': array([12., 15.]), 'currentState': array([12.58710529, 15.94876788,  4.23551802]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 1.1157298538690708}
episode index:390
target Thresh 31.481807066987407
target distance 13.0
model initialize at round 390
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 22.]), 'previousTarget': array([18., 22.]), 'currentState': array([18.98958351, 10.34626906,  3.22414196]), 'targetState': array([18, 22], dtype=int32), 'currentDistance': 11.69567101305254}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8990881419881165
{'scaleFactor': 20, 'currentTarget': array([18., 22.]), 'previousTarget': array([18., 22.]), 'currentState': array([18.61539915, 21.2619323 ,  2.10265421]), 'targetState': array([18, 22], dtype=int32), 'currentDistance': 0.9609682829302155}
episode index:391
target Thresh 31.48696317282089
target distance 6.0
model initialize at round 391
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 21.]), 'previousTarget': array([10., 21.]), 'currentState': array([15.67148073, 21.65059479,  2.77725983]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 5.708674742181064}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8992698023401876
{'scaleFactor': 20, 'currentTarget': array([10., 21.]), 'previousTarget': array([10., 21.]), 'currentState': array([10.9157957 , 20.71186395,  2.81543271]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 0.9600542446055282}
episode index:392
target Thresh 31.49206797454412
target distance 18.0
model initialize at round 392
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 22.]), 'previousTarget': array([11., 22.]), 'currentState': array([8.40388018, 5.57386514, 2.94284976]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 16.630025393333273}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.899259798401049
{'scaleFactor': 20, 'currentTarget': array([11., 22.]), 'previousTarget': array([11., 22.]), 'currentState': array([11.38221213, 21.6135123 ,  1.30703634]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 0.5435612669972767}
episode index:393
target Thresh 31.497121982641527
target distance 4.0
model initialize at round 393
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 14.]), 'previousTarget': array([15., 14.]), 'currentState': array([12.07478372, 14.70492161,  0.13191145]), 'targetState': array([15, 14], dtype=int32), 'currentDistance': 3.0089541001718376}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8994401009431784
{'scaleFactor': 20, 'currentTarget': array([15., 14.]), 'previousTarget': array([15., 14.]), 'currentState': array([15.24193814, 14.40205334,  2.2795766 ]), 'targetState': array([15, 14], dtype=int32), 'currentDistance': 0.46923443323063124}
episode index:394
target Thresh 31.502125702518136
target distance 11.0
model initialize at round 394
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 13.]), 'previousTarget': array([13., 13.]), 'currentState': array([24.33102547, 13.349906  ,  3.90037107]), 'targetState': array([13, 13], dtype=int32), 'currentDistance': 11.336426791401628}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8995226965051121
{'scaleFactor': 20, 'currentTarget': array([13., 13.]), 'previousTarget': array([13., 13.]), 'currentState': array([12.93067248, 13.22667756,  2.61275232]), 'targetState': array([13, 13], dtype=int32), 'currentDistance': 0.2370422300027266}
episode index:395
target Thresh 31.507079634550095
target distance 13.0
model initialize at round 395
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 11.]), 'previousTarget': array([25., 11.]), 'currentState': array([25.76241554, 22.15207954,  5.15841831]), 'targetState': array([25, 11], dtype=int32), 'currentDistance': 11.178110553081725}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.899604874917743
{'scaleFactor': 20, 'currentTarget': array([25., 11.]), 'previousTarget': array([25., 11.]), 'currentState': array([24.91485858, 10.45901206,  5.2321425 ]), 'targetState': array([25, 11], dtype=int32), 'currentDistance': 0.5476467964650445}
episode index:396
target Thresh 31.511984274134743
target distance 10.0
model initialize at round 396
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  4.]), 'previousTarget': array([20.,  4.]), 'currentState': array([11.73426796,  7.16517569,  5.35185953]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 8.851026110868125}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8997103541985573
{'scaleFactor': 20, 'currentTarget': array([20.,  4.]), 'previousTarget': array([20.,  4.]), 'currentState': array([20.78399535,  3.74593782,  6.15215652]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 0.8241336674659502}
episode index:397
target Thresh 31.516840111740123
target distance 6.0
model initialize at round 397
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 17.]), 'previousTarget': array([ 7., 17.]), 'currentState': array([12.24561743, 23.50442515,  3.0455972 ]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 8.356078551212763}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8998391976550936
{'scaleFactor': 20, 'currentTarget': array([ 7., 17.]), 'previousTarget': array([ 7., 17.]), 'currentState': array([ 6.90930613, 16.94969685,  4.90178442]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 0.10371010003763655}
episode index:398
target Thresh 31.521647632954043
target distance 11.0
model initialize at round 398
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 25.]), 'previousTarget': array([11., 25.]), 'currentState': array([20.43357684, 22.61539139,  3.77725303]), 'targetState': array([11, 25], dtype=int32), 'currentDistance': 9.730299593492008}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8999199649489579
{'scaleFactor': 20, 'currentTarget': array([11., 25.]), 'previousTarget': array([11., 25.]), 'currentState': array([10.88779942, 24.34791255,  5.5537374 ]), 'targetState': array([11, 25], dtype=int32), 'currentDistance': 0.6616698696215055}
episode index:399
target Thresh 31.526407318532627
target distance 22.0
model initialize at round 399
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 23.]), 'previousTarget': array([ 4., 23.]), 'currentState': array([26.98034415, 13.36795959,  1.95898503]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 24.917311644820185}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8998203109231888
{'scaleFactor': 20, 'currentTarget': array([ 4., 23.]), 'previousTarget': array([ 4., 23.]), 'currentState': array([ 4.5886337 , 22.38655489,  1.22875619]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 0.8501791180663386}
episode index:400
target Thresh 31.531119644448406
target distance 3.0
model initialize at round 400
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 22.]), 'previousTarget': array([26., 22.]), 'currentState': array([26.80969038, 23.32513556,  5.60475886]), 'targetState': array([26, 22], dtype=int32), 'currentDistance': 1.5529271573420154}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9000451979283679
{'scaleFactor': 20, 'currentTarget': array([26., 22.]), 'previousTarget': array([26., 22.]), 'currentState': array([26.64764331, 21.64922911,  3.60725904]), 'targetState': array([26, 22], dtype=int32), 'currentDistance': 0.7365338244353059}
episode index:401
target Thresh 31.53578508193789
target distance 6.0
model initialize at round 401
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([ 4.56419146, 24.41441617,  6.06424427]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 7.111651951211123}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9001958218389938
{'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([ 9.34644563, 29.15275   ,  6.06412768]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 0.671167548935307}
episode index:402
target Thresh 31.540404097548727
target distance 9.0
model initialize at round 402
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 22.]), 'previousTarget': array([22., 22.]), 'currentState': array([21.10096323, 14.67993884,  2.52076918]), 'targetState': array([22, 22], dtype=int32), 'currentDistance': 7.375063557495732}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9003456982364157
{'scaleFactor': 20, 'currentTarget': array([22., 22.]), 'previousTarget': array([22., 22.]), 'currentState': array([21.46966978, 21.41212378,  2.48317581]), 'targetState': array([22, 22], dtype=int32), 'currentDistance': 0.7917376995813877}
episode index:403
target Thresh 31.54497715318632
target distance 14.0
model initialize at round 403
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 16.]), 'previousTarget': array([ 7., 16.]), 'currentState': array([20.00518432, 12.35747186,  3.21322715]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 13.50565920241673}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9004242122207489
{'scaleFactor': 20, 'currentTarget': array([ 7., 16.]), 'previousTarget': array([ 7., 16.]), 'currentState': array([ 7.21075866, 15.47387536,  3.80423143]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 0.5667683413974652}
episode index:404
target Thresh 31.54950470616004
target distance 10.0
model initialize at round 404
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 12.]), 'previousTarget': array([ 8., 12.]), 'currentState': array([16.69186169, 13.05884949,  3.47112668]), 'targetState': array([ 8, 12], dtype=int32), 'currentDistance': 8.756119105536056}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9005255849051446
{'scaleFactor': 20, 'currentTarget': array([ 8., 12.]), 'previousTarget': array([ 8., 12.]), 'currentState': array([ 8.082823  , 11.45100425,  5.24868818]), 'targetState': array([ 8, 12], dtype=int32), 'currentDistance': 0.555208057419836}
episode index:405
target Thresh 31.553987209228968
target distance 16.0
model initialize at round 405
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 3.]), 'previousTarget': array([5., 3.]), 'currentState': array([ 9.93424204, 17.60015004,  4.2908903 ]), 'targetState': array([5, 3], dtype=int32), 'currentDistance': 15.411395971421937}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9005575840740571
{'scaleFactor': 20, 'currentTarget': array([5., 3.]), 'previousTarget': array([5., 3.]), 'currentState': array([5.38528155, 3.47312105, 3.7538855 ]), 'targetState': array([5, 3], dtype=int32), 'currentDistance': 0.6101519497611693}
episode index:406
target Thresh 31.558425110647136
target distance 18.0
model initialize at round 406
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 16.]), 'previousTarget': array([ 7., 16.]), 'currentState': array([23.5731912 ,  8.89252717,  3.59260452]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 18.032937551643307}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9005447601678769
{'scaleFactor': 20, 'currentTarget': array([ 7., 16.]), 'previousTarget': array([ 7., 16.]), 'currentState': array([ 7.31920576, 16.24174234,  3.69672415]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 0.40041438529453593}
episode index:407
target Thresh 31.562818854208388
target distance 18.0
model initialize at round 407
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  4.]), 'previousTarget': array([18.,  4.]), 'currentState': array([26.46231802, 21.16692491,  4.75534711]), 'targetState': array([18,  4], dtype=int32), 'currentDistance': 19.1393348176713}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9005100545589265
{'scaleFactor': 20, 'currentTarget': array([18.,  4.]), 'previousTarget': array([18.,  4.]), 'currentState': array([17.11320105,  3.79374396,  5.7903336 ]), 'targetState': array([18,  4], dtype=int32), 'currentDistance': 0.9104690681917706}
episode index:408
target Thresh 31.567168879290744
target distance 12.0
model initialize at round 408
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([11.17837173,  3.53121987,  5.21236134]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 14.361032969524379}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9005418569866154
{'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([24.40809757, 10.5449505 ,  1.49122208]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 0.6808191241404231}
episode index:409
target Thresh 31.571475620900337
target distance 11.0
model initialize at round 409
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 14.]), 'previousTarget': array([26., 14.]), 'currentState': array([16.67060475,  5.79636346,  0.88870447]), 'targetState': array([26, 14], dtype=int32), 'currentDistance': 12.423255131917022}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9005960102486673
{'scaleFactor': 20, 'currentTarget': array([26., 14.]), 'previousTarget': array([26., 14.]), 'currentState': array([26.77385916, 14.41645957,  2.10116405]), 'targetState': array([26, 14], dtype=int32), 'currentDistance': 0.8788040615119883}
episode index:410
target Thresh 31.57573950971491
target distance 12.0
model initialize at round 410
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 13.]), 'previousTarget': array([27., 13.]), 'currentState': array([21.30703911, 23.50340655,  5.62999353]), 'targetState': array([27, 13], dtype=int32), 'currentDistance': 11.947022762412951}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9006725779801961
{'scaleFactor': 20, 'currentTarget': array([27., 13.]), 'previousTarget': array([27., 13.]), 'currentState': array([26.33111928, 13.2203777 ,  5.8990979 ]), 'targetState': array([27, 13], dtype=int32), 'currentDistance': 0.7042497738340694}
episode index:411
target Thresh 31.57996097212691
target distance 17.0
model initialize at round 411
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  4.]), 'previousTarget': array([22.,  4.]), 'currentState': array([6.28266647, 8.9104305 , 4.56901023]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 16.466538826182497}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9006815816137607
{'scaleFactor': 20, 'currentTarget': array([22.,  4.]), 'previousTarget': array([22.,  4.]), 'currentState': array([21.35997706,  3.72059829,  0.59658419]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 0.6983514019619348}
episode index:412
target Thresh 31.584140430286084
target distance 15.0
model initialize at round 412
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 17.]), 'previousTarget': array([21., 17.]), 'currentState': array([ 7.81161007, 14.84396084,  0.3693351 ]), 'targetState': array([21, 17], dtype=int32), 'currentDistance': 13.363462642881123}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9007575713626547
{'scaleFactor': 20, 'currentTarget': array([21., 17.]), 'previousTarget': array([21., 17.]), 'currentState': array([20.44700596, 17.96942689,  5.52109309]), 'targetState': array([21, 17], dtype=int32), 'currentDistance': 1.1160604394848084}
episode index:413
target Thresh 31.588278302141735
target distance 15.0
model initialize at round 413
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 27.]), 'previousTarget': array([ 5., 27.]), 'currentState': array([18.49916216, 21.7614945 ,  3.68205881]), 'targetState': array([ 5, 27], dtype=int32), 'currentDistance': 14.479962671895667}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9007883918363768
{'scaleFactor': 20, 'currentTarget': array([ 5., 27.]), 'previousTarget': array([ 5., 27.]), 'currentState': array([ 4.5558415 , 27.04201735,  3.28567344]), 'targetState': array([ 5, 27], dtype=int32), 'currentDistance': 0.4461414962426325}
episode index:414
target Thresh 31.592375001484502
target distance 7.0
model initialize at round 414
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 13.]), 'previousTarget': array([ 6., 13.]), 'currentState': array([ 9.51245131, 19.21286153,  4.6382885 ]), 'targetState': array([ 6, 13], dtype=int32), 'currentDistance': 7.137013559430275}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9009325065789399
{'scaleFactor': 20, 'currentTarget': array([ 6., 13.]), 'previousTarget': array([ 6., 13.]), 'currentState': array([ 5.2437653 , 13.88541881,  5.10685289]), 'targetState': array([ 6, 13], dtype=int32), 'currentDistance': 1.1644128938958374}
episode index:415
target Thresh 31.596430937987726
target distance 16.0
model initialize at round 415
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 5.96212191, 24.61916347,  4.31094213]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 17.187297082595503}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9009407988107424
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.43654706, 10.70892318,  4.30714544]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.8325535479608391}
episode index:416
target Thresh 31.60044651724844
target distance 9.0
model initialize at round 416
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 26.]), 'previousTarget': array([10., 26.]), 'currentState': array([ 7.5073245 , 17.7485726 ,  1.47094267]), 'targetState': array([10, 26], dtype=int32), 'currentDistance': 8.619714919845082}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9010838568711483
{'scaleFactor': 20, 'currentTarget': array([10., 26.]), 'previousTarget': array([10., 26.]), 'currentState': array([ 9.45928264, 25.3923196 ,  1.07926462]), 'targetState': array([10, 26], dtype=int32), 'currentDistance': 0.8134191613646341}
episode index:417
target Thresh 31.604422140827918
target distance 17.0
model initialize at round 417
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 19.]), 'previousTarget': array([20., 19.]), 'currentState': array([13.46380228,  3.59545536,  2.90471739]), 'targetState': array([20, 19], dtype=int32), 'currentDistance': 16.733854191434734}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9010701114103531
{'scaleFactor': 20, 'currentTarget': array([20., 19.]), 'previousTarget': array([20., 19.]), 'currentState': array([20.00015197, 18.61584868,  1.73254001]), 'targetState': array([20, 19], dtype=int32), 'currentDistance': 0.38415134969679593}
episode index:418
target Thresh 31.608358206291825
target distance 14.0
model initialize at round 418
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([21.98104658, 17.3394484 ,  3.2311269 ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 15.428997690858784}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9010780158580821
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([9.23936175, 8.00485981, 2.36998403]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 1.0235223686006438}
episode index:419
target Thresh 31.612255107249997
target distance 5.0
model initialize at round 419
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  3.]), 'previousTarget': array([20.,  3.]), 'currentState': array([22.50804854,  6.3955453 ,  4.00904906]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 4.221378374442833}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9012661634393724
{'scaleFactor': 20, 'currentTarget': array([20.,  3.]), 'previousTarget': array([20.,  3.]), 'currentState': array([20.25170365,  3.09773302,  4.1531339 ]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 0.2700119818306829}
episode index:420
target Thresh 31.616113233395772
target distance 9.0
model initialize at round 420
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 27.]), 'previousTarget': array([21., 27.]), 'currentState': array([20.55271984, 19.58963247,  0.22619057]), 'targetState': array([21, 27], dtype=int32), 'currentDistance': 7.423853879593258}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.901407089440704
{'scaleFactor': 20, 'currentTarget': array([21., 27.]), 'previousTarget': array([21., 27.]), 'currentState': array([21.27597732, 26.08277743,  1.92764077]), 'targetState': array([21, 27], dtype=int32), 'currentDistance': 0.9578417030374622}
episode index:421
target Thresh 31.619932970544976
target distance 18.0
model initialize at round 421
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  6.]), 'previousTarget': array([13.,  6.]), 'currentState': array([15.84521855, 22.00629947,  4.665224  ]), 'targetState': array([13,  6], dtype=int32), 'currentDistance': 16.257210444175083}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9014357864976779
{'scaleFactor': 20, 'currentTarget': array([13.,  6.]), 'previousTarget': array([13.,  6.]), 'currentState': array([12.49816348,  5.51885472,  3.39395826]), 'targetState': array([13,  6], dtype=int32), 'currentDistance': 0.6952270707832575}
episode index:422
target Thresh 31.623714700674515
target distance 16.0
model initialize at round 422
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([17.75302224, 16.8697635 ,  4.88792181]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 19.577601428838427}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9014002051388562
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.51154667, 3.50060683, 4.29515874]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.698555794232274}
episode index:423
target Thresh 31.627458801960547
target distance 13.0
model initialize at round 423
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 13.]), 'previousTarget': array([11., 13.]), 'currentState': array([22.46521264, 22.30948147,  4.57437301]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 14.768803136152178}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9014287830689147
{'scaleFactor': 20, 'currentTarget': array([11., 13.]), 'previousTarget': array([11., 13.]), 'currentState': array([10.4743291 , 13.02540564,  3.08415735]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 0.5262844717698723}
episode index:424
target Thresh 31.631165648816324
target distance 12.0
model initialize at round 424
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 17.]), 'previousTarget': array([14., 17.]), 'currentState': array([21.59213667, 28.07740462,  2.71213102]), 'targetState': array([14, 17], dtype=int32), 'currentDistance': 13.42942412166884}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9014789381544653
{'scaleFactor': 20, 'currentTarget': array([14., 17.]), 'previousTarget': array([14., 17.]), 'currentState': array([13.67841305, 17.48765194,  3.89134061]), 'targetState': array([14, 17], dtype=int32), 'currentDistance': 0.5841426028994017}
episode index:425
target Thresh 31.63483561192962
target distance 18.0
model initialize at round 425
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 8.33785217, 27.73608206,  4.30905938]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 18.047433718462234}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9014435060736241
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([4.95353177, 9.93385672, 3.2759268 ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.08083458863052445}
episode index:426
target Thresh 31.6384690582998
target distance 19.0
model initialize at round 426
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 4.]), 'previousTarget': array([6., 4.]), 'currentState': array([ 6.29981567, 21.86623773,  0.22049647]), 'targetState': array([6, 4], dtype=int32), 'currentDistance': 17.86875317435098}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.901408239951007
{'scaleFactor': 20, 'currentTarget': array([6., 4.]), 'previousTarget': array([6., 4.]), 'currentState': array([6.34619738, 3.88821543, 5.40315229]), 'targetState': array([6, 4], dtype=int32), 'currentDistance': 0.36379721938874826}
episode index:427
target Thresh 31.64206635127454
target distance 8.0
model initialize at round 427
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 27.]), 'previousTarget': array([ 5., 27.]), 'currentState': array([11.39269678, 19.50103662,  2.43259525]), 'targetState': array([ 5, 27], dtype=int32), 'currentDistance': 9.853985180140972}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9015240853013552
{'scaleFactor': 20, 'currentTarget': array([ 5., 27.]), 'previousTarget': array([ 5., 27.]), 'currentState': array([ 5.62879572, 26.66637596,  0.63206576]), 'targetState': array([ 5, 27], dtype=int32), 'currentDistance': 0.7118209502441782}
episode index:428
target Thresh 31.645627850586123
target distance 6.0
model initialize at round 428
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 19.]), 'previousTarget': array([21., 19.]), 'currentState': array([15.83104035, 15.53652468,  6.23883629]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 6.222041877641805}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9016617820955245
{'scaleFactor': 20, 'currentTarget': array([21., 19.]), 'previousTarget': array([21., 19.]), 'currentState': array([21.66458945, 19.02277561,  0.53088826]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 0.6649796005514181}
episode index:429
target Thresh 31.649153912387458
target distance 16.0
model initialize at round 429
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 22.]), 'previousTarget': array([20., 22.]), 'currentState': array([ 5.5494816 , 17.65688269,  5.6741553 ]), 'targetState': array([20, 22], dtype=int32), 'currentDistance': 15.08907386507058}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9016893529452643
{'scaleFactor': 20, 'currentTarget': array([20., 22.]), 'previousTarget': array([20., 22.]), 'currentState': array([19.74753596, 21.79419754,  5.88025569]), 'targetState': array([20, 22], dtype=int32), 'currentDistance': 0.3257188118932094}
episode index:430
target Thresh 31.652644889287657
target distance 14.0
model initialize at round 430
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([ 9.95272924, 25.31755029,  3.25223374]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 15.617720896634108}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9017167958560263
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([12.07569501,  9.54617715,  5.36441869]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 1.0297062189744357}
episode index:431
target Thresh 31.65610113038732
target distance 11.0
model initialize at round 431
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([10.24291065, 18.49693519,  5.25578809]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 10.213104251008167}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9018308543144613
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.45298509,  9.84325711,  3.5639641 ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.0051407193049553}
episode index:432
target Thresh 31.65952298131344
target distance 7.0
model initialize at round 432
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 12.]), 'previousTarget': array([11., 12.]), 'currentState': array([17.14482947, 17.55048493,  5.18936586]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 8.280507970051564}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9019443859439891
{'scaleFactor': 20, 'currentTarget': array([11., 12.]), 'previousTarget': array([11., 12.]), 'currentState': array([10.76577147, 12.2170536 ,  3.6372492 ]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 0.31933566963970095}
episode index:433
target Thresh 31.66291078425396
target distance 16.0
model initialize at round 433
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([19.98209858, 19.65975197,  3.05284357]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 17.826219060598838}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9019291644424101
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([5.99279765, 9.72849261, 2.66122085]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 1.029253823817572}
episode index:434
target Thresh 31.666264877991996
target distance 14.0
model initialize at round 434
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([14.47658067, 20.1924744 ,  4.77273378]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 16.796056523725355}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9019348033172754
{'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([26.50056612,  9.34868298,  5.05767586]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 0.6091091983940223}
episode index:435
target Thresh 31.669585597939715
target distance 8.0
model initialize at round 435
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 13.]), 'previousTarget': array([10., 13.]), 'currentState': array([ 0.55218667, 18.14196466,  4.68658447]), 'targetState': array([10, 13], dtype=int32), 'currentDistance': 10.756438876644706}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9019825324253273
{'scaleFactor': 20, 'currentTarget': array([10., 13.]), 'previousTarget': array([10., 13.]), 'currentState': array([10.5793631 , 13.45786682,  2.52938872]), 'targetState': array([10, 13], dtype=int32), 'currentDistance': 0.7384467648128485}
episode index:436
target Thresh 31.672873276171885
target distance 16.0
model initialize at round 436
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 21.]), 'previousTarget': array([23., 21.]), 'currentState': array([8.59653265, 6.57370176, 2.21846786]), 'targetState': array([23, 21], dtype=int32), 'currentDistance': 20.38572913740711}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9019265564312167
{'scaleFactor': 20, 'currentTarget': array([23., 21.]), 'previousTarget': array([23., 21.]), 'currentState': array([22.59162076, 21.15233134,  5.6648914 ]), 'targetState': array([23, 21], dtype=int32), 'currentDistance': 0.435865161809158}
episode index:437
target Thresh 31.676128241459065
target distance 2.0
model initialize at round 437
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([ 8.96976275, 14.66920553,  5.06360161]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 1.2285051244969667}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9021276373526066
{'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([10.55936314, 13.73848241,  0.1741305 ]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 0.6174775910218874}
episode index:438
target Thresh 31.679350819300495
target distance 9.0
model initialize at round 438
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 23.]), 'previousTarget': array([24., 23.]), 'currentState': array([16.65767474, 19.70930677,  0.83640307]), 'targetState': array([24, 23], dtype=int32), 'currentDistance': 8.046017787416167}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9022608227117124
{'scaleFactor': 20, 'currentTarget': array([24., 23.]), 'previousTarget': array([24., 23.]), 'currentState': array([23.68962227, 22.61961522,  0.28258873]), 'targetState': array([24, 23], dtype=int32), 'currentDistance': 0.49094492507266296}
episode index:439
target Thresh 31.68254133195665
target distance 15.0
model initialize at round 439
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 20.]), 'previousTarget': array([18., 20.]), 'currentState': array([4.3753085 , 5.97000753, 5.88746065]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 19.556914560918603}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9022247410049042
{'scaleFactor': 20, 'currentTarget': array([18., 20.]), 'previousTarget': array([18., 20.]), 'currentState': array([18.74491302, 19.6631473 ,  0.34989679]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 0.8175360214603657}
episode index:440
target Thresh 31.685700098481448
target distance 22.0
model initialize at round 440
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  2.]), 'previousTarget': array([26.,  2.]), 'currentState': array([5.49742959, 8.7689024 , 1.48381739]), 'targetState': array([26,  2], dtype=int32), 'currentDistance': 21.5910498362471}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.9021291256163246
{'scaleFactor': 20, 'currentTarget': array([26.,  2.]), 'previousTarget': array([26.,  2.]), 'currentState': array([25.18465886,  2.33968284,  3.64353681]), 'targetState': array([26,  2], dtype=int32), 'currentDistance': 0.8832698423107254}
episode index:441
target Thresh 31.688827434754177
target distance 12.0
model initialize at round 441
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 16.]), 'previousTarget': array([15., 16.]), 'currentState': array([22.3251639 , 26.34874093,  3.89682007]), 'targetState': array([15, 16], dtype=int32), 'currentDistance': 12.678898412984063}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9021757671747219
{'scaleFactor': 20, 'currentTarget': array([15., 16.]), 'previousTarget': array([15., 16.]), 'currentState': array([14.93599731, 15.76711045,  4.72741603]), 'targetState': array([15, 16], dtype=int32), 'currentDistance': 0.24152408743543707}
episode index:442
target Thresh 31.691923653511072
target distance 6.0
model initialize at round 442
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  7.]), 'previousTarget': array([18.,  7.]), 'currentState': array([23.60869225, 10.63684646,  2.81545419]), 'targetState': array([18,  7], dtype=int32), 'currentDistance': 6.684615237976065}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.902307641312025
{'scaleFactor': 20, 'currentTarget': array([18.,  7.]), 'previousTarget': array([18.,  7.]), 'currentState': array([18.32350351,  6.46049868,  2.80248159]), 'targetState': array([18,  7], dtype=int32), 'currentDistance': 0.6290597730846272}
episode index:443
target Thresh 31.694989064376585
target distance 7.0
model initialize at round 443
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 19.]), 'previousTarget': array([20., 19.]), 'currentState': array([25.33063696, 15.78642305,  2.25884056]), 'targetState': array([20, 19], dtype=int32), 'currentDistance': 6.224368816313402}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9024389214216827
{'scaleFactor': 20, 'currentTarget': array([20., 19.]), 'previousTarget': array([20., 19.]), 'currentState': array([19.48248073, 19.09839626,  3.21722   ]), 'targetState': array([20, 19], dtype=int32), 'currentDistance': 0.5267902998430692}
episode index:444
target Thresh 31.69802397389436
target distance 10.0
model initialize at round 444
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 20.]), 'previousTarget': array([24., 20.]), 'currentState': array([15.54646184, 21.33603927,  0.60445755]), 'targetState': array([24, 20], dtype=int32), 'currentDistance': 8.558464132021019}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9025480250811845
{'scaleFactor': 20, 'currentTarget': array([24., 20.]), 'previousTarget': array([24., 20.]), 'currentState': array([23.51801114, 20.01731683,  0.48087735]), 'targetState': array([24, 20], dtype=int32), 'currentDistance': 0.48229983901520174}
episode index:445
target Thresh 31.701028685557876
target distance 5.0
model initialize at round 445
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([6.00066276, 8.99348923, 3.18952803]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 3.1649709225883313}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9027219084330204
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 2.40323045, 10.57065816,  2.77221765]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.8257025056030023}
episode index:446
target Thresh 31.704003499840805
target distance 22.0
model initialize at round 446
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 24.]), 'previousTarget': array([ 2., 24.]), 'currentState': array([10.50908047,  2.78075429,  3.66917145]), 'targetState': array([ 2, 24], dtype=int32), 'currentDistance': 22.86177681517839}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.9026072235620178
{'scaleFactor': 20, 'currentTarget': array([ 2., 24.]), 'previousTarget': array([ 2., 24.]), 'currentState': array([ 2.60358146, 24.36189594,  0.51917636]), 'targetState': array([ 2, 24], dtype=int32), 'currentDistance': 0.7037607944174965}
episode index:447
target Thresh 31.706948714227053
target distance 8.0
model initialize at round 447
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 17.]), 'previousTarget': array([22., 17.]), 'currentState': array([17.52194964, 24.28162866,  4.83218158]), 'targetState': array([22, 17], dtype=int32), 'currentDistance': 8.548394644584201}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9027152209422366
{'scaleFactor': 20, 'currentTarget': array([22., 17.]), 'previousTarget': array([22., 17.]), 'currentState': array([21.76214451, 16.94749771,  4.30703266]), 'targetState': array([22, 17], dtype=int32), 'currentDistance': 0.24358104061782804}
episode index:448
target Thresh 31.709864623240517
target distance 23.0
model initialize at round 448
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 24.]), 'previousTarget': array([ 2., 24.]), 'currentState': array([23.51393318, 15.16008838,  4.6250627 ]), 'targetState': array([ 2, 24], dtype=int32), 'currentDistance': 23.25926392304609}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.9026010618111734
{'scaleFactor': 20, 'currentTarget': array([ 2., 24.]), 'previousTarget': array([ 2., 24.]), 'currentState': array([ 1.78281755, 24.02997344,  2.58166402]), 'targetState': array([ 2, 24], dtype=int32), 'currentDistance': 0.2192410188926067}
episode index:449
target Thresh 31.71275151847452
target distance 22.0
model initialize at round 449
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 24.]), 'previousTarget': array([18., 24.]), 'currentState': array([15.34331493,  3.54956583,  2.98163795]), 'targetState': array([18, 24], dtype=int32), 'currentDistance': 20.622275176857727}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9025258279244129
{'scaleFactor': 20, 'currentTarget': array([18., 24.]), 'previousTarget': array([18., 24.]), 'currentState': array([18.01991486, 24.26061035,  1.57554159]), 'targetState': array([18, 24], dtype=int32), 'currentDistance': 0.26137014901664446}
episode index:450
target Thresh 31.715609688620997
target distance 15.0
model initialize at round 450
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([23.12609593, 20.43829057,  3.12678468]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 14.137259410408035}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9025913257514253
{'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([ 9.91456049, 20.75687923,  1.62275241]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 0.9463237311743723}
episode index:451
target Thresh 31.718439419499344
target distance 8.0
model initialize at round 451
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 20.]), 'previousTarget': array([20., 20.]), 'currentState': array([25.11390689, 13.67911087,  2.51306248]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 8.130540150650988}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9027196546988779
{'scaleFactor': 20, 'currentTarget': array([20., 20.]), 'previousTarget': array([20., 20.]), 'currentState': array([20.50272939, 19.42362314,  0.87437493]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 0.7648183611047719}
episode index:452
target Thresh 31.721240994085004
target distance 8.0
model initialize at round 452
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 15.]), 'previousTarget': array([ 5., 15.]), 'currentState': array([ 2.71016021, 21.34217583,  5.54930687]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 6.74288962523835}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9028474170726111
{'scaleFactor': 20, 'currentTarget': array([ 5., 15.]), 'previousTarget': array([ 5., 15.]), 'currentState': array([ 4.66387025, 14.12461489,  4.27060011]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 0.9377005418929883}
episode index:453
target Thresh 31.72401469253778
target distance 8.0
model initialize at round 453
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([20.39429177, 16.94251726,  1.60442441]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 7.278982203037738}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9029746166165041
{'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([26.47269788, 19.90891433,  0.49181419]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 0.535111315866125}
episode index:454
target Thresh 31.726760792229815
target distance 1.0
model initialize at round 454
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 17.]), 'previousTarget': array([14., 17.]), 'currentState': array([14.14947263, 16.75332394,  4.46881859]), 'targetState': array([14, 17], dtype=int32), 'currentDistance': 0.28842874955269154}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9031878592173469
{'scaleFactor': 20, 'currentTarget': array([14., 17.]), 'previousTarget': array([14., 17.]), 'currentState': array([14.14947263, 16.75332394,  4.46881859]), 'targetState': array([14, 17], dtype=int32), 'currentDistance': 0.28842874955269154}
episode index:455
target Thresh 31.72947956777337
target distance 12.0
model initialize at round 455
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([16.9496305 ,  2.44462225,  0.25517248]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 10.06570273133671}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.903292688582879
{'scaleFactor': 20, 'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([26.11621459,  2.86321994,  4.73938604]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 0.8943072399067652}
episode index:456
target Thresh 31.732171291048274
target distance 12.0
model initialize at round 456
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 25.]), 'previousTarget': array([ 5., 25.]), 'currentState': array([13.90757215, 14.28023024,  1.26720643]), 'targetState': array([ 5, 25], dtype=int32), 'currentDistance': 13.93765781010368}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9033352531470914
{'scaleFactor': 20, 'currentTarget': array([ 5., 25.]), 'previousTarget': array([ 5., 25.]), 'currentState': array([ 5.26594087, 24.60632545,  1.27361286]), 'targetState': array([ 5, 25], dtype=int32), 'currentDistance': 0.47508336393516837}
episode index:457
target Thresh 31.73483623122909
target distance 13.0
model initialize at round 457
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([25.12259052, 20.67849923,  2.50788999]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 17.830456479901724}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9033375387843441
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.30023982,  6.45227761,  4.51737095]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.8331982627637016}
episode index:458
target Thresh 31.737474654812058
target distance 16.0
model initialize at round 458
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 18.]), 'previousTarget': array([ 9., 18.]), 'currentState': array([23.32349576, 20.85261749,  4.23927784]), 'targetState': array([ 9, 18], dtype=int32), 'currentDistance': 14.604792269118047}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9033597167989394
{'scaleFactor': 20, 'currentTarget': array([ 9., 18.]), 'previousTarget': array([ 9., 18.]), 'currentState': array([ 8.70422115, 17.86166697,  3.80852228]), 'targetState': array([ 9, 18], dtype=int32), 'currentDistance': 0.32652894844691494}
episode index:459
target Thresh 31.740086825641733
target distance 11.0
model initialize at round 459
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 20.]), 'previousTarget': array([19., 20.]), 'currentState': array([15.70544718,  7.4720169 ,  6.15491772]), 'targetState': array([19, 20], dtype=int32), 'currentDistance': 12.953935266476563}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9034018580546547
{'scaleFactor': 20, 'currentTarget': array([19., 20.]), 'previousTarget': array([19., 20.]), 'currentState': array([19.5159009 , 20.49250348,  1.17837748]), 'targetState': array([19, 20], dtype=int32), 'currentDistance': 0.7132414865596198}
episode index:460
target Thresh 31.74267300493738
target distance 7.0
model initialize at round 460
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 25.]), 'previousTarget': array([26., 25.]), 'currentState': array([20.32459174, 22.03819503,  1.67477315]), 'targetState': array([26, 25], dtype=int32), 'currentDistance': 6.401761290548407}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9035259234601761
{'scaleFactor': 20, 'currentTarget': array([26., 25.]), 'previousTarget': array([26., 25.]), 'currentState': array([26.20622385, 24.69337592,  6.11528137]), 'targetState': array([26, 25], dtype=int32), 'currentDistance': 0.369522126828898}
episode index:461
target Thresh 31.74523345131908
target distance 17.0
model initialize at round 461
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 27.]), 'previousTarget': array([ 7., 27.]), 'currentState': array([ 3.36887107, 11.56014883,  0.94520283]), 'targetState': array([ 7, 27], dtype=int32), 'currentDistance': 15.861087655551376}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9035277766020563
{'scaleFactor': 20, 'currentTarget': array([ 7., 27.]), 'previousTarget': array([ 7., 27.]), 'currentState': array([ 7.09835345, 27.04694048,  1.26056366]), 'targetState': array([ 7, 27], dtype=int32), 'currentDistance': 0.10898077727042303}
episode index:462
target Thresh 31.747768420833605
target distance 5.0
model initialize at round 462
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([23.71484271, 19.91337273,  4.85348058]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 4.272607126223074}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.903693159373974
{'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([21.86190572, 16.98947038,  5.25062287]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 0.9990603901452929}
episode index:463
target Thresh 31.75027816698002
target distance 11.0
model initialize at round 463
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 14.]), 'previousTarget': array([11., 14.]), 'currentState': array([5.12474427, 4.67845237, 2.50642467]), 'targetState': array([11, 14], dtype=int32), 'currentDistance': 11.018615160109839}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9037745968524805
{'scaleFactor': 20, 'currentTarget': array([11., 14.]), 'previousTarget': array([11., 14.]), 'currentState': array([10.11642533, 13.55514822,  0.79809228]), 'targetState': array([11, 14], dtype=int32), 'currentDistance': 0.989240772072521}
episode index:464
target Thresh 31.752762940735032
target distance 18.0
model initialize at round 464
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 29.]), 'previousTarget': array([ 5., 29.]), 'currentState': array([19.17219312, 12.46530674,  1.07503557]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 21.77721604916138}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9037181375538709
{'scaleFactor': 20, 'currentTarget': array([ 5., 29.]), 'previousTarget': array([ 5., 29.]), 'currentState': array([ 5.59482755, 29.23025305,  3.502321  ]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 0.6378371841648817}
episode index:465
target Thresh 31.755222990578087
target distance 23.0
model initialize at round 465
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  2.]), 'previousTarget': array([25.,  2.]), 'currentState': array([3.55948056, 6.63277838, 5.65864927]), 'targetState': array([25,  2], dtype=int32), 'currentDistance': 21.93532560543513}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.9036244470325993
{'scaleFactor': 20, 'currentTarget': array([25.,  2.]), 'previousTarget': array([25.,  2.]), 'currentState': array([25.28544087,  1.1811684 ,  0.56843871]), 'targetState': array([25,  2], dtype=int32), 'currentDistance': 0.8671572348860433}
episode index:466
target Thresh 31.757658562516216
target distance 20.0
model initialize at round 466
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([5.69081114, 6.46534432, 6.14535999]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 21.995993832175436}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9035497604495936
{'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([24.76452559, 16.39844488,  5.98323107]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 0.6460005852686252}
episode index:467
target Thresh 31.760069900108643
target distance 7.0
model initialize at round 467
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 27.]), 'previousTarget': array([ 7., 27.]), 'currentState': array([ 8.71171808, 21.1947111 ,  3.27277327]), 'targetState': array([ 7, 27], dtype=int32), 'currentDistance': 6.052384489522783}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9036923870298296
{'scaleFactor': 20, 'currentTarget': array([ 7., 27.]), 'previousTarget': array([ 7., 27.]), 'currentState': array([ 7.65165869, 26.40795141,  1.87745813]), 'targetState': array([ 7, 27], dtype=int32), 'currentDistance': 0.8804433986482199}
episode index:468
target Thresh 31.76245724449114
target distance 17.0
model initialize at round 468
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 18.]), 'previousTarget': array([26., 18.]), 'currentState': array([ 8.70210716, 21.34360393,  5.54444718]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 17.61808116343783}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.903655484012103
{'scaleFactor': 20, 'currentTarget': array([26., 18.]), 'previousTarget': array([26., 18.]), 'currentState': array([26.80902315, 17.80053399,  6.25130436]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 0.8332497499011792}
episode index:469
target Thresh 31.764820834400137
target distance 23.0
model initialize at round 469
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 28.]), 'previousTarget': array([13., 28.]), 'currentState': array([14.55298015,  6.62251698,  2.84963673]), 'targetState': array([13, 28], dtype=int32), 'currentDistance': 21.433817387328943}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9035812081158411
{'scaleFactor': 20, 'currentTarget': array([13., 28.]), 'previousTarget': array([13., 28.]), 'currentState': array([13.70194914, 27.30352694,  5.42546773]), 'targetState': array([13, 28], dtype=int32), 'currentDistance': 0.9888413997492134}
episode index:470
target Thresh 31.767160906196587
target distance 8.0
model initialize at round 470
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 16.]), 'previousTarget': array([10., 16.]), 'currentState': array([16.02274611, 13.29950004,  2.96703991]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 6.600467461262718}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.903702258650627
{'scaleFactor': 20, 'currentTarget': array([10., 16.]), 'previousTarget': array([10., 16.]), 'currentState': array([ 9.97879092, 16.86815522,  3.35356372]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 0.8684142483061446}
episode index:471
target Thresh 31.769477693889623
target distance 13.0
model initialize at round 471
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 2.]), 'previousTarget': array([6., 2.]), 'currentState': array([10.36944838, 15.41676072,  3.90135515]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 14.110334775366447}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.903742602794223
{'scaleFactor': 20, 'currentTarget': array([6., 2.]), 'previousTarget': array([6., 2.]), 'currentState': array([6.68379395, 2.92949219, 2.97182119]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 1.1539193673728698}
episode index:472
target Thresh 31.771771429159948
target distance 9.0
model initialize at round 472
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([ 4.35109285, 21.66311233,  4.35312867]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 12.967302120559912}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.903782776349474
{'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([14.4364523 , 13.7442387 ,  5.18614709]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 0.8627756672439483}
episode index:473
target Thresh 31.774042341382994
target distance 18.0
model initialize at round 473
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([ 6.58222114, 24.57355413,  5.62095172]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 24.04939224338502}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9037460719093192
{'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([22.64848452,  7.581368  ,  4.97571902]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 0.6793760960112808}
episode index:474
target Thresh 31.776290657651884
target distance 9.0
model initialize at round 474
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([13.03760009,  8.36348262,  3.07214448]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 7.22537225344642}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9038657559895102
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 5.88377244, 10.0074579 ,  3.8735763 ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.11646658376424299}
episode index:475
target Thresh 31.77851660280011
target distance 12.0
model initialize at round 475
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 12.]), 'previousTarget': array([15., 12.]), 'currentState': array([ 4.19458585, 19.81452426,  0.22842949]), 'targetState': array([15, 12], dtype=int32), 'currentDistance': 13.335057719304368}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9039054176248851
{'scaleFactor': 20, 'currentTarget': array([15., 12.]), 'previousTarget': array([15., 12.]), 'currentState': array([14.79735601, 11.77953762,  0.87092441]), 'targetState': array([15, 12], dtype=int32), 'currentDistance': 0.2994465709281772}
episode index:476
target Thresh 31.780720399424048
target distance 10.0
model initialize at round 476
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 25.]), 'previousTarget': array([ 3., 25.]), 'currentState': array([11.31748609, 14.96082038,  2.1548748 ]), 'targetState': array([ 3, 25], dtype=int32), 'currentDistance': 13.03708948807169}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9039644531181389
{'scaleFactor': 20, 'currentTarget': array([ 3., 25.]), 'previousTarget': array([ 3., 25.]), 'currentState': array([ 2.94995184, 25.04245641,  3.07231061]), 'targetState': array([ 3, 25], dtype=int32), 'currentDistance': 0.06563052561653344}
episode index:477
target Thresh 31.782902267905197
target distance 22.0
model initialize at round 477
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([17.72639189, 27.48186203,  6.16866922]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 21.735041913811546}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.903909132134626
{'scaleFactor': 20, 'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([24.6678668 ,  7.10259761,  4.68305953]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 0.34761866143914893}
episode index:478
target Thresh 31.78506242643222
target distance 12.0
model initialize at round 478
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 2.]), 'previousTarget': array([5., 2.]), 'currentState': array([15.50735438,  8.77742983,  3.67141223]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 12.503521551062278}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9039875684963512
{'scaleFactor': 20, 'currentTarget': array([5., 2.]), 'previousTarget': array([5., 2.]), 'currentState': array([5.80350736, 2.59597991, 2.44991793]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 1.0004079802125498}
episode index:479
target Thresh 31.78720109102277
target distance 15.0
model initialize at round 479
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  7.]), 'previousTarget': array([20.,  7.]), 'currentState': array([6.60262121, 6.51380281, 5.58343381]), 'targetState': array([20,  7], dtype=int32), 'currentDistance': 13.406198045937995}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.904026645842042
{'scaleFactor': 20, 'currentTarget': array([20.,  7.]), 'previousTarget': array([20.,  7.]), 'currentState': array([19.61470794,  6.83015844,  5.90738158]), 'targetState': array([20,  7], dtype=int32), 'currentDistance': 0.42106546396417377}
episode index:480
target Thresh 31.789318475545087
target distance 11.0
model initialize at round 480
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 20.]), 'previousTarget': array([25., 20.]), 'currentState': array([22.22148705, 10.95334847,  1.1338352 ]), 'targetState': array([25, 20], dtype=int32), 'currentDistance': 9.46372220866234}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9041242828567155
{'scaleFactor': 20, 'currentTarget': array([25., 20.]), 'previousTarget': array([25., 20.]), 'currentState': array([25.60708328, 19.6284027 ,  5.97168247]), 'targetState': array([25, 20], dtype=int32), 'currentDistance': 0.7117827291005185}
episode index:481
target Thresh 31.791414791739392
target distance 10.0
model initialize at round 481
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 10.]), 'previousTarget': array([20., 10.]), 'currentState': array([11.57868221, 17.41677573,  4.91930032]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 11.221726936713916}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9041822518713426
{'scaleFactor': 20, 'currentTarget': array([20., 10.]), 'previousTarget': array([20., 10.]), 'currentState': array([19.57398633,  9.34519867,  4.22686148]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 0.7811865506836857}
episode index:482
target Thresh 31.79349024923905
target distance 22.0
model initialize at round 482
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 24.]), 'previousTarget': array([ 8., 24.]), 'currentState': array([17.42644468,  2.59691853,  1.76901913]), 'targetState': array([ 8, 24], dtype=int32), 'currentDistance': 23.38695695464969}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.9040908980468497
{'scaleFactor': 20, 'currentTarget': array([ 8., 24.]), 'previousTarget': array([ 8., 24.]), 'currentState': array([ 7.92595243, 23.81613767,  1.025129  ]), 'targetState': array([ 8, 24], dtype=int32), 'currentDistance': 0.19821301080224013}
episode index:483
target Thresh 31.79554505559154
target distance 16.0
model initialize at round 483
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 17.]), 'previousTarget': array([ 3., 17.]), 'currentState': array([17.15309406, 19.30436417,  3.78680062]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 14.339461835644098}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.904148696496974
{'scaleFactor': 20, 'currentTarget': array([ 3., 17.]), 'previousTarget': array([ 3., 17.]), 'currentState': array([ 3.95272081, 16.37144125,  2.48907056]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 1.1413864628950336}
episode index:484
target Thresh 31.79757941627921
target distance 23.0
model initialize at round 484
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([ 9.32494019, 27.1629807 ,  4.05459952]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 24.40143885587574}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9040757008604214
{'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([17.23156421,  4.96850776,  4.05534971]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 0.9958058418330343}
episode index:485
target Thresh 31.79959353473982
target distance 9.0
model initialize at round 485
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  5.]), 'previousTarget': array([10.,  5.]), 'currentState': array([18.06557258, 11.60027379,  3.11375523]), 'targetState': array([10,  5], dtype=int32), 'currentDistance': 10.42195159408061}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9041526647463074
{'scaleFactor': 20, 'currentTarget': array([10.,  5.]), 'previousTarget': array([10.,  5.]), 'currentState': array([10.35974653,  5.28344718,  3.53052787]), 'targetState': array([10,  5], dtype=int32), 'currentDistance': 0.4579954952616261}
episode index:486
target Thresh 31.8015876123869
target distance 12.0
model initialize at round 486
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([ 4.25931585, 11.11647288,  1.73534602]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 13.32337404715859}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.904190841398631
{'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([14.96900834, 18.82780525,  0.06873786]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 0.1749614635829719}
episode index:487
target Thresh 31.803561848629876
target distance 12.0
model initialize at round 487
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([2.68697959, 5.46362533, 6.1428647 ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 12.188647426150075}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.904247961289017
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.28675842, 10.45453486,  0.13713908]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.8457632558218794}
episode index:488
target Thresh 31.805516440894014
target distance 8.0
model initialize at round 488
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 12.]), 'previousTarget': array([23., 12.]), 'currentState': array([16.46765284, 10.82364031,  5.78458339]), 'targetState': array([23, 12], dtype=int32), 'currentDistance': 6.63742281283775}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9043631924724751
{'scaleFactor': 20, 'currentTarget': array([23., 12.]), 'previousTarget': array([23., 12.]), 'currentState': array([23.31302247, 12.07773495,  0.23600367]), 'targetState': array([23, 12], dtype=int32), 'currentDistance': 0.3225302913509159}
episode index:489
target Thresh 31.807451584640173
target distance 18.0
model initialize at round 489
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 27.]), 'previousTarget': array([12., 27.]), 'currentState': array([18.06780207, 10.40121202,  1.14783812]), 'targetState': array([12, 27], dtype=int32), 'currentDistance': 17.673086447080674}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9043447742312225
{'scaleFactor': 20, 'currentTarget': array([12., 27.]), 'previousTarget': array([12., 27.]), 'currentState': array([11.94472967, 27.56970682,  1.51624069]), 'targetState': array([12, 27], dtype=int32), 'currentDistance': 0.5723815817597533}
episode index:490
target Thresh 31.809367473384338
target distance 4.0
model initialize at round 490
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([18.39975754, 12.63480335,  2.34097302]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 4.41488788321764}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9044791005566172
{'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([13.53435265, 13.21350092,  2.6093064 ]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 0.512259797916614}
episode index:491
target Thresh 31.811264298716978
target distance 16.0
model initialize at round 491
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 23.]), 'previousTarget': array([19., 23.]), 'currentState': array([ 4.29965068, 21.06927217,  1.6984294 ]), 'targetState': array([19, 23], dtype=int32), 'currentDistance': 14.826597054357512}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9044974707739486
{'scaleFactor': 20, 'currentTarget': array([19., 23.]), 'previousTarget': array([19., 23.]), 'currentState': array([18.32672285, 23.75832653,  5.15409684]), 'targetState': array([19, 23], dtype=int32), 'currentDistance': 1.0140814798923874}
episode index:492
target Thresh 31.813142250322212
target distance 19.0
model initialize at round 492
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 4.]), 'previousTarget': array([5., 4.]), 'currentState': array([22.43175692, 21.61073868,  3.78022158]), 'targetState': array([5, 4], dtype=int32), 'currentDistance': 24.77910947101619}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.9043898851762222
{'scaleFactor': 20, 'currentTarget': array([5., 4.]), 'previousTarget': array([5., 4.]), 'currentState': array([4.30167869, 3.49878592, 3.36165142]), 'targetState': array([5, 4], dtype=int32), 'currentDistance': 0.8595744311228561}
episode index:493
target Thresh 31.815001515996762
target distance 10.0
model initialize at round 493
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([22.37132161, 14.42402215,  1.87689924]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 11.290459065712712}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9044459083801307
{'scaleFactor': 20, 'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([14.90810899, 22.12171952,  2.71450818]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 0.91623008604354}
episode index:494
target Thresh 31.816842281668748
target distance 12.0
model initialize at round 494
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 6.]), 'previousTarget': array([5., 6.]), 'currentState': array([15.11803513,  8.34645191,  3.63029137]), 'targetState': array([5, 6], dtype=int32), 'currentDistance': 10.386552433109527}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9045399369488577
{'scaleFactor': 20, 'currentTarget': array([5., 6.]), 'previousTarget': array([5., 6.]), 'currentState': array([5.84709431, 6.08191594, 3.07470677]), 'targetState': array([5, 6], dtype=int32), 'currentDistance': 0.8510458184953291}
episode index:495
target Thresh 31.818664731416266
target distance 5.0
model initialize at round 495
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  4.]), 'previousTarget': array([19.,  4.]), 'currentState': array([22.81506485,  3.19512208,  3.3619144 ]), 'targetState': array([19,  4], dtype=int32), 'currentDistance': 3.8990445366974673}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9046922757856543
{'scaleFactor': 20, 'currentTarget': array([19.,  4.]), 'previousTarget': array([19.,  4.]), 'currentState': array([18.95744081,  3.99174318,  2.89862618]), 'targetState': array([19,  4], dtype=int32), 'currentDistance': 0.043352736524446}
episode index:496
target Thresh 31.820469047485812
target distance 7.0
model initialize at round 496
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([3.06188027, 8.31816799, 5.75916576]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 8.967760052793201}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9047854302607334
{'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([ 9.66294368, 13.41339184,  2.0733028 ]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 0.6765471818058684}
episode index:497
target Thresh 31.8222554103105
target distance 12.0
model initialize at round 497
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 13.]), 'previousTarget': array([ 9., 13.]), 'currentState': array([19.42091122,  7.58212259,  1.77840543]), 'targetState': array([ 9, 13], dtype=int32), 'currentDistance': 11.74516012176319}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9048402092118303
{'scaleFactor': 20, 'currentTarget': array([ 9., 13.]), 'previousTarget': array([ 9., 13.]), 'currentState': array([ 9.24012374, 13.01829138,  2.13361454]), 'targetState': array([ 9, 13], dtype=int32), 'currentDistance': 0.2408194039640451}
episode index:498
target Thresh 31.824023998528094
target distance 12.0
model initialize at round 498
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 28.]), 'previousTarget': array([11., 28.]), 'currentState': array([ 7.66620094, 16.23698633,  1.15128392]), 'targetState': array([11, 28], dtype=int32), 'currentDistance': 12.226312063773946}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.904894768608013
{'scaleFactor': 20, 'currentTarget': array([11., 28.]), 'previousTarget': array([11., 28.]), 'currentState': array([11.42841543, 27.4604993 ,  2.19389814]), 'targetState': array([11, 28], dtype=int32), 'currentDistance': 0.6889127578995363}
episode index:499
target Thresh 31.825774988998898
target distance 14.0
model initialize at round 499
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 10.]), 'previousTarget': array([25., 10.]), 'currentState': array([20.56836926, 23.11522799,  4.70515466]), 'targetState': array([25, 10], dtype=int32), 'currentDistance': 13.84371901568894}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9049120135657642
{'scaleFactor': 20, 'currentTarget': array([25., 10.]), 'previousTarget': array([25., 10.]), 'currentState': array([24.58268939,  9.37778547,  4.69465071]), 'targetState': array([25, 10], dtype=int32), 'currentDistance': 0.7491989482345754}
episode index:500
target Thresh 31.82750855682341
target distance 17.0
model initialize at round 500
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  6.]), 'previousTarget': array([26.,  6.]), 'currentState': array([10.68286989, 17.0183577 ,  5.28409344]), 'targetState': array([26,  6], dtype=int32), 'currentDistance': 18.86845730704565}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9048929042657502
{'scaleFactor': 20, 'currentTarget': array([26.,  6.]), 'previousTarget': array([26.,  6.]), 'currentState': array([25.81521047,  6.35907831,  4.35992316]), 'targetState': array([26,  6], dtype=int32), 'currentDistance': 0.40383709942277957}
episode index:501
target Thresh 31.829224875359866
target distance 12.0
model initialize at round 501
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 29.]), 'previousTarget': array([25., 29.]), 'currentState': array([13.26634252, 28.66176104,  0.40187073]), 'targetState': array([25, 29], dtype=int32), 'currentDistance': 11.73853156946018}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.904965787224187
{'scaleFactor': 20, 'currentTarget': array([25., 29.]), 'previousTarget': array([25., 29.]), 'currentState': array([24.6526568 , 28.96166627,  5.17610244]), 'targetState': array([25, 29], dtype=int32), 'currentDistance': 0.34945210756038114}
episode index:502
target Thresh 31.83092411624154
target distance 11.0
model initialize at round 502
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 25.]), 'previousTarget': array([ 9., 25.]), 'currentState': array([ 2.17172588, 12.32581414,  5.82460427]), 'targetState': array([ 9, 25], dtype=int32), 'currentDistance': 14.39653829305195}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9050011329641546
{'scaleFactor': 20, 'currentTarget': array([ 9., 25.]), 'previousTarget': array([ 9., 25.]), 'currentState': array([ 9.56894418, 25.22655895,  0.76301358]), 'targetState': array([ 9, 25], dtype=int32), 'currentDistance': 0.6123940230030288}
episode index:503
target Thresh 31.832606449393943
target distance 4.0
model initialize at round 503
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 15.]), 'previousTarget': array([10., 15.]), 'currentState': array([ 6.46211193, 15.38171676,  6.00054312]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 3.5584209520560424}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9051501386527178
{'scaleFactor': 20, 'currentTarget': array([10., 15.]), 'previousTarget': array([10., 15.]), 'currentState': array([10.41792846, 14.92946792,  0.04982738]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 0.42383837847953293}
episode index:504
target Thresh 31.83427204305179
target distance 5.0
model initialize at round 504
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 13.]), 'previousTarget': array([ 7., 13.]), 'currentState': array([7.79983486, 9.98593236, 1.78200564]), 'targetState': array([ 7, 13], dtype=int32), 'currentDistance': 3.1183873344454818}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9052985542197421
{'scaleFactor': 20, 'currentTarget': array([ 7., 13.]), 'previousTarget': array([ 7., 13.]), 'currentState': array([ 7.11884096, 13.88146762,  1.37170695]), 'targetState': array([ 7, 13], dtype=int32), 'currentDistance': 0.8894427102895712}
episode index:505
target Thresh 31.835921063775835
target distance 11.0
model initialize at round 505
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 12.]), 'previousTarget': array([13., 12.]), 'currentState': array([22.33635665, 21.97624606,  4.62023715]), 'targetState': array([13, 12], dtype=int32), 'currentDistance': 13.663566191645886}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9053330327577029
{'scaleFactor': 20, 'currentTarget': array([13., 12.]), 'previousTarget': array([13., 12.]), 'currentState': array([13.16833622, 12.50317044,  5.51316992]), 'targetState': array([13, 12], dtype=int32), 'currentDistance': 0.5305822954725203}
episode index:506
target Thresh 31.837553676469525
target distance 11.0
model initialize at round 506
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 12.]), 'previousTarget': array([14., 12.]), 'currentState': array([24.30847997,  9.53433638,  3.00422931]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 10.599257351050676}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.905423086045952
{'scaleFactor': 20, 'currentTarget': array([14., 12.]), 'previousTarget': array([14., 12.]), 'currentState': array([14.73278136, 11.35612023,  2.14274787]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 0.975474080431035}
episode index:507
target Thresh 31.83917004439549
target distance 14.0
model initialize at round 507
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([12.6980985 , 22.65567013,  2.76115876]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 18.41611038197568}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9054032340148748
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.96531091, 7.46636779, 5.4015314 ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.46765612229008247}
episode index:508
target Thresh 31.84077032919187
target distance 9.0
model initialize at round 508
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 12.]), 'previousTarget': array([ 5., 12.]), 'currentState': array([3.29286599, 2.76713998, 2.33608961]), 'targetState': array([ 5, 12], dtype=int32), 'currentDistance': 9.389356245638982}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9054741120411737
{'scaleFactor': 20, 'currentTarget': array([ 5., 12.]), 'previousTarget': array([ 5., 12.]), 'currentState': array([ 5.55564021, 12.62318254,  2.36234665]), 'targetState': array([ 5, 12], dtype=int32), 'currentDistance': 0.8349206647792077}
episode index:509
target Thresh 31.842354690888477
target distance 7.0
model initialize at round 509
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([13.58232555, 21.42673135,  0.6624127 ]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 6.410435564608722}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9055821941940342
{'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([18.83782737, 17.78422805,  5.00283183]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 0.2699212800606746}
episode index:510
target Thresh 31.8439232879228
target distance 6.0
model initialize at round 510
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 20.]), 'previousTarget': array([16., 20.]), 'currentState': array([13.6244297 , 25.03036376,  4.76559806]), 'targetState': array([16, 20], dtype=int32), 'currentDistance': 5.563083120413493}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9057088415635175
{'scaleFactor': 20, 'currentTarget': array([16., 20.]), 'previousTarget': array([16., 20.]), 'currentState': array([16.15591582, 19.67251334,  5.27152818]), 'targetState': array([16, 20], dtype=int32), 'currentDistance': 0.362708220943704}
episode index:511
target Thresh 31.84547627715585
target distance 17.0
model initialize at round 511
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 27.]), 'previousTarget': array([17., 27.]), 'currentState': array([ 9.88702254, 11.6791783 ,  2.64796871]), 'targetState': array([17, 27], dtype=int32), 'currentDistance': 16.891477907579507}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9057062502225903
{'scaleFactor': 20, 'currentTarget': array([17., 27.]), 'previousTarget': array([17., 27.]), 'currentState': array([1.63989562e+01, 2.68949866e+01, 1.49158128e-02]), 'targetState': array([17, 27], dtype=int32), 'currentDistance': 0.6101487285073713}
episode index:512
target Thresh 31.84701381388785
target distance 16.0
model initialize at round 512
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 26.]), 'previousTarget': array([ 2., 26.]), 'currentState': array([16.56630217, 22.97015549,  1.62018788]), 'targetState': array([ 2, 26], dtype=int32), 'currentDistance': 14.87807503618602}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.905721476338109
{'scaleFactor': 20, 'currentTarget': array([ 2., 26.]), 'previousTarget': array([ 2., 26.]), 'currentState': array([ 1.99286181, 26.12119319,  4.11409163]), 'targetState': array([ 2, 26], dtype=int32), 'currentDistance': 0.12140322279618163}
episode index:513
target Thresh 31.848536051873744
target distance 8.0
model initialize at round 513
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  7.]), 'previousTarget': array([13.,  7.]), 'currentState': array([ 7.23048044, 13.33288667,  5.85976934]), 'targetState': array([13,  7], dtype=int32), 'currentDistance': 8.566960342093056}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9058095474928985
{'scaleFactor': 20, 'currentTarget': array([13.,  7.]), 'previousTarget': array([13.,  7.]), 'currentState': array([12.41210667,  6.40233191,  3.51571308]), 'targetState': array([13,  7], dtype=int32), 'currentDistance': 0.8383470134542774}
episode index:514
target Thresh 31.85004314333861
target distance 9.0
model initialize at round 514
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([ 9.87045381, 17.7523969 ,  4.98661327]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 9.882863667840862}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9058788107975745
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.47409879,  9.50871232,  3.80639636]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.7196774734894751}
episode index:515
target Thresh 31.85153523899284
target distance 8.0
model initialize at round 515
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([13.06884594, 11.70001675,  0.12733525]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 7.052010570721868}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9059848518813002
{'scaleFactor': 20, 'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([20.30565594, 12.36375529,  4.87186513]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 0.7058561384656423}
episode index:516
target Thresh 31.853012488047252
target distance 19.0
model initialize at round 516
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  2.]), 'previousTarget': array([22.,  2.]), 'currentState': array([ 5.80918645, 19.52432884,  6.22396564]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 23.858846256151768}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.9058960191980506
{'scaleFactor': 20, 'currentTarget': array([22.,  2.]), 'previousTarget': array([22.,  2.]), 'currentState': array([22.57784638,  1.86191048,  5.1560564 ]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 0.5941171183802485}
episode index:517
target Thresh 31.854475038227978
target distance 18.0
model initialize at round 517
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([20.54729169, 21.33797543,  4.86889452]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 18.203786342171366}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9059107319939688
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.75910757,  4.45718813,  3.54902216]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.8861519584397843}
episode index:518
target Thresh 31.855923035791257
target distance 13.0
model initialize at round 518
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 21.]), 'previousTarget': array([27., 21.]), 'currentState': array([14.467393  , 23.38323413,  6.00380802]), 'targetState': array([27, 21], dtype=int32), 'currentDistance': 12.757195741673563}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9059792665169111
{'scaleFactor': 20, 'currentTarget': array([27., 21.]), 'previousTarget': array([27., 21.]), 'currentState': array([26.08519194, 21.56224837,  5.33943383]), 'targetState': array([27, 21], dtype=int32), 'currentDistance': 1.073776990053223}
episode index:519
target Thresh 31.85735662553805
target distance 11.0
model initialize at round 519
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  7.]), 'previousTarget': array([19.,  7.]), 'currentState': array([8.7862467 , 9.48802023, 0.07469988]), 'targetState': array([19,  7], dtype=int32), 'currentDistance': 10.512421278228198}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9060658257157247
{'scaleFactor': 20, 'currentTarget': array([19.,  7.]), 'previousTarget': array([19.,  7.]), 'currentState': array([18.2263565 ,  7.57857786,  5.20117355]), 'targetState': array([19,  7], dtype=int32), 'currentDistance': 0.966062425640337}
episode index:520
target Thresh 31.858775950828523
target distance 12.0
model initialize at round 520
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 28.]), 'previousTarget': array([15., 28.]), 'currentState': array([26.01480213, 26.36446811,  3.20616031]), 'targetState': array([15, 28], dtype=int32), 'currentDistance': 11.135566013609413}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9061337994656005
{'scaleFactor': 20, 'currentTarget': array([15., 28.]), 'previousTarget': array([15., 28.]), 'currentState': array([14.65190018, 27.98370607,  3.43629107]), 'targetState': array([15, 28], dtype=int32), 'currentDistance': 0.34848095476322527}
episode index:521
target Thresh 31.860181153596397
target distance 19.0
model initialize at round 521
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([13.70297527,  3.65655196,  2.75821406]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 17.654052284501706}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9061131183445145
{'scaleFactor': 20, 'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([16.86524238, 20.3097429 ,  6.02372945]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 0.7032883375736644}
episode index:522
target Thresh 31.86157237436311
target distance 18.0
model initialize at round 522
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 29.]), 'previousTarget': array([21., 29.]), 'currentState': array([27.58118474, 12.5794342 ,  2.22820295]), 'targetState': array([21, 29], dtype=int32), 'currentDistance': 17.69030734327594}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9061272753791972
{'scaleFactor': 20, 'currentTarget': array([21., 29.]), 'previousTarget': array([21., 29.]), 'currentState': array([20.8995279 , 28.7696619 ,  1.50503863]), 'targetState': array([21, 29], dtype=int32), 'currentDistance': 0.2512972002679322}
episode index:523
target Thresh 31.862949752251907
target distance 13.0
model initialize at round 523
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([5.96997168, 9.68296525, 2.59819567]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 13.876477596279535}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9061413783793966
{'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([14.07113002, 21.38794464,  0.85083419]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 0.39441161475000064}
episode index:524
target Thresh 31.86431342500172
target distance 13.0
model initialize at round 524
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 26.]), 'previousTarget': array([ 2., 26.]), 'currentState': array([14.86266726, 26.67735739,  2.66248858]), 'targetState': array([ 2, 26], dtype=int32), 'currentDistance': 12.880489975596081}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9061730037432986
{'scaleFactor': 20, 'currentTarget': array([ 2., 26.]), 'previousTarget': array([ 2., 26.]), 'currentState': array([ 1.81834749, 26.38130058,  1.76882992]), 'targetState': array([ 2, 26], dtype=int32), 'currentDistance': 0.4223597628800959}
episode index:525
target Thresh 31.865663528980956
target distance 6.0
model initialize at round 525
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 16.]), 'previousTarget': array([ 8., 16.]), 'currentState': array([ 7.24585244, 11.66500588,  0.41434646]), 'targetState': array([ 8, 16], dtype=int32), 'currentDistance': 4.400103703522883}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9062949162837106
{'scaleFactor': 20, 'currentTarget': array([ 8., 16.]), 'previousTarget': array([ 8., 16.]), 'currentState': array([ 8.1443582 , 16.67580935,  2.28516102]), 'targetState': array([ 8, 16], dtype=int32), 'currentDistance': 0.691055395850101}
episode index:526
target Thresh 31.867000199201147
target distance 17.0
model initialize at round 526
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 27.]), 'previousTarget': array([26., 27.]), 'currentState': array([10.59820505, 19.11656557,  0.21559769]), 'targetState': array([26, 27], dtype=int32), 'currentDistance': 17.302133567794428}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.906291286603872
{'scaleFactor': 20, 'currentTarget': array([26., 27.]), 'previousTarget': array([26., 27.]), 'currentState': array([25.93606125, 26.59988254,  4.73300329]), 'targetState': array([26, 27], dtype=int32), 'currentDistance': 0.40519396033120414}
episode index:527
target Thresh 31.86832356933042
target distance 8.0
model initialize at round 527
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 12.]), 'previousTarget': array([19., 12.]), 'currentState': array([15.88468863,  5.26034465,  1.28522038]), 'targetState': array([19, 12], dtype=int32), 'currentDistance': 7.424831254108357}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.906394136458789
{'scaleFactor': 20, 'currentTarget': array([19., 12.]), 'previousTarget': array([19., 12.]), 'currentState': array([19.23087814, 11.60101439,  5.80940939]), 'targetState': array([19, 12], dtype=int32), 'currentDistance': 0.4609709652573584}
episode index:528
target Thresh 31.8696337717069
target distance 11.0
model initialize at round 528
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 20.]), 'previousTarget': array([ 6., 20.]), 'currentState': array([15.44695054, 27.64840223,  3.75608945]), 'targetState': array([ 6, 20], dtype=int32), 'currentDistance': 12.15495500189138}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9064604616250314
{'scaleFactor': 20, 'currentTarget': array([ 6., 20.]), 'previousTarget': array([ 6., 20.]), 'currentState': array([ 6.62584682, 20.48644082,  2.43664252]), 'targetState': array([ 6, 20], dtype=int32), 'currentDistance': 0.792659392088134}
episode index:529
target Thresh 31.87093093735191
target distance 21.0
model initialize at round 529
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 29.]), 'previousTarget': array([18., 29.]), 'currentState': array([10.44449297,  9.58865526,  2.91716552]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 20.829930178081376}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9063893019102086
{'scaleFactor': 20, 'currentTarget': array([18., 29.]), 'previousTarget': array([18., 29.]), 'currentState': array([18.27514728, 28.73398068,  5.89369174]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 0.38271699738615594}
episode index:530
target Thresh 31.8722151959831
target distance 8.0
model initialize at round 530
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 28.]), 'previousTarget': array([18., 28.]), 'currentState': array([11.91191834, 28.81692344,  5.69197891]), 'targetState': array([18, 28], dtype=int32), 'currentDistance': 6.142646184984851}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9064913861062346
{'scaleFactor': 20, 'currentTarget': array([18., 28.]), 'previousTarget': array([18., 28.]), 'currentState': array([18.29308217, 27.4167643 ,  5.30443341]), 'targetState': array([18, 28], dtype=int32), 'currentDistance': 0.6527335117130341}
episode index:531
target Thresh 31.873486676027397
target distance 7.0
model initialize at round 531
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 24.]), 'previousTarget': array([ 3., 24.]), 'currentState': array([ 8.22535487, 18.4946861 ,  1.03983355]), 'targetState': array([ 3, 24], dtype=int32), 'currentDistance': 7.590310580738084}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9065750302111101
{'scaleFactor': 20, 'currentTarget': array([ 3., 24.]), 'previousTarget': array([ 3., 24.]), 'currentState': array([ 2.38283445, 24.20383001,  3.94050157]), 'targetState': array([ 3, 24], dtype=int32), 'currentDistance': 0.6499538374482551}
episode index:532
target Thresh 31.874745504633868
target distance 22.0
model initialize at round 532
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 17.]), 'previousTarget': array([27., 17.]), 'currentState': array([ 6.39929184, 25.06492224,  4.68408686]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 22.123113416542502}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9065205198786295
{'scaleFactor': 20, 'currentTarget': array([27., 17.]), 'previousTarget': array([27., 17.]), 'currentState': array([26.28887384, 17.48660962,  0.95146978]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 0.861678214039847}
episode index:533
target Thresh 31.875991807686425
target distance 9.0
model initialize at round 533
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([21.47829086, 14.19560934,  4.77485574]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 10.44683038187484}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9065859873496451
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.54479133,  6.854657  ,  5.19685388]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.968325113928729}
episode index:534
target Thresh 31.87722570981641
target distance 22.0
model initialize at round 534
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 29.]), 'previousTarget': array([19., 29.]), 'currentState': array([22.24739714,  8.12977364,  1.74595803]), 'targetState': array([19, 29], dtype=int32), 'currentDistance': 21.121362093125814}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9065649635494752
{'scaleFactor': 20, 'currentTarget': array([19., 29.]), 'previousTarget': array([19., 29.]), 'currentState': array([19.3823367 , 28.54339713,  6.13750477]), 'targetState': array([19, 29], dtype=int32), 'currentDistance': 0.5955396955428904}
episode index:535
target Thresh 31.878447334415068
target distance 11.0
model initialize at round 535
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([13.87324477, 10.43869095,  2.0352663 ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 11.960090847671232}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9066125388934257
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.1879264 , 9.7090781 , 4.58191659]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.7335585034348412}
episode index:536
target Thresh 31.87965680364587
target distance 12.0
model initialize at round 536
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 29.]), 'previousTarget': array([ 4., 29.]), 'currentState': array([14.41106107, 17.44532789,  2.46745062]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 15.553155314113543}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9066425801514044
{'scaleFactor': 20, 'currentTarget': array([ 4., 29.]), 'previousTarget': array([ 4., 29.]), 'currentState': array([ 4.4866966 , 28.73839978,  0.7095587 ]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 0.5525470573937751}
episode index:537
target Thresh 31.88085423845675
target distance 15.0
model initialize at round 537
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 20.]), 'previousTarget': array([18., 20.]), 'currentState': array([ 3.72240947, 20.52003706,  0.11713767]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 14.287058130178112}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9066898343665635
{'scaleFactor': 20, 'currentTarget': array([18., 20.]), 'previousTarget': array([18., 20.]), 'currentState': array([17.08864941, 20.21785311,  4.92639609]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 0.9370271505235981}
episode index:538
target Thresh 31.882039758592185
target distance 4.0
model initialize at round 538
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  5.]), 'previousTarget': array([18.,  5.]), 'currentState': array([14.6890039 ,  0.46453209,  6.14418268]), 'targetState': array([18,  5], dtype=int32), 'currentDistance': 5.615439823070829}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9068078476608741
{'scaleFactor': 20, 'currentTarget': array([18.,  5.]), 'previousTarget': array([18.,  5.]), 'currentState': array([17.98354549,  4.29214766,  1.86578148]), 'targetState': array([18,  5], dtype=int32), 'currentDistance': 0.7080435658071421}
episode index:539
target Thresh 31.883213482605182
target distance 20.0
model initialize at round 539
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 25.]), 'previousTarget': array([17., 25.]), 'currentState': array([5.47451108, 4.18870155, 0.50699156]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 23.789641402760903}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.9067212745256527
{'scaleFactor': 20, 'currentTarget': array([17., 25.]), 'previousTarget': array([17., 25.]), 'currentState': array([17.07320339, 24.47843772,  5.24102923]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 0.5266744210434718}
episode index:540
target Thresh 31.884375527869114
target distance 9.0
model initialize at round 540
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([13.09581799, 21.61151747,  2.84110002]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 7.863361036982266}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9068208581402079
{'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([ 6.09949641, 25.13270755,  2.01479661]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 0.16586388856062845}
episode index:541
target Thresh 31.885526010589484
target distance 18.0
model initialize at round 541
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  6.]), 'previousTarget': array([22.,  6.]), 'currentState': array([14.3971001 , 23.06164754,  0.41855722]), 'targetState': array([22,  6], dtype=int32), 'currentDistance': 18.67896956237932}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9067996725241904
{'scaleFactor': 20, 'currentTarget': array([22.,  6.]), 'previousTarget': array([22.,  6.]), 'currentState': array([22.62421409,  6.86574422,  3.84240959]), 'targetState': array([22,  6], dtype=int32), 'currentDistance': 1.0673126421364274}
episode index:542
target Thresh 31.886665045815512
target distance 8.0
model initialize at round 542
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 10.]), 'previousTarget': array([26., 10.]), 'currentState': array([18.42962045, 11.62721061,  0.30266404]), 'targetState': array([26, 10], dtype=int32), 'currentDistance': 7.7432848927141205}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9068987449688972
{'scaleFactor': 20, 'currentTarget': array([26., 10.]), 'previousTarget': array([26., 10.]), 'currentState': array([25.80990878,  9.87294512,  5.05525586]), 'targetState': array([26, 10], dtype=int32), 'currentDistance': 0.22864298223468785}
episode index:543
target Thresh 31.88779274745168
target distance 7.0
model initialize at round 543
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([5.96358124, 6.37981857, 6.23437125]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 6.048356276481774}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9070152895553515
{'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([11.94928243,  6.29589047,  6.07083859]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 0.30020566588012704}
episode index:544
target Thresh 31.888909228269085
target distance 3.0
model initialize at round 544
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([16.5010012 ,  4.76117244,  1.47931927]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 2.695431957869484}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9071493899414884
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.03170825,  7.10679742,  1.35261756]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.9741635409825798}
episode index:545
target Thresh 31.89001459991674
target distance 10.0
model initialize at round 545
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 14.]), 'previousTarget': array([13., 14.]), 'currentState': array([14.3218899 ,  4.12780683,  2.05557823]), 'targetState': array([13, 14], dtype=int32), 'currentDistance': 9.960300745661613}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9072296841904967
{'scaleFactor': 20, 'currentTarget': array([13., 14.]), 'previousTarget': array([13., 14.]), 'currentState': array([13.00167722, 13.48380666,  0.50348357]), 'targetState': array([13, 14], dtype=int32), 'currentDistance': 0.5161960608865419}
episode index:546
target Thresh 31.891108972932734
target distance 10.0
model initialize at round 546
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  2.]), 'previousTarget': array([11.,  2.]), 'currentState': array([20.27794697,  6.47689325,  5.27530134]), 'targetState': array([11,  2], dtype=int32), 'currentDistance': 10.30159566188997}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9072922993005708
{'scaleFactor': 20, 'currentTarget': array([11.,  2.]), 'previousTarget': array([11.,  2.]), 'currentState': array([10.83803118,  2.60997634,  1.70651618]), 'targetState': array([11,  2], dtype=int32), 'currentDistance': 0.6311141245779432}
episode index:547
target Thresh 31.892192456755275
target distance 11.0
model initialize at round 547
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 22.]), 'previousTarget': array([15., 22.]), 'currentState': array([11.37548823, 10.03024734,  0.39590996]), 'targetState': array([15, 22], dtype=int32), 'currentDistance': 12.506480888723685}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9073375055936481
{'scaleFactor': 20, 'currentTarget': array([15., 22.]), 'previousTarget': array([15., 22.]), 'currentState': array([14.71166975, 21.16654353,  0.90404987]), 'targetState': array([15, 22], dtype=int32), 'currentDistance': 0.8819206439389079}
episode index:548
target Thresh 31.893265159733655
target distance 9.0
model initialize at round 548
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([ 9.32053853, 26.89138485,  4.21617532]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 13.896316847830562}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9073655696898855
{'scaleFactor': 20, 'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([19.23669147, 17.79635885,  4.67729771]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 0.7900060926143561}
episode index:549
target Thresh 31.894327189139062
target distance 18.0
model initialize at round 549
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 24.]), 'previousTarget': array([ 8., 24.]), 'currentState': array([9.31323848, 7.53647216, 0.98113012]), 'targetState': array([ 8, 24], dtype=int32), 'currentDistance': 16.51582102977099}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9073601451541017
{'scaleFactor': 20, 'currentTarget': array([ 8., 24.]), 'previousTarget': array([ 8., 24.]), 'currentState': array([ 8.54236304, 23.38778036,  5.90446501]), 'targetState': array([ 8, 24], dtype=int32), 'currentDistance': 0.8179061977481102}
episode index:550
target Thresh 31.89537865117532
target distance 11.0
model initialize at round 550
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([15.68079815, 11.9145268 ,  0.95919102]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 12.94298188334026}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9073880662961594
{'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([ 9.17169523, 23.66588945,  1.97640627]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 0.6876685356348381}
episode index:551
target Thresh 31.89641965098951
target distance 8.0
model initialize at round 551
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 22.]), 'previousTarget': array([11., 22.]), 'currentState': array([ 4.24294561, 13.86533076,  0.27011031]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 10.575000127633535}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9074498273162769
{'scaleFactor': 20, 'currentTarget': array([11., 22.]), 'previousTarget': array([11., 22.]), 'currentState': array([10.32737876, 21.70215954,  0.67063972]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 0.7356142168816575}
episode index:552
target Thresh 31.897450292682485
target distance 13.0
model initialize at round 552
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([ 3.62772761, 18.57234253,  0.75307387]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 11.380310638388783}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9074943400117393
{'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([15.33027486, 18.32319513,  5.68458247]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 0.7530911722749233}
episode index:553
target Thresh 31.898470679319267
target distance 13.0
model initialize at round 553
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 18.]), 'previousTarget': array([24., 18.]), 'currentState': array([15.07561162,  3.70560913,  0.13255089]), 'targetState': array([24, 18], dtype=int32), 'currentDistance': 16.851537566584486}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9074887222048749
{'scaleFactor': 20, 'currentTarget': array([24., 18.]), 'previousTarget': array([24., 18.]), 'currentState': array([24.16646735, 17.90639131,  0.32540315]), 'targetState': array([24, 18], dtype=int32), 'currentDistance': 0.19098158280499913}
episode index:554
target Thresh 31.899480912939374
target distance 24.0
model initialize at round 554
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 24.]), 'previousTarget': array([25.52137473, 23.62108833]), 'currentState': array([3.67088504, 4.79867621, 0.89008873]), 'targetState': array([26, 24], dtype=int32), 'currentDistance': 29.4496215656712}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.9074032620831387
{'scaleFactor': 20, 'currentTarget': array([26., 24.]), 'previousTarget': array([26., 24.]), 'currentState': array([25.89928203, 23.56915679,  5.53635637]), 'targetState': array([26, 24], dtype=int32), 'currentDistance': 0.44245902058554165}
episode index:555
target Thresh 31.90048109456701
target distance 13.0
model initialize at round 555
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 24.]), 'previousTarget': array([12., 24.]), 'currentState': array([23.5234037 , 18.80749703,  3.65116048]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 12.639261052126646}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9074308545873561
{'scaleFactor': 20, 'currentTarget': array([12., 24.]), 'previousTarget': array([12., 24.]), 'currentState': array([11.49095604, 24.90608736,  3.75509763]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 1.03928824255604}
episode index:556
target Thresh 31.90147132422117
target distance 15.0
model initialize at round 556
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([ 7.3234893 , 26.24485253,  6.27881474]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 16.904584764546325}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9074253810154017
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([12.41554689, 10.42552406,  4.61225406]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.5947688150645892}
episode index:557
target Thresh 31.902451700925642
target distance 5.0
model initialize at round 557
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  8.]), 'previousTarget': array([10.,  8.]), 'currentState': array([11.6505656 , 11.16938162,  5.17650592]), 'targetState': array([10,  8], dtype=int32), 'currentDistance': 3.5734222577433554}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9075556222680623
{'scaleFactor': 20, 'currentTarget': array([10.,  8.]), 'previousTarget': array([10.,  8.]), 'currentState': array([9.93399314, 8.27229397, 5.175668  ]), 'targetState': array([10,  8], dtype=int32), 'currentDistance': 0.2801801443924547}
episode index:558
target Thresh 31.90342232271892
target distance 13.0
model initialize at round 558
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 26.]), 'previousTarget': array([11., 26.]), 'currentState': array([22.35846349, 14.10238597,  2.28644681]), 'targetState': array([11, 26], dtype=int32), 'currentDistance': 16.448948671206693}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9075827941323912
{'scaleFactor': 20, 'currentTarget': array([11., 26.]), 'previousTarget': array([11., 26.]), 'currentState': array([11.42139412, 25.30137173,  1.40399747]), 'targetState': array([11, 26], dtype=int32), 'currentDistance': 0.8158764991228693}
episode index:559
target Thresh 31.904383286663986
target distance 11.0
model initialize at round 559
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 22.]), 'previousTarget': array([ 7., 22.]), 'currentState': array([19.75974242, 21.05463501,  5.70449914]), 'targetState': array([ 7, 22], dtype=int32), 'currentDistance': 12.794715377083484}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9075933913705184
{'scaleFactor': 20, 'currentTarget': array([ 7., 22.]), 'previousTarget': array([ 7., 22.]), 'currentState': array([ 6.75261956, 21.76512987,  4.10724415]), 'targetState': array([ 7, 22], dtype=int32), 'currentDistance': 0.3411173704468091}
episode index:560
target Thresh 31.905334688858037
target distance 13.0
model initialize at round 560
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 18.]), 'previousTarget': array([12., 18.]), 'currentState': array([23.3625067 ,  6.13874926,  2.40445679]), 'targetState': array([12, 18], dtype=int32), 'currentDistance': 16.425462780919297}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9076203990408525
{'scaleFactor': 20, 'currentTarget': array([12., 18.]), 'previousTarget': array([12., 18.]), 'currentState': array([12.19543763, 17.26574817,  1.77248491]), 'targetState': array([12, 18], dtype=int32), 'currentDistance': 0.7598168355009574}
episode index:561
target Thresh 31.906276624442086
target distance 6.0
model initialize at round 561
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([19.26988585,  0.48364865,  3.25365543]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 6.33533137762007}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9077146616938048
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.58550376,  3.84162929,  4.03222299]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.44372109570428875}
episode index:562
target Thresh 31.907209187610476
target distance 7.0
model initialize at round 562
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 15.]), 'previousTarget': array([11., 15.]), 'currentState': array([ 5.22727311, 11.1516027 ,  1.76359957]), 'targetState': array([11, 15], dtype=int32), 'currentDistance': 6.937905840098403}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9078085894883096
{'scaleFactor': 20, 'currentTarget': array([11., 15.]), 'previousTarget': array([11., 15.]), 'currentState': array([10.83434607, 14.74085981,  0.32937419]), 'targetState': array([11, 15], dtype=int32), 'currentDistance': 0.30756277911586016}
episode index:563
target Thresh 31.908132471620302
target distance 17.0
model initialize at round 563
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 19.]), 'previousTarget': array([ 3., 19.]), 'currentState': array([7.49124173, 2.78011934, 1.49198311]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 16.83020442402286}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9078025141080267
{'scaleFactor': 20, 'currentTarget': array([ 3., 19.]), 'previousTarget': array([ 3., 19.]), 'currentState': array([ 2.92227645, 18.84200321,  0.8506914 ]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 0.17607934367847938}
episode index:564
target Thresh 31.909046568800733
target distance 8.0
model initialize at round 564
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  9.]), 'previousTarget': array([19.,  9.]), 'currentState': array([12.66771166,  9.77388962,  0.8752405 ]), 'targetState': array([19,  9], dtype=int32), 'currentDistance': 6.379402853318315}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9078959539237648
{'scaleFactor': 20, 'currentTarget': array([19.,  9.]), 'previousTarget': array([19.,  9.]), 'currentState': array([19.35601741,  8.91139539,  0.83993262]), 'targetState': array([19,  9], dtype=int32), 'currentDistance': 0.3668775970919163}
episode index:565
target Thresh 31.90995157056225
target distance 14.0
model initialize at round 565
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 21.]), 'previousTarget': array([21., 21.]), 'currentState': array([12.43553987,  8.5854882 ,  0.90282106]), 'targetState': array([21, 21], dtype=int32), 'currentDistance': 15.082111272270577}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9079221884476236
{'scaleFactor': 20, 'currentTarget': array([21., 21.]), 'previousTarget': array([21., 21.]), 'currentState': array([21.59468066, 20.3806562 ,  5.50017083]), 'targetState': array([21, 21], dtype=int32), 'currentDistance': 0.8586220523065656}
episode index:566
target Thresh 31.91084756740578
target distance 12.0
model initialize at round 566
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 18.]), 'previousTarget': array([23., 18.]), 'currentState': array([14.06021728,  7.39613629,  1.15326095]), 'targetState': array([23, 18], dtype=int32), 'currentDistance': 13.86944990549763}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9079647689757708
{'scaleFactor': 20, 'currentTarget': array([23., 18.]), 'previousTarget': array([23., 18.]), 'currentState': array([22.91756982, 17.5640448 ,  6.04400529]), 'targetState': array([23, 18], dtype=int32), 'currentDistance': 0.4436796939871837}
episode index:567
target Thresh 31.91173464893176
target distance 16.0
model initialize at round 567
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  6.]), 'previousTarget': array([19.,  6.]), 'currentState': array([11.57073622, 22.60429778,  5.64045257]), 'targetState': array([19,  6], dtype=int32), 'currentDistance': 18.190565275134905}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9079425391963394
{'scaleFactor': 20, 'currentTarget': array([19.,  6.]), 'previousTarget': array([19.,  6.]), 'currentState': array([18.20049308,  6.1427464 ,  3.11727109]), 'targetState': array([19,  6], dtype=int32), 'currentDistance': 0.8121501420228802}
episode index:568
target Thresh 31.912612903849077
target distance 9.0
model initialize at round 568
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 18.]), 'previousTarget': array([ 7., 18.]), 'currentState': array([14.70101866, 11.92993665,  2.82066107]), 'targetState': array([ 7, 18], dtype=int32), 'currentDistance': 9.80567985553066}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9080181938724443
{'scaleFactor': 20, 'currentTarget': array([ 7., 18.]), 'previousTarget': array([ 7., 18.]), 'currentState': array([ 7.45041555, 18.04069246,  1.34788462]), 'targetState': array([ 7, 18], dtype=int32), 'currentDistance': 0.45224998100640273}
episode index:569
target Thresh 31.913482419983957
target distance 11.0
model initialize at round 569
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 25.]), 'previousTarget': array([14., 25.]), 'currentState': array([23.96903486, 21.66977484,  3.04305983]), 'targetState': array([14, 25], dtype=int32), 'currentDistance': 10.510568762732053}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9080935830935454
{'scaleFactor': 20, 'currentTarget': array([14., 25.]), 'previousTarget': array([14., 25.]), 'currentState': array([14.43701113, 24.3275923 ,  2.48071516]), 'targetState': array([14, 25], dtype=int32), 'currentDistance': 0.8019419202109495}
episode index:570
target Thresh 31.914343284288737
target distance 10.0
model initialize at round 570
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 23.]), 'previousTarget': array([22., 23.]), 'currentState': array([13.68262242, 28.03420353,  5.29351002]), 'targetState': array([22, 23], dtype=int32), 'currentDistance': 9.722241256633799}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9081687082543273
{'scaleFactor': 20, 'currentTarget': array([22., 23.]), 'previousTarget': array([22., 23.]), 'currentState': array([21.4086953 , 23.9524042 ,  5.57740716]), 'targetState': array([22, 23], dtype=int32), 'currentDistance': 1.1210330129241666}
episode index:571
target Thresh 31.91519558285056
target distance 20.0
model initialize at round 571
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 14.]), 'previousTarget': array([26., 14.]), 'currentState': array([ 6.83048805, 15.46378881,  0.04472637]), 'targetState': array([26, 14], dtype=int32), 'currentDistance': 19.22531836367261}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9081620882661359
{'scaleFactor': 20, 'currentTarget': array([26., 14.]), 'previousTarget': array([26., 14.]), 'currentState': array([25.8640728 , 13.53945883,  4.75068807]), 'targetState': array([26, 14], dtype=int32), 'currentDistance': 0.48018160498666956}
episode index:572
target Thresh 31.916039400900004
target distance 12.0
model initialize at round 572
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 25.]), 'previousTarget': array([ 9., 25.]), 'currentState': array([10.2281958 , 14.1506186 ,  1.76279801]), 'targetState': array([ 9, 25], dtype=int32), 'currentDistance': 10.918678571612475}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.908220234969687
{'scaleFactor': 20, 'currentTarget': array([ 9., 25.]), 'previousTarget': array([ 9., 25.]), 'currentState': array([ 9.60917986, 25.11602585,  6.14091274]), 'targetState': array([ 9, 25], dtype=int32), 'currentDistance': 0.6201307141243978}
episode index:573
target Thresh 31.91687482281957
target distance 22.0
model initialize at round 573
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.87871928, 27.87076113]), 'previousTarget': array([24.70066632, 27.7142724 ]), 'currentState': array([4.34974213, 5.99472179, 0.36983984]), 'targetState': array([25, 28], dtype=int32), 'currentDistance': 30.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.9080628947303623
{'scaleFactor': 20, 'currentTarget': array([25., 28.]), 'previousTarget': array([25., 28.]), 'currentState': array([25.23906573, 28.23056386,  5.86409874]), 'targetState': array([25, 28], dtype=int32), 'currentDistance': 0.3321326804012349}
episode index:574
target Thresh 31.917701932152145
target distance 21.0
model initialize at round 574
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([22.10601251,  4.57260062,  5.16071117]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 20.269928782524065}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9080251938207723
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.92096622, 2.11529884, 4.7458089 ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.928155482690556}
episode index:575
target Thresh 31.91852081160935
target distance 11.0
model initialize at round 575
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 26.]), 'previousTarget': array([20., 26.]), 'currentState': array([14.84123214, 16.45764191,  0.03737307]), 'targetState': array([20, 26], dtype=int32), 'currentDistance': 10.84755657476816}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9080832753408767
{'scaleFactor': 20, 'currentTarget': array([20., 26.]), 'previousTarget': array([20., 26.]), 'currentState': array([19.92444517, 25.20571022,  0.11140108]), 'targetState': array([20, 26], dtype=int32), 'currentDistance': 0.7978751715502921}
episode index:576
target Thresh 31.91933154307982
target distance 17.0
model initialize at round 576
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  6.]), 'previousTarget': array([19.,  6.]), 'currentState': array([ 3.14769488, 17.23092832,  6.09356147]), 'targetState': array([19,  6], dtype=int32), 'currentDistance': 19.42754046401196}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9080768607822423
{'scaleFactor': 20, 'currentTarget': array([19.,  6.]), 'previousTarget': array([19.,  6.]), 'currentState': array([19.10650282,  6.06754746,  4.93377271]), 'targetState': array([19,  6], dtype=int32), 'currentDistance': 0.12611705089387332}
episode index:577
target Thresh 31.920134207637375
target distance 10.0
model initialize at round 577
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'previousTarget': array([22., 18.]), 'currentState': array([21.12815711,  9.67808338,  2.50457317]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 8.36746115654716}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9081511050540724
{'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'previousTarget': array([22., 18.]), 'currentState': array([21.65855076, 18.07914079,  0.51515526]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 0.35050085028995226}
episode index:578
target Thresh 31.920928885549138
target distance 10.0
model initialize at round 578
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 29.]), 'previousTarget': array([15., 29.]), 'currentState': array([23.02207151, 25.27300085,  2.903606  ]), 'targetState': array([15, 29], dtype=int32), 'currentDistance': 8.84557256196013}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9082416834736682
{'scaleFactor': 20, 'currentTarget': array([15., 29.]), 'previousTarget': array([15., 29.]), 'currentState': array([15.63299567, 28.26099083,  2.59723865]), 'targetState': array([15, 29], dtype=int32), 'currentDistance': 0.9730457656251427}
episode index:579
target Thresh 31.921715656283563
target distance 9.0
model initialize at round 579
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([20.32768687, 16.81090506,  4.26418829]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 10.004170265629961}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9083153875537137
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13.31657858,  9.87473268,  3.51556031]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.3404613036089505}
episode index:580
target Thresh 31.92249459851838
target distance 19.0
model initialize at round 580
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 22.]), 'previousTarget': array([25., 22.]), 'currentState': array([13.87426901,  4.79831999,  1.08222547]), 'targetState': array([25, 22], dtype=int32), 'currentDistance': 20.486085159168468}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.908293051696063
{'scaleFactor': 20, 'currentTarget': array([25., 22.]), 'previousTarget': array([25., 22.]), 'currentState': array([25.75800658, 21.61575265,  5.28755753]), 'targetState': array([25, 22], dtype=int32), 'currentDistance': 0.8498352775797928}
episode index:581
target Thresh 31.92326579014846
target distance 11.0
model initialize at round 581
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 18.]), 'previousTarget': array([ 2., 18.]), 'currentState': array([ 1.92605483, 27.70422616,  5.03032851]), 'targetState': array([ 2, 18], dtype=int32), 'currentDistance': 9.704507880274837}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9083664142359325
{'scaleFactor': 20, 'currentTarget': array([ 2., 18.]), 'previousTarget': array([ 2., 18.]), 'currentState': array([ 2.05737286, 17.9796921 ,  4.05772892]), 'targetState': array([ 2, 18], dtype=int32), 'currentDistance': 0.060860956041842586}
episode index:582
target Thresh 31.92402930829361
target distance 7.0
model initialize at round 582
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([12.00268159, 11.27019191,  5.28051085]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 6.0629069272392275}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9084726450862995
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.11015921,  6.13877678,  5.2169092 ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.17718364701141193}
episode index:583
target Thresh 31.924785229306277
target distance 18.0
model initialize at round 583
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 23.]), 'previousTarget': array([ 4., 23.]), 'currentState': array([20.40899623, 16.45127867,  2.46370792]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 17.66750996673248}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9084501546910468
{'scaleFactor': 20, 'currentTarget': array([ 4., 23.]), 'previousTarget': array([ 4., 23.]), 'currentState': array([ 4.06719466, 23.09846911,  3.97025537]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 0.11921110222780802}
episode index:584
target Thresh 31.9255336287792
target distance 5.0
model initialize at round 584
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  6.]), 'previousTarget': array([22.,  6.]), 'currentState': array([23.4880707 , 10.21384874,  4.78715307]), 'targetState': array([22,  6], dtype=int32), 'currentDistance': 4.468878562858909}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9085726330590963
{'scaleFactor': 20, 'currentTarget': array([22.,  6.]), 'previousTarget': array([22.,  6.]), 'currentState': array([22.61565269,  6.43724492,  3.94742583]), 'targetState': array([22,  6], dtype=int32), 'currentDistance': 0.755123410142015}
episode index:585
target Thresh 31.926274581552942
target distance 13.0
model initialize at round 585
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 29.]), 'previousTarget': array([ 7., 29.]), 'currentState': array([ 9.15269242, 17.45411767,  3.1083945 ]), 'targetState': array([ 7, 29], dtype=int32), 'currentDistance': 11.744849232914078}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9086127230161747
{'scaleFactor': 20, 'currentTarget': array([ 7., 29.]), 'previousTarget': array([ 7., 29.]), 'currentState': array([ 6.59437308, 28.61756972,  0.48437095]), 'targetState': array([ 7, 29], dtype=int32), 'currentDistance': 0.5574819449976691}
episode index:586
target Thresh 31.9270081617234
target distance 12.0
model initialize at round 586
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([25.33796861, 21.73534974,  4.30949998]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 10.992107749664907}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9086687152246666
{'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([15.70706363, 18.69854098,  4.73930109]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 0.9939308215118315}
episode index:587
target Thresh 31.927734442649207
target distance 3.0
model initialize at round 587
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 22.]), 'previousTarget': array([ 3., 22.]), 'currentState': array([ 5.09691439, 25.42014945,  3.14720178]), 'targetState': array([ 3, 22], dtype=int32), 'currentDistance': 4.011791653005113}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9087901970014954
{'scaleFactor': 20, 'currentTarget': array([ 3., 22.]), 'previousTarget': array([ 3., 22.]), 'currentState': array([ 3.30370655, 22.56602059,  3.16420484]), 'targetState': array([ 3, 22], dtype=int32), 'currentDistance': 0.6423526866079846}
episode index:588
target Thresh 31.928453496959058
target distance 7.0
model initialize at round 588
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 27.]), 'previousTarget': array([15., 27.]), 'currentState': array([13.22063149, 21.15864009,  1.76934927]), 'targetState': array([15, 27], dtype=int32), 'currentDistance': 6.1063604499286965}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9088946262086236
{'scaleFactor': 20, 'currentTarget': array([15., 27.]), 'previousTarget': array([15., 27.]), 'currentState': array([14.70787471, 26.75264603,  0.67187081]), 'targetState': array([15, 27], dtype=int32), 'currentDistance': 0.38278083906715216}
episode index:589
target Thresh 31.92916539655898
target distance 8.0
model initialize at round 589
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 19.]), 'previousTarget': array([12., 19.]), 'currentState': array([15.4281736 , 25.08352549,  4.4327748 ]), 'targetState': array([12, 19], dtype=int32), 'currentDistance': 6.982954716914552}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9089822556726768
{'scaleFactor': 20, 'currentTarget': array([12., 19.]), 'previousTarget': array([12., 19.]), 'currentState': array([11.45683655, 19.25531438,  2.44158743]), 'targetState': array([12, 19], dtype=int32), 'currentDistance': 0.6001766097287273}
episode index:590
target Thresh 31.92987021263953
target distance 25.0
model initialize at round 590
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([11.67066957, 27.34956686,  5.52543402]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 25.272991956140395}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.9088849215194148
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.80629611, 4.0226642 , 3.3128992 ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.1950252836672537}
episode index:591
target Thresh 31.9305680156829
target distance 8.0
model initialize at round 591
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([ 4.43652349, 14.41335835,  3.36241484]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 6.572272447184732}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9089722713310374
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.64279285, 7.83257953, 3.83410549]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.394495324029443}
episode index:592
target Thresh 31.931258875469982
target distance 17.0
model initialize at round 592
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 19.]), 'previousTarget': array([10., 19.]), 'currentState': array([17.88426409,  3.64548217,  2.35778764]), 'targetState': array([10, 19], dtype=int32), 'currentDistance': 17.260441422364014}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9089799357090351
{'scaleFactor': 20, 'currentTarget': array([10., 19.]), 'previousTarget': array([10., 19.]), 'currentState': array([10.29369683, 18.94260273,  0.40265362]), 'targetState': array([10, 19], dtype=int32), 'currentDistance': 0.2992528562279601}
episode index:593
target Thresh 31.931942861087325
target distance 11.0
model initialize at round 593
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 26.]), 'previousTarget': array([13., 26.]), 'currentState': array([23.27260037, 26.51765538,  3.02774048]), 'targetState': array([13, 26], dtype=int32), 'currentDistance': 10.285634905683068}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9090506598069998
{'scaleFactor': 20, 'currentTarget': array([13., 26.]), 'previousTarget': array([13., 26.]), 'currentState': array([13.42377794, 25.48837234,  2.9490926 ]), 'targetState': array([13, 26], dtype=int32), 'currentDistance': 0.6643422353162604}
episode index:594
target Thresh 31.93262004093406
target distance 6.0
model initialize at round 594
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 8.]), 'previousTarget': array([6., 8.]), 'currentState': array([8.81977316, 3.46981633, 2.07203127]), 'targetState': array([6, 8], dtype=int32), 'currentDistance': 5.3360739073182755}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9091535981938788
{'scaleFactor': 20, 'currentTarget': array([6., 8.]), 'previousTarget': array([6., 8.]), 'currentState': array([5.93459618, 8.60177448, 1.49795004]), 'targetState': array([6, 8], dtype=int32), 'currentDistance': 0.6053182534420575}
episode index:595
target Thresh 31.933290482728744
target distance 16.0
model initialize at round 595
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 25.]), 'previousTarget': array([ 2., 25.]), 'currentState': array([3.31708512, 9.01362378, 2.12349749]), 'targetState': array([ 2, 25], dtype=int32), 'currentDistance': 16.04054045290962}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9091764020466204
{'scaleFactor': 20, 'currentTarget': array([ 2., 25.]), 'previousTarget': array([ 2., 25.]), 'currentState': array([ 1.46770765, 24.64779952,  1.22141978]), 'targetState': array([ 2, 25], dtype=int32), 'currentDistance': 0.6382635255221972}
episode index:596
target Thresh 31.933954253516106
target distance 22.0
model initialize at round 596
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 29.]), 'previousTarget': array([27., 29.]), 'currentState': array([ 6.89964919, 26.58247368,  0.0974189 ]), 'targetState': array([27, 29], dtype=int32), 'currentDistance': 20.245210199636603}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9091683713480646
{'scaleFactor': 20, 'currentTarget': array([27., 29.]), 'previousTarget': array([27., 29.]), 'currentState': array([26.50443827, 29.6613434 ,  6.02136405]), 'targetState': array([27, 29], dtype=int32), 'currentDistance': 0.8264118372552938}
episode index:597
target Thresh 31.93461141967378
target distance 2.0
model initialize at round 597
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  6.]), 'previousTarget': array([20.,  6.]), 'currentState': array([20.19723848,  6.14561961,  3.70825949]), 'targetState': array([20,  6], dtype=int32), 'currentDistance': 0.245169509672136}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9093202637036698
{'scaleFactor': 20, 'currentTarget': array([20.,  6.]), 'previousTarget': array([20.,  6.]), 'currentState': array([20.19723848,  6.14561961,  3.70825949]), 'targetState': array([20,  6], dtype=int32), 'currentDistance': 0.245169509672136}
episode index:598
target Thresh 31.935262046918933
target distance 19.0
model initialize at round 598
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 28.]), 'previousTarget': array([ 5., 28.]), 'currentState': array([25.0577689 , 17.30901224,  1.90115612]), 'targetState': array([ 5, 28], dtype=int32), 'currentDistance': 22.729085165207408}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9092525267238124
{'scaleFactor': 20, 'currentTarget': array([ 5., 28.]), 'previousTarget': array([ 5., 28.]), 'currentState': array([ 4.89106185, 28.28310683,  1.58488408]), 'targetState': array([ 5, 28], dtype=int32), 'currentDistance': 0.30334303870724233}
episode index:599
target Thresh 31.935906200314825
target distance 10.0
model initialize at round 599
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 17.]), 'previousTarget': array([ 3., 17.]), 'currentState': array([ 2.49760368, 25.06306138,  4.98631522]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 8.078697970554222}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9093380991959393
{'scaleFactor': 20, 'currentTarget': array([ 3., 17.]), 'previousTarget': array([ 3., 17.]), 'currentState': array([ 3.35105546, 17.26070829,  4.2221097 ]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 0.4372742245562741}
episode index:600
target Thresh 31.936543944277343
target distance 20.0
model initialize at round 600
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([21.61239392, 21.51774079,  4.98255885]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 20.308647688961038}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9092999074696835
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.80666442,  2.38189965,  3.42606883]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.8924992015751183}
episode index:601
target Thresh 31.937175342581405
target distance 6.0
model initialize at round 601
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([7.56371298, 5.95137504, 1.75375579]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 4.07206464112358}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9094175155968102
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([7.77675296, 9.85435967, 1.04374634]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.2665527114820368}
episode index:602
target Thresh 31.93780045836737
target distance 11.0
model initialize at round 602
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([ 4.62653387, 14.43217549,  5.53288812]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 9.383423839876286}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9094706874604986
{'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([14.344898  , 13.57495603,  5.82615136]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 0.5473728251111021}
episode index:603
target Thresh 31.938419354147342
target distance 13.0
model initialize at round 603
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([15.42487581, 19.10439015,  0.44884985]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 11.609720798926077}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9095236832584133
{'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([27.21774301, 20.04679222,  5.85155128]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 0.22271401173183097}
episode index:604
target Thresh 31.93903209181141
target distance 16.0
model initialize at round 604
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([3.8423523 , 6.77842729, 4.96385241]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 17.457474194698758}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9095302841910171
{'scaleFactor': 20, 'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([20.63684164, 10.47328636,  6.25685278]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 0.5965601190114529}
episode index:605
target Thresh 31.93963873263385
target distance 21.0
model initialize at round 605
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([15.18820782, 23.01695104,  4.96397382]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 19.054035386263564}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.909521788796327
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.83741789,  3.75962694,  3.47638224]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.2901933026668823}
episode index:606
target Thresh 31.940239337279255
target distance 17.0
model initialize at round 606
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([14.14581235, 24.76731912,  4.45128087]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 19.142145696802146}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9094836719642344
{'scaleFactor': 20, 'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([24.90752605,  8.33438553,  4.77580053]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 0.6720074833924793}
episode index:607
target Thresh 31.940833965808586
target distance 15.0
model initialize at round 607
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([18.65030581, 11.99465743,  4.7918005 ]), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 15.338059894758196}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9094903061344967
{'scaleFactor': 20, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.3624056 , 5.44790746, 3.28317979]), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.5761587539952354}
episode index:608
target Thresh 31.941422677685193
target distance 9.0
model initialize at round 608
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 1.37914528, 20.45301815,  3.87905014]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 12.142362981076085}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9095273751686059
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.4109142 , 10.65543867,  3.47319661]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.6824548278366555}
episode index:609
target Thresh 31.942005531780755
target distance 5.0
model initialize at round 609
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 18.]), 'previousTarget': array([18., 18.]), 'currentState': array([21.75226426, 12.87060034,  2.86725092]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 6.355330674003766}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9096270007830836
{'scaleFactor': 20, 'currentTarget': array([18., 18.]), 'previousTarget': array([18., 18.]), 'currentState': array([18.60057664, 17.75583177,  2.29023458]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 0.6483135255971377}
episode index:610
target Thresh 31.942582586381164
target distance 7.0
model initialize at round 610
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 19.]), 'previousTarget': array([21., 19.]), 'currentState': array([15.82010183, 24.79990484,  4.94547677]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 7.776261384761826}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9097104197834386
{'scaleFactor': 20, 'currentTarget': array([21., 19.]), 'previousTarget': array([21., 19.]), 'currentState': array([20.32934515, 19.72759448,  5.22903901]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 0.9895310237636709}
episode index:611
target Thresh 31.943153899192364
target distance 10.0
model initialize at round 611
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 17.]), 'previousTarget': array([17., 17.]), 'currentState': array([23.07701613, 25.70677754,  4.39682176]), 'targetState': array([17, 17], dtype=int32), 'currentDistance': 10.617819933591159}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9097778701594461
{'scaleFactor': 20, 'currentTarget': array([17., 17.]), 'previousTarget': array([17., 17.]), 'currentState': array([17.74397952, 17.36047573,  3.89764671]), 'targetState': array([17, 17], dtype=int32), 'currentDistance': 0.8267093051486115}
episode index:612
target Thresh 31.94371952734611
target distance 7.0
model initialize at round 612
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 28.]), 'previousTarget': array([ 5., 28.]), 'currentState': array([ 3.61681944, 22.56586138,  2.20554487]), 'targetState': array([ 5, 28], dtype=int32), 'currentDistance': 5.607410365212097}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9098765995719102
{'scaleFactor': 20, 'currentTarget': array([ 5., 28.]), 'previousTarget': array([ 5., 28.]), 'currentState': array([ 5.27430061, 27.37133469,  0.33160584]), 'targetState': array([ 5, 28], dtype=int32), 'currentDistance': 0.6859015161136165}
episode index:613
target Thresh 31.944279527405694
target distance 22.0
model initialize at round 613
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  5.]), 'previousTarget': array([26.,  5.]), 'currentState': array([12.9725585 , 25.66705137,  5.06570601]), 'targetState': array([26,  5], dtype=int32), 'currentDistance': 24.430334517348825}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.9097954623651828
{'scaleFactor': 20, 'currentTarget': array([26.,  5.]), 'previousTarget': array([26.,  5.]), 'currentState': array([25.42779287,  4.8952375 ,  0.06850689]), 'targetState': array([26,  5], dtype=int32), 'currentDistance': 0.5817183018876929}
episode index:614
target Thresh 31.944833955371582
target distance 7.0
model initialize at round 614
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 19.]), 'previousTarget': array([ 7., 19.]), 'currentState': array([ 3.13433504, 24.55673555,  5.18211913]), 'targetState': array([ 7, 19], dtype=int32), 'currentDistance': 6.769097098275686}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9098938421011743
{'scaleFactor': 20, 'currentTarget': array([ 7., 19.]), 'previousTarget': array([ 7., 19.]), 'currentState': array([ 6.8241842 , 19.84313206,  5.38054137]), 'targetState': array([ 7, 19], dtype=int32), 'currentDistance': 0.8612681699133464}
episode index:615
target Thresh 31.945382866687034
target distance 13.0
model initialize at round 615
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([14.52895592, 26.81756797,  3.64432728]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 14.86127545094925}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9099147038744321
{'scaleFactor': 20, 'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([19.8493483 , 12.98633659,  4.11189894]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 0.15127003457796584}
episode index:616
target Thresh 31.945926316243643
target distance 18.0
model initialize at round 616
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([24.86675489, 25.32231213,  5.64313209]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 19.608779539079336}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9098765680038352
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.46129769,  9.54682814,  2.15207541]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.7676074493105924}
episode index:617
target Thresh 31.946464358386812
target distance 7.0
model initialize at round 617
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 23.]), 'previousTarget': array([24., 23.]), 'currentState': array([25.38015494, 15.54339288,  2.4063468 ]), 'targetState': array([24, 23], dtype=int32), 'currentDistance': 7.5832590215018145}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9099586382983273
{'scaleFactor': 20, 'currentTarget': array([24., 23.]), 'previousTarget': array([24., 23.]), 'currentState': array([24.00134807, 23.28097629,  1.67803367]), 'targetState': array([24, 23], dtype=int32), 'currentDistance': 0.2809795195701043}
episode index:618
target Thresh 31.946997046921208
target distance 1.0
model initialize at round 618
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([21.22178111, 11.49322921,  3.05975211]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 0.5407974799469183}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9101041009182008
{'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([21.22178111, 11.49322921,  3.05975211]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 0.5407974799469183}
episode index:619
target Thresh 31.94752443511613
target distance 13.0
model initialize at round 619
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  2.]), 'previousTarget': array([13.,  2.]), 'currentState': array([21.45724805, 14.15808778,  4.74929339]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 14.810271537347063}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9101244889722487
{'scaleFactor': 20, 'currentTarget': array([13.,  2.]), 'previousTarget': array([13.,  2.]), 'currentState': array([13.48279549,  2.02583481,  3.66827909]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 0.4834862156276559}
episode index:620
target Thresh 31.94804657571083
target distance 7.0
model initialize at round 620
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 23.]), 'previousTarget': array([12., 23.]), 'currentState': array([ 6.02933454, 18.33148737,  1.92269235]), 'targetState': array([12, 23], dtype=int32), 'currentDistance': 7.57917253060667}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9102057635632758
{'scaleFactor': 20, 'currentTarget': array([12., 23.]), 'previousTarget': array([12., 23.]), 'currentState': array([11.03094066, 22.63996808,  0.36069157]), 'targetState': array([12, 23], dtype=int32), 'currentDistance': 1.0337789881607053}
episode index:621
target Thresh 31.948563520919805
target distance 1.0
model initialize at round 621
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 29.]), 'previousTarget': array([26., 29.]), 'currentState': array([25.65738787, 29.04375782,  3.4608624 ]), 'targetState': array([26, 29], dtype=int32), 'currentDistance': 0.34539516134643017}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9103501272874506
{'scaleFactor': 20, 'currentTarget': array([26., 29.]), 'previousTarget': array([26., 29.]), 'currentState': array([25.65738787, 29.04375782,  3.4608624 ]), 'targetState': array([26, 29], dtype=int32), 'currentDistance': 0.34539516134643017}
episode index:622
target Thresh 31.949075322438013
target distance 23.0
model initialize at round 622
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([24.25992464,  7.40190678,  2.09152639]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 21.41808777505339}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9102974320959762
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.01383453, 10.10662589,  2.19152211]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.10751964625285111}
episode index:623
target Thresh 31.949582031446024
target distance 19.0
model initialize at round 623
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 5.]), 'previousTarget': array([7., 5.]), 'currentState': array([15.04528665, 22.61340886,  3.10026002]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 19.363853156211988}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9102591106851111
{'scaleFactor': 20, 'currentTarget': array([7., 5.]), 'previousTarget': array([7., 5.]), 'currentState': array([6.58180073, 4.32885618, 3.70587349]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 0.7907747141548884}
episode index:624
target Thresh 31.950083698615163
target distance 15.0
model initialize at round 624
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 26.]), 'previousTarget': array([20., 26.]), 'currentState': array([ 6.22856372, 16.84977425,  0.2575218 ]), 'targetState': array([20, 26], dtype=int32), 'currentDistance': 16.534179399942126}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9102643237039889
{'scaleFactor': 20, 'currentTarget': array([20., 26.]), 'previousTarget': array([20., 26.]), 'currentState': array([19.74801221, 26.11820757,  0.06954126]), 'targetState': array([20, 26], dtype=int32), 'currentDistance': 0.2783359026819065}
episode index:625
target Thresh 31.95058037411257
target distance 10.0
model initialize at round 625
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 23.]), 'previousTarget': array([15., 23.]), 'currentState': array([23.73876752, 12.88569272,  2.85522032]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 13.36657284610406}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9102991496212461
{'scaleFactor': 20, 'currentTarget': array([15., 23.]), 'previousTarget': array([15., 23.]), 'currentState': array([15.00644547, 23.3678998 ,  1.71785274]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 0.36795625602525317}
episode index:626
target Thresh 31.951072107606205
target distance 24.0
model initialize at round 626
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  7.]), 'previousTarget': array([27.,  7.]), 'currentState': array([4.94524879, 5.45459203, 0.14493704]), 'targetState': array([27,  7], dtype=int32), 'currentDistance': 22.10882937976558}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.910275288544113
{'scaleFactor': 20, 'currentTarget': array([27.,  7.]), 'previousTarget': array([27.,  7.]), 'currentState': array([26.45722355,  7.36664166,  5.81179522]), 'targetState': array([27,  7], dtype=int32), 'currentDistance': 0.65500563400452}
episode index:627
target Thresh 31.951558948269824
target distance 16.0
model initialize at round 627
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  4.]), 'previousTarget': array([26.,  4.]), 'currentState': array([23.55337276, 18.41060808,  6.05742931]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 14.616826266862432}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9102804508991122
{'scaleFactor': 20, 'currentTarget': array([26.,  4.]), 'previousTarget': array([26.,  4.]), 'currentState': array([25.87051642,  3.69813157,  4.4758894 ]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 0.32846696308772594}
episode index:628
target Thresh 31.952040944787907
target distance 11.0
model initialize at round 628
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 8.]), 'previousTarget': array([8., 8.]), 'currentState': array([17.41723781,  9.42794061,  4.49841452]), 'targetState': array([8, 8], dtype=int32), 'currentDistance': 9.52488227896935}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9103300529635031
{'scaleFactor': 20, 'currentTarget': array([8., 8.]), 'previousTarget': array([8., 8.]), 'currentState': array([7.92827919, 8.19371762, 2.42732136]), 'targetState': array([8, 8], dtype=int32), 'currentDistance': 0.20656812790567816}
episode index:629
target Thresh 31.9525181453605
target distance 17.0
model initialize at round 629
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 21.]), 'previousTarget': array([23., 21.]), 'currentState': array([7.13585951, 5.6380933 , 0.82199785]), 'targetState': array([23, 21], dtype=int32), 'currentDistance': 22.08300548028768}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9103062564576225
{'scaleFactor': 20, 'currentTarget': array([23., 21.]), 'previousTarget': array([23., 21.]), 'currentState': array([22.65899263, 20.94185284,  0.41235654]), 'targetState': array([23, 21], dtype=int32), 'currentDistance': 0.345929355185837}
episode index:630
target Thresh 31.95299059770806
target distance 10.0
model initialize at round 630
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([ 7.91397474, 14.41990789,  6.00108174]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 10.32468022874627}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.910370731566089
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.83362346,  8.82667127,  5.21980286]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.8432476205418626}
episode index:631
target Thresh 31.953458349076218
target distance 20.0
model initialize at round 631
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([ 4.67903575, 17.20058912,  1.1154074 ]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 23.1800444879592}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9103048693528026
{'scaleFactor': 20, 'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([23.22118754,  2.89253421,  4.75889915]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 0.24591222500223345}
episode index:632
target Thresh 31.953921446240496
target distance 16.0
model initialize at round 632
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 28.]), 'previousTarget': array([ 8., 28.]), 'currentState': array([23.96386954, 19.68258214,  2.60226625]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 18.000682495961623}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9103099441997708
{'scaleFactor': 20, 'currentTarget': array([ 8., 28.]), 'previousTarget': array([ 8., 28.]), 'currentState': array([ 8.34599815, 27.34466736,  2.16191512]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 0.7410638191106808}
episode index:633
target Thresh 31.954379935511003
target distance 10.0
model initialize at round 633
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 21.]), 'previousTarget': array([ 3., 21.]), 'currentState': array([11.99869636, 20.64730673,  3.06517029]), 'targetState': array([ 3, 21], dtype=int32), 'currentDistance': 9.005605401050907}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9103741084043454
{'scaleFactor': 20, 'currentTarget': array([ 3., 21.]), 'previousTarget': array([ 3., 21.]), 'currentState': array([ 2.47414171, 21.94014771,  1.88694306]), 'targetState': array([ 3, 21], dtype=int32), 'currentDistance': 1.077220802126034}
episode index:634
target Thresh 31.95483386273704
target distance 13.0
model initialize at round 634
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 12.]), 'previousTarget': array([16., 12.]), 'currentState': array([10.77582057, 23.50651445,  4.18149507]), 'targetState': array([16, 12], dtype=int32), 'currentDistance': 12.636927053741045}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9104082678366331
{'scaleFactor': 20, 'currentTarget': array([16., 12.]), 'previousTarget': array([16., 12.]), 'currentState': array([16.08371822, 12.98536873,  6.17067254]), 'targetState': array([16, 12], dtype=int32), 'currentDistance': 0.9889187365103997}
episode index:635
target Thresh 31.955283273311714
target distance 23.0
model initialize at round 635
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([10.43391464, 24.37392837,  5.98315966]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 22.32129188544666}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9103565583321713
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.40816628, 2.81093544, 3.7685703 ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.449827880067259}
episode index:636
target Thresh 31.955728212176457
target distance 2.0
model initialize at round 636
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 14.]), 'previousTarget': array([25., 14.]), 'currentState': array([24.4762674 , 14.14171138,  4.78862915]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 0.5425660814341885}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9104972858701115
{'scaleFactor': 20, 'currentTarget': array([25., 14.]), 'previousTarget': array([25., 14.]), 'currentState': array([24.4762674 , 14.14171138,  4.78862915]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 0.5425660814341885}
episode index:637
target Thresh 31.956168723825524
target distance 9.0
model initialize at round 637
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 17.]), 'previousTarget': array([18., 17.]), 'currentState': array([25.39909156,  9.48088514,  2.44515753]), 'targetState': array([18, 17], dtype=int32), 'currentDistance': 10.54910632178393}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.91056075415229
{'scaleFactor': 20, 'currentTarget': array([18., 17.]), 'previousTarget': array([18., 17.]), 'currentState': array([18.04602371, 16.15253778,  2.17307998]), 'targetState': array([18, 17], dtype=int32), 'currentDistance': 0.8487110160604111}
episode index:638
target Thresh 31.956604852310445
target distance 6.0
model initialize at round 638
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([ 3.4357287 , 13.88331692,  1.5569728 ]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 4.565762523202497}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9106542412349937
{'scaleFactor': 20, 'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([ 8.62575423, 13.6631905 ,  5.57133874]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 0.7106398520902845}
episode index:639
target Thresh 31.957036641244436
target distance 21.0
model initialize at round 639
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  6.]), 'previousTarget': array([23.,  6.]), 'currentState': array([ 3.68051813, 20.09120337,  1.06418234]), 'targetState': array([23,  6], dtype=int32), 'currentDistance': 23.912431746176438}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9106024705814999
{'scaleFactor': 20, 'currentTarget': array([23.,  6.]), 'previousTarget': array([23.,  6.]), 'currentState': array([22.85192025,  6.92927186,  5.04772262]), 'targetState': array([23,  6], dtype=int32), 'currentDistance': 0.9409961756003139}
episode index:640
target Thresh 31.95746413380675
target distance 15.0
model initialize at round 640
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 27.]), 'previousTarget': array([ 2., 27.]), 'currentState': array([10.94022169, 13.3958407 ,  1.98801726]), 'targetState': array([ 2, 27], dtype=int32), 'currentDistance': 16.27884252593666}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9106214132084054
{'scaleFactor': 20, 'currentTarget': array([ 2., 27.]), 'previousTarget': array([ 2., 27.]), 'currentState': array([ 2.00205842, 26.45973838,  1.78885391]), 'targetState': array([ 2, 27], dtype=int32), 'currentDistance': 0.5402655370760321}
episode index:641
target Thresh 31.957887372746992
target distance 9.0
model initialize at round 641
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 23.]), 'previousTarget': array([ 6., 23.]), 'currentState': array([14.60794474, 15.56932829,  2.21120621]), 'targetState': array([ 6, 23], dtype=int32), 'currentDistance': 11.371525607237372}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9106694797756835
{'scaleFactor': 20, 'currentTarget': array([ 6., 23.]), 'previousTarget': array([ 6., 23.]), 'currentState': array([ 5.97172174, 23.67590009,  2.07203781]), 'targetState': array([ 6, 23], dtype=int32), 'currentDistance': 0.6764913823152303}
episode index:642
target Thresh 31.95830640038942
target distance 6.0
model initialize at round 642
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 15.]), 'previousTarget': array([25., 15.]), 'currentState': array([18.85588484, 13.67678827,  0.64653277]), 'targetState': array([25, 15], dtype=int32), 'currentDistance': 6.284985312376727}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9107622161990494
{'scaleFactor': 20, 'currentTarget': array([25., 15.]), 'previousTarget': array([25., 15.]), 'currentState': array([24.6473003 , 14.82134914,  6.27820929]), 'targetState': array([25, 15], dtype=int32), 'currentDistance': 0.39536465036015245}
episode index:643
target Thresh 31.95872125863714
target distance 7.0
model initialize at round 643
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  4.]), 'previousTarget': array([27.,  4.]), 'currentState': array([21.93035623,  6.69728762,  5.7508951 ]), 'targetState': array([27,  4], dtype=int32), 'currentDistance': 5.742529794600856}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9108546646211005
{'scaleFactor': 20, 'currentTarget': array([27.,  4.]), 'previousTarget': array([27.,  4.]), 'currentState': array([27.08896282,  3.70115228,  5.48682929]), 'targetState': array([27,  4], dtype=int32), 'currentDistance': 0.3118081795073633}
episode index:644
target Thresh 31.959131988976328
target distance 16.0
model initialize at round 644
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 12.]), 'previousTarget': array([20., 12.]), 'currentState': array([ 4.29305031, 20.34274033,  5.89740801]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 17.78509445673628}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9108587926565463
{'scaleFactor': 20, 'currentTarget': array([20., 12.]), 'previousTarget': array([20., 12.]), 'currentState': array([19.82839363, 12.10655615,  5.32727384]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 0.2019974292779866}
episode index:645
target Thresh 31.959538632480353
target distance 10.0
model initialize at round 645
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 16.]), 'previousTarget': array([20., 16.]), 'currentState': array([12.4694214 , 24.65259964,  5.68042547]), 'targetState': array([20, 16], dtype=int32), 'currentDistance': 11.470705932038966}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9109061941375749
{'scaleFactor': 20, 'currentTarget': array([20., 16.]), 'previousTarget': array([20., 16.]), 'currentState': array([20.1376077 , 15.8414215 ,  4.91980169]), 'targetState': array([20, 16], dtype=int32), 'currentDistance': 0.20995956126465024}
episode index:646
target Thresh 31.95994122981391
target distance 22.0
model initialize at round 646
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 16.]), 'previousTarget': array([ 5., 16.]), 'currentState': array([27.26721816, 30.66162046,  2.42134377]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 26.660684890680646}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.9108143109489462
{'scaleFactor': 20, 'currentTarget': array([ 5., 16.]), 'previousTarget': array([ 5., 16.]), 'currentState': array([ 5.98969546, 15.74887473,  3.08343562]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 1.0210587640192783}
episode index:647
target Thresh 31.960339821237064
target distance 7.0
model initialize at round 647
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  7.]), 'previousTarget': array([22.,  7.]), 'currentState': array([16.36057252, 10.00942921,  0.38069266]), 'targetState': array([22,  7], dtype=int32), 'currentDistance': 6.3921675842401555}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9108911345585929
{'scaleFactor': 20, 'currentTarget': array([22.,  7.]), 'previousTarget': array([22.,  7.]), 'currentState': array([22.37082993,  6.85257122,  0.52192229]), 'targetState': array([22,  7], dtype=int32), 'currentDistance': 0.3990615012506011}
episode index:648
target Thresh 31.960734446609294
target distance 18.0
model initialize at round 648
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 26.]), 'previousTarget': array([ 9., 26.]), 'currentState': array([11.16406966,  9.6749535 ,  2.48315307]), 'targetState': array([ 9, 26], dtype=int32), 'currentDistance': 16.467857805426597}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9108951809575531
{'scaleFactor': 20, 'currentTarget': array([ 9., 26.]), 'previousTarget': array([ 9., 26.]), 'currentState': array([ 8.93734246, 25.65406244,  1.33239041]), 'targetState': array([ 9, 26], dtype=int32), 'currentDistance': 0.3515661531829926}
episode index:649
target Thresh 31.961125145393463
target distance 17.0
model initialize at round 649
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([23.26225146,  9.01510786,  3.56875063]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 16.40481155225768}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9109134109782767
{'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([8.70386571, 2.75569996, 3.16419848]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 0.7450566793133381}
episode index:650
target Thresh 31.961511956659773
target distance 7.0
model initialize at round 650
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([21.31202784, 18.53593046,  3.0019182 ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 9.830154797306129}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9109603644935189
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.849196  , 10.75706432,  3.06036162]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.28593634019077485}
episode index:651
target Thresh 31.96189491908968
target distance 10.0
model initialize at round 651
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 24.]), 'previousTarget': array([13., 24.]), 'currentState': array([13.27892965, 15.52074602,  3.02344453]), 'targetState': array([13, 24], dtype=int32), 'currentDistance': 8.483840513914865}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.911021759716535
{'scaleFactor': 20, 'currentTarget': array([13., 24.]), 'previousTarget': array([13., 24.]), 'currentState': array([12.75374878, 23.86242675,  0.47340381]), 'targetState': array([13, 24], dtype=int32), 'currentDistance': 0.28207456832384903}
episode index:652
target Thresh 31.96227407097974
target distance 9.0
model initialize at round 652
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 26.]), 'previousTarget': array([23., 26.]), 'currentState': array([16.70997169, 18.52588611,  2.14530441]), 'targetState': array([23, 26], dtype=int32), 'currentDistance': 9.7686659619517}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9110684034985939
{'scaleFactor': 20, 'currentTarget': array([23., 26.]), 'previousTarget': array([23., 26.]), 'currentState': array([23.70036756, 25.45835993,  5.48991272]), 'targetState': array([23, 26], dtype=int32), 'currentDistance': 0.8853748788768715}
episode index:653
target Thresh 31.962649450245458
target distance 12.0
model initialize at round 653
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 12.]), 'previousTarget': array([16., 12.]), 'currentState': array([4.56838489, 6.41591455, 6.06689024]), 'targetState': array([16, 12], dtype=int32), 'currentDistance': 12.722571841558088}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9111005089181786
{'scaleFactor': 20, 'currentTarget': array([16., 12.]), 'previousTarget': array([16., 12.]), 'currentState': array([15.2643082 , 11.87960812,  0.2339417 ]), 'targetState': array([16, 12], dtype=int32), 'currentDistance': 0.7454774484569934}
episode index:654
target Thresh 31.963021094425073
target distance 4.0
model initialize at round 654
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 15.]), 'previousTarget': array([ 5., 15.]), 'currentState': array([ 9.66952791, 10.78769604,  0.88351065]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 6.28871971176813}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.911190888293876
{'scaleFactor': 20, 'currentTarget': array([ 5., 15.]), 'previousTarget': array([ 5., 15.]), 'currentState': array([ 5.64455037, 14.2466928 ,  2.5780229 ]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 0.9914216625855274}
episode index:655
target Thresh 31.963389040683317
target distance 19.0
model initialize at round 655
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 22.]), 'previousTarget': array([ 7., 22.]), 'currentState': array([16.93821663,  4.39718919,  1.98945301]), 'targetState': array([ 7, 22], dtype=int32), 'currentDistance': 20.21452690746132}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9111667226932126
{'scaleFactor': 20, 'currentTarget': array([ 7., 22.]), 'previousTarget': array([ 7., 22.]), 'currentState': array([ 7.71671006, 22.49548575,  0.84703867]), 'targetState': array([ 7, 22], dtype=int32), 'currentDistance': 0.8713090345971543}
episode index:656
target Thresh 31.963753325815123
target distance 12.0
model initialize at round 656
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 17.]), 'previousTarget': array([ 9., 17.]), 'currentState': array([6.98311848, 6.68288535, 2.59082729]), 'targetState': array([ 9, 17], dtype=int32), 'currentDistance': 10.512405328193735}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9112128618510631
{'scaleFactor': 20, 'currentTarget': array([ 9., 17.]), 'previousTarget': array([ 9., 17.]), 'currentState': array([ 8.41407957, 16.41267862,  0.88516113]), 'targetState': array([ 9, 17], dtype=int32), 'currentDistance': 0.8296078351917668}
episode index:657
target Thresh 31.964113986249302
target distance 12.0
model initialize at round 657
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([17.02165954,  8.22222321,  3.19781333]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 10.399504975503799}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9112733135046329
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.54880614, 10.17479388,  2.45303058]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.9910364878491017}
episode index:658
target Thresh 31.9644710580522
target distance 8.0
model initialize at round 658
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 26.]), 'previousTarget': array([19., 26.]), 'currentState': array([23.42073473, 17.41835639,  2.48447657]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 9.653367417433625}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9113335816933968
{'scaleFactor': 20, 'currentTarget': array([19., 26.]), 'previousTarget': array([19., 26.]), 'currentState': array([19.03208457, 26.12485957,  1.71440881]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 0.12891598924183578}
episode index:659
target Thresh 31.964824576931296
target distance 23.0
model initialize at round 659
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([26.10488122,  9.57481564,  3.14158344]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 22.254335521869763}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9113093463487989
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.42785565, 6.61826267, 2.98070412]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.5733967590718854}
episode index:660
target Thresh 31.965174578238774
target distance 6.0
model initialize at round 660
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([7.58716384, 4.38026309, 2.64413422]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 5.843335300671245}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9113985893951697
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.83966306, 7.93764056, 2.18468169]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.17203672247690863}
episode index:661
target Thresh 31.965521096975053
target distance 17.0
model initialize at round 661
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([6.8976307 , 4.67985375, 0.62166047]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 18.603106643882942}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9114017897850315
{'scaleFactor': 20, 'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([23.31193112, 11.72077372,  0.29091187]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 0.7425672294818809}
episode index:662
target Thresh 31.965864167792297
target distance 20.0
model initialize at round 662
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 29.]), 'previousTarget': array([27., 29.]), 'currentState': array([ 7.87221079, 17.56068196,  6.26720047]), 'targetState': array([27, 29], dtype=int32), 'currentDistance': 22.287447524592796}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.911337451961478
{'scaleFactor': 20, 'currentTarget': array([27., 29.]), 'previousTarget': array([27., 29.]), 'currentState': array([26.46446807, 29.78282101,  5.51668894]), 'targetState': array([27, 29], dtype=int32), 'currentDistance': 0.9484741378125475}
episode index:663
target Thresh 31.966203824997876
target distance 17.0
model initialize at round 663
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([19.75717941,  2.86519382,  2.87159252]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 15.757756043855839}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9113269769961878
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.72214296, 2.8082058 , 1.53184509]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.3376233806847968}
episode index:664
target Thresh 31.96654010255779
target distance 8.0
model initialize at round 664
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 23.]), 'previousTarget': array([ 2., 23.]), 'currentState': array([10.19448641, 24.32830531,  3.81820917]), 'targetState': array([ 2, 23], dtype=int32), 'currentDistance': 8.301445810164573}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9114010657676221
{'scaleFactor': 20, 'currentTarget': array([ 2., 23.]), 'previousTarget': array([ 2., 23.]), 'currentState': array([ 2.43591939, 23.01674701,  3.11372886]), 'targetState': array([ 2, 23], dtype=int32), 'currentDistance': 0.43624096500348164}
episode index:665
target Thresh 31.966873034100075
target distance 22.0
model initialize at round 665
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([23.80977542, 19.81014559,  4.93683529]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 24.975325303370294}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.9113239746097749
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.95992576, 6.32109544, 3.04880325]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.32358650948649814}
episode index:666
target Thresh 31.967202652918164
target distance 5.0
model initialize at round 666
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([15.14685137, 16.19110003,  4.26794273]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 3.390927228613015}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9114270870916192
{'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([13.43346218, 12.58891272,  4.45276814]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 0.6999698929221767}
episode index:667
target Thresh 31.967528991974216
target distance 15.0
model initialize at round 667
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  7.]), 'previousTarget': array([17.,  7.]), 'currentState': array([ 0.65821158, 14.98412997,  4.79962325]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 18.18791852590469}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9114030020125281
{'scaleFactor': 20, 'currentTarget': array([17.,  7.]), 'previousTarget': array([17.,  7.]), 'currentState': array([17.36205069,  6.92736828,  0.5759738 ]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 0.3692642258711081}
episode index:668
target Thresh 31.967852083902404
target distance 7.0
model initialize at round 668
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 3.]), 'previousTarget': array([9., 3.]), 'currentState': array([14.36619207,  8.59619343,  4.39389348]), 'targetState': array([9, 3], dtype=int32), 'currentDistance': 7.753283065755942}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9114765341619862
{'scaleFactor': 20, 'currentTarget': array([9., 3.]), 'previousTarget': array([9., 3.]), 'currentState': array([8.72221059, 3.19368108, 3.73251493]), 'targetState': array([9, 3], dtype=int32), 'currentDistance': 0.33864335253725314}
episode index:669
target Thresh 31.968171961012192
target distance 12.0
model initialize at round 669
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'previousTarget': array([16., 23.]), 'currentState': array([ 5.45444622, 16.84843689,  5.80269558]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 12.208621266354537}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.91150726373474
{'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'previousTarget': array([16., 23.]), 'currentState': array([15.5636857 , 22.92796565,  1.56998873]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 0.4422206658078704}
episode index:670
target Thresh 31.96848865529156
target distance 23.0
model initialize at round 670
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([25.23116252,  8.33298111,  3.84017849]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 24.07890283242489}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9114698235081847
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.31247868, 2.44415289, 3.06194227]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.5430605090859657}
episode index:671
target Thresh 31.968802198410195
target distance 18.0
model initialize at round 671
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 27.]), 'previousTarget': array([ 3., 27.]), 'currentState': array([ 5.73645959, 10.66252102,  0.71852577]), 'targetState': array([ 3, 27], dtype=int32), 'currentDistance': 16.565066571957416}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.911459276263394
{'scaleFactor': 20, 'currentTarget': array([ 3., 27.]), 'previousTarget': array([ 3., 27.]), 'currentState': array([ 2.89679713, 27.26763365,  0.78845346]), 'targetState': array([ 3, 27], dtype=int32), 'currentDistance': 0.2868424689402364}
episode index:672
target Thresh 31.969112621722672
target distance 9.0
model initialize at round 672
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 27.]), 'previousTarget': array([14., 27.]), 'currentState': array([24.62337885, 17.55540552,  0.7423684 ]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 14.214659443059489}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9114760450868182
{'scaleFactor': 20, 'currentTarget': array([14., 27.]), 'previousTarget': array([14., 27.]), 'currentState': array([13.82999523, 27.61234383,  2.69613856]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 0.635504987599976}
episode index:673
target Thresh 31.969419956271587
target distance 25.0
model initialize at round 673
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 27.]), 'previousTarget': array([19., 27.]), 'currentState': array([22.98845568,  3.36210993,  1.95304268]), 'targetState': array([19, 27], dtype=int32), 'currentDistance': 23.972017556619438}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9114388178266244
{'scaleFactor': 20, 'currentTarget': array([19., 27.]), 'previousTarget': array([19., 27.]), 'currentState': array([19.17732212, 26.24502791,  0.92351326]), 'targetState': array([19, 27], dtype=int32), 'currentDistance': 0.7755165980216976}
episode index:674
target Thresh 31.969724232790643
target distance 8.0
model initialize at round 674
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([21.63111139,  9.41456448,  1.2588908 ]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 8.299894125380796}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9115116432965109
{'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([25.28568278, 16.3101417 ,  0.66036418]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 0.7466720333045209}
episode index:675
target Thresh 31.970025481707747
target distance 12.0
model initialize at round 675
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 16.]), 'previousTarget': array([10., 16.]), 'currentState': array([21.45507766, 16.59230893,  2.91052479]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 11.470380724650402}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9115559754061328
{'scaleFactor': 20, 'currentTarget': array([10., 16.]), 'previousTarget': array([10., 16.]), 'currentState': array([ 9.74113999, 16.51604396,  3.04874093]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 0.5773299453950449}
episode index:676
target Thresh 31.970323733148042
target distance 15.0
model initialize at round 676
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 18.]), 'previousTarget': array([26., 18.]), 'currentState': array([24.4549496 ,  3.66723861,  1.72392964]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 14.415797927875415}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9115862699002257
{'scaleFactor': 20, 'currentTarget': array([26., 18.]), 'previousTarget': array([26., 18.]), 'currentState': array([25.9502487 , 17.05919605,  0.80828386]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 0.9421184973033816}
episode index:677
target Thresh 31.970619016936922
target distance 16.0
model initialize at round 677
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([11.35073755, 25.66485605,  4.35207129]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 16.340237336037482}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9115756442440437
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.5237246 ,  9.6517521 ,  4.57979752]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.5900125847566716}
episode index:678
target Thresh 31.970911362603015
target distance 12.0
model initialize at round 678
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 18.]), 'previousTarget': array([10., 18.]), 'currentState': array([20.75847202, 23.86377958,  2.87273121]), 'targetState': array([10, 18], dtype=int32), 'currentDistance': 12.252698934383499}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9116058205380982
{'scaleFactor': 20, 'currentTarget': array([10., 18.]), 'previousTarget': array([10., 18.]), 'currentState': array([10.81843641, 17.71775171,  4.0832237 ]), 'targetState': array([10, 18], dtype=int32), 'currentDistance': 0.8657379864089393}
episode index:679
target Thresh 31.971200799381123
target distance 6.0
model initialize at round 679
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'previousTarget': array([11., 21.]), 'currentState': array([ 6.91111227, 22.4147516 ,  6.04871211]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 4.32672219673922}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9117065472726009
{'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'previousTarget': array([11., 21.]), 'currentState': array([10.52295953, 20.76435854,  5.63191327]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 0.5320662585369471}
episode index:680
target Thresh 31.971487356215174
target distance 13.0
model initialize at round 680
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([10.02523061, 21.2827649 ,  5.25272564]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 11.66833478897265}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9117502676868864
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([12.54943696,  9.90696449,  4.64388865]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.4600681029909852}
episode index:681
target Thresh 31.971771061761082
target distance 14.0
model initialize at round 681
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([14.34193587, 17.71153607,  4.32384491]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 13.811422733122738}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9117663885472105
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.66983539,  4.38839184,  4.19842052]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.509761599599194}
episode index:682
target Thresh 31.972051944389644
target distance 16.0
model initialize at round 682
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  6.]), 'previousTarget': array([25.,  6.]), 'currentState': array([21.61977872, 22.45685549,  5.54809815]), 'targetState': array([25,  6], dtype=int32), 'currentDistance': 16.800416322667083}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.91176895203028
{'scaleFactor': 20, 'currentTarget': array([25.,  6.]), 'previousTarget': array([25.,  6.]), 'currentState': array([25.14966087,  5.20484276,  5.53527615]), 'targetState': array([25,  6], dtype=int32), 'currentDistance': 0.8091189147184348}
episode index:683
target Thresh 31.972330032189355
target distance 20.0
model initialize at round 683
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 29.]), 'previousTarget': array([ 6., 29.]), 'currentState': array([24.33653526, 14.25548607,  1.97919679]), 'targetState': array([ 6, 29], dtype=int32), 'currentDistance': 23.529326729493594}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9117060527038746
{'scaleFactor': 20, 'currentTarget': array([ 6., 29.]), 'previousTarget': array([ 6., 29.]), 'currentState': array([ 6.37831919, 29.11417931,  1.62633976]), 'targetState': array([ 6, 29], dtype=int32), 'currentDistance': 0.3951737868475733}
episode index:684
target Thresh 31.972605352969225
target distance 8.0
model initialize at round 684
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 24.]), 'previousTarget': array([13., 24.]), 'currentState': array([19.31805062, 16.05860358,  2.09676409]), 'targetState': array([13, 24], dtype=int32), 'currentDistance': 10.14808064318157}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.911763401604891
{'scaleFactor': 20, 'currentTarget': array([13., 24.]), 'previousTarget': array([13., 24.]), 'currentState': array([13.57048522, 23.97068307,  1.81444243]), 'targetState': array([13, 24], dtype=int32), 'currentDistance': 0.5712380142491496}
episode index:685
target Thresh 31.972877934261565
target distance 9.0
model initialize at round 685
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 17.]), 'previousTarget': array([11., 17.]), 'currentState': array([18.32086419, 15.88632682,  2.19920218]), 'targetState': array([11, 17], dtype=int32), 'currentDistance': 7.405087471121542}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9118205833079451
{'scaleFactor': 20, 'currentTarget': array([11., 17.]), 'previousTarget': array([11., 17.]), 'currentState': array([11.62014883, 17.88443137,  1.04773057]), 'targetState': array([11, 17], dtype=int32), 'currentDistance': 1.0801867560516385}
episode index:686
target Thresh 31.97314780332473
target distance 8.0
model initialize at round 686
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([3.87690811, 1.56353898, 6.27046728]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 10.297542935282207}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9118637558932333
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([11.26715573,  8.59802188,  0.22189232]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.4826578414600621}
episode index:687
target Thresh 31.973414987145848
target distance 3.0
model initialize at round 687
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 22.]), 'previousTarget': array([22., 22.]), 'currentState': array([20.14421473, 20.55962087,  0.48729783]), 'targetState': array([22, 22], dtype=int32), 'currentDistance': 2.3491766636933202}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9119773260154815
{'scaleFactor': 20, 'currentTarget': array([22., 22.]), 'previousTarget': array([22., 22.]), 'currentState': array([21.97899831, 21.35003577,  0.32461229]), 'targetState': array([22, 22], dtype=int32), 'currentDistance': 0.6503034505350864}
episode index:688
target Thresh 31.97367951244353
target distance 9.0
model initialize at round 688
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 16.]), 'previousTarget': array([13., 16.]), 'currentState': array([ 4.77686761, 12.50706169,  6.20219278]), 'targetState': array([13, 16], dtype=int32), 'currentDistance': 8.934233279658478}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9120339482562428
{'scaleFactor': 20, 'currentTarget': array([13., 16.]), 'previousTarget': array([13., 16.]), 'currentState': array([12.95442078, 15.56414977,  6.23611176]), 'targetState': array([13, 16], dtype=int32), 'currentDistance': 0.4382269847895363}
episode index:689
target Thresh 31.973941405670516
target distance 10.0
model initialize at round 689
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 13.]), 'previousTarget': array([ 4., 13.]), 'currentState': array([ 8.42460533, 22.40795293,  4.51107216]), 'targetState': array([ 4, 13], dtype=int32), 'currentDistance': 10.396475879932803}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9120904063745672
{'scaleFactor': 20, 'currentTarget': array([ 4., 13.]), 'previousTarget': array([ 4., 13.]), 'currentState': array([ 3.90601298, 13.75728707,  3.76686453]), 'targetState': array([ 4, 13], dtype=int32), 'currentDistance': 0.7630971496555446}
episode index:690
target Thresh 31.974200693016357
target distance 12.0
model initialize at round 690
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 17.]), 'previousTarget': array([19., 17.]), 'currentState': array([21.19039562, 27.07099937,  4.37444526]), 'targetState': array([19, 17], dtype=int32), 'currentDistance': 10.306447558961699}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9121467010829977
{'scaleFactor': 20, 'currentTarget': array([19., 17.]), 'previousTarget': array([19., 17.]), 'currentState': array([18.73811428, 17.54856959,  4.03228684]), 'targetState': array([19, 17], dtype=int32), 'currentDistance': 0.6078755890001754}
episode index:691
target Thresh 31.974457400410003
target distance 15.0
model initialize at round 691
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([ 5.24747712, 27.48302165,  5.06791711]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 14.296095504998508}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9121754852547088
{'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([ 9.52858651, 14.38265141,  4.56682927]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 0.6071678306092784}
episode index:692
target Thresh 31.9747115535224
target distance 13.0
model initialize at round 692
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 15.]), 'previousTarget': array([ 9., 15.]), 'currentState': array([6.19121621, 3.78367474, 0.6569823 ]), 'targetState': array([ 9, 15], dtype=int32), 'currentDistance': 11.562664866204832}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9122041863552172
{'scaleFactor': 20, 'currentTarget': array([ 9., 15.]), 'previousTarget': array([ 9., 15.]), 'currentState': array([ 9.56560513, 15.02849619,  0.94383526]), 'targetState': array([ 9, 15], dtype=int32), 'currentDistance': 0.5663225229210376}
episode index:693
target Thresh 31.974963177769077
target distance 10.0
model initialize at round 693
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([16.06967334,  8.29930251,  6.15521342]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 9.024351567657456}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9122600737666651
{'scaleFactor': 20, 'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([25.78646455,  6.33741698,  0.05696822]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 1.0283689727617404}
episode index:694
target Thresh 31.97521229831267
target distance 17.0
model initialize at round 694
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([4.89928886, 4.67995397, 0.62067342]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 18.231217826784604}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9122618826497112
{'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([21.06375844, 10.82155149,  5.85975763]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 0.9530960701912655}
episode index:695
target Thresh 31.975458940065433
target distance 6.0
model initialize at round 695
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'previousTarget': array([11., 19.]), 'currentState': array([15.78679108, 16.16640996,  3.38585854]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 5.562607421563933}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9123452693125707
{'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'previousTarget': array([11., 19.]), 'currentState': array([11.06092765, 19.42999844,  2.50990359]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 0.4342934929354892}
episode index:696
target Thresh 31.975703127691755
target distance 11.0
model initialize at round 696
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 27.]), 'previousTarget': array([13., 27.]), 'currentState': array([22.29544775, 27.8291202 ,  2.28308606]), 'targetState': array([13, 27], dtype=int32), 'currentDistance': 9.332351752550355}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9124007137610463
{'scaleFactor': 20, 'currentTarget': array([13., 27.]), 'previousTarget': array([13., 27.]), 'currentState': array([13.90520137, 26.75096436,  4.13066375]), 'targetState': array([13, 27], dtype=int32), 'currentDistance': 0.938833463174579}
episode index:697
target Thresh 31.975944885610595
target distance 10.0
model initialize at round 697
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 13.]), 'previousTarget': array([16., 13.]), 'currentState': array([14.69678118, 24.06489848,  3.46649384]), 'targetState': array([16, 13], dtype=int32), 'currentDistance': 11.141380425123677}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9124423748436249
{'scaleFactor': 20, 'currentTarget': array([16., 13.]), 'previousTarget': array([16., 13.]), 'currentState': array([15.59618358, 12.88403925,  4.44972675]), 'targetState': array([16, 13], dtype=int32), 'currentDistance': 0.420136403854713}
episode index:698
target Thresh 31.976184237997952
target distance 8.0
model initialize at round 698
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 26.]), 'previousTarget': array([13., 26.]), 'currentState': array([ 6.68049185, 22.09129749,  1.06427449]), 'targetState': array([13, 26], dtype=int32), 'currentDistance': 7.430621680356942}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9125112641643065
{'scaleFactor': 20, 'currentTarget': array([13., 26.]), 'previousTarget': array([13., 26.]), 'currentState': array([13.55033551, 25.78985617,  0.50525833]), 'targetState': array([13, 26], dtype=int32), 'currentDistance': 0.5890921844211542}
episode index:699
target Thresh 31.976421208789265
target distance 17.0
model initialize at round 699
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 20.]), 'previousTarget': array([22., 20.]), 'currentState': array([ 6.57840545, 16.41602712,  0.65564078]), 'targetState': array([22, 20], dtype=int32), 'currentDistance': 15.832575282285081}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9125258833503973
{'scaleFactor': 20, 'currentTarget': array([22., 20.]), 'previousTarget': array([22., 20.]), 'currentState': array([21.85361964, 19.74515138,  6.1011534 ]), 'targetState': array([22, 20], dtype=int32), 'currentDistance': 0.2938962867354476}
episode index:700
target Thresh 31.976655821681803
target distance 12.0
model initialize at round 700
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([14.57237406, 12.39997675,  0.64545267]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 12.697753583408797}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9125537570516193
{'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([13.14352748, 24.11368249,  2.72313004]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 0.8978635001488463}
episode index:701
target Thresh 31.97688810013706
target distance 8.0
model initialize at round 701
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([11.570371  , 16.39475377,  4.90531415]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 7.783667541426087}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9126221933093807
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.11438288,  8.86300668,  5.19201367]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.17846740319720253}
episode index:702
target Thresh 31.977118067383067
target distance 8.0
model initialize at round 702
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  3.]), 'previousTarget': array([20.,  3.]), 'currentState': array([22.22527099,  9.50595079,  5.24401712]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 6.875989139094031}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9126904348693957
{'scaleFactor': 20, 'currentTarget': array([20.,  3.]), 'previousTarget': array([20.,  3.]), 'currentState': array([20.37359624,  2.92520201,  5.3414923 ]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 0.381010356189991}
episode index:703
target Thresh 31.97734574641675
target distance 12.0
model initialize at round 703
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 20.]), 'previousTarget': array([15., 20.]), 'currentState': array([17.69787281,  9.06623303,  1.44546938]), 'targetState': array([15, 20], dtype=int32), 'currentDistance': 11.261695160986976}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9127313293502645
{'scaleFactor': 20, 'currentTarget': array([15., 20.]), 'previousTarget': array([15., 20.]), 'currentState': array([15.09049954, 19.38619919,  1.43092511]), 'targetState': array([15, 20], dtype=int32), 'currentDistance': 0.6204366184662221}
episode index:704
target Thresh 31.97757116000619
target distance 18.0
model initialize at round 704
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([ 8.6033078 , 29.51165632,  5.58209473]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 22.850662606472763}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9126939584883721
{'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([21.9827153 , 10.88678438,  4.92491636]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 0.11452745209523037}
episode index:705
target Thresh 31.97779433069294
target distance 21.0
model initialize at round 705
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([24.2602689 , 24.88460304,  4.54869333]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 21.935132049296854}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9126693753379052
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.06495761,  5.36073651,  3.77384416]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.36653829602440113}
episode index:706
target Thresh 31.978015280794256
target distance 20.0
model initialize at round 706
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 14.]), 'previousTarget': array([27., 14.]), 'currentState': array([ 6.87192615, 22.32191026,  5.64621544]), 'targetState': array([27, 14], dtype=int32), 'currentDistance': 21.780577291601936}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9126321978221742
{'scaleFactor': 20, 'currentTarget': array([27., 14.]), 'previousTarget': array([27., 14.]), 'currentState': array([27.05915685, 13.46096189,  5.86743728]), 'targetState': array([27, 14], dtype=int32), 'currentDistance': 0.5422744804362862}
episode index:707
target Thresh 31.978234032405332
target distance 10.0
model initialize at round 707
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 13.]), 'previousTarget': array([ 3., 13.]), 'currentState': array([11.51967265,  9.80063663,  3.6558007 ]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 9.100590529414163}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9126863755793463
{'scaleFactor': 20, 'currentTarget': array([ 3., 13.]), 'previousTarget': array([ 3., 13.]), 'currentState': array([ 3.49863677, 12.49872063,  1.83630586]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 0.7070499515869263}
episode index:708
target Thresh 31.97845060740151
target distance 15.0
model initialize at round 708
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 12.]), 'previousTarget': array([23., 12.]), 'currentState': array([ 6.55080932, 24.14429301,  4.68497705]), 'targetState': array([23, 12], dtype=int32), 'currentDistance': 20.44650891403171}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9126492789589469
{'scaleFactor': 20, 'currentTarget': array([23., 12.]), 'previousTarget': array([23., 12.]), 'currentState': array([23.23336304, 12.75775155,  4.67714453]), 'targetState': array([23, 12], dtype=int32), 'currentDistance': 0.7928718176202928}
episode index:709
target Thresh 31.978665027440467
target distance 5.0
model initialize at round 709
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 24.]), 'previousTarget': array([26., 24.]), 'currentState': array([20.18755691, 21.5261193 ,  5.21860409]), 'targetState': array([26, 24], dtype=int32), 'currentDistance': 6.317007235534751}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9127304757491455
{'scaleFactor': 20, 'currentTarget': array([26., 24.]), 'previousTarget': array([26., 24.]), 'currentState': array([25.29928536, 23.26346313,  0.19776496]), 'targetState': array([26, 24], dtype=int32), 'currentDistance': 1.0166059053613847}
episode index:710
target Thresh 31.978877313964393
target distance 25.0
model initialize at round 710
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  4.]), 'previousTarget': array([20.,  4.]), 'currentState': array([12.10133184, 27.37176411,  4.98238379]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 24.670393521740415}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9126809547185545
{'scaleFactor': 20, 'currentTarget': array([20.,  4.]), 'previousTarget': array([20.,  4.]), 'currentState': array([19.54030476,  3.40112432,  4.71114825]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 0.754964764466636}
episode index:711
target Thresh 31.97908748820211
target distance 12.0
model initialize at round 711
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([ 2.34100884, 27.28308373,  3.98258436]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 17.097071407143197}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9126821292870448
{'scaleFactor': 20, 'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([15.58272394, 16.80671398,  5.18357363]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 0.4598682407149813}
episode index:712
target Thresh 31.97929557117122
target distance 9.0
model initialize at round 712
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 12.]), 'previousTarget': array([18., 12.]), 'currentState': array([21.52335839, 20.19258582,  4.65196872]), 'targetState': array([18, 12], dtype=int32), 'currentDistance': 8.9181005178854}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9127358570859411
{'scaleFactor': 20, 'currentTarget': array([18., 12.]), 'previousTarget': array([18., 12.]), 'currentState': array([17.60497452, 11.36934217,  4.84193216]), 'targetState': array([18, 12], dtype=int32), 'currentDistance': 0.7441602186483603}
episode index:713
target Thresh 31.979501583680197
target distance 14.0
model initialize at round 713
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 13.]), 'previousTarget': array([ 5., 13.]), 'currentState': array([17.37828194, 16.54289284,  2.4095856 ]), 'targetState': array([ 5, 13], dtype=int32), 'currentDistance': 12.875323430093301}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9127498750654116
{'scaleFactor': 20, 'currentTarget': array([ 5., 13.]), 'previousTarget': array([ 5., 13.]), 'currentState': array([ 4.45565024, 12.71673106,  2.45867443]), 'targetState': array([ 5, 13], dtype=int32), 'currentDistance': 0.613643181275624}
episode index:714
target Thresh 31.979705546330457
target distance 15.0
model initialize at round 714
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([21.68745211, 16.4638367 ,  4.12317228]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 13.527663533176653}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9127638538337508
{'scaleFactor': 20, 'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([22.9952458 ,  2.77171802,  4.69742588]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 0.22833147844414167}
episode index:715
target Thresh 31.97990747951844
target distance 9.0
model initialize at round 715
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 29.]), 'previousTarget': array([11., 29.]), 'currentState': array([18.68652787, 21.05222576,  3.47617984]), 'targetState': array([11, 29], dtype=int32), 'currentDistance': 11.05666432856678}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9127908112277079
{'scaleFactor': 20, 'currentTarget': array([11., 29.]), 'previousTarget': array([11., 29.]), 'currentState': array([11.30650074, 29.70025745,  0.60272884]), 'targetState': array([11, 29], dtype=int32), 'currentDistance': 0.764397278512779}
episode index:716
target Thresh 31.98010740343763
target distance 20.0
model initialize at round 716
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 27.]), 'previousTarget': array([ 3., 27.]), 'currentState': array([7.66986594, 7.20960873, 1.13487118]), 'targetState': array([ 3, 27], dtype=int32), 'currentDistance': 20.333893738302706}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9127416204491462
{'scaleFactor': 20, 'currentTarget': array([ 3., 27.]), 'previousTarget': array([ 3., 27.]), 'currentState': array([ 3.6270474 , 27.63385903,  1.35529452]), 'targetState': array([ 3, 27], dtype=int32), 'currentDistance': 0.8916084987725077}
episode index:717
target Thresh 31.980305338080587
target distance 15.0
model initialize at round 717
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 12.]), 'previousTarget': array([ 5., 12.]), 'currentState': array([ 8.25725845, 28.5102063 ,  3.03787422]), 'targetState': array([ 5, 12], dtype=int32), 'currentDistance': 16.8284474847543}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.91274270070964
{'scaleFactor': 20, 'currentTarget': array([ 5., 12.]), 'previousTarget': array([ 5., 12.]), 'currentState': array([ 4.59796068, 11.78030433,  4.66259632]), 'targetState': array([ 5, 12], dtype=int32), 'currentDistance': 0.458150413987391}
episode index:718
target Thresh 31.98050130324094
target distance 9.0
model initialize at round 718
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 26.]), 'previousTarget': array([18., 26.]), 'currentState': array([25.41815986, 28.57460407,  3.80316257]), 'targetState': array([18, 26], dtype=int32), 'currentDistance': 7.8522405580808785}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9128092560772205
{'scaleFactor': 20, 'currentTarget': array([18., 26.]), 'previousTarget': array([18., 26.]), 'currentState': array([17.93795507, 26.37976584,  3.3225814 ]), 'targetState': array([18, 26], dtype=int32), 'currentDistance': 0.3848008129948066}
episode index:719
target Thresh 31.980695318515366
target distance 11.0
model initialize at round 719
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 29.]), 'previousTarget': array([18., 29.]), 'currentState': array([ 8.62301658, 25.16845397,  0.6441608 ]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 10.12958849016219}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.91286228495753
{'scaleFactor': 20, 'currentTarget': array([18., 29.]), 'previousTarget': array([18., 29.]), 'currentState': array([17.69600797, 28.54600144,  6.20074697]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 0.5463751913533813}
episode index:720
target Thresh 31.980887403305555
target distance 21.0
model initialize at round 720
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 21.]), 'previousTarget': array([ 2., 21.]), 'currentState': array([22.61860069, 14.63918353,  2.80940515]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 21.577457694789846}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9128379797831905
{'scaleFactor': 20, 'currentTarget': array([ 2., 21.]), 'previousTarget': array([ 2., 21.]), 'currentState': array([ 2.27759082, 21.24512511,  2.50103024]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 0.3703282134228793}
episode index:721
target Thresh 31.98107757682015
target distance 4.0
model initialize at round 721
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  5.]), 'previousTarget': array([27.,  5.]), 'currentState': array([24.90947118,  7.5711943 ,  4.2873584 ]), 'targetState': array([27,  5], dtype=int32), 'currentDistance': 3.31381213707199}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9129311404760115
{'scaleFactor': 20, 'currentTarget': array([27.,  5.]), 'previousTarget': array([27.,  5.]), 'currentState': array([26.76038872,  4.75177878,  4.31128395]), 'targetState': array([27,  5], dtype=int32), 'currentDistance': 0.34500339163838756}
episode index:722
target Thresh 31.98126585807666
target distance 14.0
model initialize at round 722
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 8.]), 'previousTarget': array([9., 8.]), 'currentState': array([15.71575908, 20.09478537,  4.08669257]), 'targetState': array([9, 8], dtype=int32), 'currentDistance': 13.83420591226074}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9129576054932051
{'scaleFactor': 20, 'currentTarget': array([9., 8.]), 'previousTarget': array([9., 8.]), 'currentState': array([9.36219664, 8.50366225, 3.52854746]), 'targetState': array([9, 8], dtype=int32), 'currentDistance': 0.6203725245108437}
episode index:723
target Thresh 31.981452265903364
target distance 5.0
model initialize at round 723
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 15.]), 'previousTarget': array([ 9., 15.]), 'currentState': array([ 5.76431429, 18.85742881,  4.89785838]), 'targetState': array([ 9, 15], dtype=int32), 'currentDistance': 5.034820656433306}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9130368063143471
{'scaleFactor': 20, 'currentTarget': array([ 9., 15.]), 'previousTarget': array([ 9., 15.]), 'currentState': array([ 8.78628261, 15.24472897,  4.69206346]), 'targetState': array([ 9, 15], dtype=int32), 'currentDistance': 0.32491136022296807}
episode index:724
target Thresh 31.9816368189412
target distance 6.0
model initialize at round 724
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 19.]), 'previousTarget': array([20., 19.]), 'currentState': array([24.89087436, 20.26579161,  3.30032301]), 'targetState': array([20, 19], dtype=int32), 'currentDistance': 5.052017462333287}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9131293072711549
{'scaleFactor': 20, 'currentTarget': array([20., 19.]), 'previousTarget': array([20., 19.]), 'currentState': array([20.9836942 , 19.43147675,  3.24368252]), 'targetState': array([20, 19], dtype=int32), 'currentDistance': 1.074163146696905}
episode index:725
target Thresh 31.98181953564563
target distance 8.0
model initialize at round 725
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([6.29240259, 4.52699039, 0.99473512]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 9.051962726831146}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9131814570543902
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.64204329,  7.37260522,  0.35572526]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.7423302694786104}
episode index:726
target Thresh 31.982000434288473
target distance 7.0
model initialize at round 726
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 17.]), 'previousTarget': array([ 4., 17.]), 'currentState': array([ 9.09256972, 22.31957775,  3.75742054]), 'targetState': array([ 4, 17], dtype=int32), 'currentDistance': 7.364249714369999}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9132466765219909
{'scaleFactor': 20, 'currentTarget': array([ 4., 17.]), 'previousTarget': array([ 4., 17.]), 'currentState': array([ 3.87930392, 16.36844633,  4.09948522]), 'targetState': array([ 4, 17], dtype=int32), 'currentDistance': 0.642983342517706}
episode index:727
target Thresh 31.982179532959748
target distance 6.0
model initialize at round 727
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([20.12816324, 10.43954463,  3.12534797]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 7.737952122826577}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9133117168152299
{'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([18.21794991,  3.2726873 ,  4.33424671]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 0.3490852766745784}
episode index:728
target Thresh 31.982356849569463
target distance 10.0
model initialize at round 728
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 3.4184086 , 20.4247116 ,  4.50045538]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 9.771888399684189}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9133503566404504
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.98439368, 10.11669061,  4.80677295]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.8834472454365209}
episode index:729
target Thresh 31.982532401849436
target distance 4.0
model initialize at round 729
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 26.]), 'previousTarget': array([20., 26.]), 'currentState': array([22.68700362, 24.0703975 ,  1.46630323]), 'targetState': array([20, 26], dtype=int32), 'currentDistance': 3.308074097462584}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9134417945080662
{'scaleFactor': 20, 'currentTarget': array([20., 26.]), 'previousTarget': array([20., 26.]), 'currentState': array([20.04488753, 26.16487825,  1.48639679]), 'targetState': array([20, 26], dtype=int32), 'currentDistance': 0.17087927466575595}
episode index:730
target Thresh 31.982706207355037
target distance 7.0
model initialize at round 730
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 19.]), 'previousTarget': array([21., 19.]), 'currentState': array([15.45077121, 15.37652531,  0.77751248]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 6.627481341341123}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9135195745429389
{'scaleFactor': 20, 'currentTarget': array([21., 19.]), 'previousTarget': array([21., 19.]), 'currentState': array([20.36902879, 18.66694621,  0.31249401]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 0.7134770401759205}
episode index:731
target Thresh 31.982878283466963
target distance 5.0
model initialize at round 731
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 18.]), 'previousTarget': array([ 2., 18.]), 'currentState': array([ 5.34422265, 19.43717482,  2.57510304]), 'targetState': array([ 2, 18], dtype=int32), 'currentDistance': 3.6399583260136756}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9136105314083173
{'scaleFactor': 20, 'currentTarget': array([ 2., 18.]), 'previousTarget': array([ 2., 18.]), 'currentState': array([ 2.2901609 , 18.01942292,  2.57958651]), 'targetState': array([ 2, 18], dtype=int32), 'currentDistance': 0.2908102405186736}
episode index:732
target Thresh 31.983048647392966
target distance 10.0
model initialize at round 732
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 12.]), 'previousTarget': array([18., 12.]), 'currentState': array([ 8.54590031, 21.15266171,  4.67921209]), 'targetState': array([18, 12], dtype=int32), 'currentDistance': 13.158693603148137}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9136229927494082
{'scaleFactor': 20, 'currentTarget': array([18., 12.]), 'previousTarget': array([18., 12.]), 'currentState': array([17.75494913, 11.66504132,  4.34339821]), 'targetState': array([18, 12], dtype=int32), 'currentDistance': 0.4150268013670675}
episode index:733
target Thresh 31.983217316169583
target distance 9.0
model initialize at round 733
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([21.68208789, 22.05448303,  5.30556411]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 9.060062422099566}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9136739015466161
{'scaleFactor': 20, 'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([21.94747876, 12.40991146,  4.6207973 ]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 0.5924212788853526}
episode index:734
target Thresh 31.98338430666383
target distance 19.0
model initialize at round 734
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([21.86449197, 19.24217938,  3.32136106]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 24.828065232023935}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.913600955224296
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.00213927, 1.9962821 , 3.12925559]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.0042894345121485405}
episode index:735
target Thresh 31.9835496355749
target distance 7.0
model initialize at round 735
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([13.68735545,  4.53620655,  2.16007595]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 7.198912411827182}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9136648072008934
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 8.79973242, 10.68347308,  1.93739405]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.7122096227689867}
episode index:736
target Thresh 31.983713319435815
target distance 16.0
model initialize at round 736
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([ 8.59996032, 21.4274184 ,  4.06714499]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 15.404896305971082}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9136646069841808
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.97116365,  7.25166141,  6.07266426]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.25330811573547524}
episode index:737
target Thresh 31.983875374615103
target distance 5.0
model initialize at round 737
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([13.44026134,  9.41283879,  5.3833406 ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 3.752366357817218}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9137546278419258
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.09160384,  5.79189861,  5.16855763]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.22737073786254064}
episode index:738
target Thresh 31.984035817318414
target distance 19.0
model initialize at round 738
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([12.55230342, 26.05147171,  4.44021068]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 19.076021111079577}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9137419450911367
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.7154719 , 8.77015676, 3.83556086]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.36576516780363977}
episode index:739
target Thresh 31.984194663590156
target distance 20.0
model initialize at round 739
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 3.]), 'previousTarget': array([9., 3.]), 'currentState': array([19.21928284, 21.5089731 ,  3.22017765]), 'targetState': array([9, 3], dtype=int32), 'currentDistance': 21.142748804899824}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9137049760730624
{'scaleFactor': 20, 'currentTarget': array([9., 3.]), 'previousTarget': array([9., 3.]), 'currentState': array([9.87919435, 3.59470755, 3.25182593]), 'targetState': array([9, 3], dtype=int32), 'currentDistance': 1.0614423079830837}
episode index:740
target Thresh 31.984351929315082
target distance 14.0
model initialize at round 740
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 25.]), 'previousTarget': array([18., 25.]), 'currentState': array([4.90932936, 9.58384033, 0.01001185]), 'targetState': array([18, 25], dtype=int32), 'currentDistance': 20.22433278590904}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9136681068364134
{'scaleFactor': 20, 'currentTarget': array([18., 25.]), 'previousTarget': array([18., 25.]), 'currentState': array([17.72377027, 24.81634224,  0.18194429]), 'targetState': array([18, 25], dtype=int32), 'currentDistance': 0.33171227895643135}
episode index:741
target Thresh 31.984507630219902
target distance 24.0
model initialize at round 741
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 15.]), 'previousTarget': array([ 2., 15.]), 'currentState': array([27.40398706,  5.92801316,  1.59405821]), 'targetState': array([ 2, 15], dtype=int32), 'currentDistance': 26.975238715898982}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.913572790241464
{'scaleFactor': 20, 'currentTarget': array([ 2., 15.]), 'previousTarget': array([ 2., 15.]), 'currentState': array([ 1.75230194, 15.63498932,  2.3341772 ]), 'targetState': array([ 2, 15], dtype=int32), 'currentDistance': 0.6815906150231399}
episode index:742
target Thresh 31.984661781874834
target distance 23.0
model initialize at round 742
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 27.]), 'previousTarget': array([14., 27.]), 'currentState': array([8.82122281, 5.9719722 , 1.41379493]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 21.656354405503695}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9135482484702893
{'scaleFactor': 20, 'currentTarget': array([14., 27.]), 'previousTarget': array([14., 27.]), 'currentState': array([13.77820112, 26.73077394,  0.88123872]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 0.3488228937395311}
episode index:743
target Thresh 31.98481439969517
target distance 16.0
model initialize at round 743
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([12.64115815, 13.37281097,  5.4965579 ]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 15.00992388955373}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9135482068022965
{'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([27.13641257,  8.70515256,  5.86117778]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 0.3248744390739315}
episode index:744
target Thresh 31.984965498942824
target distance 5.0
model initialize at round 744
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  5.]), 'previousTarget': array([10.,  5.]), 'currentState': array([6.42141846, 1.09891308, 0.4450013 ]), 'targetState': array([10,  5], dtype=int32), 'currentDistance': 5.293838394565184}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9136243823636357
{'scaleFactor': 20, 'currentTarget': array([10.,  5.]), 'previousTarget': array([10.,  5.]), 'currentState': array([10.50381258,  5.382766  ,  1.05396369]), 'targetState': array([10,  5], dtype=int32), 'currentDistance': 0.6327218402107152}
episode index:745
target Thresh 31.98511509472784
target distance 14.0
model initialize at round 745
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 23.]), 'previousTarget': array([15., 23.]), 'currentState': array([17.83132318, 10.21102542,  1.32840061]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 13.098635871453327}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9136491021565893
{'scaleFactor': 20, 'currentTarget': array([15., 23.]), 'previousTarget': array([15., 23.]), 'currentState': array([15.06873376, 22.3720181 ,  1.3344983 ]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 0.6317322200910306}
episode index:746
target Thresh 31.985263202009925
target distance 12.0
model initialize at round 746
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([0.96803262, 9.67055222, 5.06230617]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 13.138604131985415}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9136612783175951
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.19555576,  7.68583515,  5.23521864]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.37005622381120673}
episode index:747
target Thresh 31.98540983559993
target distance 24.0
model initialize at round 747
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 28.]), 'previousTarget': array([ 9., 28.]), 'currentState': array([24.9069417 ,  5.27969201,  3.28769898]), 'targetState': array([ 9, 28], dtype=int32), 'currentDistance': 27.73523371905574}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.9135667354232988
{'scaleFactor': 20, 'currentTarget': array([ 9., 28.]), 'previousTarget': array([ 9., 28.]), 'currentState': array([ 9.60940546, 28.42046669,  0.53964335]), 'targetState': array([ 9, 28], dtype=int32), 'currentDistance': 0.7403831746900825}
episode index:748
target Thresh 31.985555010161338
target distance 14.0
model initialize at round 748
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 17.]), 'previousTarget': array([19., 17.]), 'currentState': array([6.13948041, 8.54927667, 0.44896662]), 'targetState': array([19, 17], dtype=int32), 'currentDistance': 15.388557078287768}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9135789890401274
{'scaleFactor': 20, 'currentTarget': array([19., 17.]), 'previousTarget': array([19., 17.]), 'currentState': array([18.99976881, 17.04249692,  0.21701461]), 'targetState': array([19, 17], dtype=int32), 'currentDistance': 0.04249755229678012}
episode index:749
target Thresh 31.985698740211724
target distance 18.0
model initialize at round 749
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([ 7.81131911, 23.5255003 ,  4.20541143]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 16.959313470469084}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9135667264880857
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.78201588, 6.85298037, 3.986095  ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.2629293597575372}
episode index:750
target Thresh 31.985841040124214
target distance 12.0
model initialize at round 750
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 14.]), 'previousTarget': array([18., 14.]), 'currentState': array([ 7.68119845, 18.92279988,  5.22729785]), 'targetState': array([18, 14], dtype=int32), 'currentDistance': 11.432918441477476}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9135913584739964
{'scaleFactor': 20, 'currentTarget': array([18., 14.]), 'previousTarget': array([18., 14.]), 'currentState': array([18.18700297, 13.51734976,  5.54560385]), 'targetState': array([18, 14], dtype=int32), 'currentDistance': 0.5176112073068514}
episode index:751
target Thresh 31.985981924128918
target distance 12.0
model initialize at round 751
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([13.24325291, 24.49676285,  5.25601578]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 10.52400572696566}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9136410907764246
{'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([13.93945865, 14.91200398,  4.24184369]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 0.9140112223868296}
episode index:752
target Thresh 31.986121406314354
target distance 3.0
model initialize at round 752
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 22.]), 'previousTarget': array([21., 22.]), 'currentState': array([22.50552371, 18.24781102,  0.54665595]), 'targetState': array([21, 22], dtype=int32), 'currentDistance': 4.042959776201274}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9137293496200151
{'scaleFactor': 20, 'currentTarget': array([21., 22.]), 'previousTarget': array([21., 22.]), 'currentState': array([21.39686696, 21.50657898,  1.86164242]), 'targetState': array([21, 22], dtype=int32), 'currentDistance': 0.6332200964948762}
episode index:753
target Thresh 31.98625950062885
target distance 7.0
model initialize at round 753
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 16.]), 'previousTarget': array([20., 16.]), 'currentState': array([25.59237765, 10.07751021,  2.7117219 ]), 'targetState': array([20, 16], dtype=int32), 'currentDistance': 8.145586111981421}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9137915069945243
{'scaleFactor': 20, 'currentTarget': array([20., 16.]), 'previousTarget': array([20., 16.]), 'currentState': array([20.3850104 , 15.81867062,  2.07897923]), 'targetState': array([20, 16], dtype=int32), 'currentDistance': 0.42557414373526864}
episode index:754
target Thresh 31.986396220881964
target distance 18.0
model initialize at round 754
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 22.]), 'previousTarget': array([26., 22.]), 'currentState': array([14.15444443,  5.67586843,  0.46889806]), 'targetState': array([26, 22], dtype=int32), 'currentDistance': 20.16914619464447}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9137552068153476
{'scaleFactor': 20, 'currentTarget': array([26., 22.]), 'previousTarget': array([26., 22.]), 'currentState': array([26.60573828, 22.09308097,  0.88002507]), 'targetState': array([26, 22], dtype=int32), 'currentDistance': 0.6128482069755146}
episode index:755
target Thresh 31.98653158074583
target distance 14.0
model initialize at round 755
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  2.]), 'previousTarget': array([13.,  2.]), 'currentState': array([16.5325737 , 17.61675624,  2.86223596]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 16.011313264742988}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9137548920543269
{'scaleFactor': 20, 'currentTarget': array([13.,  2.]), 'previousTarget': array([13.,  2.]), 'currentState': array([12.65319488,  1.35031077,  4.85365366]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 0.7364576646524621}
episode index:756
target Thresh 31.986665593756545
target distance 25.0
model initialize at round 756
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  7.]), 'previousTarget': array([27.,  7.]), 'currentState': array([ 1.53846646, 17.38155171,  5.44459224]), 'targetState': array([27,  7], dtype=int32), 'currentDistance': 27.49665990375218}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.9136725973106552
{'scaleFactor': 20, 'currentTarget': array([27.,  7.]), 'previousTarget': array([27.,  7.]), 'currentState': array([26.66400601,  7.66022536,  4.86941984]), 'targetState': array([27,  7], dtype=int32), 'currentDistance': 0.7408032735890219}
episode index:757
target Thresh 31.98679827331552
target distance 21.0
model initialize at round 757
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 24.]), 'previousTarget': array([11., 24.]), 'currentState': array([14.77672025,  4.15584619,  3.39452791]), 'targetState': array([11, 24], dtype=int32), 'currentDistance': 20.2003479242533}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9136484095229878
{'scaleFactor': 20, 'currentTarget': array([11., 24.]), 'previousTarget': array([11., 24.]), 'currentState': array([10.99375651, 23.32444218,  1.28168312]), 'targetState': array([11, 24], dtype=int32), 'currentDistance': 0.6755866723830849}
episode index:758
target Thresh 31.98692963269083
target distance 6.0
model initialize at round 758
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([18.60039289, 11.47926936,  0.69541257]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 5.609087090492705}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9137230479821142
{'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([22.54503644,  8.36068385,  4.2715764 ]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 0.5805899415124547}
episode index:759
target Thresh 31.987059685018515
target distance 10.0
model initialize at round 759
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 20.]), 'previousTarget': array([ 5., 20.]), 'currentState': array([13.73108588, 14.89444809,  2.8482995 ]), 'targetState': array([ 5, 20], dtype=int32), 'currentDistance': 10.11427312659323}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9137720835109535
{'scaleFactor': 20, 'currentTarget': array([ 5., 20.]), 'previousTarget': array([ 5., 20.]), 'currentState': array([ 5.45976401, 19.78172016,  2.24637871]), 'targetState': array([ 5, 20], dtype=int32), 'currentDistance': 0.5089489487580384}
episode index:760
target Thresh 31.98718844330392
target distance 4.0
model initialize at round 760
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 24.]), 'previousTarget': array([ 7., 24.]), 'currentState': array([ 9.64209408, 22.00577683,  2.76358712]), 'targetState': array([ 7, 24], dtype=int32), 'currentDistance': 3.310224643933595}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9138592424025291
{'scaleFactor': 20, 'currentTarget': array([ 7., 24.]), 'previousTarget': array([ 7., 24.]), 'currentState': array([ 6.70817525, 24.64683555,  2.58310962]), 'targetState': array([ 7, 24], dtype=int32), 'currentDistance': 0.7096181455773177}
episode index:761
target Thresh 31.98731592042298
target distance 16.0
model initialize at round 761
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([15.57721477, 24.41269789,  0.65346926]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 18.064886877460754}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9138468051749784
{'scaleFactor': 20, 'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([24.77209356,  9.62331434,  4.72671085]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 0.6636732005586696}
episode index:762
target Thresh 31.987442129123508
target distance 14.0
model initialize at round 762
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 19.]), 'previousTarget': array([ 2., 19.]), 'currentState': array([15.10996618, 28.42836546,  3.13803792]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 16.14822864365008}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9138344005482861
{'scaleFactor': 20, 'currentTarget': array([ 2., 19.]), 'previousTarget': array([ 2., 19.]), 'currentState': array([ 1.60943221, 19.1313568 ,  3.73811261]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 0.41206529980921813}
episode index:763
target Thresh 31.987567082026487
target distance 17.0
model initialize at round 763
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 27.]), 'previousTarget': array([20., 27.]), 'currentState': array([15.00964106, 11.6829424 ,  2.5750677 ]), 'targetState': array([20, 27], dtype=int32), 'currentDistance': 16.109498309718855}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9138220283944387
{'scaleFactor': 20, 'currentTarget': array([20., 27.]), 'previousTarget': array([20., 27.]), 'currentState': array([20.13314169, 26.87314824,  1.82440954]), 'targetState': array([20, 27], dtype=int32), 'currentDistance': 0.1838969213997183}
episode index:764
target Thresh 31.987690791627305
target distance 6.0
model initialize at round 764
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 20.]), 'previousTarget': array([25., 20.]), 'currentState': array([25.1346816 , 15.7461868 ,  1.63360655]), 'targetState': array([25, 20], dtype=int32), 'currentDistance': 4.255944767073745}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9139086662658185
{'scaleFactor': 20, 'currentTarget': array([25., 20.]), 'previousTarget': array([25., 20.]), 'currentState': array([25.13567503, 19.73482811,  1.74569957]), 'targetState': array([25, 20], dtype=int32), 'currentDistance': 0.29786548215485514}
episode index:765
target Thresh 31.98781327029703
target distance 13.0
model initialize at round 765
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([24.07999905, 17.31893241,  3.74994135]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 16.554292738550746}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9139202015506254
{'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([12.97633456,  6.53187676,  3.24457036]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 1.1118102633642413}
episode index:766
target Thresh 31.987934530283628
target distance 13.0
model initialize at round 766
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 24.]), 'previousTarget': array([22., 24.]), 'currentState': array([10.65512071, 23.03357401,  0.17085493]), 'targetState': array([22, 24], dtype=int32), 'currentDistance': 11.385967907934987}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9139561336860235
{'scaleFactor': 20, 'currentTarget': array([22., 24.]), 'previousTarget': array([22., 24.]), 'currentState': array([22.28438727, 23.74730528,  0.11557831]), 'targetState': array([22, 24], dtype=int32), 'currentDistance': 0.38043493137268786}
episode index:767
target Thresh 31.9880545837132
target distance 7.0
model initialize at round 767
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([13.73317752, 18.51487254,  2.13004157]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 7.244970168758811}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9140168626916406
{'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([ 8.7156724 , 24.60660419,  2.6850758 ]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 0.669933449120872}
episode index:768
target Thresh 31.98817344259119
target distance 1.0
model initialize at round 768
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 7.]), 'previousTarget': array([7., 7.]), 'currentState': array([6.8937659 , 4.73156672, 2.98533273]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 2.270919467636818}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9141156704124578
{'scaleFactor': 20, 'currentTarget': array([7., 7.]), 'previousTarget': array([7., 7.]), 'currentState': array([6.20047515, 6.26513305, 0.98542118]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 1.0859417165306031}
episode index:769
target Thresh 31.988291118803584
target distance 11.0
model initialize at round 769
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 17.]), 'previousTarget': array([23., 17.]), 'currentState': array([16.9308334 ,  7.68203168,  2.62108356]), 'targetState': array([23, 17], dtype=int32), 'currentDistance': 11.120221075807526}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9141512086968584
{'scaleFactor': 20, 'currentTarget': array([23., 17.]), 'previousTarget': array([23., 17.]), 'currentState': array([23.24460027, 16.74166519,  0.93069447]), 'targetState': array([23, 17], dtype=int32), 'currentDistance': 0.35576138970492904}
episode index:770
target Thresh 31.988407624118096
target distance 7.0
model initialize at round 770
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 23.]), 'previousTarget': array([27., 23.]), 'currentState': array([21.71908176, 22.28032696,  5.53772873]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 5.329730461562881}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9142240333288988
{'scaleFactor': 20, 'currentTarget': array([27., 23.]), 'previousTarget': array([27., 23.]), 'currentState': array([26.83214879, 22.9132592 ,  0.52633917]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 0.18893912435138355}
episode index:771
target Thresh 31.988522970185365
target distance 14.0
model initialize at round 771
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 20.]), 'previousTarget': array([ 3., 20.]), 'currentState': array([16.33788028, 21.54725097,  2.98514861]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 13.427324231052435}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9142350704546747
{'scaleFactor': 20, 'currentTarget': array([ 3., 20.]), 'previousTarget': array([ 3., 20.]), 'currentState': array([ 3.04150091, 20.39916214,  2.68294574]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 0.4013137658231957}
episode index:772
target Thresh 31.988637168540087
target distance 9.0
model initialize at round 772
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 16.]), 'previousTarget': array([11., 16.]), 'currentState': array([0.86900279, 9.24628785, 1.29773569]), 'targetState': array([11, 16], dtype=int32), 'currentDistance': 12.175784669201775}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.914270316352406
{'scaleFactor': 20, 'currentTarget': array([11., 16.]), 'previousTarget': array([11., 16.]), 'currentState': array([10.53551821, 15.61890964,  0.23893293]), 'targetState': array([11, 16], dtype=int32), 'currentDistance': 0.600810449519003}
episode index:773
target Thresh 31.988750230602193
target distance 14.0
model initialize at round 773
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 3.]), 'previousTarget': array([9., 3.]), 'currentState': array([21.21162923, 15.33038892,  3.82847023]), 'targetState': array([9, 3], dtype=int32), 'currentDistance': 17.354030637188536}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9142693433952113
{'scaleFactor': 20, 'currentTarget': array([9., 3.]), 'previousTarget': array([9., 3.]), 'currentState': array([8.88750102, 3.14832932, 3.96593756]), 'targetState': array([9, 3], dtype=int32), 'currentDistance': 0.1861655448202683}
episode index:774
target Thresh 31.988862167677983
target distance 12.0
model initialize at round 774
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 15.]), 'previousTarget': array([ 4., 15.]), 'currentState': array([11.73919281,  1.88511063,  2.85575891]), 'targetState': array([ 4, 15], dtype=int32), 'currentDistance': 15.228113096797603}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9142802793320277
{'scaleFactor': 20, 'currentTarget': array([ 4., 15.]), 'previousTarget': array([ 4., 15.]), 'currentState': array([ 3.97978173, 15.3077332 ,  2.16693735]), 'targetState': array([ 4, 15], dtype=int32), 'currentDistance': 0.30839666141675814}
episode index:775
target Thresh 31.988972990961262
target distance 16.0
model initialize at round 775
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 5.]), 'previousTarget': array([9., 5.]), 'currentState': array([23.44326941,  3.36048615,  2.52137947]), 'targetState': array([9, 5], dtype=int32), 'currentDistance': 14.536025488794206}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9142792960435633
{'scaleFactor': 20, 'currentTarget': array([9., 5.]), 'previousTarget': array([9., 5.]), 'currentState': array([9.0117611 , 5.26078007, 2.59442061]), 'targetState': array([9, 5], dtype=int32), 'currentDistance': 0.26104514218786196}
episode index:776
target Thresh 31.989082711534447
target distance 12.0
model initialize at round 776
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([10.52220078, 19.07381032,  4.25164255]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 13.195058185006852}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9143021867152022
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.7012998 , 9.07899016, 4.06634871]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.3089680500022233}
episode index:777
target Thresh 31.989191340369686
target distance 11.0
model initialize at round 777
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 16.]), 'previousTarget': array([19., 16.]), 'currentState': array([18.28997735,  6.82651102,  1.5891124 ]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 9.200925616197633}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9143493433516866
{'scaleFactor': 20, 'currentTarget': array([19., 16.]), 'previousTarget': array([19., 16.]), 'currentState': array([19.07604886, 16.73738496,  1.83709557]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 0.7412961633095565}
episode index:778
target Thresh 31.98929888832996
target distance 6.0
model initialize at round 778
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  3.]), 'previousTarget': array([20.,  3.]), 'currentState': array([25.38531499,  0.43329951,  3.32850027]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 5.9656993719657745}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9144211657607345
{'scaleFactor': 20, 'currentTarget': array([20.,  3.]), 'previousTarget': array([20.,  3.]), 'currentState': array([20.15305144,  2.93564004,  2.77737275]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 0.1660329666277676}
episode index:779
target Thresh 31.989405366170143
target distance 16.0
model initialize at round 779
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 16.]), 'previousTarget': array([ 4., 16.]), 'currentState': array([19.52618809, 11.38510361,  3.41699743]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 16.197524052845985}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9144318369513335
{'scaleFactor': 20, 'currentTarget': array([ 4., 16.]), 'previousTarget': array([ 4., 16.]), 'currentState': array([ 4.85521301, 15.61328446,  2.37793196]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 0.9385830845268899}
episode index:780
target Thresh 31.98951078453812
target distance 18.0
model initialize at round 780
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 20.]), 'previousTarget': array([12., 20.]), 'currentState': array([21.46444723,  1.31118523,  2.55326319]), 'targetState': array([12, 20], dtype=int32), 'currentDistance': 20.948688716951448}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9144073893422521
{'scaleFactor': 20, 'currentTarget': array([12., 20.]), 'previousTarget': array([12., 20.]), 'currentState': array([12.12157946, 20.47512981,  2.06876733]), 'targetState': array([12, 20], dtype=int32), 'currentDistance': 0.4904384836364329}
episode index:781
target Thresh 31.989615153975805
target distance 12.0
model initialize at round 781
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([18.57466966,  5.37166275,  3.44689226]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 12.465732366787943}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9144420092400253
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([7.75321105, 9.58644173, 2.48984082]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.8592772175967948}
episode index:782
target Thresh 31.989718484920235
target distance 23.0
model initialize at round 782
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([15.11135421, 25.57077057,  5.16611886]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 22.41978575902107}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9144061763696244
{'scaleFactor': 20, 'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([8.37762743, 3.54923411, 4.35331785]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 0.7684643735204089}
episode index:783
target Thresh 31.989820787704588
target distance 11.0
model initialize at round 783
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 18.]), 'previousTarget': array([ 4., 18.]), 'currentState': array([13.39218833, 25.50267744,  4.45157456]), 'targetState': array([ 4, 18], dtype=int32), 'currentDistance': 12.02095547052647}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9144407094984909
{'scaleFactor': 20, 'currentTarget': array([ 4., 18.]), 'previousTarget': array([ 4., 18.]), 'currentState': array([ 4.37837754, 18.45852245,  3.53283691]), 'targetState': array([ 4, 18], dtype=int32), 'currentDistance': 0.5944849865093053}
episode index:784
target Thresh 31.98992207255923
target distance 20.0
model initialize at round 784
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 23.]), 'previousTarget': array([21., 23.]), 'currentState': array([16.51274103,  3.78768575,  1.64452887]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 19.729381940267476}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9144278959513703
{'scaleFactor': 20, 'currentTarget': array([21., 23.]), 'previousTarget': array([21., 23.]), 'currentState': array([20.78535397, 22.68634728,  1.0390117 ]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 0.38006702427778216}
episode index:785
target Thresh 31.99002234961273
target distance 13.0
model initialize at round 785
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  2.]), 'previousTarget': array([19.,  2.]), 'currentState': array([10.58228001, 14.09310524,  4.72068262]), 'targetState': array([19,  2], dtype=int32), 'currentDistance': 14.734354553323534}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9144384771199155
{'scaleFactor': 20, 'currentTarget': array([19.,  2.]), 'previousTarget': array([19.,  2.]), 'currentState': array([18.62138688,  2.96247933,  4.98259535]), 'targetState': array([19,  2], dtype=int32), 'currentDistance': 1.0342699649317344}
episode index:786
target Thresh 31.990121628892876
target distance 2.0
model initialize at round 786
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([13.3173564 , 25.96669292,  4.17137921]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 1.0174528386120627}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9145471957004493
{'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([13.3173564 , 25.96669292,  4.17137921]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 1.0174528386120627}
episode index:787
target Thresh 31.99021992032768
target distance 4.0
model initialize at round 787
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 18.]), 'previousTarget': array([10., 18.]), 'currentState': array([ 9.68245801, 22.04151047,  5.29785282]), 'targetState': array([10, 18], dtype=int32), 'currentDistance': 4.053965935198221}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9146303845383928
{'scaleFactor': 20, 'currentTarget': array([10., 18.]), 'previousTarget': array([10., 18.]), 'currentState': array([ 9.95403145, 18.14036575,  4.81247086]), 'targetState': array([10, 18], dtype=int32), 'currentDistance': 0.1477012256270394}
episode index:788
target Thresh 31.990317233746364
target distance 23.0
model initialize at round 788
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 4.]), 'previousTarget': array([8., 4.]), 'currentState': array([ 0.67084071, 25.96766107,  4.81195498]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 23.158037670362855}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9145945854093406
{'scaleFactor': 20, 'currentTarget': array([8., 4.]), 'previousTarget': array([8., 4.]), 'currentState': array([7.95366748, 4.40146447, 4.96575614]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 0.40412921768808124}
episode index:789
target Thresh 31.990413578880357
target distance 14.0
model initialize at round 789
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 27.]), 'previousTarget': array([17., 27.]), 'currentState': array([ 7.94622794, 14.68211077,  0.59275246]), 'targetState': array([17, 27], dtype=int32), 'currentDistance': 15.287288296294733}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9145932216904473
{'scaleFactor': 20, 'currentTarget': array([17., 27.]), 'previousTarget': array([17., 27.]), 'currentState': array([17.55255444, 27.2881274 ,  1.02207072]), 'targetState': array([17, 27], dtype=int32), 'currentDistance': 0.6231643550652908}
episode index:790
target Thresh 31.990508965364246
target distance 13.0
model initialize at round 790
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 13.]), 'previousTarget': array([11., 13.]), 'currentState': array([ 5.14966516, 24.36451383,  5.3765172 ]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 12.781963561665837}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9146272127494999
{'scaleFactor': 20, 'currentTarget': array([11., 13.]), 'previousTarget': array([11., 13.]), 'currentState': array([10.42918602, 13.74380001,  5.07571549]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 0.9375857596201538}
episode index:791
target Thresh 31.990603402736767
target distance 22.0
model initialize at round 791
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 16.]), 'previousTarget': array([ 2., 16.]), 'currentState': array([24.69522675, 25.53266038,  2.15494618]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 24.61594871962663}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9145803614998148
{'scaleFactor': 20, 'currentTarget': array([ 2., 16.]), 'previousTarget': array([ 2., 16.]), 'currentState': array([ 2.27577758, 16.27343615,  2.94211501]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 0.3883562822874666}
episode index:792
target Thresh 31.990696900441726
target distance 9.0
model initialize at round 792
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 21.]), 'previousTarget': array([19., 21.]), 'currentState': array([19.57308668, 13.62792294,  2.83726531]), 'targetState': array([19, 21], dtype=int32), 'currentDistance': 7.394318663295167}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9146383887993107
{'scaleFactor': 20, 'currentTarget': array([19., 21.]), 'previousTarget': array([19., 21.]), 'currentState': array([19.00889958, 20.29670794,  1.49365038]), 'targetState': array([19, 21], dtype=int32), 'currentDistance': 0.7033483655578868}
episode index:793
target Thresh 31.99078946782898
target distance 8.0
model initialize at round 793
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  2.]), 'previousTarget': array([22.,  2.]), 'currentState': array([24.42098052,  8.41394591,  3.36155999]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 6.855643574971804}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9146962699343242
{'scaleFactor': 20, 'currentTarget': array([22.,  2.]), 'previousTarget': array([22.,  2.]), 'currentState': array([22.15157281,  2.0715044 ,  3.37682676]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 0.16759235035726552}
episode index:794
target Thresh 31.99088111415534
target distance 11.0
model initialize at round 794
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 21.]), 'previousTarget': array([18., 21.]), 'currentState': array([ 6.92684365, 12.31862073,  5.67890668]), 'targetState': array([18, 21], dtype=int32), 'currentDistance': 14.070577014645764}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9147181178311452
{'scaleFactor': 20, 'currentTarget': array([18., 21.]), 'previousTarget': array([18., 21.]), 'currentState': array([17.20403261, 20.48819724,  0.2270466 ]), 'targetState': array([18, 21], dtype=int32), 'currentDistance': 0.9463118660653544}
episode index:795
target Thresh 31.990971848585517
target distance 6.0
model initialize at round 795
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 27.]), 'previousTarget': array([22., 27.]), 'currentState': array([17.90293046, 28.40356559,  6.11261773]), 'targetState': array([22, 27], dtype=int32), 'currentDistance': 4.330816913459393}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9148002558740709
{'scaleFactor': 20, 'currentTarget': array([22., 27.]), 'previousTarget': array([22., 27.]), 'currentState': array([21.63162092, 27.01196538,  6.08513367]), 'targetState': array([22, 27], dtype=int32), 'currentDistance': 0.36857335096021243}
episode index:796
target Thresh 31.991061680193027
target distance 3.0
model initialize at round 796
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 18.]), 'previousTarget': array([ 4., 18.]), 'currentState': array([ 1.50995373, 13.10190819,  4.80921322]), 'targetState': array([ 4, 18], dtype=int32), 'currentDistance': 5.49469142235447}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9148577160423593
{'scaleFactor': 20, 'currentTarget': array([ 4., 18.]), 'previousTarget': array([ 4., 18.]), 'currentState': array([ 3.52889827, 17.41457331,  2.2016989 ]), 'targetState': array([ 4, 18], dtype=int32), 'currentDistance': 0.7514394571405651}
episode index:797
target Thresh 31.99115061796111
target distance 17.0
model initialize at round 797
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([14.33298529, 25.97260607,  0.35335272]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 18.670171876484094}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9148332555639338
{'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([23.85360446,  9.80068275,  5.17208736]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 0.24730349598451506}
episode index:798
target Thresh 31.991238670783616
target distance 19.0
model initialize at round 798
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([11.75565432,  7.50378676,  2.11514994]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 18.418605653788315}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9148316084949972
{'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([ 6.07114049, 24.15264588,  1.62739055]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 0.8503352135848696}
episode index:799
target Thresh 31.991325847465895
target distance 16.0
model initialize at round 799
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 18.]), 'previousTarget': array([18., 18.]), 'currentState': array([ 3.38849183, 29.95104076,  5.87373573]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 18.8765342714385}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9148299655437332
{'scaleFactor': 20, 'currentTarget': array([18., 18.]), 'previousTarget': array([18., 18.]), 'currentState': array([17.00722418, 18.94897555,  5.2388656 ]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 1.3733748299441537}
episode index:800
target Thresh 31.991412156725694
target distance 7.0
model initialize at round 800
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 16.]), 'previousTarget': array([ 6., 16.]), 'currentState': array([11.31752482, 19.04080875,  4.12734222]), 'targetState': array([ 6, 16], dtype=int32), 'currentDistance': 6.125568385035402}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9148992152746398
{'scaleFactor': 20, 'currentTarget': array([ 6., 16.]), 'previousTarget': array([ 6., 16.]), 'currentState': array([ 6.19864598, 16.16611998,  3.84710681]), 'targetState': array([ 6, 16], dtype=int32), 'currentDistance': 0.2589518757363554}
episode index:801
target Thresh 31.99149760719401
target distance 24.0
model initialize at round 801
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 10.]), 'previousTarget': array([26., 10.]), 'currentState': array([ 3.28425821, 10.08816399,  1.71241575]), 'targetState': array([26, 10], dtype=int32), 'currentDistance': 22.71591287539348}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9148416673912163
{'scaleFactor': 20, 'currentTarget': array([26., 10.]), 'previousTarget': array([26., 10.]), 'currentState': array([25.93370175,  9.7595897 ,  5.97815353]), 'targetState': array([26, 10], dtype=int32), 'currentDistance': 0.24938438039243238}
episode index:802
target Thresh 31.99158220741596
target distance 6.0
model initialize at round 802
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 18.]), 'previousTarget': array([10., 18.]), 'currentState': array([13.75097955, 22.87202128,  4.88611269]), 'targetState': array([10, 18], dtype=int32), 'currentDistance': 6.148694081190994}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9149107300719246
{'scaleFactor': 20, 'currentTarget': array([10., 18.]), 'previousTarget': array([10., 18.]), 'currentState': array([10.78898263, 18.70549502,  3.13409543]), 'targetState': array([10, 18], dtype=int32), 'currentDistance': 1.0584029527068304}
episode index:803
target Thresh 31.991665965851638
target distance 24.0
model initialize at round 803
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  5.]), 'previousTarget': array([19.,  5.]), 'currentState': array([ 5.62573016, 28.02852149,  4.76693797]), 'targetState': array([19,  5], dtype=int32), 'currentDistance': 26.630506858958107}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.9148425057243741
{'scaleFactor': 20, 'currentTarget': array([19.,  5.]), 'previousTarget': array([19.,  5.]), 'currentState': array([18.9011495 ,  5.90396908,  4.73679761]), 'targetState': array([19,  5], dtype=int32), 'currentDistance': 0.909357751790346}
episode index:804
target Thresh 31.991748890876952
target distance 5.0
model initialize at round 804
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'previousTarget': array([19.,  3.]), 'currentState': array([18.35698167,  8.99548428,  5.9061088 ]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 6.029867686165662}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9148639005593836
{'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'previousTarget': array([19.,  3.]), 'currentState': array([18.0932855 ,  2.98288645,  4.1584772 ]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 0.9068759898740971}
episode index:805
target Thresh 31.991830990784482
target distance 16.0
model initialize at round 805
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([19.29602728, 22.34326896,  5.89920366]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 14.972815862221797}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9148736782192701
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.53241962,  8.77390489,  4.26773208]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.9393611870787301}
episode index:806
target Thresh 31.99191227378428
target distance 19.0
model initialize at round 806
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 7.]), 'previousTarget': array([9., 7.]), 'currentState': array([15.08660024, 24.71481985,  4.40425587]), 'targetState': array([9, 7], dtype=int32), 'currentDistance': 18.731298535941317}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9148719973881232
{'scaleFactor': 20, 'currentTarget': array([9., 7.]), 'previousTarget': array([9., 7.]), 'currentState': array([9.6527948 , 7.98069048, 4.13967317]), 'targetState': array([9, 7], dtype=int32), 'currentDistance': 1.178089501669864}
episode index:807
target Thresh 31.99199274800472
target distance 9.0
model initialize at round 807
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 13.]), 'previousTarget': array([17., 13.]), 'currentState': array([24.60347972,  7.060788  ,  2.72364855]), 'targetState': array([17, 13], dtype=int32), 'currentDistance': 9.648167858627112}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9149166979481627
{'scaleFactor': 20, 'currentTarget': array([17., 13.]), 'previousTarget': array([17., 13.]), 'currentState': array([17.02972368, 13.22419305,  2.73288592]), 'targetState': array([17, 13], dtype=int32), 'currentDistance': 0.226154862093454}
episode index:808
target Thresh 31.992072421493283
target distance 5.0
model initialize at round 808
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 23.]), 'previousTarget': array([21., 23.]), 'currentState': array([14.32135656, 25.49720424,  4.32025039]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 7.130238933513019}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9149731618691168
{'scaleFactor': 20, 'currentTarget': array([21., 23.]), 'previousTarget': array([21., 23.]), 'currentState': array([21.0385577 , 22.83094973,  0.15277567]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 0.17339172100991587}
episode index:809
target Thresh 31.99215130221739
target distance 6.0
model initialize at round 809
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 12.]), 'previousTarget': array([20., 12.]), 'currentState': array([20.85865227, 16.00997603,  4.51819322]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 4.100876916449796}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9150535653729821
{'scaleFactor': 20, 'currentTarget': array([20., 12.]), 'previousTarget': array([20., 12.]), 'currentState': array([19.86988167, 12.20260972,  4.90730706]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 0.2407934398135686}
episode index:810
target Thresh 31.99222939806518
target distance 12.0
model initialize at round 810
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([10.64083704,  8.62577829,  0.78576773]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 12.482536164572496}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9150745416769698
{'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([ 8.82367719, 20.33815114,  2.00644201]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 0.6849333162226106}
episode index:811
target Thresh 31.9923067168463
target distance 12.0
model initialize at round 811
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'previousTarget': array([ 5., 22.]), 'currentState': array([15.44944725, 28.34564975,  4.55092883]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 12.22531057108554}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9151070608983047
{'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'previousTarget': array([ 5., 22.]), 'currentState': array([ 5.78072618, 22.29445056,  3.34572567]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 0.8344066758008518}
episode index:812
target Thresh 31.992383266292695
target distance 8.0
model initialize at round 812
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 16.]), 'previousTarget': array([18., 16.]), 'currentState': array([18.24287286,  9.84905014,  1.88229257]), 'targetState': array([18, 16], dtype=int32), 'currentDistance': 6.155742960644583}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9151749476622674
{'scaleFactor': 20, 'currentTarget': array([18., 16.]), 'previousTarget': array([18., 16.]), 'currentState': array([18.17834298, 15.74958524,  1.90102977]), 'targetState': array([18, 16], dtype=int32), 'currentDistance': 0.30743092163433744}
episode index:813
target Thresh 31.99245905405937
target distance 11.0
model initialize at round 813
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'previousTarget': array([ 5., 22.]), 'currentState': array([16.58816594, 20.57684778,  2.22377928]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 11.675228139152948}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.915195697539718
{'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'previousTarget': array([ 5., 22.]), 'currentState': array([ 5.20378526, 21.73574793,  2.48616175]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 0.3337028462825999}
episode index:814
target Thresh 31.99253408772517
target distance 19.0
model initialize at round 814
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  3.]), 'previousTarget': array([22.,  3.]), 'currentState': array([1.94262271, 3.69067141, 5.0430479 ]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 20.06926532501589}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9151824292912139
{'scaleFactor': 20, 'currentTarget': array([22.,  3.]), 'previousTarget': array([22.,  3.]), 'currentState': array([21.19597534,  3.1125315 ,  6.15860842]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 0.8118614414812948}
episode index:815
target Thresh 31.99260837479352
target distance 21.0
model initialize at round 815
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([ 7.81123095, 21.32765006,  5.60998821]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 20.182379985347968}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9151471381667345
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.06239944, 2.10318232, 4.0403938 ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.1205830851231802}
episode index:816
target Thresh 31.992681922693187
target distance 14.0
model initialize at round 816
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([18.62652375, 22.56778644,  5.01345909]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 15.441922832556465}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9151564375012036
{'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([25.99326025,  8.69935636,  5.40205665]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 0.30071917257473835}
episode index:817
target Thresh 31.992754738779027
target distance 14.0
model initialize at round 817
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([13.67449523, 21.8313174 ,  5.17278761]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 13.84775378154426}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9151771085408196
{'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([13.16519657,  8.4849099 ,  4.45647935]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 0.512276799771514}
episode index:818
target Thresh 31.992826830332707
target distance 11.0
model initialize at round 818
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 20.]), 'previousTarget': array([14., 20.]), 'currentState': array([23.30590287, 14.0452575 ,  2.7584123 ]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 11.04802183106974}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9152208361859467
{'scaleFactor': 20, 'currentTarget': array([14., 20.]), 'previousTarget': array([14., 20.]), 'currentState': array([14.96404084, 19.39283393,  2.50615054]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 1.1393091689115737}
episode index:819
target Thresh 31.99289820456344
target distance 11.0
model initialize at round 819
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 14.]), 'previousTarget': array([23., 14.]), 'currentState': array([21.4531003 ,  2.15094899,  0.4812023 ]), 'targetState': array([23, 14], dtype=int32), 'currentDistance': 11.94959867445694}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9152413782734116
{'scaleFactor': 20, 'currentTarget': array([23., 14.]), 'previousTarget': array([23., 14.]), 'currentState': array([23.1999185 , 13.8777789 ,  1.73188132]), 'targetState': array([23, 14], dtype=int32), 'currentDistance': 0.2343190258312887}
episode index:820
target Thresh 31.992968868608713
target distance 12.0
model initialize at round 820
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([20.63754467,  9.6116245 ,  0.77713316]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 12.911374008619234}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.915250517513551
{'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([16.71820714, 22.52804008,  0.86416822]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 0.5985261390105548}
episode index:821
target Thresh 31.99303882953499
target distance 5.0
model initialize at round 821
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 23.]), 'previousTarget': array([18., 23.]), 'currentState': array([14.39852273, 18.93622767,  1.59992045]), 'targetState': array([18, 23], dtype=int32), 'currentDistance': 5.429998532205768}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9153174864703472
{'scaleFactor': 20, 'currentTarget': array([18., 23.]), 'previousTarget': array([18., 23.]), 'currentState': array([18.24173118, 23.21423771,  1.08595576]), 'targetState': array([18, 23], dtype=int32), 'currentDistance': 0.3230042780032291}
episode index:822
target Thresh 31.993108094338414
target distance 20.0
model initialize at round 822
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 28.]), 'previousTarget': array([26., 28.]), 'currentState': array([14.9489808 ,  9.43082829,  0.03346682]), 'targetState': array([26, 28], dtype=int32), 'currentDistance': 21.608775147256736}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9152715612413418
{'scaleFactor': 20, 'currentTarget': array([26., 28.]), 'previousTarget': array([26., 28.]), 'currentState': array([26.59581425, 28.0510896 ,  1.44908142]), 'targetState': array([26, 28], dtype=int32), 'currentDistance': 0.5980006411471025}
episode index:823
target Thresh 31.993176669945534
target distance 6.0
model initialize at round 823
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  9.]), 'previousTarget': array([23.,  9.]), 'currentState': array([18.64665214,  4.85311728,  0.89623863]), 'targetState': array([23,  9], dtype=int32), 'currentDistance': 6.01234346464859}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9153383421136216
{'scaleFactor': 20, 'currentTarget': array([23.,  9.]), 'previousTarget': array([23.,  9.]), 'currentState': array([23.10014877,  8.74987824,  1.11396357]), 'targetState': array([23,  9], dtype=int32), 'currentDistance': 0.2694265566363688}
episode index:824
target Thresh 31.99324456321396
target distance 4.0
model initialize at round 824
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([8.74898795, 5.33256175, 5.5675019 ]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 2.4498627021855706}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9154288410928779
{'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([8.52287987, 3.66476653, 3.56777585]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 0.8457646807917475}
episode index:825
target Thresh 31.993311780933077
target distance 20.0
model initialize at round 825
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 28.]), 'previousTarget': array([21., 28.]), 'currentState': array([21.5199586 ,  9.61305559,  0.85004759]), 'targetState': array([21, 28], dtype=int32), 'currentDistance': 18.39429481248661}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9154045183485267
{'scaleFactor': 20, 'currentTarget': array([21., 28.]), 'previousTarget': array([21., 28.]), 'currentState': array([20.72675324, 27.85213756,  0.95050493]), 'targetState': array([21, 28], dtype=int32), 'currentDistance': 0.31068809636813266}
episode index:826
target Thresh 31.993378329824715
target distance 16.0
model initialize at round 826
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 23.]), 'previousTarget': array([ 2., 23.]), 'currentState': array([17.82537316, 20.67388576,  2.68474454]), 'targetState': array([ 2, 23], dtype=int32), 'currentDistance': 15.99541318557679}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9154133940148862
{'scaleFactor': 20, 'currentTarget': array([ 2., 23.]), 'previousTarget': array([ 2., 23.]), 'currentState': array([ 2.42783799, 22.87935077,  3.04546322]), 'targetState': array([ 2, 23], dtype=int32), 'currentDistance': 0.4445239923915948}
episode index:827
target Thresh 31.993444216543818
target distance 21.0
model initialize at round 827
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 28.]), 'previousTarget': array([14., 28.]), 'currentState': array([20.62421925,  6.55920536,  0.74499434]), 'targetState': array([14, 28], dtype=int32), 'currentDistance': 22.440765484429782}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9153783354130761
{'scaleFactor': 20, 'currentTarget': array([14., 28.]), 'previousTarget': array([14., 28.]), 'currentState': array([14.28742969, 28.11689114,  1.78588326]), 'targetState': array([14, 28], dtype=int32), 'currentDistance': 0.3102891660240454}
episode index:828
target Thresh 31.99350944767911
target distance 14.0
model initialize at round 828
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 20.]), 'previousTarget': array([12., 20.]), 'currentState': array([6.17956611, 7.20042154, 1.80416077]), 'targetState': array([12, 20], dtype=int32), 'currentDistance': 14.060820011816375}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9153984644993174
{'scaleFactor': 20, 'currentTarget': array([12., 20.]), 'previousTarget': array([12., 20.]), 'currentState': array([11.89758376, 19.50371194,  1.16127475]), 'targetState': array([12, 20], dtype=int32), 'currentDistance': 0.5067454224200117}
episode index:829
target Thresh 31.993574029753763
target distance 6.0
model initialize at round 829
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([15.96029355, 18.31614938,  5.69654894]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 4.740453174806052}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9154646097228121
{'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([13.51882783, 13.51510828,  3.94437748]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 0.6831153937534553}
episode index:830
target Thresh 31.993637969226036
target distance 5.0
model initialize at round 830
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  5.]), 'previousTarget': array([17.,  5.]), 'currentState': array([18.74268374,  8.48976527,  4.1594286 ]), 'targetState': array([17,  5], dtype=int32), 'currentDistance': 3.9006933028236634}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9155423899758532
{'scaleFactor': 20, 'currentTarget': array([17.,  5.]), 'previousTarget': array([17.,  5.]), 'currentState': array([16.64961868,  5.17996655,  4.65897426]), 'targetState': array([17,  5], dtype=int32), 'currentDistance': 0.39389722665706434}
episode index:831
target Thresh 31.99370127248993
target distance 23.0
model initialize at round 831
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  4.]), 'previousTarget': array([22.,  4.]), 'currentState': array([25.67927069, 25.34744447,  3.51139843]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 21.662188673634123}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9154966912174676
{'scaleFactor': 20, 'currentTarget': array([22.,  4.]), 'previousTarget': array([22.,  4.]), 'currentState': array([21.55166435,  3.756137  ,  5.08229816]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 0.5103665502384986}
episode index:832
target Thresh 31.993763945875823
target distance 11.0
model initialize at round 832
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 18.]), 'previousTarget': array([12., 18.]), 'currentState': array([12.45355945, 27.37929894,  3.97526217]), 'targetState': array([12, 18], dtype=int32), 'currentDistance': 9.390259041865322}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9155393002915163
{'scaleFactor': 20, 'currentTarget': array([12., 18.]), 'previousTarget': array([12., 18.]), 'currentState': array([12.53794151, 18.9897291 ,  4.98825049]), 'targetState': array([12, 18], dtype=int32), 'currentDistance': 1.1264744850502004}
episode index:833
target Thresh 31.993825995651108
target distance 13.0
model initialize at round 833
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 17.]), 'previousTarget': array([13., 17.]), 'currentState': array([24.23433637,  4.88117299,  2.39242285]), 'targetState': array([13, 17], dtype=int32), 'currentDistance': 16.525019871604748}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9155479398528309
{'scaleFactor': 20, 'currentTarget': array([13., 17.]), 'previousTarget': array([13., 17.]), 'currentState': array([13.56805961, 16.24172301,  2.06560049]), 'targetState': array([13, 17], dtype=int32), 'currentDistance': 0.9474575010330432}
episode index:834
target Thresh 31.993887428020813
target distance 15.0
model initialize at round 834
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([10.68266889, 25.0318354 ,  5.29210264]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 15.944594247805687}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9155565587205855
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.92595659, 10.2405772 ,  5.19037081]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.2517137597583535}
episode index:835
target Thresh 31.993948249128227
target distance 24.0
model initialize at round 835
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 29.]), 'previousTarget': array([23., 29.]), 'currentState': array([12.95240725,  6.68229695,  2.60907912]), 'targetState': array([23, 29], dtype=int32), 'currentDistance': 24.475170882073474}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.9154901733090074
{'scaleFactor': 20, 'currentTarget': array([23., 29.]), 'previousTarget': array([23., 29.]), 'currentState': array([22.62548849, 28.76911764,  6.19916207]), 'targetState': array([23, 29], dtype=int32), 'currentDistance': 0.4399608396202447}
episode index:836
target Thresh 31.994008465055508
target distance 12.0
model initialize at round 836
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 7.]), 'previousTarget': array([7., 7.]), 'currentState': array([17.04035417,  3.68533256,  3.08450785]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 10.573350092226478}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9155325865426884
{'scaleFactor': 20, 'currentTarget': array([7., 7.]), 'previousTarget': array([7., 7.]), 'currentState': array([7.82169955, 6.59024351, 2.66671094]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 0.918199609922481}
episode index:837
target Thresh 31.994068081824306
target distance 11.0
model initialize at round 837
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  6.]), 'previousTarget': array([13.,  6.]), 'currentState': array([ 9.6827127 , 16.97057144,  5.25569833]), 'targetState': array([13,  6], dtype=int32), 'currentDistance': 11.461144474089528}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9155635502215169
{'scaleFactor': 20, 'currentTarget': array([13.,  6.]), 'previousTarget': array([13.,  6.]), 'currentState': array([13.09882278,  5.59080269,  5.11572604]), 'targetState': array([13,  6], dtype=int32), 'currentDistance': 0.4209612524308173}
episode index:838
target Thresh 31.994127105396338
target distance 21.0
model initialize at round 838
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 25.]), 'previousTarget': array([15., 25.]), 'currentState': array([18.76721897,  5.14570459,  1.38278818]), 'targetState': array([15, 25], dtype=int32), 'currentDistance': 20.208537425990603}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9155394437900952
{'scaleFactor': 20, 'currentTarget': array([15., 25.]), 'previousTarget': array([15., 25.]), 'currentState': array([14.93906398, 24.69452635,  1.62059047]), 'targetState': array([15, 25], dtype=int32), 'currentDistance': 0.31149213788213964}
episode index:839
target Thresh 31.99418554167402
target distance 14.0
model initialize at round 839
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 17.]), 'previousTarget': array([ 5., 17.]), 'currentState': array([15.70340605,  4.0729549 ,  1.44029617]), 'targetState': array([ 5, 17], dtype=int32), 'currentDistance': 16.783068732814808}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9155261612082127
{'scaleFactor': 20, 'currentTarget': array([ 5., 17.]), 'previousTarget': array([ 5., 17.]), 'currentState': array([ 5.23613   , 16.79107483,  1.66581796]), 'targetState': array([ 5, 17], dtype=int32), 'currentDistance': 0.31528892545708115}
episode index:840
target Thresh 31.994243396501023
target distance 25.0
model initialize at round 840
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 2.]), 'previousTarget': array([9., 2.]), 'currentState': array([ 4.14124938, 25.76309101,  0.18439632]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 24.25473050538842}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.9154602066225208
{'scaleFactor': 20, 'currentTarget': array([9., 2.]), 'previousTarget': array([9., 2.]), 'currentState': array([8.54222949, 1.41755047, 4.39998279]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 0.7408112423424128}
episode index:841
target Thresh 31.994300675662878
target distance 14.0
model initialize at round 841
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 28.]), 'previousTarget': array([ 2., 28.]), 'currentState': array([14.2865296 , 15.84729778,  2.15845037]), 'targetState': array([ 2, 28], dtype=int32), 'currentDistance': 17.281405639416413}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9154578990700994
{'scaleFactor': 20, 'currentTarget': array([ 2., 28.]), 'previousTarget': array([ 2., 28.]), 'currentState': array([ 1.92266971, 28.31762565,  2.43929409]), 'targetState': array([ 2, 28], dtype=int32), 'currentDistance': 0.3269036983053778}
episode index:842
target Thresh 31.994357384887547
target distance 20.0
model initialize at round 842
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 29.]), 'previousTarget': array([18., 29.]), 'currentState': array([14.65079356, 10.64634229,  2.78980899]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 18.656739664854083}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9154340323502758
{'scaleFactor': 20, 'currentTarget': array([18., 29.]), 'previousTarget': array([18., 29.]), 'currentState': array([18.32645588, 29.32442232,  1.19968784]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 0.460242629261077}
episode index:843
target Thresh 31.994413529846007
target distance 18.0
model initialize at round 843
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([ 4.17548786, 25.20441537,  6.07073778]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 21.571523092318394}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9153891117230822
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.84733844,  6.65195425,  4.21040652]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.6695893490438899}
episode index:844
target Thresh 31.994469116152793
target distance 5.0
model initialize at round 844
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([16.33892573, 15.27059258,  3.99010897]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 6.239196351198423}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9154540938393863
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([12.95671759, 10.45431749,  4.70801449]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.4563745660694297}
episode index:845
target Thresh 31.994524149366587
target distance 13.0
model initialize at round 845
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([ 1.77212758, 20.84903632,  4.90467167]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 12.280831484976739}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9154848574984424
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([4.86127883, 9.52926834, 4.89962019]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 0.5471458167747904}
episode index:846
target Thresh 31.994578634990756
target distance 22.0
model initialize at round 846
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  8.]), 'previousTarget': array([25.,  8.]), 'currentState': array([ 2.54596572, 26.3794319 ,  5.4492228 ]), 'targetState': array([25,  8], dtype=int32), 'currentDistance': 29.017015228115884}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.9154194188882215
{'scaleFactor': 20, 'currentTarget': array([25.,  8.]), 'previousTarget': array([25.,  8.]), 'currentState': array([25.00525343,  7.82586931,  5.66298179]), 'targetState': array([25,  8], dtype=int32), 'currentDistance': 0.17420992247933165}
episode index:847
target Thresh 31.994632578473905
target distance 5.0
model initialize at round 847
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  7.]), 'previousTarget': array([17.,  7.]), 'currentState': array([13.67970881,  3.10472057,  1.07226389]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 5.118352803393811}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9154956931584005
{'scaleFactor': 20, 'currentTarget': array([17.,  7.]), 'previousTarget': array([17.,  7.]), 'currentState': array([16.30421898,  6.1009881 ,  0.8813315 ]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 1.1368085264986854}
episode index:848
target Thresh 31.994685985210428
target distance 13.0
model initialize at round 848
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 22.]), 'previousTarget': array([20., 22.]), 'currentState': array([19.73647935, 10.11171204,  1.41002083]), 'targetState': array([20, 22], dtype=int32), 'currentDistance': 11.891208254293593}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9155262991139276
{'scaleFactor': 20, 'currentTarget': array([20., 22.]), 'previousTarget': array([20., 22.]), 'currentState': array([19.96244431, 21.7155932 ,  1.61350768]), 'targetState': array([20, 22], dtype=int32), 'currentDistance': 0.28687568148472026}
episode index:849
target Thresh 31.99473886054104
target distance 20.0
model initialize at round 849
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 27.]), 'previousTarget': array([19., 27.]), 'currentState': array([23.34603724,  8.87923066,  2.08278683]), 'targetState': array([19, 27], dtype=int32), 'currentDistance': 18.63465377639395}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9155131882620392
{'scaleFactor': 20, 'currentTarget': array([19., 27.]), 'previousTarget': array([19., 27.]), 'currentState': array([19.68459417, 27.6611946 ,  0.83798211]), 'targetState': array([19, 27], dtype=int32), 'currentDistance': 0.9517601941690533}
episode index:850
target Thresh 31.994791209753327
target distance 11.0
model initialize at round 850
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 26.]), 'previousTarget': array([ 2., 26.]), 'currentState': array([13.36599323, 22.64269201,  2.36157602]), 'targetState': array([ 2, 26], dtype=int32), 'currentDistance': 11.85146906409997}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9155326385083906
{'scaleFactor': 20, 'currentTarget': array([ 2., 26.]), 'previousTarget': array([ 2., 26.]), 'currentState': array([ 1.60507595, 26.73323431,  2.18458386]), 'targetState': array([ 2, 26], dtype=int32), 'currentDistance': 0.8328250480899855}
episode index:851
target Thresh 31.994843038082248
target distance 19.0
model initialize at round 851
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 17.]), 'previousTarget': array([ 6., 17.]), 'currentState': array([23.04267292, 15.82495652,  3.55829024]), 'targetState': array([ 6, 17], dtype=int32), 'currentDistance': 17.08313283666962}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9155302730259672
{'scaleFactor': 20, 'currentTarget': array([ 6., 17.]), 'previousTarget': array([ 6., 17.]), 'currentState': array([ 5.49064466, 17.14865348,  2.86581221]), 'targetState': array([ 6, 17], dtype=int32), 'currentDistance': 0.5306041105970415}
episode index:852
target Thresh 31.994894350710677
target distance 12.0
model initialize at round 852
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 22.]), 'previousTarget': array([24., 22.]), 'currentState': array([15.96818567, 11.67985159,  1.48243475]), 'targetState': array([24, 22], dtype=int32), 'currentDistance': 13.077289656848825}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9155496576389579
{'scaleFactor': 20, 'currentTarget': array([24., 22.]), 'previousTarget': array([24., 22.]), 'currentState': array([24.36204931, 22.19987907,  0.91083031]), 'targetState': array([24, 22], dtype=int32), 'currentDistance': 0.41355935649058256}
episode index:853
target Thresh 31.994945152769926
target distance 6.0
model initialize at round 853
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 19.]), 'previousTarget': array([18., 19.]), 'currentState': array([12.8440853 , 23.45599042,  0.03541255]), 'targetState': array([18, 19], dtype=int32), 'currentDistance': 6.814639170089961}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9156137669391464
{'scaleFactor': 20, 'currentTarget': array([18., 19.]), 'previousTarget': array([18., 19.]), 'currentState': array([17.22813809, 19.7231687 ,  5.6612027 ]), 'targetState': array([18, 19], dtype=int32), 'currentDistance': 1.0577068481417424}
episode index:854
target Thresh 31.994995449340237
target distance 9.0
model initialize at round 854
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 20.]), 'previousTarget': array([16., 20.]), 'currentState': array([ 7.85846495, 11.55244136,  6.25767756]), 'targetState': array([16, 20], dtype=int32), 'currentDistance': 11.732256389096703}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9156330085543136
{'scaleFactor': 20, 'currentTarget': array([16., 20.]), 'previousTarget': array([16., 20.]), 'currentState': array([15.59319803, 19.93140237,  0.11755886]), 'targetState': array([16, 20], dtype=int32), 'currentDistance': 0.4125451251903414}
episode index:855
target Thresh 31.995045245451312
target distance 11.0
model initialize at round 855
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 19.]), 'previousTarget': array([24., 19.]), 'currentState': array([14.60436139, 27.49165698,  4.96634174]), 'targetState': array([24, 19], dtype=int32), 'currentDistance': 12.664369829248026}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.915652205212436
{'scaleFactor': 20, 'currentTarget': array([24., 19.]), 'previousTarget': array([24., 19.]), 'currentState': array([23.53707686, 19.88077887,  5.03002235]), 'targetState': array([24, 19], dtype=int32), 'currentDistance': 0.9950222367390447}
episode index:856
target Thresh 31.995094546082804
target distance 12.0
model initialize at round 856
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 20.]), 'previousTarget': array([25., 20.]), 'currentState': array([12.21918343, 26.50912322,  5.23993826]), 'targetState': array([25, 20], dtype=int32), 'currentDistance': 14.342871308959717}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9156604811625124
{'scaleFactor': 20, 'currentTarget': array([25., 20.]), 'previousTarget': array([25., 20.]), 'currentState': array([24.46139796, 20.49301243,  5.71813104]), 'targetState': array([25, 20], dtype=int32), 'currentDistance': 0.7301735488034522}
episode index:857
target Thresh 31.995143356164817
target distance 19.0
model initialize at round 857
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 6.]), 'previousTarget': array([8., 6.]), 'currentState': array([19.91543016, 26.28689407,  3.28108442]), 'targetState': array([8, 6], dtype=int32), 'currentDistance': 23.52733616045801}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9156058020618206
{'scaleFactor': 20, 'currentTarget': array([8., 6.]), 'previousTarget': array([8., 6.]), 'currentState': array([8.15349453, 5.90804403, 3.2940743 ]), 'targetState': array([8, 6], dtype=int32), 'currentDistance': 0.178931473854488}
episode index:858
target Thresh 31.995191680578397
target distance 8.0
model initialize at round 858
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([24.09566585, 23.41935473,  3.14808118]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 7.236231184202152}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9156581771583727
{'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([16.29735538, 21.8799393 ,  3.50160388]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 0.7128281915562006}
episode index:859
target Thresh 31.99523952415603
target distance 5.0
model initialize at round 859
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([15.63428767, 13.59813947,  5.03207541]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 4.926928768653781}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9157217129988862
{'scaleFactor': 20, 'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([19.27477544,  9.23171278,  5.17853287]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 0.8159453398345018}
episode index:860
target Thresh 31.99528689168211
target distance 7.0
model initialize at round 860
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 23.]), 'previousTarget': array([12., 23.]), 'currentState': array([ 4.96134753, 28.68858068,  0.59387422]), 'targetState': array([12, 23], dtype=int32), 'currentDistance': 9.050004356723662}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9157626750626505
{'scaleFactor': 20, 'currentTarget': array([12., 23.]), 'previousTarget': array([12., 23.]), 'currentState': array([11.89266005, 23.59894593,  4.89607048]), 'targetState': array([12, 23], dtype=int32), 'currentDistance': 0.608488368450474}
episode index:861
target Thresh 31.995333787893433
target distance 12.0
model initialize at round 861
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 19.]), 'previousTarget': array([14., 19.]), 'currentState': array([12.38711158,  8.56755084,  2.95327777]), 'targetState': array([14, 19], dtype=int32), 'currentDistance': 10.556391647153516}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9157925097196555
{'scaleFactor': 20, 'currentTarget': array([14., 19.]), 'previousTarget': array([14., 19.]), 'currentState': array([14.33266067, 19.46759823,  1.41096066]), 'targetState': array([14, 19], dtype=int32), 'currentDistance': 0.5738564543325487}
episode index:862
target Thresh 31.995380217479653
target distance 23.0
model initialize at round 862
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  2.]), 'previousTarget': array([25.,  2.]), 'currentState': array([2.24326176, 5.33470369, 5.86744022]), 'targetState': array([25,  2], dtype=int32), 'currentDistance': 22.999769214023814}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.915758433661714
{'scaleFactor': 20, 'currentTarget': array([25.,  2.]), 'previousTarget': array([25.,  2.]), 'currentState': array([2.50792517e+01, 1.76889251e+00, 7.98959042e-04]), 'targetState': array([25,  2], dtype=int32), 'currentDistance': 0.2443184569200603}
episode index:863
target Thresh 31.995426185083772
target distance 11.0
model initialize at round 863
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 22.]), 'previousTarget': array([25., 22.]), 'currentState': array([22.34228218, 10.70955256,  2.30504131]), 'targetState': array([25, 22], dtype=int32), 'currentDistance': 11.599037349565746}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9157773074050536
{'scaleFactor': 20, 'currentTarget': array([25., 22.]), 'previousTarget': array([25., 22.]), 'currentState': array([24.82899924, 22.24839833,  0.37035632]), 'targetState': array([25, 22], dtype=int32), 'currentDistance': 0.30156755724697454}
episode index:864
target Thresh 31.99547169530259
target distance 6.0
model initialize at round 864
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 14.]), 'previousTarget': array([12., 14.]), 'currentState': array([17.01233419,  9.3444581 ,  1.93539828]), 'targetState': array([12, 14], dtype=int32), 'currentDistance': 6.840874537636522}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9158291209340651
{'scaleFactor': 20, 'currentTarget': array([12., 14.]), 'previousTarget': array([12., 14.]), 'currentState': array([11.68098114, 13.52874359,  1.16308361]), 'targetState': array([12, 14], dtype=int32), 'currentDistance': 0.5690831552725278}
episode index:865
target Thresh 31.995516752687163
target distance 18.0
model initialize at round 865
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 29.]), 'previousTarget': array([19., 29.]), 'currentState': array([21.35597072, 11.99686082,  1.64393824]), 'targetState': array([19, 29], dtype=int32), 'currentDistance': 17.165585922377222}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9158264513342378
{'scaleFactor': 20, 'currentTarget': array([19., 29.]), 'previousTarget': array([19., 29.]), 'currentState': array([19.19409276, 29.43693395,  1.62767634]), 'targetState': array([19, 29], dtype=int32), 'currentDistance': 0.47810383672681844}
episode index:866
target Thresh 31.99556136174327
target distance 12.0
model initialize at round 866
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 27.]), 'previousTarget': array([17., 27.]), 'currentState': array([16.4057974 , 16.90152626,  1.71911588]), 'targetState': array([17, 27], dtype=int32), 'currentDistance': 10.115940317501295}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9158670091180507
{'scaleFactor': 20, 'currentTarget': array([17., 27.]), 'previousTarget': array([17., 27.]), 'currentState': array([17.08554034, 26.63554274,  1.90917835]), 'targetState': array([17, 27], dtype=int32), 'currentDistance': 0.37436111206662587}
episode index:867
target Thresh 31.99560552693185
target distance 19.0
model initialize at round 867
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 22.]), 'previousTarget': array([18., 22.]), 'currentState': array([2.59229632, 3.92236567, 1.55155158]), 'targetState': array([18, 22], dtype=int32), 'currentDistance': 23.7528565742362}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9158330435219656
{'scaleFactor': 20, 'currentTarget': array([18., 22.]), 'previousTarget': array([18., 22.]), 'currentState': array([17.0607072 , 21.69895952,  0.22319059]), 'targetState': array([18, 22], dtype=int32), 'currentDistance': 0.9863550710468129}
episode index:868
target Thresh 31.995649252669466
target distance 10.0
model initialize at round 868
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 4.]), 'previousTarget': array([8., 4.]), 'currentState': array([16.45852413,  9.74859247,  4.0344708 ]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 10.22706926968366}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9158735003762557
{'scaleFactor': 20, 'currentTarget': array([8., 4.]), 'previousTarget': array([8., 4.]), 'currentState': array([8.30795318, 4.30121639, 4.29343691]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 0.4307742744236771}
episode index:869
target Thresh 31.99569254332872
target distance 15.0
model initialize at round 869
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 14.]), 'previousTarget': array([18., 14.]), 'currentState': array([ 4.8713163 , 11.69175239,  0.23133035]), 'targetState': array([18, 14], dtype=int32), 'currentDistance': 13.33005411159577}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9158921116952565
{'scaleFactor': 20, 'currentTarget': array([18., 14.]), 'previousTarget': array([18., 14.]), 'currentState': array([18.38421262, 14.07566491,  0.24584486]), 'targetState': array([18, 14], dtype=int32), 'currentDistance': 0.3915922818256155}
episode index:870
target Thresh 31.995735403238715
target distance 15.0
model initialize at round 870
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([11.01993662,  8.68285193,  0.54894996]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 14.598205053246224}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9158999791840426
{'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([16.22678644, 21.36160077,  0.98238918]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 1.0027027395160424}
episode index:871
target Thresh 31.995777836685484
target distance 13.0
model initialize at round 871
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 23.]), 'previousTarget': array([12., 23.]), 'currentState': array([12.12654649, 11.68712717,  0.50093699]), 'targetState': array([12, 23], dtype=int32), 'currentDistance': 11.313580589612101}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9159185174509268
{'scaleFactor': 20, 'currentTarget': array([12., 23.]), 'previousTarget': array([12., 23.]), 'currentState': array([12.64741078, 23.49720839,  2.46599221]), 'targetState': array([12, 23], dtype=int32), 'currentDistance': 0.8163068664085567}
episode index:872
target Thresh 31.9958198479124
target distance 1.0
model initialize at round 872
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([ 4.20344907, 15.17649232,  6.04727513]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 1.1939538133294934}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9160033759647288
{'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([ 4.77248185, 13.59262568,  4.04731828]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 0.873316687788396}
episode index:873
target Thresh 31.995861441120628
target distance 7.0
model initialize at round 873
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([17.6102983 , 10.51040699,  4.97821367]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 10.794565115851817}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9160325255910859
{'scaleFactor': 20, 'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([9.97806553, 4.16311489, 3.4342396 ]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 0.9915738262538165}
episode index:874
target Thresh 31.995902620469515
target distance 23.0
model initialize at round 874
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 29.]), 'previousTarget': array([ 4., 29.]), 'currentState': array([28.09487225, 25.27814039,  1.87247532]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 24.380629761276168}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.915978483633575
{'scaleFactor': 20, 'currentTarget': array([ 4., 29.]), 'previousTarget': array([ 4., 29.]), 'currentState': array([ 4.90458635, 29.43922593,  2.07674056]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 1.0055823595784306}
episode index:875
target Thresh 31.995943390077038
target distance 17.0
model initialize at round 875
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 16.]), 'previousTarget': array([ 4., 16.]), 'currentState': array([19.35611847, 10.3606128 ,  3.93564653]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 16.358883287006776}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.915965245724186
{'scaleFactor': 20, 'currentTarget': array([ 4., 16.]), 'previousTarget': array([ 4., 16.]), 'currentState': array([ 3.91986486, 16.18137548,  3.11618543]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 0.19828944770915538}
episode index:876
target Thresh 31.995983754020184
target distance 5.0
model initialize at round 876
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  7.]), 'previousTarget': array([20.,  7.]), 'currentState': array([16.63852407,  3.55759066,  2.19104317]), 'targetState': array([20,  7], dtype=int32), 'currentDistance': 4.811413774337542}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9160271998339646
{'scaleFactor': 20, 'currentTarget': array([20.,  7.]), 'previousTarget': array([20.,  7.]), 'currentState': array([20.4932714 ,  6.70401362,  5.63803545]), 'targetState': array([20,  7], dtype=int32), 'currentDistance': 0.5752604760629977}
episode index:877
target Thresh 31.996023716335387
target distance 2.0
model initialize at round 877
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  9.]), 'previousTarget': array([21.,  9.]), 'currentState': array([22.40862622, 11.92110271,  5.85243279]), 'targetState': array([21,  9], dtype=int32), 'currentDistance': 3.243003061918899}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9161001756883678
{'scaleFactor': 20, 'currentTarget': array([21.,  9.]), 'previousTarget': array([21.,  9.]), 'currentState': array([21.08481869,  9.02614884,  3.756297  ]), 'targetState': array([21,  9], dtype=int32), 'currentDistance': 0.08875794345056094}
episode index:878
target Thresh 31.996063281018913
target distance 16.0
model initialize at round 878
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  5.]), 'previousTarget': array([24.,  5.]), 'currentState': array([ 6.32581052, 20.8283076 ,  4.25378764]), 'targetState': array([24,  5], dtype=int32), 'currentDistance': 23.725772806790445}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.916066369881801
{'scaleFactor': 20, 'currentTarget': array([24.,  5.]), 'previousTarget': array([24.,  5.]), 'currentState': array([23.59257121,  5.45553274,  5.69819949]), 'targetState': array([24,  5], dtype=int32), 'currentDistance': 0.6111532511134744}
episode index:879
target Thresh 31.996102452027255
target distance 13.0
model initialize at round 879
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 20.]), 'previousTarget': array([25., 20.]), 'currentState': array([13.67587889, 26.15433086,  5.36501581]), 'targetState': array([25, 20], dtype=int32), 'currentDistance': 12.888425316098813}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9160739588869671
{'scaleFactor': 20, 'currentTarget': array([25., 20.]), 'previousTarget': array([25., 20.]), 'currentState': array([25.59123942, 20.32962767,  4.41407007]), 'targetState': array([25, 20], dtype=int32), 'currentDistance': 0.6769183460964477}
episode index:880
target Thresh 31.996141233277555
target distance 7.0
model initialize at round 880
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([19.48662196, 26.3889174 ,  3.99572229]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 9.224420798062141}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9161135912263689
{'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([11.3284082 , 21.17049861,  2.85360444]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 0.6928963255009368}
episode index:881
target Thresh 31.99617962864797
target distance 15.0
model initialize at round 881
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  2.]), 'previousTarget': array([26.,  2.]), 'currentState': array([12.43504783, 13.12078114,  4.72349757]), 'targetState': array([26,  2], dtype=int32), 'currentDistance': 17.540801021285674}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9161002901875736
{'scaleFactor': 20, 'currentTarget': array([26.,  2.]), 'previousTarget': array([26.,  2.]), 'currentState': array([25.65613293,  2.98812978,  4.09216434]), 'targetState': array([26,  2], dtype=int32), 'currentDistance': 1.046252848884841}
episode index:882
target Thresh 31.99621764197806
target distance 15.0
model initialize at round 882
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 17.]), 'previousTarget': array([23., 17.]), 'currentState': array([ 9.67855026, 14.87811024,  0.93751114]), 'targetState': array([23, 17], dtype=int32), 'currentDistance': 13.489382468096188}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9161183706606421
{'scaleFactor': 20, 'currentTarget': array([23., 17.]), 'previousTarget': array([23., 17.]), 'currentState': array([23.04215658, 16.84375574,  0.2607284 ]), 'targetState': array([23, 17], dtype=int32), 'currentDistance': 0.16183153142890394}
episode index:883
target Thresh 31.9962552770692
target distance 17.0
model initialize at round 883
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  4.]), 'previousTarget': array([21.,  4.]), 'currentState': array([13.51564915, 20.20684765,  4.64233565]), 'targetState': array([21,  4], dtype=int32), 'currentDistance': 17.85153826136423}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9160948637416353
{'scaleFactor': 20, 'currentTarget': array([21.,  4.]), 'previousTarget': array([21.,  4.]), 'currentState': array([20.68404813,  3.72925839,  4.38641615]), 'targetState': array([21,  4], dtype=int32), 'currentDistance': 0.416084851142113}
episode index:884
target Thresh 31.996292537684923
target distance 18.0
model initialize at round 884
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 29.]), 'previousTarget': array([11., 29.]), 'currentState': array([24.3309522 , 12.6501087 ,  2.38285863]), 'targetState': array([11, 29], dtype=int32), 'currentDistance': 21.09581077188895}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9160714099456094
{'scaleFactor': 20, 'currentTarget': array([11., 29.]), 'previousTarget': array([11., 29.]), 'currentState': array([11.16133599, 29.36940829,  2.18415049]), 'targetState': array([11, 29], dtype=int32), 'currentDistance': 0.40310270020963973}
episode index:885
target Thresh 31.996329427551327
target distance 3.0
model initialize at round 885
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 16.]), 'previousTarget': array([23., 16.]), 'currentState': array([24.66408747, 14.64910718,  2.78173906]), 'targetState': array([23, 16], dtype=int32), 'currentDistance': 2.1433848286407926}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.916154850792172
{'scaleFactor': 20, 'currentTarget': array([23., 16.]), 'previousTarget': array([23., 16.]), 'currentState': array([23.25057492, 15.9873501 ,  1.9772985 ]), 'targetState': array([23, 16], dtype=int32), 'currentDistance': 0.2508940267452195}
episode index:886
target Thresh 31.996365950357426
target distance 2.0
model initialize at round 886
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 16.]), 'previousTarget': array([ 5., 16.]), 'currentState': array([ 4.85892899, 15.69809149,  0.14970613]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 0.3332413212132944}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9162493774541876
{'scaleFactor': 20, 'currentTarget': array([ 5., 16.]), 'previousTarget': array([ 5., 16.]), 'currentState': array([ 4.85892899, 15.69809149,  0.14970613]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 0.3332413212132944}
episode index:887
target Thresh 31.996402109755536
target distance 10.0
model initialize at round 887
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 26.]), 'previousTarget': array([ 7., 26.]), 'currentState': array([15.21089628, 19.88686848,  2.77952792]), 'targetState': array([ 7, 26], dtype=int32), 'currentDistance': 10.236659348084155}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.916288499833068
{'scaleFactor': 20, 'currentTarget': array([ 7., 26.]), 'previousTarget': array([ 7., 26.]), 'currentState': array([ 7.46838126, 25.74001917,  2.42502663]), 'targetState': array([ 7, 26], dtype=int32), 'currentDistance': 0.5356967799025236}
episode index:888
target Thresh 31.996437909361617
target distance 10.0
model initialize at round 888
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 24.]), 'previousTarget': array([20., 24.]), 'currentState': array([ 9.61089457, 24.36262862,  5.48907638]), 'targetState': array([20, 24], dtype=int32), 'currentDistance': 10.395432220946859}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.916316836896699
{'scaleFactor': 20, 'currentTarget': array([20., 24.]), 'previousTarget': array([20., 24.]), 'currentState': array([19.69549411, 23.94136432,  5.46470064]), 'targetState': array([20, 24], dtype=int32), 'currentDistance': 0.31009994770559435}
episode index:889
target Thresh 31.99647335275567
target distance 10.0
model initialize at round 889
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 25.]), 'previousTarget': array([11., 25.]), 'currentState': array([17.75048748, 16.12743362,  3.41750896]), 'targetState': array([11, 25], dtype=int32), 'currentDistance': 11.148610474869786}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9163345318528903
{'scaleFactor': 20, 'currentTarget': array([11., 25.]), 'previousTarget': array([11., 25.]), 'currentState': array([11.11856607, 25.64604433,  1.38903579]), 'targetState': array([11, 25], dtype=int32), 'currentDistance': 0.6568342209723976}
episode index:890
target Thresh 31.99650844348206
target distance 12.0
model initialize at round 890
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 16.]), 'previousTarget': array([16., 16.]), 'currentState': array([14.66918568,  3.78504128,  0.88192433]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 12.287240666129549}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9163521870897636
{'scaleFactor': 20, 'currentTarget': array([16., 16.]), 'previousTarget': array([16., 16.]), 'currentState': array([15.4138111 , 15.33238304,  2.2227208 ]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 0.8884423665593264}
episode index:891
target Thresh 31.996543185049887
target distance 6.0
model initialize at round 891
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 25.]), 'previousTarget': array([25., 25.]), 'currentState': array([22.30228019, 20.9183481 ,  0.99139476]), 'targetState': array([25, 25], dtype=int32), 'currentDistance': 4.892604054517637}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9164236532477348
{'scaleFactor': 20, 'currentTarget': array([25., 25.]), 'previousTarget': array([25., 25.]), 'currentState': array([24.65210096, 24.12372627,  1.21322627]), 'targetState': array([25, 25], dtype=int32), 'currentDistance': 0.9428093119318824}
episode index:892
target Thresh 31.99657758093334
target distance 8.0
model initialize at round 892
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([19.4037011 , 12.20139909,  2.41588189]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 8.100219607331862}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9164731183728773
{'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([15.11029002, 18.75573755,  2.17494798]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 0.2680075284492358}
episode index:893
target Thresh 31.996611634572034
target distance 11.0
model initialize at round 893
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 12.]), 'previousTarget': array([ 6., 12.]), 'currentState': array([15.32926379,  8.20255522,  2.01094437]), 'targetState': array([ 6, 12], dtype=int32), 'currentDistance': 10.072524501587788}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9165010904433785
{'scaleFactor': 20, 'currentTarget': array([ 6., 12.]), 'previousTarget': array([ 6., 12.]), 'currentState': array([ 5.96201529, 11.9586897 ,  2.01653373]), 'targetState': array([ 6, 12], dtype=int32), 'currentDistance': 0.0561193279496831}
episode index:894
target Thresh 31.99664534937136
target distance 25.0
model initialize at round 894
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 28.]), 'previousTarget': array([14., 28.]), 'currentState': array([17.33807601,  4.87969622,  1.76124087]), 'targetState': array([14, 28], dtype=int32), 'currentDistance': 23.36003420611295}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9164674410369793
{'scaleFactor': 20, 'currentTarget': array([14., 28.]), 'previousTarget': array([14., 28.]), 'currentState': array([14.01162191, 28.471905  ,  2.01208805]), 'targetState': array([14, 28], dtype=int32), 'currentDistance': 0.472048090253292}
episode index:895
target Thresh 31.99667872870283
target distance 10.0
model initialize at round 895
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 12.]), 'previousTarget': array([18., 12.]), 'currentState': array([ 6.47229425, 19.29395239,  4.58451438]), 'targetState': array([18, 12], dtype=int32), 'currentDistance': 13.641471379283859}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9164848494151825
{'scaleFactor': 20, 'currentTarget': array([18., 12.]), 'previousTarget': array([18., 12.]), 'currentState': array([17.45029142, 12.16056673,  5.80900142]), 'targetState': array([18, 12], dtype=int32), 'currentDistance': 0.572678963799233}
episode index:896
target Thresh 31.996711775904398
target distance 10.0
model initialize at round 896
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 22.]), 'previousTarget': array([19., 22.]), 'currentState': array([12.18937144, 13.56606125,  1.24224037]), 'targetState': array([19, 22], dtype=int32), 'currentDistance': 10.840478967928386}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9165233167512861
{'scaleFactor': 20, 'currentTarget': array([19., 22.]), 'previousTarget': array([19., 22.]), 'currentState': array([18.49426041, 21.13565652,  0.82099241]), 'targetState': array([19, 22], dtype=int32), 'currentDistance': 1.0014300696169407}
episode index:897
target Thresh 31.996744494280822
target distance 21.0
model initialize at round 897
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 20.]), 'previousTarget': array([ 6., 20.]), 'currentState': array([25.13643715,  6.55395392,  2.43747461]), 'targetState': array([ 6, 20], dtype=int32), 'currentDistance': 23.388017920260598}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9164897550084852
{'scaleFactor': 20, 'currentTarget': array([ 6., 20.]), 'previousTarget': array([ 6., 20.]), 'currentState': array([ 6.47604876, 19.02278715,  1.74357026]), 'targetState': array([ 6, 20], dtype=int32), 'currentDistance': 1.0869992505652426}
episode index:898
target Thresh 31.99677688710396
target distance 15.0
model initialize at round 898
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 20.]), 'previousTarget': array([ 3., 20.]), 'currentState': array([18.71218132, 22.47514399,  4.13932943]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 15.905941646364292}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9164967126719106
{'scaleFactor': 20, 'currentTarget': array([ 3., 20.]), 'previousTarget': array([ 3., 20.]), 'currentState': array([ 3.29956306, 20.13022112,  3.9081347 ]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 0.32664287953388954}
episode index:899
target Thresh 31.99680895761312
target distance 15.0
model initialize at round 899
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 17.]), 'previousTarget': array([14., 17.]), 'currentState': array([3.30934988, 3.65430665, 2.39591482]), 'targetState': array([14, 17], dtype=int32), 'currentDistance': 17.099635403953574}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9164832519633961
{'scaleFactor': 20, 'currentTarget': array([14., 17.]), 'previousTarget': array([14., 17.]), 'currentState': array([14.31600319, 17.05228812,  1.26343426]), 'targetState': array([14, 17], dtype=int32), 'currentDistance': 0.3202999629779266}
episode index:900
target Thresh 31.99684070901538
target distance 3.0
model initialize at round 900
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([17.70513082, 11.54941214,  3.56958723]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 2.30394207745138}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9165648465783091
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.96848278, 10.5628136 ,  3.74866305]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.56369538083193}
episode index:901
target Thresh 31.996872144485913
target distance 5.0
model initialize at round 901
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 24.]), 'previousTarget': array([21., 24.]), 'currentState': array([17.67885727, 28.11758553,  5.34311015]), 'targetState': array([21, 24], dtype=int32), 'currentDistance': 5.290037776386695}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9166244188104838
{'scaleFactor': 20, 'currentTarget': array([21., 24.]), 'previousTarget': array([21., 24.]), 'currentState': array([21.13261379, 23.25931724,  5.53957475]), 'targetState': array([21, 24], dtype=int32), 'currentDistance': 0.7524608740560919}
episode index:902
target Thresh 31.996903267168285
target distance 16.0
model initialize at round 902
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 26.]), 'previousTarget': array([ 6., 26.]), 'currentState': array([ 7.78892865, 11.73408133,  0.79417908]), 'targetState': array([ 6, 26], dtype=int32), 'currentDistance': 14.377645882735832}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9166209778677078
{'scaleFactor': 20, 'currentTarget': array([ 6., 26.]), 'previousTarget': array([ 6., 26.]), 'currentState': array([ 6.57108426, 26.08344842,  0.5200157 ]), 'targetState': array([ 6, 26], dtype=int32), 'currentDistance': 0.5771489143714491}
episode index:903
target Thresh 31.996934080174796
target distance 17.0
model initialize at round 903
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([10.28841858, 21.08275835,  5.97206659]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 15.711799374960542}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9166277518904514
{'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([25.63803918, 21.02821711,  0.94518448]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 0.3630590055837425}
episode index:904
target Thresh 31.99696458658677
target distance 6.0
model initialize at round 904
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  8.]), 'previousTarget': array([17.,  8.]), 'currentState': array([13.64741452, 14.34411259,  5.47910494]), 'targetState': array([17,  8], dtype=int32), 'currentDistance': 7.175485622609206}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9166763356010696
{'scaleFactor': 20, 'currentTarget': array([17.,  8.]), 'previousTarget': array([17.,  8.]), 'currentState': array([17.13111509,  7.39315151,  5.74094852]), 'targetState': array([17,  8], dtype=int32), 'currentDistance': 0.6208512326963187}
episode index:905
target Thresh 31.99699478945487
target distance 17.0
model initialize at round 905
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 25.]), 'previousTarget': array([19., 25.]), 'currentState': array([ 1.83891389, 11.32342136,  5.62437141]), 'targetState': array([19, 25], dtype=int32), 'currentDistance': 21.94428581052207}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9166527836349082
{'scaleFactor': 20, 'currentTarget': array([19., 25.]), 'previousTarget': array([19., 25.]), 'currentState': array([18.20037583, 24.3144033 ,  0.27701843]), 'targetState': array([19, 25], dtype=int32), 'currentDistance': 1.053300361520051}
episode index:906
target Thresh 31.997024691799414
target distance 17.0
model initialize at round 906
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 2.]), 'previousTarget': array([7., 2.]), 'currentState': array([19.62799448, 18.02532624,  4.76926494]), 'targetState': array([7, 2], dtype=int32), 'currentDistance': 20.4028754277876}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9166292836025198
{'scaleFactor': 20, 'currentTarget': array([7., 2.]), 'previousTarget': array([7., 2.]), 'currentState': array([6.58238142, 1.57810042, 4.38520048]), 'targetState': array([7, 2], dtype=int32), 'currentDistance': 0.5936367019084108}
episode index:907
target Thresh 31.997054296610656
target distance 3.0
model initialize at round 907
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([21.4272579 , 18.89507553,  1.56721919]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 4.90217498133482}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9166883912196977
{'scaleFactor': 20, 'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([16.93148232, 21.30975675,  3.70812815]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 0.3172442491447057}
episode index:908
target Thresh 31.997083606849106
target distance 14.0
model initialize at round 908
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 29.]), 'previousTarget': array([18., 29.]), 'currentState': array([14.9922583 , 16.99382701,  1.43733096]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 12.377184654650108}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9167156648810632
{'scaleFactor': 20, 'currentTarget': array([18., 29.]), 'previousTarget': array([18., 29.]), 'currentState': array([17.8053854 , 28.53140413,  1.63363304]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 0.5074021350756317}
episode index:909
target Thresh 31.997112625445812
target distance 20.0
model initialize at round 909
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  4.]), 'previousTarget': array([20.,  4.]), 'currentState': array([13.56378159, 24.62207334,  5.65179426]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 21.603120518786348}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9166921732210386
{'scaleFactor': 20, 'currentTarget': array([20.,  4.]), 'previousTarget': array([20.,  4.]), 'currentState': array([19.84375944,  4.30334171,  5.04344846]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 0.3412144570011367}
episode index:910
target Thresh 31.997141355302652
target distance 14.0
model initialize at round 910
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 24.]), 'previousTarget': array([12., 24.]), 'currentState': array([ 6.35556884, 10.3580981 ,  1.91717601]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 14.763505361426585}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9166988170423415
{'scaleFactor': 20, 'currentTarget': array([12., 24.]), 'previousTarget': array([12., 24.]), 'currentState': array([12.5249675 , 24.60920956,  1.19340116]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 0.8041934833580586}
episode index:911
target Thresh 31.997169799292642
target distance 12.0
model initialize at round 911
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 10.]), 'previousTarget': array([18., 10.]), 'currentState': array([24.63761186, 21.61190789,  5.04049153]), 'targetState': array([18, 10], dtype=int32), 'currentDistance': 13.375137235051044}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9167156663086404
{'scaleFactor': 20, 'currentTarget': array([18., 10.]), 'previousTarget': array([18., 10.]), 'currentState': array([17.95688707,  9.97874668,  4.59762475]), 'targetState': array([18, 10], dtype=int32), 'currentDistance': 0.048066917891683256}
episode index:912
target Thresh 31.997197960260205
target distance 15.0
model initialize at round 912
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([ 7.76960407, 24.49669558,  0.08585167]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 18.29421159210379}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9167121631116799
{'scaleFactor': 20, 'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([21.49480418, 13.65444668,  5.63694302]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 0.8267546697732147}
episode index:913
target Thresh 31.997225841021457
target distance 1.0
model initialize at round 913
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([5.63669843, 9.60804567, 5.03814423]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 0.8804001500686081}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9168032876596978
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([5.63669843, 9.60804567, 5.03814423]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 0.8804001500686081}
episode index:914
target Thresh 31.9972534443645
target distance 15.0
model initialize at round 914
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([24.51154294, 14.71779354,  3.52253485]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 14.31151174558297}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9168199675069627
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([11.68217964, 10.04699402,  3.65001146]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.6837963867320963}
episode index:915
target Thresh 31.99728077304969
target distance 10.0
model initialize at round 915
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([ 9.70944632, 12.98251109,  5.56555551]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 11.568823553674456}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9168468891029168
{'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([17.06944647,  3.80337462,  5.67505915]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 0.20852901686498898}
episode index:916
target Thresh 31.99730782980992
target distance 9.0
model initialize at round 916
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([17.64554831,  1.00107623,  2.76705313]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 11.808261425052942}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9168737519821949
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.01246493,  9.76999043,  2.62838275]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.23034708303053483}
episode index:917
target Thresh 31.997334617350887
target distance 14.0
model initialize at round 917
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 18.]), 'previousTarget': array([18., 18.]), 'currentState': array([12.68086212,  4.08420581,  1.06005495]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 14.897736594176646}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9168801473443362
{'scaleFactor': 20, 'currentTarget': array([18., 18.]), 'previousTarget': array([18., 18.]), 'currentState': array([18.30705725, 18.72817324,  0.89934826]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 0.7902660462076513}
episode index:918
target Thresh 31.99736113835137
target distance 2.0
model initialize at round 918
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 25.]), 'previousTarget': array([14., 25.]), 'currentState': array([14.14311465, 25.28817313,  5.74596524]), 'targetState': array([14, 25], dtype=int32), 'currentDistance': 0.3217538692800009}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9169705933211106
{'scaleFactor': 20, 'currentTarget': array([14., 25.]), 'previousTarget': array([14., 25.]), 'currentState': array([14.14311465, 25.28817313,  5.74596524]), 'targetState': array([14, 25], dtype=int32), 'currentDistance': 0.3217538692800009}
episode index:919
target Thresh 31.997387395463484
target distance 23.0
model initialize at round 919
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 20.]), 'previousTarget': array([ 3., 20.]), 'currentState': array([27.00784287, 19.34782819,  1.93873471]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 24.016699346604394}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.9169087321921108
{'scaleFactor': 20, 'currentTarget': array([ 3., 20.]), 'previousTarget': array([ 3., 20.]), 'currentState': array([ 2.75141807, 20.08860132,  3.24093097]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 0.2638999268184828}
episode index:920
target Thresh 31.99741339131297
target distance 24.0
model initialize at round 920
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 28.]), 'previousTarget': array([14., 28.]), 'currentState': array([21.39509612,  4.50662771,  1.82581806]), 'targetState': array([14, 28], dtype=int32), 'currentDistance': 24.629778487425792}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.916865965949773
{'scaleFactor': 20, 'currentTarget': array([14., 28.]), 'previousTarget': array([14., 28.]), 'currentState': array([14.05048282, 28.49012158,  1.381752  ]), 'targetState': array([14, 28], dtype=int32), 'currentDistance': 0.4927146047554498}
episode index:921
target Thresh 31.997439128499433
target distance 19.0
model initialize at round 921
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 18.]), 'previousTarget': array([ 4., 18.]), 'currentState': array([21.47672601,  3.71555883,  3.71243608]), 'targetState': array([ 4, 18], dtype=int32), 'currentDistance': 22.571690489290983}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9168232924758567
{'scaleFactor': 20, 'currentTarget': array([ 4., 18.]), 'previousTarget': array([ 4., 18.]), 'currentState': array([ 4.43721173, 17.15610266,  2.15005141]), 'targetState': array([ 4, 18], dtype=int32), 'currentDistance': 0.950429807931982}
episode index:922
target Thresh 31.99746460959661
target distance 3.0
model initialize at round 922
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([21.62384034,  0.43633109,  4.08291936]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 4.438988292273945}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9168918479552978
{'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([18.92182746,  2.23772604,  2.13084034]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 1.1961720031701701}
episode index:923
target Thresh 31.997489837152635
target distance 1.0
model initialize at round 923
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  8.]), 'previousTarget': array([21.,  8.]), 'currentState': array([19.90722848,  8.68041924,  0.61596131]), 'targetState': array([21,  8], dtype=int32), 'currentDistance': 1.2872917101037442}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9169709693319695
{'scaleFactor': 20, 'currentTarget': array([21.,  8.]), 'previousTarget': array([21.,  8.]), 'currentState': array([21.47515651,  8.06700356,  4.9014054 ]), 'targetState': array([21,  8], dtype=int32), 'currentDistance': 0.47985746022422476}
episode index:924
target Thresh 31.997514813690284
target distance 15.0
model initialize at round 924
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 14.]), 'previousTarget': array([ 7., 14.]), 'currentState': array([11.139893  , 27.06162174,  4.36651236]), 'targetState': array([ 7, 14], dtype=int32), 'currentDistance': 13.701995351482912}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9169872875790778
{'scaleFactor': 20, 'currentTarget': array([ 7., 14.]), 'previousTarget': array([ 7., 14.]), 'currentState': array([ 6.92321523, 14.03888702,  4.89801321]), 'targetState': array([ 7, 14], dtype=int32), 'currentDistance': 0.08607032843215952}
episode index:925
target Thresh 31.99753954170723
target distance 13.0
model initialize at round 925
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 12.]), 'previousTarget': array([27., 12.]), 'currentState': array([15.7141501 , 11.03026314,  5.34472967]), 'targetState': array([27, 12], dtype=int32), 'currentDistance': 11.32743561537851}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9170035705815918
{'scaleFactor': 20, 'currentTarget': array([27., 12.]), 'previousTarget': array([27., 12.]), 'currentState': array([27.52977239, 11.67801515,  0.89636272]), 'targetState': array([27, 12], dtype=int32), 'currentDistance': 0.61994598942267}
episode index:926
target Thresh 31.997564023676293
target distance 19.0
model initialize at round 926
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([16.76501076, 23.14332395,  3.40471756]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 24.975168521639308}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.9169421410066831
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.30696654, 2.75645765, 3.60591276]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.734580327027572}
episode index:927
target Thresh 31.9975882620457
target distance 4.0
model initialize at round 927
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  2.]), 'previousTarget': array([26.,  2.]), 'currentState': array([23.47182267,  3.18380948,  0.50365442]), 'targetState': array([26,  2], dtype=int32), 'currentDistance': 2.7916098423972286}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9170101990443914
{'scaleFactor': 20, 'currentTarget': array([26.,  2.]), 'previousTarget': array([26.,  2.]), 'currentState': array([26.38026635,  1.36042671,  0.24456173]), 'targetState': array([26,  2], dtype=int32), 'currentDistance': 0.7440809672839406}
episode index:928
target Thresh 31.997612259239297
target distance 15.0
model initialize at round 928
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  6.]), 'previousTarget': array([11.,  6.]), 'currentState': array([25.00832643, 17.64023105,  3.07227087]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 18.213406822170203}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9169868707938148
{'scaleFactor': 20, 'currentTarget': array([11.,  6.]), 'previousTarget': array([11.,  6.]), 'currentState': array([11.14011855,  6.03810859,  4.10632655]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 0.14520837844455775}
episode index:929
target Thresh 31.99763601765683
target distance 18.0
model initialize at round 929
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  7.]), 'previousTarget': array([20.,  7.]), 'currentState': array([11.68213818, 25.05290772,  5.3046276 ]), 'targetState': array([20,  7], dtype=int32), 'currentDistance': 19.87697920679698}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.91697331724996
{'scaleFactor': 20, 'currentTarget': array([20.,  7.]), 'previousTarget': array([20.,  7.]), 'currentState': array([20.0418499 ,  7.66308075,  4.93928992]), 'targetState': array([20,  7], dtype=int32), 'currentDistance': 0.6644000983384013}
episode index:930
target Thresh 31.99765953967416
target distance 15.0
model initialize at round 930
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  8.]), 'previousTarget': array([20.,  8.]), 'currentState': array([ 5.13588415, 20.32252464,  5.80321765]), 'targetState': array([20,  8], dtype=int32), 'currentDistance': 19.307681203169828}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9169597928222036
{'scaleFactor': 20, 'currentTarget': array([20.,  8.]), 'previousTarget': array([20.,  8.]), 'currentState': array([20.06284046,  7.80408029,  5.76369907]), 'targetState': array([20,  8], dtype=int32), 'currentDistance': 0.20575095131332807}
episode index:931
target Thresh 31.997682827643505
target distance 14.0
model initialize at round 931
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 19.]), 'previousTarget': array([27., 19.]), 'currentState': array([14.16401673,  6.2155053 ,  1.81703299]), 'targetState': array([27, 19], dtype=int32), 'currentDistance': 18.11645029295846}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9169365937464917
{'scaleFactor': 20, 'currentTarget': array([27., 19.]), 'previousTarget': array([27., 19.]), 'currentState': array([26.92630778, 19.64361706,  6.04794335]), 'targetState': array([27, 19], dtype=int32), 'currentDistance': 0.647822091036647}
episode index:932
target Thresh 31.997705883893683
target distance 17.0
model initialize at round 932
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([ 5.5742546 , 18.40492811,  4.91178507]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 17.566913156609083}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9169231376706743
{'scaleFactor': 20, 'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([20.44515573, 10.9133311 ,  4.27343554]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 1.0686560969888759}
episode index:933
target Thresh 31.99772871073034
target distance 9.0
model initialize at round 933
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 24.]), 'previousTarget': array([27., 24.]), 'currentState': array([19.64466783, 14.64299046,  0.79624527]), 'targetState': array([27, 24], dtype=int32), 'currentDistance': 11.901871235495673}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9169494299744541
{'scaleFactor': 20, 'currentTarget': array([27., 24.]), 'previousTarget': array([27., 24.]), 'currentState': array([26.75925357, 23.93001578,  1.24122417]), 'targetState': array([27, 24], dtype=int32), 'currentDistance': 0.2507122533201486}
episode index:934
target Thresh 31.997751310436175
target distance 15.0
model initialize at round 934
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 12.]), 'previousTarget': array([ 2., 12.]), 'currentState': array([ 3.16125467, 25.01441083,  4.94875115]), 'targetState': array([ 2, 12], dtype=int32), 'currentDistance': 13.066116550864816}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9169655967316012
{'scaleFactor': 20, 'currentTarget': array([ 2., 12.]), 'previousTarget': array([ 2., 12.]), 'currentState': array([ 1.90007543, 11.33883477,  4.2611758 ]), 'targetState': array([ 2, 12], dtype=int32), 'currentDistance': 0.66867360269759}
episode index:935
target Thresh 31.997773685271184
target distance 4.0
model initialize at round 935
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 18.]), 'previousTarget': array([13., 18.]), 'currentState': array([15.96172234, 20.37302931,  3.67918593]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 3.7951373231379137}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9170330480171444
{'scaleFactor': 20, 'currentTarget': array([13., 18.]), 'previousTarget': array([13., 18.]), 'currentState': array([12.85405504, 17.91165704,  4.11381734]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 0.17060014757375602}
episode index:936
target Thresh 31.99779583747286
target distance 20.0
model initialize at round 936
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 15.]), 'previousTarget': array([26., 15.]), 'currentState': array([7.07246422, 9.32199416, 1.86638087]), 'targetState': array([26, 15], dtype=int32), 'currentDistance': 19.76085425875875}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9170003391843792
{'scaleFactor': 20, 'currentTarget': array([26., 15.]), 'previousTarget': array([26., 15.]), 'currentState': array([26.03349393, 14.82180189,  6.06203467]), 'targetState': array([26, 15], dtype=int32), 'currentDistance': 0.1813185269768558}
episode index:937
target Thresh 31.99781776925645
target distance 4.0
model initialize at round 937
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 22.]), 'previousTarget': array([15., 22.]), 'currentState': array([17.65538327, 22.01212354,  3.50635123]), 'targetState': array([15, 22], dtype=int32), 'currentDistance': 2.6554109449523158}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.91707816398269
{'scaleFactor': 20, 'currentTarget': array([15., 22.]), 'previousTarget': array([15., 22.]), 'currentState': array([15.71665268, 22.11354442,  2.66387856]), 'targetState': array([15, 22], dtype=int32), 'currentDistance': 0.7255917562558651}
episode index:938
target Thresh 31.997839482815145
target distance 15.0
model initialize at round 938
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 18.]), 'previousTarget': array([27., 18.]), 'currentState': array([13.23530634,  5.14298133,  1.75659769]), 'targetState': array([27, 18], dtype=int32), 'currentDistance': 18.835331707154022}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9170550117891608
{'scaleFactor': 20, 'currentTarget': array([27., 18.]), 'previousTarget': array([27., 18.]), 'currentState': array([26.6813395 , 17.86946071,  6.04091263]), 'targetState': array([27, 18], dtype=int32), 'currentDistance': 0.34436176044102473}
episode index:939
target Thresh 31.997860980320322
target distance 18.0
model initialize at round 939
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 12.]), 'previousTarget': array([27., 12.]), 'currentState': array([ 9.19163637, 17.3279762 ,  5.83650446]), 'targetState': array([27, 12], dtype=int32), 'currentDistance': 18.5883066879254}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9170512482101124
{'scaleFactor': 20, 'currentTarget': array([27., 12.]), 'previousTarget': array([27., 12.]), 'currentState': array([26.0025603 , 12.56323603,  5.7156904 ]), 'targetState': array([27, 12], dtype=int32), 'currentDistance': 1.1454784118641277}
episode index:940
target Thresh 31.99788226392175
target distance 25.0
model initialize at round 940
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([26.83593067, 13.67495353,  2.67843938]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 25.476038741429583}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.9169906819045133
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.94437007, 8.25226543, 3.28576217]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.2583264115992916}
episode index:941
target Thresh 31.997903335747804
target distance 1.0
model initialize at round 941
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([13.66432714, 12.52539317,  5.68828386]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 3.070091766182916}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9170576769343386
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.81801758, 11.10696998,  2.29283971]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.8249820238417965}
episode index:942
target Thresh 31.997924197905686
target distance 13.0
model initialize at round 942
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 19.]), 'previousTarget': array([ 9., 19.]), 'currentState': array([22.24698868, 17.66474763,  2.43350673]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 13.314113109287508}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9170637077058058
{'scaleFactor': 20, 'currentTarget': array([ 9., 19.]), 'previousTarget': array([ 9., 19.]), 'currentState': array([ 8.7923919 , 19.39441497,  2.89902172]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 0.44571773149697785}
episode index:943
target Thresh 31.997944852481627
target distance 9.0
model initialize at round 943
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 11.]), 'previousTarget': array([18., 11.]), 'currentState': array([22.35784131,  3.09413696,  2.26729321]), 'targetState': array([18, 11], dtype=int32), 'currentDistance': 9.02737233791704}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9170996466276218
{'scaleFactor': 20, 'currentTarget': array([18., 11.]), 'previousTarget': array([18., 11.]), 'currentState': array([17.58214944, 11.73463255,  2.02981929]), 'targetState': array([18, 11], dtype=int32), 'currentDistance': 0.8451532796799827}
episode index:944
target Thresh 31.997965301541107
target distance 6.0
model initialize at round 944
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  5.]), 'previousTarget': array([24.,  5.]), 'currentState': array([17.2015119 ,  7.51851262,  5.22804785]), 'targetState': array([24,  5], dtype=int32), 'currentDistance': 7.249989396031925}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9171456745253703
{'scaleFactor': 20, 'currentTarget': array([24.,  5.]), 'previousTarget': array([24.,  5.]), 'currentState': array([23.30378299,  5.06249003,  4.44969177]), 'targetState': array([24,  5], dtype=int32), 'currentDistance': 0.6990158278584233}
episode index:945
target Thresh 31.99798554712904
target distance 10.0
model initialize at round 945
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([ 7.37779205, 24.55186   ,  4.4211235 ]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 12.867808643009846}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9171614458502981
{'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([15.13422576, 15.67364078,  0.16303795]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 1.096976273467315}
episode index:946
target Thresh 31.99800559127001
target distance 10.0
model initialize at round 946
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 26.]), 'previousTarget': array([17., 26.]), 'currentState': array([25.16830049, 17.75336829,  2.50721788]), 'targetState': array([17, 26], dtype=int32), 'currentDistance': 11.60724202664183}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9171871255794962
{'scaleFactor': 20, 'currentTarget': array([17., 26.]), 'previousTarget': array([17., 26.]), 'currentState': array([16.9814129 , 26.06019049,  2.69196502]), 'targetState': array([17, 26], dtype=int32), 'currentDistance': 0.06299504381888851}
episode index:947
target Thresh 31.998025435968447
target distance 7.0
model initialize at round 947
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 21.]), 'previousTarget': array([18., 21.]), 'currentState': array([25.52146345, 18.39885544,  4.01883209]), 'targetState': array([18, 21], dtype=int32), 'currentDistance': 7.958540406723115}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9172329155419652
{'scaleFactor': 20, 'currentTarget': array([18., 21.]), 'previousTarget': array([18., 21.]), 'currentState': array([18.93197862, 20.07449932,  3.07981801]), 'targetState': array([18, 21], dtype=int32), 'currentDistance': 1.3134441961096925}
episode index:948
target Thresh 31.998045083208837
target distance 18.0
model initialize at round 948
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([ 3.22198563, 18.66826571,  0.42850971]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 19.778691189389125}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9172004097002098
{'scaleFactor': 20, 'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([20.84142972,  9.72091727,  5.27359379]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 0.32098551603252484}
episode index:949
target Thresh 31.998064534955912
target distance 8.0
model initialize at round 949
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 11.]), 'previousTarget': array([17., 11.]), 'currentState': array([23.72859659,  9.10268828,  3.43714035]), 'targetState': array([17, 11], dtype=int32), 'currentDistance': 6.990980175107261}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9172460892794727
{'scaleFactor': 20, 'currentTarget': array([17., 11.]), 'previousTarget': array([17., 11.]), 'currentState': array([16.20870826, 11.26429667,  2.98010144]), 'targetState': array([17, 11], dtype=int32), 'currentDistance': 0.8342633578487424}
episode index:950
target Thresh 31.998083793154876
target distance 7.0
model initialize at round 950
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.50215153, 18.75889978,  5.74099106]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 7.7748555528153}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9172815718879066
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.0477048 , 10.33610251,  3.82033152]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.6656092146169743}
episode index:951
target Thresh 31.998102859731556
target distance 12.0
model initialize at round 951
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 10.]), 'previousTarget': array([27., 10.]), 'currentState': array([16.58633878,  4.21791813,  0.64225151]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 11.911205264179221}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9173069905617648
{'scaleFactor': 20, 'currentTarget': array([27., 10.]), 'previousTarget': array([27., 10.]), 'currentState': array([27.00805994,  9.83486282,  1.00006689]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 0.16533375937731035}
episode index:952
target Thresh 31.998121736592626
target distance 12.0
model initialize at round 952
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 12.]), 'previousTarget': array([14., 12.]), 'currentState': array([ 3.65700903, 13.29446422,  1.18587225]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 10.423679768724394}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9173323558910819
{'scaleFactor': 20, 'currentTarget': array([14., 12.]), 'previousTarget': array([14., 12.]), 'currentState': array([13.90867198, 11.93846834,  5.68264741]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 0.11012244165363178}
episode index:953
target Thresh 31.99814042562579
target distance 3.0
model initialize at round 953
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([8.31263766, 7.05326818, 5.94939322]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 4.065307519236431}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9173981500672966
{'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([8.17481324, 3.5768269 , 4.9060669 ]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 0.6027345516771606}
episode index:954
target Thresh 31.998158928699972
target distance 11.0
model initialize at round 954
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  6.]), 'previousTarget': array([13.,  6.]), 'currentState': array([22.41067567,  5.55356684,  3.81643021]), 'targetState': array([13,  6], dtype=int32), 'currentDistance': 9.421258888415323}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9174333248315194
{'scaleFactor': 20, 'currentTarget': array([13.,  6.]), 'previousTarget': array([13.,  6.]), 'currentState': array([12.91072553,  6.2373371 ,  3.33739839]), 'targetState': array([13,  6], dtype=int32), 'currentDistance': 0.2535721420007394}
episode index:955
target Thresh 31.998177247665488
target distance 8.0
model initialize at round 955
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 22.]), 'previousTarget': array([21., 22.]), 'currentState': array([18.51157893, 14.73994406,  1.46522635]), 'targetState': array([21, 22], dtype=int32), 'currentDistance': 7.674675996120744}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9174784740837878
{'scaleFactor': 20, 'currentTarget': array([21., 22.]), 'previousTarget': array([21., 22.]), 'currentState': array([21.19398783, 22.16750531,  1.47737911]), 'targetState': array([21, 22], dtype=int32), 'currentDistance': 0.25629925551086963}
episode index:956
target Thresh 31.99819538435425
target distance 8.0
model initialize at round 956
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([26.1850021 ,  3.19505569,  1.79962223]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 9.896035682149122}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9175134914043899
{'scaleFactor': 20, 'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([19.8271097 ,  9.12243929,  1.7876012 ]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 1.2059117891177262}
episode index:957
target Thresh 31.998213340579944
target distance 25.0
model initialize at round 957
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 29.]), 'previousTarget': array([12., 29.]), 'currentState': array([10.43113923,  3.11443318,  0.45588272]), 'targetState': array([12, 29], dtype=int32), 'currentDistance': 25.93306564430378}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.9174535173576643
{'scaleFactor': 20, 'currentTarget': array([12., 29.]), 'previousTarget': array([12., 29.]), 'currentState': array([12.34324036, 28.29701978,  2.56823605]), 'targetState': array([12, 29], dtype=int32), 'currentDistance': 0.7823011748670413}
episode index:958
target Thresh 31.998231118138207
target distance 13.0
model initialize at round 958
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 15.]), 'previousTarget': array([ 3., 15.]), 'currentState': array([11.23545283, 26.15667826,  4.43547446]), 'targetState': array([ 3, 15], dtype=int32), 'currentDistance': 13.867016735910857}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9174687538858701
{'scaleFactor': 20, 'currentTarget': array([ 3., 15.]), 'previousTarget': array([ 3., 15.]), 'currentState': array([ 3.16654615, 15.33774656,  4.39615054]), 'targetState': array([ 3, 15], dtype=int32), 'currentDistance': 0.37657716173566597}
episode index:959
target Thresh 31.99824871880681
target distance 17.0
model initialize at round 959
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 15.]), 'previousTarget': array([ 8., 15.]), 'currentState': array([25.66580334, 22.45433057,  4.10912323]), 'targetState': array([ 8, 15], dtype=int32), 'currentDistance': 19.174140182120453}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9174551219287065
{'scaleFactor': 20, 'currentTarget': array([ 8., 15.]), 'previousTarget': array([ 8., 15.]), 'currentState': array([ 7.51778892, 15.07829786,  3.63453024]), 'targetState': array([ 8, 15], dtype=int32), 'currentDistance': 0.488526434953772}
episode index:960
target Thresh 31.998266144345834
target distance 19.0
model initialize at round 960
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 29.]), 'previousTarget': array([ 6., 29.]), 'currentState': array([21.74947196, 11.66421867,  2.73021233]), 'targetState': array([ 6, 29], dtype=int32), 'currentDistance': 23.421681861716813}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9174227907630327
{'scaleFactor': 20, 'currentTarget': array([ 6., 29.]), 'previousTarget': array([ 6., 29.]), 'currentState': array([ 6.14569764, 28.74563232,  2.39429576]), 'targetState': array([ 6, 29], dtype=int32), 'currentDistance': 0.29313942051427616}
episode index:961
target Thresh 31.998283396497847
target distance 6.0
model initialize at round 961
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([26.84203408,  6.45717765,  2.05682079]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 7.400445739561613}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9174676693693081
{'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([21.00523645, 11.23569704,  2.85924027]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 0.23575519710953008}
episode index:962
target Thresh 31.99830047698808
target distance 20.0
model initialize at round 962
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 23.]), 'previousTarget': array([ 7., 23.]), 'currentState': array([28.64183723,  7.36980888,  1.23154372]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 26.69591715717585}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9174170755410628
{'scaleFactor': 20, 'currentTarget': array([ 7., 23.]), 'previousTarget': array([ 7., 23.]), 'currentState': array([ 7.15398247, 22.92642947,  2.71300474]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 0.17065527252700172}
episode index:963
target Thresh 31.998317387524597
target distance 3.0
model initialize at round 963
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 24.]), 'previousTarget': array([12., 24.]), 'currentState': array([12.6170073 , 25.43421171,  4.07776165]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 1.5613011360173608}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9174923690311654
{'scaleFactor': 20, 'currentTarget': array([12., 24.]), 'previousTarget': array([12., 24.]), 'currentState': array([11.66398763, 23.68304873,  4.3529774 ]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 0.461911699625865}
episode index:964
target Thresh 31.99833412979846
target distance 5.0
model initialize at round 964
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([ 7.61096443, 13.9025572 ,  1.18686719]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 3.121778434097634}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9175572474052264
{'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([ 8.13085259, 17.84620771,  1.48145075]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 0.8562650848976036}
episode index:965
target Thresh 31.998350705483922
target distance 3.0
model initialize at round 965
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  6.]), 'previousTarget': array([24.,  6.]), 'currentState': array([21.64764976,  3.44663543,  6.11740565]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 3.471775089697215}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9176219914555315
{'scaleFactor': 20, 'currentTarget': array([24.,  6.]), 'previousTarget': array([24.,  6.]), 'currentState': array([23.91410552,  5.93649261,  6.11552107]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 0.10682251884650396}
episode index:966
target Thresh 31.99836711623855
target distance 6.0
model initialize at round 966
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([14.11740266, 18.74128646,  0.16492122]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 5.10136942501512}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9176764661282765
{'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([15.37535241, 13.96501689,  3.55326021]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 0.6256264286592828}
episode index:967
target Thresh 31.998383363703446
target distance 8.0
model initialize at round 967
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([23.68102511,  6.47376784,  2.03971541]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 6.739254529636576}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9177308282500448
{'scaleFactor': 20, 'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([22.29795202, 12.24582028,  2.02620277]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 0.810902253320879}
episode index:968
target Thresh 31.998399449503363
target distance 15.0
model initialize at round 968
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 28.]), 'previousTarget': array([16., 28.]), 'currentState': array([12.83664479, 14.4602786 ,  2.0605153 ]), 'targetState': array([16, 28], dtype=int32), 'currentDistance': 13.904347227683218}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9177456213559859
{'scaleFactor': 20, 'currentTarget': array([16., 28.]), 'previousTarget': array([16., 28.]), 'currentState': array([15.99942177, 27.54898115,  1.72252212]), 'targetState': array([16, 28], dtype=int32), 'currentDistance': 0.45101922062017197}
episode index:969
target Thresh 31.9984153752469
target distance 2.0
model initialize at round 969
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 4.]), 'previousTarget': array([5., 4.]), 'currentState': array([4.98954095, 3.65800309, 1.49518949]), 'targetState': array([5, 4], dtype=int32), 'currentDistance': 0.34215680039811436}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.917830419684485
{'scaleFactor': 20, 'currentTarget': array([5., 4.]), 'previousTarget': array([5., 4.]), 'currentState': array([4.98954095, 3.65800309, 1.49518949]), 'targetState': array([5, 4], dtype=int32), 'currentDistance': 0.34215680039811436}
episode index:970
target Thresh 31.998431142526638
target distance 9.0
model initialize at round 970
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([14.42153273, 23.65173363,  5.87856996]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 10.76950294340735}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9178645696641096
{'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([21.30220228, 16.62826383,  5.71236689]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 0.9389553204429573}
episode index:971
target Thresh 31.99844675291932
target distance 3.0
model initialize at round 971
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 18.]), 'previousTarget': array([ 3., 18.]), 'currentState': array([ 5.94438873, 16.39302477,  1.98502892]), 'targetState': array([ 3, 18], dtype=int32), 'currentDistance': 3.354369441129317}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9179285978846198
{'scaleFactor': 20, 'currentTarget': array([ 3., 18.]), 'previousTarget': array([ 3., 18.]), 'currentState': array([ 2.73653041, 17.63733585,  1.69758159]), 'targetState': array([ 3, 18], dtype=int32), 'currentDistance': 0.4482650065178034}
episode index:972
target Thresh 31.998462207986
target distance 11.0
model initialize at round 972
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([23.53345913, 11.17438147,  2.64433908]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 9.535053847701706}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9179625767664444
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.78608667, 11.07349027,  3.27097344]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.7895144523268316}
episode index:973
target Thresh 31.998477509272202
target distance 10.0
model initialize at round 973
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 16.]), 'previousTarget': array([12., 16.]), 'currentState': array([20.4280408 , 21.60110928,  3.78635454]), 'targetState': array([12, 16], dtype=int32), 'currentDistance': 10.119500824482063}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9179964858764379
{'scaleFactor': 20, 'currentTarget': array([12., 16.]), 'previousTarget': array([12., 16.]), 'currentState': array([12.21932318, 16.22954936,  4.17209249]), 'targetState': array([12, 16], dtype=int32), 'currentDistance': 0.31748317288199607}
episode index:974
target Thresh 31.99849265830806
target distance 5.0
model initialize at round 974
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([6.85555588, 5.76996116, 0.79661703]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 5.2544807533711175}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9180601817883595
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.1168581 ,  8.06281072,  0.68890977]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 1.2877357541564367}
episode index:975
target Thresh 31.99850765660849
target distance 11.0
model initialize at round 975
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([3.64841391, 6.66070713, 0.8070051 ]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 11.580566477052509}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.918084177656815
{'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([ 5.65667432, 17.16934465,  1.5296475 ]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 0.8988107860314867}
episode index:976
target Thresh 31.99852250567334
target distance 7.0
model initialize at round 976
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 26.]), 'previousTarget': array([ 7., 26.]), 'currentState': array([15.55902535, 20.63389909,  1.39618271]), 'targetState': array([ 7, 26], dtype=int32), 'currentDistance': 10.102076710982297}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9181178581811172
{'scaleFactor': 20, 'currentTarget': array([ 7., 26.]), 'previousTarget': array([ 7., 26.]), 'currentState': array([ 7.55905173, 25.5196619 ,  2.82436725]), 'targetState': array([ 7, 26], dtype=int32), 'currentDistance': 0.737064126498255}
episode index:977
target Thresh 31.998537206987525
target distance 7.0
model initialize at round 977
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([18.65797786, 20.45098314,  6.12406588]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 7.220147470725325}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9181612918741835
{'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([25.05576558, 16.21569756,  4.78201646]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 0.7862824681568675}
episode index:978
target Thresh 31.998551762021187
target distance 19.0
model initialize at round 978
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([21.53253956, 18.82398297,  3.63996112]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 21.72198155427826}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9181379792719205
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.32093264, 6.10581071, 4.00024827]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.3379255360100583}
episode index:979
target Thresh 31.998566172229847
target distance 5.0
model initialize at round 979
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([18.43292121, 16.11731531,  0.45789247]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 4.737267760965445}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9181912048032757
{'scaleFactor': 20, 'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([22.63793174, 13.2734585 ,  4.62915611]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 0.6940723748238999}
episode index:980
target Thresh 31.998580439054535
target distance 22.0
model initialize at round 980
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 19.]), 'previousTarget': array([25., 19.]), 'currentState': array([ 4.98681581, 23.2284123 ,  0.1317528 ]), 'targetState': array([25, 19], dtype=int32), 'currentDistance': 20.454999680014733}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9181771282183679
{'scaleFactor': 20, 'currentTarget': array([25., 19.]), 'previousTarget': array([25., 19.]), 'currentState': array([24.18812057, 19.54491971,  6.03185418]), 'targetState': array([25, 19], dtype=int32), 'currentDistance': 0.9777963441034683}
episode index:981
target Thresh 31.998594563921948
target distance 18.0
model initialize at round 981
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 15.]), 'previousTarget': array([25., 15.]), 'currentState': array([ 7.80080783, 18.48023474,  0.06488872]), 'targetState': array([25, 15], dtype=int32), 'currentDistance': 17.547770372018462}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9181723829223042
{'scaleFactor': 20, 'currentTarget': array([25., 15.]), 'previousTarget': array([25., 15.]), 'currentState': array([25.09226866, 14.81138052,  5.91293472]), 'targetState': array([25, 15], dtype=int32), 'currentDistance': 0.2099781317136577}
episode index:982
target Thresh 31.99860854824458
target distance 12.0
model initialize at round 982
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([14.47445954, 24.38529376,  3.98818159]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 14.750207771574136}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.918186516152197
{'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([ 4.68175312, 14.64084327,  4.13008783]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 0.9356641507320808}
episode index:983
target Thresh 31.998622393420874
target distance 2.0
model initialize at round 983
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 16.]), 'previousTarget': array([18., 16.]), 'currentState': array([19.04188817, 12.61637792,  3.09673715]), 'targetState': array([18, 16], dtype=int32), 'currentDistance': 3.5403996018589914}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9182494363593595
{'scaleFactor': 20, 'currentTarget': array([18., 16.]), 'previousTarget': array([18., 16.]), 'currentState': array([18.13278316, 15.97879534,  2.10682106]), 'targetState': array([18, 16], dtype=int32), 'currentDistance': 0.13446563132815242}
episode index:984
target Thresh 31.998636100835366
target distance 24.0
model initialize at round 984
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 29.]), 'previousTarget': array([ 2., 29.]), 'currentState': array([7.42030257, 6.62964224, 2.32838604]), 'targetState': array([ 2, 29], dtype=int32), 'currentDistance': 23.017658139334596}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.918217086547539
{'scaleFactor': 20, 'currentTarget': array([ 2., 29.]), 'previousTarget': array([ 2., 29.]), 'currentState': array([ 1.98425304, 29.48857026,  1.74831741]), 'targetState': array([ 2, 29], dtype=int32), 'currentDistance': 0.48882396142968476}
episode index:985
target Thresh 31.998649671858804
target distance 4.0
model initialize at round 985
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 19.]), 'previousTarget': array([14., 19.]), 'currentState': array([16.62824285, 17.0933158 ,  1.58444858]), 'targetState': array([14, 19], dtype=int32), 'currentDistance': 3.24701479774524}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9182798481230485
{'scaleFactor': 20, 'currentTarget': array([14., 19.]), 'previousTarget': array([14., 19.]), 'currentState': array([13.77442825, 18.88186205,  1.58266973]), 'targetState': array([14, 19], dtype=int32), 'currentDistance': 0.25463540429667975}
episode index:986
target Thresh 31.9986631078483
target distance 11.0
model initialize at round 986
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 24.]), 'previousTarget': array([26., 24.]), 'currentState': array([22.67676547, 13.1443802 ,  1.09589451]), 'targetState': array([26, 24], dtype=int32), 'currentDistance': 11.35290134782131}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9183033540007364
{'scaleFactor': 20, 'currentTarget': array([26., 24.]), 'previousTarget': array([26., 24.]), 'currentState': array([26.11214311, 24.17007291,  1.4477854 ]), 'targetState': array([26, 24], dtype=int32), 'currentDistance': 0.20371762974174548}
episode index:987
target Thresh 31.99867641014747
target distance 10.0
model initialize at round 987
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 18.]), 'previousTarget': array([26., 18.]), 'currentState': array([16.14484216, 17.67672563,  0.4746263 ]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 9.860458524523974}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9183364377010393
{'scaleFactor': 20, 'currentTarget': array([26., 18.]), 'previousTarget': array([26., 18.]), 'currentState': array([25.87665825, 17.97609227,  0.42646617]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 0.12563744629991286}
episode index:988
target Thresh 31.998689580086552
target distance 17.0
model initialize at round 988
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 15.]), 'previousTarget': array([ 3., 15.]), 'currentState': array([18.05067944,  8.22339998,  2.68583781]), 'targetState': array([ 3, 15], dtype=int32), 'currentDistance': 16.50591589027907}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9183408949879219
{'scaleFactor': 20, 'currentTarget': array([ 3., 15.]), 'previousTarget': array([ 3., 15.]), 'currentState': array([ 3.6854377 , 14.52477506,  2.62223091]), 'targetState': array([ 3, 15], dtype=int32), 'currentDistance': 0.8340644933230754}
episode index:989
target Thresh 31.99870261898255
target distance 4.0
model initialize at round 989
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 20.]), 'previousTarget': array([16., 20.]), 'currentState': array([16.395671  , 15.49155477,  2.43849993]), 'targetState': array([16, 20], dtype=int32), 'currentDistance': 4.52577440012702}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9184032779222775
{'scaleFactor': 20, 'currentTarget': array([16., 20.]), 'previousTarget': array([16., 20.]), 'currentState': array([15.90007523, 19.10852398,  2.44253802]), 'targetState': array([16, 20], dtype=int32), 'currentDistance': 0.8970587840623939}
episode index:990
target Thresh 31.99871552813936
target distance 19.0
model initialize at round 990
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 15.]), 'previousTarget': array([21., 15.]), 'currentState': array([ 3.50875784, 24.74567946,  5.73220951]), 'targetState': array([21, 15], dtype=int32), 'currentDistance': 20.02303225019371}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9183891293825062
{'scaleFactor': 20, 'currentTarget': array([21., 15.]), 'previousTarget': array([21., 15.]), 'currentState': array([20.63730679, 15.29286542,  5.98529941]), 'targetState': array([21, 15], dtype=int32), 'currentDistance': 0.46617219393687714}
episode index:991
target Thresh 31.99872830884792
target distance 4.0
model initialize at round 991
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  7.]), 'previousTarget': array([21.,  7.]), 'currentState': array([18.9336166 ,  7.2401159 ,  6.00804508]), 'targetState': array([21,  7], dtype=int32), 'currentDistance': 2.0802874789319032}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9184613177601447
{'scaleFactor': 20, 'currentTarget': array([21.,  7.]), 'previousTarget': array([21.,  7.]), 'currentState': array([20.91212987,  7.01805626,  0.05490494]), 'targetState': array([21,  7], dtype=int32), 'currentDistance': 0.08970611921148432}
episode index:992
target Thresh 31.998740962386304
target distance 13.0
model initialize at round 992
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 15.]), 'previousTarget': array([13., 15.]), 'currentState': array([11.08639224, 26.66228224,  0.05174213]), 'targetState': array([13, 15], dtype=int32), 'currentDistance': 11.818236832282699}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9184750176897992
{'scaleFactor': 20, 'currentTarget': array([13., 15.]), 'previousTarget': array([13., 15.]), 'currentState': array([12.72677737, 15.47509109,  3.27062488]), 'targetState': array([13, 15], dtype=int32), 'currentDistance': 0.5480530497524443}
episode index:993
target Thresh 31.998753490019876
target distance 4.0
model initialize at round 993
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 26.]), 'previousTarget': array([25., 26.]), 'currentState': array([22.32164887, 22.12460649,  2.05748582]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 4.710863998088822}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.918537014653894
{'scaleFactor': 20, 'currentTarget': array([25., 26.]), 'previousTarget': array([25., 26.]), 'currentState': array([24.01787526, 25.04834575,  2.00739348]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 1.3675579754704061}
episode index:994
target Thresh 31.998765893001412
target distance 4.0
model initialize at round 994
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([5.24634434, 5.3351569 , 5.86929154]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 3.6104815789902664}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9185988870009755
{'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([8.16964809, 2.71037164, 5.88934237]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 0.33565616528602354}
episode index:995
target Thresh 31.998778172571217
target distance 20.0
model initialize at round 995
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 20.]), 'previousTarget': array([ 6., 20.]), 'currentState': array([27.49805338, 18.77689331,  1.48076337]), 'targetState': array([ 6, 20], dtype=int32), 'currentDistance': 21.532818885312764}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9185755329520375
{'scaleFactor': 20, 'currentTarget': array([ 6., 20.]), 'previousTarget': array([ 6., 20.]), 'currentState': array([ 6.64787876, 19.81831121,  3.23832284]), 'targetState': array([ 6, 20], dtype=int32), 'currentDistance': 0.6728727272605612}
episode index:996
target Thresh 31.998790329957263
target distance 9.0
model initialize at round 996
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 29.]), 'previousTarget': array([18., 29.]), 'currentState': array([21.9144787 , 21.64345934,  2.46026415]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 8.33317670885192}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9186176798698389
{'scaleFactor': 20, 'currentTarget': array([18., 29.]), 'previousTarget': array([18., 29.]), 'currentState': array([18.27464333, 28.51954505,  2.3879886 ]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 0.5534129730662226}
episode index:997
target Thresh 31.998802366375298
target distance 15.0
model initialize at round 997
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 14.]), 'previousTarget': array([11., 14.]), 'currentState': array([24.31797059, 15.05625967,  4.11815763]), 'targetState': array([11, 14], dtype=int32), 'currentDistance': 13.359791358080939}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9186218151549672
{'scaleFactor': 20, 'currentTarget': array([11., 14.]), 'previousTarget': array([11., 14.]), 'currentState': array([10.83485092, 14.52496867,  4.20815492]), 'targetState': array([11, 14], dtype=int32), 'currentDistance': 0.5503329250651045}
episode index:998
target Thresh 31.99881428302897
target distance 20.0
model initialize at round 998
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  7.]), 'previousTarget': array([18.,  7.]), 'currentState': array([ 5.87982925, 25.7439723 ,  4.99410272]), 'targetState': array([18,  7], dtype=int32), 'currentDistance': 22.32117910428891}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9185985082872032
{'scaleFactor': 20, 'currentTarget': array([18.,  7.]), 'previousTarget': array([18.,  7.]), 'currentState': array([17.63210537,  7.91206515,  5.31851503]), 'targetState': array([18,  7], dtype=int32), 'currentDistance': 0.9834679922986396}
episode index:999
target Thresh 31.99882608110996
target distance 11.0
model initialize at round 999
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  7.]), 'previousTarget': array([13.,  7.]), 'currentState': array([22.41574786,  2.4320799 ,  2.47580051]), 'targetState': array([13,  7], dtype=int32), 'currentDistance': 10.465285555758511}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9186308998288161
{'scaleFactor': 20, 'currentTarget': array([13.,  7.]), 'previousTarget': array([13.,  7.]), 'currentState': array([13.60414839,  6.62667815,  2.88772361]), 'targetState': array([13,  7], dtype=int32), 'currentDistance': 0.7101862278562115}
episode index:1000
target Thresh 31.99883776179808
target distance 15.0
model initialize at round 1000
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([ 7.36507792, 19.60072833,  4.39111876]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 16.476446918793467}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9186166652385863
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.30755732,  5.15608007,  3.64188457]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.3448949080492722}
episode index:1001
target Thresh 31.998849326261414
target distance 15.0
model initialize at round 1001
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  2.]), 'previousTarget': array([20.,  2.]), 'currentState': array([27.52079235, 16.27918191,  4.83057332]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 16.138691200103665}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9186207850281964
{'scaleFactor': 20, 'currentTarget': array([20.,  2.]), 'previousTarget': array([20.,  2.]), 'currentState': array([20.23818149,  2.54740469,  4.446072  ]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 0.5969776554667923}
episode index:1002
target Thresh 31.99886077565641
target distance 22.0
model initialize at round 1002
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 23.]), 'previousTarget': array([26., 23.]), 'currentState': array([ 5.68179931, 23.06276269,  1.04730147]), 'targetState': array([26, 23], dtype=int32), 'currentDistance': 20.318297623457035}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9185886455333688
{'scaleFactor': 20, 'currentTarget': array([26., 23.]), 'previousTarget': array([26., 23.]), 'currentState': array([25.80586118, 22.88264938,  6.08408916]), 'targetState': array([26, 23], dtype=int32), 'currentDistance': 0.22685028264421545}
episode index:1003
target Thresh 31.998872111128026
target distance 20.0
model initialize at round 1003
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  9.]), 'previousTarget': array([17.,  9.]), 'currentState': array([19.41890695, 28.42334339,  4.5013206 ]), 'targetState': array([17,  9], dtype=int32), 'currentDistance': 19.57338446163297}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9185744955627269
{'scaleFactor': 20, 'currentTarget': array([17.,  9.]), 'previousTarget': array([17.,  9.]), 'currentState': array([16.89898759,  9.00105091,  4.43394126]), 'targetState': array([17,  9], dtype=int32), 'currentDistance': 0.1010178719230188}
episode index:1004
target Thresh 31.998883333809815
target distance 9.0
model initialize at round 1004
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 29.]), 'previousTarget': array([12., 29.]), 'currentState': array([19.54822984, 26.14867662,  2.66195488]), 'targetState': array([12, 29], dtype=int32), 'currentDistance': 8.06881768374341}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9186163080149032
{'scaleFactor': 20, 'currentTarget': array([12., 29.]), 'previousTarget': array([12., 29.]), 'currentState': array([12.18162478, 29.08493876,  2.96135108]), 'targetState': array([12, 29], dtype=int32), 'currentDistance': 0.2005047431340728}
episode index:1005
target Thresh 31.998894444824057
target distance 9.0
model initialize at round 1005
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'previousTarget': array([26., 11.]), 'currentState': array([18.09882794, 11.27474124,  6.13256163]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 7.905947296257349}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9186580373409322
{'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'previousTarget': array([26., 11.]), 'currentState': array([26.02597922, 10.74324623,  0.0483071 ]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 0.2580647549663677}
episode index:1006
target Thresh 31.998905445281856
target distance 17.0
model initialize at round 1006
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([10.52487221,  6.71214664,  1.44691771]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 16.359077062528563}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9186620955902738
{'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([ 9.00627718, 22.15490987,  1.69672814]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 0.8451134414903176}
episode index:1007
target Thresh 31.998916336283273
target distance 10.0
model initialize at round 1007
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 16.]), 'previousTarget': array([14., 16.]), 'currentState': array([ 5.1123329 , 18.73702589,  0.16126698]), 'targetState': array([14, 16], dtype=int32), 'currentDistance': 9.299566505237056}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9186941669735176
{'scaleFactor': 20, 'currentTarget': array([14., 16.]), 'previousTarget': array([14., 16.]), 'currentState': array([14.47518162, 15.72867761,  5.95606839]), 'targetState': array([14, 16], dtype=int32), 'currentDistance': 0.5471868137113818}
episode index:1008
target Thresh 31.998927118917415
target distance 7.0
model initialize at round 1008
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 16.]), 'previousTarget': array([12., 16.]), 'currentState': array([ 8.56914997, 21.78268069,  5.41598678]), 'targetState': array([12, 16], dtype=int32), 'currentDistance': 6.723847701384094}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9187453115057539
{'scaleFactor': 20, 'currentTarget': array([12., 16.]), 'previousTarget': array([12., 16.]), 'currentState': array([11.42741837, 16.60489086,  5.65616767]), 'targetState': array([12, 16], dtype=int32), 'currentDistance': 0.8329121647445391}
episode index:1009
target Thresh 31.998937794262552
target distance 21.0
model initialize at round 1009
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 22.]), 'previousTarget': array([23., 22.]), 'currentState': array([ 3.92605174, 22.3141921 ,  0.5483551 ]), 'targetState': array([23, 22], dtype=int32), 'currentDistance': 19.076535818527354}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9187310904795193
{'scaleFactor': 20, 'currentTarget': array([23., 22.]), 'previousTarget': array([23., 22.]), 'currentState': array([23.30247991, 21.66805887,  6.09075278]), 'targetState': array([23, 22], dtype=int32), 'currentDistance': 0.44908685948916616}
episode index:1010
target Thresh 31.998948363386233
target distance 2.0
model initialize at round 1010
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 18.]), 'previousTarget': array([10., 18.]), 'currentState': array([10.25369363, 17.98324321,  1.48630062]), 'targetState': array([10, 18], dtype=int32), 'currentDistance': 0.2542464291774689}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9188114751575811
{'scaleFactor': 20, 'currentTarget': array([10., 18.]), 'previousTarget': array([10., 18.]), 'currentState': array([10.25369363, 17.98324321,  1.48630062]), 'targetState': array([10, 18], dtype=int32), 'currentDistance': 0.2542464291774689}
episode index:1011
target Thresh 31.998958827345376
target distance 3.0
model initialize at round 1011
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'previousTarget': array([ 2., 17.]), 'currentState': array([ 3.18996694, 18.52387373,  5.21914876]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 1.9334457459017713}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9188818195497179
{'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'previousTarget': array([ 2., 17.]), 'currentState': array([ 2.40839182, 17.03315664,  3.21984828]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 0.4097355773757863}
episode index:1012
target Thresh 31.998969187186383
target distance 13.0
model initialize at round 1012
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  4.]), 'previousTarget': array([24.,  4.]), 'currentState': array([12.27094845,  9.89678736,  0.2951352 ]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 13.127937823090022}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9188948338916304
{'scaleFactor': 20, 'currentTarget': array([24.,  4.]), 'previousTarget': array([24.,  4.]), 'currentState': array([24.43407311,  3.6768677 ,  5.82803375]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 0.5411413389435394}
episode index:1013
target Thresh 31.998979443945252
target distance 6.0
model initialize at round 1013
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 14.]), 'previousTarget': array([ 9., 14.]), 'currentState': array([10.44620185,  9.58924375,  2.91610432]), 'targetState': array([ 9, 14], dtype=int32), 'currentDistance': 4.641796042162305}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9189455283355241
{'scaleFactor': 20, 'currentTarget': array([ 9., 14.]), 'previousTarget': array([ 9., 14.]), 'currentState': array([ 8.69932063, 14.33337384,  0.93753254]), 'targetState': array([ 9, 14], dtype=int32), 'currentDistance': 0.44893897034143515}
episode index:1014
target Thresh 31.998989598647665
target distance 22.0
model initialize at round 1014
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 27.]), 'previousTarget': array([ 7., 27.]), 'currentState': array([1.43952844, 5.63033056, 1.74769783]), 'targetState': array([ 7, 27], dtype=int32), 'currentDistance': 22.081250328208192}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.918922269937419
{'scaleFactor': 20, 'currentTarget': array([ 7., 27.]), 'previousTarget': array([ 7., 27.]), 'currentState': array([ 6.62093449, 26.4238413 ,  1.38881941]), 'targetState': array([ 7, 27], dtype=int32), 'currentDistance': 0.6896734814219156}
episode index:1015
target Thresh 31.998999652309102
target distance 13.0
model initialize at round 1015
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 29.]), 'previousTarget': array([15., 29.]), 'currentState': array([ 3.68003696, 25.90068291,  0.9509527 ]), 'targetState': array([15, 29], dtype=int32), 'currentDistance': 11.736580829615539}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9189444725746861
{'scaleFactor': 20, 'currentTarget': array([15., 29.]), 'previousTarget': array([15., 29.]), 'currentState': array([14.92740789, 28.91222332,  0.4906222 ]), 'targetState': array([15, 29], dtype=int32), 'currentDistance': 0.11390504983720873}
episode index:1016
target Thresh 31.999009605934933
target distance 12.0
model initialize at round 1016
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 25.]), 'previousTarget': array([14., 25.]), 'currentState': array([ 3.29513597, 18.92528567,  0.31734579]), 'targetState': array([14, 25], dtype=int32), 'currentDistance': 12.308382024822675}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.91896663154895
{'scaleFactor': 20, 'currentTarget': array([14., 25.]), 'previousTarget': array([14., 25.]), 'currentState': array([13.51913116, 24.72868731,  0.96078454]), 'targetState': array([14, 25], dtype=int32), 'currentDistance': 0.5521280798062317}
episode index:1017
target Thresh 31.999019460520536
target distance 3.0
model initialize at round 1017
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 25.]), 'previousTarget': array([14., 25.]), 'currentState': array([15.64240705, 23.4414124 ,  2.07898962]), 'targetState': array([14, 25], dtype=int32), 'currentDistance': 2.2642208859646575}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9190364089246387
{'scaleFactor': 20, 'currentTarget': array([14., 25.]), 'previousTarget': array([14., 25.]), 'currentState': array([14.20295394, 24.78179086,  2.71097004]), 'targetState': array([14, 25], dtype=int32), 'currentDistance': 0.2980025727530203}
episode index:1018
target Thresh 31.999029217051373
target distance 9.0
model initialize at round 1018
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([ 0.49176669, 11.25326017,  4.61131978]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 11.157502899489215}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9190491949295282
{'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([7.97184805, 2.79007054, 4.61979942]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 0.21180866844282015}
episode index:1019
target Thresh 31.999038876503104
target distance 4.0
model initialize at round 1019
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 26.]), 'previousTarget': array([10., 26.]), 'currentState': array([ 7.65239192, 28.49092998,  0.63135284]), 'targetState': array([10, 26], dtype=int32), 'currentDistance': 3.4228636908870342}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9191090486599893
{'scaleFactor': 20, 'currentTarget': array([10., 26.]), 'previousTarget': array([10., 26.]), 'currentState': array([10.01907884, 26.07909772,  5.33377528]), 'targetState': array([10, 26], dtype=int32), 'currentDistance': 0.08136615627101838}
episode index:1020
target Thresh 31.999048439841687
target distance 18.0
model initialize at round 1020
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([24.45925276, 28.67711596,  3.73752785]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 22.45145073445896}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9190683160197729
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([12.30002689, 10.25256575,  4.13763929]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.3921805672860438}
episode index:1021
target Thresh 31.99905790802346
target distance 9.0
model initialize at round 1021
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([20.33473802,  2.24349664,  1.98640037]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 9.323932255488142}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9190995505930413
{'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([13.38880034,  7.67201595,  2.53900999]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 0.5086641703504777}
episode index:1022
target Thresh 31.99906728199525
target distance 5.0
model initialize at round 1022
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([18.51545765,  2.20720614,  2.62209415]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 5.943846858473108}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9191495989306824
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.25607402,  7.02669436,  2.66301833]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.25746163078643247}
episode index:1023
target Thresh 31.999076562694462
target distance 15.0
model initialize at round 1023
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 13.]), 'previousTarget': array([12., 13.]), 'currentState': array([ 2.53839891, 28.83433228,  3.63289452]), 'targetState': array([12, 13], dtype=int32), 'currentDistance': 18.445811823386382}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9191351775206025
{'scaleFactor': 20, 'currentTarget': array([12., 13.]), 'previousTarget': array([12., 13.]), 'currentState': array([12.05917433, 12.5738819 ,  5.15795704]), 'targetState': array([12, 13], dtype=int32), 'currentDistance': 0.43020720282159225}
episode index:1024
target Thresh 31.999085751049176
target distance 13.0
model initialize at round 1024
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 22.]), 'previousTarget': array([10., 22.]), 'currentState': array([14.62038245,  9.45469638,  1.28357476]), 'targetState': array([10, 22], dtype=int32), 'currentDistance': 13.369090358885433}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.919138699000512
{'scaleFactor': 20, 'currentTarget': array([10., 22.]), 'previousTarget': array([10., 22.]), 'currentState': array([ 9.98631849, 22.47420878,  1.61046321]), 'targetState': array([10, 22], dtype=int32), 'currentDistance': 0.4744061000237058}
episode index:1025
target Thresh 31.99909484797823
target distance 15.0
model initialize at round 1025
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 13.]), 'previousTarget': array([ 7., 13.]), 'currentState': array([22.5940736 ,  3.57463159,  2.22003014]), 'targetState': array([ 7, 13], dtype=int32), 'currentDistance': 18.22121569044822}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9191243163260562
{'scaleFactor': 20, 'currentTarget': array([ 7., 13.]), 'previousTarget': array([ 7., 13.]), 'currentState': array([ 7.88153928, 12.18770135,  2.24038471]), 'targetState': array([ 7, 13], dtype=int32), 'currentDistance': 1.1987245668330382}
episode index:1026
target Thresh 31.99910385439133
target distance 15.0
model initialize at round 1026
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 2.]), 'previousTarget': array([6., 2.]), 'currentState': array([ 3.10441567, 15.73009681,  4.41820074]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 14.032104867704511}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9191278415238185
{'scaleFactor': 20, 'currentTarget': array([6., 2.]), 'previousTarget': array([6., 2.]), 'currentState': array([5.79286288, 1.50904312, 4.76647525]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 0.5328643753396183}
episode index:1027
target Thresh 31.999112771189118
target distance 16.0
model initialize at round 1027
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 12.]), 'previousTarget': array([ 4., 12.]), 'currentState': array([12.7303528 , 29.10470995,  3.43554914]), 'targetState': array([ 4, 12], dtype=int32), 'currentDistance': 19.20391008554046}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9191046999019653
{'scaleFactor': 20, 'currentTarget': array([ 4., 12.]), 'previousTarget': array([ 4., 12.]), 'currentState': array([ 4.45129145, 12.45912022,  3.90857306]), 'targetState': array([ 4, 12], dtype=int32), 'currentDistance': 0.6437820613726544}
episode index:1028
target Thresh 31.99912159926329
target distance 18.0
model initialize at round 1028
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 22.]), 'previousTarget': array([26., 22.]), 'currentState': array([17.66179246,  5.54739097,  2.17665555]), 'targetState': array([26, 22], dtype=int32), 'currentDistance': 18.444892213517637}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9190816032589689
{'scaleFactor': 20, 'currentTarget': array([26., 22.]), 'previousTarget': array([26., 22.]), 'currentState': array([26.15327897, 21.80076571,  5.84615846]), 'targetState': array([26, 22], dtype=int32), 'currentDistance': 0.2513737187748508}
episode index:1029
target Thresh 31.999130339496656
target distance 6.0
model initialize at round 1029
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 15.]), 'previousTarget': array([23., 15.]), 'currentState': array([21.86241325, 19.75972404,  4.98014951]), 'targetState': array([23, 15], dtype=int32), 'currentDistance': 4.8937793720134355}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9191408444208534
{'scaleFactor': 20, 'currentTarget': array([23., 15.]), 'previousTarget': array([23., 15.]), 'currentState': array([22.63156356, 15.88808478,  5.30517271]), 'targetState': array([23, 15], dtype=int32), 'currentDistance': 0.9614780227176927}
episode index:1030
target Thresh 31.999138992763246
target distance 16.0
model initialize at round 1030
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 15.]), 'previousTarget': array([19., 15.]), 'currentState': array([4.46170529, 5.83902042, 5.79833228]), 'targetState': array([19, 15], dtype=int32), 'currentDistance': 17.183874995860457}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.919126529416574
{'scaleFactor': 20, 'currentTarget': array([19., 15.]), 'previousTarget': array([19., 15.]), 'currentState': array([18.71174328, 14.82701002,  5.87985748]), 'targetState': array([19, 15], dtype=int32), 'currentDistance': 0.33618071366185426}
episode index:1031
target Thresh 31.999147559928396
target distance 12.0
model initialize at round 1031
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 29.]), 'previousTarget': array([20., 29.]), 'currentState': array([28.05545026, 18.31088246,  1.90292614]), 'targetState': array([20, 29], dtype=int32), 'currentDistance': 13.384599835511164}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9191390670313904
{'scaleFactor': 20, 'currentTarget': array([20., 29.]), 'previousTarget': array([20., 29.]), 'currentState': array([19.94498534, 29.33771478,  2.17825576]), 'targetState': array([20, 29], dtype=int32), 'currentDistance': 0.34216646319806426}
episode index:1032
target Thresh 31.999156041848824
target distance 11.0
model initialize at round 1032
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 24.]), 'previousTarget': array([ 8., 24.]), 'currentState': array([13.81930455, 14.57981256,  2.50454235]), 'targetState': array([ 8, 24], dtype=int32), 'currentDistance': 11.072679746123901}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9191606944102574
{'scaleFactor': 20, 'currentTarget': array([ 8., 24.]), 'previousTarget': array([ 8., 24.]), 'currentState': array([ 7.72764758, 24.6041962 ,  2.14024793]), 'targetState': array([ 8, 24], dtype=int32), 'currentDistance': 0.6627434560620318}
episode index:1033
target Thresh 31.999164439372738
target distance 16.0
model initialize at round 1033
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([25.31714962, 17.02006677,  4.13966894]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 16.410430958427323}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9191641605611448
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([11.6792304 ,  9.33367604,  3.64576183]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.7567652501402529}
episode index:1034
target Thresh 31.99917275333989
target distance 5.0
model initialize at round 1034
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([ 5.91989381, 10.53810498,  5.76814841]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 3.442792624315035}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9192230357683322
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([9.31121566, 8.54778238, 6.21035277]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.5489589839254324}
episode index:1035
target Thresh 31.99918098458169
target distance 25.0
model initialize at round 1035
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([ 7.86671967, 27.26864736,  4.73241079]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 24.063397046779077}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9191913386987837
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.90252642,  4.44002154,  5.13389442]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.45068842428030553}
episode index:1036
target Thresh 31.999189133921266
target distance 16.0
model initialize at round 1036
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 16.]), 'previousTarget': array([11., 16.]), 'currentState': array([28.29281987, 10.07749936,  1.7048065 ]), 'targetState': array([11, 16], dtype=int32), 'currentDistance': 18.27888489284192}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9191858670582677
{'scaleFactor': 20, 'currentTarget': array([11., 16.]), 'previousTarget': array([11., 16.]), 'currentState': array([11.87591619, 15.64630826,  2.87570651]), 'targetState': array([11, 16], dtype=int32), 'currentDistance': 0.9446306247621103}
episode index:1037
target Thresh 31.999197202173555
target distance 15.0
model initialize at round 1037
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([13.33784458, 18.11615925,  4.27576816]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 16.700439034594776}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.919189295601013
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.86367697, 5.86361612, 3.86699791]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.221380656994821}
episode index:1038
target Thresh 31.999205190145396
target distance 8.0
model initialize at round 1038
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 13.]), 'previousTarget': array([ 6., 13.]), 'currentState': array([12.60780831, 19.9456164 ,  3.55494368]), 'targetState': array([ 6, 13], dtype=int32), 'currentDistance': 9.586694832153988}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9192199026792605
{'scaleFactor': 20, 'currentTarget': array([ 6., 13.]), 'previousTarget': array([ 6., 13.]), 'currentState': array([ 5.7560419 , 12.88316205,  4.02411404]), 'targetState': array([ 6, 13], dtype=int32), 'currentDistance': 0.27049336342796065}
episode index:1039
target Thresh 31.999213098635586
target distance 14.0
model initialize at round 1039
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 16.]), 'previousTarget': array([18., 16.]), 'currentState': array([9.59848737, 3.90009928, 1.11032152]), 'targetState': array([18, 16], dtype=int32), 'currentDistance': 14.73068267317335}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9192322540689025
{'scaleFactor': 20, 'currentTarget': array([18., 16.]), 'previousTarget': array([18., 16.]), 'currentState': array([17.41783066, 15.1670463 ,  0.83702195]), 'targetState': array([18, 16], dtype=int32), 'currentDistance': 1.0162347200755377}
episode index:1040
target Thresh 31.999220928434983
target distance 16.0
model initialize at round 1040
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 26.]), 'previousTarget': array([18., 26.]), 'currentState': array([ 1.33978637, 15.54806526,  0.96391702]), 'targetState': array([18, 26], dtype=int32), 'currentDistance': 19.667375477824084}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9192179887672117
{'scaleFactor': 20, 'currentTarget': array([18., 26.]), 'previousTarget': array([18., 26.]), 'currentState': array([18.02441829, 25.92946928,  0.63142936]), 'targetState': array([18, 26], dtype=int32), 'currentDistance': 0.07463802374620913}
episode index:1041
target Thresh 31.999228680326578
target distance 5.0
model initialize at round 1041
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([11.21290154, 16.16702671,  6.03965563]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 4.729466392544342}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9192670108509284
{'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([15.54701039, 18.88956432,  0.68757451]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 0.5580469546691816}
episode index:1042
target Thresh 31.99923635508556
target distance 13.0
model initialize at round 1042
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([15.7696921 , 13.33286283,  3.56511211]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 15.248004433233408}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9192703451592477
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.67582514, 4.94737076, 3.54362049]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.32841921022150566}
episode index:1043
target Thresh 31.999243953479418
target distance 18.0
model initialize at round 1043
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 14.]), 'previousTarget': array([21., 14.]), 'currentState': array([ 4.67887334, 16.88264412,  0.94021195]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 16.57373864438288}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9192560843640845
{'scaleFactor': 20, 'currentTarget': array([21., 14.]), 'previousTarget': array([21., 14.]), 'currentState': array([20.94909819, 13.45119984,  0.18879479]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 0.5511557069311818}
episode index:1044
target Thresh 31.99925147626799
target distance 3.0
model initialize at round 1044
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 29.]), 'previousTarget': array([ 7., 29.]), 'currentState': array([ 6.39604308, 25.49037474,  2.43923891]), 'targetState': array([ 7, 29], dtype=int32), 'currentDistance': 3.5612123490427483}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9193143082067982
{'scaleFactor': 20, 'currentTarget': array([ 7., 29.]), 'previousTarget': array([ 7., 29.]), 'currentState': array([ 7.42457233, 28.81987025,  1.78772748]), 'targetState': array([ 7, 29], dtype=int32), 'currentDistance': 0.4612031942091173}
episode index:1045
target Thresh 31.999258924203566
target distance 9.0
model initialize at round 1045
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 20.]), 'previousTarget': array([23., 20.]), 'currentState': array([12.90729372, 27.72000739,  5.01576138]), 'targetState': array([23, 20], dtype=int32), 'currentDistance': 12.706739710141512}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9193264984933185
{'scaleFactor': 20, 'currentTarget': array([23., 20.]), 'previousTarget': array([23., 20.]), 'currentState': array([22.97607617, 19.9468676 ,  5.52861445]), 'targetState': array([23, 20], dtype=int32), 'currentDistance': 0.05827007622809901}
episode index:1046
target Thresh 31.999266298030946
target distance 3.0
model initialize at round 1046
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 12.]), 'previousTarget': array([14., 12.]), 'currentState': array([13.62829765, 13.35457307,  5.94791555]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 1.4046461614567267}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9193939994498674
{'scaleFactor': 20, 'currentTarget': array([14., 12.]), 'previousTarget': array([14., 12.]), 'currentState': array([14.05606632, 11.72031086,  3.96908784]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 0.2852533007551288}
episode index:1047
target Thresh 31.999273598487516
target distance 20.0
model initialize at round 1047
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([22.09207448, 12.54253289,  5.10660958]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 21.809454552692387}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9193625021905795
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.97964636, 2.1642642 , 3.35798628]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.16552038575197467}
episode index:1048
target Thresh 31.99928082630333
target distance 16.0
model initialize at round 1048
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 10.]), 'previousTarget': array([25., 10.]), 'currentState': array([10.36911778,  5.021274  ,  0.38937109]), 'targetState': array([25, 10], dtype=int32), 'currentDistance': 15.454786538049406}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9193657263967161
{'scaleFactor': 20, 'currentTarget': array([25., 10.]), 'previousTarget': array([25., 10.]), 'currentState': array([25.2955585 ,  9.8215778 ,  0.14361203]), 'targetState': array([25, 10], dtype=int32), 'currentDistance': 0.34523804286770954}
episode index:1049
target Thresh 31.999287982201174
target distance 12.0
model initialize at round 1049
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  9.]), 'previousTarget': array([19.,  9.]), 'currentState': array([ 9.68228482, 19.46153469,  6.13981104]), 'targetState': array([19,  9], dtype=int32), 'currentDistance': 14.009408420544567}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.919377821274345
{'scaleFactor': 20, 'currentTarget': array([19.,  9.]), 'previousTarget': array([19.,  9.]), 'currentState': array([18.61351786,  9.12439414,  5.5421519 ]), 'targetState': array([19,  9], dtype=int32), 'currentDistance': 0.4060078169723831}
episode index:1050
target Thresh 31.999295066896646
target distance 17.0
model initialize at round 1050
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 10.]), 'previousTarget': array([26., 10.]), 'currentState': array([10.89199521,  7.55243048,  0.5826798 ]), 'targetState': array([26, 10], dtype=int32), 'currentDistance': 15.30497975101444}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9193810247692581
{'scaleFactor': 20, 'currentTarget': array([26., 10.]), 'previousTarget': array([26., 10.]), 'currentState': array([26.30897453,  9.82979833,  6.25482804]), 'targetState': array([26, 10], dtype=int32), 'currentDistance': 0.3527518507578408}
episode index:1051
target Thresh 31.99930208109822
target distance 4.0
model initialize at round 1051
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([18.75171128,  4.33544577,  5.57431841]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 5.014758850564982}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9194294258863975
{'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([23.51590131,  7.00596002,  0.73241043]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 0.5159357387692276}
episode index:1052
target Thresh 31.999309025507323
target distance 15.0
model initialize at round 1052
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 14.]), 'previousTarget': array([18., 14.]), 'currentState': array([14.06700236, 27.69850083,  0.12591403]), 'targetState': array([18, 14], dtype=int32), 'currentDistance': 14.251926025177195}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9194325742895708
{'scaleFactor': 20, 'currentTarget': array([18., 14.]), 'previousTarget': array([18., 14.]), 'currentState': array([17.97965726, 14.03537157,  4.84763967]), 'targetState': array([18, 14], dtype=int32), 'currentDistance': 0.04080410008560322}
episode index:1053
target Thresh 31.9993159008184
target distance 3.0
model initialize at round 1053
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 16.]), 'previousTarget': array([13., 16.]), 'currentState': array([ 8.87535497, 16.74797698,  4.99053478]), 'targetState': array([13, 16], dtype=int32), 'currentDistance': 4.191916770833951}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.919490133517
{'scaleFactor': 20, 'currentTarget': array([13., 16.]), 'previousTarget': array([13., 16.]), 'currentState': array([12.12180611, 15.81073762,  5.03703439]), 'targetState': array([13, 16], dtype=int32), 'currentDistance': 0.8983566970000092}
episode index:1054
target Thresh 31.99932270771899
target distance 8.0
model initialize at round 1054
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 3.]), 'previousTarget': array([9., 3.]), 'currentState': array([15.13285229,  2.3472688 ,  3.21770561]), 'targetState': array([9, 3], dtype=int32), 'currentDistance': 6.16749018406332}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9195382935800172
{'scaleFactor': 20, 'currentTarget': array([9., 3.]), 'previousTarget': array([9., 3.]), 'currentState': array([9.2228679 , 3.06688185, 3.30892991]), 'targetState': array([9, 3], dtype=int32), 'currentDistance': 0.2326870886821433}
episode index:1055
target Thresh 31.999329446889785
target distance 15.0
model initialize at round 1055
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([11.56704829, 29.6155101 ,  5.64850372]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 16.232535742446615}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9195413299444564
{'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([15.80141047, 14.64901197,  5.17464495]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 0.6787152082677909}
episode index:1056
target Thresh 31.999336119004713
target distance 7.0
model initialize at round 1056
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  3.]), 'previousTarget': array([24.,  3.]), 'currentState': array([24.05129222,  8.07360816,  5.21406525]), 'targetState': array([24,  3], dtype=int32), 'currentDistance': 5.073867425843811}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.919589350445928
{'scaleFactor': 20, 'currentTarget': array([24.,  3.]), 'previousTarget': array([24.,  3.]), 'currentState': array([23.75983644,  2.20562081,  4.80134124]), 'targetState': array([24,  3], dtype=int32), 'currentDistance': 0.8298896493905251}
episode index:1057
target Thresh 31.999342724730987
target distance 8.0
model initialize at round 1057
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([22.41810735, 27.4255405 ,  4.49993134]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 8.404081844446912}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9196281091033516
{'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([16.47000185, 22.53304615,  4.12005553]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 0.7106616216609993}
episode index:1058
target Thresh 31.999349264729187
target distance 14.0
model initialize at round 1058
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  6.]), 'previousTarget': array([10.,  6.]), 'currentState': array([ 5.26523418, 18.45173808,  5.35775007]), 'targetState': array([10,  6], dtype=int32), 'currentDistance': 13.321553535908913}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9196398534270567
{'scaleFactor': 20, 'currentTarget': array([10.,  6.]), 'previousTarget': array([10.,  6.]), 'currentState': array([10.16199842,  5.63669077,  4.93065944]), 'targetState': array([10,  6], dtype=int32), 'currentDistance': 0.39779024890578807}
episode index:1059
target Thresh 31.999355739653314
target distance 9.0
model initialize at round 1059
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([ 4.65528049, 24.30403055,  5.45483381]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 7.697626999169767}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9196784913106161
{'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([11.44892149, 22.70394017,  5.06492502]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 0.8939906547423943}
episode index:1060
target Thresh 31.999362150150873
target distance 13.0
model initialize at round 1060
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 13.]), 'previousTarget': array([25., 13.]), 'currentState': array([11.75232127, 16.33535489,  5.57468486]), 'targetState': array([25, 13], dtype=int32), 'currentDistance': 13.661097467586385}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9196813812287286
{'scaleFactor': 20, 'currentTarget': array([25., 13.]), 'previousTarget': array([25., 13.]), 'currentState': array([24.9880536 , 13.35221859,  4.93555927]), 'targetState': array([25, 13], dtype=int32), 'currentDistance': 0.35242112646443663}
episode index:1061
target Thresh 31.999368496862914
target distance 5.0
model initialize at round 1061
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 14.]), 'previousTarget': array([ 6., 14.]), 'currentState': array([ 4.07629627, 17.70616515,  4.39627727]), 'targetState': array([ 6, 14], dtype=int32), 'currentDistance': 4.175679125734116}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9197290437699444
{'scaleFactor': 20, 'currentTarget': array([ 6., 14.]), 'previousTarget': array([ 6., 14.]), 'currentState': array([ 5.42563439, 13.14555274,  4.35736749]), 'targetState': array([ 6, 14], dtype=int32), 'currentDistance': 1.0295513456875314}
episode index:1062
target Thresh 31.999374780424116
target distance 19.0
model initialize at round 1062
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([22.1114387 , 20.45192266,  4.7940731 ]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 18.123676116556087}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9197232001233909
{'scaleFactor': 20, 'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([26.87505008,  3.45723969,  5.10487854]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 0.47400487165749317}
episode index:1063
target Thresh 31.99938100146284
target distance 19.0
model initialize at round 1063
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 23.]), 'previousTarget': array([25., 23.]), 'currentState': array([15.15790514,  2.77867131,  0.19795101]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 22.489307800211662}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9196835364230861
{'scaleFactor': 20, 'currentTarget': array([25., 23.]), 'previousTarget': array([25., 23.]), 'currentState': array([24.28938206, 22.54871229,  0.90444344]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 0.8418066616541149}
episode index:1064
target Thresh 31.999387160601195
target distance 20.0
model initialize at round 1064
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([23.07944802,  7.59110961,  3.12363577]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 19.254588690678336}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.919669168853683
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.64736788, 5.1190477 , 3.05267025]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.3721851236311754}
episode index:1065
target Thresh 31.999393258455097
target distance 11.0
model initialize at round 1065
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 16.]), 'previousTarget': array([14., 16.]), 'currentState': array([24.02067435,  9.36868892,  3.20186329]), 'targetState': array([14, 16], dtype=int32), 'currentDistance': 12.016164157944445}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9196896294358099
{'scaleFactor': 20, 'currentTarget': array([14., 16.]), 'previousTarget': array([14., 16.]), 'currentState': array([14.2531518 , 15.94193078,  2.75480056]), 'targetState': array([14, 16], dtype=int32), 'currentDistance': 0.2597265261250417}
episode index:1066
target Thresh 31.99939929563434
target distance 15.0
model initialize at round 1066
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([17.69651966,  6.34461854,  3.52107286]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 14.934481604036382}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.919692492664481
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.72177715, 9.38907957, 2.55809001]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.47832088208367907}
episode index:1067
target Thresh 31.999405272742646
target distance 19.0
model initialize at round 1067
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  6.]), 'previousTarget': array([11.,  6.]), 'currentState': array([10.19374136, 23.32821674,  3.81776679]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 17.346963667594093}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9196781570674252
{'scaleFactor': 20, 'currentTarget': array([11.,  6.]), 'previousTarget': array([11.,  6.]), 'currentState': array([11.71911679,  6.54696602,  3.71947396]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 0.9034936570572707}
episode index:1068
target Thresh 31.99941119037773
target distance 8.0
model initialize at round 1068
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([ 5.4778006 , 18.61372075,  2.29293394]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 6.56518496509487}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9197164338241441
{'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([ 7.58790006, 25.09014265,  1.95071304]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 0.5947706909948911}
episode index:1069
target Thresh 31.999417049131363
target distance 21.0
model initialize at round 1069
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 26.]), 'previousTarget': array([10., 26.]), 'currentState': array([22.57820102,  6.39149893,  2.54707733]), 'targetState': array([10, 26], dtype=int32), 'currentDistance': 23.296018007329796}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9196852828315198
{'scaleFactor': 20, 'currentTarget': array([10., 26.]), 'previousTarget': array([10., 26.]), 'currentState': array([ 9.93727838, 26.20763356,  2.037628  ]), 'targetState': array([10, 26], dtype=int32), 'currentDistance': 0.2169001967215023}
episode index:1070
target Thresh 31.999422849589422
target distance 12.0
model initialize at round 1070
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([22.83085771,  5.57438163,  1.863096  ]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 10.458673094302384}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9197145123059068
{'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([22.01944269, 15.32859655,  1.80321965]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 0.6716849056874358}
episode index:1071
target Thresh 31.99942859233196
target distance 19.0
model initialize at round 1071
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 23.]), 'previousTarget': array([19., 23.]), 'currentState': array([5.15877849, 5.67606223, 2.48534498]), 'targetState': array([19, 23], dtype=int32), 'currentDistance': 22.174269613202064}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9196834212232672
{'scaleFactor': 20, 'currentTarget': array([19., 23.]), 'previousTarget': array([19., 23.]), 'currentState': array([19.50136689, 23.28951561,  0.72566669]), 'targetState': array([19, 23], dtype=int32), 'currentDistance': 0.5789542666645706}
episode index:1072
target Thresh 31.999434277933254
target distance 9.0
model initialize at round 1072
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  5.]), 'previousTarget': array([10.,  5.]), 'currentState': array([10.60857618, 13.50515583,  4.9747445 ]), 'targetState': array([10,  5], dtype=int32), 'currentDistance': 8.526901004390508}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9197215503833573
{'scaleFactor': 20, 'currentTarget': array([10.,  5.]), 'previousTarget': array([10.,  5.]), 'currentState': array([9.70113749, 5.67391242, 4.80307851]), 'targetState': array([10,  5], dtype=int32), 'currentDistance': 0.7372087548759607}
episode index:1073
target Thresh 31.999439906961868
target distance 12.0
model initialize at round 1073
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([ 6.15466406, 17.19647315,  4.43251649]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 11.010417713403706}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9197418097865394
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.54752762, 6.19397314, 4.1037146 ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.9243433079005992}
episode index:1074
target Thresh 31.999445479980714
target distance 12.0
model initialize at round 1074
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([9.03972264, 0.67660855, 0.10513323]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 15.031906750823687}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9197360194960251
{'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([16.11888765, 14.1580501 ,  1.70650595]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 0.19777287199640628}
episode index:1075
target Thresh 31.999450997547097
target distance 9.0
model initialize at round 1075
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 19.]), 'previousTarget': array([19., 19.]), 'currentState': array([16.4818649 , 10.72639789,  1.68530846]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 8.648323321091556}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9197739934648951
{'scaleFactor': 20, 'currentTarget': array([19., 19.]), 'previousTarget': array([19., 19.]), 'currentState': array([18.83732795, 18.20880928,  1.55374097]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 0.8077406487323163}
episode index:1076
target Thresh 31.999456460212773
target distance 13.0
model initialize at round 1076
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([13.62116014,  9.63982521,  0.78791642]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 11.47526606849538}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9197854060502638
{'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([12.31067181, 21.26343345,  1.44863904]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 0.40732561129647765}
episode index:1077
target Thresh 31.99946186852402
target distance 4.0
model initialize at round 1077
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([20.64995429,  8.33176177,  5.47162611]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 2.3733479961895627}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9198505401819426
{'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([22.4450077 ,  8.28896876,  0.77980304]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 0.6257151100919945}
episode index:1078
target Thresh 31.99946722302167
target distance 19.0
model initialize at round 1078
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 7.]), 'previousTarget': array([6., 7.]), 'currentState': array([18.98955176, 27.34587608,  3.22479236]), 'targetState': array([6, 7], dtype=int32), 'currentDistance': 24.13883030356083}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9198031771352206
{'scaleFactor': 20, 'currentTarget': array([6., 7.]), 'previousTarget': array([6., 7.]), 'currentState': array([6.7138091 , 7.43950832, 2.61588442]), 'targetState': array([6, 7], dtype=int32), 'currentDistance': 0.8382666599203022}
episode index:1079
target Thresh 31.99947252424118
target distance 18.0
model initialize at round 1079
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([ 4.45582447, 20.37993453,  5.99665999]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 16.44450303243351}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9197888983369554
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.31542164, 3.49334159, 3.74293013]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.8516749840259703}
episode index:1080
target Thresh 31.999477772712673
target distance 11.0
model initialize at round 1080
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  7.]), 'previousTarget': array([21.,  7.]), 'currentState': array([11.84290572, 15.28315032,  5.6488626 ]), 'targetState': array([21,  7], dtype=int32), 'currentDistance': 12.347589035955224}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.919808964249133
{'scaleFactor': 20, 'currentTarget': array([21.,  7.]), 'previousTarget': array([21.,  7.]), 'currentState': array([20.63065312,  7.27246429,  5.77346541]), 'targetState': array([21,  7], dtype=int32), 'currentDistance': 0.4589704893966395}
episode index:1081
target Thresh 31.999482968961004
target distance 8.0
model initialize at round 1081
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  4.]), 'previousTarget': array([25.,  4.]), 'currentState': array([25.13588739, 10.0055477 ,  4.7272376 ]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 6.007084861922185}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9198556278681265
{'scaleFactor': 20, 'currentTarget': array([25.,  4.]), 'previousTarget': array([25.,  4.]), 'currentState': array([24.92430168,  4.03443631,  4.90175527]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 0.08316306789967037}
episode index:1082
target Thresh 31.999488113505798
target distance 10.0
model initialize at round 1082
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 13.]), 'previousTarget': array([ 9., 13.]), 'currentState': array([9.71993278, 4.09261887, 1.42503333]), 'targetState': array([ 9, 13], dtype=int32), 'currentDistance': 8.936427797081333}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.919884376180252
{'scaleFactor': 20, 'currentTarget': array([ 9., 13.]), 'previousTarget': array([ 9., 13.]), 'currentState': array([ 9.45594668, 13.90290711,  0.97087233]), 'targetState': array([ 9, 13], dtype=int32), 'currentDistance': 1.0114982083740718}
episode index:1083
target Thresh 31.999493206861516
target distance 5.0
model initialize at round 1083
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 16.]), 'previousTarget': array([ 7., 16.]), 'currentState': array([ 2.52348893, 20.19234714,  4.65213037]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 6.133100837640821}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9199308841358051
{'scaleFactor': 20, 'currentTarget': array([ 7., 16.]), 'previousTarget': array([ 7., 16.]), 'currentState': array([ 6.4796626 , 16.94015734,  0.05498284]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 1.0745449417051436}
episode index:1084
target Thresh 31.9994982495375
target distance 12.0
model initialize at round 1084
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([9.05291211, 5.60880893, 5.12468266]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 14.024693442401393}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9199420679733824
{'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([21.17043754, 10.51502161,  0.61913087]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 0.9609255529348768}
episode index:1085
target Thresh 31.999503242038017
target distance 25.0
model initialize at round 1085
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2.29864925, 2.35809328]), 'previousTarget': array([2.84750197, 3.11513418]), 'currentState': array([21.51322373, 25.39719325,  4.01227617]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 30.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.919886926432561
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.890676  , 2.81516829, 4.03710872]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 1.2073951603834474}
episode index:1086
target Thresh 31.999508184862325
target distance 18.0
model initialize at round 1086
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 19.]), 'previousTarget': array([27., 19.]), 'currentState': array([ 7.59804224, 25.068924  ,  4.73783398]), 'targetState': array([27, 19], dtype=int32), 'currentDistance': 20.32898923591933}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.919864342557516
{'scaleFactor': 20, 'currentTarget': array([27., 19.]), 'previousTarget': array([27., 19.]), 'currentState': array([27.58264726, 18.46738431,  5.50687782]), 'targetState': array([27, 19], dtype=int32), 'currentDistance': 0.7894031303320728}
episode index:1087
target Thresh 31.999513078504712
target distance 10.0
model initialize at round 1087
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 17.]), 'previousTarget': array([18., 17.]), 'currentState': array([18.15915654, 25.77985894,  4.46216144]), 'targetState': array([18, 17], dtype=int32), 'currentDistance': 8.781301372837008}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9199017797518565
{'scaleFactor': 20, 'currentTarget': array([18., 17.]), 'previousTarget': array([18., 17.]), 'currentState': array([17.97545213, 17.87051551,  4.99014929]), 'targetState': array([18, 17], dtype=int32), 'currentDistance': 0.8708615572001026}
episode index:1088
target Thresh 31.99951792345454
target distance 11.0
model initialize at round 1088
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'previousTarget': array([27.,  8.]), 'currentState': array([17.6431894 , 16.63440616,  0.79040676]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 12.731962710994866}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9199129492359293
{'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'previousTarget': array([27.,  8.]), 'currentState': array([27.17582092,  7.73617091,  5.36732575]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 0.31704697348113686}
episode index:1089
target Thresh 31.999522720196314
target distance 10.0
model initialize at round 1089
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 25.]), 'previousTarget': array([27., 25.]), 'currentState': array([27.67329116, 14.81976464,  0.9027006 ]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 10.202475828694663}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.919932735658099
{'scaleFactor': 20, 'currentTarget': array([27., 25.]), 'previousTarget': array([27., 25.]), 'currentState': array([27.05291947, 24.85916948,  1.03931847]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 0.15044502303890633}
episode index:1090
target Thresh 31.999527469209706
target distance 21.0
model initialize at round 1090
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([17.85847386, 22.32299122,  5.63819647]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 19.32350950164062}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.919901986011956
{'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([17.77332176,  2.91655221,  3.74597271]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 0.24155032331976253}
episode index:1091
target Thresh 31.99953217096963
target distance 18.0
model initialize at round 1091
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  5.]), 'previousTarget': array([27.,  5.]), 'currentState': array([15.59871013, 22.47415402,  4.95541299]), 'targetState': array([27,  5], dtype=int32), 'currentDistance': 20.864694324899247}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9198794917521087
{'scaleFactor': 20, 'currentTarget': array([27.,  5.]), 'previousTarget': array([27.,  5.]), 'currentState': array([27.07827532,  4.26130408,  4.84465303]), 'targetState': array([27,  5], dtype=int32), 'currentDistance': 0.742831537354348}
episode index:1092
target Thresh 31.999536825946258
target distance 22.0
model initialize at round 1092
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  7.]), 'previousTarget': array([22.,  7.]), 'currentState': array([24.0042648 , 27.64320249,  5.08928061]), 'targetState': array([22,  7], dtype=int32), 'currentDistance': 20.740272089034278}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9198570386528467
{'scaleFactor': 20, 'currentTarget': array([22.,  7.]), 'previousTarget': array([22.,  7.]), 'currentState': array([21.3150474 ,  6.40616921,  3.83696082]), 'targetState': array([22,  7], dtype=int32), 'currentDistance': 0.9065291293573966}
episode index:1093
target Thresh 31.999541434605096
target distance 17.0
model initialize at round 1093
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  6.]), 'previousTarget': array([23.,  6.]), 'currentState': array([ 7.93834602, 11.91207457,  5.80860627]), 'targetState': array([23,  6], dtype=int32), 'currentDistance': 16.180421694528647}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9198596781919465
{'scaleFactor': 20, 'currentTarget': array([23.,  6.]), 'previousTarget': array([23.,  6.]), 'currentState': array([22.64341089,  6.1839259 ,  5.89268182]), 'targetState': array([23,  6], dtype=int32), 'currentDistance': 0.40122877533353735}
episode index:1094
target Thresh 31.99954599740701
target distance 9.0
model initialize at round 1094
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 19.]), 'previousTarget': array([16., 19.]), 'currentState': array([ 8.98566506, 21.79150912,  6.07624391]), 'targetState': array([16, 19], dtype=int32), 'currentDistance': 7.549398512018543}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9198968803214516
{'scaleFactor': 20, 'currentTarget': array([16., 19.]), 'previousTarget': array([16., 19.]), 'currentState': array([16.37373859, 18.87540355,  5.94349501]), 'targetState': array([16, 19], dtype=int32), 'currentDistance': 0.3939604141594308}
episode index:1095
target Thresh 31.99955051480829
target distance 14.0
model initialize at round 1095
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([17.28881722, 16.47467802,  3.26610327]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 14.391728385988994}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9199079829378617
{'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([ 4.61784013, 21.69178888,  2.75666533]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 0.6904495046305958}
episode index:1096
target Thresh 31.999554987260677
target distance 10.0
model initialize at round 1096
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  5.]), 'previousTarget': array([18.,  5.]), 'currentState': array([10.29490246, 13.34306891,  5.89852571]), 'targetState': array([18,  5], dtype=int32), 'currentDistance': 11.356730473515452}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9199276476292593
{'scaleFactor': 20, 'currentTarget': array([18.,  5.]), 'previousTarget': array([18.,  5.]), 'currentState': array([18.2644013 ,  4.6149898 ,  5.32582895]), 'targetState': array([18,  5], dtype=int32), 'currentDistance': 0.4670555644702492}
episode index:1097
target Thresh 31.999559415211415
target distance 10.0
model initialize at round 1097
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 21.]), 'previousTarget': array([ 8., 21.]), 'currentState': array([16.60173814, 18.93661724,  3.56139362]), 'targetState': array([ 8, 21], dtype=int32), 'currentDistance': 8.845758727003549}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9199559376131125
{'scaleFactor': 20, 'currentTarget': array([ 8., 21.]), 'previousTarget': array([ 8., 21.]), 'currentState': array([ 7.41336163, 21.77379154,  2.12567049]), 'targetState': array([ 8, 21], dtype=int32), 'currentDistance': 0.9710293113280233}
episode index:1098
target Thresh 31.99956379910331
target distance 1.0
model initialize at round 1098
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.50821112, 10.39540465,  6.02880859]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.7793534741615576}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9200287711548657
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.50821112, 10.39540465,  6.02880859]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.7793534741615576}
episode index:1099
target Thresh 31.99956813937475
target distance 16.0
model initialize at round 1099
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 23.]), 'previousTarget': array([ 3., 23.]), 'currentState': array([3.12889284, 5.75180569, 0.17445582]), 'targetState': array([ 3, 23], dtype=int32), 'currentDistance': 17.248675903341553}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.920022851587892
{'scaleFactor': 20, 'currentTarget': array([ 3., 23.]), 'previousTarget': array([ 3., 23.]), 'currentState': array([ 3.03083288, 23.00770646,  1.48009718]), 'targetState': array([ 3, 23], dtype=int32), 'currentDistance': 0.03178137609468012}
episode index:1100
target Thresh 31.99957243645977
target distance 20.0
model initialize at round 1100
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([21.70929898, 26.10264406,  4.06093735]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 20.088743384702674}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9200086456146139
{'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([13.27940556,  8.52852147,  4.15964662]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 0.5978314230653331}
episode index:1101
target Thresh 31.99957669078808
target distance 23.0
model initialize at round 1101
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([11.31823542, 26.06368652,  4.11374187]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 24.19690487125983}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9199781340230545
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.32859228, 3.81464075, 4.41253757]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.8784147314876416}
episode index:1102
target Thresh 31.999580902785112
target distance 20.0
model initialize at round 1102
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 28.]), 'previousTarget': array([27., 28.]), 'currentState': array([12.76773071,  9.14625499,  1.38234162]), 'targetState': array([27, 28], dtype=int32), 'currentDistance': 23.62247214115983}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9199476777562304
{'scaleFactor': 20, 'currentTarget': array([27., 28.]), 'previousTarget': array([27., 28.]), 'currentState': array([27.02081001, 27.93110475,  0.76441328]), 'targetState': array([27, 28], dtype=int32), 'currentDistance': 0.0719695232441051}
episode index:1103
target Thresh 31.999585072872076
target distance 22.0
model initialize at round 1103
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 16.]), 'previousTarget': array([26., 16.]), 'currentState': array([ 5.95304819, 20.22343147,  6.07266307]), 'targetState': array([26, 16], dtype=int32), 'currentDistance': 20.487011749776503}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9199335784783795
{'scaleFactor': 20, 'currentTarget': array([26., 16.]), 'previousTarget': array([26., 16.]), 'currentState': array([25.01164501, 16.24140729,  5.98479478]), 'targetState': array([26, 16], dtype=int32), 'currentDistance': 1.0174099794233178}
episode index:1104
target Thresh 31.99958920146598
target distance 16.0
model initialize at round 1104
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([ 5.55965084, 24.36764145,  4.88799054]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 15.579352237391273}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.919936122474714
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.94452499, 8.88273788, 4.34693208]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.12972232015566526}
episode index:1105
target Thresh 31.99959328897969
target distance 6.0
model initialize at round 1105
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([10.67466785, 13.83303987,  5.17381621]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 6.714327072581316}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9199816585303426
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.53008149,  8.57989502,  5.46936395]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.7463924131093623}
episode index:1106
target Thresh 31.999597335821957
target distance 15.0
model initialize at round 1106
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 20.]), 'previousTarget': array([22., 20.]), 'currentState': array([ 8.28221337, 24.09009952,  5.97777814]), 'targetState': array([22, 20], dtype=int32), 'currentDistance': 14.314558471713669}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9199925742389033
{'scaleFactor': 20, 'currentTarget': array([22., 20.]), 'previousTarget': array([22., 20.]), 'currentState': array([21.57175663, 20.14638043,  5.99312936]), 'targetState': array([22, 20], dtype=int32), 'currentDistance': 0.4525700128184279}
episode index:1107
target Thresh 31.999601342397472
target distance 12.0
model initialize at round 1107
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 12.]), 'previousTarget': array([25., 12.]), 'currentState': array([12.57139591,  6.62747861,  0.81830406]), 'targetState': array([25, 12], dtype=int32), 'currentDistance': 13.540095483495813}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.920003470244019
{'scaleFactor': 20, 'currentTarget': array([25., 12.]), 'previousTarget': array([25., 12.]), 'currentState': array([25.25647225, 12.12889081,  0.37237007]), 'targetState': array([25, 12], dtype=int32), 'currentDistance': 0.2870380714702539}
episode index:1108
target Thresh 31.999605309106894
target distance 14.0
model initialize at round 1108
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([25.15211698,  5.22679034,  1.82677763]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 14.177567785013006}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9200143465989902
{'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([19.32742049, 17.56281745,  1.94565995]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 0.5461984627994174}
episode index:1109
target Thresh 31.9996092363469
target distance 6.0
model initialize at round 1109
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 28.]), 'previousTarget': array([17., 28.]), 'currentState': array([11.00917789, 23.68294499,  0.55534291]), 'targetState': array([17, 28], dtype=int32), 'currentDistance': 7.384234118648619}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9200509066561082
{'scaleFactor': 20, 'currentTarget': array([17., 28.]), 'previousTarget': array([17., 28.]), 'currentState': array([17.56347925, 28.20690292,  0.56811759]), 'targetState': array([17, 28], dtype=int32), 'currentDistance': 0.6002646808615754}
episode index:1110
target Thresh 31.999613124510212
target distance 3.0
model initialize at round 1110
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  5.]), 'previousTarget': array([25.,  5.]), 'currentState': array([20.55161603,  7.14292826,  4.68591905]), 'targetState': array([25,  5], dtype=int32), 'currentDistance': 4.9376372369171415}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9200961344628984
{'scaleFactor': 20, 'currentTarget': array([25.,  5.]), 'previousTarget': array([25.,  5.]), 'currentState': array([24.78862774,  4.37469785,  0.44622892]), 'targetState': array([25,  5], dtype=int32), 'currentDistance': 0.6600613679327395}
episode index:1111
target Thresh 31.999616973985653
target distance 5.0
model initialize at round 1111
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  9.]), 'previousTarget': array([24.,  9.]), 'currentState': array([24.2168665 ,  5.78623166,  1.59275806]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 3.221077155196448}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9201500947736332
{'scaleFactor': 20, 'currentTarget': array([24.,  9.]), 'previousTarget': array([24.,  9.]), 'currentState': array([24.2952672 ,  9.78024672,  1.43211307]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 0.8342467603929659}
episode index:1112
target Thresh 31.999620785158175
target distance 5.0
model initialize at round 1112
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([ 8.66254019, 21.73856513,  5.1172123 ]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 4.784658716405403}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.920203958120647
{'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([ 8.16804507, 17.87152555,  4.44088355]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 0.8875786860498095}
episode index:1113
target Thresh 31.999624558408897
target distance 10.0
model initialize at round 1113
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 24.]), 'previousTarget': array([11., 24.]), 'currentState': array([19.39980966, 16.52134777,  3.83662188]), 'targetState': array([11, 24], dtype=int32), 'currentDistance': 11.246645792087605}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9202230570356205
{'scaleFactor': 20, 'currentTarget': array([11., 24.]), 'previousTarget': array([11., 24.]), 'currentState': array([11.49922343, 23.32208829,  2.12377545]), 'targetState': array([11, 24], dtype=int32), 'currentDistance': 0.841895673832766}
episode index:1114
target Thresh 31.999628294115144
target distance 13.0
model initialize at round 1114
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 20.]), 'previousTarget': array([ 9., 20.]), 'currentState': array([20.1722102 , 21.20937106,  3.38806662]), 'targetState': array([ 9, 20], dtype=int32), 'currentDistance': 11.2374756601172}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9202421216924505
{'scaleFactor': 20, 'currentTarget': array([ 9., 20.]), 'previousTarget': array([ 9., 20.]), 'currentState': array([ 8.30827337, 19.90459177,  3.2401523 ]), 'targetState': array([ 9, 20], dtype=int32), 'currentDistance': 0.6982753454003544}
episode index:1115
target Thresh 31.999631992650496
target distance 11.0
model initialize at round 1115
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 16.]), 'previousTarget': array([16., 16.]), 'currentState': array([25.64694287,  9.99918808,  2.76844788]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 11.361040902839127}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9202611521832288
{'scaleFactor': 20, 'currentTarget': array([16., 16.]), 'previousTarget': array([16., 16.]), 'currentState': array([15.60628347, 16.34675363,  2.6305481 ]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 0.5246434799848104}
episode index:1116
target Thresh 31.999635654384804
target distance 8.0
model initialize at round 1116
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  5.]), 'previousTarget': array([21.,  5.]), 'currentState': array([17.86580707, 11.5568208 ,  6.2627573 ]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 7.2673973561850085}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9202972621723217
{'scaleFactor': 20, 'currentTarget': array([21.,  5.]), 'previousTarget': array([21.,  5.]), 'currentState': array([20.6047108 ,  5.34578001,  5.26058072]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 0.5251831782984977}
episode index:1117
target Thresh 31.99963927968425
target distance 17.0
model initialize at round 1117
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 23.]), 'previousTarget': array([23., 23.]), 'currentState': array([7.47398515, 7.81225358, 1.51365631]), 'targetState': array([23, 23], dtype=int32), 'currentDistance': 21.719226013705224}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9202749374783022
{'scaleFactor': 20, 'currentTarget': array([23., 23.]), 'previousTarget': array([23., 23.]), 'currentState': array([22.75007655, 22.55803879,  0.65397406]), 'targetState': array([23, 23], dtype=int32), 'currentDistance': 0.5077316621139168}
episode index:1118
target Thresh 31.99964286891136
target distance 21.0
model initialize at round 1118
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  6.]), 'previousTarget': array([23.,  6.]), 'currentState': array([0.86414689, 0.75813616, 4.9815464 ]), 'targetState': array([23,  6], dtype=int32), 'currentDistance': 22.748035726863314}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9202446514499177
{'scaleFactor': 20, 'currentTarget': array([23.,  6.]), 'previousTarget': array([23.,  6.]), 'currentState': array([23.24533399,  5.89989714,  0.20537415]), 'targetState': array([23,  6], dtype=int32), 'currentDistance': 0.2649704649177234}
episode index:1119
target Thresh 31.999646422425066
target distance 17.0
model initialize at round 1119
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([ 7.56625278, 21.41515115,  6.06554461]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 16.040105966476084}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.920230488435238
{'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([11.50393936,  5.41991526,  4.33593386]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 0.7632656535143589}
episode index:1120
target Thresh 31.999649940580717
target distance 9.0
model initialize at round 1120
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([ 6.57837826, 13.58404719,  5.62759203]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 7.433268863963931}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9202664969290514
{'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([13.61244049, 14.15838832,  5.99160052]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 0.41867557147782036}
episode index:1121
target Thresh 31.999653423730138
target distance 25.0
model initialize at round 1121
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6.45312717, 4.01569688]), 'previousTarget': array([6.25914857, 3.57393572]), 'currentState': array([23.99677998, 28.35127261,  3.21943247]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 30.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.9201826307319985
{'scaleFactor': 20, 'currentTarget': array([5., 2.]), 'previousTarget': array([5., 2.]), 'currentState': array([5.67840646, 2.45158333, 3.71043706]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 0.8149618560594015}
episode index:1122
target Thresh 31.999656872221642
target distance 23.0
model initialize at round 1122
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.0580566 , 26.69122029,  4.38224899]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 21.711662653130613}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9201446417669646
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.74952705,  4.66029452,  5.61489021]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.42206221722794857}
episode index:1123
target Thresh 31.999660286400083
target distance 8.0
model initialize at round 1123
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 20.]), 'previousTarget': array([18., 20.]), 'currentState': array([23.05442847, 26.60777783,  5.12577224]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 8.319253273595296}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9201720843008907
{'scaleFactor': 20, 'currentTarget': array([18., 20.]), 'previousTarget': array([18., 20.]), 'currentState': array([18.19345726, 19.71075   ,  3.02800047]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 0.3479817126087836}
episode index:1124
target Thresh 31.999663666606878
target distance 9.0
model initialize at round 1124
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([15.73290969, 16.02237121,  4.46184148]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 7.535415894024426}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.92020801667929
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([12.86692022,  8.59564159,  4.31471811]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.42569466510227344}
episode index:1125
target Thresh 31.999667013180055
target distance 16.0
model initialize at round 1125
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 13.]), 'previousTarget': array([18., 13.]), 'currentState': array([0.70567384, 4.92431052, 4.84500003]), 'targetState': array([18, 13], dtype=int32), 'currentDistance': 19.08691903125005}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9201939616689253
{'scaleFactor': 20, 'currentTarget': array([18., 13.]), 'previousTarget': array([18., 13.]), 'currentState': array([17.72016186, 12.60378392,  0.42985457]), 'targetState': array([18, 13], dtype=int32), 'currentDistance': 0.4850737699512819}
episode index:1126
target Thresh 31.999670326454275
target distance 6.0
model initialize at round 1126
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([ 9.78261495, 17.66887142,  0.69032574]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 6.0452470068458855}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9202384204429547
{'scaleFactor': 20, 'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([14.09089014, 21.82152796,  0.8860804 ]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 0.20028301404217985}
episode index:1127
target Thresh 31.99967360676086
target distance 9.0
model initialize at round 1127
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 11.]), 'previousTarget': array([24., 11.]), 'currentState': array([16.82491357, 18.19716976,  5.72983149]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 10.16273181026001}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9202656825258066
{'scaleFactor': 20, 'currentTarget': array([24., 11.]), 'previousTarget': array([24., 11.]), 'currentState': array([23.73163914, 11.12523684,  5.42982192]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 0.2961449281602648}
episode index:1128
target Thresh 31.999676854427854
target distance 15.0
model initialize at round 1128
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 28.]), 'previousTarget': array([ 2., 28.]), 'currentState': array([15.13738684, 25.43424681,  3.03072083]), 'targetState': array([ 2, 28], dtype=int32), 'currentDistance': 13.385590103507756}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9202761339566137
{'scaleFactor': 20, 'currentTarget': array([ 2., 28.]), 'previousTarget': array([ 2., 28.]), 'currentState': array([ 1.61003716, 28.19831982,  2.89761136]), 'targetState': array([ 2, 28], dtype=int32), 'currentDistance': 0.43749487922823216}
episode index:1129
target Thresh 31.99968006978002
target distance 13.0
model initialize at round 1129
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 14.]), 'previousTarget': array([12., 14.]), 'currentState': array([25.66376358, 22.54654647,  2.17538136]), 'targetState': array([12, 14], dtype=int32), 'currentDistance': 16.11650991318308}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9202783185234024
{'scaleFactor': 20, 'currentTarget': array([12., 14.]), 'previousTarget': array([12., 14.]), 'currentState': array([12.71729922, 14.54610096,  3.74960413]), 'targetState': array([12, 14], dtype=int32), 'currentDistance': 0.901523390966077}
episode index:1130
target Thresh 31.999683253138897
target distance 11.0
model initialize at round 1130
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([11.76327443, 18.14144555,  3.40623748]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 12.712382280123576}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9202887403000456
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.59034421, 10.56799341,  4.73416793]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.8192208499491234}
episode index:1131
target Thresh 31.999686404822825
target distance 24.0
model initialize at round 1131
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([12.19019123, 24.32780405,  5.83562779]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 24.543244097101862}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9202509596310517
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.38676327, 2.75690729, 4.12753292]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.8499967470938967}
episode index:1132
target Thresh 31.999689525146973
target distance 12.0
model initialize at round 1132
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  2.]), 'previousTarget': array([23.,  2.]), 'currentState': array([16.60249162, 13.06225888,  4.74259567]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 12.778954769982377}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9202613871582149
{'scaleFactor': 20, 'currentTarget': array([23.,  2.]), 'previousTarget': array([23.,  2.]), 'currentState': array([22.90675588,  2.02209902,  5.08212778]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 0.09582709991917969}
episode index:1133
target Thresh 31.999692614423378
target distance 18.0
model initialize at round 1133
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 19.]), 'previousTarget': array([ 3., 19.]), 'currentState': array([21.55272761, 17.58961639,  2.24616188]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 18.60625925769413}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9202394090868748
{'scaleFactor': 20, 'currentTarget': array([ 3., 19.]), 'previousTarget': array([ 3., 19.]), 'currentState': array([ 3.0065866, 19.1521838,  2.6671377]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 0.15232626803593838}
episode index:1134
target Thresh 31.999695672960968
target distance 14.0
model initialize at round 1134
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([14.68296388, 16.99545615,  5.27048541]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 18.64355576442679}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9202334864775329
{'scaleFactor': 20, 'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([26.39964474,  3.84428934,  5.36197103]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 1.0359782449649722}
episode index:1135
target Thresh 31.9996987010656
target distance 15.0
model initialize at round 1135
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 27.]), 'previousTarget': array([ 6., 27.]), 'currentState': array([19.48849914, 24.74010352,  3.69626081]), 'targetState': array([ 6, 27], dtype=int32), 'currentDistance': 13.67650324575212}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9202439018485095
{'scaleFactor': 20, 'currentTarget': array([ 6., 27.]), 'previousTarget': array([ 6., 27.]), 'currentState': array([ 5.91732803, 27.05592365,  3.03047785]), 'targetState': array([ 6, 27], dtype=int32), 'currentDistance': 0.09981036880134389}
episode index:1136
target Thresh 31.99970169904009
target distance 7.0
model initialize at round 1136
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 19.]), 'previousTarget': array([24., 19.]), 'currentState': array([15.30512829, 22.50081096,  3.71359694]), 'targetState': array([24, 19], dtype=int32), 'currentDistance': 9.373178294069971}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9202625792869901
{'scaleFactor': 20, 'currentTarget': array([24., 19.]), 'previousTarget': array([24., 19.]), 'currentState': array([24.67304647, 18.24565223,  5.44161597]), 'targetState': array([24, 19], dtype=int32), 'currentDistance': 1.0109560406889657}
episode index:1137
target Thresh 31.999704667184233
target distance 6.0
model initialize at round 1137
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 14.]), 'previousTarget': array([ 7., 14.]), 'currentState': array([2.04871663, 7.95778681, 2.77749479]), 'targetState': array([ 7, 14], dtype=int32), 'currentDistance': 7.811756985825496}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9202980216689876
{'scaleFactor': 20, 'currentTarget': array([ 7., 14.]), 'previousTarget': array([ 7., 14.]), 'currentState': array([ 6.42907363, 13.24986522,  0.82994385]), 'targetState': array([ 7, 14], dtype=int32), 'currentDistance': 0.9426871727006361}
episode index:1138
target Thresh 31.99970760579485
target distance 12.0
model initialize at round 1138
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 24.]), 'previousTarget': array([ 3., 24.]), 'currentState': array([15.94452755, 21.39293065,  1.98492926]), 'targetState': array([ 3, 24], dtype=int32), 'currentDistance': 13.204453947515756}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.92030016975745
{'scaleFactor': 20, 'currentTarget': array([ 3., 24.]), 'previousTarget': array([ 3., 24.]), 'currentState': array([ 2.80378611, 24.35877006,  1.64748174]), 'targetState': array([ 3, 24], dtype=int32), 'currentDistance': 0.40892034046011894}
episode index:1139
target Thresh 31.999710515165802
target distance 3.0
model initialize at round 1139
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 23.]), 'previousTarget': array([ 2., 23.]), 'currentState': array([ 2.17919094, 21.46926646,  1.07029963]), 'targetState': array([ 2, 23], dtype=int32), 'currentDistance': 1.5411860858223891}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9203613099594172
{'scaleFactor': 20, 'currentTarget': array([ 2., 23.]), 'previousTarget': array([ 2., 23.]), 'currentState': array([ 2.40324047, 23.40623751,  1.84876108]), 'targetState': array([ 2, 23], dtype=int32), 'currentDistance': 0.5723912934660773}
episode index:1140
target Thresh 31.999713395588028
target distance 11.0
model initialize at round 1140
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.27927541, 13.34032098,  3.86916828]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 9.368086243631478}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9203881537279891
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.83010434,  4.86928253,  5.03168845]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.8857294451075162}
episode index:1141
target Thresh 31.99971624734958
target distance 11.0
model initialize at round 1141
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 20.]), 'previousTarget': array([16., 20.]), 'currentState': array([ 6.36762572, 13.98080985,  1.63215178]), 'targetState': array([16, 20], dtype=int32), 'currentDistance': 11.358401482740126}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.920398378941806
{'scaleFactor': 20, 'currentTarget': array([16., 20.]), 'previousTarget': array([16., 20.]), 'currentState': array([16.8110045 , 19.61900434,  5.71021545]), 'targetState': array([16, 20], dtype=int32), 'currentDistance': 0.8960390593303861}
episode index:1142
target Thresh 31.999719070735622
target distance 6.0
model initialize at round 1142
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 16.]), 'previousTarget': array([14., 16.]), 'currentState': array([18.81476161, 20.80399176,  2.92292774]), 'targetState': array([14, 16], dtype=int32), 'currentDistance': 6.801489997146445}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9204335474729155
{'scaleFactor': 20, 'currentTarget': array([14., 16.]), 'previousTarget': array([14., 16.]), 'currentState': array([14.00556111, 16.08445556,  2.90983403]), 'targetState': array([14, 16], dtype=int32), 'currentDistance': 0.08463845123508766}
episode index:1143
target Thresh 31.99972186602851
target distance 25.0
model initialize at round 1143
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  4.]), 'previousTarget': array([25.,  4.]), 'currentState': array([16.23883913, 27.8608487 ,  4.52968365]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 25.41845865505237}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.920388365886636
{'scaleFactor': 20, 'currentTarget': array([25.,  4.]), 'previousTarget': array([25.,  4.]), 'currentState': array([24.84930913,  4.29749428,  4.83692381]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 0.3334825115272927}
episode index:1144
target Thresh 31.999724633507764
target distance 5.0
model initialize at round 1144
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 25.]), 'previousTarget': array([ 9., 25.]), 'currentState': array([14.3051525 , 23.65507403,  2.39846998]), 'targetState': array([ 9, 25], dtype=int32), 'currentDistance': 5.472976235078535}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9204319559600974
{'scaleFactor': 20, 'currentTarget': array([ 9., 25.]), 'previousTarget': array([ 9., 25.]), 'currentState': array([ 8.65087278, 25.28795733,  2.93334994]), 'targetState': array([ 9, 25], dtype=int32), 'currentDistance': 0.4525585474916451}
episode index:1145
target Thresh 31.999727373450142
target distance 11.0
model initialize at round 1145
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  2.]), 'previousTarget': array([12.,  2.]), 'currentState': array([ 7.3221047 , 13.13059653,  4.07391572]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 12.07364416753353}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9204421072619707
{'scaleFactor': 20, 'currentTarget': array([12.,  2.]), 'previousTarget': array([12.,  2.]), 'currentState': array([11.60348777,  2.09520516,  5.93074453]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 0.4077817676168776}
episode index:1146
target Thresh 31.999730086129638
target distance 5.0
model initialize at round 1146
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 27.]), 'previousTarget': array([ 2., 27.]), 'currentState': array([ 5.09755236, 27.06473202,  3.65490699]), 'targetState': array([ 2, 27], dtype=int32), 'currentDistance': 3.098228659771857}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9204941193742097
{'scaleFactor': 20, 'currentTarget': array([ 2., 27.]), 'previousTarget': array([ 2., 27.]), 'currentState': array([ 1.18399128, 27.03465986,  3.10326268]), 'targetState': array([ 2, 27], dtype=int32), 'currentDistance': 0.8167444706121325}
episode index:1147
target Thresh 31.99973277181752
target distance 24.0
model initialize at round 1147
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  2.]), 'previousTarget': array([19.,  2.]), 'currentState': array([ 5.68275681, 25.9732119 ,  5.25726748]), 'targetState': array([19,  2], dtype=int32), 'currentDistance': 27.423782655936833}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9204490424520797
{'scaleFactor': 20, 'currentTarget': array([19.,  2.]), 'previousTarget': array([19.,  2.]), 'currentState': array([18.9136673 ,  2.06421627,  4.98240712]), 'targetState': array([19,  2], dtype=int32), 'currentDistance': 0.10759677301156231}
episode index:1148
target Thresh 31.999735430782362
target distance 14.0
model initialize at round 1148
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 25.]), 'previousTarget': array([26., 25.]), 'currentState': array([23.42477286,  9.37151724,  5.97754335]), 'targetState': array([26, 25], dtype=int32), 'currentDistance': 15.839231932963626}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9204510404085425
{'scaleFactor': 20, 'currentTarget': array([26., 25.]), 'previousTarget': array([26., 25.]), 'currentState': array([26.31778554, 24.20848144,  1.60542378]), 'targetState': array([26, 25], dtype=int32), 'currentDistance': 0.852929823118354}
episode index:1149
target Thresh 31.999738063290064
target distance 17.0
model initialize at round 1149
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 15.]), 'previousTarget': array([21., 15.]), 'currentState': array([ 4.85759655, 19.44807329,  0.02610755]), 'targetState': array([21, 15], dtype=int32), 'currentDistance': 16.744030131177855}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9204450110233905
{'scaleFactor': 20, 'currentTarget': array([21., 15.]), 'previousTarget': array([21., 15.]), 'currentState': array([21.77393227, 14.6383681 ,  6.06599096]), 'targetState': array([21, 15], dtype=int32), 'currentDistance': 0.8542533493839014}
episode index:1150
target Thresh 31.999740669603874
target distance 1.0
model initialize at round 1150
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  3.]), 'previousTarget': array([20.,  3.]), 'currentState': array([20.77114891,  4.66736126,  2.7171585 ]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 1.8370531331096878}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.92050544107463
{'scaleFactor': 20, 'currentTarget': array([20.,  3.]), 'previousTarget': array([20.,  3.]), 'currentState': array([19.35023446,  3.76547396,  4.71714813]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 1.0040645611717771}
episode index:1151
target Thresh 31.99974324998443
target distance 12.0
model initialize at round 1151
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([ 8.60020873, 12.42769673,  4.06705427]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 12.340972994202625}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9205236482867188
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.1424547 , 2.45833178, 4.16047649]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.47995974926122686}
episode index:1152
target Thresh 31.99974580468977
target distance 16.0
model initialize at round 1152
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 15.]), 'previousTarget': array([21., 15.]), 'currentState': array([ 6.69490618, 11.05149475,  0.42642164]), 'targetState': array([21, 15], dtype=int32), 'currentDistance': 14.840027050642655}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.920533658433831
{'scaleFactor': 20, 'currentTarget': array([21., 15.]), 'previousTarget': array([21., 15.]), 'currentState': array([20.15347868, 14.51646364,  0.35854901]), 'targetState': array([21, 15], dtype=int32), 'currentDistance': 0.9748875579406444}
episode index:1153
target Thresh 31.99974833397537
target distance 4.0
model initialize at round 1153
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  9.]), 'previousTarget': array([21.,  9.]), 'currentState': array([18.11321583, 11.56961477,  5.16742086]), 'targetState': array([21,  9], dtype=int32), 'currentDistance': 3.8647694482307897}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9205852757142176
{'scaleFactor': 20, 'currentTarget': array([21.,  9.]), 'previousTarget': array([21.,  9.]), 'currentState': array([21.04971745,  8.92774455,  5.59014885]), 'targetState': array([21,  9], dtype=int32), 'currentDistance': 0.08770789680479955}
episode index:1154
target Thresh 31.99975083809415
target distance 9.0
model initialize at round 1154
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 20.]), 'previousTarget': array([ 4., 20.]), 'currentState': array([11.42004607, 16.42022964,  2.48329067]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 8.238436713572314}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9206199170426035
{'scaleFactor': 20, 'currentTarget': array([ 4., 20.]), 'previousTarget': array([ 4., 20.]), 'currentState': array([ 4.30772919, 20.00511448,  2.78202232]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 0.30777169188721837}
episode index:1155
target Thresh 31.999753317296538
target distance 6.0
model initialize at round 1155
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([9.96915397, 0.62408988, 0.05285853]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 7.9743387032737365}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9206461887838295
{'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([13.6192088 ,  8.38215609,  1.56245738]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 0.7276419546409885}
episode index:1156
target Thresh 31.99975577183045
target distance 19.0
model initialize at round 1156
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 12.]), 'previousTarget': array([ 5., 12.]), 'currentState': array([23.33437215, 19.45425499,  3.29576826]), 'targetState': array([ 5, 12], dtype=int32), 'currentDistance': 19.79179424510355}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9206321316414137
{'scaleFactor': 20, 'currentTarget': array([ 5., 12.]), 'previousTarget': array([ 5., 12.]), 'currentState': array([ 5.13388436, 12.35346804,  3.43085752]), 'targetState': array([ 5, 12], dtype=int32), 'currentDistance': 0.37797443442575723}
episode index:1157
target Thresh 31.99975820194134
target distance 4.0
model initialize at round 1157
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 15.]), 'previousTarget': array([11., 15.]), 'currentState': array([ 8.58530289, 14.11434548,  0.17420334]), 'targetState': array([11, 15], dtype=int32), 'currentDistance': 2.5719926237901016}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9206920348092535
{'scaleFactor': 20, 'currentTarget': array([11., 15.]), 'previousTarget': array([11., 15.]), 'currentState': array([10.53267962, 14.56580645,  0.28249165]), 'targetState': array([11, 15], dtype=int32), 'currentDistance': 0.6378968378421735}
episode index:1158
target Thresh 31.999760607872222
target distance 20.0
model initialize at round 1158
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 13.]), 'previousTarget': array([25., 13.]), 'currentState': array([ 6.68296566, 27.00383226,  5.27546239]), 'targetState': array([25, 13], dtype=int32), 'currentDistance': 23.056909273355704}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.920662434150847
{'scaleFactor': 20, 'currentTarget': array([25., 13.]), 'previousTarget': array([25., 13.]), 'currentState': array([25.10762708, 12.8881763 ,  5.45148488]), 'targetState': array([25, 13], dtype=int32), 'currentDistance': 0.15520350667273847}
episode index:1159
target Thresh 31.999762989863694
target distance 13.0
model initialize at round 1159
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 8.]), 'previousTarget': array([6., 8.]), 'currentState': array([ 5.44784419, 19.4101849 ,  5.38811421]), 'targetState': array([6, 8], dtype=int32), 'currentDistance': 11.423536911587036}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9206803804570972
{'scaleFactor': 20, 'currentTarget': array([6., 8.]), 'previousTarget': array([6., 8.]), 'currentState': array([6.15729124, 7.63407452, 4.82106244]), 'targetState': array([6, 8], dtype=int32), 'currentDistance': 0.3982988713696447}
episode index:1160
target Thresh 31.999765348153954
target distance 21.0
model initialize at round 1160
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([21.03378981,  4.07293371,  3.42037255]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 19.661158073707234}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9206663422956429
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.04851068, 9.03352611, 2.78401184]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.05896852112602537}
episode index:1161
target Thresh 31.99976768297883
target distance 6.0
model initialize at round 1161
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 11.]), 'previousTarget': array([20., 11.]), 'currentState': array([22.6392976 , 16.00960762,  4.78076887]), 'targetState': array([20, 11], dtype=int32), 'currentDistance': 5.662337003852545}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9207090554261975
{'scaleFactor': 20, 'currentTarget': array([20., 11.]), 'previousTarget': array([20., 11.]), 'currentState': array([19.79065352, 10.86539071,  4.24692494]), 'targetState': array([20, 11], dtype=int32), 'currentDistance': 0.24888874772935998}
episode index:1162
target Thresh 31.999769994571814
target distance 9.0
model initialize at round 1162
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([13.80432396, 11.81562384,  4.93224311]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 10.19734301085125}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9207350923947906
{'scaleFactor': 20, 'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([23.18063754, 11.94381419,  0.13803172]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 0.8212866013661155}
episode index:1163
target Thresh 31.999772283164063
target distance 7.0
model initialize at round 1163
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 20.]), 'previousTarget': array([23., 20.]), 'currentState': array([17.48189697, 14.60123619,  0.87372923]), 'targetState': array([23, 20], dtype=int32), 'currentDistance': 7.719851794416627}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9207693371693655
{'scaleFactor': 20, 'currentTarget': array([23., 20.]), 'previousTarget': array([23., 20.]), 'currentState': array([23.1186309 , 20.22019971,  0.89095105]), 'targetState': array([23, 20], dtype=int32), 'currentDistance': 0.2501223754648375}
episode index:1164
target Thresh 31.99977454898444
target distance 7.0
model initialize at round 1164
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 13.]), 'previousTarget': array([15., 13.]), 'currentState': array([21.37618644, 21.64038771,  2.3553665 ]), 'targetState': array([15, 13], dtype=int32), 'currentDistance': 10.73834499947856}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9207871146905945
{'scaleFactor': 20, 'currentTarget': array([15., 13.]), 'previousTarget': array([15., 13.]), 'currentState': array([14.85102959, 12.65648967,  4.35000858]), 'targetState': array([15, 13], dtype=int32), 'currentDistance': 0.3744215972870083}
episode index:1165
target Thresh 31.999776792259524
target distance 15.0
model initialize at round 1165
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 20.]), 'previousTarget': array([ 7., 20.]), 'currentState': array([20.17413786,  9.81304217,  2.78569315]), 'targetState': array([ 7, 20], dtype=int32), 'currentDistance': 16.65328850759342}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9207887935754463
{'scaleFactor': 20, 'currentTarget': array([ 7., 20.]), 'previousTarget': array([ 7., 20.]), 'currentState': array([ 7.79486116, 19.50670616,  2.4326812 ]), 'targetState': array([ 7, 20], dtype=int32), 'currentDistance': 0.9354908183515869}
episode index:1166
target Thresh 31.99977901321365
target distance 17.0
model initialize at round 1166
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 28.]), 'previousTarget': array([11., 28.]), 'currentState': array([18.38972032, 12.56842175,  2.95187473]), 'targetState': array([11, 28], dtype=int32), 'currentDistance': 17.10969238715094}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9207747346906419
{'scaleFactor': 20, 'currentTarget': array([11., 28.]), 'previousTarget': array([11., 28.]), 'currentState': array([11.41472146, 27.91058941,  3.00127828]), 'targetState': array([11, 28], dtype=int32), 'currentDistance': 0.4242500965431174}
episode index:1167
target Thresh 31.999781212068914
target distance 5.0
model initialize at round 1167
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 19.]), 'previousTarget': array([17., 19.]), 'currentState': array([15.57848717, 14.90093832,  1.56669879]), 'targetState': array([17, 19], dtype=int32), 'currentDistance': 4.33854876685005}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9208255268698451
{'scaleFactor': 20, 'currentTarget': array([17., 19.]), 'previousTarget': array([17., 19.]), 'currentState': array([16.99685146, 18.59092888,  1.28894609]), 'targetState': array([17, 19], dtype=int32), 'currentDistance': 0.40908323509100813}
episode index:1168
target Thresh 31.999783389045202
target distance 14.0
model initialize at round 1168
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  2.]), 'previousTarget': array([22.,  2.]), 'currentState': array([ 9.70291825, 12.05254951,  6.17308706]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 15.883071841152722}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9208271685871744
{'scaleFactor': 20, 'currentTarget': array([22.,  2.]), 'previousTarget': array([22.,  2.]), 'currentState': array([21.73576859,  1.9704549 ,  5.53815061]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 0.2658780701076452}
episode index:1169
target Thresh 31.999785544360215
target distance 22.0
model initialize at round 1169
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([24.20527845, 22.48351129,  3.07259679]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 28.130126618655463}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.9207606010870006
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.09083311, 4.16204657, 3.40468806]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.185767985312712}
episode index:1170
target Thresh 31.999787678229485
target distance 20.0
model initialize at round 1170
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([23.12517007, 27.20251896,  4.20597445]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 25.68761348757149}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9207236757427751
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([5.16781446, 9.16994746, 3.7802632 ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 0.23883850248040311}
episode index:1171
target Thresh 31.9997897908664
target distance 10.0
model initialize at round 1171
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  7.]), 'previousTarget': array([17.,  7.]), 'currentState': array([8.85806581, 7.26460442, 0.76129454]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 8.146232736619204}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9207576965057933
{'scaleFactor': 20, 'currentTarget': array([17.,  7.]), 'previousTarget': array([17.,  7.]), 'currentState': array([16.61443979,  7.03962006,  6.24755043]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 0.38759054110679664}
episode index:1172
target Thresh 31.999791882482228
target distance 6.0
model initialize at round 1172
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([5.52521354, 8.2885847 , 0.57356184]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 4.53098505472371}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9208082867048506
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([9.41628781, 8.92447715, 0.19137147]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.5885776297041416}
episode index:1173
target Thresh 31.99979395328613
target distance 12.0
model initialize at round 1173
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 24.]), 'previousTarget': array([25., 24.]), 'currentState': array([14.97856947, 28.83518917,  5.98861767]), 'targetState': array([25, 24], dtype=int32), 'currentDistance': 11.126909908916893}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9208258947650688
{'scaleFactor': 20, 'currentTarget': array([25., 24.]), 'previousTarget': array([25., 24.]), 'currentState': array([25.82628981, 23.90405717,  6.07695709]), 'targetState': array([25, 24], dtype=int32), 'currentDistance': 0.8318412527631789}
episode index:1174
target Thresh 31.999796003485187
target distance 10.0
model initialize at round 1174
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 17.]), 'previousTarget': array([ 9., 17.]), 'currentState': array([13.08971915, 25.04285482,  4.40373808]), 'targetState': array([ 9, 17], dtype=int32), 'currentDistance': 9.02293280945819}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9208597416716517
{'scaleFactor': 20, 'currentTarget': array([ 9., 17.]), 'previousTarget': array([ 9., 17.]), 'currentState': array([ 9.36844578, 17.9964343 ,  4.37845351]), 'targetState': array([ 9, 17], dtype=int32), 'currentDistance': 1.0623716902829456}
episode index:1175
target Thresh 31.999798033284424
target distance 7.0
model initialize at round 1175
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 26.]), 'previousTarget': array([ 2., 26.]), 'currentState': array([ 2.58757063, 20.81519198,  2.45233774]), 'targetState': array([ 2, 26], dtype=int32), 'currentDistance': 5.217995160372493}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9209017818573052
{'scaleFactor': 20, 'currentTarget': array([ 2., 26.]), 'previousTarget': array([ 2., 26.]), 'currentState': array([ 1.92818099, 26.5853118 ,  1.70540202]), 'targetState': array([ 2, 26], dtype=int32), 'currentDistance': 0.5897015150088283}
episode index:1176
target Thresh 31.99980004288682
target distance 21.0
model initialize at round 1176
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 28.]), 'previousTarget': array([20., 28.]), 'currentState': array([20.22450723,  8.49365418,  3.05967724]), 'targetState': array([20, 28], dtype=int32), 'currentDistance': 19.507637759017747}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9208800626324974
{'scaleFactor': 20, 'currentTarget': array([20., 28.]), 'previousTarget': array([20., 28.]), 'currentState': array([19.94572646, 28.48876602,  1.59818555]), 'targetState': array([20, 28], dtype=int32), 'currentDistance': 0.4917701046335482}
episode index:1177
target Thresh 31.99980203249334
target distance 20.0
model initialize at round 1177
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 26.]), 'previousTarget': array([22., 26.]), 'currentState': array([1.82860703, 4.32578002, 5.62037253]), 'targetState': array([22, 26], dtype=int32), 'currentDistance': 29.608392491771202}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.9208284313014353
{'scaleFactor': 20, 'currentTarget': array([22., 26.]), 'previousTarget': array([22., 26.]), 'currentState': array([21.32780666, 25.43058814,  0.66479387]), 'targetState': array([22, 26], dtype=int32), 'currentDistance': 0.8809504822567525}
episode index:1178
target Thresh 31.999804002302945
target distance 20.0
model initialize at round 1178
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 29.]), 'previousTarget': array([27., 29.]), 'currentState': array([24.51249162,  9.73807668,  1.46399134]), 'targetState': array([27, 29], dtype=int32), 'currentDistance': 19.421879099731914}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9208144818898214
{'scaleFactor': 20, 'currentTarget': array([27., 29.]), 'previousTarget': array([27., 29.]), 'currentState': array([27.00972539, 29.29414489,  1.41672003]), 'targetState': array([27, 29], dtype=int32), 'currentDistance': 0.29430562327990645}
episode index:1179
target Thresh 31.999805952512613
target distance 21.0
model initialize at round 1179
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 26.]), 'previousTarget': array([27., 26.]), 'currentState': array([18.01816135,  6.36688732,  1.18370056]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 21.590102803419047}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9207928918664051
{'scaleFactor': 20, 'currentTarget': array([27., 26.]), 'previousTarget': array([27., 26.]), 'currentState': array([26.96635923, 25.94150763,  1.01505108]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 0.06747635808224572}
episode index:1180
target Thresh 31.999807883317374
target distance 12.0
model initialize at round 1180
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 21.]), 'previousTarget': array([15., 21.]), 'currentState': array([26.07863087, 26.55314197,  5.09987152]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 12.39247544472954}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9208024367064056
{'scaleFactor': 20, 'currentTarget': array([15., 21.]), 'previousTarget': array([15., 21.]), 'currentState': array([14.17608535, 20.84563915,  3.47062755]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 0.8382497361333249}
episode index:1181
target Thresh 31.999809794910306
target distance 18.0
model initialize at round 1181
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 21.]), 'previousTarget': array([ 5., 21.]), 'currentState': array([13.4046345 ,  3.53590765,  1.80752015]), 'targetState': array([ 5, 21], dtype=int32), 'currentDistance': 19.38123842292667}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9207885446914329
{'scaleFactor': 20, 'currentTarget': array([ 5., 21.]), 'previousTarget': array([ 5., 21.]), 'currentState': array([ 4.99232771, 21.11389293,  2.03489173]), 'targetState': array([ 5, 21], dtype=int32), 'currentDistance': 0.1141510566574694}
episode index:1182
target Thresh 31.99981168748257
target distance 12.0
model initialize at round 1182
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 16.]), 'previousTarget': array([15., 16.]), 'currentState': array([24.36908497,  5.15166411,  2.42397778]), 'targetState': array([15, 16], dtype=int32), 'currentDistance': 14.334090298882137}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.920798077069468
{'scaleFactor': 20, 'currentTarget': array([15., 16.]), 'previousTarget': array([15., 16.]), 'currentState': array([15.33821329, 15.70356915,  2.2526018 ]), 'targetState': array([15, 16], dtype=int32), 'currentDistance': 0.4497326715667374}
episode index:1183
target Thresh 31.999813561223426
target distance 18.0
model initialize at round 1183
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 22.]), 'previousTarget': array([24., 22.]), 'currentState': array([ 7.61654959, 21.53185952,  0.72811859]), 'targetState': array([24, 22], dtype=int32), 'currentDistance': 16.39013736643596}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9207997211719666
{'scaleFactor': 20, 'currentTarget': array([24., 22.]), 'previousTarget': array([24., 22.]), 'currentState': array([23.22762547, 21.77542786,  6.19117295]), 'targetState': array([24, 22], dtype=int32), 'currentDistance': 0.8043600328597432}
episode index:1184
target Thresh 31.99981541632025
target distance 12.0
model initialize at round 1184
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([11.93186382, 22.52152936,  6.21341032]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 10.666233411953417}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9208251982426231
{'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([21.29089331, 19.25095401,  5.94553419]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 0.7522035713076243}
episode index:1185
target Thresh 31.99981725295855
target distance 2.0
model initialize at round 1185
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.37722261, 6.03236673, 4.66105104]), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.2056669311036459}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9208835243823849
{'scaleFactor': 20, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.39119493, 4.46111543, 5.92250908]), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.6659054370071937}
episode index:1186
target Thresh 31.999819071321994
target distance 7.0
model initialize at round 1186
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 29.]), 'previousTarget': array([ 8., 29.]), 'currentState': array([13.33057852, 23.2131197 ,  2.00461864]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 7.8678491927043295}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9209169805623492
{'scaleFactor': 20, 'currentTarget': array([ 8., 29.]), 'previousTarget': array([ 8., 29.]), 'currentState': array([ 7.95602651, 29.0489021 ,  2.37935243]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 0.06576536565975864}
episode index:1187
target Thresh 31.99982087159242
target distance 14.0
model initialize at round 1187
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  7.]), 'previousTarget': array([18.,  7.]), 'currentState': array([ 5.60775701, 20.58614865,  5.67117769]), 'targetState': array([18,  7], dtype=int32), 'currentDistance': 18.388885810541147}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9209107518308014
{'scaleFactor': 20, 'currentTarget': array([18.,  7.]), 'previousTarget': array([18.,  7.]), 'currentState': array([17.5918535 ,  7.32064676,  5.3946346 ]), 'targetState': array([18,  7], dtype=int32), 'currentDistance': 0.5190355528081413}
episode index:1188
target Thresh 31.999822653949856
target distance 11.0
model initialize at round 1188
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 11.]), 'previousTarget': array([20., 11.]), 'currentState': array([8.97777855, 1.31717669, 5.70918489]), 'targetState': array([20, 11], dtype=int32), 'currentDistance': 14.671279186242927}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9209122942551892
{'scaleFactor': 20, 'currentTarget': array([20., 11.]), 'previousTarget': array([20., 11.]), 'currentState': array([20.71137559, 11.28554035,  0.71410703]), 'targetState': array([20, 11], dtype=int32), 'currentDistance': 0.7665432301141961}
episode index:1189
target Thresh 31.999824418572537
target distance 13.0
model initialize at round 1189
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 23.]), 'previousTarget': array([12., 23.]), 'currentState': array([26.6826066 , 19.96502692,  0.98921793]), 'targetState': array([12, 23], dtype=int32), 'currentDistance': 14.99299830385777}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.920913834087267
{'scaleFactor': 20, 'currentTarget': array([12., 23.]), 'previousTarget': array([12., 23.]), 'currentState': array([12.41640624, 23.13367007,  2.84967547]), 'targetState': array([12, 23], dtype=int32), 'currentDistance': 0.4373349320823082}
episode index:1190
target Thresh 31.999826165636932
target distance 22.0
model initialize at round 1190
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  4.]), 'previousTarget': array([20.,  4.]), 'currentState': array([10.33973444, 24.5160103 ,  5.39974451]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 22.676582837627112}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9208848425151669
{'scaleFactor': 20, 'currentTarget': array([20.,  4.]), 'previousTarget': array([20.,  4.]), 'currentState': array([19.9517203 ,  3.04774277,  4.56476839]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 0.9534803413404315}
episode index:1191
target Thresh 31.999827895317743
target distance 4.0
model initialize at round 1191
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 17.]), 'previousTarget': array([18., 17.]), 'currentState': array([18.35508767, 20.64411866,  4.36466146]), 'targetState': array([18, 17], dtype=int32), 'currentDistance': 3.6613778926265885}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9209345196607078
{'scaleFactor': 20, 'currentTarget': array([18., 17.]), 'previousTarget': array([18., 17.]), 'currentState': array([18.02275088, 16.68167422,  4.63848407]), 'targetState': array([18, 17], dtype=int32), 'currentDistance': 0.3191377489775703}
episode index:1192
target Thresh 31.999829607787944
target distance 9.0
model initialize at round 1192
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 16.]), 'previousTarget': array([15., 16.]), 'currentState': array([ 6.62638526, 24.02759544,  4.76761198]), 'targetState': array([15, 16], dtype=int32), 'currentDistance': 11.599987599448292}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.920943849776589
{'scaleFactor': 20, 'currentTarget': array([15., 16.]), 'previousTarget': array([15., 16.]), 'currentState': array([15.21483491, 15.43159737,  5.27906591]), 'targetState': array([15, 16], dtype=int32), 'currentDistance': 0.6076475874402432}
episode index:1193
target Thresh 31.999831303218784
target distance 22.0
model initialize at round 1193
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 27.]), 'previousTarget': array([ 3., 27.]), 'currentState': array([23.32860113, 15.19701248,  2.01426125]), 'targetState': array([ 3, 27], dtype=int32), 'currentDistance': 23.50664881413307}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9209074822499745
{'scaleFactor': 20, 'currentTarget': array([ 3., 27.]), 'previousTarget': array([ 3., 27.]), 'currentState': array([ 3.71137587, 26.51822048,  2.39860713]), 'targetState': array([ 3, 27], dtype=int32), 'currentDistance': 0.8591665365238417}
episode index:1194
target Thresh 31.9998329817798
target distance 11.0
model initialize at round 1194
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 18.]), 'previousTarget': array([ 9., 18.]), 'currentState': array([ 4.11793562, 27.7419825 ,  4.42889713]), 'targetState': array([ 9, 18], dtype=int32), 'currentDistance': 10.896824106463276}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9209246978710215
{'scaleFactor': 20, 'currentTarget': array([ 9., 18.]), 'previousTarget': array([ 9., 18.]), 'currentState': array([ 8.75002331, 18.73513426,  4.28140019]), 'targetState': array([ 9, 18], dtype=int32), 'currentDistance': 0.776473261266626}
episode index:1195
target Thresh 31.99983464363886
target distance 12.0
model initialize at round 1195
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  3.]), 'previousTarget': array([12.,  3.]), 'currentState': array([ 3.32652201, 14.82150771,  4.25785041]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 14.662103022179542}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9209262196072731
{'scaleFactor': 20, 'currentTarget': array([12.,  3.]), 'previousTarget': array([12.,  3.]), 'currentState': array([11.33167656,  3.9692911 ,  4.37027168]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 1.1773620767006303}
episode index:1196
target Thresh 31.999836288962143
target distance 8.0
model initialize at round 1196
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 22.]), 'previousTarget': array([15., 22.]), 'currentState': array([ 8.18888819, 23.19118972,  6.05955047]), 'targetState': array([15, 22], dtype=int32), 'currentDistance': 6.914490366004412}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9209674667086872
{'scaleFactor': 20, 'currentTarget': array([15., 22.]), 'previousTarget': array([15., 22.]), 'currentState': array([14.04811203, 21.96799973,  6.17905021]), 'targetState': array([15, 22], dtype=int32), 'currentDistance': 0.9524257069379103}
episode index:1197
target Thresh 31.99983791791419
target distance 10.0
model initialize at round 1197
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 20.]), 'previousTarget': array([ 9., 20.]), 'currentState': array([15.69520885, 11.5326685 ,  2.15495786]), 'targetState': array([ 9, 20], dtype=int32), 'currentDistance': 10.794513615693848}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9209925272956582
{'scaleFactor': 20, 'currentTarget': array([ 9., 20.]), 'previousTarget': array([ 9., 20.]), 'currentState': array([ 9.54047186, 19.29377551,  2.271688  ]), 'targetState': array([ 9, 20], dtype=int32), 'currentDistance': 0.8893047030533078}
episode index:1198
target Thresh 31.999839530657887
target distance 7.0
model initialize at round 1198
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 24.]), 'previousTarget': array([20., 24.]), 'currentState': array([14.37143032, 18.56118168,  0.94356298]), 'targetState': array([20, 24], dtype=int32), 'currentDistance': 7.826975240925752}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9210255577232682
{'scaleFactor': 20, 'currentTarget': array([20., 24.]), 'previousTarget': array([20., 24.]), 'currentState': array([20.20011894, 23.93940158,  0.60440203]), 'targetState': array([20, 24], dtype=int32), 'currentDistance': 0.2090927048932065}
episode index:1199
target Thresh 31.999841127354518
target distance 17.0
model initialize at round 1199
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([26.36237993, 10.64349289,  2.3637751 ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 17.374300557696163}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9210116881543393
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([9.78605652, 9.568709  , 1.91179833]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.8966029098131129}
episode index:1200
target Thresh 31.999842708163754
target distance 10.0
model initialize at round 1200
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'previousTarget': array([22., 18.]), 'currentState': array([22.0593424 , 26.31807653,  5.75765681]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 8.318288204503377}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9210366493214881
{'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'previousTarget': array([22., 18.]), 'currentState': array([22.2188366 , 17.88215617,  3.66368783]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 0.24854903527296912}
episode index:1201
target Thresh 31.999844273243674
target distance 7.0
model initialize at round 1201
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([6.094934  , 6.27808749, 6.13561231]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 6.041797090319975}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9210776329742989
{'scaleFactor': 20, 'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([11.92109795,  4.98664305,  6.04618107]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 0.08002463236206049}
episode index:1202
target Thresh 31.99984582275079
target distance 14.0
model initialize at round 1202
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 15.]), 'previousTarget': array([ 2., 15.]), 'currentState': array([14.98880009, 21.34531142,  3.22535098]), 'targetState': array([ 2, 15], dtype=int32), 'currentDistance': 14.455860568021361}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9210867665694216
{'scaleFactor': 20, 'currentTarget': array([ 2., 15.]), 'previousTarget': array([ 2., 15.]), 'currentState': array([ 2.47664949, 15.48017663,  3.5037432 ]), 'targetState': array([ 2, 15], dtype=int32), 'currentDistance': 0.6765828327862146}
episode index:1203
target Thresh 31.999847356840053
target distance 13.0
model initialize at round 1203
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  2.]), 'previousTarget': array([11.,  2.]), 'currentState': array([22.70360769, 13.47116625,  4.13371766]), 'targetState': array([11,  2], dtype=int32), 'currentDistance': 16.38786404301897}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9210881435859153
{'scaleFactor': 20, 'currentTarget': array([11.,  2.]), 'previousTarget': array([11.,  2.]), 'currentState': array([11.46273124,  2.4234638 ,  3.7285839 ]), 'targetState': array([11,  2], dtype=int32), 'currentDistance': 0.6272493882449623}
episode index:1204
target Thresh 31.99984887566487
target distance 19.0
model initialize at round 1204
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 23.]), 'previousTarget': array([17., 23.]), 'currentState': array([24.35164241,  5.00272173,  1.64826744]), 'targetState': array([17, 23], dtype=int32), 'currentDistance': 19.44090201531866}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9210742796285898
{'scaleFactor': 20, 'currentTarget': array([17., 23.]), 'previousTarget': array([17., 23.]), 'currentState': array([16.83452693, 23.30550105,  1.91221083]), 'targetState': array([17, 23], dtype=int32), 'currentDistance': 0.3474366495495574}
episode index:1205
target Thresh 31.99985037937713
target distance 15.0
model initialize at round 1205
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 29.]), 'previousTarget': array([12., 29.]), 'currentState': array([26.08901307, 22.41509396,  3.15277541]), 'targetState': array([12, 29], dtype=int32), 'currentDistance': 15.551889821101069}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.921075664715488
{'scaleFactor': 20, 'currentTarget': array([12., 29.]), 'previousTarget': array([12., 29.]), 'currentState': array([11.73261807, 29.12784043,  2.7136861 ]), 'targetState': array([12, 29], dtype=int32), 'currentDistance': 0.2963718465413415}
episode index:1206
target Thresh 31.9998518681272
target distance 18.0
model initialize at round 1206
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([18.3860613 ,  6.04542043,  0.40689867]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 19.70496746792026}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9210618340695006
{'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([13.19238149, 24.57074733,  1.88760405]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 0.47039185104834425}
episode index:1207
target Thresh 31.999853342063965
target distance 19.0
model initialize at round 1207
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 16.]), 'previousTarget': array([21., 16.]), 'currentState': array([1.2115015 , 2.51317176, 5.23477864]), 'targetState': array([21, 16], dtype=int32), 'currentDistance': 23.947425929661588}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.921033127974837
{'scaleFactor': 20, 'currentTarget': array([21., 16.]), 'previousTarget': array([21., 16.]), 'currentState': array([20.09503747, 15.49820476,  0.56308803]), 'targetState': array([21, 16], dtype=int32), 'currentDistance': 1.034773232905376}
episode index:1208
target Thresh 31.99985480133481
target distance 12.0
model initialize at round 1208
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 24.]), 'previousTarget': array([13., 24.]), 'currentState': array([ 7.85824757, 13.76275947,  1.46604156]), 'targetState': array([13, 24], dtype=int32), 'currentDistance': 11.455946567157683}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9210500403167943
{'scaleFactor': 20, 'currentTarget': array([13., 24.]), 'previousTarget': array([13., 24.]), 'currentState': array([13.19428675, 24.35200263,  1.19422705]), 'targetState': array([13, 24], dtype=int32), 'currentDistance': 0.40206117822106896}
episode index:1209
target Thresh 31.999856246085667
target distance 6.0
model initialize at round 1209
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([24.65512191, 16.30489268,  1.19216936]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 8.980251900187795}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9210747841263671
{'scaleFactor': 20, 'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([16.66391427, 21.12892109,  2.73122359]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 0.3599642535621589}
episode index:1210
target Thresh 31.999857676461012
target distance 18.0
model initialize at round 1210
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([24.772814  , 25.16390198,  4.95528102]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 16.210419882617664}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.921076163077896
{'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([25.99277106,  9.35112707,  4.78705269]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 0.3512014746416415}
episode index:1211
target Thresh 31.999859092603888
target distance 14.0
model initialize at round 1211
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 13.]), 'previousTarget': array([ 4., 13.]), 'currentState': array([15.01297   , 25.63685427,  3.07568562]), 'targetState': array([ 4, 13], dtype=int32), 'currentDistance': 16.76232663448568}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9210623890778389
{'scaleFactor': 20, 'currentTarget': array([ 4., 13.]), 'previousTarget': array([ 4., 13.]), 'currentState': array([ 3.80768986, 12.65301746,  3.68201259]), 'targetState': array([ 4, 13], dtype=int32), 'currentDistance': 0.39671156956848513}
episode index:1212
target Thresh 31.999860494655906
target distance 13.0
model initialize at round 1212
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 17.]), 'previousTarget': array([14., 17.]), 'currentState': array([10.22087989,  5.49176404,  1.04210949]), 'targetState': array([14, 17], dtype=int32), 'currentDistance': 12.112854484695996}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9210792215265802
{'scaleFactor': 20, 'currentTarget': array([14., 17.]), 'previousTarget': array([14., 17.]), 'currentState': array([13.97056546, 16.82442041,  1.47000528]), 'targetState': array([14, 17], dtype=int32), 'currentDistance': 0.17802973035857383}
episode index:1213
target Thresh 31.999861882757273
target distance 7.0
model initialize at round 1213
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 13.]), 'previousTarget': array([21., 13.]), 'currentState': array([19.38229912, 21.56551392,  2.95661068]), 'targetState': array([21, 13], dtype=int32), 'currentDistance': 8.71693666542639}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9211038597707099
{'scaleFactor': 20, 'currentTarget': array([21., 13.]), 'previousTarget': array([21., 13.]), 'currentState': array([21.00517639, 12.92598076,  5.30998299]), 'targetState': array([21, 13], dtype=int32), 'currentDistance': 0.07420001602770265}
episode index:1214
target Thresh 31.999863257046798
target distance 18.0
model initialize at round 1214
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  5.]), 'previousTarget': array([13.,  5.]), 'currentState': array([24.47110824, 21.4022949 ,  5.40271091]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 20.01553402164284}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.921082653511029
{'scaleFactor': 20, 'currentTarget': array([13.,  5.]), 'previousTarget': array([13.,  5.]), 'currentState': array([13.17525193,  5.33345717,  4.118222  ]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 0.376705351470676}
episode index:1215
target Thresh 31.999864617661917
target distance 10.0
model initialize at round 1215
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  7.]), 'previousTarget': array([22.,  7.]), 'currentState': array([25.18467955, 15.17377275,  4.28189922]), 'targetState': array([22,  7], dtype=int32), 'currentDistance': 8.772271361921325}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9211151480476154
{'scaleFactor': 20, 'currentTarget': array([22.,  7.]), 'previousTarget': array([22.,  7.]), 'currentState': array([22.2240568 ,  7.77888495,  4.5182184 ]), 'targetState': array([22,  7], dtype=int32), 'currentDistance': 0.8104709836855231}
episode index:1216
target Thresh 31.99986596473869
target distance 12.0
model initialize at round 1216
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 14.]), 'previousTarget': array([17., 14.]), 'currentState': array([22.41509275,  2.9109888 ,  1.5819804 ]), 'targetState': array([17, 14], dtype=int32), 'currentDistance': 12.340559101449864}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9211318818202969
{'scaleFactor': 20, 'currentTarget': array([17., 14.]), 'previousTarget': array([17., 14.]), 'currentState': array([17.32906211, 13.58346557,  2.28982212]), 'targetState': array([17, 14], dtype=int32), 'currentDistance': 0.5308321795977441}
episode index:1217
target Thresh 31.99986729841182
target distance 26.0
model initialize at round 1217
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([19.19977988, 29.48055255,  3.07630694]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 27.896646420140907}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.9210817393513486
{'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([24.11748152,  1.35847632,  4.89391308]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 0.6521921007917336}
episode index:1218
target Thresh 31.999868618814688
target distance 13.0
model initialize at round 1218
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 21.]), 'previousTarget': array([18., 21.]), 'currentState': array([ 5.44900279, 19.62196935,  0.29073358]), 'targetState': array([18, 21], dtype=int32), 'currentDistance': 12.626420687611903}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9210984730757537
{'scaleFactor': 20, 'currentTarget': array([18., 21.]), 'previousTarget': array([18., 21.]), 'currentState': array([17.29319914, 20.88454325,  0.12057382]), 'targetState': array([18, 21], dtype=int32), 'currentDistance': 0.7161687725651721}
episode index:1219
target Thresh 31.999869926079324
target distance 22.0
model initialize at round 1219
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([22.02450529, 22.01217535,  2.86127162]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 27.61238171023749}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.9210413905331464
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.51752588, 3.42594355, 2.91924644]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.670269308805508}
episode index:1220
target Thresh 31.99987122033646
target distance 5.0
model initialize at round 1220
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 25.]), 'previousTarget': array([22., 25.]), 'currentState': array([18.56097609, 29.62908008,  5.65627891]), 'targetState': array([22, 25], dtype=int32), 'currentDistance': 5.766738057290813}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9210817325556419
{'scaleFactor': 20, 'currentTarget': array([22., 25.]), 'previousTarget': array([22., 25.]), 'currentState': array([21.97355466, 24.73880048,  5.28534114]), 'targetState': array([22, 25], dtype=int32), 'currentDistance': 0.262534848371349}
episode index:1221
target Thresh 31.99987250171552
target distance 5.0
model initialize at round 1221
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 14.]), 'previousTarget': array([24., 14.]), 'currentState': array([20.85218198, 13.64335595,  6.26981062]), 'targetState': array([24, 14], dtype=int32), 'currentDistance': 3.167957265829659}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9211300290101789
{'scaleFactor': 20, 'currentTarget': array([24., 14.]), 'previousTarget': array([24., 14.]), 'currentState': array([24.83798842, 13.93806555,  0.12106402]), 'targetState': array([24, 14], dtype=int32), 'currentDistance': 0.8402740398275617}
episode index:1222
target Thresh 31.999873770344646
target distance 25.0
model initialize at round 1222
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 29.]), 'previousTarget': array([25.15249803, 27.88486582]), 'currentState': array([7.47674744, 5.61403221, 0.27358651]), 'targetState': array([26, 29], dtype=int32), 'currentDistance': 29.83310870757925}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.9210730606880895
{'scaleFactor': 20, 'currentTarget': array([26., 29.]), 'previousTarget': array([26., 29.]), 'currentState': array([25.14810561, 28.36609007,  0.85935345]), 'targetState': array([26, 29], dtype=int32), 'currentDistance': 1.0618690399879451}
episode index:1223
target Thresh 31.999875026350704
target distance 13.0
model initialize at round 1223
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([24.10365831,  6.57558445,  3.14072514]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 12.344080978737516}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9210897331461884
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([12.49040123,  8.94333743,  2.75948454]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.49366386260850154}
episode index:1224
target Thresh 31.999876269859293
target distance 4.0
model initialize at round 1224
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  5.]), 'previousTarget': array([10.,  5.]), 'currentState': array([7.40234955, 8.47094317, 4.47137403]), 'targetState': array([10,  5], dtype=int32), 'currentDistance': 4.335347087474465}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9211379047925996
{'scaleFactor': 20, 'currentTarget': array([10.,  5.]), 'previousTarget': array([10.,  5.]), 'currentState': array([9.64109391, 5.93854977, 4.42473304]), 'targetState': array([10,  5], dtype=int32), 'currentDistance': 1.00483294405893}
episode index:1225
target Thresh 31.99987750099476
target distance 5.0
model initialize at round 1225
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 20.]), 'previousTarget': array([12., 20.]), 'currentState': array([ 8.08739521, 16.64594341,  0.69733387]), 'targetState': array([12, 20], dtype=int32), 'currentDistance': 5.153462121282621}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9211859978555746
{'scaleFactor': 20, 'currentTarget': array([12., 20.]), 'previousTarget': array([12., 20.]), 'currentState': array([11.28330035, 19.0219856 ,  0.86414731]), 'targetState': array([12, 20], dtype=int32), 'currentDistance': 1.212505894609748}
episode index:1226
target Thresh 31.99987871988023
target distance 14.0
model initialize at round 1226
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([26.73372203, 15.48146991,  6.16655648]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 12.48430996033671}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9211948644815333
{'scaleFactor': 20, 'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([26.89493183,  2.32930053,  4.67986805]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 0.6788792977490404}
episode index:1227
target Thresh 31.999879926637583
target distance 14.0
model initialize at round 1227
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 17.]), 'previousTarget': array([20., 17.]), 'currentState': array([ 7.51015484, 11.30653016,  0.61572069]), 'targetState': array([20, 17], dtype=int32), 'currentDistance': 13.726318915115206}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9212037166667332
{'scaleFactor': 20, 'currentTarget': array([20., 17.]), 'previousTarget': array([20., 17.]), 'currentState': array([20.18088367, 17.09956581,  0.45874664]), 'targetState': array([20, 17], dtype=int32), 'currentDistance': 0.20647579060544757}
episode index:1228
target Thresh 31.999881121387503
target distance 15.0
model initialize at round 1228
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 26.]), 'previousTarget': array([18., 26.]), 'currentState': array([13.64642387, 10.65117837,  0.80122107]), 'targetState': array([18, 26], dtype=int32), 'currentDistance': 15.954308208574354}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9212049705135689
{'scaleFactor': 20, 'currentTarget': array([18., 26.]), 'previousTarget': array([18., 26.]), 'currentState': array([17.95150053, 25.80548232,  1.33357751]), 'targetState': array([18, 26], dtype=int32), 'currentDistance': 0.20047275742581278}
episode index:1229
target Thresh 31.99988230424946
target distance 15.0
model initialize at round 1229
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'previousTarget': array([ 5., 22.]), 'currentState': array([18.50631162, 12.2437818 ,  2.86398643]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 16.661459936044697}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9212062223216293
{'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'previousTarget': array([ 5., 22.]), 'currentState': array([ 5.65099084, 21.46306875,  2.50697445]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 0.8438508364971983}
episode index:1230
target Thresh 31.999883475341743
target distance 22.0
model initialize at round 1230
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 13.]), 'previousTarget': array([25., 13.]), 'currentState': array([ 1.63689147, 27.01292187,  4.77833533]), 'targetState': array([25, 13], dtype=int32), 'currentDistance': 27.243289438787215}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.9211495623287561
{'scaleFactor': 20, 'currentTarget': array([25., 13.]), 'previousTarget': array([25., 13.]), 'currentState': array([24.65935295, 12.82927887,  4.66655028]), 'targetState': array([25, 13], dtype=int32), 'currentDistance': 0.3810329553026445}
episode index:1231
target Thresh 31.999884634781466
target distance 13.0
model initialize at round 1231
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  6.]), 'previousTarget': array([22.,  6.]), 'currentState': array([12.28203608, 17.47785812,  5.28165293]), 'targetState': array([22,  6], dtype=int32), 'currentDistance': 15.039283555727344}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9211584225443229
{'scaleFactor': 20, 'currentTarget': array([22.,  6.]), 'previousTarget': array([22.,  6.]), 'currentState': array([21.27334608,  6.81086667,  5.50868897]), 'targetState': array([22,  6], dtype=int32), 'currentDistance': 1.0888207779658945}
episode index:1232
target Thresh 31.999885782684572
target distance 11.0
model initialize at round 1232
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 14.]), 'previousTarget': array([24., 14.]), 'currentState': array([14.58983238, 23.44789395,  4.93894184]), 'targetState': array([24, 14], dtype=int32), 'currentDistance': 13.334689898877476}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9211672683880882
{'scaleFactor': 20, 'currentTarget': array([24., 14.]), 'previousTarget': array([24., 14.]), 'currentState': array([23.36015263, 14.97762697,  5.17844724]), 'targetState': array([24, 14], dtype=int32), 'currentDistance': 1.1684002564516374}
episode index:1233
target Thresh 31.99988691916585
target distance 12.0
model initialize at round 1233
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 14.]), 'previousTarget': array([18., 14.]), 'currentState': array([16.40835081,  1.07862272,  0.4306609 ]), 'targetState': array([18, 14], dtype=int32), 'currentDistance': 13.019037518980781}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9211685466911997
{'scaleFactor': 20, 'currentTarget': array([18., 14.]), 'previousTarget': array([18., 14.]), 'currentState': array([18.26332841, 14.42783193,  0.44221049]), 'targetState': array([18, 14], dtype=int32), 'currentDistance': 0.5023763632623495}
episode index:1234
target Thresh 31.99988804433895
target distance 12.0
model initialize at round 1234
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([20.45223759, 14.6609233 ,  3.7480135 ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 13.42093135410233}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9211773700120223
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.21476685,  2.51494206,  4.23728438]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.5579337996320554}
episode index:1235
target Thresh 31.99988915831639
target distance 3.0
model initialize at round 1235
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 29.]), 'previousTarget': array([13., 29.]), 'currentState': array([14.38113272, 26.53993798,  2.40848041]), 'targetState': array([13, 29], dtype=int32), 'currentDistance': 2.8212466600425925}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9212330517514947
{'scaleFactor': 20, 'currentTarget': array([13., 29.]), 'previousTarget': array([13., 29.]), 'currentState': array([13.29124775, 28.19071178,  1.89503968]), 'targetState': array([13, 29], dtype=int32), 'currentDistance': 0.860100391113746}
episode index:1236
target Thresh 31.99989026120957
target distance 14.0
model initialize at round 1236
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 13.]), 'previousTarget': array([16., 13.]), 'currentState': array([ 0.50064861, 15.23558291,  4.62307477]), 'targetState': array([16, 13], dtype=int32), 'currentDistance': 15.659748544566222}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9212342737746768
{'scaleFactor': 20, 'currentTarget': array([16., 13.]), 'previousTarget': array([16., 13.]), 'currentState': array([15.62092277, 13.04579643,  6.21482519]), 'targetState': array([16, 13], dtype=int32), 'currentDistance': 0.38183355011419784}
episode index:1237
target Thresh 31.999891353128778
target distance 12.0
model initialize at round 1237
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.67678513, 21.34835832,  5.52913809]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 10.353404647317381}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9212583091350365
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.92758557, 11.66923698,  4.73090647]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.6731433585843682}
episode index:1238
target Thresh 31.99989243418321
target distance 6.0
model initialize at round 1238
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 21.]), 'previousTarget': array([ 3., 21.]), 'currentState': array([ 8.05767835, 19.6055761 ,  3.10810471]), 'targetState': array([ 3, 21], dtype=int32), 'currentDistance': 5.246382404357246}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9212978899993344
{'scaleFactor': 20, 'currentTarget': array([ 3., 21.]), 'previousTarget': array([ 3., 21.]), 'currentState': array([ 2.29315543, 21.09960836,  2.99178594]), 'targetState': array([ 3, 21], dtype=int32), 'currentDistance': 0.7138284623653844}
episode index:1239
target Thresh 31.99989350448097
target distance 11.0
model initialize at round 1239
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 23.]), 'previousTarget': array([ 5., 23.]), 'currentState': array([ 8.39950843, 11.06524642,  0.42113322]), 'targetState': array([ 5, 23], dtype=int32), 'currentDistance': 12.40947220745076}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9213065734331308
{'scaleFactor': 20, 'currentTarget': array([ 5., 23.]), 'previousTarget': array([ 5., 23.]), 'currentState': array([ 4.7383203 , 23.83587512,  1.92575206]), 'targetState': array([ 5, 23], dtype=int32), 'currentDistance': 0.8758786938282741}
episode index:1240
target Thresh 31.999894564129086
target distance 9.0
model initialize at round 1240
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 28.]), 'previousTarget': array([12., 28.]), 'currentState': array([19.64503995, 23.99823412,  3.51664126]), 'targetState': array([12, 28], dtype=int32), 'currentDistance': 8.629065189210221}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9213382329307673
{'scaleFactor': 20, 'currentTarget': array([12., 28.]), 'previousTarget': array([12., 28.]), 'currentState': array([12.92440672, 27.70542251,  2.8228527 ]), 'targetState': array([12, 28], dtype=int32), 'currentDistance': 0.9702080645058659}
episode index:1241
target Thresh 31.99989561323353
target distance 5.0
model initialize at round 1241
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 16.]), 'previousTarget': array([13., 16.]), 'currentState': array([12.32242366, 12.948295  ,  1.12851268]), 'targetState': array([13, 16], dtype=int32), 'currentDistance': 3.126021933462562}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9213855451425783
{'scaleFactor': 20, 'currentTarget': array([13., 16.]), 'previousTarget': array([13., 16.]), 'currentState': array([13.05042574, 16.84880317,  1.55801114]), 'targetState': array([13, 16], dtype=int32), 'currentDistance': 0.8502996976339601}
episode index:1242
target Thresh 31.999896651899213
target distance 22.0
model initialize at round 1242
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 13.]), 'previousTarget': array([ 5., 13.]), 'currentState': array([25.66231884, 20.97872765,  4.80365562]), 'targetState': array([ 5, 13], dtype=int32), 'currentDistance': 22.149300550989558}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9213502559051336
{'scaleFactor': 20, 'currentTarget': array([ 5., 13.]), 'previousTarget': array([ 5., 13.]), 'currentState': array([ 4.72238705, 12.91148441,  3.3167873 ]), 'targetState': array([ 5, 13], dtype=int32), 'currentDistance': 0.2913828353384541}
episode index:1243
target Thresh 31.999897680229996
target distance 9.0
model initialize at round 1243
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  8.]), 'previousTarget': array([21.,  8.]), 'currentState': array([13.21947057,  9.15986189,  6.03353602]), 'targetState': array([21,  8], dtype=int32), 'currentDistance': 7.866506074489804}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9213818039389721
{'scaleFactor': 20, 'currentTarget': array([21.,  8.]), 'previousTarget': array([21.,  8.]), 'currentState': array([21.08479978,  8.0331727 ,  0.04714104]), 'targetState': array([21,  8], dtype=int32), 'currentDistance': 0.09105729328793911}
episode index:1244
target Thresh 31.999898698328717
target distance 14.0
model initialize at round 1244
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  2.]), 'previousTarget': array([10.,  2.]), 'currentState': array([ 7.45272271, 14.37904686,  3.97477233]), 'targetState': array([10,  2], dtype=int32), 'currentDistance': 12.638410611029325}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9213903850987857
{'scaleFactor': 20, 'currentTarget': array([10.,  2.]), 'previousTarget': array([10.,  2.]), 'currentState': array([9.65998693, 2.76203471, 5.0298233 ]), 'targetState': array([10,  2], dtype=int32), 'currentDistance': 0.8344493954610723}
episode index:1245
target Thresh 31.99989970629719
target distance 9.0
model initialize at round 1245
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([ 3.81992903, 19.80007476,  4.94533277]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 12.242338900971532}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.921398952484667
{'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([13.75451748, 12.74962832,  5.6392833 ]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 0.35063890782731}
episode index:1246
target Thresh 31.999900704236207
target distance 14.0
model initialize at round 1246
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 24.]), 'previousTarget': array([ 5., 24.]), 'currentState': array([17.36464009, 28.3974746 ,  3.91316557]), 'targetState': array([ 5, 24], dtype=int32), 'currentDistance': 13.123342086563696}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.921407506129753
{'scaleFactor': 20, 'currentTarget': array([ 5., 24.]), 'previousTarget': array([ 5., 24.]), 'currentState': array([ 4.34740208, 23.73744929,  3.69377937]), 'targetState': array([ 5, 24], dtype=int32), 'currentDistance': 0.7034322397161292}
episode index:1247
target Thresh 31.999901692245565
target distance 16.0
model initialize at round 1247
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 26.]), 'previousTarget': array([16., 26.]), 'currentState': array([16.42654256, 11.58225619,  0.90850163]), 'targetState': array([16, 26], dtype=int32), 'currentDistance': 14.42405196560396}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9214085775947354
{'scaleFactor': 20, 'currentTarget': array([16., 26.]), 'previousTarget': array([16., 26.]), 'currentState': array([15.95290817, 25.64615859,  1.40113737]), 'targetState': array([16, 26], dtype=int32), 'currentDistance': 0.35696131758290045}
episode index:1248
target Thresh 31.999902670424063
target distance 11.0
model initialize at round 1248
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 15.]), 'previousTarget': array([ 9., 15.]), 'currentState': array([ 8.688428  , 24.94540672,  4.82880926]), 'targetState': array([ 9, 15], dtype=int32), 'currentDistance': 9.950286024760395}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9214322617198797
{'scaleFactor': 20, 'currentTarget': array([ 9., 15.]), 'previousTarget': array([ 9., 15.]), 'currentState': array([ 9.01948127, 14.97780759,  4.84095316]), 'targetState': array([ 9, 15], dtype=int32), 'currentDistance': 0.029530038490573802}
episode index:1249
target Thresh 31.999903638869526
target distance 11.0
model initialize at round 1249
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 18.]), 'previousTarget': array([24., 18.]), 'currentState': array([22.02429405, 30.37127167,  3.19922113]), 'targetState': array([24, 18], dtype=int32), 'currentDistance': 12.528039617541657}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9214407681888294
{'scaleFactor': 20, 'currentTarget': array([24., 18.]), 'previousTarget': array([24., 18.]), 'currentState': array([24.28026017, 17.40653506,  5.18956169]), 'targetState': array([24, 18], dtype=int32), 'currentDistance': 0.6563127284815943}
episode index:1250
target Thresh 31.999904597678796
target distance 9.0
model initialize at round 1250
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 16.]), 'previousTarget': array([ 7., 16.]), 'currentState': array([12.34035559,  8.08334518,  2.3278074 ]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 9.54949325113977}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.921464388717775
{'scaleFactor': 20, 'currentTarget': array([ 7., 16.]), 'previousTarget': array([ 7., 16.]), 'currentState': array([ 6.72547611, 16.28960176,  2.31546022]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 0.3990395336806005}
episode index:1251
target Thresh 31.99990554694775
target distance 5.0
model initialize at round 1251
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([13.57269696, 13.07093156,  4.22190672]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 3.4502168574052274}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9215112222731122
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([11.78803012,  9.49478107,  4.34545375]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.5478844759170627}
episode index:1252
target Thresh 31.999906486771323
target distance 14.0
model initialize at round 1252
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([23.26714615, 25.10757786,  1.72830456]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 18.880255229517665}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9214975517645213
{'scaleFactor': 20, 'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([ 8.13527754, 14.26710683,  3.91794493]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 0.29940953979133655}
episode index:1253
target Thresh 31.999907417243495
target distance 3.0
model initialize at round 1253
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 25.]), 'previousTarget': array([10., 25.]), 'currentState': array([14.51091954, 25.74129634,  1.46611529]), 'targetState': array([10, 25], dtype=int32), 'currentDistance': 4.57142377666137}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9215364683899085
{'scaleFactor': 20, 'currentTarget': array([10., 25.]), 'previousTarget': array([10., 25.]), 'currentState': array([ 9.71862195, 24.88130776,  3.75109783]), 'targetState': array([10, 25], dtype=int32), 'currentDistance': 0.3053873888862313}
episode index:1254
target Thresh 31.999908338457317
target distance 14.0
model initialize at round 1254
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  8.]), 'previousTarget': array([11.,  8.]), 'currentState': array([14.57846084, 20.04626313,  4.56308298]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 12.566536406486541}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9215523597692004
{'scaleFactor': 20, 'currentTarget': array([11.,  8.]), 'previousTarget': array([11.,  8.]), 'currentState': array([11.05213217,  8.63724947,  4.5332176 ]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 0.6393783328504771}
episode index:1255
target Thresh 31.999909250504903
target distance 13.0
model initialize at round 1255
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 20.]), 'previousTarget': array([24., 20.]), 'currentState': array([19.52696849,  5.40165952,  6.04086351]), 'targetState': array([24, 20], dtype=int32), 'currentDistance': 15.268253197777389}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9215533090802343
{'scaleFactor': 20, 'currentTarget': array([24., 20.]), 'previousTarget': array([24., 20.]), 'currentState': array([24.20017451, 19.9353387 ,  1.3911072 ]), 'targetState': array([24, 20], dtype=int32), 'currentDistance': 0.21035902476625107}
episode index:1256
target Thresh 31.99991015347747
target distance 6.0
model initialize at round 1256
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 18.]), 'previousTarget': array([ 5., 18.]), 'currentState': array([ 9.80938409, 13.57491598,  2.49902898]), 'targetState': array([ 5, 18], dtype=int32), 'currentDistance': 6.535406947810639}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9215920884683964
{'scaleFactor': 20, 'currentTarget': array([ 5., 18.]), 'previousTarget': array([ 5., 18.]), 'currentState': array([ 5.5346195 , 17.76200949,  2.48011099]), 'targetState': array([ 5, 18], dtype=int32), 'currentDistance': 0.5851986793442238}
episode index:1257
target Thresh 31.999911047465304
target distance 15.0
model initialize at round 1257
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 26.]), 'previousTarget': array([14., 26.]), 'currentState': array([ 8.1801441 , 12.19985332,  1.80367916]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 14.977141622070182}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9215930046893498
{'scaleFactor': 20, 'currentTarget': array([14., 26.]), 'previousTarget': array([14., 26.]), 'currentState': array([14.53046407, 26.48671844,  0.84324504]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 0.7199215004027145}
episode index:1258
target Thresh 31.99991193255781
target distance 25.0
model initialize at round 1258
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 28.]), 'previousTarget': array([13., 28.]), 'currentState': array([8.45778135, 3.84098848, 1.53325814]), 'targetState': array([13, 28], dtype=int32), 'currentDistance': 24.58230233074189}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9215650395321033
{'scaleFactor': 20, 'currentTarget': array([13., 28.]), 'previousTarget': array([13., 28.]), 'currentState': array([12.61812174, 27.10269464,  1.23228664]), 'targetState': array([13, 28], dtype=int32), 'currentDistance': 0.9751860871781951}
episode index:1259
target Thresh 31.999912808843504
target distance 15.0
model initialize at round 1259
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([ 6.34515059, 16.69363174,  4.33465362]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 15.06960006330557}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9215659757661475
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.82756412, 1.82864282, 4.04663551]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.24309959691173083}
episode index:1260
target Thresh 31.999913676410007
target distance 4.0
model initialize at round 1260
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 13.]), 'previousTarget': array([18., 13.]), 'currentState': array([15.31092413, 14.9406081 ,  4.59772036]), 'targetState': array([18, 13], dtype=int32), 'currentDistance': 3.316185889643391}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9216123945006709
{'scaleFactor': 20, 'currentTarget': array([18., 13.]), 'previousTarget': array([18., 13.]), 'currentState': array([17.91526177, 12.78558314,  4.61066434]), 'targetState': array([18, 13], dtype=int32), 'currentDistance': 0.2305540283536934}
episode index:1261
target Thresh 31.99991453534408
target distance 4.0
model initialize at round 1261
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([11.82867482, 22.46854211,  5.45529947]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 2.6213105401151666}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9216665843623977
{'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([13.37805194, 21.21563881,  5.75407303]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 0.6582700689043053}
episode index:1262
target Thresh 31.99991538573161
target distance 19.0
model initialize at round 1262
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  3.]), 'previousTarget': array([24.,  3.]), 'currentState': array([ 6.81144441, 14.17209892,  6.01425996]), 'targetState': array([24,  3], dtype=int32), 'currentDistance': 20.500298471440306}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9216528990818327
{'scaleFactor': 20, 'currentTarget': array([24.,  3.]), 'previousTarget': array([24.,  3.]), 'currentState': array([23.16997659,  3.90248528,  5.37811978]), 'targetState': array([24,  3], dtype=int32), 'currentDistance': 1.2261396907886541}
episode index:1263
target Thresh 31.99991622765765
target distance 9.0
model initialize at round 1263
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 14.]), 'previousTarget': array([11., 14.]), 'currentState': array([ 8.52058309, 22.27874059,  4.83028311]), 'targetState': array([11, 14], dtype=int32), 'currentDistance': 8.642051486265572}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9216837085050275
{'scaleFactor': 20, 'currentTarget': array([11., 14.]), 'previousTarget': array([11., 14.]), 'currentState': array([10.54870525, 14.56026253,  5.12585248]), 'targetState': array([11, 14], dtype=int32), 'currentDistance': 0.719417164582161}
episode index:1264
target Thresh 31.999917061206386
target distance 3.0
model initialize at round 1264
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 27.]), 'previousTarget': array([14., 27.]), 'currentState': array([14.87289873, 25.25017623,  1.29492259]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 1.9554629660224752}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9217377134785413
{'scaleFactor': 20, 'currentTarget': array([14., 27.]), 'previousTarget': array([14., 27.]), 'currentState': array([14.50235192, 27.14007467,  2.24337667]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 0.5215154469610227}
episode index:1265
target Thresh 31.99991788646117
target distance 8.0
model initialize at round 1265
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([9.23764341, 5.14045029, 6.01773602]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 6.763814965265753}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9217760715247668
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.19471573,  4.82451301,  0.05674425]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.8241834992862116}
episode index:1266
target Thresh 31.99991870350453
target distance 10.0
model initialize at round 1266
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 29.]), 'previousTarget': array([22., 29.]), 'currentState': array([13.54561745, 29.66592401,  5.67999763]), 'targetState': array([22, 29], dtype=int32), 'currentDistance': 8.48056832260501}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9218067107816533
{'scaleFactor': 20, 'currentTarget': array([22., 29.]), 'previousTarget': array([22., 29.]), 'currentState': array([21.24266214, 28.78581489,  6.15516413]), 'targetState': array([22, 29], dtype=int32), 'currentDistance': 0.7870424989461511}
episode index:1267
target Thresh 31.999919512418177
target distance 2.0
model initialize at round 1267
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 23.]), 'previousTarget': array([ 5., 23.]), 'currentState': array([ 1.89675806, 23.72907698,  5.00750113]), 'targetState': array([ 5, 23], dtype=int32), 'currentDistance': 3.1877364629191387}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9218526834072198
{'scaleFactor': 20, 'currentTarget': array([ 5., 23.]), 'previousTarget': array([ 5., 23.]), 'currentState': array([ 5.19261519, 22.86426861,  5.1201793 ]), 'targetState': array([ 5, 23], dtype=int32), 'currentDistance': 0.23563450907019487}
episode index:1268
target Thresh 31.999920313283
target distance 9.0
model initialize at round 1268
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([21.32661201, 16.82066587,  4.25835347]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 8.84633244608878}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.921875644294921
{'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([22.00641227,  7.08098387,  4.92132869]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 0.9190384954930136}
episode index:1269
target Thresh 31.99992110617908
target distance 11.0
model initialize at round 1269
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 19.]), 'previousTarget': array([23., 19.]), 'currentState': array([13.98371796, 21.20482954,  6.25353057]), 'targetState': array([23, 19], dtype=int32), 'currentDistance': 9.281951039493773}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9218985690237438
{'scaleFactor': 20, 'currentTarget': array([23., 19.]), 'previousTarget': array([23., 19.]), 'currentState': array([23.62045577, 18.76155877,  6.1683    ]), 'targetState': array([23, 19], dtype=int32), 'currentDistance': 0.6646951083381031}
episode index:1270
target Thresh 31.999921891185714
target distance 20.0
model initialize at round 1270
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([23.93086855, 23.68154956,  2.62188494]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 26.643511705136827}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9218567493886103
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.66022055, 5.62955522, 3.89433315]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.5026722663287522}
episode index:1271
target Thresh 31.9999226683814
target distance 25.0
model initialize at round 1271
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 28.]), 'previousTarget': array([27., 28.]), 'currentState': array([15.52095553,  4.61335194,  0.84942961]), 'targetState': array([27, 28], dtype=int32), 'currentDistance': 26.051943679255853}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.921808165744941
{'scaleFactor': 20, 'currentTarget': array([27., 28.]), 'previousTarget': array([27., 28.]), 'currentState': array([26.51947932, 27.4853524 ,  1.06154907]), 'targetState': array([27, 28], dtype=int32), 'currentDistance': 0.7041038827922104}
episode index:1272
target Thresh 31.999923437843865
target distance 19.0
model initialize at round 1272
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([20.07435184,  1.52342898,  3.16087767]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 17.13807902919398}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9218016528476423
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.29906031, 2.83221082, 3.09393168]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.7207424375413491}
episode index:1273
target Thresh 31.999924199650046
target distance 20.0
model initialize at round 1273
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 14.]), 'previousTarget': array([26., 14.]), 'currentState': array([7.73387045, 9.98962704, 0.41385037]), 'targetState': array([26, 14], dtype=int32), 'currentDistance': 18.701191936159308}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9217951501746722
{'scaleFactor': 20, 'currentTarget': array([26., 14.]), 'previousTarget': array([26., 14.]), 'currentState': array([25.14137838, 13.87612858,  0.26821624]), 'targetState': array([26, 14], dtype=int32), 'currentDistance': 0.8675109313903012}
episode index:1274
target Thresh 31.99992495387613
target distance 16.0
model initialize at round 1274
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 23.]), 'previousTarget': array([10., 23.]), 'currentState': array([8.45469663, 7.84631306, 1.53691452]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 15.23227462898807}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9217958949152628
{'scaleFactor': 20, 'currentTarget': array([10., 23.]), 'previousTarget': array([10., 23.]), 'currentState': array([ 9.97788922, 23.66927374,  1.56701521]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 0.6696388805706275}
episode index:1275
target Thresh 31.99992570059754
target distance 10.0
model initialize at round 1275
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 7.]), 'previousTarget': array([8., 7.]), 'currentState': array([ 7.63929639, 18.64386161,  2.79679769]), 'targetState': array([8, 7], dtype=int32), 'currentDistance': 11.649447208212614}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9218113214469915
{'scaleFactor': 20, 'currentTarget': array([8., 7.]), 'previousTarget': array([8., 7.]), 'currentState': array([7.84116795, 7.77984772, 4.90084748]), 'targetState': array([8, 7], dtype=int32), 'currentDistance': 0.7958580870273596}
episode index:1276
target Thresh 31.999926439888945
target distance 13.0
model initialize at round 1276
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([ 1.49610044, 10.2445702 ,  4.61708975]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 14.862379966936437}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9218120523577048
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.49387482,  6.82099577,  6.00341943]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.5253140488873868}
episode index:1277
target Thresh 31.99992717182428
target distance 9.0
model initialize at round 1277
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 15.]), 'previousTarget': array([11., 15.]), 'currentState': array([20.75921888, 14.50199027,  2.11277813]), 'targetState': array([11, 15], dtype=int32), 'currentDistance': 9.771917251920266}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.921827442105
{'scaleFactor': 20, 'currentTarget': array([11., 15.]), 'previousTarget': array([11., 15.]), 'currentState': array([10.66348937, 14.79512499,  2.11124868]), 'targetState': array([11, 15], dtype=int32), 'currentDistance': 0.39397103298275743}
episode index:1278
target Thresh 31.999927896476734
target distance 4.0
model initialize at round 1278
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 13.]), 'previousTarget': array([ 9., 13.]), 'currentState': array([11.69640918, 10.50658472,  2.13019893]), 'targetState': array([ 9, 13], dtype=int32), 'currentDistance': 3.6725661682164934}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9218730031354105
{'scaleFactor': 20, 'currentTarget': array([ 9., 13.]), 'previousTarget': array([ 9., 13.]), 'currentState': array([ 8.80803159, 13.23919453,  2.30507223]), 'targetState': array([ 9, 13], dtype=int32), 'currentDistance': 0.30670163662214567}
episode index:1279
target Thresh 31.99992861391878
target distance 10.0
model initialize at round 1279
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 13.]), 'previousTarget': array([ 9., 13.]), 'currentState': array([9.23897884, 4.82781865, 1.71702775]), 'targetState': array([ 9, 13], dtype=int32), 'currentDistance': 8.175674829778519}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9219032554845235
{'scaleFactor': 20, 'currentTarget': array([ 9., 13.]), 'previousTarget': array([ 9., 13.]), 'currentState': array([ 8.88637499, 12.80048553,  1.62255373]), 'targetState': array([ 9, 13], dtype=int32), 'currentDistance': 0.22960110646443582}
episode index:1280
target Thresh 31.999929324222155
target distance 8.0
model initialize at round 1280
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  4.]), 'previousTarget': array([25.,  4.]), 'currentState': array([16.77565647,  4.33204974,  5.58868885]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 8.231043890552426}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.921925961803349
{'scaleFactor': 20, 'currentTarget': array([25.,  4.]), 'previousTarget': array([25.,  4.]), 'currentState': array([25.69973994,  3.84710777,  6.05102767]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 0.7162485737036519}
episode index:1281
target Thresh 31.999930027457896
target distance 15.0
model initialize at round 1281
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 25.]), 'previousTarget': array([12., 25.]), 'currentState': array([26.07078106, 26.59681069,  3.11747169]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 14.161097553079749}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9219338708408712
{'scaleFactor': 20, 'currentTarget': array([12., 25.]), 'previousTarget': array([12., 25.]), 'currentState': array([12.27251954, 25.10301453,  3.15731779]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 0.2913398283918317}
episode index:1282
target Thresh 31.999930723696323
target distance 10.0
model initialize at round 1282
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 14.]), 'previousTarget': array([12., 14.]), 'currentState': array([ 8.63826619, 22.44161149,  4.09289587]), 'targetState': array([12, 14], dtype=int32), 'currentDistance': 9.08636664325433}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9219565179017123
{'scaleFactor': 20, 'currentTarget': array([12., 14.]), 'previousTarget': array([12., 14.]), 'currentState': array([11.7611177 , 14.62356311,  6.08736932]), 'targetState': array([12, 14], dtype=int32), 'currentDistance': 0.6677542229578985}
episode index:1283
target Thresh 31.99993141300706
target distance 9.0
model initialize at round 1283
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([12.331541  , 21.1161828 ,  4.31340144]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 7.4884004373494895}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9219866109640942
{'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([ 9.75813448, 13.57507307,  4.47606588]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 0.48893948698937645}
episode index:1284
target Thresh 31.999932095459044
target distance 6.0
model initialize at round 1284
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 19.]), 'previousTarget': array([18., 19.]), 'currentState': array([17.21885111, 23.33132013,  5.85279679]), 'targetState': array([18, 19], dtype=int32), 'currentDistance': 4.4011961591157265}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9220318353913595
{'scaleFactor': 20, 'currentTarget': array([18., 19.]), 'previousTarget': array([18., 19.]), 'currentState': array([17.69450977, 19.99792593,  5.85315776]), 'targetState': array([18, 19], dtype=int32), 'currentDistance': 1.0436380820597801}
episode index:1285
target Thresh 31.999932771120516
target distance 25.0
model initialize at round 1285
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([27.0551955 , 26.31793533,  3.73519135]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 26.24983517458789}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.9219769566477385
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.09001   ,  3.09590493,  3.51959236]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.13152777579996652}
episode index:1286
target Thresh 31.999933440059042
target distance 4.0
model initialize at round 1286
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 23.]), 'previousTarget': array([ 7., 23.]), 'currentState': array([ 8.81290268, 25.80702545,  4.93946004]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 3.341557721297608}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9220221182975848
{'scaleFactor': 20, 'currentTarget': array([ 7., 23.]), 'previousTarget': array([ 7., 23.]), 'currentState': array([ 6.5103446 , 23.33946062,  4.96865368]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 0.5958153458897554}
episode index:1287
target Thresh 31.99993410234152
target distance 12.0
model initialize at round 1287
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 21.]), 'previousTarget': array([ 8., 21.]), 'currentState': array([ 7.98774528, 10.34451793,  1.20613527]), 'targetState': array([ 8, 21], dtype=int32), 'currentDistance': 10.655489119696513}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9220446089277109
{'scaleFactor': 20, 'currentTarget': array([ 8., 21.]), 'previousTarget': array([ 8., 21.]), 'currentState': array([ 7.95603177, 20.1811561 ,  1.52325827]), 'targetState': array([ 8, 21], dtype=int32), 'currentDistance': 0.820023496752141}
episode index:1288
target Thresh 31.99993475803418
target distance 7.0
model initialize at round 1288
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 13.]), 'previousTarget': array([12., 13.]), 'currentState': array([17.40052005, 13.92007513,  3.26985097]), 'targetState': array([12, 13], dtype=int32), 'currentDistance': 5.478335058752581}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9220820444522045
{'scaleFactor': 20, 'currentTarget': array([12., 13.]), 'previousTarget': array([12., 13.]), 'currentState': array([11.51531032, 12.85798129,  3.34085565]), 'targetState': array([12, 13], dtype=int32), 'currentDistance': 0.5050677211800795}
episode index:1289
target Thresh 31.999935407202585
target distance 5.0
model initialize at round 1289
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 21.]), 'previousTarget': array([10., 21.]), 'currentState': array([ 6.93118893, 21.50747783,  6.17986798]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 3.11048792325873}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.922127019611544
{'scaleFactor': 20, 'currentTarget': array([10., 21.]), 'previousTarget': array([10., 21.]), 'currentState': array([10.74962194, 20.44921827,  5.71857738]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 0.9302115685042885}
episode index:1290
target Thresh 31.999936049911657
target distance 2.0
model initialize at round 1290
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 26.]), 'previousTarget': array([14., 26.]), 'currentState': array([14.6496244 , 25.4747308 ,  2.34779144]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 0.8354158237503154}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9221873395034018
{'scaleFactor': 20, 'currentTarget': array([14., 26.]), 'previousTarget': array([14., 26.]), 'currentState': array([14.6496244 , 25.4747308 ,  2.34779144]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 0.8354158237503154}
episode index:1291
target Thresh 31.99993668622567
target distance 4.0
model initialize at round 1291
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([22.24557385, 20.50440329,  3.04562616]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 4.099486003808457}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9222245776307212
{'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([18.20030319, 18.66660069,  2.53495958]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 1.0410914841532382}
episode index:1292
target Thresh 31.99993731620825
target distance 9.0
model initialize at round 1292
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([5.31926944, 7.34759112, 5.91325164]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 10.936618625688562}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.922239469797597
{'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([13.46117926, 13.32295136,  5.92001903]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 0.8652876096671444}
episode index:1293
target Thresh 31.999937939922397
target distance 22.0
model initialize at round 1293
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 5.]), 'previousTarget': array([8., 5.]), 'currentState': array([ 6.3259354 , 27.17290401,  4.04867363]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 22.236010536520762}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9222049114924975
{'scaleFactor': 20, 'currentTarget': array([8., 5.]), 'previousTarget': array([8., 5.]), 'currentState': array([8.06013842, 5.02999699, 4.3460377 ]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 0.06720453187050339}
episode index:1294
target Thresh 31.99993855743049
target distance 5.0
model initialize at round 1294
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 26.]), 'previousTarget': array([14., 26.]), 'currentState': array([12.94118457, 22.94236103,  1.1825186 ]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 3.2357759804260335}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9222496181245496
{'scaleFactor': 20, 'currentTarget': array([14., 26.]), 'previousTarget': array([14., 26.]), 'currentState': array([14.17323663, 26.7452171 ,  1.30146744]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 0.7650878764595572}
episode index:1295
target Thresh 31.99993916879427
target distance 23.0
model initialize at round 1295
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 26.]), 'previousTarget': array([19., 26.]), 'currentState': array([25.87281477,  4.24973664,  3.31468117]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 22.81029458779221}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9222151053196688
{'scaleFactor': 20, 'currentTarget': array([19., 26.]), 'previousTarget': array([19., 26.]), 'currentState': array([19.04751163, 25.32086086,  1.49703625]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 0.680799031195821}
episode index:1296
target Thresh 31.99993977407488
target distance 1.0
model initialize at round 1296
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 22.]), 'previousTarget': array([ 8., 22.]), 'currentState': array([ 8.33779186, 23.54725592,  2.98512977]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 1.5836995402566285}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9222673681528841
{'scaleFactor': 20, 'currentTarget': array([ 8., 22.]), 'previousTarget': array([ 8., 22.]), 'currentState': array([ 7.20621778, 22.30140327,  4.9849121 ]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 0.8490784128741936}
episode index:1297
target Thresh 31.999940373332848
target distance 21.0
model initialize at round 1297
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 22.]), 'previousTarget': array([24., 22.]), 'currentState': array([2.65181169, 8.35344208, 5.51399469]), 'targetState': array([24, 22], dtype=int32), 'currentDistance': 25.33719967268324}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9222328948515329
{'scaleFactor': 20, 'currentTarget': array([24., 22.]), 'previousTarget': array([24., 22.]), 'currentState': array([23.77392076, 21.71641139,  0.37596252]), 'targetState': array([24, 22], dtype=int32), 'currentDistance': 0.3626766099561554}
episode index:1298
target Thresh 31.9999409666281
target distance 8.0
model initialize at round 1298
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 19.]), 'previousTarget': array([20., 19.]), 'currentState': array([15.07873383, 12.68298528,  0.9462046 ]), 'targetState': array([20, 19], dtype=int32), 'currentDistance': 8.007717260287295}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9222624276576519
{'scaleFactor': 20, 'currentTarget': array([20., 19.]), 'previousTarget': array([20., 19.]), 'currentState': array([20.07113281, 18.91290296,  0.88261344]), 'targetState': array([20, 19], dtype=int32), 'currentDistance': 0.11245342098993998}
episode index:1299
target Thresh 31.999941554019962
target distance 7.0
model initialize at round 1299
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 13.]), 'previousTarget': array([ 5., 13.]), 'currentState': array([ 8.40575647, 19.05710451,  4.69877661]), 'targetState': array([ 5, 13], dtype=int32), 'currentDistance': 6.948934603834699}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.922299378867146
{'scaleFactor': 20, 'currentTarget': array([ 5., 13.]), 'previousTarget': array([ 5., 13.]), 'currentState': array([ 5.43409872, 13.99377144,  4.24300747]), 'targetState': array([ 5, 13], dtype=int32), 'currentDistance': 1.0844461104640084}
episode index:1300
target Thresh 31.99994213556718
target distance 5.0
model initialize at round 1300
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 14.]), 'previousTarget': array([12., 14.]), 'currentState': array([15.77045061, 12.85082796,  2.88321352]), 'targetState': array([12, 14], dtype=int32), 'currentDistance': 3.941686716884172}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9223438067081398
{'scaleFactor': 20, 'currentTarget': array([12., 14.]), 'previousTarget': array([12., 14.]), 'currentState': array([12.0151556 , 14.21896672,  2.82854408]), 'targetState': array([12, 14], dtype=int32), 'currentDistance': 0.21949058229432922}
episode index:1301
target Thresh 31.999942711327908
target distance 14.0
model initialize at round 1301
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 22.]), 'previousTarget': array([ 7., 22.]), 'currentState': array([22.27759725, 14.09550598,  1.71881693]), 'targetState': array([ 7, 22], dtype=int32), 'currentDistance': 17.201337261358816}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.922330011215283
{'scaleFactor': 20, 'currentTarget': array([ 7., 22.]), 'previousTarget': array([ 7., 22.]), 'currentState': array([ 6.99128976, 21.17656401,  1.64519876]), 'targetState': array([ 7, 22], dtype=int32), 'currentDistance': 0.8234820597026205}
episode index:1302
target Thresh 31.99994328135972
target distance 3.0
model initialize at round 1302
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([10.44549549, 15.41100257,  5.3866365 ]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 3.8243209277324586}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.922374347354028
{'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([14.09709518, 14.0335746 ,  5.86050174]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 0.10273620784131238}
episode index:1303
target Thresh 31.99994384571962
target distance 21.0
model initialize at round 1303
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 8.]), 'previousTarget': array([8., 8.]), 'currentState': array([22.33883286, 29.27002188,  3.99045253]), 'targetState': array([8, 8], dtype=int32), 'currentDistance': 25.65182173880533}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9223399506328968
{'scaleFactor': 20, 'currentTarget': array([8., 8.]), 'previousTarget': array([8., 8.]), 'currentState': array([8.09594744, 8.26641234, 3.99713076]), 'targetState': array([8, 8], dtype=int32), 'currentDistance': 0.2831632806338488}
episode index:1304
target Thresh 31.999944404464046
target distance 7.0
model initialize at round 1304
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 14.]), 'previousTarget': array([23., 14.]), 'currentState': array([14.46029341, 19.67947989,  3.73599255]), 'targetState': array([23, 14], dtype=int32), 'currentDistance': 10.255880283552044}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9223546174518762
{'scaleFactor': 20, 'currentTarget': array([23., 14.]), 'previousTarget': array([23., 14.]), 'currentState': array([23.44254559, 13.5895755 ,  5.82603243]), 'targetState': array([23, 14], dtype=int32), 'currentDistance': 0.6035684508423304}
episode index:1305
target Thresh 31.99994495764887
target distance 19.0
model initialize at round 1305
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  9.]), 'previousTarget': array([21.,  9.]), 'currentState': array([ 6.24621562, 26.8689232 ,  4.53618208]), 'targetState': array([21,  9], dtype=int32), 'currentDistance': 23.172668599775946}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9223202885127851
{'scaleFactor': 20, 'currentTarget': array([21.,  9.]), 'previousTarget': array([21.,  9.]), 'currentState': array([20.45718663,  9.83538997,  5.083177  ]), 'targetState': array([21,  9], dtype=int32), 'currentDistance': 0.9962543599190352}
episode index:1306
target Thresh 31.999945505329414
target distance 10.0
model initialize at round 1306
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([ 3.20303417, 19.32932193,  5.84332395]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 11.409219078068922}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9223349479319805
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.98356143, 10.38201268,  5.11277314]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.6182059194388726}
episode index:1307
target Thresh 31.999946047560446
target distance 11.0
model initialize at round 1307
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([ 6.68412124, 14.982563  ,  6.05541235]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 11.642241326516551}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9223495849361617
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.11204671,  7.85874776,  5.56130271]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.1802960310557273}
episode index:1308
target Thresh 31.99994658439619
target distance 5.0
model initialize at round 1308
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 15.]), 'previousTarget': array([ 9., 15.]), 'currentState': array([ 2.93804283, 13.69438327,  5.03954506]), 'targetState': array([ 9, 15], dtype=int32), 'currentDistance': 6.200964422260419}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9223862155053473
{'scaleFactor': 20, 'currentTarget': array([ 9., 15.]), 'previousTarget': array([ 9., 15.]), 'currentState': array([ 8.23257999, 14.74949411,  0.32620295]), 'targetState': array([ 9, 15], dtype=int32), 'currentDistance': 0.8072711253901835}
episode index:1309
target Thresh 31.99994711589033
target distance 16.0
model initialize at round 1309
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'previousTarget': array([25.,  3.]), 'currentState': array([11.59969164, 17.80393895,  5.73052663]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 19.968096369565774}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9223724718866476
{'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'previousTarget': array([25.,  3.]), 'currentState': array([24.7686775 ,  3.10320842,  5.35377606]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 0.25330234009763153}
episode index:1310
target Thresh 31.99994764209601
target distance 13.0
model initialize at round 1310
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([ 4.23126073, 10.56501193,  0.74088225]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 13.92057008271611}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9223798653847561
{'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([16.09229201, 17.87566124,  0.59011745]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 0.15484812909806464}
episode index:1311
target Thresh 31.999948163065863
target distance 9.0
model initialize at round 1311
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([24.55882649, 13.86914153,  3.60891223]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 9.732588734563972}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9223944235280612
{'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([17.41316471, 20.86046831,  2.98868072]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 0.9545212332310822}
episode index:1312
target Thresh 31.999948678851975
target distance 13.0
model initialize at round 1312
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 27.]), 'previousTarget': array([14., 27.]), 'currentState': array([10.61828288, 13.5378868 ,  0.73184508]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 13.880363921278029}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9224017890454861
{'scaleFactor': 20, 'currentTarget': array([14., 27.]), 'previousTarget': array([14., 27.]), 'currentState': array([13.82338826, 26.0357327 ,  1.29570517]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 0.980307674536246}
episode index:1313
target Thresh 31.99994918950593
target distance 8.0
model initialize at round 1313
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([ 7.49875569, 17.76069285,  3.68259287]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 10.909441635859926}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9224163083456045
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.10314933,  8.69284382,  5.41682596]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.3240134340086304}
episode index:1314
target Thresh 31.999949695078794
target distance 2.0
model initialize at round 1314
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([23.71820074, 10.13360959,  5.10407494]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 0.31186911466565326}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9224753073506649
{'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([23.71820074, 10.13360959,  5.10407494]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 0.31186911466565326}
episode index:1315
target Thresh 31.99995019562112
target distance 22.0
model initialize at round 1315
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 23.]), 'previousTarget': array([26., 23.]), 'currentState': array([ 5.67230176, 22.8108046 ,  0.89734429]), 'targetState': array([26, 23], dtype=int32), 'currentDistance': 20.3285786652276}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.92246155869387
{'scaleFactor': 20, 'currentTarget': array([26., 23.]), 'previousTarget': array([26., 23.]), 'currentState': array([25.22092487, 22.98025127,  6.24524858]), 'targetState': array([26, 23], dtype=int32), 'currentDistance': 0.7793253917532741}
episode index:1316
target Thresh 31.99995069118297
target distance 8.0
model initialize at round 1316
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([ 9.62209635, 15.55145624,  0.74022024]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 6.401699689725526}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9224905142377624
{'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([16.34407709, 14.32347984,  0.61072057]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 0.7589918096372392}
episode index:1317
target Thresh 31.9999511818139
target distance 20.0
model initialize at round 1317
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 16.]), 'previousTarget': array([25., 16.]), 'currentState': array([ 6.66046572, 12.27917992,  5.44110626]), 'targetState': array([25, 16], dtype=int32), 'currentDistance': 18.713177698889634}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9224699131300391
{'scaleFactor': 20, 'currentTarget': array([25., 16.]), 'previousTarget': array([25., 16.]), 'currentState': array([24.95213391, 15.47110884,  1.02777171]), 'targetState': array([25, 16], dtype=int32), 'currentDistance': 0.5310527465685632}
episode index:1318
target Thresh 31.999951667562968
target distance 22.0
model initialize at round 1318
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 24.]), 'previousTarget': array([ 6., 24.]), 'currentState': array([10.45501723,  1.15423824,  0.48346442]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 23.27608233335501}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9224425552517874
{'scaleFactor': 20, 'currentTarget': array([ 6., 24.]), 'previousTarget': array([ 6., 24.]), 'currentState': array([ 5.92959815, 23.87949489,  1.60366415]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 0.1395632551765535}
episode index:1319
target Thresh 31.99995214847875
target distance 11.0
model initialize at round 1319
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 25.]), 'previousTarget': array([14., 25.]), 'currentState': array([24.21700457, 20.48973362,  3.06470907]), 'targetState': array([14, 25], dtype=int32), 'currentDistance': 11.16824450349336}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9224569776715975
{'scaleFactor': 20, 'currentTarget': array([14., 25.]), 'previousTarget': array([14., 25.]), 'currentState': array([13.62517933, 25.74012906,  2.120687  ]), 'targetState': array([14, 25], dtype=int32), 'currentDistance': 0.8296273610332664}
episode index:1320
target Thresh 31.999952624609342
target distance 2.0
model initialize at round 1320
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 12.]), 'previousTarget': array([14., 12.]), 'currentState': array([14.33224681, 12.24002751,  1.99195492]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 0.409879425962999}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9225156779156009
{'scaleFactor': 20, 'currentTarget': array([14., 12.]), 'previousTarget': array([14., 12.]), 'currentState': array([14.33224681, 12.24002751,  1.99195492]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 0.409879425962999}
episode index:1321
target Thresh 31.999953096002354
target distance 8.0
model initialize at round 1321
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 26.]), 'previousTarget': array([10., 26.]), 'currentState': array([ 2.85982988, 19.55325169,  6.25862074]), 'targetState': array([10, 26], dtype=int32), 'currentDistance': 9.61990608388717}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9225372167748931
{'scaleFactor': 20, 'currentTarget': array([10., 26.]), 'previousTarget': array([10., 26.]), 'currentState': array([ 9.29407367, 25.90688429,  0.54556854]), 'targetState': array([10, 26], dtype=int32), 'currentDistance': 0.712041093655872}
episode index:1322
target Thresh 31.99995356270493
target distance 13.0
model initialize at round 1322
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 25.]), 'previousTarget': array([18., 25.]), 'currentState': array([20.58524088, 13.63141088,  0.81032264]), 'targetState': array([18, 25], dtype=int32), 'currentDistance': 11.658828803723296}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9225444186880692
{'scaleFactor': 20, 'currentTarget': array([18., 25.]), 'previousTarget': array([18., 25.]), 'currentState': array([18.32739607, 25.28020512,  2.17472792]), 'targetState': array([18, 25], dtype=int32), 'currentDistance': 0.430932817126845}
episode index:1323
target Thresh 31.999954024763735
target distance 5.0
model initialize at round 1323
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 15.]), 'previousTarget': array([27., 15.]), 'currentState': array([23.23556532, 12.14286991,  6.01982183]), 'targetState': array([27, 15], dtype=int32), 'currentDistance': 4.725903173611394}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9225804871029575
{'scaleFactor': 20, 'currentTarget': array([27., 15.]), 'previousTarget': array([27., 15.]), 'currentState': array([27.66189744, 14.67049965,  0.55948329]), 'targetState': array([27, 15], dtype=int32), 'currentDistance': 0.7393772407799687}
episode index:1324
target Thresh 31.99995448222498
target distance 15.0
model initialize at round 1324
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 20.]), 'previousTarget': array([24., 20.]), 'currentState': array([10.93833244, 24.51018038,  6.08313502]), 'targetState': array([24, 20], dtype=int32), 'currentDistance': 13.818425617635146}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9225876454884698
{'scaleFactor': 20, 'currentTarget': array([24., 20.]), 'previousTarget': array([24., 20.]), 'currentState': array([24.06524732, 19.7877463 ,  5.92035905]), 'targetState': array([24, 20], dtype=int32), 'currentDistance': 0.22205594742374557}
episode index:1325
target Thresh 31.999954935134408
target distance 3.0
model initialize at round 1325
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 6.]), 'previousTarget': array([7., 6.]), 'currentState': array([8.21717112, 6.51684785, 2.71416199]), 'targetState': array([7, 6], dtype=int32), 'currentDistance': 1.3223604780063336}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.922638484368192
{'scaleFactor': 20, 'currentTarget': array([7., 6.]), 'previousTarget': array([7., 6.]), 'currentState': array([6.29981971, 6.40444412, 3.69595617]), 'targetState': array([7, 6], dtype=int32), 'currentDistance': 0.8085959945977944}
episode index:1326
target Thresh 31.999955383537312
target distance 6.0
model initialize at round 1326
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 21.]), 'previousTarget': array([20., 21.]), 'currentState': array([27.46273516, 17.83234243,  1.52733725]), 'targetState': array([20, 21], dtype=int32), 'currentDistance': 8.107186356180737}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9226670883814789
{'scaleFactor': 20, 'currentTarget': array([20., 21.]), 'previousTarget': array([20., 21.]), 'currentState': array([20.73183247, 20.96879062,  2.74943476]), 'targetState': array([20, 21], dtype=int32), 'currentDistance': 0.7324976353685692}
episode index:1327
target Thresh 31.999955827478534
target distance 12.0
model initialize at round 1327
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  3.]), 'previousTarget': array([13.,  3.]), 'currentState': array([25.18253592,  5.67304176,  2.47212195]), 'targetState': array([13,  3], dtype=int32), 'currentDistance': 12.472342750706147}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9226741653841337
{'scaleFactor': 20, 'currentTarget': array([13.,  3.]), 'previousTarget': array([13.,  3.]), 'currentState': array([13.71955523,  3.13505541,  3.25823683]), 'targetState': array([13,  3], dtype=int32), 'currentDistance': 0.7321199981826123}
episode index:1328
target Thresh 31.999956267002467
target distance 14.0
model initialize at round 1328
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([21.69066488, 22.12548695,  4.9815437 ]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 12.145141076127015}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9226883158611968
{'scaleFactor': 20, 'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([20.99624014, 10.31566496,  4.54168444]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 0.3156873504098964}
episode index:1329
target Thresh 31.999956702153064
target distance 16.0
model initialize at round 1329
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 19.]), 'previousTarget': array([ 8., 19.]), 'currentState': array([7.60879049, 3.49414698, 1.30800742]), 'targetState': array([ 8, 19], dtype=int32), 'currentDistance': 15.510787303597958}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9226883582510965
{'scaleFactor': 20, 'currentTarget': array([ 8., 19.]), 'previousTarget': array([ 8., 19.]), 'currentState': array([ 8.17595906, 19.18104639,  1.44077232]), 'targetState': array([ 8, 19], dtype=int32), 'currentDistance': 0.25246660962620404}
episode index:1330
target Thresh 31.99995713297384
target distance 2.0
model initialize at round 1330
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 20.]), 'previousTarget': array([16., 20.]), 'currentState': array([13.43557073, 21.3795573 ,  4.52915907]), 'targetState': array([16, 20], dtype=int32), 'currentDistance': 2.911953953414141}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9227314924672866
{'scaleFactor': 20, 'currentTarget': array([16., 20.]), 'previousTarget': array([16., 20.]), 'currentState': array([16.2772072 , 19.37950898,  5.0748657 ]), 'targetState': array([16, 20], dtype=int32), 'currentDistance': 0.6795976279776368}
episode index:1331
target Thresh 31.999957559507877
target distance 20.0
model initialize at round 1331
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 24.]), 'previousTarget': array([15., 24.]), 'currentState': array([12.05786071,  5.68197556,  0.52641022]), 'targetState': array([15, 24], dtype=int32), 'currentDistance': 18.55279501515821}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9227109269731358
{'scaleFactor': 20, 'currentTarget': array([15., 24.]), 'previousTarget': array([15., 24.]), 'currentState': array([15.14945499, 24.44379398,  1.08362659]), 'targetState': array([15, 24], dtype=int32), 'currentDistance': 0.4682839836835349}
episode index:1332
target Thresh 31.999957981797827
target distance 11.0
model initialize at round 1332
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 11.]), 'previousTarget': array([18., 11.]), 'currentState': array([ 8.01599957, 12.34169034,  6.19585449]), 'targetState': array([18, 11], dtype=int32), 'currentDistance': 10.073747940870938}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9227321416189924
{'scaleFactor': 20, 'currentTarget': array([18., 11.]), 'previousTarget': array([18., 11.]), 'currentState': array([17.9419634 , 11.13720734,  6.14342222]), 'targetState': array([18, 11], dtype=int32), 'currentDistance': 0.14897684676578526}
episode index:1333
target Thresh 31.999958399885926
target distance 14.0
model initialize at round 1333
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([16.99135219, 20.00148781,  4.64099972]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 12.368665959364842}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9227461955978395
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.11658084,  8.38194393,  4.56752574]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.39933978235942397}
episode index:1334
target Thresh 31.999958813813976
target distance 14.0
model initialize at round 1334
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 22.]), 'previousTarget': array([ 8., 22.]), 'currentState': array([4.57059714, 9.88766002, 1.57053322]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 12.58847027461889}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9227602285220368
{'scaleFactor': 20, 'currentTarget': array([ 8., 22.]), 'previousTarget': array([ 8., 22.]), 'currentState': array([ 7.77083822, 21.37168041,  1.37948976]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 0.6688053793881378}
episode index:1335
target Thresh 31.999959223623375
target distance 11.0
model initialize at round 1335
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([ 5.25151539, 10.1252099 ,  1.74230545]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 12.53177047970808}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.922767193431756
{'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([14.26545351, 17.75381314,  5.83002317]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 0.7747041512548463}
episode index:1336
target Thresh 31.9999596293551
target distance 16.0
model initialize at round 1336
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  2.]), 'previousTarget': array([23.,  2.]), 'currentState': array([21.06352959, 16.35651026,  4.92529595]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 14.48652147622526}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.922774147922762
{'scaleFactor': 20, 'currentTarget': array([23.,  2.]), 'previousTarget': array([23.,  2.]), 'currentState': array([22.89053854,  2.54868335,  4.85567046]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 0.5594955169592484}
episode index:1337
target Thresh 31.999960031049728
target distance 13.0
model initialize at round 1337
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 16.]), 'previousTarget': array([ 5., 16.]), 'currentState': array([13.58916609,  3.55402096,  1.34544819]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 15.122042464003368}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9227672294620453
{'scaleFactor': 20, 'currentTarget': array([ 5., 16.]), 'previousTarget': array([ 5., 16.]), 'currentState': array([ 5.04760707, 16.55312984,  1.76998588]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 0.5551747931371115}
episode index:1338
target Thresh 31.99996042874743
target distance 21.0
model initialize at round 1338
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 28.]), 'previousTarget': array([20., 28.]), 'currentState': array([26.1197046 ,  8.43438771,  3.13123441]), 'targetState': array([20, 28], dtype=int32), 'currentDistance': 20.50034069884089}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9227400581717197
{'scaleFactor': 20, 'currentTarget': array([20., 28.]), 'previousTarget': array([20., 28.]), 'currentState': array([20.15329486, 27.34891022,  1.82470918]), 'targetState': array([20, 28], dtype=int32), 'currentDistance': 0.6688925268649257}
episode index:1339
target Thresh 31.99996082248797
target distance 24.0
model initialize at round 1339
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  4.]), 'previousTarget': array([10.,  4.]), 'currentState': array([ 1.30390951, 26.46773171,  5.29597521]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 24.09192723423335}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9227129274355587
{'scaleFactor': 20, 'currentTarget': array([10.,  4.]), 'previousTarget': array([10.,  4.]), 'currentState': array([9.98580152, 4.28482055, 5.10132309]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 0.2851742322875092}
episode index:1340
target Thresh 31.99996121231073
target distance 8.0
model initialize at round 1340
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 12.]), 'previousTarget': array([25., 12.]), 'currentState': array([24.78895191, 18.17789176,  4.91068977]), 'targetState': array([25, 12], dtype=int32), 'currentDistance': 6.181495607868062}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9227484129482839
{'scaleFactor': 20, 'currentTarget': array([25., 12.]), 'previousTarget': array([25., 12.]), 'currentState': array([25.00720791, 12.20497879,  4.7870545 ]), 'targetState': array([25, 12], dtype=int32), 'currentDistance': 0.20510547959535158}
episode index:1341
target Thresh 31.999961598254686
target distance 7.0
model initialize at round 1341
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([2.99278352, 0.31704545, 5.71810102]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 9.68312781381747}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.922769457387145
{'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([9.85175457, 6.71675478, 0.80778196]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 0.31969448183809235}
episode index:1342
target Thresh 31.999961980358435
target distance 3.0
model initialize at round 1342
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 28.]), 'previousTarget': array([10., 28.]), 'currentState': array([11.69213413, 26.51260422,  2.31778506]), 'targetState': array([10, 28], dtype=int32), 'currentDistance': 2.252923463606089}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9228195173593065
{'scaleFactor': 20, 'currentTarget': array([10., 28.]), 'previousTarget': array([10., 28.]), 'currentState': array([10.2761789 , 27.92430682,  2.39841813]), 'targetState': array([10, 28], dtype=int32), 'currentDistance': 0.286363827955225}
episode index:1343
target Thresh 31.99996235866019
target distance 17.0
model initialize at round 1343
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  8.]), 'previousTarget': array([12.,  8.]), 'currentState': array([ 9.32637611, 23.96408761,  4.61013166]), 'targetState': array([12,  8], dtype=int32), 'currentDistance': 16.186425107333147}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9228194616874825
{'scaleFactor': 20, 'currentTarget': array([12.,  8.]), 'previousTarget': array([12.,  8.]), 'currentState': array([11.78271329,  8.42808344,  4.88136574]), 'targetState': array([12,  8], dtype=int32), 'currentDistance': 0.4800718156510764}
episode index:1344
target Thresh 31.99996273319778
target distance 19.0
model initialize at round 1344
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 27.]), 'previousTarget': array([20., 27.]), 'currentState': array([20.77501281,  9.154034  ,  1.37601018]), 'targetState': array([20, 27], dtype=int32), 'currentDistance': 17.862786659397777}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.922812545543093
{'scaleFactor': 20, 'currentTarget': array([20., 27.]), 'previousTarget': array([20., 27.]), 'currentState': array([20.01378645, 26.86102636,  1.60043603]), 'targetState': array([20, 27], dtype=int32), 'currentDistance': 0.13965579029696465}
episode index:1345
target Thresh 31.999963104008657
target distance 17.0
model initialize at round 1345
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 27.]), 'previousTarget': array([ 8., 27.]), 'currentState': array([23.4450694 , 16.64387802,  1.7390008 ]), 'targetState': array([ 8, 27], dtype=int32), 'currentDistance': 18.595683132114498}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9227988527715222
{'scaleFactor': 20, 'currentTarget': array([ 8., 27.]), 'previousTarget': array([ 8., 27.]), 'currentState': array([ 8.71975369, 26.27518979,  2.41776911]), 'targetState': array([ 8, 27], dtype=int32), 'currentDistance': 1.0214671814271947}
episode index:1346
target Thresh 31.999963471129906
target distance 17.0
model initialize at round 1346
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 24.]), 'previousTarget': array([ 6., 24.]), 'currentState': array([18.68076936,  6.91396257,  0.95885533]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 21.27756063732216}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9227784662841332
{'scaleFactor': 20, 'currentTarget': array([ 6., 24.]), 'previousTarget': array([ 6., 24.]), 'currentState': array([ 6.15795963, 24.19999422,  2.01500814]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 0.25485080430996715}
episode index:1347
target Thresh 31.999963834598237
target distance 21.0
model initialize at round 1347
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 27.]), 'previousTarget': array([ 9., 27.]), 'currentState': array([12.06684086,  7.68164217,  2.54106987]), 'targetState': array([ 9, 27], dtype=int32), 'currentDistance': 19.56027766023528}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9227648191095965
{'scaleFactor': 20, 'currentTarget': array([ 9., 27.]), 'previousTarget': array([ 9., 27.]), 'currentState': array([ 9.06009401, 27.04027579,  1.76298704]), 'targetState': array([ 9, 27], dtype=int32), 'currentDistance': 0.07234244760315116}
episode index:1348
target Thresh 31.999964194449994
target distance 3.0
model initialize at round 1348
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 13.]), 'previousTarget': array([ 3., 13.]), 'currentState': array([3.62084207, 9.54694475, 0.7374379 ]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 3.508423500609877}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9228073210969133
{'scaleFactor': 20, 'currentTarget': array([ 3., 13.]), 'previousTarget': array([ 3., 13.]), 'currentState': array([ 3.10164767, 12.87829656,  0.72279518]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 0.15856852343498837}
episode index:1349
target Thresh 31.99996455072117
target distance 14.0
model initialize at round 1349
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 23.]), 'previousTarget': array([ 3., 23.]), 'currentState': array([15.35309527, 10.11826227,  2.71410826]), 'targetState': array([ 3, 23], dtype=int32), 'currentDistance': 17.847636531558944}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9228004395609036
{'scaleFactor': 20, 'currentTarget': array([ 3., 23.]), 'previousTarget': array([ 3., 23.]), 'currentState': array([ 3.13155433, 22.85442698,  2.29330551]), 'targetState': array([ 3, 23], dtype=int32), 'currentDistance': 0.1962091878218503}
episode index:1350
target Thresh 31.99996490344739
target distance 12.0
model initialize at round 1350
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 16.]), 'previousTarget': array([18., 16.]), 'currentState': array([ 7.51781132, 28.79718555,  3.65813053]), 'targetState': array([18, 16], dtype=int32), 'currentDistance': 16.542195669528585}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9227935682122158
{'scaleFactor': 20, 'currentTarget': array([18., 16.]), 'previousTarget': array([18., 16.]), 'currentState': array([17.46704896, 15.29974578,  4.44539372]), 'targetState': array([18, 16], dtype=int32), 'currentDistance': 0.8799959048809367}
episode index:1351
target Thresh 31.999965252663923
target distance 9.0
model initialize at round 1351
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 20.]), 'previousTarget': array([23., 20.]), 'currentState': array([19.96168486, 12.68691811,  1.47663224]), 'targetState': array([23, 20], dtype=int32), 'currentDistance': 7.91912404472441}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9228215285981535
{'scaleFactor': 20, 'currentTarget': array([23., 20.]), 'previousTarget': array([23., 20.]), 'currentState': array([23.1376595 , 19.97082909,  1.10445748]), 'targetState': array([23, 20], dtype=int32), 'currentDistance': 0.14071631444373195}
episode index:1352
target Thresh 31.999965598405694
target distance 9.0
model initialize at round 1352
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'previousTarget': array([13., 22.]), 'currentState': array([ 8.43315984, 14.62627204,  0.30048835]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 8.673401468575474}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9228423479043633
{'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'previousTarget': array([13., 22.]), 'currentState': array([13.69474506, 21.50171252,  6.13439989]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 0.8549626381830775}
episode index:1353
target Thresh 31.999965940707277
target distance 24.0
model initialize at round 1353
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([ 7.15411555, 24.5450541 ,  5.19576907]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 22.924575027492054}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.9227959786331202
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.4583068 , 2.62745548, 1.61889391]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.7770106231681239}
episode index:1354
target Thresh 31.9999662796029
target distance 5.0
model initialize at round 1354
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  2.]), 'previousTarget': array([27.,  2.]), 'currentState': array([26.15180254,  5.32119176,  3.79708123]), 'targetState': array([27,  2], dtype=int32), 'currentDistance': 3.427791365384451}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9228382694237969
{'scaleFactor': 20, 'currentTarget': array([27.,  2.]), 'previousTarget': array([27.,  2.]), 'currentState': array([26.44721212,  1.96397018,  3.81085074]), 'targetState': array([27,  2], dtype=int32), 'currentDistance': 0.5539608163108746}
episode index:1355
target Thresh 31.999966615126457
target distance 5.0
model initialize at round 1355
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'previousTarget': array([13., 22.]), 'currentState': array([16.24793718, 22.11074258,  3.28223759]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 3.24982458447879}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.922859030323853
{'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'previousTarget': array([13., 22.]), 'currentState': array([13.35697095, 21.68254838,  0.32420803]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 0.4777068053159872}
episode index:1356
target Thresh 31.9999669473115
target distance 12.0
model initialize at round 1356
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 18.]), 'previousTarget': array([27., 18.]), 'currentState': array([16.41817291,  7.34173125,  1.14119375]), 'targetState': array([27, 18], dtype=int32), 'currentDistance': 15.01911306633766}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9228658146404213
{'scaleFactor': 20, 'currentTarget': array([27., 18.]), 'previousTarget': array([27., 18.]), 'currentState': array([26.05799175, 17.26405355,  0.6832145 ]), 'targetState': array([27, 18], dtype=int32), 'currentDistance': 1.1954065107693803}
episode index:1357
target Thresh 31.999967276191246
target distance 9.0
model initialize at round 1357
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 13.]), 'previousTarget': array([11., 13.]), 'currentState': array([18.40920138, 17.54931579,  3.81910372]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 8.694397007291206}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9228935982894342
{'scaleFactor': 20, 'currentTarget': array([11., 13.]), 'previousTarget': array([11., 13.]), 'currentState': array([11.49969411, 13.55739141,  3.74921376]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 0.7485849260272132}
episode index:1358
target Thresh 31.999967601798584
target distance 5.0
model initialize at round 1358
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([18.64482587, 13.35628071,  1.2233116 ]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 5.344083900221729}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9229284808513993
{'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([16.50018126, 18.00357063,  2.11142373]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 0.5001940072273303}
episode index:1359
target Thresh 31.99996792416607
target distance 7.0
model initialize at round 1359
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([3.25694817, 3.52497674, 1.15167201]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 6.123741751337964}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9229633121154792
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.14361736, 8.75533494, 1.24747443]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.28370219730603136}
episode index:1360
target Thresh 31.999968243325952
target distance 23.0
model initialize at round 1360
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  5.]), 'previousTarget': array([18.,  5.]), 'currentState': array([13.64210466, 28.99423757,  3.51958764]), 'targetState': array([18,  5], dtype=int32), 'currentDistance': 24.386772810458538}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9229299232182592
{'scaleFactor': 20, 'currentTarget': array([18.,  5.]), 'previousTarget': array([18.,  5.]), 'currentState': array([18.00130668,  4.32960467,  4.98224616]), 'targetState': array([18,  5], dtype=int32), 'currentDistance': 0.6703966048830117}
episode index:1361
target Thresh 31.99996855931014
target distance 12.0
model initialize at round 1361
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 15.]), 'previousTarget': array([ 4., 15.]), 'currentState': array([16.48518843,  3.79158286,  1.49968594]), 'targetState': array([ 4, 15], dtype=int32), 'currentDistance': 16.778216378221373}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.922916305121189
{'scaleFactor': 20, 'currentTarget': array([ 4., 15.]), 'previousTarget': array([ 4., 15.]), 'currentState': array([ 3.98066334, 15.39965313,  1.93876263]), 'targetState': array([ 4, 15], dtype=int32), 'currentDistance': 0.40012064386984547}
episode index:1362
target Thresh 31.99996887215023
target distance 12.0
model initialize at round 1362
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 28.]), 'previousTarget': array([22., 28.]), 'currentState': array([11.0144617 , 22.34285351,  6.19700021]), 'targetState': array([22, 28], dtype=int32), 'currentDistance': 12.35659168917105}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9229299249629204
{'scaleFactor': 20, 'currentTarget': array([22., 28.]), 'previousTarget': array([22., 28.]), 'currentState': array([21.64335594, 27.48074466,  0.53632151]), 'targetState': array([22, 28], dtype=int32), 'currentDistance': 0.6299373723096278}
episode index:1363
target Thresh 31.99996918187751
target distance 8.0
model initialize at round 1363
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 25.]), 'previousTarget': array([23., 25.]), 'currentState': array([16.91048489, 24.58191214,  0.15402335]), 'targetState': array([23, 25], dtype=int32), 'currentDistance': 6.103850562859374}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.922964653023798
{'scaleFactor': 20, 'currentTarget': array([23., 25.]), 'previousTarget': array([23., 25.]), 'currentState': array([22.88760354, 24.6852025 ,  0.15784457]), 'targetState': array([23, 25], dtype=int32), 'currentDistance': 0.33426102433378874}
episode index:1364
target Thresh 31.999969488522954
target distance 4.0
model initialize at round 1364
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([16.99115728, 12.63984871,  0.06893199]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 5.718035426225406}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9229993302010701
{'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([15.79888342, 17.55539571,  2.22391132]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 0.9142689425368117}
episode index:1365
target Thresh 31.999969792117223
target distance 21.0
model initialize at round 1365
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 29.]), 'previousTarget': array([13., 29.]), 'currentState': array([14.20761271,  9.81532549,  1.73914891]), 'targetState': array([13, 29], dtype=int32), 'currentDistance': 19.222644573761663}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9229857011709146
{'scaleFactor': 20, 'currentTarget': array([13., 29.]), 'previousTarget': array([13., 29.]), 'currentState': array([13.18383096, 29.43511582,  1.6200639 ]), 'targetState': array([13, 29], dtype=int32), 'currentDistance': 0.4723553702999751}
episode index:1366
target Thresh 31.999970092690678
target distance 18.0
model initialize at round 1366
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([10.99206653, 10.15368943,  0.15551452]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 16.049452738902964}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9229855248675181
{'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([2.68342172e+01, 8.91653845e+00, 2.55421978e-02]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 0.18560648255367831}
episode index:1367
target Thresh 31.99997039027338
target distance 12.0
model initialize at round 1367
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 16.]), 'previousTarget': array([17., 16.]), 'currentState': array([16.28023155,  5.09242631,  1.7164089 ]), 'targetState': array([17, 16], dtype=int32), 'currentDistance': 10.931295912773255}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9230059960115475
{'scaleFactor': 20, 'currentTarget': array([17., 16.]), 'previousTarget': array([17., 16.]), 'currentState': array([17.03775471, 15.01026151,  1.56983923]), 'targetState': array([17, 16], dtype=int32), 'currentDistance': 0.9904583298744895}
episode index:1368
target Thresh 31.99997068489508
target distance 21.0
model initialize at round 1368
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 22.]), 'previousTarget': array([24., 22.]), 'currentState': array([ 4.01234361, 16.655549  ,  0.08460873]), 'targetState': array([24, 22], dtype=int32), 'currentDistance': 20.68984206159642}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9229923919786748
{'scaleFactor': 20, 'currentTarget': array([24., 22.]), 'previousTarget': array([24., 22.]), 'currentState': array([23.10373883, 21.58458803,  0.20666525]), 'targetState': array([24, 22], dtype=int32), 'currentDistance': 0.9878518035687158}
episode index:1369
target Thresh 31.99997097658525
target distance 4.0
model initialize at round 1369
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  4.]), 'previousTarget': array([26.,  4.]), 'currentState': array([25.86517615,  6.18021399,  5.38072062]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 2.1843787446862377}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9230413026414641
{'scaleFactor': 20, 'currentTarget': array([26.,  4.]), 'previousTarget': array([26.,  4.]), 'currentState': array([25.56700887,  4.43570817,  3.68857157]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 0.6142661680551927}
episode index:1370
target Thresh 31.99997126537305
target distance 10.0
model initialize at round 1370
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([22.61500674,  2.47343564,  1.29515856]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 11.714895660652857}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9230547518367664
{'scaleFactor': 20, 'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([11.55503503,  4.09515119,  3.1998873 ]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 0.5631319904397584}
episode index:1371
target Thresh 31.999971551287366
target distance 14.0
model initialize at round 1371
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 14.]), 'previousTarget': array([11., 14.]), 'currentState': array([25.2300362 , 11.66717469,  2.44368243]), 'targetState': array([11, 14], dtype=int32), 'currentDistance': 14.41998627728658}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9230613193266135
{'scaleFactor': 20, 'currentTarget': array([11., 14.]), 'previousTarget': array([11., 14.]), 'currentState': array([11.75172539, 14.09255841,  3.01853968]), 'targetState': array([11, 14], dtype=int32), 'currentDistance': 0.7574022207641896}
episode index:1372
target Thresh 31.999971834356785
target distance 17.0
model initialize at round 1372
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 4.]), 'previousTarget': array([8., 4.]), 'currentState': array([23.01158944,  2.80943989,  3.32425907]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 15.058726730938872}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.92306108871853
{'scaleFactor': 20, 'currentTarget': array([8., 4.]), 'previousTarget': array([8., 4.]), 'currentState': array([7.26180448, 4.1852175 , 3.16919328]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 0.7610769669855731}
episode index:1373
target Thresh 31.99997211460962
target distance 9.0
model initialize at round 1373
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'previousTarget': array([11., 19.]), 'currentState': array([3.73737971, 8.33764665, 5.56570292]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 12.900830688386558}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9230676420367165
{'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'previousTarget': array([11., 19.]), 'currentState': array([11.15519989, 19.18119382,  1.01090045]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 0.23857537251580538}
episode index:1374
target Thresh 31.99997239207389
target distance 20.0
model initialize at round 1374
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  7.]), 'previousTarget': array([27.,  7.]), 'currentState': array([7.95137946, 4.38825978, 6.24318725]), 'targetState': array([27,  7], dtype=int32), 'currentDistance': 19.226833629258174}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9230540525334234
{'scaleFactor': 20, 'currentTarget': array([27.,  7.]), 'previousTarget': array([27.,  7.]), 'currentState': array([27.53282701,  7.10050601,  0.2369958 ]), 'targetState': array([27,  7], dtype=int32), 'currentDistance': 0.5422232727584818}
episode index:1375
target Thresh 31.999972666777342
target distance 9.0
model initialize at round 1375
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 7.87312619, 12.56123708,  4.24783659]), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.088705727794949}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9230813359327451
{'scaleFactor': 20, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.12845473, 5.05896315, 4.46597641]), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.14134097338417032}
episode index:1376
target Thresh 31.99997293874745
target distance 7.0
model initialize at round 1376
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  5.]), 'previousTarget': array([21.,  5.]), 'currentState': array([15.58769835,  6.55821335,  5.61127288]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 5.632143284121819}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9231156261753503
{'scaleFactor': 20, 'currentTarget': array([21.,  5.]), 'previousTarget': array([21.,  5.]), 'currentState': array([21.29051332,  4.86347122,  6.14785993]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 0.32099547354525937}
episode index:1377
target Thresh 31.999973208011415
target distance 9.0
model initialize at round 1377
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([13.98645939, 14.65645105,  3.05609345]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 9.786673193691941}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9231358543493159
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.60617953, 9.42275495, 3.75827572]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.7390367807229521}
episode index:1378
target Thresh 31.999973474596157
target distance 13.0
model initialize at round 1378
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 27.]), 'previousTarget': array([15., 27.]), 'currentState': array([19.50867109, 15.60965337,  2.87705255]), 'targetState': array([15, 27], dtype=int32), 'currentDistance': 12.25022902765407}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9231491569563149
{'scaleFactor': 20, 'currentTarget': array([15., 27.]), 'previousTarget': array([15., 27.]), 'currentState': array([15.36776171, 26.48302936,  1.946912  ]), 'targetState': array([15, 27], dtype=int32), 'currentDistance': 0.6344346416296838}
episode index:1379
target Thresh 31.999973738528332
target distance 16.0
model initialize at round 1379
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 8.]), 'previousTarget': array([7., 8.]), 'currentState': array([21.18771565, 12.275599  ,  3.90774542]), 'targetState': array([7, 8], dtype=int32), 'currentDistance': 14.817962825137036}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9231556179642502
{'scaleFactor': 20, 'currentTarget': array([7., 8.]), 'previousTarget': array([7., 8.]), 'currentState': array([7.89825936, 8.40287325, 3.44607666]), 'targetState': array([7, 8], dtype=int32), 'currentDistance': 0.9844677397286875}
episode index:1380
target Thresh 31.999973999834342
target distance 24.0
model initialize at round 1380
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 22.]), 'previousTarget': array([ 2., 22.]), 'currentState': array([27.01497608, 13.34246477,  1.9334318 ]), 'targetState': array([ 2, 22], dtype=int32), 'currentDistance': 26.47077528835705}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.9231099284180352
{'scaleFactor': 20, 'currentTarget': array([ 2., 22.]), 'previousTarget': array([ 2., 22.]), 'currentState': array([ 2.74705694, 21.41095953,  2.49595886]), 'targetState': array([ 2, 22], dtype=int32), 'currentDistance': 0.951347857520295}
episode index:1381
target Thresh 31.999974258540313
target distance 16.0
model initialize at round 1381
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 16.]), 'previousTarget': array([26., 16.]), 'currentState': array([11.9011862 , 20.7163032 ,  0.34261632]), 'targetState': array([26, 16], dtype=int32), 'currentDistance': 14.866743641531976}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9231096641387369
{'scaleFactor': 20, 'currentTarget': array([26., 16.]), 'previousTarget': array([26., 16.]), 'currentState': array([26.51726553, 15.13791604,  5.3900608 ]), 'targetState': array([26, 16], dtype=int32), 'currentDistance': 1.0053618158099542}
episode index:1382
target Thresh 31.999974514672118
target distance 17.0
model initialize at round 1382
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([10.08487343, 25.71336185,  4.402913  ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 16.85038353282512}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9231094002416212
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.34214785, 10.86127726,  4.35390672]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.9267489763805249}
episode index:1383
target Thresh 31.999974768255367
target distance 5.0
model initialize at round 1383
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 21.]), 'previousTarget': array([22., 21.]), 'currentState': array([25.48078693, 18.72429386,  1.68682313]), 'targetState': array([22, 21], dtype=int32), 'currentDistance': 4.158691636287897}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9231505784206374
{'scaleFactor': 20, 'currentTarget': array([22., 21.]), 'previousTarget': array([22., 21.]), 'currentState': array([22.48560958, 20.28174971,  1.65385544]), 'targetState': array([22, 21], dtype=int32), 'currentDistance': 0.8670064275325428}
episode index:1384
target Thresh 31.99997501931542
target distance 12.0
model initialize at round 1384
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 23.]), 'previousTarget': array([24., 23.]), 'currentState': array([12.04236887, 21.31756338,  5.7475667 ]), 'targetState': array([24, 23], dtype=int32), 'currentDistance': 12.07541035297488}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9231570150773062
{'scaleFactor': 20, 'currentTarget': array([24., 23.]), 'previousTarget': array([24., 23.]), 'currentState': array([24.83937347, 22.20973555,  5.60653594]), 'targetState': array([24, 23], dtype=int32), 'currentDistance': 1.152851129175083}
episode index:1385
target Thresh 31.999975267877385
target distance 17.0
model initialize at round 1385
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  9.]), 'previousTarget': array([20.,  9.]), 'currentState': array([4.86254688, 6.37511893, 0.75635236]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 15.363348835820231}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.92315671758766
{'scaleFactor': 20, 'currentTarget': array([20.,  9.]), 'previousTarget': array([20.,  9.]), 'currentState': array([20.15291437,  9.02967731,  0.04025662]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 0.15576761166998823}
episode index:1386
target Thresh 31.999975513966117
target distance 16.0
model initialize at round 1386
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([16.03879057, 16.67587672,  4.36761452]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 16.696173600976646}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9231497677173617
{'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([23.61171879,  2.95637721,  5.0234281 ]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 1.0321916770918511}
episode index:1387
target Thresh 31.999975757606226
target distance 8.0
model initialize at round 1387
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 2.]), 'previousTarget': array([6., 2.]), 'currentState': array([12.38395204,  6.5301308 ,  4.43454361]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 7.8279581431389165}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.923176746278084
{'scaleFactor': 20, 'currentTarget': array([6., 2.]), 'previousTarget': array([6., 2.]), 'currentState': array([6.01277618, 2.09618334, 3.79705036]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 0.09702816676529566}
episode index:1388
target Thresh 31.999975998822073
target distance 17.0
model initialize at round 1388
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([22.88021942, 16.3212978 ,  3.6311568 ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 17.699096533735556}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9231697919952947
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.95859791, 11.16372301,  3.53571695]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.16887674632824393}
episode index:1389
target Thresh 31.99997623763778
target distance 13.0
model initialize at round 1389
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 15.]), 'previousTarget': array([ 3., 15.]), 'currentState': array([14.44722665, 15.29237432,  4.53924847]), 'targetState': array([ 3, 15], dtype=int32), 'currentDistance': 11.450959813338104}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9231829649142916
{'scaleFactor': 20, 'currentTarget': array([ 3., 15.]), 'previousTarget': array([ 3., 15.]), 'currentState': array([ 3.04682178, 15.16589757,  3.39073456]), 'targetState': array([ 3, 15], dtype=int32), 'currentDistance': 0.17237831065196216}
episode index:1390
target Thresh 31.999976474077236
target distance 2.0
model initialize at round 1390
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  6.]), 'previousTarget': array([18.,  6.]), 'currentState': array([18.67940226,  3.89047313,  0.94487446]), 'targetState': array([18,  6], dtype=int32), 'currentDistance': 2.2162335241342377}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9232310001659708
{'scaleFactor': 20, 'currentTarget': array([18.,  6.]), 'previousTarget': array([18.,  6.]), 'currentState': array([18.40523159,  5.66948039,  2.51827496]), 'targetState': array([18,  6], dtype=int32), 'currentDistance': 0.5229300660560475}
episode index:1391
target Thresh 31.99997670816408
target distance 6.0
model initialize at round 1391
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([6.10064994, 6.45367465, 3.69112903]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 4.778686981083288}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9232718543325182
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.59451437, 4.53433346, 3.72241636]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.7993494776533884}
episode index:1392
target Thresh 31.99997693992172
target distance 15.0
model initialize at round 1392
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  8.]), 'previousTarget': array([19.,  8.]), 'currentState': array([ 7.35164697, 23.33958855,  3.94841838]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 19.2610255503276}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9232582938304911
{'scaleFactor': 20, 'currentTarget': array([19.,  8.]), 'previousTarget': array([19.,  8.]), 'currentState': array([18.93210132,  8.05419655,  5.39702483]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 0.08687632965099991}
episode index:1393
target Thresh 31.999977169373334
target distance 17.0
model initialize at round 1393
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([ 4.65010372, 20.33097706,  5.47113806]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 21.694657719546395}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9232382651076992
{'scaleFactor': 20, 'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([19.96246909,  4.80392542,  5.62516128]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 0.19963419136389723}
episode index:1394
target Thresh 31.999977396541865
target distance 15.0
model initialize at round 1394
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  8.]), 'previousTarget': array([12.,  8.]), 'currentState': array([24.03075031, 21.26298404,  4.38590705]), 'targetState': array([12,  8], dtype=int32), 'currentDistance': 17.906582554244853}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9232312966362841
{'scaleFactor': 20, 'currentTarget': array([12.,  8.]), 'previousTarget': array([12.,  8.]), 'currentState': array([11.9941467 ,  8.36454442,  4.08100476]), 'targetState': array([12,  8], dtype=int32), 'currentDistance': 0.3645914088265236}
episode index:1395
target Thresh 31.99997762145003
target distance 7.0
model initialize at round 1395
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 22.]), 'previousTarget': array([20., 22.]), 'currentState': array([14.68875392, 20.05310736,  0.3843413 ]), 'targetState': array([20, 22], dtype=int32), 'currentDistance': 5.656830015968003}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9232650127561722
{'scaleFactor': 20, 'currentTarget': array([20., 22.]), 'previousTarget': array([20., 22.]), 'currentState': array([20.35430721, 21.90650519,  0.60852262]), 'targetState': array([20, 22], dtype=int32), 'currentDistance': 0.3664353661074221}
episode index:1396
target Thresh 31.999977844120323
target distance 5.0
model initialize at round 1396
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 24.]), 'previousTarget': array([22., 24.]), 'currentState': array([18.6265327 , 23.43471156,  1.27006512]), 'targetState': array([22, 24], dtype=int32), 'currentDistance': 3.420501807655868}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.923305696354772
{'scaleFactor': 20, 'currentTarget': array([22., 24.]), 'previousTarget': array([22., 24.]), 'currentState': array([22.11212573, 23.5489551 ,  0.36911136]), 'targetState': array([22, 24], dtype=int32), 'currentDistance': 0.4647727224299213}
episode index:1397
target Thresh 31.99997806457501
target distance 17.0
model initialize at round 1397
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([14.88661569, 25.73795267,  4.99949288]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 18.07360209414738}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9232986946030759
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 6.32967001, 10.41122093,  4.13593951]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.5270530985363597}
episode index:1398
target Thresh 31.999978282836135
target distance 3.0
model initialize at round 1398
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 14.]), 'previousTarget': array([ 7., 14.]), 'currentState': array([ 4.76155673, 13.50080799,  0.09122407]), 'targetState': array([ 7, 14], dtype=int32), 'currentDistance': 2.2934299529007647}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9233463724482488
{'scaleFactor': 20, 'currentTarget': array([ 7., 14.]), 'previousTarget': array([ 7., 14.]), 'currentState': array([ 6.69849433, 13.96915884,  0.38621771]), 'targetState': array([ 7, 14], dtype=int32), 'currentDistance': 0.303078940790358}
episode index:1399
target Thresh 31.999978498925525
target distance 12.0
model initialize at round 1399
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 17.]), 'previousTarget': array([ 7., 17.]), 'currentState': array([10.55288658,  6.8996161 ,  2.18854964]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 10.707042447622609}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.923366117932143
{'scaleFactor': 20, 'currentTarget': array([ 7., 17.]), 'previousTarget': array([ 7., 17.]), 'currentState': array([ 7.33427847, 16.30216978,  1.99604708]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 0.7737629585182392}
episode index:1400
target Thresh 31.99997871286479
target distance 9.0
model initialize at round 1400
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 21.]), 'previousTarget': array([13., 21.]), 'currentState': array([ 2.59038759, 28.91944869,  3.57362044]), 'targetState': array([13, 21], dtype=int32), 'currentDistance': 13.07966735318453}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9233723272326245
{'scaleFactor': 20, 'currentTarget': array([13., 21.]), 'previousTarget': array([13., 21.]), 'currentState': array([12.65616855, 21.28157221,  5.81840644]), 'targetState': array([13, 21], dtype=int32), 'currentDistance': 0.4444130646680477}
episode index:1401
target Thresh 31.999978924675325
target distance 12.0
model initialize at round 1401
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 17.]), 'previousTarget': array([14., 17.]), 'currentState': array([24.83721101, 20.21667985,  3.34355009]), 'targetState': array([14, 17], dtype=int32), 'currentDistance': 11.30451997374952}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9233852429403053
{'scaleFactor': 20, 'currentTarget': array([14., 17.]), 'previousTarget': array([14., 17.]), 'currentState': array([13.46996801, 16.74747086,  3.84473528]), 'targetState': array([14, 17], dtype=int32), 'currentDistance': 0.5871157236074585}
episode index:1402
target Thresh 31.99997913437831
target distance 1.0
model initialize at round 1402
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  5.]), 'previousTarget': array([22.,  5.]), 'currentState': array([23.02384536,  2.65582966,  0.0788266 ]), 'targetState': array([22,  5], dtype=int32), 'currentDistance': 2.5580058408920494}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9234256668583807
{'scaleFactor': 20, 'currentTarget': array([22.,  5.]), 'previousTarget': array([22.,  5.]), 'currentState': array([22.2180623 ,  5.16546506,  3.07540768]), 'targetState': array([22,  5], dtype=int32), 'currentDistance': 0.2737331809955088}
episode index:1403
target Thresh 31.999979341994713
target distance 13.0
model initialize at round 1403
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 18.]), 'previousTarget': array([12., 18.]), 'currentState': array([5.00793993, 6.64379266, 1.49204212]), 'targetState': array([12, 18], dtype=int32), 'currentDistance': 13.336129465743868}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.923431820477361
{'scaleFactor': 20, 'currentTarget': array([12., 18.]), 'previousTarget': array([12., 18.]), 'currentState': array([12.19288373, 18.39670035,  1.34420839]), 'targetState': array([12, 18], dtype=int32), 'currentDistance': 0.4411069015240074}
episode index:1404
target Thresh 31.9999795475453
target distance 8.0
model initialize at round 1404
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 19.]), 'previousTarget': array([ 7., 19.]), 'currentState': array([12.7143327 , 12.08603047,  1.4301784 ]), 'targetState': array([ 7, 19], dtype=int32), 'currentDistance': 8.96975879787508}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9234514348755266
{'scaleFactor': 20, 'currentTarget': array([ 7., 19.]), 'previousTarget': array([ 7., 19.]), 'currentState': array([ 7.43272246, 18.67434366,  2.40727663]), 'targetState': array([ 7, 19], dtype=int32), 'currentDistance': 0.5415725084986177}
episode index:1405
target Thresh 31.999979751050624
target distance 22.0
model initialize at round 1405
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 28.]), 'previousTarget': array([21., 28.]), 'currentState': array([14.92010616,  7.94119667,  1.1918543 ]), 'targetState': array([21, 28], dtype=int32), 'currentDistance': 20.959978532142266}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9234314397257279
{'scaleFactor': 20, 'currentTarget': array([21., 28.]), 'previousTarget': array([21., 28.]), 'currentState': array([21.20283257, 28.4240984 ,  1.44588136]), 'targetState': array([21, 28], dtype=int32), 'currentDistance': 0.4701069086683073}
episode index:1406
target Thresh 31.99997995253104
target distance 17.0
model initialize at round 1406
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 16.]), 'previousTarget': array([24., 16.]), 'currentState': array([8.21640805, 5.47714561, 0.36407232]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 18.969771722625712}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9234179007316149
{'scaleFactor': 20, 'currentTarget': array([24., 16.]), 'previousTarget': array([24., 16.]), 'currentState': array([24.58780517, 16.37623621,  0.6582362 ]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 0.6979030077400881}
episode index:1407
target Thresh 31.99998015200669
target distance 16.0
model initialize at round 1407
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 16.]), 'previousTarget': array([11., 16.]), 'currentState': array([25.38370614, 25.53097736,  4.4340198 ]), 'targetState': array([11, 16], dtype=int32), 'currentDistance': 17.254869797709848}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9234108690176605
{'scaleFactor': 20, 'currentTarget': array([11., 16.]), 'previousTarget': array([11., 16.]), 'currentState': array([10.65499097, 15.63492027,  4.00713939]), 'targetState': array([11, 16], dtype=int32), 'currentDistance': 0.5023091106931405}
episode index:1408
target Thresh 31.99998034949752
target distance 18.0
model initialize at round 1408
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 20.]), 'previousTarget': array([25., 20.]), 'currentState': array([14.73555177,  3.51372126,  0.1084739 ]), 'targetState': array([25, 20], dtype=int32), 'currentDistance': 19.420511947688553}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9233909452314581
{'scaleFactor': 20, 'currentTarget': array([25., 20.]), 'previousTarget': array([25., 20.]), 'currentState': array([24.46690646, 19.1108836 ,  1.68407381]), 'targetState': array([25, 20], dtype=int32), 'currentDistance': 1.036685438900378}
episode index:1409
target Thresh 31.999980545023288
target distance 7.0
model initialize at round 1409
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 27.]), 'previousTarget': array([20., 27.]), 'currentState': array([14.78404795, 28.42125445,  0.30931824]), 'targetState': array([20, 27], dtype=int32), 'currentDistance': 5.406118753986446}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9234242133554075
{'scaleFactor': 20, 'currentTarget': array([20., 27.]), 'previousTarget': array([20., 27.]), 'currentState': array([20.4421665, 26.9042016,  5.9449949]), 'targetState': array([20, 27], dtype=int32), 'currentDistance': 0.4524251833668301}
episode index:1410
target Thresh 31.99998073860354
target distance 24.0
model initialize at round 1410
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([ 5.27605072, 24.13540087,  4.26066279]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 22.37651626013173}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.923404308352504
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.10914718, 2.8777446 , 4.51566284]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.884504774895949}
episode index:1411
target Thresh 31.99998093025764
target distance 10.0
model initialize at round 1411
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  5.]), 'previousTarget': array([24.,  5.]), 'currentState': array([15.91315458,  3.57931082,  0.23706682]), 'targetState': array([24,  5], dtype=int32), 'currentDistance': 8.210689775832194}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.923430648084549
{'scaleFactor': 20, 'currentTarget': array([24.,  5.]), 'previousTarget': array([24.,  5.]), 'currentState': array([23.77804948,  4.7794132 ,  0.49434628]), 'targetState': array([24,  5], dtype=int32), 'currentDistance': 0.31292262576609553}
episode index:1412
target Thresh 31.999981120004744
target distance 25.0
model initialize at round 1412
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([14.88492872, 25.32096853,  5.65396166]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 23.34761137758125}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9234044302668785
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.0527081 ,  2.35895232,  4.61743815]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.36280147593883233}
episode index:1413
target Thresh 31.999981307863834
target distance 6.0
model initialize at round 1413
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  2.]), 'previousTarget': array([21.,  2.]), 'currentState': array([23.31946232,  7.90955006,  4.2053628 ]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 6.348439759386493}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9234375947433516
{'scaleFactor': 20, 'currentTarget': array([21.,  2.]), 'previousTarget': array([21.,  2.]), 'currentState': array([21.01835681,  2.40426335,  4.61896325]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 0.40467991085734467}
episode index:1414
target Thresh 31.9999814938537
target distance 14.0
model initialize at round 1414
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 20.]), 'previousTarget': array([20., 20.]), 'currentState': array([21.62392562,  7.64041492,  0.78616011]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 12.465812362029348}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9234436920954108
{'scaleFactor': 20, 'currentTarget': array([20., 20.]), 'previousTarget': array([20., 20.]), 'currentState': array([19.94716002, 19.35208191,  1.87318504]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 0.650069160953978}
episode index:1415
target Thresh 31.99998167799293
target distance 5.0
model initialize at round 1415
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 19.]), 'previousTarget': array([21., 19.]), 'currentState': array([25.98233728, 25.68287733,  2.59129149]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 8.335738374016849}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9234699296080553
{'scaleFactor': 20, 'currentTarget': array([21., 19.]), 'previousTarget': array([21., 19.]), 'currentState': array([21.41971758, 19.87781381,  4.14316034]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 0.97299533745941}
episode index:1416
target Thresh 31.999981860299947
target distance 9.0
model initialize at round 1416
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 14.]), 'previousTarget': array([ 6., 14.]), 'currentState': array([ 9.14693899, 21.19280685,  4.20169069]), 'targetState': array([ 6, 14], dtype=int32), 'currentDistance': 7.8510951716809245}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.923496130088219
{'scaleFactor': 20, 'currentTarget': array([ 6., 14.]), 'previousTarget': array([ 6., 14.]), 'currentState': array([ 5.96523374, 13.91222025,  4.66473845]), 'targetState': array([ 6, 14], dtype=int32), 'currentDistance': 0.09441386188978768}
episode index:1417
target Thresh 31.99998204079298
target distance 8.0
model initialize at round 1417
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 23.]), 'previousTarget': array([26., 23.]), 'currentState': array([18.42448748, 25.37144283,  5.97736812]), 'targetState': array([26, 23], dtype=int32), 'currentDistance': 7.938018077370006}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9235222936142499
{'scaleFactor': 20, 'currentTarget': array([26., 23.]), 'previousTarget': array([26., 23.]), 'currentState': array([26.02994553, 22.96926615,  6.23305153]), 'targetState': array([26, 23], dtype=int32), 'currentDistance': 0.04291042759464858}
episode index:1418
target Thresh 31.999982219490075
target distance 22.0
model initialize at round 1418
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 25.]), 'previousTarget': array([18., 25.]), 'currentState': array([11.90834219,  4.9975793 ,  1.64789427]), 'targetState': array([18, 25], dtype=int32), 'currentDistance': 20.90945070159795}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9235024317119556
{'scaleFactor': 20, 'currentTarget': array([18., 25.]), 'previousTarget': array([18., 25.]), 'currentState': array([18.60091436, 25.42365501,  0.7320454 ]), 'targetState': array([18, 25], dtype=int32), 'currentDistance': 0.7352425678558544}
episode index:1419
target Thresh 31.999982396409102
target distance 4.0
model initialize at round 1419
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 20.]), 'previousTarget': array([19., 20.]), 'currentState': array([13.9921493 , 21.65217767,  5.08032155]), 'targetState': array([19, 20], dtype=int32), 'currentDistance': 5.2733537439695555}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9235353870417359
{'scaleFactor': 20, 'currentTarget': array([19., 20.]), 'previousTarget': array([19., 20.]), 'currentState': array([18.79816503, 19.82101679,  6.19653261]), 'targetState': array([19, 20], dtype=int32), 'currentDistance': 0.2697634924485668}
episode index:1420
target Thresh 31.99998257156776
target distance 5.0
model initialize at round 1420
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  3.]), 'previousTarget': array([13.,  3.]), 'currentState': array([9.98784359, 2.11536735, 0.22230079]), 'targetState': array([13,  3], dtype=int32), 'currentDistance': 3.139372734123995}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.923575193243677
{'scaleFactor': 20, 'currentTarget': array([13.,  3.]), 'previousTarget': array([13.,  3.]), 'currentState': array([13.79624992,  3.27870907,  0.51809469]), 'targetState': array([13,  3], dtype=int32), 'currentDistance': 0.8436187953771901}
episode index:1421
target Thresh 31.99998274498356
target distance 15.0
model initialize at round 1421
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  8.]), 'previousTarget': array([19.,  8.]), 'currentState': array([21.94885451, 21.61001324,  4.30136571]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 13.925810686373666}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9235811638165765
{'scaleFactor': 20, 'currentTarget': array([19.,  8.]), 'previousTarget': array([19.,  8.]), 'currentState': array([18.88712907,  8.0569143 ,  4.82986006]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 0.12640840025128475}
episode index:1422
target Thresh 31.99998291667384
target distance 17.0
model initialize at round 1422
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 21.]), 'previousTarget': array([10., 21.]), 'currentState': array([26.10219282, 16.5765065 ,  3.13969803]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 16.698739721132213}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9235740914930819
{'scaleFactor': 20, 'currentTarget': array([10., 21.]), 'previousTarget': array([10., 21.]), 'currentState': array([ 9.27013106, 21.23933227,  2.80115488]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 0.7681071560697031}
episode index:1423
target Thresh 31.999983086655774
target distance 9.0
model initialize at round 1423
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 17.]), 'previousTarget': array([13., 17.]), 'currentState': array([18.0324626 , 24.62127041,  3.09279346]), 'targetState': array([13, 17], dtype=int32), 'currentDistance': 9.132877011517792}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.923593344272862
{'scaleFactor': 20, 'currentTarget': array([13., 17.]), 'previousTarget': array([13., 17.]), 'currentState': array([13.15958571, 17.7570676 ,  5.09864032]), 'targetState': array([13, 17], dtype=int32), 'currentDistance': 0.7737046962901392}
episode index:1424
target Thresh 31.99998325494636
target distance 4.0
model initialize at round 1424
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 22.]), 'previousTarget': array([21., 22.]), 'currentState': array([18.27351739, 21.89975387,  0.29746693]), 'targetState': array([21, 22], dtype=int32), 'currentDistance': 2.728324891872382}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9236399454347758
{'scaleFactor': 20, 'currentTarget': array([21., 22.]), 'previousTarget': array([21., 22.]), 'currentState': array([20.24813896, 22.14404514,  6.2283757 ]), 'targetState': array([21, 22], dtype=int32), 'currentDistance': 0.7655351232736614}
episode index:1425
target Thresh 31.99998342156243
target distance 16.0
model initialize at round 1425
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 19.]), 'previousTarget': array([25., 19.]), 'currentState': array([14.69831883,  4.65571029,  0.74102569]), 'targetState': array([25, 19], dtype=int32), 'currentDistance': 17.660217500407853}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9236328467686109
{'scaleFactor': 20, 'currentTarget': array([25., 19.]), 'previousTarget': array([25., 19.]), 'currentState': array([25.17141181, 18.80421959,  1.01664149]), 'targetState': array([25, 19], dtype=int32), 'currentDistance': 0.26021525183559574}
episode index:1426
target Thresh 31.99998358652064
target distance 15.0
model initialize at round 1426
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([ 4.94127505, 22.30825329,  3.26114333]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 19.704175262328597}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9236130187430258
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.39656745,  5.41514864,  5.38246642]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.7066235597553285}
episode index:1427
target Thresh 31.999983749837487
target distance 17.0
model initialize at round 1427
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 19.]), 'previousTarget': array([14., 19.]), 'currentState': array([16.33389048,  2.25010933,  1.98562431]), 'targetState': array([14, 19], dtype=int32), 'currentDistance': 16.911708436061623}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9236059488751971
{'scaleFactor': 20, 'currentTarget': array([14., 19.]), 'previousTarget': array([14., 19.]), 'currentState': array([13.82624299, 19.76272832,  1.80392433]), 'targetState': array([14, 19], dtype=int32), 'currentDistance': 0.7822697646917712}
episode index:1428
target Thresh 31.999983911529306
target distance 6.0
model initialize at round 1428
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  8.]), 'previousTarget': array([11.,  8.]), 'currentState': array([ 9.66930409, 15.03035745,  3.49272025]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 7.1551853583704155}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9236318341524014
{'scaleFactor': 20, 'currentTarget': array([11.,  8.]), 'previousTarget': array([11.,  8.]), 'currentState': array([11.14701291,  7.78077627,  5.19395142]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 0.2639542365890493}
episode index:1429
target Thresh 31.999984071612264
target distance 16.0
model initialize at round 1429
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 24.]), 'previousTarget': array([20., 24.]), 'currentState': array([ 5.35070914, 24.00397853,  5.9123829 ]), 'targetState': array([20, 24], dtype=int32), 'currentDistance': 14.649291397766458}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9236377317144674
{'scaleFactor': 20, 'currentTarget': array([20., 24.]), 'previousTarget': array([20., 24.]), 'currentState': array([19.08814909, 24.06963044,  6.17490591]), 'targetState': array([20, 24], dtype=int32), 'currentDistance': 0.9145055979017858}
episode index:1430
target Thresh 31.99998423010237
target distance 19.0
model initialize at round 1430
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  6.]), 'previousTarget': array([22.,  6.]), 'currentState': array([4.62069861, 2.45356818, 1.28287857]), 'targetState': array([22,  6], dtype=int32), 'currentDistance': 17.737454591932874}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9236242756301168
{'scaleFactor': 20, 'currentTarget': array([22.,  6.]), 'previousTarget': array([22.,  6.]), 'currentState': array([21.01841654,  6.10925117,  0.12933516]), 'targetState': array([22,  6], dtype=int32), 'currentDistance': 0.9876446261383256}
episode index:1431
target Thresh 31.999984387015473
target distance 25.0
model initialize at round 1431
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6.80561645, 3.18694837]), 'previousTarget': array([6.84750197, 3.11513418]), 'currentState': array([23.65333945, 28.00940252,  3.50837338]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 30.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.9235620476174212
{'scaleFactor': 20, 'currentTarget': array([6., 2.]), 'previousTarget': array([6., 2.]), 'currentState': array([5.82469397, 1.71205873, 4.01597673]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 0.3371088573404433}
episode index:1432
target Thresh 31.999984542367265
target distance 6.0
model initialize at round 1432
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 6.21924528, 11.12983238,  3.36101329]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 4.3679001718278}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9236015018758879
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.37502982, 10.05341638,  3.56052595]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.37881482901693164}
episode index:1433
target Thresh 31.999984696173282
target distance 20.0
model initialize at round 1433
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 22.]), 'previousTarget': array([19., 22.]), 'currentState': array([4.76226088, 3.66609369, 0.70253205]), 'targetState': array([19, 22], dtype=int32), 'currentDistance': 23.213042360229903}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9235755488562505
{'scaleFactor': 20, 'currentTarget': array([19., 22.]), 'previousTarget': array([19., 22.]), 'currentState': array([18.99859749, 22.17477773,  0.90457082]), 'targetState': array([19, 22], dtype=int32), 'currentDistance': 0.1747833549164145}
episode index:1434
target Thresh 31.9999848484489
target distance 22.0
model initialize at round 1434
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([ 8.9677573 , 25.30318057,  5.60972118]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 21.180346859441045}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9235558712990396
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.18240157,  4.54046907,  5.08226336]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.4944077309651087}
episode index:1435
target Thresh 31.999984999209353
target distance 6.0
model initialize at round 1435
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 15.]), 'previousTarget': array([ 5., 15.]), 'currentState': array([ 8.54356786, 20.15667711,  4.67645288]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 6.256851602007102}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9235884222243189
{'scaleFactor': 20, 'currentTarget': array([ 5., 15.]), 'previousTarget': array([ 5., 15.]), 'currentState': array([ 5.24672345, 15.28944922,  4.07053418]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 0.38033316147428947}
episode index:1436
target Thresh 31.999985148469715
target distance 16.0
model initialize at round 1436
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  9.]), 'previousTarget': array([20.,  9.]), 'currentState': array([ 5.43694844, 23.12389083,  4.7256631 ]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 20.287108297031093}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.923562532488405
{'scaleFactor': 20, 'currentTarget': array([20.,  9.]), 'previousTarget': array([20.,  9.]), 'currentState': array([20.08656596,  9.37715829,  4.95750314]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 0.3869651717854459}
episode index:1437
target Thresh 31.999985296244912
target distance 16.0
model initialize at round 1437
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 24.]), 'previousTarget': array([27., 24.]), 'currentState': array([22.34062634,  9.01740312,  1.65917366]), 'targetState': array([27, 24], dtype=int32), 'currentDistance': 15.690378335015769}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9235619637554005
{'scaleFactor': 20, 'currentTarget': array([27., 24.]), 'previousTarget': array([27., 24.]), 'currentState': array([27.01702108, 24.05351002,  1.28336336]), 'targetState': array([27, 24], dtype=int32), 'currentDistance': 0.056151927590917215}
episode index:1438
target Thresh 31.99998544254972
target distance 5.0
model initialize at round 1438
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([20.49332942, 11.60489034,  2.88659763]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 4.7713396714138465}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9236012535651603
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.91140523, 10.16282639,  3.41834083]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.9258358007160484}
episode index:1439
target Thresh 31.99998558739877
target distance 11.0
model initialize at round 1439
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 24.]), 'previousTarget': array([10., 24.]), 'currentState': array([ 5.92712077, 14.98100159,  1.37478402]), 'targetState': array([10, 24], dtype=int32), 'currentDistance': 9.895993002940074}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9236202735626151
{'scaleFactor': 20, 'currentTarget': array([10., 24.]), 'previousTarget': array([10., 24.]), 'currentState': array([10.05475829, 24.04179006,  1.22802919]), 'targetState': array([10, 24], dtype=int32), 'currentDistance': 0.06888308009971995}
episode index:1440
target Thresh 31.999985730806547
target distance 23.0
model initialize at round 1440
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([25.03101812,  4.68268415,  2.5623647 ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 23.26869958157441}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9235944335890921
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.91234527, 8.15161694, 3.05595342]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.17513152154070022}
episode index:1441
target Thresh 31.999985872787395
target distance 23.0
model initialize at round 1441
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 21.]), 'previousTarget': array([ 2., 21.]), 'currentState': array([23.399828  , 26.99186188,  4.28547406]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 22.222849663769743}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.923574838457795
{'scaleFactor': 20, 'currentTarget': array([ 2., 21.]), 'previousTarget': array([ 2., 21.]), 'currentState': array([ 2.98018252, 21.13474116,  3.12520874]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 0.9894002959303756}
episode index:1442
target Thresh 31.999986013355507
target distance 21.0
model initialize at round 1442
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 21.]), 'previousTarget': array([25., 21.]), 'currentState': array([4.9389713 , 3.60563515, 1.70319521]), 'targetState': array([25, 21], dtype=int32), 'currentDistance': 26.552001826482176}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.9235308214904931
{'scaleFactor': 20, 'currentTarget': array([25., 21.]), 'previousTarget': array([25., 21.]), 'currentState': array([24.11728138, 20.02759483,  6.00528258]), 'targetState': array([25, 21], dtype=int32), 'currentDistance': 1.3133027009500267}
episode index:1443
target Thresh 31.999986152524947
target distance 14.0
model initialize at round 1443
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 27.]), 'previousTarget': array([21., 27.]), 'currentState': array([25.68425047, 14.53759207,  2.16209623]), 'targetState': array([21, 27], dtype=int32), 'currentDistance': 13.313670187993656}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9235367318273466
{'scaleFactor': 20, 'currentTarget': array([21., 27.]), 'previousTarget': array([21., 27.]), 'currentState': array([20.74195409, 27.53338699,  2.12700255]), 'targetState': array([21, 27], dtype=int32), 'currentDistance': 0.5925279529236815}
episode index:1444
target Thresh 31.999986290309625
target distance 15.0
model initialize at round 1444
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 10.]), 'previousTarget': array([20., 10.]), 'currentState': array([ 3.38412695, 23.47047036,  3.86826968]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 21.390203576286524}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.923517217309998
{'scaleFactor': 20, 'currentTarget': array([20., 10.]), 'previousTarget': array([20., 10.]), 'currentState': array([19.43533868, 10.40103612,  5.47404441]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 0.6925838365581584}
episode index:1445
target Thresh 31.999986426723325
target distance 23.0
model initialize at round 1445
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 3.]), 'previousTarget': array([9., 3.]), 'currentState': array([ 4.43829861, 24.37510541,  5.9858582 ]), 'targetState': array([9, 3], dtype=int32), 'currentDistance': 21.85644644160945}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.923485408046989
{'scaleFactor': 20, 'currentTarget': array([9., 3.]), 'previousTarget': array([9., 3.]), 'currentState': array([9.13146719, 2.70598863, 4.62659961]), 'targetState': array([9, 3], dtype=int32), 'currentDistance': 0.322065691716397}
episode index:1446
target Thresh 31.999986561779682
target distance 8.0
model initialize at round 1446
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 19.]), 'previousTarget': array([ 7., 19.]), 'currentState': array([ 3.61402716, 11.47676453,  1.2972204 ]), 'targetState': array([ 7, 19], dtype=int32), 'currentDistance': 8.25008388177409}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.923511054627468
{'scaleFactor': 20, 'currentTarget': array([ 7., 19.]), 'previousTarget': array([ 7., 19.]), 'currentState': array([ 7.04202401, 18.6654884 ,  1.38155981]), 'targetState': array([ 7, 19], dtype=int32), 'currentDistance': 0.3371409593849801}
episode index:1447
target Thresh 31.999986695492208
target distance 3.0
model initialize at round 1447
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 17.]), 'previousTarget': array([ 5., 17.]), 'currentState': array([ 5.97988851, 15.33971348,  1.21310973]), 'targetState': array([ 5, 17], dtype=int32), 'currentDistance': 1.9278829886891906}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9235569724074214
{'scaleFactor': 20, 'currentTarget': array([ 5., 17.]), 'previousTarget': array([ 5., 17.]), 'currentState': array([ 5.1463213 , 16.89335269,  2.93053591]), 'targetState': array([ 5, 17], dtype=int32), 'currentDistance': 0.18106233946722272}
episode index:1448
target Thresh 31.999986827874274
target distance 14.0
model initialize at round 1448
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  9.]), 'previousTarget': array([19.,  9.]), 'currentState': array([ 6.83107689, 16.29318243,  6.25334179]), 'targetState': array([19,  9], dtype=int32), 'currentDistance': 14.187078610745171}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9235628443021761
{'scaleFactor': 20, 'currentTarget': array([19.,  9.]), 'previousTarget': array([19.,  9.]), 'currentState': array([18.46754861,  9.30836562,  5.57317898]), 'targetState': array([19,  9], dtype=int32), 'currentDistance': 0.6152997932233731}
episode index:1449
target Thresh 31.999986958939115
target distance 17.0
model initialize at round 1449
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 18.]), 'previousTarget': array([24., 18.]), 'currentState': array([6.34196766, 3.54989671, 0.96369326]), 'targetState': array([24, 18], dtype=int32), 'currentDistance': 22.81691458810089}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9235372043210821
{'scaleFactor': 20, 'currentTarget': array([24., 18.]), 'previousTarget': array([24., 18.]), 'currentState': array([24.64379903, 18.40422107,  0.73659497]), 'targetState': array([24, 18], dtype=int32), 'currentDistance': 0.7601788380653175}
episode index:1450
target Thresh 31.99998708869984
target distance 6.0
model initialize at round 1450
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 23.]), 'previousTarget': array([19., 23.]), 'currentState': array([23.76918644, 23.14781796,  3.4010725 ]), 'targetState': array([19, 23], dtype=int32), 'currentDistance': 4.771476656025307}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9235761862615913
{'scaleFactor': 20, 'currentTarget': array([19., 23.]), 'previousTarget': array([19., 23.]), 'currentState': array([19.78999116, 22.96296025,  3.12952259]), 'targetState': array([19, 23], dtype=int32), 'currentDistance': 0.7908590071533318}
episode index:1451
target Thresh 31.999987217169423
target distance 13.0
model initialize at round 1451
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 25.]), 'previousTarget': array([12., 25.]), 'currentState': array([23.42455315, 17.59190819,  1.7722013 ]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 13.616175634389913}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9235756136088132
{'scaleFactor': 20, 'currentTarget': array([12., 25.]), 'previousTarget': array([12., 25.]), 'currentState': array([11.45002415, 25.40359125,  2.40935363]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 0.6821725095517215}
episode index:1452
target Thresh 31.99998734436071
target distance 6.0
model initialize at round 1452
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([13.39421861, 29.72794269,  3.5833813 ]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 8.47026960934739}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9235944810804521
{'scaleFactor': 20, 'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([20.79780184, 25.9454986 ,  6.21308443]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 0.20941466086222352}
episode index:1453
target Thresh 31.999987470286424
target distance 9.0
model initialize at round 1453
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([11.35047459, 17.64607279,  2.37101331]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 12.93709174021894}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9236003069861098
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.95280516, 6.77942421, 4.25408454]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.22556823994596378}
episode index:1454
target Thresh 31.99998759495916
target distance 5.0
model initialize at round 1454
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 12.]), 'previousTarget': array([14., 12.]), 'currentState': array([16.58898332, 16.31645955,  4.94805265]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 5.0333545103525585}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.923632402307769
{'scaleFactor': 20, 'currentTarget': array([14., 12.]), 'previousTarget': array([14., 12.]), 'currentState': array([13.80321642, 11.29600558,  4.26354735]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 0.7309801109700556}
episode index:1455
target Thresh 31.999987718391377
target distance 26.0
model initialize at round 1455
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  3.]), 'previousTarget': array([22.,  3.]), 'currentState': array([14.89086259, 30.2657813 ,  3.30033231]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 28.1773431062903}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.9235887388134926
{'scaleFactor': 20, 'currentTarget': array([22.,  3.]), 'previousTarget': array([22.,  3.]), 'currentState': array([21.98085953,  2.50181081,  4.96559361]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 0.4985567463424569}
episode index:1456
target Thresh 31.999987840595423
target distance 15.0
model initialize at round 1456
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 17.]), 'previousTarget': array([ 9., 17.]), 'currentState': array([22.4537905 ,  1.15213135,  0.48201578]), 'targetState': array([ 9, 17], dtype=int32), 'currentDistance': 20.788444381207736}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9235693493251227
{'scaleFactor': 20, 'currentTarget': array([ 9., 17.]), 'previousTarget': array([ 9., 17.]), 'currentState': array([ 8.97369492, 17.05658063,  2.31709298]), 'targetState': array([ 9, 17], dtype=int32), 'currentDistance': 0.06239651314208897}
episode index:1457
target Thresh 31.99998796158352
target distance 5.0
model initialize at round 1457
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 20.]), 'previousTarget': array([ 4., 20.]), 'currentState': array([ 9.99423447, 22.3578976 ,  1.94879359]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 6.441314153312759}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9235947448399888
{'scaleFactor': 20, 'currentTarget': array([ 4., 20.]), 'previousTarget': array([ 4., 20.]), 'currentState': array([ 3.27428805, 19.53380545,  3.70798778]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 0.8625515604318355}
episode index:1458
target Thresh 31.999988081367768
target distance 16.0
model initialize at round 1458
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([18.68868048, 15.9450928 ,  4.82904863]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 17.198023707601656}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9235815764576507
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.9158012 , 6.9395326 , 3.28659497]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.10366168273816152}
episode index:1459
target Thresh 31.99998819996014
target distance 16.0
model initialize at round 1459
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  4.]), 'previousTarget': array([27.,  4.]), 'currentState': array([22.8117331 , 18.17235636,  5.15522756]), 'targetState': array([27,  4], dtype=int32), 'currentDistance': 14.778270006624588}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9235873872600132
{'scaleFactor': 20, 'currentTarget': array([27.,  4.]), 'previousTarget': array([27.,  4.]), 'currentState': array([26.774057  ,  4.78095855,  4.94764542]), 'targetState': array([27,  4], dtype=int32), 'currentDistance': 0.8129861617512849}
episode index:1460
target Thresh 31.999988317372498
target distance 15.0
model initialize at round 1460
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 27.]), 'previousTarget': array([11., 27.]), 'currentState': array([ 7.63961687, 12.37953208,  1.23746985]), 'targetState': array([11, 27], dtype=int32), 'currentDistance': 15.001675142093834}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9235868104682048
{'scaleFactor': 20, 'currentTarget': array([11., 27.]), 'previousTarget': array([11., 27.]), 'currentState': array([11.43585047, 27.72780327,  1.27705386]), 'targetState': array([11, 27], dtype=int32), 'currentDistance': 0.8483296704841994}
episode index:1461
target Thresh 31.999988433616583
target distance 15.0
model initialize at round 1461
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 14.]), 'previousTarget': array([12., 14.]), 'currentState': array([ 5.33543288, 28.75179766,  4.29961133]), 'targetState': array([12, 14], dtype=int32), 'currentDistance': 16.18740217098568}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9235799229422236
{'scaleFactor': 20, 'currentTarget': array([12., 14.]), 'previousTarget': array([12., 14.]), 'currentState': array([11.91893331, 14.54153463,  4.83576319]), 'targetState': array([12, 14], dtype=int32), 'currentDistance': 0.5475687804239907}
episode index:1462
target Thresh 31.99998854870402
target distance 11.0
model initialize at round 1462
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 25.]), 'previousTarget': array([19., 25.]), 'currentState': array([15.74798825, 15.50761456,  2.12024129]), 'targetState': array([19, 25], dtype=int32), 'currentDistance': 10.033990320445735}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.923592158230302
{'scaleFactor': 20, 'currentTarget': array([19., 25.]), 'previousTarget': array([19., 25.]), 'currentState': array([19.29596292, 25.02007193,  1.13885672]), 'targetState': array([19, 25], dtype=int32), 'currentDistance': 0.2966427645594528}
episode index:1463
target Thresh 31.99998866264632
target distance 5.0
model initialize at round 1463
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([19.67977375, 17.10367371,  1.07164067]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 7.49892265305894}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.923617434085336
{'scaleFactor': 20, 'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([14.02508438, 21.8896875 ,  2.32759673]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 0.11312857341504899}
episode index:1464
target Thresh 31.999988775454874
target distance 14.0
model initialize at round 1464
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 19.]), 'previousTarget': array([25., 19.]), 'currentState': array([22.88995787,  6.42841278,  2.02360791]), 'targetState': array([25, 19], dtype=int32), 'currentDistance': 12.7474343686984}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9236296270650737
{'scaleFactor': 20, 'currentTarget': array([25., 19.]), 'previousTarget': array([25., 19.]), 'currentState': array([24.8258538 , 18.07293113,  1.24743745]), 'targetState': array([25, 19], dtype=int32), 'currentDistance': 0.9432834036396718}
episode index:1465
target Thresh 31.999988887140965
target distance 11.0
model initialize at round 1465
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([ 5.61247592, 14.51801505,  0.71954363]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 10.012483588004546}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9236482903821507
{'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([14.93829204, 17.8223525 ,  0.25461248]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 0.18805985006616344}
episode index:1466
target Thresh 31.99998899771576
target distance 9.0
model initialize at round 1466
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([12.8590805 , 17.01169393,  4.78497317]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 7.064125643020947}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.923673476285094
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([11.63755568,  9.13319357,  4.69212278]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.9395314099088168}
episode index:1467
target Thresh 31.999989107190316
target distance 18.0
model initialize at round 1467
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 27.]), 'previousTarget': array([21., 27.]), 'currentState': array([ 4.19176517, 19.18832696,  1.79393512]), 'targetState': array([21, 27], dtype=int32), 'currentDistance': 18.5348049326812}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9236603350035706
{'scaleFactor': 20, 'currentTarget': array([21., 27.]), 'previousTarget': array([21., 27.]), 'currentState': array([20.83142949, 26.74906557,  0.13867904]), 'targetState': array([21, 27], dtype=int32), 'currentDistance': 0.30229804197759785}
episode index:1468
target Thresh 31.999989215575585
target distance 18.0
model initialize at round 1468
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 24.]), 'previousTarget': array([17., 24.]), 'currentState': array([14.83077846,  7.21049952,  1.32885051]), 'targetState': array([17, 24], dtype=int32), 'currentDistance': 16.929053386085783}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9236534302469199
{'scaleFactor': 20, 'currentTarget': array([17., 24.]), 'previousTarget': array([17., 24.]), 'currentState': array([17.45134918, 24.7047828 ,  1.18047347]), 'targetState': array([17, 24], dtype=int32), 'currentDistance': 0.836919876568775}
episode index:1469
target Thresh 31.9999893228824
target distance 11.0
model initialize at round 1469
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([12.84369239,  3.22284128,  3.33823645]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 11.413756733168107}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9236655572667527
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.6694476 , 9.12561181, 2.44862657]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.35361449569449704}
episode index:1470
target Thresh 31.999989429121495
target distance 9.0
model initialize at round 1470
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([ 8.43470754, 22.12022596,  0.45992535]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 7.64778113014365}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9236906629450213
{'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([16.18141447, 21.16023997,  5.91950138]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 0.2420496984421281}
episode index:1471
target Thresh 31.999989534303495
target distance 4.0
model initialize at round 1471
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  6.]), 'previousTarget': array([18.,  6.]), 'currentState': array([15.34424704,  8.98705129,  0.3638975 ]), 'targetState': array([18,  6], dtype=int32), 'currentDistance': 3.9969362254837892}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9237289845055207
{'scaleFactor': 20, 'currentTarget': array([18.,  6.]), 'previousTarget': array([18.,  6.]), 'currentState': array([17.52462905,  6.26977582,  5.66949511]), 'targetState': array([18,  6], dtype=int32), 'currentDistance': 0.5465862587582582}
episode index:1472
target Thresh 31.999989638438915
target distance 11.0
model initialize at round 1472
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 26.]), 'previousTarget': array([10., 26.]), 'currentState': array([22.70006158, 22.93702514,  0.94197708]), 'targetState': array([10, 26], dtype=int32), 'currentDistance': 13.064202201845772}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9237346439511428
{'scaleFactor': 20, 'currentTarget': array([10., 26.]), 'previousTarget': array([10., 26.]), 'currentState': array([10.20368227, 25.94244415,  3.07398452]), 'targetState': array([10, 26], dtype=int32), 'currentDistance': 0.21165808424810312}
episode index:1473
target Thresh 31.99998974153817
target distance 10.0
model initialize at round 1473
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 19.]), 'previousTarget': array([24., 19.]), 'currentState': array([15.18337588, 10.19685328,  6.06446892]), 'targetState': array([24, 19], dtype=int32), 'currentDistance': 12.45906309067316}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9237402957177342
{'scaleFactor': 20, 'currentTarget': array([24., 19.]), 'previousTarget': array([24., 19.]), 'currentState': array([23.5570717 , 18.48817873,  1.77352738]), 'targetState': array([24, 19], dtype=int32), 'currentDistance': 0.6768651955025315}
episode index:1474
target Thresh 31.99998984361157
target distance 19.0
model initialize at round 1474
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 3.]), 'previousTarget': array([5., 3.]), 'currentState': array([17.86659741, 20.75589926,  4.98351789]), 'targetState': array([5, 3], dtype=int32), 'currentDistance': 21.927637524615445}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.923721040096406
{'scaleFactor': 20, 'currentTarget': array([5., 3.]), 'previousTarget': array([5., 3.]), 'currentState': array([5.08635622, 3.4119964 , 3.91504212]), 'targetState': array([5, 3], dtype=int32), 'currentDistance': 0.42094944176389293}
episode index:1475
target Thresh 31.999989944669323
target distance 16.0
model initialize at round 1475
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([25.25163969,  1.87492837,  0.27780503]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 19.95906296893368}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9237018105667056
{'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([15.07266799, 19.5830964 ,  1.99372555]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 0.5876070500635839}
episode index:1476
target Thresh 31.999990044721535
target distance 14.0
model initialize at round 1476
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  8.]), 'previousTarget': array([21.,  8.]), 'currentState': array([ 5.54158459, 12.16011157,  4.67409635]), 'targetState': array([21,  8], dtype=int32), 'currentDistance': 16.00840827061603}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9237011625530707
{'scaleFactor': 20, 'currentTarget': array([21.,  8.]), 'previousTarget': array([21.,  8.]), 'currentState': array([20.3943521 ,  8.11802943,  5.88184528]), 'targetState': array([21,  8], dtype=int32), 'currentDistance': 0.6170415941530335}
episode index:1477
target Thresh 31.999990143778213
target distance 15.0
model initialize at round 1477
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([23.35647744, 21.1581804 ,  3.98757774]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 16.74497943989562}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9237005154163147
{'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([13.49421336,  8.75717186,  3.88419414]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 0.9041880751328102}
episode index:1478
target Thresh 31.999990241849257
target distance 19.0
model initialize at round 1478
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  4.]), 'previousTarget': array([13.,  4.]), 'currentState': array([14.15030287, 21.02325581,  5.02034569]), 'targetState': array([13,  4], dtype=int32), 'currentDistance': 17.06207593097529}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9236936301776855
{'scaleFactor': 20, 'currentTarget': array([13.,  4.]), 'previousTarget': array([13.,  4.]), 'currentState': array([12.89672377,  3.43004992,  4.40202709]), 'targetState': array([13,  4], dtype=int32), 'currentDistance': 0.5792314540686307}
episode index:1479
target Thresh 31.999990338944478
target distance 27.0
model initialize at round 1479
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([16.89673093,  3.27973299,  3.28102553]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 26.628875922051556}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.9236448221647917
{'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([10.09853256, 29.32785985,  2.58954394]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 0.3423459462865098}
episode index:1480
target Thresh 31.999990435073588
target distance 5.0
model initialize at round 1480
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 15.]), 'previousTarget': array([22., 15.]), 'currentState': array([23.43369066, 10.88143019,  1.5612294 ]), 'targetState': array([22, 15], dtype=int32), 'currentDistance': 4.360973082287596}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9236829417987115
{'scaleFactor': 20, 'currentTarget': array([22., 15.]), 'previousTarget': array([22., 15.]), 'currentState': array([22.48639492, 14.73288248,  2.00737579]), 'targetState': array([22, 15], dtype=int32), 'currentDistance': 0.5549160126364036}
episode index:1481
target Thresh 31.999990530246194
target distance 15.0
model initialize at round 1481
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 22.]), 'previousTarget': array([15., 22.]), 'currentState': array([21.3740551 ,  6.02821778,  0.39443367]), 'targetState': array([15, 22], dtype=int32), 'currentDistance': 17.19669751770615}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9236760823558539
{'scaleFactor': 20, 'currentTarget': array([15., 22.]), 'previousTarget': array([15., 22.]), 'currentState': array([15.21297482, 22.06289463,  1.76087976]), 'targetState': array([15, 22], dtype=int32), 'currentDistance': 0.22206757155404477}
episode index:1482
target Thresh 31.99999062447182
target distance 4.0
model initialize at round 1482
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([ 3.7483279 , 22.90975502,  0.77907673]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 3.0723201817009094}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.92372080515939
{'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([ 5.23439868, 24.24717084,  0.68558294]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 1.0737304664383098}
episode index:1483
target Thresh 31.999990717759886
target distance 9.0
model initialize at round 1483
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([23.45811746, 17.32545805,  2.5439961 ]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 7.8122513473981}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9237391806612367
{'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([15.45065681, 15.59273284,  2.85886294]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 0.8081523113200069}
episode index:1484
target Thresh 31.999990810119716
target distance 20.0
model initialize at round 1484
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  2.]), 'previousTarget': array([10.,  2.]), 'currentState': array([20.58862912, 22.91674442,  3.57553959]), 'targetState': array([10,  2], dtype=int32), 'currentDistance': 23.444173345983366}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9237080573227436
{'scaleFactor': 20, 'currentTarget': array([10.,  2.]), 'previousTarget': array([10.,  2.]), 'currentState': array([9.49517188, 1.81551714, 3.93497197]), 'targetState': array([10,  2], dtype=int32), 'currentDistance': 0.5374805626627086}
episode index:1485
target Thresh 31.999990901560555
target distance 6.0
model initialize at round 1485
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 13.]), 'previousTarget': array([23., 13.]), 'currentState': array([18.72363738, 14.50358369,  5.19535375]), 'targetState': array([23, 13], dtype=int32), 'currentDistance': 4.532994720219483}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9237394105816112
{'scaleFactor': 20, 'currentTarget': array([23., 13.]), 'previousTarget': array([23., 13.]), 'currentState': array([23.15776861, 12.36341601,  5.44739992]), 'targetState': array([23, 13], dtype=int32), 'currentDistance': 0.6558430505591933}
episode index:1486
target Thresh 31.99999099209154
target distance 8.0
model initialize at round 1486
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 15.]), 'previousTarget': array([10., 15.]), 'currentState': array([5.17166603, 5.7918664 , 0.20927924]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 10.397236815611596}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9237513411389882
{'scaleFactor': 20, 'currentTarget': array([10., 15.]), 'previousTarget': array([10., 15.]), 'currentState': array([10.16165225, 14.6146719 ,  6.13482237]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 0.4178626497244773}
episode index:1487
target Thresh 31.999991081721724
target distance 7.0
model initialize at round 1487
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  5.]), 'previousTarget': array([18.,  5.]), 'currentState': array([23.31948231,  4.90917733,  4.20558441]), 'targetState': array([18,  5], dtype=int32), 'currentDistance': 5.320257593026461}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9237826231677926
{'scaleFactor': 20, 'currentTarget': array([18.,  5.]), 'previousTarget': array([18.,  5.]), 'currentState': array([18.28027864,  4.57929113,  2.20174205]), 'targetState': array([18,  5], dtype=int32), 'currentDistance': 0.5055215771034879}
episode index:1488
target Thresh 31.999991170460078
target distance 11.0
model initialize at round 1488
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 14.]), 'previousTarget': array([12., 14.]), 'currentState': array([5.03908478, 4.38167652, 1.16847587]), 'targetState': array([12, 14], dtype=int32), 'currentDistance': 11.87293086199051}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9237945086790306
{'scaleFactor': 20, 'currentTarget': array([12., 14.]), 'previousTarget': array([12., 14.]), 'currentState': array([12.17620406, 13.90184839,  0.67789141]), 'targetState': array([12, 14], dtype=int32), 'currentDistance': 0.20169682443049325}
episode index:1489
target Thresh 31.999991258315468
target distance 4.0
model initialize at round 1489
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 17.]), 'previousTarget': array([ 4., 17.]), 'currentState': array([ 3.84903955, 14.67621545,  0.65066373]), 'targetState': array([ 4, 17], dtype=int32), 'currentDistance': 2.328682824900692}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9238389418946822
{'scaleFactor': 20, 'currentTarget': array([ 4., 17.]), 'previousTarget': array([ 4., 17.]), 'currentState': array([ 3.79021511, 16.37936398,  2.57926476]), 'targetState': array([ 4, 17], dtype=int32), 'currentDistance': 0.6551326370475605}
episode index:1490
target Thresh 31.99999134529668
target distance 14.0
model initialize at round 1490
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 14.]), 'previousTarget': array([ 5., 14.]), 'currentState': array([17.34399839, 10.30007792,  3.97233152]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 12.886571293538275}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9238382079929607
{'scaleFactor': 20, 'currentTarget': array([ 5., 14.]), 'previousTarget': array([ 5., 14.]), 'currentState': array([ 4.83461066, 14.68106584,  3.43066144]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 0.7008596954958634}
episode index:1491
target Thresh 31.99999143141242
target distance 16.0
model initialize at round 1491
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([ 2.68868832, 21.46439052,  6.14397717]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 20.389361869188388}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9238251676893519
{'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([17.55163387,  8.55378507,  5.30489887]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 0.7125377782179402}
episode index:1492
target Thresh 31.99999151667129
target distance 4.0
model initialize at round 1492
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 27.]), 'previousTarget': array([ 6., 27.]), 'currentState': array([ 3.64575966, 27.88622191,  5.88622698]), 'targetState': array([ 6, 27], dtype=int32), 'currentDistance': 2.5155192041663734}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9238694910867469
{'scaleFactor': 20, 'currentTarget': array([ 6., 27.]), 'previousTarget': array([ 6., 27.]), 'currentState': array([ 5.4918101 , 27.11674669,  5.89033707]), 'targetState': array([ 6, 27], dtype=int32), 'currentDistance': 0.5214276174590367}
episode index:1493
target Thresh 31.99999160108182
target distance 17.0
model initialize at round 1493
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  6.]), 'previousTarget': array([26.,  6.]), 'currentState': array([10.85244834, 19.24858383,  5.84383613]), 'targetState': array([26,  6], dtype=int32), 'currentDistance': 20.12394829134707}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9238564473008847
{'scaleFactor': 20, 'currentTarget': array([26.,  6.]), 'previousTarget': array([26.,  6.]), 'currentState': array([25.70709002,  6.66952192,  5.12853655]), 'targetState': array([26,  6], dtype=int32), 'currentDistance': 0.7307912581036983}
episode index:1494
target Thresh 31.999991684652453
target distance 8.0
model initialize at round 1494
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 12.]), 'previousTarget': array([ 2., 12.]), 'currentState': array([8.50040831, 3.23605442, 2.60276031]), 'targetState': array([ 2, 12], dtype=int32), 'currentDistance': 10.911555811763527}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9238745968678406
{'scaleFactor': 20, 'currentTarget': array([ 2., 12.]), 'previousTarget': array([ 2., 12.]), 'currentState': array([ 2.5391025 , 11.04937339,  2.00167988]), 'targetState': array([ 2, 12], dtype=int32), 'currentDistance': 1.092850609450946}
episode index:1495
target Thresh 31.999991767391542
target distance 10.0
model initialize at round 1495
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 26.]), 'previousTarget': array([ 4., 26.]), 'currentState': array([ 8.51708399, 16.72859059,  1.45772904]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 10.313247797666152}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9238927221706695
{'scaleFactor': 20, 'currentTarget': array([ 4., 26.]), 'previousTarget': array([ 4., 26.]), 'currentState': array([ 4.22619589, 25.6251665 ,  1.92974645]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 0.4377953067197753}
episode index:1496
target Thresh 31.999991849307367
target distance 6.0
model initialize at round 1496
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([ 9.82046453, 17.3266336 ,  5.61550784]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 6.216114620766662}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9239172400650111
{'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([16.87456952, 17.96156063,  6.23966414]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 0.8754138601740943}
episode index:1497
target Thresh 31.99999193040811
target distance 5.0
model initialize at round 1497
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([13.48915475, 18.25855902,  4.60780954]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 5.505408779077475}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9239482025215765
{'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([ 9.66729123, 13.89115327,  3.62116864]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 0.35006105153565537}
episode index:1498
target Thresh 31.99999201070189
target distance 8.0
model initialize at round 1498
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 13.]), 'previousTarget': array([13., 13.]), 'currentState': array([ 6.66847085, 13.22044883,  1.14136308]), 'targetState': array([13, 13], dtype=int32), 'currentDistance': 6.335365737211955}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9239726506920092
{'scaleFactor': 20, 'currentTarget': array([13., 13.]), 'previousTarget': array([13., 13.]), 'currentState': array([13.11211638, 12.44531281,  5.63161358]), 'targetState': array([13, 13], dtype=int32), 'currentDistance': 0.5659045491696763}
episode index:1499
target Thresh 31.999992090196738
target distance 7.0
model initialize at round 1499
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 22.]), 'previousTarget': array([ 7., 22.]), 'currentState': array([12.31050198, 21.36102722,  1.99837315]), 'targetState': array([ 7, 22], dtype=int32), 'currentDistance': 5.3488052415963745}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9240035349248812
{'scaleFactor': 20, 'currentTarget': array([ 7., 22.]), 'previousTarget': array([ 7., 22.]), 'currentState': array([ 7.31386722, 22.10329591,  3.99086654]), 'targetState': array([ 7, 22], dtype=int32), 'currentDistance': 0.33042802625981643}
episode index:1500
target Thresh 31.999992168900594
target distance 16.0
model initialize at round 1500
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 27.]), 'previousTarget': array([26., 27.]), 'currentState': array([11.52783898, 23.70575926,  1.44273299]), 'targetState': array([26, 27], dtype=int32), 'currentDistance': 14.842353808651119}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.924002696256995
{'scaleFactor': 20, 'currentTarget': array([26., 27.]), 'previousTarget': array([26., 27.]), 'currentState': array([25.04276035, 27.05101884,  6.21731158]), 'targetState': array([26, 27], dtype=int32), 'currentDistance': 0.9585982792361719}
episode index:1501
target Thresh 31.999992246821336
target distance 6.0
model initialize at round 1501
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([20.68304437,  9.99933234,  1.00947839]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 4.431113968707371}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9240400446616176
{'scaleFactor': 20, 'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([24.05887375,  9.13880208,  0.02976542]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 0.9513068059296412}
episode index:1502
target Thresh 31.999992323966755
target distance 9.0
model initialize at round 1502
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 16.]), 'previousTarget': array([15., 16.]), 'currentState': array([ 4.60954871, 19.05182634,  4.75007963]), 'targetState': array([15, 16], dtype=int32), 'currentDistance': 10.829363883310219}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9240516481910517
{'scaleFactor': 20, 'currentTarget': array([15., 16.]), 'previousTarget': array([15., 16.]), 'currentState': array([15.42947416, 15.68063874,  5.74333301]), 'targetState': array([15, 16], dtype=int32), 'currentDistance': 0.5352005863821585}
episode index:1503
target Thresh 31.99999240034456
target distance 25.0
model initialize at round 1503
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 27.]), 'previousTarget': array([19., 27.]), 'currentState': array([13.06325775,  3.98842769,  1.71896866]), 'targetState': array([19, 27], dtype=int32), 'currentDistance': 23.765045106298285}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9240266037917997
{'scaleFactor': 20, 'currentTarget': array([19., 27.]), 'previousTarget': array([19., 27.]), 'currentState': array([18.91655947, 26.57573663,  0.80791484]), 'targetState': array([19, 27], dtype=int32), 'currentDistance': 0.4323907185862464}
episode index:1504
target Thresh 31.999992475962394
target distance 19.0
model initialize at round 1504
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 24.]), 'previousTarget': array([24., 24.]), 'currentState': array([14.92013075,  3.59083447,  0.01765793]), 'targetState': array([24, 24], dtype=int32), 'currentDistance': 22.33781688514745}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9240015926741413
{'scaleFactor': 20, 'currentTarget': array([24., 24.]), 'previousTarget': array([24., 24.]), 'currentState': array([24.53910884, 24.55970842,  0.92630467]), 'targetState': array([24, 24], dtype=int32), 'currentDistance': 0.7771176628058447}
episode index:1505
target Thresh 31.99999255082782
target distance 9.0
model initialize at round 1505
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 15.]), 'previousTarget': array([ 2., 15.]), 'currentState': array([10.7877045 , 18.33047351,  3.57590866]), 'targetState': array([ 2, 15], dtype=int32), 'currentDistance': 9.397648861562656}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9240195132964691
{'scaleFactor': 20, 'currentTarget': array([ 2., 15.]), 'previousTarget': array([ 2., 15.]), 'currentState': array([ 1.41412819, 14.91627268,  3.35593727]), 'targetState': array([ 2, 15], dtype=int32), 'currentDistance': 0.5918243386249167}
episode index:1506
target Thresh 31.999992624948323
target distance 6.0
model initialize at round 1506
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 25.]), 'previousTarget': array([15., 25.]), 'currentState': array([19.31831333, 26.9342884 ,  4.1906476 ]), 'targetState': array([15, 25], dtype=int32), 'currentDistance': 4.731733466197303}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9240567266254032
{'scaleFactor': 20, 'currentTarget': array([15., 25.]), 'previousTarget': array([15., 25.]), 'currentState': array([15.82885073, 25.27713914,  3.37127724]), 'targetState': array([15, 25], dtype=int32), 'currentDistance': 0.8739563132549111}
episode index:1507
target Thresh 31.99999269833131
target distance 21.0
model initialize at round 1507
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([26.53967803, 24.40590599,  4.02882576]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 22.08334214651497}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9240376825455844
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.33544471,  5.44593581,  3.88497916]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.5580160409656945}
episode index:1508
target Thresh 31.99999277098413
target distance 4.0
model initialize at round 1508
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 11.]), 'previousTarget': array([17., 11.]), 'currentState': array([15.21107168,  8.16862888,  1.7775647 ]), 'targetState': array([17, 11], dtype=int32), 'currentDistance': 3.349168095578426}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9240748345120883
{'scaleFactor': 20, 'currentTarget': array([17., 11.]), 'previousTarget': array([17., 11.]), 'currentState': array([17.55357883, 10.60057099,  1.81297057]), 'targetState': array([17, 11], dtype=int32), 'currentDistance': 0.6826368401856043}
episode index:1509
target Thresh 31.99999284291404
target distance 13.0
model initialize at round 1509
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 10.]), 'previousTarget': array([26., 10.]), 'currentState': array([16.50887134, 21.69327717,  5.67943877]), 'targetState': array([26, 10], dtype=int32), 'currentDistance': 15.060353724292549}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9240739536246152
{'scaleFactor': 20, 'currentTarget': array([26., 10.]), 'previousTarget': array([26., 10.]), 'currentState': array([26.36122817,  9.38903704,  5.30493511]), 'targetState': array([26, 10], dtype=int32), 'currentDistance': 0.7097616000169673}
episode index:1510
target Thresh 31.999992914128235
target distance 11.0
model initialize at round 1510
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 21.]), 'previousTarget': array([27., 21.]), 'currentState': array([17.41602452, 10.0904602 ,  0.43904322]), 'targetState': array([27, 21], dtype=int32), 'currentDistance': 14.52138576929239}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9240792424361852
{'scaleFactor': 20, 'currentTarget': array([27., 21.]), 'previousTarget': array([27., 21.]), 'currentState': array([26.32258941, 20.49032451,  0.69359627]), 'targetState': array([27, 21], dtype=int32), 'currentDistance': 0.8477347543262495}
episode index:1511
target Thresh 31.999992984633835
target distance 23.0
model initialize at round 1511
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([12.60887032, 28.50611298,  4.97533948]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 22.514347528508953}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9240543122968201
{'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([11.60083314,  5.14131738,  4.42883185]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 0.9469266180793613}
episode index:1512
target Thresh 31.999993054437894
target distance 7.0
model initialize at round 1512
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  6.]), 'previousTarget': array([13.,  6.]), 'currentState': array([ 7.34178007, 12.71243307,  4.32330394]), 'targetState': array([13,  6], dtype=int32), 'currentDistance': 8.779078002746079}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9240721151637091
{'scaleFactor': 20, 'currentTarget': array([13.,  6.]), 'previousTarget': array([13.,  6.]), 'currentState': array([12.34945106,  5.93390823,  0.08826953]), 'targetState': array([13,  6], dtype=int32), 'currentDistance': 0.6538975772539845}
episode index:1513
target Thresh 31.999993123547394
target distance 14.0
model initialize at round 1513
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 25.]), 'previousTarget': array([26., 25.]), 'currentState': array([18.47382   , 10.18744681,  0.50614041]), 'targetState': array([26, 25], dtype=int32), 'currentDistance': 16.614906482297545}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9240651436526919
{'scaleFactor': 20, 'currentTarget': array([26., 25.]), 'previousTarget': array([26., 25.]), 'currentState': array([26.15531107, 25.05522293,  0.83163819]), 'targetState': array([26, 25], dtype=int32), 'currentDistance': 0.16483658652558192}
episode index:1514
target Thresh 31.99999319196924
target distance 6.0
model initialize at round 1514
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 17.]), 'previousTarget': array([12., 17.]), 'currentState': array([ 9.99592561, 21.26651654,  5.18462805]), 'targetState': array([12, 17], dtype=int32), 'currentDistance': 4.713754087120972}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9241021303565515
{'scaleFactor': 20, 'currentTarget': array([12., 17.]), 'previousTarget': array([12., 17.]), 'currentState': array([11.70179434, 17.64902768,  5.17326054]), 'targetState': array([12, 17], dtype=int32), 'currentDistance': 0.7142573387513691}
episode index:1515
target Thresh 31.999993259710276
target distance 16.0
model initialize at round 1515
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([10.57371538, 18.59649643,  5.6354906 ]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 14.514354578159065}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9241012349502661
{'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([24.04103465, 16.75153247,  5.13943237]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 0.9906314402630993}
episode index:1516
target Thresh 31.999993326777282
target distance 1.0
model initialize at round 1516
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 8.72178345, 10.18817397,  1.03366702]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.7459093748613457}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9241512670959812
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 8.72178345, 10.18817397,  1.03366702]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.7459093748613457}
episode index:1517
target Thresh 31.999993393176958
target distance 22.0
model initialize at round 1517
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  2.]), 'previousTarget': array([13.,  2.]), 'currentState': array([25.3754785 , 24.43967942,  3.88727355]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 25.62599618578861}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9241205488851135
{'scaleFactor': 20, 'currentTarget': array([13.,  2.]), 'previousTarget': array([13.,  2.]), 'currentState': array([13.19838804,  2.26734001,  3.94586257]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 0.33290913678947626}
episode index:1518
target Thresh 31.999993458915945
target distance 9.0
model initialize at round 1518
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([21.32218923, 11.86832193,  4.22991419]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 10.342767135470313}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9241382378258738
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.03070448,  3.39276   ,  4.22801411]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.3939583486644716}
episode index:1519
target Thresh 31.999993524000818
target distance 18.0
model initialize at round 1519
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 20.]), 'previousTarget': array([25., 20.]), 'currentState': array([25.51300606,  3.60287644,  2.27104494]), 'targetState': array([25, 20], dtype=int32), 'currentDistance': 16.40514664367817}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9241373210210067
{'scaleFactor': 20, 'currentTarget': array([25., 20.]), 'previousTarget': array([25., 20.]), 'currentState': array([25.15176385, 19.42411736,  1.62897879]), 'targetState': array([25, 20], dtype=int32), 'currentDistance': 0.5955443524231261}
episode index:1520
target Thresh 31.999993588438087
target distance 24.0
model initialize at round 1520
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 22.]), 'previousTarget': array([ 3., 22.]), 'currentState': array([27.21182891, 28.66958576,  2.45459548]), 'targetState': array([ 3, 22], dtype=int32), 'currentDistance': 25.11366228741812}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.9240951915230581
{'scaleFactor': 20, 'currentTarget': array([ 3., 22.]), 'previousTarget': array([ 3., 22.]), 'currentState': array([ 2.97562948, 22.18183583,  3.04363444]), 'targetState': array([ 3, 22], dtype=int32), 'currentDistance': 0.18346169163207493}
episode index:1521
target Thresh 31.999993652234195
target distance 7.0
model initialize at round 1521
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 11.]), 'previousTarget': array([20., 11.]), 'currentState': array([20.6736186 , 17.82283096,  5.16771824]), 'targetState': array([20, 11], dtype=int32), 'currentDistance': 6.856003522540107}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9241255488216632
{'scaleFactor': 20, 'currentTarget': array([20., 11.]), 'previousTarget': array([20., 11.]), 'currentState': array([20.2056573 , 11.98488282,  4.46826735]), 'targetState': array([20, 11], dtype=int32), 'currentDistance': 1.0061257838832662}
episode index:1522
target Thresh 31.99999371539552
target distance 21.0
model initialize at round 1522
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 28.]), 'previousTarget': array([ 3., 28.]), 'currentState': array([22.32257619,  7.13651908,  2.05038548]), 'targetState': array([ 3, 28], dtype=int32), 'currentDistance': 28.436715470586154}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.924083482377684
{'scaleFactor': 20, 'currentTarget': array([ 3., 28.]), 'previousTarget': array([ 3., 28.]), 'currentState': array([ 2.75785986, 28.60627485,  2.31802837]), 'targetState': array([ 3, 28], dtype=int32), 'currentDistance': 0.652840741982771}
episode index:1523
target Thresh 31.999993777928378
target distance 14.0
model initialize at round 1523
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([ 9.92161713, 10.55339083,  0.31001109]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 12.32368570159723}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9240948975135261
{'scaleFactor': 20, 'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([21.69378953, 12.73035987,  0.36132565]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 0.40800815235597004}
episode index:1524
target Thresh 31.999993839839025
target distance 12.0
model initialize at round 1524
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  4.]), 'previousTarget': array([23.,  4.]), 'currentState': array([12.86975444,  5.63288933,  0.04353702]), 'targetState': array([23,  4], dtype=int32), 'currentDistance': 10.261003983323704}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9241125336790255
{'scaleFactor': 20, 'currentTarget': array([23.,  4.]), 'previousTarget': array([23.,  4.]), 'currentState': array([22.65140004,  3.79137832,  6.25386073]), 'targetState': array([23,  4], dtype=int32), 'currentDistance': 0.406257228445704}
episode index:1525
target Thresh 31.999993901133653
target distance 15.0
model initialize at round 1525
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([12.95942371, 23.76619906,  0.16755485]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 13.330734679986044}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9241177452217698
{'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([26.3479203 , 20.63189056,  6.24663955]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 0.506510708565943}
episode index:1526
target Thresh 31.999993961818387
target distance 21.0
model initialize at round 1526
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 24.]), 'previousTarget': array([27., 24.]), 'currentState': array([11.77673381,  4.75065307,  0.64055359]), 'targetState': array([27, 24], dtype=int32), 'currentDistance': 24.541499361352315}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9240930347610588
{'scaleFactor': 20, 'currentTarget': array([27., 24.]), 'previousTarget': array([27., 24.]), 'currentState': array([26.17157797, 23.02107264,  0.57500153]), 'targetState': array([27, 24], dtype=int32), 'currentDistance': 1.2824125025327526}
episode index:1527
target Thresh 31.999994021899298
target distance 4.0
model initialize at round 1527
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  3.]), 'previousTarget': array([21.,  3.]), 'currentState': array([18.84026026,  3.01399408,  0.7153396 ]), 'targetState': array([21,  3], dtype=int32), 'currentDistance': 2.159785073236856}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9241361675917125
{'scaleFactor': 20, 'currentTarget': array([21.,  3.]), 'previousTarget': array([21.,  3.]), 'currentState': array([20.47685329,  2.58142104,  5.03136647]), 'targetState': array([21,  3], dtype=int32), 'currentDistance': 0.6699931492874468}
episode index:1528
target Thresh 31.999994081382393
target distance 15.0
model initialize at round 1528
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 25.]), 'previousTarget': array([ 4., 25.]), 'currentState': array([16.29814499, 11.04692293,  2.51360016]), 'targetState': array([ 4, 25], dtype=int32), 'currentDistance': 18.59926691741947}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9241292225818315
{'scaleFactor': 20, 'currentTarget': array([ 4., 25.]), 'previousTarget': array([ 4., 25.]), 'currentState': array([ 4.55595217, 24.40715214,  2.2320455 ]), 'targetState': array([ 4, 25], dtype=int32), 'currentDistance': 0.8127431310445438}
episode index:1529
target Thresh 31.99999414027362
target distance 13.0
model initialize at round 1529
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([20.26456291, 22.48622314,  3.2501421 ]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 13.43570421778334}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9241344095918479
{'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([ 7.68808807, 16.85660996,  3.59923279]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 0.34329252165092533}
episode index:1530
target Thresh 31.99999419857887
target distance 18.0
model initialize at round 1530
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([12.54954899,  3.6567237 ,  1.41086739]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 17.403822592580642}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9241274748027505
{'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([13.97835382, 21.41810078,  1.44458556]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 0.41866073993777136}
episode index:1531
target Thresh 31.999994256303978
target distance 25.0
model initialize at round 1531
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 28.]), 'previousTarget': array([14., 28.]), 'currentState': array([4.43922703, 4.79430342, 2.02247179]), 'targetState': array([14, 28], dtype=int32), 'currentDistance': 25.098062343881278}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9240970528368212
{'scaleFactor': 20, 'currentTarget': array([14., 28.]), 'previousTarget': array([14., 28.]), 'currentState': array([14.2131741 , 28.16516185,  1.03182737]), 'targetState': array([14, 28], dtype=int32), 'currentDistance': 0.26966949406517543}
episode index:1532
target Thresh 31.999994313454707
target distance 20.0
model initialize at round 1532
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  2.]), 'previousTarget': array([19.,  2.]), 'currentState': array([22.97319067, 23.33343567,  3.23700511]), 'targetState': array([19,  2], dtype=int32), 'currentDistance': 21.700270076853897}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.924078293020397
{'scaleFactor': 20, 'currentTarget': array([19.,  2.]), 'previousTarget': array([19.,  2.]), 'currentState': array([18.84137954,  2.529378  ,  4.50283816]), 'targetState': array([19,  2], dtype=int32), 'currentDistance': 0.5526314484496205}
episode index:1533
target Thresh 31.999994370036777
target distance 7.0
model initialize at round 1533
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 16.]), 'previousTarget': array([ 2., 16.]), 'currentState': array([ 7.22786964, 14.84583701,  3.03077281]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 5.353756923686862}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9241084238593668
{'scaleFactor': 20, 'currentTarget': array([ 2., 16.]), 'previousTarget': array([ 2., 16.]), 'currentState': array([ 1.47397432, 16.47848262,  3.01630793]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 0.7110897571444544}
episode index:1534
target Thresh 31.99999442605585
target distance 7.0
model initialize at round 1534
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 24.]), 'previousTarget': array([ 7., 24.]), 'currentState': array([ 4.01521468, 18.72072556,  1.11817567]), 'targetState': array([ 7, 24], dtype=int32), 'currentDistance': 6.064625460501477}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9241385154399144
{'scaleFactor': 20, 'currentTarget': array([ 7., 24.]), 'previousTarget': array([ 7., 24.]), 'currentState': array([ 6.95993367, 23.94584143,  1.08482695]), 'targetState': array([ 7, 24], dtype=int32), 'currentDistance': 0.0673681014702161}
episode index:1535
target Thresh 31.99999448151752
target distance 11.0
model initialize at round 1535
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([ 9.38528169, 13.09693643,  4.41892623]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 10.571447924648226}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9241559969076619
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.28881339, 4.56815266, 4.17875401]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.63734654561754}
episode index:1536
target Thresh 31.999994536427337
target distance 1.0
model initialize at round 1536
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 17.]), 'previousTarget': array([26., 17.]), 'currentState': array([26.54278985, 17.67423522,  5.6863944 ]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 0.8655714586216189}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9242053423878781
{'scaleFactor': 20, 'currentTarget': array([26., 17.]), 'previousTarget': array([26., 17.]), 'currentState': array([26.54278985, 17.67423522,  5.6863944 ]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 0.8655714586216189}
episode index:1537
target Thresh 31.999994590790795
target distance 18.0
model initialize at round 1537
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'previousTarget': array([ 2., 17.]), 'currentState': array([18.86346839, 10.24124292,  3.3221854 ]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 18.167480932654584}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9241983930413864
{'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'previousTarget': array([ 2., 17.]), 'currentState': array([ 2.40001129, 16.73216191,  2.60475839]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 0.48140032568750574}
episode index:1538
target Thresh 31.999994644613324
target distance 5.0
model initialize at round 1538
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 16.]), 'previousTarget': array([ 7., 16.]), 'currentState': array([ 8.57042441, 19.37067009,  5.46118176]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 3.718554786765853}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9242347163727436
{'scaleFactor': 20, 'currentTarget': array([ 7., 16.]), 'previousTarget': array([ 7., 16.]), 'currentState': array([ 6.7884839 , 16.38650053,  4.37990022]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 0.4405924634746596}
episode index:1539
target Thresh 31.999994697900313
target distance 11.0
model initialize at round 1539
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 28.]), 'previousTarget': array([19., 28.]), 'currentState': array([23.14848133, 18.45165562,  3.11129308]), 'targetState': array([19, 28], dtype=int32), 'currentDistance': 10.410608903916295}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9242459147058788
{'scaleFactor': 20, 'currentTarget': array([19., 28.]), 'previousTarget': array([19., 28.]), 'currentState': array([19.3936682 , 27.8312101 ,  2.50822306]), 'targetState': array([19, 28], dtype=int32), 'currentDistance': 0.42832777160868013}
episode index:1540
target Thresh 31.999994750657088
target distance 2.0
model initialize at round 1540
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 25.]), 'previousTarget': array([ 2., 25.]), 'currentState': array([ 0.53701153, 28.01146791,  3.37414467]), 'targetState': array([ 2, 25], dtype=int32), 'currentDistance': 3.3480254255770223}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9242821600564916
{'scaleFactor': 20, 'currentTarget': array([ 2., 25.]), 'previousTarget': array([ 2., 25.]), 'currentState': array([ 1.46321686, 25.12663679,  5.76611039]), 'targetState': array([ 2, 25], dtype=int32), 'currentDistance': 0.5515188318574765}
episode index:1541
target Thresh 31.99999480288892
target distance 11.0
model initialize at round 1541
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([13.32710805, 15.18390376,  4.04210067]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 11.771569080462564}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9242872075194295
{'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([16.52473088,  3.77627756,  5.18538344]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 0.5252927442341536}
episode index:1542
target Thresh 31.99999485460104
target distance 6.0
model initialize at round 1542
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  9.]), 'previousTarget': array([23.,  9.]), 'currentState': array([18.49469671,  6.96629564,  1.40891331]), 'targetState': array([23,  9], dtype=int32), 'currentDistance': 4.943046747597796}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9243170272164358
{'scaleFactor': 20, 'currentTarget': array([23.,  9.]), 'previousTarget': array([23.,  9.]), 'currentState': array([23.12462548,  9.02654706,  5.71095598]), 'targetState': array([23,  9], dtype=int32), 'currentDistance': 0.12742157578098692}
episode index:1543
target Thresh 31.999994905798616
target distance 15.0
model initialize at round 1543
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([17.49220728,  6.74762906,  1.67127585]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 15.273978469210498}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.924316008866184
{'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([11.85035639, 21.47046106,  1.77546553]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 0.493686967618897}
episode index:1544
target Thresh 31.999994956486766
target distance 20.0
model initialize at round 1544
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  2.]), 'previousTarget': array([26.,  2.]), 'currentState': array([17.49269287, 21.22266091,  4.79306567]), 'targetState': array([26,  2], dtype=int32), 'currentDistance': 21.021060079245576}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9242972530379591
{'scaleFactor': 20, 'currentTarget': array([26.,  2.]), 'previousTarget': array([26.,  2.]), 'currentState': array([26.22984088,  1.3947974 ,  4.92597553]), 'targetState': array([26,  2], dtype=int32), 'currentDistance': 0.647377032282264}
episode index:1545
target Thresh 31.999995006670563
target distance 5.0
model initialize at round 1545
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([ 8.39162048, 16.94645683,  1.60725277]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 5.426934217393045}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9243270083723459
{'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([12.13535127, 21.31179015,  0.48439258]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 0.33990155302901054}
episode index:1546
target Thresh 31.99999505635502
target distance 6.0
model initialize at round 1546
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([0.36144204, 8.61592202, 4.38183594]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 5.8500813913699}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9243567252382979
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.15128341, 3.06162145, 4.83777739]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.16335198888313313}
episode index:1547
target Thresh 31.999995105545107
target distance 19.0
model initialize at round 1547
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 14.]), 'previousTarget': array([26., 14.]), 'currentState': array([ 8.41393583, 13.9127834 ,  5.84643442]), 'targetState': array([26, 14], dtype=int32), 'currentDistance': 17.586280436469224}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9243497229916865
{'scaleFactor': 20, 'currentTarget': array([26., 14.]), 'previousTarget': array([26., 14.]), 'currentState': array([26.12829842, 14.18955148,  5.97499228]), 'targetState': array([26, 14], dtype=int32), 'currentDistance': 0.22888916374457258}
episode index:1548
target Thresh 31.99999515424575
target distance 9.0
model initialize at round 1548
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 20.]), 'previousTarget': array([23., 20.]), 'currentState': array([19.43016861, 11.60664444,  1.76283097]), 'targetState': array([23, 20], dtype=int32), 'currentDistance': 9.120971092426107}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9243669213951133
{'scaleFactor': 20, 'currentTarget': array([23., 20.]), 'previousTarget': array([23., 20.]), 'currentState': array([23.30994813, 20.58025085,  0.90910813]), 'targetState': array([23, 20], dtype=int32), 'currentDistance': 0.6578441260581324}
episode index:1549
target Thresh 31.999995202461808
target distance 17.0
model initialize at round 1549
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([ 7.66519594, 12.45406907,  6.1287303 ]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 16.5181197061875}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9243658747970699
{'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([23.34609874, 10.29840599,  5.97424475]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 0.7187718613361013}
episode index:1550
target Thresh 31.99999525019811
target distance 17.0
model initialize at round 1550
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([22.87986598, 18.56534884,  4.25252771]), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 22.443446629181732}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9243471593744146
{'scaleFactor': 20, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.90494186, 5.01427511, 3.57258302]), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.905054442150259}
episode index:1551
target Thresh 31.99999529745943
target distance 11.0
model initialize at round 1551
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 19.]), 'previousTarget': array([ 3., 19.]), 'currentState': array([12.94335088, 13.6900837 ,  3.02360392]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 11.272330627220812}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9243581986721121
{'scaleFactor': 20, 'currentTarget': array([ 3., 19.]), 'previousTarget': array([ 3., 19.]), 'currentState': array([ 2.52448576, 19.36959144,  2.51212049]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 0.6022554462475787}
episode index:1552
target Thresh 31.99999534425049
target distance 9.0
model initialize at round 1552
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 26.]), 'previousTarget': array([25., 26.]), 'currentState': array([20.6377221 , 16.61237337,  0.77759045]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 10.351666641732598}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9243753473206813
{'scaleFactor': 20, 'currentTarget': array([25., 26.]), 'previousTarget': array([25., 26.]), 'currentState': array([24.63806322, 25.7201643 ,  1.16428251]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 0.45750000252991185}
episode index:1553
target Thresh 31.99999539057597
target distance 9.0
model initialize at round 1553
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 14.]), 'previousTarget': array([27., 14.]), 'currentState': array([22.84136147,  6.45756611,  2.05728231]), 'targetState': array([27, 14], dtype=int32), 'currentDistance': 8.61293117494473}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9243924738989175
{'scaleFactor': 20, 'currentTarget': array([27., 14.]), 'previousTarget': array([27., 14.]), 'currentState': array([26.83641918, 13.87221238,  0.07778846]), 'targetState': array([27, 14], dtype=int32), 'currentDistance': 0.2075773605171588}
episode index:1554
target Thresh 31.999995436440507
target distance 23.0
model initialize at round 1554
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 5.]), 'previousTarget': array([6., 5.]), 'currentState': array([22.66988564, 26.45609543,  4.11176586]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 27.170740118226583}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9243566882647505
{'scaleFactor': 20, 'currentTarget': array([6., 5.]), 'previousTarget': array([6., 5.]), 'currentState': array([6.00799808, 5.04678654, 3.74640885]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 0.047465248290696156}
episode index:1555
target Thresh 31.999995481848682
target distance 18.0
model initialize at round 1555
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 29.]), 'previousTarget': array([ 3., 29.]), 'currentState': array([19.3341872 , 12.01093683,  2.99442512]), 'targetState': array([ 3, 29], dtype=int32), 'currentDistance': 23.56764601812163}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.924332284783678
{'scaleFactor': 20, 'currentTarget': array([ 3., 29.]), 'previousTarget': array([ 3., 29.]), 'currentState': array([ 3.09651512, 28.85761158,  2.18419803]), 'targetState': array([ 3, 29], dtype=int32), 'currentDistance': 0.17201636681782845}
episode index:1556
target Thresh 31.99999552680504
target distance 14.0
model initialize at round 1556
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([1.87847425, 8.67857665, 0.63306832]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 14.231430084667402}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9243372514266601
{'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([ 8.68124657, 20.65424443,  1.00674305]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 0.4702665897796426}
episode index:1557
target Thresh 31.999995571314074
target distance 25.0
model initialize at round 1557
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  4.]), 'previousTarget': array([19.,  4.]), 'currentState': array([15.62732272, 28.57080441,  5.01531411]), 'targetState': array([19,  4], dtype=int32), 'currentDistance': 24.80119717338207}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9243072024995563
{'scaleFactor': 20, 'currentTarget': array([19.,  4.]), 'previousTarget': array([19.,  4.]), 'currentState': array([18.90933519,  3.66056123,  4.53772428]), 'targetState': array([19,  4], dtype=int32), 'currentDistance': 0.3513385662190715}
episode index:1558
target Thresh 31.999995615380236
target distance 6.0
model initialize at round 1558
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 15.]), 'previousTarget': array([10., 15.]), 'currentState': array([9.67793961, 8.86997409, 0.93266314]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 6.138480317188552}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9243367033318209
{'scaleFactor': 20, 'currentTarget': array([10., 15.]), 'previousTarget': array([10., 15.]), 'currentState': array([ 9.72272751, 14.72281679,  1.446612  ]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 0.39205938731509204}
episode index:1559
target Thresh 31.99999565900793
target distance 21.0
model initialize at round 1559
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 26.]), 'previousTarget': array([25., 26.]), 'currentState': array([8.88521763, 3.56864476, 6.27626228]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 27.61977386974209}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.9242845280049313
{'scaleFactor': 20, 'currentTarget': array([25., 26.]), 'previousTarget': array([25., 26.]), 'currentState': array([25.9487589 , 25.49370094,  6.07489207]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 1.0753986200896128}
episode index:1560
target Thresh 31.999995702201524
target distance 19.0
model initialize at round 1560
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 27.]), 'previousTarget': array([21., 27.]), 'currentState': array([16.82835201,  9.46499864,  2.06618502]), 'targetState': array([21, 27], dtype=int32), 'currentDistance': 18.024397894664453}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9242717781951963
{'scaleFactor': 20, 'currentTarget': array([21., 27.]), 'previousTarget': array([21., 27.]), 'currentState': array([21.78681971, 26.7881108 ,  5.68511165]), 'targetState': array([21, 27], dtype=int32), 'currentDistance': 0.8148510833610163}
episode index:1561
target Thresh 31.999995744965332
target distance 17.0
model initialize at round 1561
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([ 5.51963361, 24.27674225,  0.5657832 ]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 21.058658854621378}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9242418481342513
{'scaleFactor': 20, 'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([21.67271759, 10.2061346 ,  4.35740519]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 0.7035910988700652}
episode index:1562
target Thresh 31.999995787303636
target distance 16.0
model initialize at round 1562
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 18.]), 'previousTarget': array([11., 18.]), 'currentState': array([26.51484984,  1.26677559,  0.55921429]), 'targetState': array([11, 18], dtype=int32), 'currentDistance': 22.81910087778448}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9242176274199722
{'scaleFactor': 20, 'currentTarget': array([11., 18.]), 'previousTarget': array([11., 18.]), 'currentState': array([10.82778462, 17.93421637,  2.03392144]), 'targetState': array([11, 18], dtype=int32), 'currentDistance': 0.18435189967113425}
episode index:1563
target Thresh 31.999995829220666
target distance 7.0
model initialize at round 1563
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  6.]), 'previousTarget': array([13.,  6.]), 'currentState': array([18.3269123 ,  3.81788569,  2.24001503]), 'targetState': array([13,  6], dtype=int32), 'currentDistance': 5.756528250680445}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9242470912131819
{'scaleFactor': 20, 'currentTarget': array([13.,  6.]), 'previousTarget': array([13.,  6.]), 'currentState': array([13.66684897,  6.14501337,  3.59698725]), 'targetState': array([13,  6], dtype=int32), 'currentDistance': 0.6824341894689665}
episode index:1564
target Thresh 31.99999587072061
target distance 9.0
model initialize at round 1564
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([13.76808036, 18.50252186,  4.1763159 ]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 10.755840131776587}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9242641793656975
{'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([ 4.92273904, 14.25753541,  3.43562057]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 0.9580040812522072}
episode index:1565
target Thresh 31.999995911807627
target distance 12.0
model initialize at round 1565
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([15.5068964 ,  2.22345012,  2.61118364]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 12.50265972231191}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9242751729608669
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([5.29015872, 8.48195721, 2.4254539 ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 0.5937679806803363}
episode index:1566
target Thresh 31.999995952485822
target distance 7.0
model initialize at round 1566
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 26.]), 'previousTarget': array([11., 26.]), 'currentState': array([ 5.91264536, 26.64920163,  0.23229974]), 'targetState': array([11, 26], dtype=int32), 'currentDistance': 5.128609950794869}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.924304543622666
{'scaleFactor': 20, 'currentTarget': array([11., 26.]), 'previousTarget': array([11., 26.]), 'currentState': array([11.73416487, 25.82852638,  5.69246344]), 'targetState': array([11, 26], dtype=int32), 'currentDistance': 0.753923907153824}
episode index:1567
target Thresh 31.99999599275926
target distance 12.0
model initialize at round 1567
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 0.726897  , 23.10072559,  3.43868303]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 13.503414914805402}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9243094931151943
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.31436049, 10.30612258,  4.95440478]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.4387864494188596}
episode index:1568
target Thresh 31.999996032631973
target distance 14.0
model initialize at round 1568
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 28.]), 'previousTarget': array([21., 28.]), 'currentState': array([22.35691795, 15.64468771,  2.36709729]), 'targetState': array([21, 28], dtype=int32), 'currentDistance': 12.429600481915246}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9243204368094491
{'scaleFactor': 20, 'currentTarget': array([21., 28.]), 'previousTarget': array([21., 28.]), 'currentState': array([20.63748934, 27.35590258,  1.63147071]), 'targetState': array([21, 28], dtype=int32), 'currentDistance': 0.7391045041674207}
episode index:1569
target Thresh 31.999996072107944
target distance 13.0
model initialize at round 1569
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 26.]), 'previousTarget': array([25., 26.]), 'currentState': array([13.21588672, 14.66906591,  0.43216467]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 16.347947671412207}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9243194331518811
{'scaleFactor': 20, 'currentTarget': array([25., 26.]), 'previousTarget': array([25., 26.]), 'currentState': array([24.19253669, 25.89286405,  0.55762091]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 0.8145398119937408}
episode index:1570
target Thresh 31.999996111191123
target distance 13.0
model initialize at round 1570
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 29.]), 'previousTarget': array([12., 29.]), 'currentState': array([ 4.64499862, 16.35548224,  1.22282618]), 'targetState': array([12, 29], dtype=int32), 'currentDistance': 14.628050956117539}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9243243637150607
{'scaleFactor': 20, 'currentTarget': array([12., 29.]), 'previousTarget': array([12., 29.]), 'currentState': array([11.50764205, 28.5128876 ,  1.02575176]), 'targetState': array([12, 29], dtype=int32), 'currentDistance': 0.6926000622032447}
episode index:1571
target Thresh 31.99999614988542
target distance 7.0
model initialize at round 1571
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 22.]), 'previousTarget': array([27., 22.]), 'currentState': array([21.67966727, 23.89460156,  0.94733113]), 'targetState': array([27, 22], dtype=int32), 'currentDistance': 5.647606163868286}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9243474372813998
{'scaleFactor': 20, 'currentTarget': array([27., 22.]), 'previousTarget': array([27., 22.]), 'currentState': array([27.04852442, 21.08948541,  4.77322927]), 'targetState': array([27, 22], dtype=int32), 'currentDistance': 0.9118066866007576}
episode index:1572
target Thresh 31.999996188194697
target distance 4.0
model initialize at round 1572
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 11.]), 'previousTarget': array([19., 11.]), 'currentState': array([16.01676073, 12.34111361,  6.19528705]), 'targetState': array([19, 11], dtype=int32), 'currentDistance': 3.2708259286517127}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9243828807414879
{'scaleFactor': 20, 'currentTarget': array([19., 11.]), 'previousTarget': array([19., 11.]), 'currentState': array([19.30868131, 10.42847582,  5.12437049]), 'targetState': array([19, 11], dtype=int32), 'currentDistance': 0.6495567978548338}
episode index:1573
target Thresh 31.999996226122793
target distance 20.0
model initialize at round 1573
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 8.]), 'previousTarget': array([6., 8.]), 'currentState': array([24.24662419, 12.10623078,  3.92655385]), 'targetState': array([6, 8], dtype=int32), 'currentDistance': 18.702952322869113}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9243701737492815
{'scaleFactor': 20, 'currentTarget': array([6., 8.]), 'previousTarget': array([6., 8.]), 'currentState': array([5.14776294, 8.20279417, 2.73058003]), 'targetState': array([6, 8], dtype=int32), 'currentDistance': 0.8760328049763876}
episode index:1574
target Thresh 31.9999962636735
target distance 16.0
model initialize at round 1574
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 18.]), 'previousTarget': array([10., 18.]), 'currentState': array([25.31762885,  8.53842702,  2.99827451]), 'targetState': array([10, 18], dtype=int32), 'currentDistance': 18.004197198575056}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9243632830024462
{'scaleFactor': 20, 'currentTarget': array([10., 18.]), 'previousTarget': array([10., 18.]), 'currentState': array([ 9.94423049, 17.58931555,  2.437686  ]), 'targetState': array([10, 18], dtype=int32), 'currentDistance': 0.4144538071287143}
episode index:1575
target Thresh 31.99999630085057
target distance 4.0
model initialize at round 1575
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 26.]), 'previousTarget': array([19., 26.]), 'currentState': array([13.08366795, 21.44286879,  3.309448  ]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 7.467960212175972}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.924380178159107
{'scaleFactor': 20, 'currentTarget': array([19., 26.]), 'previousTarget': array([19., 26.]), 'currentState': array([18.72575973, 26.20261673,  0.16712831]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 0.3409710584175689}
episode index:1576
target Thresh 31.999996337657723
target distance 16.0
model initialize at round 1576
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 12.]), 'previousTarget': array([ 4., 12.]), 'currentState': array([18.03526792, 10.97751975,  3.4818486 ]), 'targetState': array([ 4, 12], dtype=int32), 'currentDistance': 14.07246288222461}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.924385051443665
{'scaleFactor': 20, 'currentTarget': array([ 4., 12.]), 'previousTarget': array([ 4., 12.]), 'currentState': array([ 4.19614423, 11.7651703 ,  2.97800153]), 'targetState': array([ 4, 12], dtype=int32), 'currentDistance': 0.3059698494640791}
episode index:1577
target Thresh 31.999996374098636
target distance 8.0
model initialize at round 1577
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([21.34649693,  4.31355331,  1.94418812]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 7.339476307058876}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9244079988191759
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.12290406,  7.57238242,  2.19329733]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.4449294387300023}
episode index:1578
target Thresh 31.99999641017696
target distance 19.0
model initialize at round 1578
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  8.]), 'previousTarget': array([25.,  8.]), 'currentState': array([11.92844195, 25.7022514 ,  5.03216934]), 'targetState': array([25,  8], dtype=int32), 'currentDistance': 22.005347866633166}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9243895885946284
{'scaleFactor': 20, 'currentTarget': array([25.,  8.]), 'previousTarget': array([25.,  8.]), 'currentState': array([24.95588579,  8.59877498,  5.09309058]), 'targetState': array([25,  8], dtype=int32), 'currentDistance': 0.6003978208999464}
episode index:1579
target Thresh 31.999996445896294
target distance 2.0
model initialize at round 1579
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 20.]), 'previousTarget': array([15., 20.]), 'currentState': array([15.19886737, 19.77462026,  2.38732553]), 'targetState': array([15, 20], dtype=int32), 'currentDistance': 0.30057321257940484}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9244374432853913
{'scaleFactor': 20, 'currentTarget': array([15., 20.]), 'previousTarget': array([15., 20.]), 'currentState': array([15.19886737, 19.77462026,  2.38732553]), 'targetState': array([15, 20], dtype=int32), 'currentDistance': 0.30057321257940484}
episode index:1580
target Thresh 31.999996481260215
target distance 7.0
model initialize at round 1580
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 16.]), 'previousTarget': array([19., 16.]), 'currentState': array([24.24248602, 13.7527149 ,  3.25964379]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 5.703853955913278}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9244664512276524
{'scaleFactor': 20, 'currentTarget': array([19., 16.]), 'previousTarget': array([19., 16.]), 'currentState': array([18.9106898 , 15.94607524,  2.12263511]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 0.104327329252068}
episode index:1581
target Thresh 31.99999651627226
target distance 20.0
model initialize at round 1581
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  9.]), 'previousTarget': array([17.,  9.]), 'currentState': array([ 8.40983009, 27.59185966,  5.34787378]), 'targetState': array([17,  9], dtype=int32), 'currentDistance': 20.480436146593547}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9244537556674634
{'scaleFactor': 20, 'currentTarget': array([17.,  9.]), 'previousTarget': array([17.,  9.]), 'currentState': array([17.15611195,  9.789357  ,  5.00471914]), 'targetState': array([17,  9], dtype=int32), 'currentDistance': 0.8046461416180588}
episode index:1582
target Thresh 31.999996550935933
target distance 8.0
model initialize at round 1582
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 13.]), 'previousTarget': array([ 4., 13.]), 'currentState': array([3.91107156, 3.58496053, 0.01124256]), 'targetState': array([ 4, 13], dtype=int32), 'currentDistance': 9.415459437931055}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9244705189613562
{'scaleFactor': 20, 'currentTarget': array([ 4., 13.]), 'previousTarget': array([ 4., 13.]), 'currentState': array([ 3.75099395, 12.86345275,  1.42055943]), 'targetState': array([ 4, 13], dtype=int32), 'currentDistance': 0.2839879635162064}
episode index:1583
target Thresh 31.999996585254692
target distance 9.0
model initialize at round 1583
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 21.]), 'previousTarget': array([10., 21.]), 'currentState': array([17.37866099, 28.45127364,  3.88012958]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 10.486473040439325}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.924487261089474
{'scaleFactor': 20, 'currentTarget': array([10., 21.]), 'previousTarget': array([10., 21.]), 'currentState': array([10.46331043, 21.24926239,  3.8207123 ]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 0.5261067310215171}
episode index:1584
target Thresh 31.999996619231975
target distance 17.0
model initialize at round 1584
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 27.]), 'previousTarget': array([20., 27.]), 'currentState': array([13.31193026, 11.79553316,  0.6514101 ]), 'targetState': array([20, 27], dtype=int32), 'currentDistance': 16.610421083918762}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9244861616783311
{'scaleFactor': 20, 'currentTarget': array([20., 27.]), 'previousTarget': array([20., 27.]), 'currentState': array([19.36449468, 26.33140149,  1.01226127]), 'targetState': array([20, 27], dtype=int32), 'currentDistance': 0.9224375262365948}
episode index:1585
target Thresh 31.999996652871182
target distance 17.0
model initialize at round 1585
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 4.]), 'previousTarget': array([8., 4.]), 'currentState': array([23.65047979, 12.5246393 ,  4.01165532]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 17.821531722895607}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.924479245591197
{'scaleFactor': 20, 'currentTarget': array([8., 4.]), 'previousTarget': array([8., 4.]), 'currentState': array([8.18856602, 4.07766853, 3.34932365]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 0.2039351481513834}
episode index:1586
target Thresh 31.99999668617567
target distance 4.0
model initialize at round 1586
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 4.78925755, 12.40917269,  4.11263309]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 3.0009257947372108}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9245205315107993
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.64753725, 10.76710329,  4.09686661]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.0038684929165864}
episode index:1587
target Thresh 31.999996719148772
target distance 4.0
model initialize at round 1587
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.1267546 ,  6.68105641,  0.05617791]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 2.819684561725742}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9245555311760947
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.75891386,  3.37442649,  4.7160117 ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.6704213141102665}
episode index:1588
target Thresh 31.999996751793788
target distance 8.0
model initialize at round 1588
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([25.42129291, 11.41684306,  4.50543499]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 6.575746872445393}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9245782124088348
{'scaleFactor': 20, 'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([18.82943213, 10.15912429,  3.21320337]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 0.23326794647848842}
episode index:1589
target Thresh 31.99999678411398
target distance 4.0
model initialize at round 1589
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 14.]), 'previousTarget': array([20., 14.]), 'currentState': array([17.60419034, 13.75049403,  1.28969043]), 'targetState': array([20, 14], dtype=int32), 'currentDistance': 2.408766729572332}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.924619358187194
{'scaleFactor': 20, 'currentTarget': array([20., 14.]), 'previousTarget': array([20., 14.]), 'currentState': array([19.21214856, 14.24738327,  5.57291567]), 'targetState': array([20, 14], dtype=int32), 'currentDistance': 0.8257774315103964}
episode index:1590
target Thresh 31.999996816112578
target distance 15.0
model initialize at round 1590
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  9.]), 'previousTarget': array([17.,  9.]), 'currentState': array([19.0796215 , 22.31891449,  3.74971676]), 'targetState': array([17,  9], dtype=int32), 'currentDistance': 13.48029334618265}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9246181798944477
{'scaleFactor': 20, 'currentTarget': array([17.,  9.]), 'previousTarget': array([17.,  9.]), 'currentState': array([16.79011985,  9.45388187,  2.86931694]), 'targetState': array([17,  9], dtype=int32), 'currentDistance': 0.5000584210570174}
episode index:1591
target Thresh 31.99999684779279
target distance 13.0
model initialize at round 1591
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([15.7745069 ,  7.66779523,  2.7151857 ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 12.791949692572075}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9246287715838363
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.9376657 , 6.60241774, 3.19290102]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.0184736691747907}
episode index:1592
target Thresh 31.999996879157774
target distance 11.0
model initialize at round 1592
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([17.85288232, 19.76853228,  4.97243643]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 11.417322572700892}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9246393499754353
{'scaleFactor': 20, 'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([ 8.92656102, 13.93823335,  3.43690755]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 0.9286174868402793}
episode index:1593
target Thresh 31.999996910210672
target distance 23.0
model initialize at round 1593
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([ 4.65545679, 16.44991468,  6.12243891]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 22.624803140205298}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9246153509301032
{'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([27.15044793, 19.20436344,  4.73297196]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 0.8097358327312544}
episode index:1594
target Thresh 31.999996940954592
target distance 14.0
model initialize at round 1594
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 19.]), 'previousTarget': array([12., 19.]), 'currentState': array([24.32180643, 14.87329393,  2.20695114]), 'targetState': array([12, 19], dtype=int32), 'currentDistance': 12.994484087815225}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9246200217746027
{'scaleFactor': 20, 'currentTarget': array([12., 19.]), 'previousTarget': array([12., 19.]), 'currentState': array([12.02578262, 18.85756875,  2.54514393]), 'targetState': array([12, 19], dtype=int32), 'currentDistance': 0.1447459979841971}
episode index:1595
target Thresh 31.9999969713926
target distance 9.0
model initialize at round 1595
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 21.]), 'previousTarget': array([22., 21.]), 'currentState': array([13.66333414, 10.45326928,  6.12752628]), 'targetState': array([22, 21], dtype=int32), 'currentDistance': 13.443716989475123}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9246246867659137
{'scaleFactor': 20, 'currentTarget': array([22., 21.]), 'previousTarget': array([22., 21.]), 'currentState': array([21.94074002, 20.96247162,  0.68105139]), 'targetState': array([22, 21], dtype=int32), 'currentDistance': 0.0701436011248324}
episode index:1596
target Thresh 31.99999700152775
target distance 8.0
model initialize at round 1596
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 12.]), 'previousTarget': array([11., 12.]), 'currentState': array([7.6666536 , 4.23378165, 1.14936083]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 8.45135170384052}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9246472110760164
{'scaleFactor': 20, 'currentTarget': array([11., 12.]), 'previousTarget': array([11., 12.]), 'currentState': array([10.8150329 , 11.58690817,  1.11700857]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 0.4526120679071035}
episode index:1597
target Thresh 31.999997031363044
target distance 15.0
model initialize at round 1597
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 15.]), 'previousTarget': array([25., 15.]), 'currentState': array([10.430717  , 18.3730793 ,  5.98119521]), 'targetState': array([25, 15], dtype=int32), 'currentDistance': 14.954653827248144}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.92464602051491
{'scaleFactor': 20, 'currentTarget': array([25., 15.]), 'previousTarget': array([25., 15.]), 'currentState': array([25.10055427, 14.02341552,  4.78577089]), 'targetState': array([25, 15], dtype=int32), 'currentDistance': 0.981747627751767}
episode index:1598
target Thresh 31.999997060901478
target distance 15.0
model initialize at round 1598
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 19.]), 'previousTarget': array([17., 19.]), 'currentState': array([16.96569993,  5.92932559,  2.05513865]), 'targetState': array([17, 19], dtype=int32), 'currentDistance': 13.070719416962213}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9246506604945172
{'scaleFactor': 20, 'currentTarget': array([17., 19.]), 'previousTarget': array([17., 19.]), 'currentState': array([17.61863479, 19.53573068,  0.76229069]), 'targetState': array([17, 19], dtype=int32), 'currentDistance': 0.8183620076490514}
episode index:1599
target Thresh 31.999997090145996
target distance 16.0
model initialize at round 1599
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  9.]), 'previousTarget': array([23.,  9.]), 'currentState': array([26.82136913, 23.00911847,  4.68147684]), 'targetState': array([23,  9], dtype=int32), 'currentDistance': 14.520959420317357}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.92465529467415
{'scaleFactor': 20, 'currentTarget': array([23.,  9.]), 'previousTarget': array([23.,  9.]), 'currentState': array([23.46387602,  9.51256723,  4.29034061]), 'targetState': array([23,  9], dtype=int32), 'currentDistance': 0.6913075493375167}
episode index:1600
target Thresh 31.99999711909953
target distance 18.0
model initialize at round 1600
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 22.]), 'previousTarget': array([ 9., 22.]), 'currentState': array([19.6048425 ,  4.94731894,  3.5493654 ]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 20.081250354401494}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.924642631826139
{'scaleFactor': 20, 'currentTarget': array([ 9., 22.]), 'previousTarget': array([ 9., 22.]), 'currentState': array([ 9.15997534, 21.0679837 ,  1.97833099]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 0.9456460674773679}
episode index:1601
target Thresh 31.999997147764965
target distance 21.0
model initialize at round 1601
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 6.]), 'previousTarget': array([6., 6.]), 'currentState': array([ 7.98538404, 25.31709338,  3.69370437]), 'targetState': array([6, 6], dtype=int32), 'currentDistance': 19.41885285812805}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9246243394556225
{'scaleFactor': 20, 'currentTarget': array([6., 6.]), 'previousTarget': array([6., 6.]), 'currentState': array([6.45782818, 6.95075985, 4.40960014]), 'targetState': array([6, 6], dtype=int32), 'currentDistance': 1.0552492252017103}
episode index:1602
target Thresh 31.99999717614518
target distance 16.0
model initialize at round 1602
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([18.66673121,  5.23322772,  1.14902848]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 17.388730239496557}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9246174105149039
{'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([26.33903263, 21.30110678,  0.67182398]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 0.4534406433590234}
episode index:1603
target Thresh 31.999997204243005
target distance 8.0
model initialize at round 1603
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 9.58027194, 19.57890299,  5.6243344 ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 10.147464151191516}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9246338523100317
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.07532055, 11.31156366,  5.07430545]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.3205387609599302}
episode index:1604
target Thresh 31.99999723206125
target distance 10.0
model initialize at round 1604
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 5.]), 'previousTarget': array([8., 5.]), 'currentState': array([12.36658399, 15.40538898,  3.90832317]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 11.284466117948757}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9246443484452909
{'scaleFactor': 20, 'currentTarget': array([8., 5.]), 'previousTarget': array([8., 5.]), 'currentState': array([7.72562667, 4.54862266, 3.75719441]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 0.5282255495301661}
episode index:1605
target Thresh 31.9999972596027
target distance 5.0
model initialize at round 1605
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([15.16939712,  8.6744231 ,  2.47997192]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 5.179639648001716}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9246727759991855
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([9.97471113, 8.74006215, 3.58463088]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.2611651056384723}
episode index:1606
target Thresh 31.99999728687011
target distance 3.0
model initialize at round 1606
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 17.]), 'previousTarget': array([12., 17.]), 'currentState': array([13.45470985, 18.33331674,  4.55889618]), 'targetState': array([12, 17], dtype=int32), 'currentDistance': 1.973300355399751}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9247134276631562
{'scaleFactor': 20, 'currentTarget': array([12., 17.]), 'previousTarget': array([12., 17.]), 'currentState': array([11.94385988, 17.48609343,  2.72811377]), 'targetState': array([12, 17], dtype=int32), 'currentDistance': 0.489324570159646}
episode index:1607
target Thresh 31.999997313866203
target distance 14.0
model initialize at round 1607
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([10.46474079, 22.82880788,  5.78810782]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 19.417164229552135}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9247007837871273
{'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([23.25967457,  7.69322475,  4.84964586]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 0.40192279406980724}
episode index:1608
target Thresh 31.99999734059368
target distance 14.0
model initialize at round 1608
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 26.]), 'previousTarget': array([11., 26.]), 'currentState': array([ 6.10689067, 13.26774647,  1.86303395]), 'targetState': array([11, 26], dtype=int32), 'currentDistance': 13.64011725506652}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.924705360893479
{'scaleFactor': 20, 'currentTarget': array([11., 26.]), 'previousTarget': array([11., 26.]), 'currentState': array([10.85837015, 26.23828998,  1.00611319]), 'targetState': array([11, 26], dtype=int32), 'currentDistance': 0.2772023250544668}
episode index:1609
target Thresh 31.999997367055215
target distance 11.0
model initialize at round 1609
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 12.]), 'previousTarget': array([12., 12.]), 'currentState': array([ 6.55367136, 22.13946438,  4.68831229]), 'targetState': array([12, 12], dtype=int32), 'currentDistance': 11.5096148339415}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9247157800167756
{'scaleFactor': 20, 'currentTarget': array([12., 12.]), 'previousTarget': array([12., 12.]), 'currentState': array([12.19727172, 11.69923359,  4.96747997]), 'targetState': array([12, 12], dtype=int32), 'currentDistance': 0.35968953516076196}
episode index:1610
target Thresh 31.999997393253455
target distance 21.0
model initialize at round 1610
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([ 3.3926982 , 25.50103205,  4.45259809]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 20.50479278357789}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9247031582259574
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.49776875, 5.73685819, 4.56597412]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.8892320942086512}
episode index:1611
target Thresh 31.999997419191015
target distance 7.0
model initialize at round 1611
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([13.06513042,  8.60056906,  5.1334393 ]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 8.057329722762534}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.924725424263038
{'scaleFactor': 20, 'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([20.24203172, 10.36976077,  0.12053005]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 0.8433498304745494}
episode index:1612
target Thresh 31.999997444870495
target distance 1.0
model initialize at round 1612
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 27.]), 'previousTarget': array([ 3., 27.]), 'currentState': array([ 3.57276606, 26.59975999,  1.37386125]), 'targetState': array([ 3, 27], dtype=int32), 'currentDistance': 0.6987510502995626}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9247720916999488
{'scaleFactor': 20, 'currentTarget': array([ 3., 27.]), 'previousTarget': array([ 3., 27.]), 'currentState': array([ 3.57276606, 26.59975999,  1.37386125]), 'targetState': array([ 3, 27], dtype=int32), 'currentDistance': 0.6987510502995626}
episode index:1613
target Thresh 31.999997470294456
target distance 23.0
model initialize at round 1613
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 25.]), 'previousTarget': array([20., 25.]), 'currentState': array([25.62402888,  1.55850449,  0.7445628 ]), 'targetState': array([20, 25], dtype=int32), 'currentDistance': 24.1067088728613}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.924748307796613
{'scaleFactor': 20, 'currentTarget': array([20., 25.]), 'previousTarget': array([20., 25.]), 'currentState': array([19.80976262, 24.07899633,  1.70536848]), 'targetState': array([20, 25], dtype=int32), 'currentDistance': 0.9404456533968062}
episode index:1614
target Thresh 31.999997495465447
target distance 11.0
model initialize at round 1614
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.66796912,  3.77579672,  0.87638396]), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 12.156178465666466}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9247586680700524
{'scaleFactor': 20, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.07406125, 14.21001964,  1.85706867]), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7934444189862053}
episode index:1615
target Thresh 31.999997520385985
target distance 17.0
model initialize at round 1615
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 17.]), 'previousTarget': array([23., 17.]), 'currentState': array([ 7.85181915, 24.39381259,  0.08216559]), 'targetState': array([23, 17], dtype=int32), 'currentDistance': 16.856329599928547}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.924757421799234
{'scaleFactor': 20, 'currentTarget': array([23., 17.]), 'previousTarget': array([23., 17.]), 'currentState': array([22.18380142, 17.63366265,  5.71280581]), 'targetState': array([23, 17], dtype=int32), 'currentDistance': 1.0332998030631706}
episode index:1616
target Thresh 31.999997545058555
target distance 24.0
model initialize at round 1616
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 26.]), 'previousTarget': array([ 5., 26.]), 'currentState': array([4.39513058, 3.63592784, 2.34380233]), 'targetState': array([ 5, 26], dtype=int32), 'currentDistance': 22.372250454637676}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9247392281272856
{'scaleFactor': 20, 'currentTarget': array([ 5., 26.]), 'previousTarget': array([ 5., 26.]), 'currentState': array([ 4.60991508, 25.12223136,  1.34384722]), 'targetState': array([ 5, 26], dtype=int32), 'currentDistance': 0.9605436168386631}
episode index:1617
target Thresh 31.999997569485632
target distance 12.0
model initialize at round 1617
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 25.]), 'previousTarget': array([15., 25.]), 'currentState': array([ 3.70800357, 15.47319974,  6.15659142]), 'targetState': array([15, 25], dtype=int32), 'currentDistance': 14.773933342681206}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9247322924161339
{'scaleFactor': 20, 'currentTarget': array([15., 25.]), 'previousTarget': array([15., 25.]), 'currentState': array([15.72978536, 25.13233882,  6.18218563]), 'targetState': array([15, 25], dtype=int32), 'currentDistance': 0.7416874209903433}
episode index:1618
target Thresh 31.999997593669654
target distance 8.0
model initialize at round 1618
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 16.]), 'previousTarget': array([ 9., 16.]), 'currentState': array([4.67501773, 7.83658733, 0.91274899]), 'targetState': array([ 9, 16], dtype=int32), 'currentDistance': 9.238331993224135}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9247544441873408
{'scaleFactor': 20, 'currentTarget': array([ 9., 16.]), 'previousTarget': array([ 9., 16.]), 'currentState': array([ 8.06854446, 15.0569596 ,  1.08790065]), 'targetState': array([ 9, 16], dtype=int32), 'currentDistance': 1.3254941028721527}
episode index:1619
target Thresh 31.99999761761304
target distance 14.0
model initialize at round 1619
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 13.]), 'previousTarget': array([ 2., 13.]), 'currentState': array([14.3291181 , 20.20134983,  4.0316658 ]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 14.278185897198227}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9247589570908713
{'scaleFactor': 20, 'currentTarget': array([ 2., 13.]), 'previousTarget': array([ 2., 13.]), 'currentState': array([ 2.39642101, 13.15457286,  3.48029022]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 0.42549075697778344}
episode index:1620
target Thresh 31.999997641318185
target distance 5.0
model initialize at round 1620
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 29.]), 'previousTarget': array([24., 29.]), 'currentState': array([20.65261529, 28.32288276,  5.46762795]), 'targetState': array([24, 29], dtype=int32), 'currentDistance': 3.41518258963169}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9247930971543563
{'scaleFactor': 20, 'currentTarget': array([24., 29.]), 'previousTarget': array([24., 29.]), 'currentState': array([23.9619272 , 28.94404661,  5.4721219 ]), 'targetState': array([24, 29], dtype=int32), 'currentDistance': 0.06767806036407933}
episode index:1621
target Thresh 31.99999766478746
target distance 9.0
model initialize at round 1621
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  6.]), 'previousTarget': array([11.,  6.]), 'currentState': array([19.76066327,  4.33413506,  3.55969429]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 8.917641331222978}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9248151704668381
{'scaleFactor': 20, 'currentTarget': array([11.,  6.]), 'previousTarget': array([11.,  6.]), 'currentState': array([11.98244142,  5.41393845,  2.83103255]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 1.1439664732029862}
episode index:1622
target Thresh 31.999997688023214
target distance 25.0
model initialize at round 1622
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'previousTarget': array([25.,  3.]), 'currentState': array([ 9.32940692, 28.20373228,  4.03023982]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 29.678200891083772}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.924764725625752
{'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'previousTarget': array([25.,  3.]), 'currentState': array([25.01337206,  2.4782545 ,  4.57703523]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 0.5219168278296765}
episode index:1623
target Thresh 31.99999771102777
target distance 16.0
model initialize at round 1623
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 29.]), 'previousTarget': array([12., 29.]), 'currentState': array([18.64900531, 12.66359315,  0.80875557]), 'targetState': array([12, 29], dtype=int32), 'currentDistance': 17.637671630243293}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9247577998387186
{'scaleFactor': 20, 'currentTarget': array([12., 29.]), 'previousTarget': array([12., 29.]), 'currentState': array([12.01014548, 28.69918942,  1.62495396]), 'targetState': array([12, 29], dtype=int32), 'currentDistance': 0.3009816175812245}
episode index:1624
target Thresh 31.999997733803422
target distance 10.0
model initialize at round 1624
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 14.]), 'previousTarget': array([24., 14.]), 'currentState': array([19.54946064, 22.7583607 ,  5.39726344]), 'targetState': array([24, 14], dtype=int32), 'currentDistance': 9.824264993429356}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9247739427618333
{'scaleFactor': 20, 'currentTarget': array([24., 14.]), 'previousTarget': array([24., 14.]), 'currentState': array([23.94321113, 13.82141717,  4.92335977]), 'targetState': array([24, 14], dtype=int32), 'currentDistance': 0.1873947737287306}
episode index:1625
target Thresh 31.999997756352457
target distance 8.0
model initialize at round 1625
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([23.2515789 , 22.50739973,  3.04163849]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 9.105868892651982}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9247900658289538
{'scaleFactor': 20, 'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([16.81817447, 17.12056079,  4.63504624]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 0.8270092870650361}
episode index:1626
target Thresh 31.999997778677123
target distance 8.0
model initialize at round 1626
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([10.40075446, 13.93288533,  1.59753245]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 9.016410617668798}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9248061690766927
{'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([16.58549405, 21.42156784,  0.55265303]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 0.721472604462943}
episode index:1627
target Thresh 31.999997800779653
target distance 6.0
model initialize at round 1627
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 14.]), 'previousTarget': array([20., 14.]), 'currentState': array([17.14920377, 18.32365686,  5.81116056]), 'targetState': array([20, 14], dtype=int32), 'currentDistance': 5.178904111489005}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9248341130760314
{'scaleFactor': 20, 'currentTarget': array([20., 14.]), 'previousTarget': array([20., 14.]), 'currentState': array([20.22660159, 13.37374422,  4.87457625]), 'targetState': array([20, 14], dtype=int32), 'currentDistance': 0.6659914288155359}
episode index:1628
target Thresh 31.999997822662262
target distance 14.0
model initialize at round 1628
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  2.]), 'previousTarget': array([20.,  2.]), 'currentState': array([ 6.5760141 , 13.41867276,  6.07171059]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 17.623549161351562}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9248271659516653
{'scaleFactor': 20, 'currentTarget': array([20.,  2.]), 'previousTarget': array([20.,  2.]), 'currentState': array([20.03471172,  1.84427225,  5.27201443]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 0.15954947498711644}
episode index:1629
target Thresh 31.999997844327133
target distance 11.0
model initialize at round 1629
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'previousTarget': array([11., 21.]), 'currentState': array([10.10741631,  8.73271266,  0.15738076]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 12.299717240686622}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9248373825059287
{'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'previousTarget': array([11., 21.]), 'currentState': array([10.61247397, 20.09729993,  1.38691847]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 0.9823664452139794}
episode index:1630
target Thresh 31.99999786577644
target distance 9.0
model initialize at round 1630
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 13.]), 'previousTarget': array([ 3., 13.]), 'currentState': array([11.62667554, 10.35895858,  3.4787035 ]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 9.021897283477147}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9248534172498858
{'scaleFactor': 20, 'currentTarget': array([ 3., 13.]), 'previousTarget': array([ 3., 13.]), 'currentState': array([ 2.32363137, 13.41921244,  2.6540072 ]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 0.7957471921081684}
episode index:1631
target Thresh 31.999997887012317
target distance 5.0
model initialize at round 1631
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  6.]), 'previousTarget': array([11.,  6.]), 'currentState': array([14.52628741,  2.81275847,  3.64758694]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 4.7532316881603816}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9248812638079434
{'scaleFactor': 20, 'currentTarget': array([11.,  6.]), 'previousTarget': array([11.,  6.]), 'currentState': array([10.59328979,  6.04813989,  1.25210655]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 0.4095493150787091}
episode index:1632
target Thresh 31.999997908036896
target distance 17.0
model initialize at round 1632
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 25.]), 'previousTarget': array([23., 25.]), 'currentState': array([19.11159859,  6.73637954,  0.16068571]), 'targetState': array([23, 25], dtype=int32), 'currentDistance': 18.672961673970438}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9248687107223347
{'scaleFactor': 20, 'currentTarget': array([23., 25.]), 'previousTarget': array([23., 25.]), 'currentState': array([23.3963344 , 25.35231827,  0.67848053]), 'targetState': array([23, 25], dtype=int32), 'currentDistance': 0.5302915437329916}
episode index:1633
target Thresh 31.999997928852277
target distance 14.0
model initialize at round 1633
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 17.]), 'previousTarget': array([27., 17.]), 'currentState': array([14.40945941,  7.39968747,  0.98630762]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 15.833120760343139}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9248674108347615
{'scaleFactor': 20, 'currentTarget': array([27., 17.]), 'previousTarget': array([27., 17.]), 'currentState': array([26.9571799 , 17.11568817,  0.2686455 ]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 0.12335847543650158}
episode index:1634
target Thresh 31.99999794946054
target distance 10.0
model initialize at round 1634
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 14.]), 'previousTarget': array([12., 14.]), 'currentState': array([11.97874334, 25.33769313,  3.23284757]), 'targetState': array([12, 14], dtype=int32), 'currentDistance': 11.33771306075511}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9248775715311324
{'scaleFactor': 20, 'currentTarget': array([12., 14.]), 'previousTarget': array([12., 14.]), 'currentState': array([11.92578742, 14.05282465,  4.37447891]), 'targetState': array([12, 14], dtype=int32), 'currentDistance': 0.09109308885901342}
episode index:1635
target Thresh 31.99999796986375
target distance 3.0
model initialize at round 1635
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 12.]), 'previousTarget': array([ 4., 12.]), 'currentState': array([ 5.35385952, 12.35016137,  3.94199836]), 'targetState': array([ 4, 12], dtype=int32), 'currentDistance': 1.398409304971459}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9249173774165045
{'scaleFactor': 20, 'currentTarget': array([ 4., 12.]), 'previousTarget': array([ 4., 12.]), 'currentState': array([ 3.64818102, 12.56915722,  2.06705022]), 'targetState': array([ 4, 12], dtype=int32), 'currentDistance': 0.6691162353254514}
episode index:1636
target Thresh 31.999997990063942
target distance 23.0
model initialize at round 1636
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 28.]), 'previousTarget': array([12., 28.]), 'currentState': array([25.60722398,  4.50078145,  0.70883864]), 'targetState': array([12, 28], dtype=int32), 'currentDistance': 27.154554259897573}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9248830636934455
{'scaleFactor': 20, 'currentTarget': array([12., 28.]), 'previousTarget': array([12., 28.]), 'currentState': array([12.07818699, 27.57146313,  1.62258925]), 'targetState': array([12, 28], dtype=int32), 'currentDistance': 0.4356111242624213}
episode index:1637
target Thresh 31.99999801006314
target distance 15.0
model initialize at round 1637
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 28.]), 'previousTarget': array([11., 28.]), 'currentState': array([24.03836987, 22.38891715,  2.96983586]), 'targetState': array([11, 28], dtype=int32), 'currentDistance': 14.19448271755695}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9248874484823427
{'scaleFactor': 20, 'currentTarget': array([11., 28.]), 'previousTarget': array([11., 28.]), 'currentState': array([11.20916531, 27.72493321,  2.38899391]), 'targetState': array([11, 28], dtype=int32), 'currentDistance': 0.3455602194959285}
episode index:1638
target Thresh 31.99999802986334
target distance 15.0
model initialize at round 1638
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  4.]), 'previousTarget': array([10.,  4.]), 'currentState': array([25.03197945, 20.68266616,  2.56179339]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 22.455996001597068}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9248639569772992
{'scaleFactor': 20, 'currentTarget': array([10.,  4.]), 'previousTarget': array([10.,  4.]), 'currentState': array([9.70927533, 3.849138  , 3.18128228]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 0.3275365271827549}
episode index:1639
target Thresh 31.99999804946653
target distance 13.0
model initialize at round 1639
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 18.]), 'previousTarget': array([26., 18.]), 'currentState': array([12.98660357, 22.3170833 ,  5.7144289 ]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 13.710787538337048}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9248683480693294
{'scaleFactor': 20, 'currentTarget': array([26., 18.]), 'previousTarget': array([26., 18.]), 'currentState': array([26.07189502, 18.24227658,  5.5125191 ]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 0.25271888728747366}
episode index:1640
target Thresh 31.999998068874664
target distance 7.0
model initialize at round 1640
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  6.]), 'previousTarget': array([24.,  6.]), 'currentState': array([26.03624317, 11.28192302,  4.50524449]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 5.660830068106386}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9248960328054238
{'scaleFactor': 20, 'currentTarget': array([24.,  6.]), 'previousTarget': array([24.,  6.]), 'currentState': array([24.00763527,  5.70234324,  3.97478284]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 0.2977546740268081}
episode index:1641
target Thresh 31.99999808808968
target distance 21.0
model initialize at round 1641
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([16.61704031, 22.36118031,  5.49282813]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 21.969550632234665}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9248780317222649
{'scaleFactor': 20, 'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([27.39787448,  3.70977852,  4.82283639]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 0.8136889142998794}
episode index:1642
target Thresh 31.999998107113505
target distance 19.0
model initialize at round 1642
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 23.]), 'previousTarget': array([10., 23.]), 'currentState': array([8.08004556, 5.6873843 , 0.52402318]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 17.418751027432155}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9248600525515627
{'scaleFactor': 20, 'currentTarget': array([10., 23.]), 'previousTarget': array([10., 23.]), 'currentState': array([10.78404348, 23.3406254 ,  0.38278741]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 0.8548390748504082}
episode index:1643
target Thresh 31.999998125948043
target distance 12.0
model initialize at round 1643
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 16.]), 'previousTarget': array([ 6., 16.]), 'currentState': array([17.13089194, 25.22482532,  3.98850268]), 'targetState': array([ 6, 16], dtype=int32), 'currentDistance': 14.456630231910335}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9248644353346256
{'scaleFactor': 20, 'currentTarget': array([ 6., 16.]), 'previousTarget': array([ 6., 16.]), 'currentState': array([ 6.65656688, 16.10413159,  3.64142846]), 'targetState': array([ 6, 16], dtype=int32), 'currentDistance': 0.6647732406966074}
episode index:1644
target Thresh 31.99999814459517
target distance 10.0
model initialize at round 1644
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 26.]), 'previousTarget': array([ 2., 26.]), 'currentState': array([11.80001217, 19.67104547,  2.69990808]), 'targetState': array([ 2, 26], dtype=int32), 'currentDistance': 11.666014915501815}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9248745360726599
{'scaleFactor': 20, 'currentTarget': array([ 2., 26.]), 'previousTarget': array([ 2., 26.]), 'currentState': array([ 1.77240359, 26.10175173,  2.11200996]), 'targetState': array([ 2, 26], dtype=int32), 'currentDistance': 0.24930611507004613}
episode index:1645
target Thresh 31.999998163056755
target distance 7.0
model initialize at round 1645
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 12.]), 'previousTarget': array([ 8., 12.]), 'currentState': array([3.49289929, 5.7769426 , 1.48985403]), 'targetState': array([ 8, 12], dtype=int32), 'currentDistance': 7.683775127872906}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9248962380616802
{'scaleFactor': 20, 'currentTarget': array([ 8., 12.]), 'previousTarget': array([ 8., 12.]), 'currentState': array([ 7.82951941, 12.23518904,  0.47075566]), 'targetState': array([ 8, 12], dtype=int32), 'currentDistance': 0.290478078927759}
episode index:1646
target Thresh 31.999998181334647
target distance 11.0
model initialize at round 1646
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 22.]), 'previousTarget': array([18., 22.]), 'currentState': array([ 8.68967737, 18.05663313,  0.70677438]), 'targetState': array([18, 22], dtype=int32), 'currentDistance': 10.110996478573298}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9249120812989833
{'scaleFactor': 20, 'currentTarget': array([18., 22.]), 'previousTarget': array([18., 22.]), 'currentState': array([17.58341717, 22.38677873,  0.05526675]), 'targetState': array([18, 22], dtype=int32), 'currentDistance': 0.5684532031414216}
episode index:1647
target Thresh 31.99999819943067
target distance 7.0
model initialize at round 1647
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 14.]), 'previousTarget': array([26., 14.]), 'currentState': array([26.83336687,  8.96881806,  1.92755651]), 'targetState': array([26, 14], dtype=int32), 'currentDistance': 5.099734513156923}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9249396219049912
{'scaleFactor': 20, 'currentTarget': array([26., 14.]), 'previousTarget': array([26., 14.]), 'currentState': array([26.24194447, 14.53115239,  0.61463934]), 'targetState': array([26, 14], dtype=int32), 'currentDistance': 0.5836608526494893}
episode index:1648
target Thresh 31.999998217346633
target distance 18.0
model initialize at round 1648
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 21.]), 'previousTarget': array([23., 21.]), 'currentState': array([5.86560071, 6.44330298, 0.02057099]), 'targetState': array([23, 21], dtype=int32), 'currentDistance': 22.48299506724885}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9249162412196128
{'scaleFactor': 20, 'currentTarget': array([23., 21.]), 'previousTarget': array([23., 21.]), 'currentState': array([22.17442763, 21.23927058,  0.42583719]), 'targetState': array([23, 21], dtype=int32), 'currentDistance': 0.8595464749497168}
episode index:1649
target Thresh 31.99999823508433
target distance 12.0
model initialize at round 1649
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([16.00944099,  7.65336829,  0.08245152]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 16.671648010829074}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9249037962703941
{'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([25.4931989 , 21.45359917,  6.03283513]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 0.6801467169012949}
episode index:1650
target Thresh 31.999998252645536
target distance 6.0
model initialize at round 1650
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 2.]), 'previousTarget': array([5., 2.]), 'currentState': array([4.35600597, 7.6399004 , 4.3672266 ]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 5.676548669372393}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9249312918510905
{'scaleFactor': 20, 'currentTarget': array([5., 2.]), 'previousTarget': array([5., 2.]), 'currentState': array([4.92698357, 1.74487624, 4.66490883]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 0.2653667849216219}
episode index:1651
target Thresh 31.999998270032002
target distance 5.0
model initialize at round 1651
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 18.]), 'previousTarget': array([11., 18.]), 'currentState': array([ 8.43854637, 14.35487387,  1.0262166 ]), 'targetState': array([11, 18], dtype=int32), 'currentDistance': 4.4551082107691595}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9249646869528755
{'scaleFactor': 20, 'currentTarget': array([11., 18.]), 'previousTarget': array([11., 18.]), 'currentState': array([10.6419194 , 17.6883571 ,  0.88171115]), 'targetState': array([11, 18], dtype=int32), 'currentDistance': 0.4747030801383798}
episode index:1652
target Thresh 31.99999828724547
target distance 16.0
model initialize at round 1652
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 14.]), 'previousTarget': array([ 2., 14.]), 'currentState': array([17.40542942, 26.425556  ,  3.34130716]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 19.79196042565379}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9249522352820079
{'scaleFactor': 20, 'currentTarget': array([ 2., 14.]), 'previousTarget': array([ 2., 14.]), 'currentState': array([ 2.70623111, 13.93186897,  3.45546877]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 0.7095098485906555}
episode index:1653
target Thresh 31.999998304287665
target distance 23.0
model initialize at round 1653
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  2.]), 'previousTarget': array([22.,  2.]), 'currentState': array([10.76230928, 23.49957584,  6.19246578]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 24.259378685640698}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9249235586119455
{'scaleFactor': 20, 'currentTarget': array([22.,  2.]), 'previousTarget': array([22.,  2.]), 'currentState': array([21.64351533,  1.88436585,  3.31333039]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 0.37477003430928785}
episode index:1654
target Thresh 31.999998321160284
target distance 17.0
model initialize at round 1654
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  7.]), 'previousTarget': array([20.,  7.]), 'currentState': array([ 4.68253806, 10.03812833,  1.03265732]), 'targetState': array([20,  7], dtype=int32), 'currentDistance': 15.615852974049545}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9249166665810523
{'scaleFactor': 20, 'currentTarget': array([20.,  7.]), 'previousTarget': array([20.,  7.]), 'currentState': array([20.17335211,  6.86702855,  5.61290491]), 'targetState': array([20,  7], dtype=int32), 'currentDistance': 0.21847737065530992}
episode index:1655
target Thresh 31.99999833786502
target distance 21.0
model initialize at round 1655
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([10.51383145, 27.73532471,  5.72535795]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 24.13263497750411}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9248880460233337
{'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([20.41265672,  5.68747464,  3.35282195]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 0.6653151376634596}
episode index:1656
target Thresh 31.999998354403537
target distance 12.0
model initialize at round 1656
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([15.94152602, 12.29269464,  6.09749544]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 10.583701469482863}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9249037985905495
{'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([25.53608946,  9.49994426,  5.95269218]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 0.682024373453665}
episode index:1657
target Thresh 31.999998370777497
target distance 13.0
model initialize at round 1657
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 18.]), 'previousTarget': array([11., 18.]), 'currentState': array([24.59045097,  5.55032153,  1.34312123]), 'targetState': array([11, 18], dtype=int32), 'currentDistance': 18.430812559124078}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9248914211939381
{'scaleFactor': 20, 'currentTarget': array([11., 18.]), 'previousTarget': array([11., 18.]), 'currentState': array([11.06467871, 18.77615851,  1.27017045]), 'targetState': array([11, 18], dtype=int32), 'currentDistance': 0.7788487489179916}
episode index:1658
target Thresh 31.99999838698853
target distance 17.0
model initialize at round 1658
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 20.]), 'previousTarget': array([21., 20.]), 'currentState': array([24.70031459,  4.06920809,  1.44318247]), 'targetState': array([21, 20], dtype=int32), 'currentDistance': 16.354890983704387}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9248901272055317
{'scaleFactor': 20, 'currentTarget': array([21., 20.]), 'previousTarget': array([21., 20.]), 'currentState': array([20.93131531, 19.50034026,  1.71380228]), 'targetState': array([21, 20], dtype=int32), 'currentDistance': 0.5043584429498477}
episode index:1659
target Thresh 31.999998403038262
target distance 16.0
model initialize at round 1659
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 5.]), 'previousTarget': array([7., 5.]), 'currentState': array([21.45329753,  9.74020782,  3.69932963]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 15.210765257545752}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9248888347761475
{'scaleFactor': 20, 'currentTarget': array([7., 5.]), 'previousTarget': array([7., 5.]), 'currentState': array([6.48593286, 5.02624886, 2.68200108]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 0.5147368519721398}
episode index:1660
target Thresh 31.9999984189283
target distance 13.0
model initialize at round 1660
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 13.]), 'previousTarget': array([13., 13.]), 'currentState': array([24.8067722 ,  6.81315734,  2.91430807]), 'targetState': array([13, 13], dtype=int32), 'currentDistance': 13.32954957550767}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9248931553740589
{'scaleFactor': 20, 'currentTarget': array([13., 13.]), 'previousTarget': array([13., 13.]), 'currentState': array([13.15479288, 13.42368615,  1.27772855]), 'targetState': array([13, 13], dtype=int32), 'currentDistance': 0.45107736926610187}
episode index:1661
target Thresh 31.999998434660224
target distance 11.0
model initialize at round 1661
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 22.]), 'previousTarget': array([ 7., 22.]), 'currentState': array([ 9.70904321, 12.5016431 ,  2.03484333]), 'targetState': array([ 7, 22], dtype=int32), 'currentDistance': 9.877130098357561}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9249088574766617
{'scaleFactor': 20, 'currentTarget': array([ 7., 22.]), 'previousTarget': array([ 7., 22.]), 'currentState': array([ 6.75145659, 21.99588081,  1.489887  ]), 'targetState': array([ 7, 22], dtype=int32), 'currentDistance': 0.24857754464005757}
episode index:1662
target Thresh 31.999998450235616
target distance 21.0
model initialize at round 1662
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 29.]), 'previousTarget': array([22., 29.]), 'currentState': array([8.37145028, 9.64155116, 2.35812563]), 'targetState': array([22, 29], dtype=int32), 'currentDistance': 23.67460472851278}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9248803620861159
{'scaleFactor': 20, 'currentTarget': array([22., 29.]), 'previousTarget': array([22., 29.]), 'currentState': array([21.93716084, 28.8125979 ,  0.52657081]), 'targetState': array([22, 29], dtype=int32), 'currentDistance': 0.19765704768712933}
episode index:1663
target Thresh 31.999998465656027
target distance 7.0
model initialize at round 1663
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 19.]), 'previousTarget': array([10., 19.]), 'currentState': array([15.40998339, 13.44842476,  2.46550226]), 'targetState': array([10, 19], dtype=int32), 'currentDistance': 7.751639047023846}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9249018258168333
{'scaleFactor': 20, 'currentTarget': array([10., 19.]), 'previousTarget': array([10., 19.]), 'currentState': array([ 9.93668176, 19.24517808,  2.08772617]), 'targetState': array([10, 19], dtype=int32), 'currentDistance': 0.2532222149286999}
episode index:1664
target Thresh 31.999998480923008
target distance 5.0
model initialize at round 1664
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([ 9.31827351, 22.0646845 ,  4.11314845]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 5.233438082779902}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9249290913869134
{'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([ 7.70050537, 16.34645765,  4.21832541]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 0.7188982126579262}
episode index:1665
target Thresh 31.999998496038074
target distance 24.0
model initialize at round 1665
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4.40054263, 6.22822576]), 'previousTarget': array([3.42269916, 5.24486176]), 'currentState': array([26.38822701, 26.63757988,  2.34802017]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 30.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.9248798801636223
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.81571794, 3.60906824, 3.42962037]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.9045570138786911}
episode index:1666
target Thresh 31.999998511002747
target distance 17.0
model initialize at round 1666
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 17.]), 'previousTarget': array([ 7., 17.]), 'currentState': array([23.11660955, 14.56751635,  3.14979172]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 16.299143540814025}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9248785993083518
{'scaleFactor': 20, 'currentTarget': array([ 7., 17.]), 'previousTarget': array([ 7., 17.]), 'currentState': array([ 7.3307335 , 16.56283393,  2.79766171]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 0.5481777296319427}
episode index:1667
target Thresh 31.999998525818516
target distance 14.0
model initialize at round 1667
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 29.]), 'previousTarget': array([11., 29.]), 'currentState': array([25.72290346, 18.51980218,  2.13681266]), 'targetState': array([11, 29], dtype=int32), 'currentDistance': 18.07203454457134}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9248717879463466
{'scaleFactor': 20, 'currentTarget': array([11., 29.]), 'previousTarget': array([11., 29.]), 'currentState': array([10.95804742, 28.37773402,  2.1973858 ]), 'targetState': array([11, 29], dtype=int32), 'currentDistance': 0.6236785818435899}
episode index:1668
target Thresh 31.999998540486867
target distance 20.0
model initialize at round 1668
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 26.]), 'previousTarget': array([24., 26.]), 'currentState': array([9.62919702, 7.56092895, 0.1776278 ]), 'targetState': array([24, 26], dtype=int32), 'currentDistance': 23.377752658427546}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9248434172064142
{'scaleFactor': 20, 'currentTarget': array([24., 26.]), 'previousTarget': array([24., 26.]), 'currentState': array([23.65104981, 26.13067786,  0.44036614]), 'targetState': array([24, 26], dtype=int32), 'currentDistance': 0.37261634263284893}
episode index:1669
target Thresh 31.999998555009267
target distance 22.0
model initialize at round 1669
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  8.]), 'previousTarget': array([25.,  8.]), 'currentState': array([4.8671839 , 9.25866204, 0.73611694]), 'targetState': array([25,  8], dtype=int32), 'currentDistance': 20.172122208263456}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.924831164905697
{'scaleFactor': 20, 'currentTarget': array([25.,  8.]), 'previousTarget': array([25.,  8.]), 'currentState': array([24.3506908 ,  8.47098053,  5.907003  ]), 'targetState': array([25,  8], dtype=int32), 'currentDistance': 0.8021378306899016}
episode index:1670
target Thresh 31.999998569387163
target distance 17.0
model initialize at round 1670
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 28.]), 'previousTarget': array([21., 28.]), 'currentState': array([ 9.70452004, 12.78945019,  0.70788616]), 'targetState': array([21, 28], dtype=int32), 'currentDistance': 18.945941333105015}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9248243941591847
{'scaleFactor': 20, 'currentTarget': array([21., 28.]), 'previousTarget': array([21., 28.]), 'currentState': array([20.16962394, 27.18608001,  0.77974982]), 'targetState': array([21, 28], dtype=int32), 'currentDistance': 1.1627511130438373}
episode index:1671
target Thresh 31.999998583622
target distance 5.0
model initialize at round 1671
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([27.4925546 , 11.77760457,  1.49029749]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 6.927970785161034}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9248457886662665
{'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([22.13130214, 15.85052547,  1.49476689]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 0.19895448066709454}
episode index:1672
target Thresh 31.999998597715194
target distance 20.0
model initialize at round 1672
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 25.]), 'previousTarget': array([23., 25.]), 'currentState': array([25.47526177,  6.78954853,  2.50575554]), 'targetState': array([23, 25], dtype=int32), 'currentDistance': 18.377906938743838}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9248390172728519
{'scaleFactor': 20, 'currentTarget': array([23., 25.]), 'previousTarget': array([23., 25.]), 'currentState': array([22.97354411, 24.25305152,  1.53462536]), 'targetState': array([23, 25], dtype=int32), 'currentDistance': 0.7474168530567643}
episode index:1673
target Thresh 31.99999861166816
target distance 12.0
model initialize at round 1673
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 19.]), 'previousTarget': array([24., 19.]), 'currentState': array([25.32862693,  6.80276882,  2.24905491]), 'targetState': array([24, 19], dtype=int32), 'currentDistance': 12.269380501822724}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9248489582119966
{'scaleFactor': 20, 'currentTarget': array([24., 19.]), 'previousTarget': array([24., 19.]), 'currentState': array([23.67571478, 18.49738324,  1.44937425]), 'targetState': array([24, 19], dtype=int32), 'currentDistance': 0.5981509132737581}
episode index:1674
target Thresh 31.999998625482295
target distance 8.0
model initialize at round 1674
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 12.]), 'previousTarget': array([19., 12.]), 'currentState': array([12.66260595, 12.26102217,  5.42891151]), 'targetState': array([19, 12], dtype=int32), 'currentDistance': 6.342767217622451}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9248702997354522
{'scaleFactor': 20, 'currentTarget': array([19., 12.]), 'previousTarget': array([19., 12.]), 'currentState': array([19.3328009 , 11.63846771,  4.8392703 ]), 'targetState': array([19, 12], dtype=int32), 'currentDistance': 0.49138786273728646}
episode index:1675
target Thresh 31.999998639158974
target distance 4.0
model initialize at round 1675
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([15.67322859, 20.03540606,  3.48892069]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 5.045997793332009}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9248974051652044
{'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([16.20937267, 14.68132709,  4.4755201 ]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 0.3812995327045952}
episode index:1676
target Thresh 31.99999865269957
target distance 17.0
model initialize at round 1676
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 25.]), 'previousTarget': array([26., 25.]), 'currentState': array([20.65893889,  7.71661008,  0.84080714]), 'targetState': array([26, 25], dtype=int32), 'currentDistance': 18.089845247578}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9248906191439273
{'scaleFactor': 20, 'currentTarget': array([26., 25.]), 'previousTarget': array([26., 25.]), 'currentState': array([25.49738389, 24.80952917,  1.02651764]), 'targetState': array([26, 25], dtype=int32), 'currentDistance': 0.5374961267954897}
episode index:1677
target Thresh 31.999998666105434
target distance 20.0
model initialize at round 1677
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 22.]), 'previousTarget': array([ 2., 22.]), 'currentState': array([20.5625638 ,  2.12469133,  2.67855787]), 'targetState': array([ 2, 22], dtype=int32), 'currentDistance': 27.19552664806137}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9248571597837516
{'scaleFactor': 20, 'currentTarget': array([ 2., 22.]), 'previousTarget': array([ 2., 22.]), 'currentState': array([ 1.75778363, 22.06836463,  1.92777508]), 'targetState': array([ 2, 22], dtype=int32), 'currentDistance': 0.25167934494464833}
episode index:1678
target Thresh 31.999998679377907
target distance 14.0
model initialize at round 1678
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'previousTarget': array([13., 22.]), 'currentState': array([5.31689119, 9.53809962, 0.97875404]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 14.639983677112658}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9248614529273628
{'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'previousTarget': array([13., 22.]), 'currentState': array([12.32599689, 21.6201809 ,  0.93731649]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 0.7736554432762258}
episode index:1679
target Thresh 31.999998692518314
target distance 1.0
model initialize at round 1679
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 20.]), 'previousTarget': array([ 9., 20.]), 'currentState': array([ 7.21947811, 19.50895819,  5.24012291]), 'targetState': array([ 9, 20], dtype=int32), 'currentDistance': 1.8469922172817939}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9249002258720489
{'scaleFactor': 20, 'currentTarget': array([ 9., 20.]), 'previousTarget': array([ 9., 20.]), 'currentState': array([ 8.90012198, 19.41955687,  0.95677155]), 'targetState': array([ 9, 20], dtype=int32), 'currentDistance': 0.5889735516888056}
episode index:1680
target Thresh 31.999998705527975
target distance 16.0
model initialize at round 1680
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 22.]), 'previousTarget': array([27., 22.]), 'currentState': array([24.32473538,  6.16086191,  2.03586435]), 'targetState': array([27, 22], dtype=int32), 'currentDistance': 16.063478334897155}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9248989435808863
{'scaleFactor': 20, 'currentTarget': array([27., 22.]), 'previousTarget': array([27., 22.]), 'currentState': array([26.69467987, 21.8267425 ,  1.20444411]), 'targetState': array([27, 22], dtype=int32), 'currentDistance': 0.3510534736172555}
episode index:1681
target Thresh 31.999998718408186
target distance 8.0
model initialize at round 1681
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([ 9.77288334, 10.08202039,  5.70110476]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 8.037655949070114}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9249201665692449
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.07030439,  5.18648584,  5.38554206]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.19929795606142062}
episode index:1682
target Thresh 31.99999873116024
target distance 4.0
model initialize at round 1682
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([11.3138809 , 25.05173781,  1.68501108]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 5.314132764391201}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9249471296312953
{'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([ 6.00922287, 24.80208977,  3.04963895]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 0.19812500998086446}
episode index:1683
target Thresh 31.999998743785405
target distance 13.0
model initialize at round 1683
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 13.]), 'previousTarget': array([10., 13.]), 'currentState': array([17.76292727, 24.8589307 ,  4.89664364]), 'targetState': array([10, 13], dtype=int32), 'currentDistance': 14.173823659049356}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9249513566017677
{'scaleFactor': 20, 'currentTarget': array([10., 13.]), 'previousTarget': array([10., 13.]), 'currentState': array([10.35130438, 13.21799975,  3.96772576]), 'targetState': array([10, 13], dtype=int32), 'currentDistance': 0.41344729131099667}
episode index:1684
target Thresh 31.99999875628495
target distance 6.0
model initialize at round 1684
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 28.]), 'previousTarget': array([18., 28.]), 'currentState': array([17.26400199, 23.98058896,  1.51441976]), 'targetState': array([18, 28], dtype=int32), 'currentDistance': 4.086240106347325}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9249840857669893
{'scaleFactor': 20, 'currentTarget': array([18., 28.]), 'previousTarget': array([18., 28.]), 'currentState': array([17.80032878, 27.93132972,  1.2528495 ]), 'targetState': array([18, 28], dtype=int32), 'currentDistance': 0.211149720319109}
episode index:1685
target Thresh 31.99999876866012
target distance 15.0
model initialize at round 1685
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([21.3979791 , 24.51567153,  3.84017789]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 16.183777660326395}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9249827575396232
{'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([26.16539999,  9.8515654 ,  4.86788755]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 0.8674795595328996}
episode index:1686
target Thresh 31.999998780912158
target distance 22.0
model initialize at round 1686
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 27.]), 'previousTarget': array([ 9., 27.]), 'currentState': array([12.41126873,  6.57663679,  2.93817192]), 'targetState': array([ 9, 27], dtype=int32), 'currentDistance': 20.706291775049174}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9249598779392536
{'scaleFactor': 20, 'currentTarget': array([ 9., 27.]), 'previousTarget': array([ 9., 27.]), 'currentState': array([ 8.92871246, 27.36876807,  1.16366182]), 'targetState': array([ 9, 27], dtype=int32), 'currentDistance': 0.3755952693212298}
episode index:1687
target Thresh 31.999998793042284
target distance 15.0
model initialize at round 1687
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([17.95685558,  4.67930404,  3.03387117]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 13.960539517957828}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9249640873408932
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.02260684, 4.8488727 , 2.85641242]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.1528087994769106}
episode index:1688
target Thresh 31.999998805051714
target distance 12.0
model initialize at round 1688
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 24.]), 'previousTarget': array([ 3., 24.]), 'currentState': array([13.87212176, 13.75088881,  2.96794939]), 'targetState': array([ 3, 24], dtype=int32), 'currentDistance': 14.941462836076072}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.924968291758043
{'scaleFactor': 20, 'currentTarget': array([ 3., 24.]), 'previousTarget': array([ 3., 24.]), 'currentState': array([ 3.60424975, 23.0571509 ,  2.33521324]), 'targetState': array([ 3, 24], dtype=int32), 'currentDistance': 1.1198581091136925}
episode index:1689
target Thresh 31.99999881694165
target distance 9.0
model initialize at round 1689
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 22.]), 'previousTarget': array([11., 22.]), 'currentState': array([17.61463094, 14.38854424,  2.69952279]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 10.084027033030978}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9249836892480677
{'scaleFactor': 20, 'currentTarget': array([11., 22.]), 'previousTarget': array([11., 22.]), 'currentState': array([10.869949  , 21.60355947,  1.99307933]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 0.4172269897875823}
episode index:1690
target Thresh 31.999998828713274
target distance 9.0
model initialize at round 1690
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 17.]), 'previousTarget': array([18., 17.]), 'currentState': array([13.02514843,  9.95059631,  1.16824448]), 'targetState': array([18, 17], dtype=int32), 'currentDistance': 8.62804963789679}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9250047491657212
{'scaleFactor': 20, 'currentTarget': array([18., 17.]), 'previousTarget': array([18., 17.]), 'currentState': array([17.38990807, 16.64155171,  0.91495235]), 'targetState': array([18, 17], dtype=int32), 'currentDistance': 0.7075996991824637}
episode index:1691
target Thresh 31.999998840367773
target distance 18.0
model initialize at round 1691
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 20.]), 'previousTarget': array([11., 20.]), 'currentState': array([15.54394472,  2.6697931 ,  1.41931694]), 'targetState': array([11, 20], dtype=int32), 'currentDistance': 17.916012521308005}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9249979598621266
{'scaleFactor': 20, 'currentTarget': array([11., 20.]), 'previousTarget': array([11., 20.]), 'currentState': array([10.81960257, 19.7624514 ,  1.59066891]), 'targetState': array([11, 20], dtype=int32), 'currentDistance': 0.29828270021462955}
episode index:1692
target Thresh 31.999998851906305
target distance 12.0
model initialize at round 1692
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 17.]), 'previousTarget': array([13., 17.]), 'currentState': array([ 7.11914666, 27.37317642,  5.59432358]), 'targetState': array([13, 17], dtype=int32), 'currentDistance': 11.924228487944113}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9250076953550617
{'scaleFactor': 20, 'currentTarget': array([13., 17.]), 'previousTarget': array([13., 17.]), 'currentState': array([13.13752455, 17.08537466,  5.07093278]), 'targetState': array([13, 17], dtype=int32), 'currentDistance': 0.16186980290862715}
episode index:1693
target Thresh 31.999998863330028
target distance 17.0
model initialize at round 1693
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 19.]), 'previousTarget': array([14., 19.]), 'currentState': array([8.78317834, 3.16264059, 1.36896086]), 'targetState': array([14, 19], dtype=int32), 'currentDistance': 16.67444695947796}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9250063594631329
{'scaleFactor': 20, 'currentTarget': array([14., 19.]), 'previousTarget': array([14., 19.]), 'currentState': array([13.64035423, 18.28372628,  1.09581834]), 'targetState': array([14, 19], dtype=int32), 'currentDistance': 0.8014943053456465}
episode index:1694
target Thresh 31.999998874640085
target distance 14.0
model initialize at round 1694
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 6.]), 'previousTarget': array([6., 6.]), 'currentState': array([ 7.49684314, 19.23068296,  4.79843241]), 'targetState': array([6, 6], dtype=int32), 'currentDistance': 13.315085842620602}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.925010524058085
{'scaleFactor': 20, 'currentTarget': array([6., 6.]), 'previousTarget': array([6., 6.]), 'currentState': array([5.93069579, 5.42673267, 4.15277667]), 'targetState': array([6, 6], dtype=int32), 'currentDistance': 0.5774413443892783}
episode index:1695
target Thresh 31.999998885837602
target distance 5.0
model initialize at round 1695
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 28.]), 'previousTarget': array([13., 28.]), 'currentState': array([ 9.68215515, 22.9476346 ,  0.9788801 ]), 'targetState': array([13, 28], dtype=int32), 'currentDistance': 6.044376773449831}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9250372271688998
{'scaleFactor': 20, 'currentTarget': array([13., 28.]), 'previousTarget': array([13., 28.]), 'currentState': array([12.84737266, 28.03889884,  0.91001217]), 'targetState': array([13, 28], dtype=int32), 'currentDistance': 0.15750627151407287}
episode index:1696
target Thresh 31.999998896923703
target distance 12.0
model initialize at round 1696
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  9.]), 'previousTarget': array([19.,  9.]), 'currentState': array([27.38874826, 19.36254378,  3.93548346]), 'targetState': array([19,  9], dtype=int32), 'currentDistance': 13.332419544238462}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9250413686660938
{'scaleFactor': 20, 'currentTarget': array([19.,  9.]), 'previousTarget': array([19.,  9.]), 'currentState': array([18.68095427,  8.47204059,  3.71621355]), 'targetState': array([19,  9], dtype=int32), 'currentDistance': 0.6168722023074138}
episode index:1697
target Thresh 31.999998907899496
target distance 9.0
model initialize at round 1697
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 17.]), 'previousTarget': array([13., 17.]), 'currentState': array([21.4633928 , 25.59513034,  2.90530735]), 'targetState': array([13, 17], dtype=int32), 'currentDistance': 12.062557076742184}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9250510499268328
{'scaleFactor': 20, 'currentTarget': array([13., 17.]), 'previousTarget': array([13., 17.]), 'currentState': array([13.71439272, 17.26083127,  3.80077583]), 'targetState': array([13, 17], dtype=int32), 'currentDistance': 0.7605195034932719}
episode index:1698
target Thresh 31.999998918766078
target distance 11.0
model initialize at round 1698
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 16.]), 'previousTarget': array([16., 16.]), 'currentState': array([24.47222799, 26.18456549,  4.76737002]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 13.24779307552609}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9250551784129895
{'scaleFactor': 20, 'currentTarget': array([16., 16.]), 'previousTarget': array([16., 16.]), 'currentState': array([15.66976981, 15.63705199,  3.64796498]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 0.4906966838596954}
episode index:1699
target Thresh 31.999998929524534
target distance 11.0
model initialize at round 1699
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 15.]), 'previousTarget': array([13., 15.]), 'currentState': array([ 1.26322022, 14.4868762 ,  5.26925492]), 'targetState': array([13, 15], dtype=int32), 'currentDistance': 11.747991133005607}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9250648401606295
{'scaleFactor': 20, 'currentTarget': array([13., 15.]), 'previousTarget': array([13., 15.]), 'currentState': array([12.60390064, 15.08584028,  6.12244492]), 'targetState': array([13, 15], dtype=int32), 'currentDistance': 0.40529404139308317}
episode index:1700
target Thresh 31.999998940175946
target distance 6.0
model initialize at round 1700
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 12.]), 'previousTarget': array([13., 12.]), 'currentState': array([ 8.31384776, 17.0517567 ,  5.94824106]), 'targetState': array([13, 12], dtype=int32), 'currentDistance': 6.890592761142732}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9250857285614756
{'scaleFactor': 20, 'currentTarget': array([13., 12.]), 'previousTarget': array([13., 12.]), 'currentState': array([13.29293252, 11.17633135,  4.75660474]), 'targetState': array([13, 12], dtype=int32), 'currentDistance': 0.874207932187901}
episode index:1701
target Thresh 31.99999895072137
target distance 9.0
model initialize at round 1701
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 29.]), 'previousTarget': array([21., 29.]), 'currentState': array([13.07680239, 19.70659956,  0.13347119]), 'targetState': array([21, 29], dtype=int32), 'currentDistance': 12.212467081599415}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9250898293954036
{'scaleFactor': 20, 'currentTarget': array([21., 29.]), 'previousTarget': array([21., 29.]), 'currentState': array([20.79297652, 29.19907207,  0.44931867]), 'targetState': array([21, 29], dtype=int32), 'currentDistance': 0.2872079511559027}
episode index:1702
target Thresh 31.999998961161868
target distance 15.0
model initialize at round 1702
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  3.]), 'previousTarget': array([10.,  3.]), 'currentState': array([10.1732793 , 16.01724682,  4.97189456]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 13.018400073858302}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.92509392541332
{'scaleFactor': 20, 'currentTarget': array([10.,  3.]), 'previousTarget': array([10.,  3.]), 'currentState': array([9.76284642, 2.20136863, 4.06716933]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 0.8330989663784341}
episode index:1703
target Thresh 31.999998971498478
target distance 21.0
model initialize at round 1703
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  5.]), 'previousTarget': array([18.,  5.]), 'currentState': array([13.51390798, 25.26483287,  4.82111675]), 'targetState': array([18,  5], dtype=int32), 'currentDistance': 20.755444414736928}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9250764631649898
{'scaleFactor': 20, 'currentTarget': array([18.,  5.]), 'previousTarget': array([18.,  5.]), 'currentState': array([17.78714992,  4.17563305,  4.39052195]), 'targetState': array([18,  5], dtype=int32), 'currentDistance': 0.8514023862271759}
episode index:1704
target Thresh 31.99999898173224
target distance 4.0
model initialize at round 1704
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  8.]), 'previousTarget': array([19.,  8.]), 'currentState': array([16.7489123 , 10.49284425,  6.18355703]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 3.3588194783053327}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9251087350341012
{'scaleFactor': 20, 'currentTarget': array([19.,  8.]), 'previousTarget': array([19.,  8.]), 'currentState': array([19.24046294,  7.71077201,  5.95340538]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 0.37613196835718415}
episode index:1705
target Thresh 31.999998991864175
target distance 16.0
model initialize at round 1705
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([ 9.50739582, 24.93447651,  0.07947415]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 20.810434028449844}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9250912845764367
{'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([24.20058914,  9.3985142 ,  4.70108883]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 0.6340513906541163}
episode index:1706
target Thresh 31.999999001895294
target distance 9.0
model initialize at round 1706
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'previousTarget': array([13., 22.]), 'currentState': array([ 5.51668145, 14.72942818,  1.45828122]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 10.433660485156826}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9251064566709438
{'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'previousTarget': array([13., 22.]), 'currentState': array([12.16996911, 21.93057597,  0.6318423 ]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 0.8329291555152609}
episode index:1707
target Thresh 31.9999990118266
target distance 12.0
model initialize at round 1707
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 17.]), 'previousTarget': array([21., 17.]), 'currentState': array([10.11293561, 12.73755697,  0.16174429]), 'targetState': array([21, 17], dtype=int32), 'currentDistance': 11.691731764758002}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9251160431420973
{'scaleFactor': 20, 'currentTarget': array([21., 17.]), 'previousTarget': array([21., 17.]), 'currentState': array([21.12364692, 17.1958832 ,  0.03142193]), 'targetState': array([21, 17], dtype=int32), 'currentDistance': 0.23164366667310068}
episode index:1708
target Thresh 31.99999902165909
target distance 12.0
model initialize at round 1708
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([13.50565691, 19.39126976,  5.42425966]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 12.59748476002959}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9251201094409649
{'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([25.14914908, 21.40720937,  1.10391718]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 0.9432744844273724}
episode index:1709
target Thresh 31.999999031393745
target distance 11.0
model initialize at round 1709
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 15.]), 'previousTarget': array([12., 15.]), 'currentState': array([24.67664501,  3.8542276 ,  0.92327517]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 16.879738479052893}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9251133241415748
{'scaleFactor': 20, 'currentTarget': array([12., 15.]), 'previousTarget': array([12., 15.]), 'currentState': array([11.79576612, 15.19412002,  1.97947575]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 0.2817695111984769}
episode index:1710
target Thresh 31.999999041031536
target distance 20.0
model initialize at round 1710
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([ 3.86249558, 14.79520453,  5.54595089]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 19.368631113967094}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9251012076897145
{'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([22.15788564,  8.11461089,  5.23877718]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 0.19509877302758766}
episode index:1711
target Thresh 31.999999050573432
target distance 11.0
model initialize at round 1711
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 10.]), 'previousTarget': array([17., 10.]), 'currentState': array([7.85307368, 9.73263526, 0.22638929]), 'targetState': array([17, 10], dtype=int32), 'currentDistance': 9.150833023413949}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9251163296769868
{'scaleFactor': 20, 'currentTarget': array([17., 10.]), 'previousTarget': array([17., 10.]), 'currentState': array([17.62159744,  9.70647587,  5.4987041 ]), 'targetState': array([17, 10], dtype=int32), 'currentDistance': 0.6874152969049486}
episode index:1712
target Thresh 31.999999060020386
target distance 1.0
model initialize at round 1712
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 27.]), 'previousTarget': array([24., 27.]), 'currentState': array([24.38723402, 26.04650939,  0.4072414 ]), 'targetState': array([24, 27], dtype=int32), 'currentDistance': 1.0291231822563698}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.925160044604204
{'scaleFactor': 20, 'currentTarget': array([24., 27.]), 'previousTarget': array([24., 27.]), 'currentState': array([24.38723402, 26.04650939,  0.4072414 ]), 'targetState': array([24, 27], dtype=int32), 'currentDistance': 1.0291231822563698}
episode index:1713
target Thresh 31.999999069373338
target distance 24.0
model initialize at round 1713
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  5.]), 'previousTarget': array([23.,  5.]), 'currentState': array([15.76114621, 27.33406574,  5.57998419]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 23.47789378934988}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9251374219829157
{'scaleFactor': 20, 'currentTarget': array([23.,  5.]), 'previousTarget': array([23.,  5.]), 'currentState': array([23.04502064,  5.02311359,  4.40740134]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 0.050607271234961004}
episode index:1714
target Thresh 31.99999907863323
target distance 3.0
model initialize at round 1714
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([8.5639825 , 9.95000749, 1.86584449]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.1918727011413783}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9251752427281151
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.69379942, 11.74146229,  2.18327343]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.8022001741419085}
episode index:1715
target Thresh 31.99999908780098
target distance 2.0
model initialize at round 1715
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 16.]), 'previousTarget': array([16., 16.]), 'currentState': array([15.25093453, 16.50515994,  5.80501175]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 0.9034852735981298}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9252188468990195
{'scaleFactor': 20, 'currentTarget': array([16., 16.]), 'previousTarget': array([16., 16.]), 'currentState': array([15.25093453, 16.50515994,  5.80501175]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 0.9034852735981298}
episode index:1716
target Thresh 31.999999096877513
target distance 3.0
model initialize at round 1716
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 14.]), 'previousTarget': array([ 2., 14.]), 'currentState': array([ 3.85709423, 16.23537625,  3.32733285]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 2.906149683095667}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9252508102962828
{'scaleFactor': 20, 'currentTarget': array([ 2., 14.]), 'previousTarget': array([ 2., 14.]), 'currentState': array([ 2.44144325, 13.16126906,  3.26509416]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 0.9478089076119772}
episode index:1717
target Thresh 31.999999105863733
target distance 4.0
model initialize at round 1717
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 19.]), 'previousTarget': array([23., 19.]), 'currentState': array([25.87114596, 19.2482294 ,  3.31601739]), 'targetState': array([23, 19], dtype=int32), 'currentDistance': 2.881856510194626}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9252884989980893
{'scaleFactor': 20, 'currentTarget': array([23., 19.]), 'previousTarget': array([23., 19.]), 'currentState': array([23.89655082, 18.93095419,  3.28549645]), 'targetState': array([23, 19], dtype=int32), 'currentDistance': 0.8992055894082861}
episode index:1718
target Thresh 31.999999114760538
target distance 10.0
model initialize at round 1718
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  7.]), 'previousTarget': array([21.,  7.]), 'currentState': array([17.65853509, 17.28574368,  5.44379718]), 'targetState': array([21,  7], dtype=int32), 'currentDistance': 10.814893008989152}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.925297918224618
{'scaleFactor': 20, 'currentTarget': array([21.,  7.]), 'previousTarget': array([21.,  7.]), 'currentState': array([20.65785664,  6.20570462,  3.95064522]), 'targetState': array([21,  7], dtype=int32), 'currentDistance': 0.8648509871994507}
episode index:1719
target Thresh 31.999999123568816
target distance 7.0
model initialize at round 1719
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 18.]), 'previousTarget': array([ 2., 18.]), 'currentState': array([ 5.29193826, 12.65745592,  2.40644827]), 'targetState': array([ 2, 18], dtype=int32), 'currentDistance': 6.275319493285165}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.925324081644255
{'scaleFactor': 20, 'currentTarget': array([ 2., 18.]), 'previousTarget': array([ 2., 18.]), 'currentState': array([ 1.92980541, 17.57792231,  1.93263607]), 'targetState': array([ 2, 18], dtype=int32), 'currentDistance': 0.4278748147104672}
episode index:1720
target Thresh 31.999999132289453
target distance 17.0
model initialize at round 1720
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 21.]), 'previousTarget': array([19., 21.]), 'currentState': array([23.36633266,  4.40437497,  1.88894391]), 'targetState': array([19, 21], dtype=int32), 'currentDistance': 17.160408823654198}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9253172211944232
{'scaleFactor': 20, 'currentTarget': array([19., 21.]), 'previousTarget': array([19., 21.]), 'currentState': array([19.31437231, 21.2031519 ,  0.33187478]), 'targetState': array([19, 21], dtype=int32), 'currentDistance': 0.37430020546218024}
episode index:1721
target Thresh 31.999999140923318
target distance 18.0
model initialize at round 1721
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  5.]), 'previousTarget': array([13.,  5.]), 'currentState': array([ 5.27501616, 21.4673652 ,  5.26679912]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 18.18926859365034}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9253103687125934
{'scaleFactor': 20, 'currentTarget': array([13.,  5.]), 'previousTarget': array([13.,  5.]), 'currentState': array([13.06041799,  5.27914014,  4.90487242]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 0.2856038340204679}
episode index:1722
target Thresh 31.99999914947127
target distance 14.0
model initialize at round 1722
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  3.]), 'previousTarget': array([12.,  3.]), 'currentState': array([ 9.64707483, 16.65426516,  5.06628085]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 13.855512038048207}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.92531428918804
{'scaleFactor': 20, 'currentTarget': array([12.,  3.]), 'previousTarget': array([12.,  3.]), 'currentState': array([11.97778767,  2.98796754,  4.36112327]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 0.025261981432792926}
episode index:1723
target Thresh 31.999999157934173
target distance 21.0
model initialize at round 1723
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 24.]), 'previousTarget': array([10., 24.]), 'currentState': array([14.16447938,  4.72905281,  2.5176878 ]), 'targetState': array([10, 24], dtype=int32), 'currentDistance': 19.71578794203484}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9253021475324835
{'scaleFactor': 20, 'currentTarget': array([10., 24.]), 'previousTarget': array([10., 24.]), 'currentState': array([ 9.94448483, 23.85269991,  1.32953933]), 'targetState': array([10, 24], dtype=int32), 'currentDistance': 0.15741426160911084}
episode index:1724
target Thresh 31.99999916631287
target distance 12.0
model initialize at round 1724
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 16.]), 'previousTarget': array([20., 16.]), 'currentState': array([ 8.16804763, 21.32544093,  5.82240748]), 'targetState': array([20, 16], dtype=int32), 'currentDistance': 12.975184697176797}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9253115260842913
{'scaleFactor': 20, 'currentTarget': array([20., 16.]), 'previousTarget': array([20., 16.]), 'currentState': array([19.32775712, 16.95452455,  5.78153417]), 'targetState': array([20, 16], dtype=int32), 'currentDistance': 1.1674877314413012}
episode index:1725
target Thresh 31.999999174608195
target distance 14.0
model initialize at round 1725
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 17.]), 'previousTarget': array([17., 17.]), 'currentState': array([ 2.90194571, 12.68011114,  0.61909199]), 'targetState': array([17, 17], dtype=int32), 'currentDistance': 14.745052543875532}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9253100389280594
{'scaleFactor': 20, 'currentTarget': array([17., 17.]), 'previousTarget': array([17., 17.]), 'currentState': array([17.57658354, 16.63883492,  5.38424274]), 'targetState': array([17, 17], dtype=int32), 'currentDistance': 0.6803593124757074}
episode index:1726
target Thresh 31.999999182820982
target distance 21.0
model initialize at round 1726
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 25.]), 'previousTarget': array([ 8., 25.]), 'currentState': array([8.47331653, 4.81346572, 1.51447886]), 'targetState': array([ 8, 25], dtype=int32), 'currentDistance': 20.192082476627284}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9252979208250371
{'scaleFactor': 20, 'currentTarget': array([ 8., 25.]), 'previousTarget': array([ 8., 25.]), 'currentState': array([ 7.81408039, 24.67167361,  1.26894579]), 'targetState': array([ 8, 25], dtype=int32), 'currentDistance': 0.37731196383225024}
episode index:1727
target Thresh 31.99999919095205
target distance 6.0
model initialize at round 1727
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  7.]), 'previousTarget': array([17.,  7.]), 'currentState': array([12.86133713,  7.62892434,  0.3664084 ]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 4.186176798658561}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9253296349912263
{'scaleFactor': 20, 'currentTarget': array([17.,  7.]), 'previousTarget': array([17.,  7.]), 'currentState': array([16.72009312,  7.27122668,  5.78300279]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 0.38975860936930995}
episode index:1728
target Thresh 31.99999919900221
target distance 9.0
model initialize at round 1728
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 23.]), 'previousTarget': array([10., 23.]), 'currentState': array([19.76116292, 27.49899397,  4.17170191]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 10.748081146289445}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9253444761797218
{'scaleFactor': 20, 'currentTarget': array([10., 23.]), 'previousTarget': array([10., 23.]), 'currentState': array([10.93740151, 23.12585491,  3.42192671]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 0.9458123722670202}
episode index:1729
target Thresh 31.99999920697227
target distance 11.0
model initialize at round 1729
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 13.]), 'previousTarget': array([ 2., 13.]), 'currentState': array([11.32677488,  2.81915263,  2.23925781]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 13.807185911233576}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.925348361076674
{'scaleFactor': 20, 'currentTarget': array([ 2., 13.]), 'previousTarget': array([ 2., 13.]), 'currentState': array([ 1.95937102, 12.99915307,  1.77765505]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 0.04063780438931119}
episode index:1730
target Thresh 31.999999214863028
target distance 21.0
model initialize at round 1730
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 13.]), 'previousTarget': array([26., 13.]), 'currentState': array([ 3.9091543 , 24.71842138,  5.01721406]), 'targetState': array([26, 13], dtype=int32), 'currentDistance': 25.00653641280211}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9253156617420075
{'scaleFactor': 20, 'currentTarget': array([26., 13.]), 'previousTarget': array([26., 13.]), 'currentState': array([25.99108567, 13.70958005,  5.25708887]), 'targetState': array([26, 13], dtype=int32), 'currentDistance': 0.7096360423758458}
episode index:1731
target Thresh 31.99999922267527
target distance 8.0
model initialize at round 1731
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 7.]), 'previousTarget': array([5., 7.]), 'currentState': array([12.25696447, 15.51006168,  3.03806889]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 11.18412639677896}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9253195587894468
{'scaleFactor': 20, 'currentTarget': array([5., 7.]), 'previousTarget': array([5., 7.]), 'currentState': array([4.66503729, 6.40396679, 4.69742751]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 0.6837072489728722}
episode index:1732
target Thresh 31.99999923040978
target distance 14.0
model initialize at round 1732
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'previousTarget': array([16., 23.]), 'currentState': array([20.44934229,  8.34589853,  2.53076839]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 15.314677168363575}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.925318073005049
{'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'previousTarget': array([16., 23.]), 'currentState': array([15.91577476, 23.39854573,  1.22499352]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 0.4073482402140152}
episode index:1733
target Thresh 31.999999238067332
target distance 7.0
model initialize at round 1733
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  2.]), 'previousTarget': array([13.,  2.]), 'currentState': array([18.00821619,  1.84221381,  3.29842825]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 5.010701135468717}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9253440135627162
{'scaleFactor': 20, 'currentTarget': array([13.,  2.]), 'previousTarget': array([13.,  2.]), 'currentState': array([12.64785997,  2.58936541,  1.69853755]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 0.6865523908241046}
episode index:1734
target Thresh 31.999999245648688
target distance 19.0
model initialize at round 1734
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  7.]), 'previousTarget': array([22.,  7.]), 'currentState': array([4.41946011, 5.09583128, 0.44283169]), 'targetState': array([22,  7], dtype=int32), 'currentDistance': 17.68336057370666}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9253371969828436
{'scaleFactor': 20, 'currentTarget': array([22.,  7.]), 'previousTarget': array([22.,  7.]), 'currentState': array([22.13498533,  7.2212832 ,  6.05943598]), 'targetState': array([22,  7], dtype=int32), 'currentDistance': 0.25920511624581277}
episode index:1735
target Thresh 31.99999925315461
target distance 13.0
model initialize at round 1735
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  5.]), 'previousTarget': array([21.,  5.]), 'currentState': array([ 9.95521831, 10.59756844,  5.97237574]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 12.382244344880188}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9253464959185683
{'scaleFactor': 20, 'currentTarget': array([21.,  5.]), 'previousTarget': array([21.,  5.]), 'currentState': array([20.74123656,  5.3774516 ,  5.6470074 ]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 0.457633287704539}
episode index:1736
target Thresh 31.999999260585845
target distance 6.0
model initialize at round 1736
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([8.2695923 , 7.48379003, 3.25346184]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 5.483383601677277}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9253723753106705
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.59852874, 9.081527  , 2.48657267]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.40966550478694447}
episode index:1737
target Thresh 31.99999926794314
target distance 16.0
model initialize at round 1737
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  9.]), 'previousTarget': array([24.,  9.]), 'currentState': array([25.05343762, 23.68749961,  4.37872477]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 14.725229220946947}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.925376226273039
{'scaleFactor': 20, 'currentTarget': array([24.,  9.]), 'previousTarget': array([24.,  9.]), 'currentState': array([24.19125472,  9.89486243,  4.45202276]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 0.9150722031377527}
episode index:1738
target Thresh 31.99999927522723
target distance 7.0
model initialize at round 1738
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([17.6379792 , 16.43524311,  2.58610886]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 7.9216998684262085}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.925396479167649
{'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([12.05299288, 22.10711697,  2.09602032]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 0.11950853666160406}
episode index:1739
target Thresh 31.999999282438836
target distance 21.0
model initialize at round 1739
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 23.]), 'previousTarget': array([23., 23.]), 'currentState': array([ 0.67812514, 19.9583499 ,  4.8189795 ]), 'targetState': array([23, 23], dtype=int32), 'currentDistance': 22.528154224590583}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9253740587035963
{'scaleFactor': 20, 'currentTarget': array([23., 23.]), 'previousTarget': array([23., 23.]), 'currentState': array([23.41176922, 23.14924135,  6.10845252]), 'targetState': array([23, 23], dtype=int32), 'currentDistance': 0.43798044469076214}
episode index:1740
target Thresh 31.99999928957869
target distance 18.0
model initialize at round 1740
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 13.]), 'previousTarget': array([26., 13.]), 'currentState': array([9.61284393, 3.48075205, 1.29969185]), 'targetState': array([26, 13], dtype=int32), 'currentDistance': 18.951384263686762}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9253620012747078
{'scaleFactor': 20, 'currentTarget': array([26., 13.]), 'previousTarget': array([26., 13.]), 'currentState': array([26.47464115, 13.3553676 ,  6.27038889]), 'targetState': array([26, 13], dtype=int32), 'currentDistance': 0.592933677958013}
episode index:1741
target Thresh 31.9999992966475
target distance 14.0
model initialize at round 1741
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 27.]), 'previousTarget': array([ 5., 27.]), 'currentState': array([14.01938647, 14.68285836,  2.56927687]), 'targetState': array([ 5, 27], dtype=int32), 'currentDistance': 15.26634568592745}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9253604988023503
{'scaleFactor': 20, 'currentTarget': array([ 5., 27.]), 'previousTarget': array([ 5., 27.]), 'currentState': array([ 4.53723584, 27.46646471,  2.02035785]), 'targetState': array([ 5, 27], dtype=int32), 'currentDistance': 0.6570692439867225}
episode index:1742
target Thresh 31.999999303645975
target distance 16.0
model initialize at round 1742
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'previousTarget': array([16., 23.]), 'currentState': array([9.61488052, 7.95594566, 1.5275054 ]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 16.342989988150197}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.925358998054
{'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'previousTarget': array([16., 23.]), 'currentState': array([15.71501016, 22.60321757,  0.96569932]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 0.4885238043029525}
episode index:1743
target Thresh 31.99999931057481
target distance 13.0
model initialize at round 1743
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  8.]), 'previousTarget': array([20.,  8.]), 'currentState': array([8.64733751, 3.34448106, 1.21614331]), 'targetState': array([20,  8], dtype=int32), 'currentDistance': 12.270159009396455}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9253682418334421
{'scaleFactor': 20, 'currentTarget': array([20.,  8.]), 'previousTarget': array([20.,  8.]), 'currentState': array([19.4521668 ,  7.96637109,  0.20530407]), 'targetState': array([20,  8], dtype=int32), 'currentDistance': 0.5488643852778602}
episode index:1744
target Thresh 31.999999317434707
target distance 3.0
model initialize at round 1744
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 21.]), 'previousTarget': array([24., 21.]), 'currentState': array([21.64391339, 19.44508404,  6.11500359]), 'targetState': array([24, 21], dtype=int32), 'currentDistance': 2.8229253853744964}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9253996067378355
{'scaleFactor': 20, 'currentTarget': array([24., 21.]), 'previousTarget': array([24., 21.]), 'currentState': array([23.9979844 , 21.86875942,  6.07250786]), 'targetState': array([24, 21], dtype=int32), 'currentDistance': 0.8687617579682222}
episode index:1745
target Thresh 31.999999324226348
target distance 10.0
model initialize at round 1745
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 19.]), 'previousTarget': array([ 8., 19.]), 'currentState': array([ 3.22809045, 27.42351776,  5.30366467]), 'targetState': array([ 8, 19], dtype=int32), 'currentDistance': 9.681258813281222}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9254142633490395
{'scaleFactor': 20, 'currentTarget': array([ 8., 19.]), 'previousTarget': array([ 8., 19.]), 'currentState': array([ 8.05518214, 18.69908555,  4.98243376]), 'targetState': array([ 8, 19], dtype=int32), 'currentDistance': 0.30593230561478535}
episode index:1746
target Thresh 31.999999330950406
target distance 13.0
model initialize at round 1746
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  9.]), 'previousTarget': array([22.,  9.]), 'currentState': array([9.37092767, 4.35841517, 5.94461417]), 'targetState': array([22,  9], dtype=int32), 'currentDistance': 13.455027969595026}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9254180704953233
{'scaleFactor': 20, 'currentTarget': array([22.,  9.]), 'previousTarget': array([22.,  9.]), 'currentState': array([21.43625121,  9.07699214,  0.05207657]), 'targetState': array([22,  9], dtype=int32), 'currentDistance': 0.5689819715257968}
episode index:1747
target Thresh 31.999999337607562
target distance 13.0
model initialize at round 1747
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 26.]), 'previousTarget': array([ 8., 26.]), 'currentState': array([19.67988958, 12.95611468,  2.80067158]), 'targetState': array([ 8, 26], dtype=int32), 'currentDistance': 17.508933856262473}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9254112622441726
{'scaleFactor': 20, 'currentTarget': array([ 8., 26.]), 'previousTarget': array([ 8., 26.]), 'currentState': array([ 7.81625765, 26.19196286,  2.018174  ]), 'targetState': array([ 8, 26], dtype=int32), 'currentDistance': 0.2657272929818035}
episode index:1748
target Thresh 31.999999344198475
target distance 11.0
model initialize at round 1748
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 22.]), 'previousTarget': array([ 6., 22.]), 'currentState': array([16.22194338, 25.49231899,  3.06139672]), 'targetState': array([ 6, 22], dtype=int32), 'currentDistance': 10.802056209078648}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9254258870512941
{'scaleFactor': 20, 'currentTarget': array([ 6., 22.]), 'previousTarget': array([ 6., 22.]), 'currentState': array([ 6.9621544 , 21.97857607,  3.38574172]), 'targetState': array([ 6, 22], dtype=int32), 'currentDistance': 0.9623928867025716}
episode index:1749
target Thresh 31.99999935072381
target distance 13.0
model initialize at round 1749
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 12.]), 'previousTarget': array([13., 12.]), 'currentState': array([12.04222848, 23.31755986,  5.74748325]), 'targetState': array([13, 12], dtype=int32), 'currentDistance': 11.35801423527341}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.925435060915494
{'scaleFactor': 20, 'currentTarget': array([13., 12.]), 'previousTarget': array([13., 12.]), 'currentState': array([13.04651485, 11.73665525,  4.43657558]), 'targetState': array([13, 12], dtype=int32), 'currentDistance': 0.26742117839118806}
episode index:1750
target Thresh 31.999999357184215
target distance 10.0
model initialize at round 1750
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 24.]), 'previousTarget': array([17., 24.]), 'currentState': array([25.6272554 , 16.97363256,  3.53467906]), 'targetState': array([17, 24], dtype=int32), 'currentDistance': 11.12651676086813}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9254388474871624
{'scaleFactor': 20, 'currentTarget': array([17., 24.]), 'previousTarget': array([17., 24.]), 'currentState': array([16.36732959, 24.28069866,  1.22376549]), 'targetState': array([17, 24], dtype=int32), 'currentDistance': 0.6921441970399672}
episode index:1751
target Thresh 31.99999936358034
target distance 21.0
model initialize at round 1751
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 22.]), 'previousTarget': array([26., 22.]), 'currentState': array([4.90758267, 7.68043064, 0.6157372 ]), 'targetState': array([26, 22], dtype=int32), 'currentDistance': 25.493923505182675}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9254114971307195
{'scaleFactor': 20, 'currentTarget': array([26., 22.]), 'previousTarget': array([26., 22.]), 'currentState': array([25.87336236, 21.78200097,  0.16936474]), 'targetState': array([26, 22], dtype=int32), 'currentDistance': 0.25211241579588645}
episode index:1752
target Thresh 31.99999936991282
target distance 16.0
model initialize at round 1752
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  4.]), 'previousTarget': array([19.,  4.]), 'currentState': array([ 4.61979937, 15.45676918,  5.5480395 ]), 'targetState': array([19,  4], dtype=int32), 'currentDistance': 18.38607435493432}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9254047120482054
{'scaleFactor': 20, 'currentTarget': array([19.,  4.]), 'previousTarget': array([19.,  4.]), 'currentState': array([18.71461765,  4.43519663,  5.38774669]), 'targetState': array([19,  4], dtype=int32), 'currentDistance': 0.5204221290468992}
episode index:1753
target Thresh 31.999999376182295
target distance 24.0
model initialize at round 1753
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 25.]), 'previousTarget': array([26., 25.]), 'currentState': array([ 2.21812055, 27.66877545,  0.43082619]), 'targetState': array([26, 25], dtype=int32), 'currentDistance': 23.931154439629253}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9253824658450515
{'scaleFactor': 20, 'currentTarget': array([26., 25.]), 'previousTarget': array([26., 25.]), 'currentState': array([25.72948215, 25.02617826,  5.83888145]), 'targetState': array([26, 25], dtype=int32), 'currentDistance': 0.2717815465798225}
episode index:1754
target Thresh 31.999999382389383
target distance 12.0
model initialize at round 1754
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 13.]), 'previousTarget': array([ 4., 13.]), 'currentState': array([ 3.07279696, 23.5954778 ,  5.13890767]), 'targetState': array([ 4, 13], dtype=int32), 'currentDistance': 10.635969881295118}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9253970570610371
{'scaleFactor': 20, 'currentTarget': array([ 4., 13.]), 'previousTarget': array([ 4., 13.]), 'currentState': array([ 4.07592835, 13.7221529 ,  4.62005591]), 'targetState': array([ 4, 13], dtype=int32), 'currentDistance': 0.7261335440430665}
episode index:1755
target Thresh 31.99999938853471
target distance 24.0
model initialize at round 1755
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  3.]), 'previousTarget': array([22.,  3.]), 'currentState': array([10.26889286, 25.48412717,  5.27300048]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 25.360497810307184}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9253697928047375
{'scaleFactor': 20, 'currentTarget': array([22.,  3.]), 'previousTarget': array([22.,  3.]), 'currentState': array([21.88431839,  2.7272931 ,  4.71416058]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 0.29622844247951663}
episode index:1756
target Thresh 31.999999394618893
target distance 6.0
model initialize at round 1756
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'previousTarget': array([16., 23.]), 'currentState': array([17.43750158, 28.12479871,  4.72629479]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 5.322590781798253}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9253953643512346
{'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'previousTarget': array([16., 23.]), 'currentState': array([15.53520369, 22.50303552,  4.21853204]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 0.6804478691304556}
episode index:1757
target Thresh 31.999999400642537
target distance 11.0
model initialize at round 1757
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 12.]), 'previousTarget': array([16., 12.]), 'currentState': array([26.36678613, 19.55930378,  2.96653956]), 'targetState': array([16, 12], dtype=int32), 'currentDistance': 12.830172573790838}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9253991584260671
{'scaleFactor': 20, 'currentTarget': array([16., 12.]), 'previousTarget': array([16., 12.]), 'currentState': array([16.59606307, 12.42528333,  3.14921607]), 'targetState': array([16, 12], dtype=int32), 'currentDistance': 0.732227488453706}
episode index:1758
target Thresh 31.999999406606243
target distance 18.0
model initialize at round 1758
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([25.14990621, 23.33906369,  3.15470511]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 16.1534650974521}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.925397649350457
{'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([ 9.23318988, 23.18476437,  2.89917663]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 0.2975153702092202}
episode index:1759
target Thresh 31.999999412510608
target distance 5.0
model initialize at round 1759
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 4.]), 'previousTarget': array([8., 4.]), 'currentState': array([4.55939979, 3.36702261, 0.62440842]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 3.498341059087071}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9254287302315078
{'scaleFactor': 20, 'currentTarget': array([8., 4.]), 'previousTarget': array([8., 4.]), 'currentState': array([8.39715116, 3.96318498, 6.07814953]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 0.3988538471735203}
episode index:1760
target Thresh 31.999999418356225
target distance 8.0
model initialize at round 1760
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 3.]), 'previousTarget': array([5., 3.]), 'currentState': array([13.3491366 ,  4.64635711,  2.37182608]), 'targetState': array([5, 3], dtype=int32), 'currentDistance': 8.509910323137325}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9254432454613025
{'scaleFactor': 20, 'currentTarget': array([5., 3.]), 'previousTarget': array([5., 3.]), 'currentState': array([5.1513442 , 2.73002974, 4.36454406]), 'targetState': array([5, 3], dtype=int32), 'currentDistance': 0.3094979927599792}
episode index:1761
target Thresh 31.999999424143677
target distance 24.0
model initialize at round 1761
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 13.]), 'previousTarget': array([26., 13.]), 'currentState': array([ 3.74747521, 27.54399803,  0.52696317]), 'targetState': array([26, 13], dtype=int32), 'currentDistance': 26.583881174137826}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.925401255975283
{'scaleFactor': 20, 'currentTarget': array([26., 13.]), 'previousTarget': array([26., 13.]), 'currentState': array([26.17455767, 12.41502899,  5.28208244]), 'targetState': array([26, 13], dtype=int32), 'currentDistance': 0.6104600396734596}
episode index:1762
target Thresh 31.999999429873544
target distance 19.0
model initialize at round 1762
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 5.]), 'previousTarget': array([6., 5.]), 'currentState': array([ 4.68295972, 23.99411166,  5.26968652]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 19.039718302655988}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9253893335810875
{'scaleFactor': 20, 'currentTarget': array([6., 5.]), 'previousTarget': array([6., 5.]), 'currentState': array([5.88772719, 4.33903491, 4.62114873]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 0.6704327183496965}
episode index:1763
target Thresh 31.999999435546396
target distance 7.0
model initialize at round 1763
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 29.]), 'previousTarget': array([ 4., 29.]), 'currentState': array([ 8.65994788, 22.27741864,  1.1755945 ]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 8.179744150833857}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9254038464588193
{'scaleFactor': 20, 'currentTarget': array([ 4., 29.]), 'previousTarget': array([ 4., 29.]), 'currentState': array([ 3.86230921, 29.19584674,  3.17319864]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 0.23940488553630757}
episode index:1764
target Thresh 31.999999441162803
target distance 19.0
model initialize at round 1764
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 2.]), 'previousTarget': array([9., 2.]), 'currentState': array([26.87740673, 19.5638435 ,  4.25081444]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 25.061729187654887}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9253767173803717
{'scaleFactor': 20, 'currentTarget': array([9., 2.]), 'previousTarget': array([9., 2.]), 'currentState': array([8.52434987, 1.82416947, 3.5238844 ]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 0.5071088908376067}
episode index:1765
target Thresh 31.999999446723326
target distance 21.0
model initialize at round 1765
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 24.]), 'previousTarget': array([25., 24.]), 'currentState': array([5.74010633, 4.67390377, 1.88483325]), 'targetState': array([25, 24], dtype=int32), 'currentDistance': 27.284455271445598}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.9253348606723958
{'scaleFactor': 20, 'currentTarget': array([25., 24.]), 'previousTarget': array([25., 24.]), 'currentState': array([24.91737565, 23.86777231,  6.24408305]), 'targetState': array([25, 24], dtype=int32), 'currentDistance': 0.1559196735689161}
episode index:1766
target Thresh 31.999999452228522
target distance 12.0
model initialize at round 1766
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  5.]), 'previousTarget': array([17.,  5.]), 'currentState': array([13.28021698, 15.47871749,  5.2804575 ]), 'targetState': array([17,  5], dtype=int32), 'currentDistance': 11.119366254706689}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9253439977910878
{'scaleFactor': 20, 'currentTarget': array([17.,  5.]), 'previousTarget': array([17.,  5.]), 'currentState': array([17.1580469 ,  4.18171561,  4.97912422]), 'targetState': array([17,  5], dtype=int32), 'currentDistance': 0.8334075633989712}
episode index:1767
target Thresh 31.99999945767894
target distance 23.0
model initialize at round 1767
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 25.]), 'previousTarget': array([26., 25.]), 'currentState': array([22.24309277,  0.86549198,  0.27024   ]), 'targetState': array([26, 25], dtype=int32), 'currentDistance': 24.425167952422235}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9253169485972008
{'scaleFactor': 20, 'currentTarget': array([26., 25.]), 'previousTarget': array([26., 25.]), 'currentState': array([26.34191018, 25.64971441,  1.08469353]), 'targetState': array([26, 25], dtype=int32), 'currentDistance': 0.7341875655978687}
episode index:1768
target Thresh 31.999999463075124
target distance 6.0
model initialize at round 1768
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  4.]), 'previousTarget': array([20.,  4.]), 'currentState': array([24.31822682,  8.93654097,  4.18930817]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 6.558698010694686}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.92534237655164
{'scaleFactor': 20, 'currentTarget': array([20.,  4.]), 'previousTarget': array([20.,  4.]), 'currentState': array([20.40366985,  4.47655738,  3.65972608]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 0.6245448644742104}
episode index:1769
target Thresh 31.999999468417617
target distance 18.0
model initialize at round 1769
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 18.]), 'previousTarget': array([23., 18.]), 'currentState': array([ 6.94699049, 25.54980064,  6.12675974]), 'targetState': array([23, 18], dtype=int32), 'currentDistance': 17.739746447886095}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9253356956877598
{'scaleFactor': 20, 'currentTarget': array([23., 18.]), 'previousTarget': array([23., 18.]), 'currentState': array([22.98928438, 17.79139422,  5.47488601]), 'targetState': array([23, 18], dtype=int32), 'currentDistance': 0.20888081722508056}
episode index:1770
target Thresh 31.99999947370695
target distance 16.0
model initialize at round 1770
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([24.29799949, 23.22698943,  4.87290514]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 17.562916093242873}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9253290223686158
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.99820403,  9.29120565,  3.51245204]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.2912111930663546}
episode index:1771
target Thresh 31.999999478943653
target distance 19.0
model initialize at round 1771
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 15.]), 'previousTarget': array([22., 15.]), 'currentState': array([ 3.72655404, 19.48193962,  6.16877604]), 'targetState': array([22, 15], dtype=int32), 'currentDistance': 18.81506338230955}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9253223565814347
{'scaleFactor': 20, 'currentTarget': array([22., 15.]), 'previousTarget': array([22., 15.]), 'currentState': array([21.06157427, 14.99077049,  5.7842426 ]), 'targetState': array([22, 15], dtype=int32), 'currentDistance': 0.9384711117132292}
episode index:1772
target Thresh 31.99999948412825
target distance 19.0
model initialize at round 1772
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([ 8.68173817, 28.06438013,  5.31144852]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 18.703680366259313}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9253105459319294
{'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([26.5548426 , 20.42363891,  5.7361696 ]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 0.8000265098750373}
episode index:1773
target Thresh 31.99999948926126
target distance 18.0
model initialize at round 1773
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 23.]), 'previousTarget': array([ 7., 23.]), 'currentState': array([23.55467507, 10.86222034,  3.61370778]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 20.527614609000782}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9252886036127548
{'scaleFactor': 20, 'currentTarget': array([ 7., 23.]), 'previousTarget': array([ 7., 23.]), 'currentState': array([ 7.09206378, 23.01560647,  1.94060033]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 0.09337719732918404}
episode index:1774
target Thresh 31.999999494343196
target distance 5.0
model initialize at round 1774
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 11.]), 'previousTarget': array([25., 11.]), 'currentState': array([25.35780099,  7.88992261,  1.78841262]), 'targetState': array([25, 11], dtype=int32), 'currentDistance': 3.13059146800322}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9253194832726912
{'scaleFactor': 20, 'currentTarget': array([25., 11.]), 'previousTarget': array([25., 11.]), 'currentState': array([24.96559847, 11.86573646,  1.65633946]), 'targetState': array([25, 11], dtype=int32), 'currentDistance': 0.8664196894577734}
episode index:1775
target Thresh 31.999999499374564
target distance 25.0
model initialize at round 1775
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 28.]), 'previousTarget': array([10., 28.]), 'currentState': array([25.08263888,  4.28851898,  1.88200777]), 'targetState': array([10, 28], dtype=int32), 'currentDistance': 28.101963060390574}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.9252827371417051
{'scaleFactor': 20, 'currentTarget': array([10., 28.]), 'previousTarget': array([10., 28.]), 'currentState': array([10.51908176, 27.49858424,  1.68348575]), 'targetState': array([10, 28], dtype=int32), 'currentDistance': 0.7217088320150133}
episode index:1776
target Thresh 31.99999950435587
target distance 6.0
model initialize at round 1776
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 23.]), 'previousTarget': array([21., 23.]), 'currentState': array([24.62162294, 17.96913119,  3.53542638]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 6.198854230256793}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9253080698726327
{'scaleFactor': 20, 'currentTarget': array([21., 23.]), 'previousTarget': array([21., 23.]), 'currentState': array([21.34348665, 22.32298552,  1.93277061]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 0.7591651189183689}
episode index:1777
target Thresh 31.999999509287612
target distance 19.0
model initialize at round 1777
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 23.]), 'previousTarget': array([22., 23.]), 'currentState': array([4.02397546, 8.54049182, 0.29819787]), 'targetState': array([22, 23], dtype=int32), 'currentDistance': 23.069781862450807}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.925286178310115
{'scaleFactor': 20, 'currentTarget': array([22., 23.]), 'previousTarget': array([22., 23.]), 'currentState': array([22.32879128, 22.85050393,  0.43663564]), 'targetState': array([22, 23], dtype=int32), 'currentDistance': 0.3611824728553505}
episode index:1778
target Thresh 31.99999951417028
target distance 20.0
model initialize at round 1778
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 19.]), 'previousTarget': array([23., 19.]), 'currentState': array([2.13573845, 6.44410528, 1.10009408]), 'targetState': array([23, 19], dtype=int32), 'currentDistance': 24.350932272004982}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9252643113586848
{'scaleFactor': 20, 'currentTarget': array([23., 19.]), 'previousTarget': array([23., 19.]), 'currentState': array([22.23104414, 18.63780131,  0.09394955]), 'targetState': array([23, 19], dtype=int32), 'currentDistance': 0.8499888204499731}
episode index:1779
target Thresh 31.99999951900437
target distance 12.0
model initialize at round 1779
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  9.]), 'previousTarget': array([22.,  9.]), 'currentState': array([11.36476597, 14.01521478,  0.38493841]), 'targetState': array([22,  9], dtype=int32), 'currentDistance': 11.75842600615132}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.925273421380057
{'scaleFactor': 20, 'currentTarget': array([22.,  9.]), 'previousTarget': array([22.,  9.]), 'currentState': array([21.76395254,  8.79119177,  5.5571702 ]), 'targetState': array([22,  9], dtype=int32), 'currentDistance': 0.3151496169705761}
episode index:1780
target Thresh 31.999999523790354
target distance 15.0
model initialize at round 1780
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  7.]), 'previousTarget': array([20.,  7.]), 'currentState': array([23.67840939, 21.87618561,  5.1995498 ]), 'targetState': array([20,  7], dtype=int32), 'currentDistance': 15.324215930177088}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9252720015445981
{'scaleFactor': 20, 'currentTarget': array([20.,  7.]), 'previousTarget': array([20.,  7.]), 'currentState': array([19.57233989,  6.70153756,  4.17818694]), 'targetState': array([20,  7], dtype=int32), 'currentDistance': 0.5215103090645251}
episode index:1781
target Thresh 31.99999952852872
target distance 8.0
model initialize at round 1781
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([12.91932378,  6.40969212,  2.00291462]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 6.67832456956701}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.925297269220499
{'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([14.00050588, 12.11529524,  1.06095655]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 0.8847049006178982}
episode index:1782
target Thresh 31.99999953321994
target distance 16.0
model initialize at round 1782
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 11.]), 'previousTarget': array([24., 11.]), 'currentState': array([ 9.10149594, 19.27243655,  6.13046676]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 17.04108651989598}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9252906623659074
{'scaleFactor': 20, 'currentTarget': array([24., 11.]), 'previousTarget': array([24., 11.]), 'currentState': array([24.41056127, 10.37846487,  5.55287955]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 0.7448935971897387}
episode index:1783
target Thresh 31.999999537864475
target distance 9.0
model initialize at round 1783
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([23.63489778, 16.00010737,  1.51496792]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 9.71032315360172}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9252997371904788
{'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([15.46239185, 21.93493838,  1.50769472]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 0.5415307310641886}
episode index:1784
target Thresh 31.999999542462803
target distance 15.0
model initialize at round 1784
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 21.]), 'previousTarget': array([ 8., 21.]), 'currentState': array([23.74510652, 30.50904087,  2.12215184]), 'targetState': array([ 8, 21], dtype=int32), 'currentDistance': 18.393755398168267}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9252931363559092
{'scaleFactor': 20, 'currentTarget': array([ 8., 21.]), 'previousTarget': array([ 8., 21.]), 'currentState': array([ 8.99865243, 21.66792247,  3.43278726]), 'targetState': array([ 8, 21], dtype=int32), 'currentDistance': 1.201427115274393}
episode index:1785
target Thresh 31.999999547015374
target distance 16.0
model initialize at round 1785
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([23.12042676, 17.38272293,  3.44437003]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 14.576389996550503}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9252969281876846
{'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([ 9.80252954, 21.07543538,  2.66125899]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 0.8060670914457259}
episode index:1786
target Thresh 31.999999551522645
target distance 12.0
model initialize at round 1786
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  9.]), 'previousTarget': array([24.,  9.]), 'currentState': array([13.08059256, 13.70976444,  0.13640517]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 11.891818194223756}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9253059842711839
{'scaleFactor': 20, 'currentTarget': array([24.,  9.]), 'previousTarget': array([24.,  9.]), 'currentState': array([23.68140366,  9.07154418,  5.24987765]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 0.3265305432250874}
episode index:1787
target Thresh 31.99999955598507
target distance 13.0
model initialize at round 1787
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  5.]), 'previousTarget': array([25.,  5.]), 'currentState': array([13.68296652,  7.00381529,  5.27545369]), 'targetState': array([25,  5], dtype=int32), 'currentDistance': 11.493064104156621}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.925315030224836
{'scaleFactor': 20, 'currentTarget': array([25.,  5.]), 'previousTarget': array([25.,  5.]), 'currentState': array([24.02630489,  5.89965736,  5.5531497 ]), 'targetState': array([25,  5], dtype=int32), 'currentDistance': 1.3256943611515999}
episode index:1788
target Thresh 31.999999560403094
target distance 20.0
model initialize at round 1788
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([10.59436181, 21.46111366,  4.94724512]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 23.631463771303427}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9252932693760328
{'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([24.30504532,  2.90092529,  4.69801917]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 0.9511671919999163}
episode index:1789
target Thresh 31.999999564777156
target distance 14.0
model initialize at round 1789
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([7.01032986, 8.68293832, 0.55465841]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 15.85886244364291}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9252918455911456
{'scaleFactor': 20, 'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([16.54015245, 21.03320861,  0.31884588]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 0.4610450923919858}
episode index:1790
target Thresh 31.999999569107693
target distance 7.0
model initialize at round 1790
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 22.]), 'previousTarget': array([11., 22.]), 'currentState': array([ 2.62691638, 25.02684562,  4.76815796]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 8.903388329426585}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9253061941139311
{'scaleFactor': 20, 'currentTarget': array([11., 22.]), 'previousTarget': array([11., 22.]), 'currentState': array([11.26123642, 22.01292184,  5.24826513]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 0.261555806011242}
episode index:1791
target Thresh 31.999999573395144
target distance 10.0
model initialize at round 1791
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 22.]), 'previousTarget': array([11., 22.]), 'currentState': array([20.12464875, 27.43741027,  3.12779117]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 10.621894619634487}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9253205266227402
{'scaleFactor': 20, 'currentTarget': array([11., 22.]), 'previousTarget': array([11., 22.]), 'currentState': array([11.8424313 , 22.09698084,  3.49231965]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 0.8479951536128085}
episode index:1792
target Thresh 31.999999577639933
target distance 10.0
model initialize at round 1792
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 23.]), 'previousTarget': array([ 2., 23.]), 'currentState': array([ 5.91218526, 14.41432179,  2.00797019]), 'targetState': array([ 2, 23], dtype=int32), 'currentDistance': 9.434991457248437}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9253348431443672
{'scaleFactor': 20, 'currentTarget': array([ 2., 23.]), 'previousTarget': array([ 2., 23.]), 'currentState': array([ 1.62401409, 23.3382668 ,  1.59531287]), 'targetState': array([ 2, 23], dtype=int32), 'currentDistance': 0.5057566962170468}
episode index:1793
target Thresh 31.999999581842488
target distance 16.0
model initialize at round 1793
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 29.]), 'previousTarget': array([26., 29.]), 'currentState': array([14.0080466 , 14.34767583,  1.93858355]), 'targetState': array([26, 29], dtype=int32), 'currentDistance': 18.9340315305891}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9253181226377419
{'scaleFactor': 20, 'currentTarget': array([26., 29.]), 'previousTarget': array([26., 29.]), 'currentState': array([25.70048562, 29.0748829 ,  0.41491711]), 'targetState': array([26, 29], dtype=int32), 'currentDistance': 0.3087334034371326}
episode index:1794
target Thresh 31.999999586003224
target distance 4.0
model initialize at round 1794
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([24.62487678, 14.56266354,  2.2003939 ]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 2.4660345595652813}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9253541571098101
{'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([25.02665176, 16.27381531,  0.46257959]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 0.7266736033026551}
episode index:1795
target Thresh 31.99999959012256
target distance 8.0
model initialize at round 1795
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  9.]), 'previousTarget': array([24.,  9.]), 'currentState': array([15.35474299, 10.44564114,  5.32891011]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 8.765292180763948}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9253684309922099
{'scaleFactor': 20, 'currentTarget': array([24.,  9.]), 'previousTarget': array([24.,  9.]), 'currentState': array([24.14660784,  8.90939227,  5.67254972]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 0.17234738306819067}
episode index:1796
target Thresh 31.999999594200908
target distance 19.0
model initialize at round 1796
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  9.]), 'previousTarget': array([22.,  9.]), 'currentState': array([2.41794089, 5.42088782, 5.36924195]), 'targetState': array([22,  9], dtype=int32), 'currentDistance': 19.90645832628659}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9253567524413009
{'scaleFactor': 20, 'currentTarget': array([22.,  9.]), 'previousTarget': array([22.,  9.]), 'currentState': array([21.20011877,  9.16680382,  6.09695058]), 'targetState': array([22,  9], dtype=int32), 'currentDistance': 0.8170884293001025}
episode index:1797
target Thresh 31.999999598238677
target distance 13.0
model initialize at round 1797
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 26.]), 'previousTarget': array([ 6., 26.]), 'currentState': array([ 3.63214255, 13.98048666,  1.50967717]), 'targetState': array([ 6, 26], dtype=int32), 'currentDistance': 12.250528554336086}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9253657198478413
{'scaleFactor': 20, 'currentTarget': array([ 6., 26.]), 'previousTarget': array([ 6., 26.]), 'currentState': array([ 5.97935159, 25.73328165,  1.32209768]), 'targetState': array([ 6, 26], dtype=int32), 'currentDistance': 0.26751642179365576}
episode index:1798
target Thresh 31.99999960223627
target distance 12.0
model initialize at round 1798
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([12.34148108, 13.89729424,  3.88837543]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 11.904801986433478}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9253746772850583
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.93261998, 7.98726476, 3.58322122]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.06857297932783425}
episode index:1799
target Thresh 31.999999606194084
target distance 17.0
model initialize at round 1799
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 20.]), 'previousTarget': array([ 6., 20.]), 'currentState': array([22.00398059, 24.35658889,  3.21411419]), 'targetState': array([ 6, 20], dtype=int32), 'currentDistance': 16.586357687993093}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9253732161834709
{'scaleFactor': 20, 'currentTarget': array([ 6., 20.]), 'previousTarget': array([ 6., 20.]), 'currentState': array([ 6.90101748, 20.25888351,  3.11668255]), 'targetState': array([ 6, 20], dtype=int32), 'currentDistance': 0.9374716932764182}
episode index:1800
target Thresh 31.999999610112518
target distance 5.0
model initialize at round 1800
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 15.]), 'previousTarget': array([17., 15.]), 'currentState': array([21.44055464, 18.41273538,  3.36352539]), 'targetState': array([17, 15], dtype=int32), 'currentDistance': 5.600472150883072}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9253981610939742
{'scaleFactor': 20, 'currentTarget': array([17., 15.]), 'previousTarget': array([17., 15.]), 'currentState': array([16.78181078, 14.81449242,  4.22646152]), 'targetState': array([17, 15], dtype=int32), 'currentDistance': 0.28639064061515296}
episode index:1801
target Thresh 31.999999613991964
target distance 16.0
model initialize at round 1801
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 22.]), 'previousTarget': array([25., 22.]), 'currentState': array([10.2947741 , 18.92484972,  0.31700915]), 'targetState': array([25, 22], dtype=int32), 'currentDistance': 15.0233224684454}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9253966885819509
{'scaleFactor': 20, 'currentTarget': array([25., 22.]), 'previousTarget': array([25., 22.]), 'currentState': array([25.8075622 , 22.36439544,  0.66659749]), 'targetState': array([25, 22], dtype=int32), 'currentDistance': 0.8859688205267143}
episode index:1802
target Thresh 31.99999961783281
target distance 17.0
model initialize at round 1802
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  7.]), 'previousTarget': array([21.,  7.]), 'currentState': array([16.92019931, 22.59087924,  4.2808919 ]), 'targetState': array([21,  7], dtype=int32), 'currentDistance': 16.115839699141304}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9253900998736324
{'scaleFactor': 20, 'currentTarget': array([21.,  7.]), 'previousTarget': array([21.,  7.]), 'currentState': array([21.14937004,  6.24973135,  5.16086033]), 'targetState': array([21,  7], dtype=int32), 'currentDistance': 0.7649931117469345}
episode index:1803
target Thresh 31.999999621635435
target distance 20.0
model initialize at round 1803
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([6.68219985, 8.0509093 , 1.04025429]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 18.3423710456096}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9253835184698685
{'scaleFactor': 20, 'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([24.21232114,  8.77896799,  0.23697777]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 0.8181033819874686}
episode index:1804
target Thresh 31.999999625400225
target distance 10.0
model initialize at round 1804
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 20.]), 'previousTarget': array([ 3., 20.]), 'currentState': array([ 9.99149114, 11.68294851,  2.58585221]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 10.865279283721565}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9253977049138741
{'scaleFactor': 20, 'currentTarget': array([ 3., 20.]), 'previousTarget': array([ 3., 20.]), 'currentState': array([ 3.82500112, 19.45280298,  2.50654334]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 0.989975468761405}
episode index:1805
target Thresh 31.999999629127554
target distance 15.0
model initialize at round 1805
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([12.89973177, 19.32001953,  3.64277554]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 17.211248446590787}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9253911265875008
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.01273047, 6.16627827, 3.63555304]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.1667648853240036}
episode index:1806
target Thresh 31.999999632817797
target distance 7.0
model initialize at round 1806
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 13.]), 'previousTarget': array([19., 13.]), 'currentState': array([13.3318138 , 18.35006434,  5.92084885]), 'targetState': array([19, 13], dtype=int32), 'currentDistance': 7.794326348331642}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9254106090907729
{'scaleFactor': 20, 'currentTarget': array([19., 13.]), 'previousTarget': array([19., 13.]), 'currentState': array([18.92156318, 12.84202205,  5.8997817 ]), 'targetState': array([19, 13], dtype=int32), 'currentDistance': 0.17637847799572276}
episode index:1807
target Thresh 31.999999636471323
target distance 5.0
model initialize at round 1807
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  6.]), 'previousTarget': array([23.,  6.]), 'currentState': array([21.67190665, 10.80734426,  5.15846014]), 'targetState': array([23,  6], dtype=int32), 'currentDistance': 4.98742325859791}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9254408576476917
{'scaleFactor': 20, 'currentTarget': array([23.,  6.]), 'previousTarget': array([23.,  6.]), 'currentState': array([22.65217694,  6.94925521,  4.82427488]), 'targetState': array([23,  6], dtype=int32), 'currentDistance': 1.0109729622811405}
episode index:1808
target Thresh 31.999999640088493
target distance 12.0
model initialize at round 1808
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 18.]), 'previousTarget': array([14., 18.]), 'currentState': array([19.78580536,  7.6692839 ,  2.70841426]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 11.840575950087235}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9254497240334039
{'scaleFactor': 20, 'currentTarget': array([14., 18.]), 'previousTarget': array([14., 18.]), 'currentState': array([13.88324196, 17.88925411,  2.08626647]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 0.16092573116151074}
episode index:1809
target Thresh 31.99999964366967
target distance 18.0
model initialize at round 1809
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  6.]), 'previousTarget': array([20.,  6.]), 'currentState': array([ 8.67873424, 23.88067088,  5.20222187]), 'targetState': array([20,  6], dtype=int32), 'currentDistance': 21.163398820102657}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9254330878622575
{'scaleFactor': 20, 'currentTarget': array([20.,  6.]), 'previousTarget': array([20.,  6.]), 'currentState': array([20.60150372,  5.61506743,  5.26916383]), 'targetState': array([20,  6], dtype=int32), 'currentDistance': 0.7141287080591403}
episode index:1810
target Thresh 31.99999964721522
target distance 13.0
model initialize at round 1810
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 29.]), 'previousTarget': array([24., 29.]), 'currentState': array([26.83980361, 17.67532839,  2.67612737]), 'targetState': array([24, 29], dtype=int32), 'currentDistance': 11.67530178177669}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.925441948746597
{'scaleFactor': 20, 'currentTarget': array([24., 29.]), 'previousTarget': array([24., 29.]), 'currentState': array([23.83866464, 28.91615986,  1.71800931]), 'targetState': array([24, 29], dtype=int32), 'currentDistance': 0.18181932574310605}
episode index:1811
target Thresh 31.999999650725485
target distance 12.0
model initialize at round 1811
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([10.82377859, 28.50807507,  0.32447594]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 10.480740564191981}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9254560481401695
{'scaleFactor': 20, 'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([20.36147461, 26.21010757,  5.97551988]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 0.6722052287420948}
episode index:1812
target Thresh 31.999999654200824
target distance 9.0
model initialize at round 1812
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 17.]), 'previousTarget': array([14., 17.]), 'currentState': array([16.33137106,  7.78076145,  2.26223278]), 'targetState': array([14, 17], dtype=int32), 'currentDistance': 9.509450590988036}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9254701319800811
{'scaleFactor': 20, 'currentTarget': array([14., 17.]), 'previousTarget': array([14., 17.]), 'currentState': array([13.89711468, 17.32195179,  1.91763287]), 'targetState': array([14, 17], dtype=int32), 'currentDistance': 0.3379916283203844}
episode index:1813
target Thresh 31.999999657641585
target distance 8.0
model initialize at round 1813
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 21.]), 'previousTarget': array([13., 21.]), 'currentState': array([ 9.58057295, 29.90422062,  3.58438778]), 'targetState': array([13, 21], dtype=int32), 'currentDistance': 9.538219236727205}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9254842002920546
{'scaleFactor': 20, 'currentTarget': array([13., 21.]), 'previousTarget': array([13., 21.]), 'currentState': array([12.84559368, 21.20044747,  4.96920136]), 'targetState': array([13, 21], dtype=int32), 'currentDistance': 0.2530227296310143}
episode index:1814
target Thresh 31.999999661048108
target distance 22.0
model initialize at round 1814
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'previousTarget': array([7., 3.]), 'currentState': array([19.68166799, 23.95386961,  4.82237339]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 24.492638785380837}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9254626579622606
{'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'previousTarget': array([7., 3.]), 'currentState': array([7.49144801, 3.75736239, 3.8780917 ]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 0.9028393712085842}
episode index:1815
target Thresh 31.999999664420734
target distance 16.0
model initialize at round 1815
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  3.]), 'previousTarget': array([24.,  3.]), 'currentState': array([ 9.1477681 , 10.23086005,  6.09350199]), 'targetState': array([24,  3], dtype=int32), 'currentDistance': 16.518902183097712}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9254611612863055
{'scaleFactor': 20, 'currentTarget': array([24.,  3.]), 'previousTarget': array([24.,  3.]), 'currentState': array([23.2960331 ,  3.30676736,  5.68260138]), 'targetState': array([24,  3], dtype=int32), 'currentDistance': 0.7679033880665033}
episode index:1816
target Thresh 31.999999667759806
target distance 4.0
model initialize at round 1816
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 28.]), 'previousTarget': array([14., 28.]), 'currentState': array([ 9.58959434, 27.36783724,  5.47604704]), 'targetState': array([14, 28], dtype=int32), 'currentDistance': 4.455480650425923}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9254858381375514
{'scaleFactor': 20, 'currentTarget': array([14., 28.]), 'previousTarget': array([14., 28.]), 'currentState': array([14.58946427, 28.24254964,  1.09495848]), 'targetState': array([14, 28], dtype=int32), 'currentDistance': 0.6374154501368421}
episode index:1817
target Thresh 31.99999967106565
target distance 17.0
model initialize at round 1817
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([12.43336008, 27.61483937,  3.77760541]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 18.433772478262423}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9254742299070075
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 7.5839407 , 10.66707848,  4.03872626]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.8865553787088358}
episode index:1818
target Thresh 31.9999996743386
target distance 9.0
model initialize at round 1818
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([15.78403022,  6.33094484,  5.59370756]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 9.594683654941791}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.925483029203046
{'scaleFactor': 20, 'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([25.38763059,  9.46026346,  5.60249221]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 0.6017473964838661}
episode index:1819
target Thresh 31.999999677578987
target distance 11.0
model initialize at round 1819
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([22.34086835, 19.28225918,  1.96308136]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 10.95191223535821}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9254918188295285
{'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([12.46855659, 25.34488356,  2.7338405 ]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 0.6335430285482996}
episode index:1820
target Thresh 31.999999680787127
target distance 22.0
model initialize at round 1820
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 29.]), 'previousTarget': array([ 2., 29.]), 'currentState': array([23.17197515, 26.46518359,  3.09518433]), 'targetState': array([ 2, 29], dtype=int32), 'currentDistance': 21.323175799434317}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9254752600351457
{'scaleFactor': 20, 'currentTarget': array([ 2., 29.]), 'previousTarget': array([ 2., 29.]), 'currentState': array([ 1.65823152, 29.37904519,  3.10471226]), 'targetState': array([ 2, 29], dtype=int32), 'currentDistance': 0.5103733459173188}
episode index:1821
target Thresh 31.99999968396335
target distance 15.0
model initialize at round 1821
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  5.]), 'previousTarget': array([21.,  5.]), 'currentState': array([7.57840601, 1.41602863, 0.65564173]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 13.891869425235237}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9254788769878745
{'scaleFactor': 20, 'currentTarget': array([21.,  5.]), 'previousTarget': array([21.,  5.]), 'currentState': array([21.00551846,  4.76806673,  0.40308739]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 0.23199891490245278}
episode index:1822
target Thresh 31.999999687107966
target distance 11.0
model initialize at round 1822
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 21.]), 'previousTarget': array([25., 21.]), 'currentState': array([21.42006255, 10.57981525,  1.77986622]), 'targetState': array([25, 21], dtype=int32), 'currentDistance': 11.017994480194083}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9254876544274867
{'scaleFactor': 20, 'currentTarget': array([25., 21.]), 'previousTarget': array([25., 21.]), 'currentState': array([25.22999434, 21.74805507,  1.50827245]), 'targetState': array([25, 21], dtype=int32), 'currentDistance': 0.7826134321994148}
episode index:1823
target Thresh 31.999999690221294
target distance 4.0
model initialize at round 1823
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 22.]), 'previousTarget': array([ 3., 22.]), 'currentState': array([ 5.35638225, 18.9963008 ,  1.6435253 ]), 'targetState': array([ 3, 22], dtype=int32), 'currentDistance': 3.8176885941037857}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.925517595406419
{'scaleFactor': 20, 'currentTarget': array([ 3., 22.]), 'previousTarget': array([ 3., 22.]), 'currentState': array([ 2.83336918, 21.92302644,  2.24373203]), 'targetState': array([ 3, 22], dtype=int32), 'currentDistance': 0.18355042413827682}
episode index:1824
target Thresh 31.999999693303643
target distance 10.0
model initialize at round 1824
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  2.]), 'previousTarget': array([17.,  2.]), 'currentState': array([13.95188506, 10.68324521,  5.050102  ]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 9.202703521031331}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9255315529157305
{'scaleFactor': 20, 'currentTarget': array([17.,  2.]), 'previousTarget': array([17.,  2.]), 'currentState': array([17.33645168,  1.31054612,  5.3441615 ]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 0.7671677690477035}
episode index:1825
target Thresh 31.999999696355324
target distance 5.0
model initialize at round 1825
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 14.]), 'previousTarget': array([17., 14.]), 'currentState': array([20.88012555, 16.25629189,  3.30884671]), 'targetState': array([17, 14], dtype=int32), 'currentDistance': 4.488454900099915}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9255614370598073
{'scaleFactor': 20, 'currentTarget': array([17., 14.]), 'previousTarget': array([17., 14.]), 'currentState': array([17.40124937, 14.39938608,  3.88745967]), 'targetState': array([17, 14], dtype=int32), 'currentDistance': 0.5661362874478171}
episode index:1826
target Thresh 31.99999969937664
target distance 3.0
model initialize at round 1826
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 5.]), 'previousTarget': array([9., 5.]), 'currentState': array([9.53263355, 3.94453789, 1.79051799]), 'targetState': array([9, 5], dtype=int32), 'currentDistance': 1.1822431027217606}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9255967072092
{'scaleFactor': 20, 'currentTarget': array([9., 5.]), 'previousTarget': array([9., 5.]), 'currentState': array([8.69802944, 5.74490417, 2.22357536]), 'targetState': array([9, 5], dtype=int32), 'currentDistance': 0.8037838262238532}
episode index:1827
target Thresh 31.99999970236789
target distance 6.0
model initialize at round 1827
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  5.]), 'previousTarget': array([22.,  5.]), 'currentState': array([17.94295713,  4.47059221,  0.18616179]), 'targetState': array([22,  5], dtype=int32), 'currentDistance': 4.091438556002433}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9256265230148841
{'scaleFactor': 20, 'currentTarget': array([22.,  5.]), 'previousTarget': array([22.,  5.]), 'currentState': array([21.91457959,  4.91775714,  0.17804416]), 'targetState': array([22,  5], dtype=int32), 'currentDistance': 0.11857711966309306}
episode index:1828
target Thresh 31.99999970532938
target distance 20.0
model initialize at round 1828
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  7.]), 'previousTarget': array([20.,  7.]), 'currentState': array([12.40054874, 26.06680585,  4.68543238]), 'targetState': array([20,  7], dtype=int32), 'currentDistance': 20.52546576595822}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.925614907679725
{'scaleFactor': 20, 'currentTarget': array([20.,  7.]), 'previousTarget': array([20.,  7.]), 'currentState': array([19.56970597,  7.72049706,  4.9855754 ]), 'targetState': array([20,  7], dtype=int32), 'currentDistance': 0.839207346411004}
episode index:1829
target Thresh 31.999999708261402
target distance 17.0
model initialize at round 1829
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([24.17329727, 25.24002356,  3.7928673 ]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 15.754578696906309}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9256133392571829
{'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([ 8.91057226, 20.9578135 ,  3.6841857 ]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 0.0988788256975726}
episode index:1830
target Thresh 31.99999971116425
target distance 25.0
model initialize at round 1830
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 17.]), 'previousTarget': array([27., 17.]), 'currentState': array([ 0.82904478, 13.79117745,  4.9529016 ]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 26.366938376424773}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9255822810777793
{'scaleFactor': 20, 'currentTarget': array([27., 17.]), 'previousTarget': array([27., 17.]), 'currentState': array([27.66156713, 17.23938303,  0.78821664]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 0.7035448176482556}
episode index:1831
target Thresh 31.999999714038214
target distance 10.0
model initialize at round 1831
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 17.]), 'previousTarget': array([15., 17.]), 'currentState': array([ 6.35047265, 13.45026769,  0.5838919 ]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 9.34959479974468}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9255961499472236
{'scaleFactor': 20, 'currentTarget': array([15., 17.]), 'previousTarget': array([15., 17.]), 'currentState': array([15.45519435, 17.19600659,  1.03479754]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 0.4956011293497936}
episode index:1832
target Thresh 31.99999971688358
target distance 15.0
model initialize at round 1832
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([16.23262265, 23.11522742,  4.27511668]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 14.12053575392311}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9255996792423462
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([10.90736652, 10.32244639,  4.69348662]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.33548864729967254}
episode index:1833
target Thresh 31.999999719700636
target distance 23.0
model initialize at round 1833
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 16.]), 'previousTarget': array([25., 16.]), 'currentState': array([ 3.9766547 , 22.95956875,  5.99735101]), 'targetState': array([25, 16], dtype=int32), 'currentDistance': 22.1453526697668}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9255831790106211
{'scaleFactor': 20, 'currentTarget': array([25., 16.]), 'previousTarget': array([25., 16.]), 'currentState': array([24.6418387 , 15.98089281,  6.05453079]), 'targetState': array([25, 16], dtype=int32), 'currentDistance': 0.3586705979312593}
episode index:1834
target Thresh 31.99999972248966
target distance 18.0
model initialize at round 1834
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 21.]), 'previousTarget': array([10., 21.]), 'currentState': array([5.0511    , 4.38995571, 1.15980577]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 17.33162376752283}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.925576603571097
{'scaleFactor': 20, 'currentTarget': array([10., 21.]), 'previousTarget': array([10., 21.]), 'currentState': array([10.32297572, 21.33982585,  1.61783437]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 0.4688229066012862}
episode index:1835
target Thresh 31.999999725250934
target distance 15.0
model initialize at round 1835
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([23.51817526,  3.79786184,  3.65767419]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 16.93589930603505}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9255700352943609
{'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([10.85562543, 13.01356871,  1.94499825]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 1.3058106916279983}
episode index:1836
target Thresh 31.999999727984733
target distance 3.0
model initialize at round 1836
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'previousTarget': array([10., 12.]), 'currentState': array([ 8.44078178, 12.61793084,  5.62227617]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 1.6772000387788726}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9256051087645327
{'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'previousTarget': array([10., 12.]), 'currentState': array([10.26744904, 11.86650861,  6.16903602]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 0.29891293407739034}
episode index:1837
target Thresh 31.999999730691332
target distance 7.0
model initialize at round 1837
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 15.]), 'previousTarget': array([22., 15.]), 'currentState': array([18.81615918, 20.30185632,  5.76131082]), 'targetState': array([22, 15], dtype=int32), 'currentDistance': 6.184377315457809}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9256294253538883
{'scaleFactor': 20, 'currentTarget': array([22., 15.]), 'previousTarget': array([22., 15.]), 'currentState': array([21.83519844, 15.25913968,  5.26582395]), 'targetState': array([22, 15], dtype=int32), 'currentDistance': 0.3071041011116275}
episode index:1838
target Thresh 31.999999733371
target distance 23.0
model initialize at round 1838
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 17.]), 'previousTarget': array([27., 17.]), 'currentState': array([ 5.17309965, 25.7932584 ,  0.21046656]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 23.531573941561305}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9256080851942158
{'scaleFactor': 20, 'currentTarget': array([27., 17.]), 'previousTarget': array([27., 17.]), 'currentState': array([26.91319994, 17.02613956,  6.10971872]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 0.09065057976243833}
episode index:1839
target Thresh 31.999999736024
target distance 20.0
model initialize at round 1839
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 28.]), 'previousTarget': array([21., 28.]), 'currentState': array([23.36780598,  9.89360826,  1.99871106]), 'targetState': array([21, 28], dtype=int32), 'currentDistance': 18.260556588659192}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9256015140867644
{'scaleFactor': 20, 'currentTarget': array([21., 28.]), 'previousTarget': array([21., 28.]), 'currentState': array([21.0538015 , 27.51940816,  1.78785921]), 'targetState': array([21, 28], dtype=int32), 'currentDistance': 0.4835939627232668}
episode index:1840
target Thresh 31.999999738650605
target distance 7.0
model initialize at round 1840
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 15.]), 'previousTarget': array([ 6., 15.]), 'currentState': array([ 3.60518222, 22.94173852,  3.5577265 ]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 8.294960093859748}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9256205225038818
{'scaleFactor': 20, 'currentTarget': array([ 6., 15.]), 'previousTarget': array([ 6., 15.]), 'currentState': array([ 5.45364185, 15.78916402,  4.9664812 ]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 0.9598370131071046}
episode index:1841
target Thresh 31.999999741251074
target distance 12.0
model initialize at round 1841
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 21.]), 'previousTarget': array([20., 21.]), 'currentState': array([ 8.23191801, 25.33308605,  5.8606317 ]), 'targetState': array([20, 21], dtype=int32), 'currentDistance': 12.540470024269906}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9256291325076262
{'scaleFactor': 20, 'currentTarget': array([20., 21.]), 'previousTarget': array([20., 21.]), 'currentState': array([19.28764406, 20.91474806,  5.96990718]), 'targetState': array([20, 21], dtype=int32), 'currentDistance': 0.7174391073942656}
episode index:1842
target Thresh 31.99999974382567
target distance 10.0
model initialize at round 1842
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 29.]), 'previousTarget': array([22., 29.]), 'currentState': array([12.78462493, 26.48887602,  0.07578945]), 'targetState': array([22, 29], dtype=int32), 'currentDistance': 9.55138112292492}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9256428931790274
{'scaleFactor': 20, 'currentTarget': array([22., 29.]), 'previousTarget': array([22., 29.]), 'currentState': array([22.34842675, 28.79876082,  0.84681648]), 'targetState': array([22, 29], dtype=int32), 'currentDistance': 0.40236600673859224}
episode index:1843
target Thresh 31.999999746374648
target distance 12.0
model initialize at round 1843
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([ 4.20797799, 21.51504572,  5.23240733]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 12.517897164432002}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9256514817127703
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.38907896, 11.34211595,  5.16873762]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.7001912871836238}
episode index:1844
target Thresh 31.99999974889826
target distance 15.0
model initialize at round 1844
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 13.]), 'previousTarget': array([18., 13.]), 'currentState': array([18.18116278, 26.52966205,  5.21426058]), 'targetState': array([18, 13], dtype=int32), 'currentDistance': 13.530874885605584}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.925654958063011
{'scaleFactor': 20, 'currentTarget': array([18., 13.]), 'previousTarget': array([18., 13.]), 'currentState': array([17.79589897, 12.75236864,  4.82628076]), 'targetState': array([18, 13], dtype=int32), 'currentDistance': 0.32090266257380073}
episode index:1845
target Thresh 31.999999751396764
target distance 17.0
model initialize at round 1845
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 29.]), 'previousTarget': array([ 7., 29.]), 'currentState': array([17.22253833, 13.49262904,  3.06099808]), 'targetState': array([ 7, 29], dtype=int32), 'currentDistance': 18.57360611601634}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9256483829218521
{'scaleFactor': 20, 'currentTarget': array([ 7., 29.]), 'previousTarget': array([ 7., 29.]), 'currentState': array([ 7.60743377, 28.06896389,  1.74710008]), 'targetState': array([ 7, 29], dtype=int32), 'currentDistance': 1.1116672281253537}
episode index:1846
target Thresh 31.99999975387041
target distance 18.0
model initialize at round 1846
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([ 1.75611853, 24.86635677,  4.89065719]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 23.719250080828715}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.925622325878039
{'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([20.6511285 , 11.20055833,  5.78918052]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 0.40241144441291465}
episode index:1847
target Thresh 31.99999975631944
target distance 18.0
model initialize at round 1847
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 22.]), 'previousTarget': array([ 6., 22.]), 'currentState': array([23.09934594, 22.42169278,  3.14549053]), 'targetState': array([ 6, 22], dtype=int32), 'currentDistance': 17.10454490907041}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9256157755109425
{'scaleFactor': 20, 'currentTarget': array([ 6., 22.]), 'previousTarget': array([ 6., 22.]), 'currentState': array([ 5.41879798, 21.91205992,  3.77035433]), 'targetState': array([ 6, 22], dtype=int32), 'currentDistance': 0.5878173601174382}
episode index:1848
target Thresh 31.999999758744103
target distance 8.0
model initialize at round 1848
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 17.]), 'previousTarget': array([13., 17.]), 'currentState': array([ 7.22965883, 10.57609137,  0.9627118 ]), 'targetState': array([13, 17], dtype=int32), 'currentDistance': 8.635012408128476}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9256346939719966
{'scaleFactor': 20, 'currentTarget': array([13., 17.]), 'previousTarget': array([13., 17.]), 'currentState': array([12.65602404, 16.41618967,  0.90748158]), 'targetState': array([13, 17], dtype=int32), 'currentDistance': 0.677609002097536}
episode index:1849
target Thresh 31.99999976114464
target distance 6.0
model initialize at round 1849
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 28.]), 'previousTarget': array([20., 28.]), 'currentState': array([14.66432378, 23.45369408,  6.1281662 ]), 'targetState': array([20, 28], dtype=int32), 'currentDistance': 7.009874340572916}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9256535919806604
{'scaleFactor': 20, 'currentTarget': array([20., 28.]), 'previousTarget': array([20., 28.]), 'currentState': array([19.28555737, 28.37545929,  6.05314589]), 'targetState': array([20, 28], dtype=int32), 'currentDistance': 0.8070922835253079}
episode index:1850
target Thresh 31.999999763521288
target distance 24.0
model initialize at round 1850
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 29.]), 'previousTarget': array([ 3., 29.]), 'currentState': array([25.25153316, 16.65415996,  2.13862276]), 'targetState': array([ 3, 29], dtype=int32), 'currentDistance': 25.447013464905236}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9256275884317778
{'scaleFactor': 20, 'currentTarget': array([ 3., 29.]), 'previousTarget': array([ 3., 29.]), 'currentState': array([ 3.43425486, 28.83565322,  2.67489315]), 'targetState': array([ 3, 29], dtype=int32), 'currentDistance': 0.4643136314992087}
episode index:1851
target Thresh 31.999999765874293
target distance 5.0
model initialize at round 1851
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([18.91766175,  4.68095464,  2.62974024]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 4.419141566313836}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9256570011810047
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.47599873,  7.78117261,  2.15988754]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.523889512592804}
episode index:1852
target Thresh 31.99999976820388
target distance 5.0
model initialize at round 1852
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([17.67674184, 12.03989837,  1.46553838]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 5.86284624123492}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9256810929234867
{'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([22.00808356, 15.82992943,  0.81393668]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 0.17026257256674596}
episode index:1853
target Thresh 31.99999977051029
target distance 11.0
model initialize at round 1853
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 12.]), 'previousTarget': array([13., 12.]), 'currentState': array([19.53018462, 24.61606361,  2.86371398]), 'targetState': array([13, 12], dtype=int32), 'currentDistance': 14.205927363684903}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.925684536426714
{'scaleFactor': 20, 'currentTarget': array([13., 12.]), 'previousTarget': array([13., 12.]), 'currentState': array([13.15806253, 12.92380423,  4.09370048]), 'targetState': array([13, 12], dtype=int32), 'currentDistance': 0.9372289069144977}
episode index:1854
target Thresh 31.999999772793753
target distance 20.0
model initialize at round 1854
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  3.]), 'previousTarget': array([26.,  3.]), 'currentState': array([16.37290898, 21.35886412,  5.94582129]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 20.729900465443936}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9256730526200196
{'scaleFactor': 20, 'currentTarget': array([26.,  3.]), 'previousTarget': array([26.,  3.]), 'currentState': array([25.37808917,  3.98186684,  4.84334977]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 1.1622545167748308}
episode index:1855
target Thresh 31.99999977505449
target distance 5.0
model initialize at round 1855
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 20.]), 'previousTarget': array([14., 20.]), 'currentState': array([ 9.93242512, 16.73253712,  0.7609641 ]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 5.217420725030336}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9256970967727028
{'scaleFactor': 20, 'currentTarget': array([14., 20.]), 'previousTarget': array([14., 20.]), 'currentState': array([14.39323706, 20.4903394 ,  1.41507168]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 0.6285444431066045}
episode index:1856
target Thresh 31.999999777292736
target distance 11.0
model initialize at round 1856
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([17.20991088,  4.75836742,  2.32550228]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 13.047230788635781}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9257055959932889
{'scaleFactor': 20, 'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([ 8.93536097, 13.36372433,  2.15053133]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 1.1312589749590616}
episode index:1857
target Thresh 31.999999779508713
target distance 11.0
model initialize at round 1857
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 16.]), 'previousTarget': array([ 2., 16.]), 'currentState': array([11.01191585, 12.0153887 ,  2.94318992]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 9.853514834014796}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.925719204418427
{'scaleFactor': 20, 'currentTarget': array([ 2., 16.]), 'previousTarget': array([ 2., 16.]), 'currentState': array([ 1.97405464, 16.0177558 ,  3.18059129]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 0.031439313189234944}
episode index:1858
target Thresh 31.999999781702638
target distance 5.0
model initialize at round 1858
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([20.39606314, 22.49031909,  4.45927024]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 7.057880080640292}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9257379654757598
{'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([27.21801516, 19.89496562,  0.2296379 ]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 0.24199758441428915}
episode index:1859
target Thresh 31.99999978387473
target distance 9.0
model initialize at round 1859
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([ 2.99824965, 12.3549486 ,  6.20901877]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 8.676617353791649}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9257567063599127
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.23433035,  9.1494729 ,  5.90316774]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.7801231681819295}
episode index:1860
target Thresh 31.999999786025214
target distance 17.0
model initialize at round 1860
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  4.]), 'previousTarget': array([23.,  4.]), 'currentState': array([10.77041567, 19.50372202,  6.19787598]), 'targetState': array([23,  4], dtype=int32), 'currentDistance': 19.746597922459145}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9257452207976605
{'scaleFactor': 20, 'currentTarget': array([23.,  4.]), 'previousTarget': array([23.,  4.]), 'currentState': array([22.86707298,  4.17490838,  5.6641243 ]), 'targetState': array([23,  4], dtype=int32), 'currentDistance': 0.21968735571220474}
episode index:1861
target Thresh 31.999999788154298
target distance 7.0
model initialize at round 1861
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([19.11499988,  5.62171284,  2.607649  ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 6.732950488976988}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9257691487134513
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.71032241,  9.65353255,  2.43414989]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.7903148897275728}
episode index:1862
target Thresh 31.999999790262198
target distance 8.0
model initialize at round 1862
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([20.33323809,  7.76699178,  4.29048944]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 8.565518020159775}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9257878426808622
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.45338794,  2.52741143,  3.85856019]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.6955022904441384}
episode index:1863
target Thresh 31.999999792349126
target distance 8.0
model initialize at round 1863
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 24.]), 'previousTarget': array([16., 24.]), 'currentState': array([ 7.39896845, 14.89975828,  4.16451401]), 'targetState': array([16, 24], dtype=int32), 'currentDistance': 12.521666947042046}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9257862100905977
{'scaleFactor': 20, 'currentTarget': array([16., 24.]), 'previousTarget': array([16., 24.]), 'currentState': array([16.09487637, 23.94043127,  1.37459917]), 'targetState': array([16, 24], dtype=int32), 'currentDistance': 0.11202660050911768}
episode index:1864
target Thresh 31.999999794415285
target distance 3.0
model initialize at round 1864
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 10.]), 'previousTarget': array([27., 10.]), 'currentState': array([26.63659255,  6.60763165,  0.77469414]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 3.4117778346257586}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9258153327661524
{'scaleFactor': 20, 'currentTarget': array([27., 10.]), 'previousTarget': array([27., 10.]), 'currentState': array([26.51718116, 10.38202356,  1.61403387]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 0.6156752637358354}
episode index:1865
target Thresh 31.999999796460887
target distance 4.0
model initialize at round 1865
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 21.]), 'previousTarget': array([ 8., 21.]), 'currentState': array([ 7.33894306, 24.72930103,  4.31314039]), 'targetState': array([ 8, 21], dtype=int32), 'currentDistance': 3.787437450829056}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9258444242276924
{'scaleFactor': 20, 'currentTarget': array([ 8., 21.]), 'previousTarget': array([ 8., 21.]), 'currentState': array([ 8.32858533, 21.00739618,  5.09303646]), 'targetState': array([ 8, 21], dtype=int32), 'currentDistance': 0.3286685629717131}
episode index:1866
target Thresh 31.999999798486137
target distance 4.0
model initialize at round 1866
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'previousTarget': array([10., 17.]), 'currentState': array([ 7.85291164, 17.25006956,  5.84108322]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 2.1616020021280633}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9258787871499058
{'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'previousTarget': array([10., 17.]), 'currentState': array([ 9.80344819, 17.06007346,  0.25487028]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 0.205527212875513}
episode index:1867
target Thresh 31.99999980049123
target distance 23.0
model initialize at round 1867
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 29.]), 'previousTarget': array([23., 29.]), 'currentState': array([21.69838279,  7.65572205,  2.76098686]), 'targetState': array([23, 29], dtype=int32), 'currentDistance': 21.383928744236506}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9258576447968898
{'scaleFactor': 20, 'currentTarget': array([23., 29.]), 'previousTarget': array([23., 29.]), 'currentState': array([22.95663461, 28.10690344,  1.06405984]), 'targetState': array([23, 29], dtype=int32), 'currentDistance': 0.8941487706610869}
episode index:1868
target Thresh 31.999999802476378
target distance 24.0
model initialize at round 1868
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 26.]), 'previousTarget': array([26., 26.]), 'currentState': array([ 2.43083851, 23.62688852,  0.30191541]), 'targetState': array([26, 26], dtype=int32), 'currentDistance': 23.68833112141049}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9258365250681146
{'scaleFactor': 20, 'currentTarget': array([26., 26.]), 'previousTarget': array([26., 26.]), 'currentState': array([25.59007188, 25.82272851,  0.16056679]), 'targetState': array([26, 26], dtype=int32), 'currentDistance': 0.44661644429795333}
episode index:1869
target Thresh 31.99999980444177
target distance 16.0
model initialize at round 1869
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([ 8.9292189 , 15.40318934,  6.25906438]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 16.010087383269408}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9258348716827455
{'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([23.80627426,  9.94383026,  0.03196976]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 0.20170449624966091}
episode index:1870
target Thresh 31.999999806387606
target distance 17.0
model initialize at round 1870
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([24.03867198,  7.2896209 ,  3.22608298]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 15.489627753345244}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9258332200647578
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.78710587, 11.29539495,  3.35508284]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.3641182344734893}
episode index:1871
target Thresh 31.999999808314083
target distance 10.0
model initialize at round 1871
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'previousTarget': array([27.,  8.]), 'currentState': array([18.43550192, 15.12152273,  0.46082896]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 11.138523838755823}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9258415784671811
{'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'previousTarget': array([27.,  8.]), 'currentState': array([26.49448562,  8.94070615,  6.07913291]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 1.0679292382816168}
episode index:1872
target Thresh 31.99999981022139
target distance 19.0
model initialize at round 1872
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([23.26703028, 29.66165066,  2.42145684]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 25.69019309844755}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9258157799858846
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([8.24275113, 9.56840251, 4.27457503]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.6180691859374589}
episode index:1873
target Thresh 31.999999812109717
target distance 8.0
model initialize at round 1873
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([ 4.30050918, 14.3440766 ,  5.90191054]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 10.857961225289973}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9258241387742598
{'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([12.3078815 , 22.24696217,  1.35559105]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 0.39469144007003987}
episode index:1874
target Thresh 31.999999813979258
target distance 22.0
model initialize at round 1874
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 25.]), 'previousTarget': array([14., 25.]), 'currentState': array([13.4505824 ,  3.65703361,  1.73052526]), 'targetState': array([14, 25], dtype=int32), 'currentDistance': 21.35003685770265}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9258078796358514
{'scaleFactor': 20, 'currentTarget': array([14., 25.]), 'previousTarget': array([14., 25.]), 'currentState': array([14.1069815 , 25.29793543,  2.03906626]), 'targetState': array([14, 25], dtype=int32), 'currentDistance': 0.31656052107844}
episode index:1875
target Thresh 31.999999815830193
target distance 26.0
model initialize at round 1875
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 28.]), 'previousTarget': array([ 7., 28.]), 'currentState': array([18.71982906,  3.50826642,  2.01649505]), 'targetState': array([ 7, 28], dtype=int32), 'currentDistance': 27.151416296598796}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9257774627558585
{'scaleFactor': 20, 'currentTarget': array([ 7., 28.]), 'previousTarget': array([ 7., 28.]), 'currentState': array([ 7.01141189, 28.32292233,  2.39118175]), 'targetState': array([ 7, 28], dtype=int32), 'currentDistance': 0.3231239148773335}
episode index:1876
target Thresh 31.999999817662715
target distance 7.0
model initialize at round 1876
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([ 3.59396672, 19.57467736,  2.22009061]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 6.405871406286367}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9257960128609433
{'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([ 7.37084608, 25.21460515,  1.1785972 ]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 0.4284649158347439}
episode index:1877
target Thresh 31.999999819477
target distance 14.0
model initialize at round 1877
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([17.68026471, 18.95564046,  4.82103086]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 13.374201366293711}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9257993511650147
{'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([21.02296485,  5.58530621,  5.2901812 ]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 0.4153291767587085}
episode index:1878
target Thresh 31.999999821273235
target distance 8.0
model initialize at round 1878
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([15.5429779 ,  7.59297303,  2.25228876]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 8.471989058284077}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9258178698764755
{'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([10.55480863, 13.80404529,  2.48041915]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 0.5883968565470912}
episode index:1879
target Thresh 31.999999823051596
target distance 14.0
model initialize at round 1879
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 29.]), 'previousTarget': array([16., 29.]), 'currentState': array([18.87501859, 16.43761279,  2.0340329 ]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 12.887175959785601}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9258262008762226
{'scaleFactor': 20, 'currentTarget': array([16., 29.]), 'previousTarget': array([16., 29.]), 'currentState': array([16.52400008, 28.15043441,  1.71484209]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 0.9981672148435353}
episode index:1880
target Thresh 31.99999982481226
target distance 15.0
model initialize at round 1880
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([26.43662324, 15.16767959,  3.90681577]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 16.80210969651396}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9258196570413515
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.55462754,  1.32431479,  4.57237802]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.8092633255546753}
episode index:1881
target Thresh 31.99999982655541
target distance 7.0
model initialize at round 1881
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 20.]), 'previousTarget': array([20., 20.]), 'currentState': array([25.48629903, 18.73559327,  3.69924259]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 5.630115579030609}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9258432911236888
{'scaleFactor': 20, 'currentTarget': array([20., 20.]), 'previousTarget': array([20., 20.]), 'currentState': array([19.87467758, 19.89083858,  2.86433914]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 0.16619844519281812}
episode index:1882
target Thresh 31.99999982828121
target distance 22.0
model initialize at round 1882
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  6.]), 'previousTarget': array([27.,  6.]), 'currentState': array([6.42533685, 3.10512405, 0.44936484]), 'targetState': array([27,  6], dtype=int32), 'currentDistance': 20.777321065562468}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.925827090891684
{'scaleFactor': 20, 'currentTarget': array([27.,  6.]), 'previousTarget': array([27.,  6.]), 'currentState': array([27.81310717,  6.14510753,  0.56328669]), 'targetState': array([27,  6], dtype=int32), 'currentDistance': 0.8259536651572114}
episode index:1883
target Thresh 31.999999829989843
target distance 16.0
model initialize at round 1883
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([ 3.62847161, 16.83918408,  5.64271645]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 19.951537395579297}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9258157081868629
{'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([17.67881167,  2.86012832,  5.92977595]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 0.3503227507180062}
episode index:1884
target Thresh 31.99999983168147
target distance 7.0
model initialize at round 1884
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  4.]), 'previousTarget': array([21.,  4.]), 'currentState': array([14.01397743,  1.31708802,  5.73069429]), 'targetState': array([21,  4], dtype=int32), 'currentDistance': 7.483483678872918}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.925834159275358
{'scaleFactor': 20, 'currentTarget': array([21.,  4.]), 'previousTarget': array([21.,  4.]), 'currentState': array([20.08787907,  4.22384168,  5.72672939]), 'targetState': array([21,  4], dtype=int32), 'currentDistance': 0.9391856482360156}
episode index:1885
target Thresh 31.99999983335627
target distance 17.0
model initialize at round 1885
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 24.]), 'previousTarget': array([26., 24.]), 'currentState': array([11.82213447,  8.67842849,  0.47565448]), 'targetState': array([26, 24], dtype=int32), 'currentDistance': 20.874923342672968}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9258179896544583
{'scaleFactor': 20, 'currentTarget': array([26., 24.]), 'previousTarget': array([26., 24.]), 'currentState': array([26.46279667, 24.51130394,  1.39109747]), 'targetState': array([26, 24], dtype=int32), 'currentDistance': 0.6896466323379652}
episode index:1886
target Thresh 31.9999998350144
target distance 16.0
model initialize at round 1886
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'previousTarget': array([11., 21.]), 'currentState': array([25.8764183 , 13.25297736,  3.31180155]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 16.772721347483223}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9258114709781621
{'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'previousTarget': array([11., 21.]), 'currentState': array([10.16536771, 21.24726478,  3.33377225]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 0.8704889019411466}
episode index:1887
target Thresh 31.999999836656034
target distance 11.0
model initialize at round 1887
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 12.]), 'previousTarget': array([13., 12.]), 'currentState': array([22.03977478, 10.32418406,  3.17849973]), 'targetState': array([13, 12], dtype=int32), 'currentDistance': 9.193796120899089}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9258248070898791
{'scaleFactor': 20, 'currentTarget': array([13., 12.]), 'previousTarget': array([13., 12.]), 'currentState': array([12.36887315, 12.11192714,  3.5907714 ]), 'targetState': array([13, 12], dtype=int32), 'currentDistance': 0.6409748721868547}
episode index:1888
target Thresh 31.999999838281337
target distance 10.0
model initialize at round 1888
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 20.]), 'previousTarget': array([14., 20.]), 'currentState': array([ 5.5750795 , 14.23247007,  0.67439457]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 10.209979773188572}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9258381290818379
{'scaleFactor': 20, 'currentTarget': array([14., 20.]), 'previousTarget': array([14., 20.]), 'currentState': array([13.8946937 , 19.64717809,  0.98910607]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 0.3682020044649033}
episode index:1889
target Thresh 31.999999839890464
target distance 3.0
model initialize at round 1889
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 17.]), 'previousTarget': array([20., 17.]), 'currentState': array([18.36318816, 17.6209188 ,  5.92568141]), 'targetState': array([20, 17], dtype=int32), 'currentDistance': 1.7506265049128764}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9258720771616887
{'scaleFactor': 20, 'currentTarget': array([20., 17.]), 'previousTarget': array([20., 17.]), 'currentState': array([20.27803971, 17.04890349,  6.06147256]), 'targetState': array([20, 17], dtype=int32), 'currentDistance': 0.2823076894956717}
episode index:1890
target Thresh 31.99999984148358
target distance 5.0
model initialize at round 1890
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([19.6711517 ,  9.80086999,  5.15459347]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 3.7847541148532735}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9259007540114181
{'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([23.08773254,  8.04800497,  6.04318018]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 0.10000737688646255}
episode index:1891
target Thresh 31.999999843060845
target distance 20.0
model initialize at round 1891
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'previousTarget': array([26., 11.]), 'currentState': array([5.84399558, 6.67572394, 0.65362549]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 20.614652009508806}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9258893805024315
{'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'previousTarget': array([26., 11.]), 'currentState': array([25.14446756, 10.60566532,  0.06325831]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 0.9420380036776804}
episode index:1892
target Thresh 31.999999844622415
target distance 23.0
model initialize at round 1892
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  5.]), 'previousTarget': array([13.,  5.]), 'currentState': array([21.6970898 , 26.16600793,  3.88425314]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 22.883165486294388}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9258685117709015
{'scaleFactor': 20, 'currentTarget': array([13.,  5.]), 'previousTarget': array([13.,  5.]), 'currentState': array([12.67092127,  4.5731289 ,  4.73531075]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 0.5389914195483946}
episode index:1893
target Thresh 31.999999846168446
target distance 13.0
model initialize at round 1893
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([7.08817432, 5.31934139, 5.77480507]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 17.713441949338485}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9258619905120381
{'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([16.85552137, 19.56277565,  1.25117518]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 0.46047714840552745}
episode index:1894
target Thresh 31.999999847699097
target distance 9.0
model initialize at round 1894
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 17.]), 'previousTarget': array([17., 17.]), 'currentState': array([14.53541772,  9.92699753,  1.30231531]), 'targetState': array([17, 17], dtype=int32), 'currentDistance': 7.49009544494988}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9258803198099209
{'scaleFactor': 20, 'currentTarget': array([17., 17.]), 'previousTarget': array([17., 17.]), 'currentState': array([17.17797049, 17.38388203,  1.7268851 ]), 'targetState': array([17, 17], dtype=int32), 'currentDistance': 0.42312989537136775}
episode index:1895
target Thresh 31.999999849214518
target distance 12.0
model initialize at round 1895
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  7.]), 'previousTarget': array([27.,  7.]), 'currentState': array([16.38162273, 11.96099258,  5.88092083]), 'targetState': array([27,  7], dtype=int32), 'currentDistance': 11.720127272610574}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9258885475681441
{'scaleFactor': 20, 'currentTarget': array([27.,  7.]), 'previousTarget': array([27.,  7.]), 'currentState': array([27.10930025,  6.83427517,  0.10896143]), 'targetState': array([27,  7], dtype=int32), 'currentDistance': 0.19852270326452656}
episode index:1896
target Thresh 31.999999850714858
target distance 20.0
model initialize at round 1896
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  7.]), 'previousTarget': array([27.,  7.]), 'currentState': array([12.59858407, 25.82197501,  5.43906707]), 'targetState': array([27,  7], dtype=int32), 'currentDistance': 23.69952581808667}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.925867723279345
{'scaleFactor': 20, 'currentTarget': array([27.,  7.]), 'previousTarget': array([27.,  7.]), 'currentState': array([26.85460374,  7.01531363,  5.63500411]), 'targetState': array([27,  7], dtype=int32), 'currentDistance': 0.14620047430739777}
episode index:1897
target Thresh 31.99999985220027
target distance 2.0
model initialize at round 1897
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([23.26040752,  7.48824254,  5.26738715]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 3.129021821891932}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9258962966601251
{'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([25.93320809,  9.1805789 ,  1.12174794]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 0.19253544903471845}
episode index:1898
target Thresh 31.9999998536709
target distance 17.0
model initialize at round 1898
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 26.]), 'previousTarget': array([ 5., 26.]), 'currentState': array([20.31773879, 21.04921176,  4.12232947]), 'targetState': array([ 5, 26], dtype=int32), 'currentDistance': 16.09793234670636}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9258897779401796
{'scaleFactor': 20, 'currentTarget': array([ 5., 26.]), 'previousTarget': array([ 5., 26.]), 'currentState': array([ 5.20386151, 26.00431628,  2.82875828]), 'targetState': array([ 5, 26], dtype=int32), 'currentDistance': 0.203907202815526}
episode index:1899
target Thresh 31.999999855126898
target distance 18.0
model initialize at round 1899
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 8.]), 'previousTarget': array([8., 8.]), 'currentState': array([25.00814837, 23.26328409,  4.18741139]), 'targetState': array([8, 8], dtype=int32), 'currentDistance': 22.85267932631208}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9258689858842721
{'scaleFactor': 20, 'currentTarget': array([8., 8.]), 'previousTarget': array([8., 8.]), 'currentState': array([7.41339583, 7.72578798, 4.20706932]), 'targetState': array([8, 8], dtype=int32), 'currentDistance': 0.647531228113528}
episode index:1900
target Thresh 31.99999985656841
target distance 5.0
model initialize at round 1900
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  8.]), 'previousTarget': array([20.,  8.]), 'currentState': array([16.24145719, 10.86370245,  0.26879948]), 'targetState': array([20,  8], dtype=int32), 'currentDistance': 4.725191610412851}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9258923578012189
{'scaleFactor': 20, 'currentTarget': array([20.,  8.]), 'previousTarget': array([20.,  8.]), 'currentState': array([20.17576315,  7.66900607,  4.6171279 ]), 'targetState': array([20,  8], dtype=int32), 'currentDistance': 0.374766147235387}
episode index:1901
target Thresh 31.99999985799558
target distance 16.0
model initialize at round 1901
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 18.]), 'previousTarget': array([ 9., 18.]), 'currentState': array([7.84826831, 3.96613832, 1.9412567 ]), 'targetState': array([ 9, 18], dtype=int32), 'currentDistance': 14.081042559606994}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9258956033270368
{'scaleFactor': 20, 'currentTarget': array([ 9., 18.]), 'previousTarget': array([ 9., 18.]), 'currentState': array([ 9.27040632, 17.61034681,  1.70679148]), 'targetState': array([ 9, 18], dtype=int32), 'currentDistance': 0.4742880891824374}
episode index:1902
target Thresh 31.999999859408547
target distance 10.0
model initialize at round 1902
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 16.]), 'previousTarget': array([27., 16.]), 'currentState': array([15.33704992, 23.25881485,  3.99719536]), 'targetState': array([27, 16], dtype=int32), 'currentDistance': 13.737350454945805}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9258988454418975
{'scaleFactor': 20, 'currentTarget': array([27., 16.]), 'previousTarget': array([27., 16.]), 'currentState': array([26.46669308, 16.24824057,  5.69383054]), 'targetState': array([27, 16], dtype=int32), 'currentDistance': 0.5882513562516333}
episode index:1903
target Thresh 31.999999860807456
target distance 5.0
model initialize at round 1903
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  8.]), 'previousTarget': array([12.,  8.]), 'currentState': array([14.64195171,  2.63069977,  0.78876608]), 'targetState': array([12,  8], dtype=int32), 'currentDistance': 5.984086712363092}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9259221648508041
{'scaleFactor': 20, 'currentTarget': array([12.,  8.]), 'previousTarget': array([12.,  8.]), 'currentState': array([12.16103956,  7.42065442,  1.69396305]), 'targetState': array([12,  8], dtype=int32), 'currentDistance': 0.6013111006340859}
episode index:1904
target Thresh 31.999999862192443
target distance 12.0
model initialize at round 1904
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 20.]), 'previousTarget': array([11., 20.]), 'currentState': array([9.68296981, 7.99915324, 1.00949686]), 'targetState': array([11, 20], dtype=int32), 'currentDistance': 12.072899053998732}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9259303317718278
{'scaleFactor': 20, 'currentTarget': array([11., 20.]), 'previousTarget': array([11., 20.]), 'currentState': array([11.2151577 , 19.76949347,  1.88936901]), 'targetState': array([11, 20], dtype=int32), 'currentDistance': 0.31531903404620215}
episode index:1905
target Thresh 31.99999986356365
target distance 24.0
model initialize at round 1905
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([25.10176765, 27.7277987 ,  4.4161174 ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 24.482544969688266}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9259049333936679
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.64531675,  4.13487105,  4.63722017]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.935012461000115}
episode index:1906
target Thresh 31.999999864921218
target distance 10.0
model initialize at round 1906
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([ 2.07491497, 18.40591813,  3.16277039]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 11.567234788498368}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9259131007853865
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.78303966, 7.37505507, 5.26561923]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.43328754168032624}
episode index:1907
target Thresh 31.999999866265274
target distance 8.0
model initialize at round 1907
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([10.01598575,  9.6582992 ,  4.35050583]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 6.956805786392893}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9259312784107611
{'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([7.94655178, 2.03843124, 4.96081302]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 0.9630530568493174}
episode index:1908
target Thresh 31.999999867595957
target distance 11.0
model initialize at round 1908
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([25.64189693, 19.53183584,  3.97210181]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 12.87471938416759}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9259394234453291
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.46543509, 11.8517411 ,  3.91700376]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.9706146093999275}
episode index:1909
target Thresh 31.9999998689134
target distance 11.0
model initialize at round 1909
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 27.]), 'previousTarget': array([20., 27.]), 'currentState': array([ 9.80680432, 26.4769749 ,  0.06083322]), 'targetState': array([20, 27], dtype=int32), 'currentDistance': 10.20660537756182}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9259525389565618
{'scaleFactor': 20, 'currentTarget': array([20., 27.]), 'previousTarget': array([20., 27.]), 'currentState': array([19.74757183, 26.91609258,  0.36800334]), 'targetState': array([20, 27], dtype=int32), 'currentDistance': 0.2660083418915922}
episode index:1910
target Thresh 31.99999987021773
target distance 3.0
model initialize at round 1910
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 19.]), 'previousTarget': array([13., 19.]), 'currentState': array([11.3574394 , 20.53610138,  5.35517764]), 'targetState': array([13, 19], dtype=int32), 'currentDistance': 2.248913690723166}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9259860541114773
{'scaleFactor': 20, 'currentTarget': array([13., 19.]), 'previousTarget': array([13., 19.]), 'currentState': array([12.82735715, 19.19791646,  5.73797929]), 'targetState': array([13, 19], dtype=int32), 'currentDistance': 0.262633733030869}
episode index:1911
target Thresh 31.999999871509086
target distance 15.0
model initialize at round 1911
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  2.]), 'previousTarget': array([13.,  2.]), 'currentState': array([ 7.50841497, 16.2536272 ,  4.81370151]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 15.274926988459892}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9259843588396762
{'scaleFactor': 20, 'currentTarget': array([13.,  2.]), 'previousTarget': array([13.,  2.]), 'currentState': array([13.09772072,  1.55586613,  5.47903066]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 0.45475733239691496}
episode index:1912
target Thresh 31.999999872787594
target distance 13.0
model initialize at round 1912
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  3.]), 'previousTarget': array([24.,  3.]), 'currentState': array([12.99099326,  3.06120429,  0.18794885]), 'targetState': array([24,  3], dtype=int32), 'currentDistance': 11.009176868765321}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9259924590961118
{'scaleFactor': 20, 'currentTarget': array([24.,  3.]), 'previousTarget': array([24.,  3.]), 'currentState': array([24.82982262,  3.34152501,  0.58742029]), 'targetState': array([24,  3], dtype=int32), 'currentDistance': 0.8973543961213154}
episode index:1913
target Thresh 31.999999874053376
target distance 8.0
model initialize at round 1913
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 17.]), 'previousTarget': array([26., 17.]), 'currentState': array([21.44742168, 25.64952955,  3.75536346]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 9.774473427714794}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9260055194883814
{'scaleFactor': 20, 'currentTarget': array([26., 17.]), 'previousTarget': array([26., 17.]), 'currentState': array([25.71754509, 17.34923268,  5.39861433]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 0.44915948660141564}
episode index:1914
target Thresh 31.999999875306568
target distance 14.0
model initialize at round 1914
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([11.29815917, 23.47035706,  5.29221916]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 13.190298608534702}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9260086838896443
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.80431373, 10.66842913,  4.86121278]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.385009554343002}
episode index:1915
target Thresh 31.99999987654729
target distance 5.0
model initialize at round 1915
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 24.]), 'previousTarget': array([14., 24.]), 'currentState': array([14.55063186, 28.34583723,  4.87397004]), 'targetState': array([14, 24], dtype=int32), 'currentDistance': 4.380581771657595}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9260369152654848
{'scaleFactor': 20, 'currentTarget': array([14., 24.]), 'previousTarget': array([14., 24.]), 'currentState': array([14.01833185, 24.41198668,  4.53351632]), 'targetState': array([14, 24], dtype=int32), 'currentDistance': 0.412394326382979}
episode index:1916
target Thresh 31.99999987777566
target distance 12.0
model initialize at round 1916
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  3.]), 'previousTarget': array([10.,  3.]), 'currentState': array([20.31827635, 14.06475828,  4.11310458]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 15.129299472815939}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9260351978837228
{'scaleFactor': 20, 'currentTarget': array([10.,  3.]), 'previousTarget': array([10.,  3.]), 'currentState': array([9.69905355, 2.29991375, 4.56228291]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 0.7620298700562165}
episode index:1917
target Thresh 31.999999878991815
target distance 6.0
model initialize at round 1917
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 11.]), 'previousTarget': array([18., 11.]), 'currentState': array([13.26613411,  9.10873715,  5.99240703]), 'targetState': array([18, 11], dtype=int32), 'currentDistance': 5.097681969417825}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9260582759870161
{'scaleFactor': 20, 'currentTarget': array([18., 11.]), 'previousTarget': array([18., 11.]), 'currentState': array([18.52974516, 11.59406531,  0.57925262]), 'targetState': array([18, 11], dtype=int32), 'currentDistance': 0.7959544766743996}
episode index:1918
target Thresh 31.999999880195865
target distance 20.0
model initialize at round 1918
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([26.06032917, 27.23455801,  4.23663067]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 24.298832872343947}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.926037601987917
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.34893538,  9.41994592,  4.15296203]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.5459949391743749}
episode index:1919
target Thresh 31.999999881387936
target distance 11.0
model initialize at round 1919
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 13.]), 'previousTarget': array([13., 13.]), 'currentState': array([16.54723206,  2.84961956,  1.60240364]), 'targetState': array([13, 13], dtype=int32), 'currentDistance': 10.752352224479003}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9260505980545378
{'scaleFactor': 20, 'currentTarget': array([13., 13.]), 'previousTarget': array([13., 13.]), 'currentState': array([13.53291991, 12.30789235,  2.18309367]), 'targetState': array([13, 13], dtype=int32), 'currentDistance': 0.8735082351181157}
episode index:1920
target Thresh 31.999999882568147
target distance 10.0
model initialize at round 1920
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 29.]), 'previousTarget': array([ 5., 29.]), 'currentState': array([13.31996802, 26.09945487,  2.07246876]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 8.8110742783681}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9260635805906364
{'scaleFactor': 20, 'currentTarget': array([ 5., 29.]), 'previousTarget': array([ 5., 29.]), 'currentState': array([ 4.69312577, 28.94187035,  3.14589196]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 0.3123313097655491}
episode index:1921
target Thresh 31.999999883736614
target distance 11.0
model initialize at round 1921
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 18.]), 'previousTarget': array([18., 18.]), 'currentState': array([ 8.20423423, 12.17570277,  1.78337603]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 11.396467226063523}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9260667032583348
{'scaleFactor': 20, 'currentTarget': array([18., 18.]), 'previousTarget': array([18., 18.]), 'currentState': array([18.44406367, 18.03451711,  5.47824764]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 0.44540316072662356}
episode index:1922
target Thresh 31.999999884893455
target distance 18.0
model initialize at round 1922
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  4.]), 'previousTarget': array([21.,  4.]), 'currentState': array([3.25037523, 2.33575833, 5.87171316]), 'targetState': array([21,  4], dtype=int32), 'currentDistance': 17.827475420824957}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9260601772802929
{'scaleFactor': 20, 'currentTarget': array([21.,  4.]), 'previousTarget': array([21.,  4.]), 'currentState': array([20.90092369,  3.69765419,  0.25492472]), 'targetState': array([21,  4], dtype=int32), 'currentDistance': 0.3181652200910465}
episode index:1923
target Thresh 31.999999886038783
target distance 10.0
model initialize at round 1923
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 19.]), 'previousTarget': array([16., 19.]), 'currentState': array([24.61437351, 20.95521061,  3.54803598]), 'targetState': array([16, 19], dtype=int32), 'currentDistance': 8.833474934991802}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.926078127297299
{'scaleFactor': 20, 'currentTarget': array([16., 19.]), 'previousTarget': array([16., 19.]), 'currentState': array([16.77670454, 19.61600082,  3.42691417]), 'targetState': array([16, 19], dtype=int32), 'currentDistance': 0.991325851799062}
episode index:1924
target Thresh 31.999999887172716
target distance 19.0
model initialize at round 1924
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  7.]), 'previousTarget': array([27.,  7.]), 'currentState': array([20.97500156, 24.62822741,  4.3203002 ]), 'targetState': array([27,  7], dtype=int32), 'currentDistance': 18.62941243639407}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9260668566207855
{'scaleFactor': 20, 'currentTarget': array([27.,  7.]), 'previousTarget': array([27.,  7.]), 'currentState': array([26.87471037,  6.79666735,  5.21202362]), 'targetState': array([27,  7], dtype=int32), 'currentDistance': 0.23883395138907895}
episode index:1925
target Thresh 31.999999888295367
target distance 5.0
model initialize at round 1925
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([16.56066683, 13.6298469 ,  1.3935849 ]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 5.56122744713194}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9260898224273167
{'scaleFactor': 20, 'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([20.30233727, 18.1351625 ,  1.08832687]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 0.33117477080381447}
episode index:1926
target Thresh 31.999999889406848
target distance 14.0
model initialize at round 1926
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 15.]), 'previousTarget': array([ 4., 15.]), 'currentState': array([ 5.8256444 , 27.01812566,  4.80379879]), 'targetState': array([ 4, 15], dtype=int32), 'currentDistance': 12.155999416428662}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9260978091045216
{'scaleFactor': 20, 'currentTarget': array([ 4., 15.]), 'previousTarget': array([ 4., 15.]), 'currentState': array([ 3.85740481, 15.25615446,  4.89166892]), 'targetState': array([ 4, 15], dtype=int32), 'currentDistance': 0.2931697410438826}
episode index:1927
target Thresh 31.999999890507265
target distance 1.0
model initialize at round 1927
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 17.]), 'previousTarget': array([ 5., 17.]), 'currentState': array([ 7.67502679, 16.16344358,  1.1072486 ]), 'targetState': array([ 5, 17], dtype=int32), 'currentDistance': 2.8027834383240053}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9261258185396333
{'scaleFactor': 20, 'currentTarget': array([ 5., 17.]), 'previousTarget': array([ 5., 17.]), 'currentState': array([ 4.91396611, 17.14205944,  3.66917049]), 'targetState': array([ 5, 17], dtype=int32), 'currentDistance': 0.1660804458398407}
episode index:1928
target Thresh 31.999999891596737
target distance 6.0
model initialize at round 1928
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 16.]), 'previousTarget': array([11., 16.]), 'currentState': array([12.22334164, 23.49304718,  3.06045997]), 'targetState': array([11, 16], dtype=int32), 'currentDistance': 7.592254000730369}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9261436880012509
{'scaleFactor': 20, 'currentTarget': array([11., 16.]), 'previousTarget': array([11., 16.]), 'currentState': array([11.00392862, 16.24543311,  4.83259971]), 'targetState': array([11, 16], dtype=int32), 'currentDistance': 0.24546454786288932}
episode index:1929
target Thresh 31.99999989267537
target distance 5.0
model initialize at round 1929
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 18.]), 'previousTarget': array([ 9., 18.]), 'currentState': array([ 5.51890214, 21.22758315,  0.50373524]), 'targetState': array([ 9, 18], dtype=int32), 'currentDistance': 4.747139695709148}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9261665664012503
{'scaleFactor': 20, 'currentTarget': array([ 9., 18.]), 'previousTarget': array([ 9., 18.]), 'currentState': array([ 9.65455492, 17.80714939,  6.04852098]), 'targetState': array([ 9, 18], dtype=int32), 'currentDistance': 0.6823734294343701}
episode index:1930
target Thresh 31.999999893743265
target distance 9.0
model initialize at round 1930
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 13.]), 'previousTarget': array([24., 13.]), 'currentState': array([15.8687366 , 10.44141764,  0.01839685]), 'targetState': array([24, 13], dtype=int32), 'currentDistance': 8.524305731319645}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9261843962529328
{'scaleFactor': 20, 'currentTarget': array([24., 13.]), 'previousTarget': array([24., 13.]), 'currentState': array([23.47437334, 12.8105132 ,  0.43705005]), 'targetState': array([24, 13], dtype=int32), 'currentDistance': 0.5587384329456504}
episode index:1931
target Thresh 31.99999989480054
target distance 12.0
model initialize at round 1931
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 5.]), 'previousTarget': array([6., 5.]), 'currentState': array([14.82638661, 15.79375802,  4.95070028]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 13.943109873232268}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9261874402237682
{'scaleFactor': 20, 'currentTarget': array([6., 5.]), 'previousTarget': array([6., 5.]), 'currentState': array([6.16487397, 5.2121802 , 4.21134682]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 0.2687077650030944}
episode index:1932
target Thresh 31.99999989584729
target distance 6.0
model initialize at round 1932
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 17.]), 'previousTarget': array([19., 17.]), 'currentState': array([16.52780444, 22.29416597,  4.84040338]), 'targetState': array([19, 17], dtype=int32), 'currentDistance': 5.842939691649038}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9262102604823177
{'scaleFactor': 20, 'currentTarget': array([19., 17.]), 'previousTarget': array([19., 17.]), 'currentState': array([18.86240373, 16.91494052,  5.65882342]), 'targetState': array([19, 17], dtype=int32), 'currentDistance': 0.16176478895697}
episode index:1933
target Thresh 31.999999896883626
target distance 23.0
model initialize at round 1933
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 12.]), 'previousTarget': array([27., 12.]), 'currentState': array([5.65551293, 7.30276232, 1.19088238]), 'targetState': array([27, 12], dtype=int32), 'currentDistance': 21.855232098954616}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9261942977076415
{'scaleFactor': 20, 'currentTarget': array([27., 12.]), 'previousTarget': array([27., 12.]), 'currentState': array([26.63762847, 11.57851405,  0.48810323]), 'targetState': array([27, 12], dtype=int32), 'currentDistance': 0.5558448754187679}
episode index:1934
target Thresh 31.999999897909653
target distance 1.0
model initialize at round 1934
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 20.]), 'previousTarget': array([11., 20.]), 'currentState': array([11.27309439, 19.32240381,  3.89210057]), 'targetState': array([11, 20], dtype=int32), 'currentDistance': 0.7305594668294831}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9262324401894464
{'scaleFactor': 20, 'currentTarget': array([11., 20.]), 'previousTarget': array([11., 20.]), 'currentState': array([11.27309439, 19.32240381,  3.89210057]), 'targetState': array([11, 20], dtype=int32), 'currentDistance': 0.7305594668294831}
episode index:1935
target Thresh 31.999999898925466
target distance 16.0
model initialize at round 1935
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 25.]), 'previousTarget': array([15., 25.]), 'currentState': array([20.2491178 ,  7.87212909,  0.27556628]), 'targetState': array([15, 25], dtype=int32), 'currentDistance': 17.91416198843087}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9262258724246191
{'scaleFactor': 20, 'currentTarget': array([15., 25.]), 'previousTarget': array([15., 25.]), 'currentState': array([15.48885236, 24.3931675 ,  2.1801119 ]), 'targetState': array([15, 25], dtype=int32), 'currentDistance': 0.7792447034629343}
episode index:1936
target Thresh 31.999999899931176
target distance 20.0
model initialize at round 1936
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([ 5.65347509, 19.55092179,  0.1620245 ]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 19.513975197964257}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9262145952963713
{'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([25.25369283, 16.74068334,  0.14049805]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 0.3627742823637646}
episode index:1937
target Thresh 31.999999900926877
target distance 3.0
model initialize at round 1937
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  8.]), 'previousTarget': array([21.,  8.]), 'currentState': array([20.6807801 , 11.08582804,  5.32420546]), 'targetState': array([21,  8], dtype=int32), 'currentDistance': 3.1022952895770746}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9262423999427611
{'scaleFactor': 20, 'currentTarget': array([21.,  8.]), 'previousTarget': array([21.,  8.]), 'currentState': array([21.26550753,  7.20336599,  4.78410593]), 'targetState': array([21,  8], dtype=int32), 'currentDistance': 0.8397142308151075}
episode index:1938
target Thresh 31.999999901912673
target distance 12.0
model initialize at round 1938
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 20.]), 'previousTarget': array([10., 20.]), 'currentState': array([12.35622313,  9.5549726 ,  2.97332293]), 'targetState': array([10, 20], dtype=int32), 'currentDistance': 10.707491994877346}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9262502585035958
{'scaleFactor': 20, 'currentTarget': array([10., 20.]), 'previousTarget': array([10., 20.]), 'currentState': array([ 9.93148623, 20.00840931,  2.03244466]), 'targetState': array([10, 20], dtype=int32), 'currentDistance': 0.06902791490202523}
episode index:1939
target Thresh 31.999999902888657
target distance 18.0
model initialize at round 1939
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 24.]), 'previousTarget': array([26., 24.]), 'currentState': array([24.68203012,  6.05623842,  1.0434224 ]), 'targetState': array([26, 24], dtype=int32), 'currentDistance': 17.99209893890674}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9262436950958536
{'scaleFactor': 20, 'currentTarget': array([26., 24.]), 'previousTarget': array([26., 24.]), 'currentState': array([26.03007329, 23.72805682,  1.69409172]), 'targetState': array([26, 24], dtype=int32), 'currentDistance': 0.2736009779763325}
episode index:1940
target Thresh 31.99999990385493
target distance 13.0
model initialize at round 1940
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 27.]), 'previousTarget': array([19., 27.]), 'currentState': array([10.21288089, 15.48755894,  1.04747915]), 'targetState': array([19, 27], dtype=int32), 'currentDistance': 14.482740120818036}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9262466944017841
{'scaleFactor': 20, 'currentTarget': array([19., 27.]), 'previousTarget': array([19., 27.]), 'currentState': array([18.76329825, 26.48911835,  1.26536904]), 'targetState': array([19, 27], dtype=int32), 'currentDistance': 0.5630522006037231}
episode index:1941
target Thresh 31.99999990481159
target distance 13.0
model initialize at round 1941
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 14.]), 'previousTarget': array([25., 14.]), 'currentState': array([24.76427609, 25.33361999,  5.58186269]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 11.336071098657913}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9262545386113614
{'scaleFactor': 20, 'currentTarget': array([25., 14.]), 'previousTarget': array([25., 14.]), 'currentState': array([24.95399037, 13.69544894,  5.07451176]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 0.3080068699303566}
episode index:1942
target Thresh 31.99999990575873
target distance 8.0
model initialize at round 1942
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 12.]), 'previousTarget': array([ 8., 12.]), 'currentState': array([6.66315046, 4.25752405, 1.16362112]), 'targetState': array([ 8, 12], dtype=int32), 'currentDistance': 7.857041466278285}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9262722130691013
{'scaleFactor': 20, 'currentTarget': array([ 8., 12.]), 'previousTarget': array([ 8., 12.]), 'currentState': array([ 8.09361788, 12.04456309,  1.87290163]), 'targetState': array([ 8, 12], dtype=int32), 'currentDistance': 0.10368305576694503}
episode index:1943
target Thresh 31.99999990669645
target distance 10.0
model initialize at round 1943
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([17.38449784, 20.04314117,  4.66845414]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 11.069788388286817}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9262800360816178
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.70837842, 10.33951298,  4.45543139]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.7220015568637963}
episode index:1944
target Thresh 31.999999907624833
target distance 16.0
model initialize at round 1944
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 24.]), 'previousTarget': array([26., 24.]), 'currentState': array([24.10862444,  9.42752853,  1.11897755]), 'targetState': array([26, 24], dtype=int32), 'currentDistance': 14.694700621488362}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9262830105349984
{'scaleFactor': 20, 'currentTarget': array([26., 24.]), 'previousTarget': array([26., 24.]), 'currentState': array([25.96688362, 23.10149657,  1.50547065]), 'targetState': array([26, 24], dtype=int32), 'currentDistance': 0.8991135108692654}
episode index:1945
target Thresh 31.99999990854398
target distance 11.0
model initialize at round 1945
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 12.]), 'previousTarget': array([14., 12.]), 'currentState': array([23.97449658, 18.34417986,  3.75606656]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 11.821133620193912}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9262908199588761
{'scaleFactor': 20, 'currentTarget': array([14., 12.]), 'previousTarget': array([14., 12.]), 'currentState': array([13.88346008, 11.99136802,  3.98181969]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 0.1168591616064815}
episode index:1946
target Thresh 31.999999909453983
target distance 6.0
model initialize at round 1946
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 16.]), 'previousTarget': array([ 3., 16.]), 'currentState': array([ 8.10589395, 18.42581992,  3.14089143]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 5.652853729320731}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9263134230302892
{'scaleFactor': 20, 'currentTarget': array([ 3., 16.]), 'previousTarget': array([ 3., 16.]), 'currentState': array([ 2.75852114, 15.95580528,  4.02772319]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 0.24548974281222768}
episode index:1947
target Thresh 31.99999991035493
target distance 8.0
model initialize at round 1947
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([14.9876727 ,  7.99664224,  1.4761468 ]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 6.0881122647455115}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9263360028952634
{'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([16.01136939, 13.86213759,  1.74888994]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 0.13833042870397047}
episode index:1948
target Thresh 31.999999911246913
target distance 12.0
model initialize at round 1948
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 17.]), 'previousTarget': array([ 3., 17.]), 'currentState': array([5.73258335, 6.93979882, 2.06683695]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 10.4247138864012}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9263486524832596
{'scaleFactor': 20, 'currentTarget': array([ 3., 17.]), 'previousTarget': array([ 3., 17.]), 'currentState': array([ 3.21248528, 16.53878791,  2.16309519]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 0.5078056561313152}
episode index:1949
target Thresh 31.999999912130022
target distance 14.0
model initialize at round 1949
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 22.]), 'previousTarget': array([26., 22.]), 'currentState': array([20.03636783,  9.93324823,  1.09887558]), 'targetState': array([26, 22], dtype=int32), 'currentDistance': 13.459992829080214}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9263515841219384
{'scaleFactor': 20, 'currentTarget': array([26., 22.]), 'previousTarget': array([26., 22.]), 'currentState': array([26.28592253, 22.35580539,  1.46823485]), 'targetState': array([26, 22], dtype=int32), 'currentDistance': 0.4564528152739566}
episode index:1950
target Thresh 31.999999913004345
target distance 13.0
model initialize at round 1950
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([24.11874021, 18.678776  ,  2.51018369]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 16.313887549834334}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9263450057843483
{'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([16.83345923,  3.24650436,  4.69060152]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 0.7716809592617462}
episode index:1951
target Thresh 31.999999913869964
target distance 2.0
model initialize at round 1951
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 14.]), 'previousTarget': array([13., 14.]), 'currentState': array([14.58151676, 15.63011038,  2.83209038]), 'targetState': array([13, 14], dtype=int32), 'currentDistance': 2.271223264585822}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9263776159248277
{'scaleFactor': 20, 'currentTarget': array([13., 14.]), 'previousTarget': array([13., 14.]), 'currentState': array([13.273235  , 14.57132635,  4.83185685]), 'targetState': array([13, 14], dtype=int32), 'currentDistance': 0.6333017939092302}
episode index:1952
target Thresh 31.999999914726974
target distance 6.0
model initialize at round 1952
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 24.]), 'previousTarget': array([ 2., 24.]), 'currentState': array([ 1.76735398, 19.97895029,  1.53702119]), 'targetState': array([ 2, 24], dtype=int32), 'currentDistance': 4.027774196589817}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9264051235459619
{'scaleFactor': 20, 'currentTarget': array([ 2., 24.]), 'previousTarget': array([ 2., 24.]), 'currentState': array([ 2.06753482, 23.92424926,  1.85769941]), 'targetState': array([ 2, 24], dtype=int32), 'currentDistance': 0.10148461117059487}
episode index:1953
target Thresh 31.999999915575454
target distance 12.0
model initialize at round 1953
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 16.]), 'previousTarget': array([ 5., 16.]), 'currentState': array([15.53422216,  4.82697244,  1.61792278]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 15.355988454216932}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9264032502454921
{'scaleFactor': 20, 'currentTarget': array([ 5., 16.]), 'previousTarget': array([ 5., 16.]), 'currentState': array([ 5.27898048, 15.61169431,  2.40452166]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 0.4781332628643486}
episode index:1954
target Thresh 31.999999916415494
target distance 18.0
model initialize at round 1954
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 16.]), 'previousTarget': array([ 8., 16.]), 'currentState': array([25.3396394 , 22.45199743,  3.29917336]), 'targetState': array([ 8, 16], dtype=int32), 'currentDistance': 18.50111795079913}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9263966589397316
{'scaleFactor': 20, 'currentTarget': array([ 8., 16.]), 'previousTarget': array([ 8., 16.]), 'currentState': array([ 8.68841069, 16.36426821,  3.67449829]), 'targetState': array([ 8, 16], dtype=int32), 'currentDistance': 0.7788456871482197}
episode index:1955
target Thresh 31.999999917247173
target distance 9.0
model initialize at round 1955
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 7.]), 'previousTarget': array([9., 7.]), 'currentState': array([16.22024562, 11.24098967,  3.99347675]), 'targetState': array([9, 7], dtype=int32), 'currentDistance': 8.373645569028039}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9264141432705395
{'scaleFactor': 20, 'currentTarget': array([9., 7.]), 'previousTarget': array([9., 7.]), 'currentState': array([9.39779121, 7.2669299 , 3.87060547]), 'targetState': array([9, 7], dtype=int32), 'currentDistance': 0.4790505383068321}
episode index:1956
target Thresh 31.999999918070575
target distance 5.0
model initialize at round 1956
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 27.]), 'previousTarget': array([23., 27.]), 'currentState': array([19.19921699, 24.5662297 ,  0.62721789]), 'targetState': array([23, 27], dtype=int32), 'currentDistance': 4.513223830401045}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9264415760026444
{'scaleFactor': 20, 'currentTarget': array([23., 27.]), 'previousTarget': array([23., 27.]), 'currentState': array([22.58870457, 26.63944562,  0.82420665]), 'targetState': array([23, 27], dtype=int32), 'currentDistance': 0.5469583117062976}
episode index:1957
target Thresh 31.999999918885788
target distance 2.0
model initialize at round 1957
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 22.]), 'previousTarget': array([ 9., 22.]), 'currentState': array([ 8.53402586, 22.30093284,  0.57765096]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 0.5547003458209949}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9264791441456461
{'scaleFactor': 20, 'currentTarget': array([ 9., 22.]), 'previousTarget': array([ 9., 22.]), 'currentState': array([ 8.53402586, 22.30093284,  0.57765096]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 0.5547003458209949}
episode index:1958
target Thresh 31.999999919692886
target distance 9.0
model initialize at round 1958
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 14.]), 'previousTarget': array([24., 14.]), 'currentState': array([25.47111885, 22.18256656,  4.76601139]), 'targetState': array([24, 14], dtype=int32), 'currentDistance': 8.313758852321556}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9264965595952911
{'scaleFactor': 20, 'currentTarget': array([24., 14.]), 'previousTarget': array([24., 14.]), 'currentState': array([24.0503602 , 14.38771861,  4.8837322 ]), 'targetState': array([24, 14], dtype=int32), 'currentDistance': 0.39097554028267206}
episode index:1959
target Thresh 31.999999920491955
target distance 11.0
model initialize at round 1959
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([5.99664457, 8.01202916, 6.18839416]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 9.493825820239236}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9265090562740179
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.36444188,  4.8280787 ,  0.1029472 ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.4029575877945062}
episode index:1960
target Thresh 31.999999921283074
target distance 7.0
model initialize at round 1960
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 20.]), 'previousTarget': array([12., 20.]), 'currentState': array([14.3448857 , 26.69506603,  4.33378696]), 'targetState': array([12, 20], dtype=int32), 'currentDistance': 7.093828166881279}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9265264387083504
{'scaleFactor': 20, 'currentTarget': array([12., 20.]), 'previousTarget': array([12., 20.]), 'currentState': array([12.05913646, 19.18582432,  4.90781759]), 'targetState': array([12, 20], dtype=int32), 'currentDistance': 0.8163204958021136}
episode index:1961
target Thresh 31.999999922066323
target distance 18.0
model initialize at round 1961
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 24.]), 'previousTarget': array([26., 24.]), 'currentState': array([10.04733222,  7.59732092,  1.51869407]), 'targetState': array([26, 24], dtype=int32), 'currentDistance': 22.88089793432153}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9265059791940832
{'scaleFactor': 20, 'currentTarget': array([26., 24.]), 'previousTarget': array([26., 24.]), 'currentState': array([26.49444285, 24.28100908,  1.03369883]), 'targetState': array([26, 24], dtype=int32), 'currentDistance': 0.5687177148544815}
episode index:1962
target Thresh 31.999999922841774
target distance 16.0
model initialize at round 1962
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  7.]), 'previousTarget': array([21.,  7.]), 'currentState': array([23.7869579 , 21.04099505,  4.90492988]), 'targetState': array([21,  7], dtype=int32), 'currentDistance': 14.31491097487295}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9265088112718788
{'scaleFactor': 20, 'currentTarget': array([21.,  7.]), 'previousTarget': array([21.,  7.]), 'currentState': array([20.93475312,  7.53377281,  4.77865217]), 'targetState': array([21,  7], dtype=int32), 'currentDistance': 0.5377458227749057}
episode index:1963
target Thresh 31.99999992360951
target distance 27.0
model initialize at round 1963
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 29.]), 'previousTarget': array([ 8., 29.]), 'currentState': array([9.28410979, 0.91213506, 0.30714863]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 28.117202828494186}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.9264749770271585
{'scaleFactor': 20, 'currentTarget': array([ 8., 29.]), 'previousTarget': array([ 8., 29.]), 'currentState': array([ 7.68086575, 29.8492167 ,  2.01099825]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 0.9072021080455169}
episode index:1964
target Thresh 31.99999992436961
target distance 16.0
model initialize at round 1964
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 26.]), 'previousTarget': array([13., 26.]), 'currentState': array([25.57718662, 10.91642024,  3.56346667]), 'targetState': array([13, 26], dtype=int32), 'currentDistance': 19.639246462158372}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9264637338200244
{'scaleFactor': 20, 'currentTarget': array([13., 26.]), 'previousTarget': array([13., 26.]), 'currentState': array([13.41404198, 25.59958244,  2.43350769]), 'targetState': array([13, 26], dtype=int32), 'currentDistance': 0.5759904356472518}
episode index:1965
target Thresh 31.999999925122143
target distance 18.0
model initialize at round 1965
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([20.12491151, 13.34279615,  3.67894065]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 16.294215688637646}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9264618421417985
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.5135028 , 11.06172297,  3.40643828]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.5171990398189461}
episode index:1966
target Thresh 31.999999925867193
target distance 20.0
model initialize at round 1966
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  7.]), 'previousTarget': array([27.,  7.]), 'currentState': array([14.66129561, 26.73076965,  5.11252165]), 'targetState': array([27,  7], dtype=int32), 'currentDistance': 23.271160198075172}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9264414674745765
{'scaleFactor': 20, 'currentTarget': array([27.,  7.]), 'previousTarget': array([27.,  7.]), 'currentState': array([26.94465994,  6.703473  ,  5.50553462]), 'targetState': array([27,  7], dtype=int32), 'currentDistance': 0.3016467837325656}
episode index:1967
target Thresh 31.999999926604826
target distance 20.0
model initialize at round 1967
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  6.]), 'previousTarget': array([23.,  6.]), 'currentState': array([ 9.58256739, 24.80183397,  5.42046121]), 'targetState': array([23,  6], dtype=int32), 'currentDistance': 23.09840813899424}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.926421113513317
{'scaleFactor': 20, 'currentTarget': array([23.,  6.]), 'previousTarget': array([23.,  6.]), 'currentState': array([23.24014849,  5.4741221 ,  5.55358464]), 'targetState': array([23,  6], dtype=int32), 'currentDistance': 0.5781166515006826}
episode index:1968
target Thresh 31.99999992733512
target distance 16.0
model initialize at round 1968
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 16.]), 'previousTarget': array([27., 16.]), 'currentState': array([12.84832734, 19.47946343,  6.00707347]), 'targetState': array([27, 16], dtype=int32), 'currentDistance': 14.573143274737081}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9264239800620188
{'scaleFactor': 20, 'currentTarget': array([27., 16.]), 'previousTarget': array([27., 16.]), 'currentState': array([26.33332008, 16.16687918,  6.12990987]), 'targetState': array([27, 16], dtype=int32), 'currentDistance': 0.687248698346651}
episode index:1969
target Thresh 31.99999992805815
target distance 3.0
model initialize at round 1969
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([17.27615344, 18.33984116,  5.88722134]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 2.744964866077267}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9264562521533578
{'scaleFactor': 20, 'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([19.23199407, 18.02453683,  0.0810613 ]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 0.7683977884878048}
episode index:1970
target Thresh 31.99999992877398
target distance 17.0
model initialize at round 1970
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 26.]), 'previousTarget': array([ 4., 26.]), 'currentState': array([20.45654271, 24.40719045,  3.37358046]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 16.533446103977262}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9264543690697832
{'scaleFactor': 20, 'currentTarget': array([ 4., 26.]), 'previousTarget': array([ 4., 26.]), 'currentState': array([ 4.64719938, 25.91643974,  3.20421102]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 0.6525713412286454}
episode index:1971
target Thresh 31.99999992948269
target distance 15.0
model initialize at round 1971
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 15.]), 'previousTarget': array([17., 15.]), 'currentState': array([ 0.39781052, 18.48485248,  4.46268034]), 'targetState': array([17, 15], dtype=int32), 'currentDistance': 16.96398809703162}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9264478086632993
{'scaleFactor': 20, 'currentTarget': array([17., 15.]), 'previousTarget': array([17., 15.]), 'currentState': array([17.12177439, 14.98185338,  6.26514264]), 'targetState': array([17, 15], dtype=int32), 'currentDistance': 0.12311905406469378}
episode index:1972
target Thresh 31.99999993018435
target distance 25.0
model initialize at round 1972
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 14.]), 'previousTarget': array([ 2., 14.]), 'currentState': array([26.74211772,  3.66309495,  2.73463285]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 26.8146227875565}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9264185628468299
{'scaleFactor': 20, 'currentTarget': array([ 2., 14.]), 'previousTarget': array([ 2., 14.]), 'currentState': array([ 1.57332065, 14.34764517,  2.79225645]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 0.5503748061999715}
episode index:1973
target Thresh 31.999999930879028
target distance 7.0
model initialize at round 1973
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 19.]), 'previousTarget': array([ 9., 19.]), 'currentState': array([ 2.98472688, 10.63519193,  0.06422251]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 10.303083263235873}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9264261928298868
{'scaleFactor': 20, 'currentTarget': array([ 9., 19.]), 'previousTarget': array([ 9., 19.]), 'currentState': array([ 8.93215949, 19.15590737,  1.21969831]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 0.17002777162548843}
episode index:1974
target Thresh 31.999999931566794
target distance 16.0
model initialize at round 1974
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([ 6.86146247, 21.51732128,  5.54124898]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 17.043404818244387}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9264196566550279
{'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([21.46363186, 11.56755743,  5.83742875]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 0.6340040096461451}
episode index:1975
target Thresh 31.999999932247714
target distance 8.0
model initialize at round 1975
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([9.70021318, 3.864636  , 5.23226345]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 6.569945314277344}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9264369523804049
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.50238209,  2.03430246,  0.0374988 ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.5035518025597886}
episode index:1976
target Thresh 31.99999993292186
target distance 7.0
model initialize at round 1976
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([12.23736098, 22.49973327,  5.25207996]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 6.942787201498221}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9264542306088418
{'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([ 7.96177827, 17.21962181,  4.23530149]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 0.22292295165987555}
episode index:1977
target Thresh 31.9999999335893
target distance 12.0
model initialize at round 1977
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  8.]), 'previousTarget': array([19.,  8.]), 'currentState': array([13.14623326, 18.5496654 ,  5.1903429 ]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 12.06490883349541}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9264618271299703
{'scaleFactor': 20, 'currentTarget': array([19.,  8.]), 'previousTarget': array([19.,  8.]), 'currentState': array([18.91299289,  8.1172057 ,  5.48312301]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 0.14597059012510455}
episode index:1978
target Thresh 31.9999999342501
target distance 19.0
model initialize at round 1978
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([4.53062457, 6.69969745, 1.43876904]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 17.99087769076342}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.926450670105149
{'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([21.87190124, 11.03392268,  0.28819322]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 0.1325143014260766}
episode index:1979
target Thresh 31.99999993490432
target distance 7.0
model initialize at round 1979
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 27.]), 'previousTarget': array([18., 27.]), 'currentState': array([25.93730902, 27.39779822,  1.99010247]), 'targetState': array([18, 27], dtype=int32), 'currentDistance': 7.947271102394197}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9264679152263081
{'scaleFactor': 20, 'currentTarget': array([18., 27.]), 'previousTarget': array([18., 27.]), 'currentState': array([18.54402809, 27.03475579,  3.28066544]), 'targetState': array([18, 27], dtype=int32), 'currentDistance': 0.5451371601660069}
episode index:1980
target Thresh 31.999999935552033
target distance 9.0
model initialize at round 1980
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 15.]), 'previousTarget': array([ 6., 15.]), 'currentState': array([6.26070195, 7.96446294, 1.67593622]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 7.040365559132173}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.926485142936946
{'scaleFactor': 20, 'currentTarget': array([ 6., 15.]), 'previousTarget': array([ 6., 15.]), 'currentState': array([ 5.51190543, 15.8184561 ,  2.11020343]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 0.9529463271978883}
episode index:1981
target Thresh 31.9999999361933
target distance 13.0
model initialize at round 1981
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 12.]), 'previousTarget': array([ 2., 12.]), 'currentState': array([13.93417838, 17.4041209 ,  3.62278455]), 'targetState': array([ 2, 12], dtype=int32), 'currentDistance': 13.100730369191313}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9264879583784041
{'scaleFactor': 20, 'currentTarget': array([ 2., 12.]), 'previousTarget': array([ 2., 12.]), 'currentState': array([ 1.31797075, 11.53813765,  3.89992352]), 'targetState': array([ 2, 12], dtype=int32), 'currentDistance': 0.8236994181909788}
episode index:1982
target Thresh 31.99999993682819
target distance 1.0
model initialize at round 1982
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 20.]), 'previousTarget': array([16., 20.]), 'currentState': array([16.085713  , 20.46789614,  1.19820976]), 'targetState': array([16, 20], dtype=int32), 'currentDistance': 0.47568215583613604}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9265250295037806
{'scaleFactor': 20, 'currentTarget': array([16., 20.]), 'previousTarget': array([16., 20.]), 'currentState': array([16.085713  , 20.46789614,  1.19820976]), 'targetState': array([16, 20], dtype=int32), 'currentDistance': 0.47568215583613604}
episode index:1983
target Thresh 31.999999937456757
target distance 7.0
model initialize at round 1983
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 25.]), 'previousTarget': array([21., 25.]), 'currentState': array([19.47293479, 19.61515352,  2.29594782]), 'targetState': array([21, 25], dtype=int32), 'currentDistance': 5.597186773705743}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9265470929969742
{'scaleFactor': 20, 'currentTarget': array([21., 25.]), 'previousTarget': array([21., 25.]), 'currentState': array([20.77954613, 24.49586802,  0.30826083]), 'targetState': array([21, 25], dtype=int32), 'currentDistance': 0.5502262794833307}
episode index:1984
target Thresh 31.999999938079075
target distance 12.0
model initialize at round 1984
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 17.]), 'previousTarget': array([13., 17.]), 'currentState': array([21.36067563,  5.99042916,  1.63920325]), 'targetState': array([13, 17], dtype=int32), 'currentDistance': 13.82431000724983}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.926549872974259
{'scaleFactor': 20, 'currentTarget': array([13., 17.]), 'previousTarget': array([13., 17.]), 'currentState': array([13.38065457, 17.02926851,  2.44619113]), 'targetState': array([13, 17], dtype=int32), 'currentDistance': 0.38177813934282}
episode index:1985
target Thresh 31.999999938695197
target distance 9.0
model initialize at round 1985
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 14.]), 'previousTarget': array([15., 14.]), 'currentState': array([ 9.32777476, 23.18987055,  4.03853321]), 'targetState': array([15, 14], dtype=int32), 'currentDistance': 10.79943794235417}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9265573907368102
{'scaleFactor': 20, 'currentTarget': array([15., 14.]), 'previousTarget': array([15., 14.]), 'currentState': array([15.42094293, 13.60205504,  5.63719941]), 'targetState': array([15, 14], dtype=int32), 'currentDistance': 0.5792694893558712}
episode index:1986
target Thresh 31.99999993930519
target distance 15.0
model initialize at round 1986
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([15.40720154,  5.79706812,  3.67787997]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 13.934517552926756}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9265601627333728
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.99524222, 2.24783222, 3.74154941]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.2478778873653897}
episode index:1987
target Thresh 31.999999939909113
target distance 9.0
model initialize at round 1987
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 19.]), 'previousTarget': array([27., 19.]), 'currentState': array([25.64869701, 11.84225423,  1.61233366]), 'targetState': array([27, 19], dtype=int32), 'currentDistance': 7.284184527896778}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9265772833808913
{'scaleFactor': 20, 'currentTarget': array([27., 19.]), 'previousTarget': array([27., 19.]), 'currentState': array([26.95523394, 19.67259946,  1.70690014]), 'targetState': array([27, 19], dtype=int32), 'currentDistance': 0.6740875589659396}
episode index:1988
target Thresh 31.999999940507028
target distance 11.0
model initialize at round 1988
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 15.]), 'previousTarget': array([ 9., 15.]), 'currentState': array([1.78303864, 5.16249436, 1.36908102]), 'targetState': array([ 9, 15], dtype=int32), 'currentDistance': 12.200862609312741}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9265847760234354
{'scaleFactor': 20, 'currentTarget': array([ 9., 15.]), 'previousTarget': array([ 9., 15.]), 'currentState': array([ 9.01618009, 14.42634422,  1.10459623]), 'targetState': array([ 9, 15], dtype=int32), 'currentDistance': 0.5738839154676606}
episode index:1989
target Thresh 31.99999994109899
target distance 14.0
model initialize at round 1989
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 16.]), 'previousTarget': array([12., 16.]), 'currentState': array([24.581645  , 25.62457553,  4.18464863]), 'targetState': array([12, 16], dtype=int32), 'currentDistance': 15.840777914929722}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9265828463341913
{'scaleFactor': 20, 'currentTarget': array([12., 16.]), 'previousTarget': array([12., 16.]), 'currentState': array([12.20530957, 15.73180148,  3.78516896]), 'targetState': array([12, 16], dtype=int32), 'currentDistance': 0.33776095465763256}
episode index:1990
target Thresh 31.99999994168507
target distance 17.0
model initialize at round 1990
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 14.]), 'previousTarget': array([20., 14.]), 'currentState': array([2.60101302, 1.36500842, 5.48303699]), 'targetState': array([20, 14], dtype=int32), 'currentDistance': 21.50273843480316}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9265671534200399
{'scaleFactor': 20, 'currentTarget': array([20., 14.]), 'previousTarget': array([20., 14.]), 'currentState': array([19.81784755, 14.01718645,  0.89601533]), 'targetState': array([20, 14], dtype=int32), 'currentDistance': 0.1829614364282044}
episode index:1991
target Thresh 31.999999942265312
target distance 10.0
model initialize at round 1991
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 28.]), 'previousTarget': array([ 5., 28.]), 'currentState': array([15.2755969 , 28.66025132,  2.41629919]), 'targetState': array([ 5, 28], dtype=int32), 'currentDistance': 10.296787042512344}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9265746398638054
{'scaleFactor': 20, 'currentTarget': array([ 5., 28.]), 'previousTarget': array([ 5., 28.]), 'currentState': array([ 4.24714963, 28.0735074 ,  3.3702189 ]), 'targetState': array([ 5, 28], dtype=int32), 'currentDistance': 0.7564304427328197}
episode index:1992
target Thresh 31.999999942839782
target distance 8.0
model initialize at round 1992
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([10.93183298,  4.68163995,  2.6212247 ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 6.408014864525383}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9265917102953841
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.60124592, 11.23484716,  2.16920716]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.6454841948414224}
episode index:1993
target Thresh 31.999999943408536
target distance 9.0
model initialize at round 1993
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([14.84907736,  3.5027464 ,  1.99207206]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 10.360719379650623}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9265991769147951
{'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([22.13458793, 11.37009162,  1.10590613]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 0.39380416235655297}
episode index:1994
target Thresh 31.99999994397163
target distance 21.0
model initialize at round 1994
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([23.55170398, 24.85722034,  3.61716366]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 25.81540249444813}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9265745763363912
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.38279903, 7.99242329, 3.62957653]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.3828740096044646}
episode index:1995
target Thresh 31.99999994452912
target distance 3.0
model initialize at round 1995
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  5.]), 'previousTarget': array([23.,  5.]), 'currentState': array([22.67977492,  1.89634527,  0.94837063]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 3.1201308960578564}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.926601392680912
{'scaleFactor': 20, 'currentTarget': array([23.,  5.]), 'previousTarget': array([23.,  5.]), 'currentState': array([22.74585223,  5.69362209,  2.0512914 ]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 0.738716925171529}
episode index:1996
target Thresh 31.999999945081065
target distance 6.0
model initialize at round 1996
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  6.]), 'previousTarget': array([20.,  6.]), 'currentState': array([26.05872981, 13.68194497,  2.54589272]), 'targetState': array([20,  6], dtype=int32), 'currentDistance': 9.783684655093177}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9266136053284929
{'scaleFactor': 20, 'currentTarget': array([20.,  6.]), 'previousTarget': array([20.,  6.]), 'currentState': array([20.4081302 ,  6.28675723,  4.05433558]), 'targetState': array([20,  6], dtype=int32), 'currentDistance': 0.4987985192575522}
episode index:1997
target Thresh 31.99999994562752
target distance 16.0
model initialize at round 1997
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 5.]), 'previousTarget': array([8., 5.]), 'currentState': array([21.70113983, 19.92978958,  4.8407743 ]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 20.263757043614493}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.926602478436441
{'scaleFactor': 20, 'currentTarget': array([8., 5.]), 'previousTarget': array([8., 5.]), 'currentState': array([8.59237863, 5.38349779, 3.84666456]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 0.7056791051061798}
episode index:1998
target Thresh 31.999999946168533
target distance 13.0
model initialize at round 1998
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 25.]), 'previousTarget': array([12., 25.]), 'currentState': array([25.06601723, 24.6816747 ,  2.54155964]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 13.06989431010512}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9266052112375768
{'scaleFactor': 20, 'currentTarget': array([12., 25.]), 'previousTarget': array([12., 25.]), 'currentState': array([11.40138198, 25.09991605,  3.25052792]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 0.6068992908307614}
episode index:1999
target Thresh 31.999999946704165
target distance 9.0
model initialize at round 1999
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 12.]), 'previousTarget': array([15., 12.]), 'currentState': array([9.48095794, 2.20053038, 0.51499623]), 'targetState': array([15, 12], dtype=int32), 'currentDistance': 11.246751975865479}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9266126487066585
{'scaleFactor': 20, 'currentTarget': array([15., 12.]), 'previousTarget': array([15., 12.]), 'currentState': array([15.29443697, 12.48129366,  1.3115574 ]), 'targetState': array([15, 12], dtype=int32), 'currentDistance': 0.5642133561550962}
episode index:2000
target Thresh 31.999999947234468
target distance 12.0
model initialize at round 2000
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 17.]), 'previousTarget': array([14., 17.]), 'currentState': array([21.95052135,  6.68224254,  2.61020017]), 'targetState': array([14, 17], dtype=int32), 'currentDistance': 13.025625083912205}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9266153736937651
{'scaleFactor': 20, 'currentTarget': array([14., 17.]), 'previousTarget': array([14., 17.]), 'currentState': array([13.49398969, 17.60026427,  2.40023037]), 'targetState': array([14, 17], dtype=int32), 'currentDistance': 0.785088288805498}
episode index:2001
target Thresh 31.999999947759495
target distance 12.0
model initialize at round 2001
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 25.]), 'previousTarget': array([17., 25.]), 'currentState': array([13.84286091, 11.54330045,  6.24693203]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 13.822094268966433}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9266180959586069
{'scaleFactor': 20, 'currentTarget': array([17., 25.]), 'previousTarget': array([17., 25.]), 'currentState': array([16.95316213, 24.49737771,  1.43712209]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 0.5047999141787894}
episode index:2002
target Thresh 31.999999948279296
target distance 22.0
model initialize at round 2002
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([ 8.98094103, 26.42223548,  4.6017952 ]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 20.64418608223522}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9266024794625011
{'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([12.18593754,  5.03887109,  4.94977567]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 0.978949209192226}
episode index:2003
target Thresh 31.999999948793924
target distance 5.0
model initialize at round 2003
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([ 8.76423914, 11.33362522,  5.58184052]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 5.494542447600178}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9266242841134679
{'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([13.59422747, 12.80779597,  1.29927152]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 0.4489919115692844}
episode index:2004
target Thresh 31.999999949303433
target distance 19.0
model initialize at round 2004
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 28.]), 'previousTarget': array([ 8., 28.]), 'currentState': array([25.44965467, 18.34515845,  2.53124571]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 19.942577907384035}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9266131907423433
{'scaleFactor': 20, 'currentTarget': array([ 8., 28.]), 'previousTarget': array([ 8., 28.]), 'currentState': array([ 8.37786298, 28.13558471,  2.84371578]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 0.4014519219025279}
episode index:2005
target Thresh 31.999999949807872
target distance 17.0
model initialize at round 2005
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  6.]), 'previousTarget': array([23.,  6.]), 'currentState': array([14.1346016 , 21.75699118,  0.17903655]), 'targetState': array([23,  6], dtype=int32), 'currentDistance': 18.07976935354419}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9266021084314093
{'scaleFactor': 20, 'currentTarget': array([23.,  6.]), 'previousTarget': array([23.,  6.]), 'currentState': array([23.03606129,  5.66286831,  5.33166952]), 'targetState': array([23,  6], dtype=int32), 'currentDistance': 0.3390548477068696}
episode index:2006
target Thresh 31.99999995030729
target distance 15.0
model initialize at round 2006
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([17.00048497,  4.6467011 ,  0.07581728]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 17.10073784380612}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9265955888195768
{'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([11.95364529, 20.96184696,  2.20536768]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 0.06003677004658048}
episode index:2007
target Thresh 31.999999950801744
target distance 6.0
model initialize at round 2007
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 11.]), 'previousTarget': array([25., 11.]), 'currentState': array([20.66335975, 13.25616877,  5.4259916 ]), 'targetState': array([25, 11], dtype=int32), 'currentDistance': 4.8884298314843}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9266222344426747
{'scaleFactor': 20, 'currentTarget': array([25., 11.]), 'previousTarget': array([25., 11.]), 'currentState': array([24.1166874 , 11.32142217,  5.8301244 ]), 'targetState': array([25, 11], dtype=int32), 'currentDistance': 0.9399751933028814}
episode index:2008
target Thresh 31.999999951291276
target distance 10.0
model initialize at round 2008
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 25.]), 'previousTarget': array([23., 25.]), 'currentState': array([22.57429406, 14.40503252,  0.64866608]), 'targetState': array([23, 25], dtype=int32), 'currentDistance': 10.603516470689701}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9266296301196076
{'scaleFactor': 20, 'currentTarget': array([23., 25.]), 'previousTarget': array([23., 25.]), 'currentState': array([23.06854172, 24.79225407,  1.68123841]), 'targetState': array([23, 25], dtype=int32), 'currentDistance': 0.2187609146963217}
episode index:2009
target Thresh 31.999999951775933
target distance 16.0
model initialize at round 2009
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 18.]), 'previousTarget': array([ 2., 18.]), 'currentState': array([16.89171682, 13.7334707 ,  2.98352766]), 'targetState': array([ 2, 18], dtype=int32), 'currentDistance': 15.490852207218143}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9266276973157809
{'scaleFactor': 20, 'currentTarget': array([ 2., 18.]), 'previousTarget': array([ 2., 18.]), 'currentState': array([ 1.53525525, 17.99265671,  3.21755094]), 'targetState': array([ 2, 18], dtype=int32), 'currentDistance': 0.46480276350121236}
episode index:2010
target Thresh 31.99999995225577
target distance 17.0
model initialize at round 2010
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 23.]), 'previousTarget': array([10., 23.]), 'currentState': array([25.34682986, 25.31530392,  3.96312928]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 15.520496735739096}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9266257664341857
{'scaleFactor': 20, 'currentTarget': array([10., 23.]), 'previousTarget': array([10., 23.]), 'currentState': array([ 9.78098608, 23.09932207,  3.57423099]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 0.24048278278133245}
episode index:2011
target Thresh 31.999999952730835
target distance 11.0
model initialize at round 2011
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 23.]), 'previousTarget': array([19., 23.]), 'currentState': array([ 9.32894809, 15.9673892 ,  0.34943312]), 'targetState': array([19, 23], dtype=int32), 'currentDistance': 11.9577113088147}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9266331493283044
{'scaleFactor': 20, 'currentTarget': array([19., 23.]), 'previousTarget': array([19., 23.]), 'currentState': array([18.9153378 , 22.89825498,  0.65526507]), 'targetState': array([19, 23], dtype=int32), 'currentDistance': 0.13236214106482894}
episode index:2012
target Thresh 31.999999953201172
target distance 11.0
model initialize at round 2012
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 14.]), 'previousTarget': array([12., 14.]), 'currentState': array([19.01169491,  4.34493922,  1.93587368]), 'targetState': array([12, 14], dtype=int32), 'currentDistance': 11.93247938528014}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9266405248872079
{'scaleFactor': 20, 'currentTarget': array([12., 14.]), 'previousTarget': array([12., 14.]), 'currentState': array([11.91793931, 13.91450679,  2.36208059]), 'targetState': array([12, 14], dtype=int32), 'currentDistance': 0.11850336469852996}
episode index:2013
target Thresh 31.999999953666826
target distance 5.0
model initialize at round 2013
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 17.]), 'previousTarget': array([15., 17.]), 'currentState': array([18.12393706, 15.69240556,  2.81516491]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 3.386559602765254}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.926667068817254
{'scaleFactor': 20, 'currentTarget': array([15., 17.]), 'previousTarget': array([15., 17.]), 'currentState': array([14.372116  , 17.00596235,  3.08116224]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 0.6279123064358839}
episode index:2014
target Thresh 31.99999995412785
target distance 21.0
model initialize at round 2014
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 4.]), 'previousTarget': array([8., 4.]), 'currentState': array([ 7.01998989, 23.66134071,  4.35349359]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 19.685749623907743}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9266560092669768
{'scaleFactor': 20, 'currentTarget': array([8., 4.]), 'previousTarget': array([8., 4.]), 'currentState': array([8.19105091, 4.21417302, 4.68856978]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 0.2870026719114748}
episode index:2015
target Thresh 31.999999954584286
target distance 9.0
model initialize at round 2015
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 18.]), 'previousTarget': array([ 8., 18.]), 'currentState': array([12.29531746,  9.91198567,  2.19624734]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 9.157823314659593}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.92666807972364
{'scaleFactor': 20, 'currentTarget': array([ 8., 18.]), 'previousTarget': array([ 8., 18.]), 'currentState': array([ 7.44505916, 18.55491487,  2.04556602]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 0.784786499592207}
episode index:2016
target Thresh 31.99999995503618
target distance 6.0
model initialize at round 2016
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 26.]), 'previousTarget': array([11., 26.]), 'currentState': array([15.00523261, 24.92963803,  3.06628273]), 'targetState': array([11, 26], dtype=int32), 'currentDistance': 4.145788586727393}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9266945705120764
{'scaleFactor': 20, 'currentTarget': array([11., 26.]), 'previousTarget': array([11., 26.]), 'currentState': array([11.10811082, 25.79888605,  2.93515617]), 'targetState': array([11, 26], dtype=int32), 'currentDistance': 0.22833039348030237}
episode index:2017
target Thresh 31.999999955483577
target distance 6.0
model initialize at round 2017
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  9.]), 'previousTarget': array([20.,  9.]), 'currentState': array([15.52158036, 13.70232852,  5.60242479]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 6.493699718553743}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.926716178257115
{'scaleFactor': 20, 'currentTarget': array([20.,  9.]), 'previousTarget': array([20.,  9.]), 'currentState': array([19.67198873,  9.3973501 ,  5.39145991]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 0.5152460525841459}
episode index:2018
target Thresh 31.999999955926523
target distance 11.0
model initialize at round 2018
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 26.]), 'previousTarget': array([14., 26.]), 'currentState': array([16.52168396, 14.28106606,  0.56862658]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 11.987172422641876}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9267234907737787
{'scaleFactor': 20, 'currentTarget': array([14., 26.]), 'previousTarget': array([14., 26.]), 'currentState': array([13.99184156, 25.318022  ,  1.46319166]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 0.6820267928484156}
episode index:2019
target Thresh 31.99999995636506
target distance 17.0
model initialize at round 2019
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 22.]), 'previousTarget': array([ 7., 22.]), 'currentState': array([21.82591817,  6.20578603,  3.35287344]), 'targetState': array([ 7, 22], dtype=int32), 'currentDistance': 21.66252627525806}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9267079535279792
{'scaleFactor': 20, 'currentTarget': array([ 7., 22.]), 'previousTarget': array([ 7., 22.]), 'currentState': array([ 7.07296209, 21.61380827,  2.16182626]), 'targetState': array([ 7, 22], dtype=int32), 'currentDistance': 0.39302356108059733}
episode index:2020
target Thresh 31.999999956799236
target distance 10.0
model initialize at round 2020
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 19.]), 'previousTarget': array([14., 19.]), 'currentState': array([22.86861219, 23.39679065,  3.75684547]), 'targetState': array([14, 19], dtype=int32), 'currentDistance': 9.898689315966474}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.926719968419801
{'scaleFactor': 20, 'currentTarget': array([14., 19.]), 'previousTarget': array([14., 19.]), 'currentState': array([13.97060044, 18.93281371,  3.81671336]), 'targetState': array([14, 19], dtype=int32), 'currentDistance': 0.07333710972266602}
episode index:2021
target Thresh 31.99999995722909
target distance 10.0
model initialize at round 2021
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 28.]), 'previousTarget': array([ 4., 28.]), 'currentState': array([ 3.3323609 , 19.89683929,  1.8767814 ]), 'targetState': array([ 4, 28], dtype=int32), 'currentDistance': 8.130618392723061}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9267367221495637
{'scaleFactor': 20, 'currentTarget': array([ 4., 28.]), 'previousTarget': array([ 4., 28.]), 'currentState': array([ 3.90150813, 27.79033543,  1.56138553]), 'targetState': array([ 4, 28], dtype=int32), 'currentDistance': 0.23164601950408895}
episode index:2022
target Thresh 31.99999995765467
target distance 13.0
model initialize at round 2022
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 26.]), 'previousTarget': array([ 9., 26.]), 'currentState': array([15.63984262, 13.3785555 ,  1.23687428]), 'targetState': array([ 9, 26], dtype=int32), 'currentDistance': 14.261429493329643}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9267347488288906
{'scaleFactor': 20, 'currentTarget': array([ 9., 26.]), 'previousTarget': array([ 9., 26.]), 'currentState': array([ 8.91666575, 26.05200478,  2.17275737]), 'targetState': array([ 9, 26], dtype=int32), 'currentDistance': 0.09822980188284443}
episode index:2023
target Thresh 31.99999995807601
target distance 21.0
model initialize at round 2023
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 24.]), 'previousTarget': array([27., 24.]), 'currentState': array([ 4.78002799, 14.84066554,  4.91151094]), 'targetState': array([27, 24], dtype=int32), 'currentDistance': 24.03373803052718}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9267104337469588
{'scaleFactor': 20, 'currentTarget': array([27., 24.]), 'previousTarget': array([27., 24.]), 'currentState': array([27.81400246, 24.34767703,  0.59636182]), 'targetState': array([27, 24], dtype=int32), 'currentDistance': 0.8851436725043516}
episode index:2024
target Thresh 31.999999958493163
target distance 17.0
model initialize at round 2024
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([11.986269  , 10.81703517,  6.06365316]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 16.926847236424873}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9267084753571716
{'scaleFactor': 20, 'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([26.05037649,  3.51194271,  5.65740017]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 1.0788281374778261}
episode index:2025
target Thresh 31.999999958906162
target distance 20.0
model initialize at round 2025
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 29.]), 'previousTarget': array([19., 29.]), 'currentState': array([17.46340291,  9.68648196,  1.7114408 ]), 'targetState': array([19, 29], dtype=int32), 'currentDistance': 19.374547985941426}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9266974554162296
{'scaleFactor': 20, 'currentTarget': array([19., 29.]), 'previousTarget': array([19., 29.]), 'currentState': array([19.0905188 , 29.26068942,  1.68506561]), 'targetState': array([19, 29], dtype=int32), 'currentDistance': 0.27595765787726695}
episode index:2026
target Thresh 31.99999995931505
target distance 17.0
model initialize at round 2026
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  3.]), 'previousTarget': array([11.,  3.]), 'currentState': array([24.2545911 , 18.49110847,  3.2435441 ]), 'targetState': array([11,  3], dtype=int32), 'currentDistance': 20.38770773763559}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9266819846707152
{'scaleFactor': 20, 'currentTarget': array([11.,  3.]), 'previousTarget': array([11.,  3.]), 'currentState': array([10.69431632,  2.76673185,  4.10751508]), 'targetState': array([11,  3], dtype=int32), 'currentDistance': 0.3845211859692838}
episode index:2027
target Thresh 31.999999959719876
target distance 19.0
model initialize at round 2027
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  8.]), 'previousTarget': array([26.,  8.]), 'currentState': array([8.67651733, 4.14723356, 1.09759634]), 'targetState': array([26,  8], dtype=int32), 'currentDistance': 17.746742264291832}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9266754931829504
{'scaleFactor': 20, 'currentTarget': array([26.,  8.]), 'previousTarget': array([26.,  8.]), 'currentState': array([25.80829067,  8.04026769,  0.15288023]), 'targetState': array([26,  8], dtype=int32), 'currentDistance': 0.19589270754559823}
episode index:2028
target Thresh 31.999999960120668
target distance 5.0
model initialize at round 2028
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([9.16370847, 0.32501116, 5.81981659]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 7.456219829947895}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9266922110325398
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.93877887,  5.99270799,  1.05722673]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.06165387408209499}
episode index:2029
target Thresh 31.999999960517474
target distance 14.0
model initialize at round 2029
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 11.]), 'previousTarget': array([19., 11.]), 'currentState': array([ 3.95722652, 23.67901114,  5.05415201]), 'targetState': array([19, 11], dtype=int32), 'currentDistance': 19.673392116160983}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9266812208177498
{'scaleFactor': 20, 'currentTarget': array([19., 11.]), 'previousTarget': array([19., 11.]), 'currentState': array([18.92756125, 11.13497722,  5.57917186]), 'targetState': array([19, 11], dtype=int32), 'currentDistance': 0.15318688894099827}
episode index:2030
target Thresh 31.999999960910333
target distance 19.0
model initialize at round 2030
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 25.]), 'previousTarget': array([17., 25.]), 'currentState': array([13.89606988,  7.99728026,  1.61546186]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 17.28371663606432}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9266747392946902
{'scaleFactor': 20, 'currentTarget': array([17., 25.]), 'previousTarget': array([17., 25.]), 'currentState': array([17.24909927, 25.5175319 ,  1.52877975]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 0.5743602650046672}
episode index:2031
target Thresh 31.99999996129928
target distance 24.0
model initialize at round 2031
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 29.]), 'previousTarget': array([11., 29.]), 'currentState': array([21.66928577,  6.03033379,  1.47273803]), 'targetState': array([11, 29], dtype=int32), 'currentDistance': 25.3266504810142}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9266505494736785
{'scaleFactor': 20, 'currentTarget': array([11., 29.]), 'previousTarget': array([11., 29.]), 'currentState': array([10.97669206, 28.65346341,  1.86900403]), 'targetState': array([11, 29], dtype=int32), 'currentDistance': 0.3473195457588164}
episode index:2032
target Thresh 31.99999996168436
target distance 9.0
model initialize at round 2032
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 16.]), 'previousTarget': array([ 8., 16.]), 'currentState': array([7.39234088, 8.88107185, 1.85809577]), 'targetState': array([ 8, 16], dtype=int32), 'currentDistance': 7.144815437690957}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9266672466997121
{'scaleFactor': 20, 'currentTarget': array([ 8., 16.]), 'previousTarget': array([ 8., 16.]), 'currentState': array([ 8.06213935, 16.76020774,  1.67331254]), 'targetState': array([ 8, 16], dtype=int32), 'currentDistance': 0.7627431419623183}
episode index:2033
target Thresh 31.999999962065605
target distance 10.0
model initialize at round 2033
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 19.]), 'previousTarget': array([26., 19.]), 'currentState': array([17.42351325, 10.58115499,  0.91041684]), 'targetState': array([26, 19], dtype=int32), 'currentDistance': 12.018031291835765}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9266745293460746
{'scaleFactor': 20, 'currentTarget': array([26., 19.]), 'previousTarget': array([26., 19.]), 'currentState': array([25.89076632, 19.00903605,  0.87988746]), 'targetState': array([26, 19], dtype=int32), 'currentDistance': 0.1096067847746206}
episode index:2034
target Thresh 31.99999996244306
target distance 10.0
model initialize at round 2034
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 11.]), 'previousTarget': array([24., 11.]), 'currentState': array([15.29399399,  1.92391095,  0.31628388]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 12.576562853534476}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9266818048350451
{'scaleFactor': 20, 'currentTarget': array([24., 11.]), 'previousTarget': array([24., 11.]), 'currentState': array([23.41861522, 10.53608979,  0.68394199]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 0.7437882347621841}
episode index:2035
target Thresh 31.99999996281676
target distance 8.0
model initialize at round 2035
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 14.]), 'previousTarget': array([21., 14.]), 'currentState': array([14.87199729, 15.54242601,  6.16860891]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 6.319137218603924}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.926703227818918
{'scaleFactor': 20, 'currentTarget': array([21., 14.]), 'previousTarget': array([21., 14.]), 'currentState': array([20.69104319, 14.14961423,  6.10773133]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 0.3432764581674618}
episode index:2036
target Thresh 31.99999996318674
target distance 5.0
model initialize at round 2036
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  7.]), 'previousTarget': array([24.,  7.]), 'currentState': array([26.25937761,  3.66286239,  2.42606062]), 'targetState': array([24,  7], dtype=int32), 'currentDistance': 4.030046474366422}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9267294412564148
{'scaleFactor': 20, 'currentTarget': array([24.,  7.]), 'previousTarget': array([24.,  7.]), 'currentState': array([23.9398062 ,  6.89790577,  2.24448127]), 'targetState': array([24,  7], dtype=int32), 'currentDistance': 0.11851804138997467}
episode index:2037
target Thresh 31.999999963553034
target distance 22.0
model initialize at round 2037
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 24.]), 'previousTarget': array([14., 24.]), 'currentState': array([3.49385498, 1.22489653, 0.53137749]), 'targetState': array([14, 24], dtype=int32), 'currentDistance': 25.081555403529485}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9267052958107537
{'scaleFactor': 20, 'currentTarget': array([14., 24.]), 'previousTarget': array([14., 24.]), 'currentState': array([13.99341823, 23.99436343,  1.17867611]), 'targetState': array([14, 24], dtype=int32), 'currentDistance': 0.008665489154596539}
episode index:2038
target Thresh 31.999999963915688
target distance 16.0
model initialize at round 2038
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 17.]), 'previousTarget': array([27., 17.]), 'currentState': array([ 9.3206726 , 22.88933121,  4.21739817]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 18.634453017160208}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.926694347688732
{'scaleFactor': 20, 'currentTarget': array([27., 17.]), 'previousTarget': array([27., 17.]), 'currentState': array([27.34385131, 16.78852809,  6.22332824]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 0.4036757300700258}
episode index:2039
target Thresh 31.999999964274732
target distance 16.0
model initialize at round 2039
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 21.]), 'previousTarget': array([ 8., 21.]), 'currentState': array([22.31688418, 18.99683052,  2.13372529]), 'targetState': array([ 8, 21], dtype=int32), 'currentDistance': 14.456343261711744}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9266924115841924
{'scaleFactor': 20, 'currentTarget': array([ 8., 21.]), 'previousTarget': array([ 8., 21.]), 'currentState': array([ 8.04253645, 21.03471384,  3.15378596]), 'targetState': array([ 8, 21], dtype=int32), 'currentDistance': 0.05490355921185047}
episode index:2040
target Thresh 31.999999964630206
target distance 10.0
model initialize at round 2040
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  2.]), 'previousTarget': array([26.,  2.]), 'currentState': array([17.95719231,  4.59310333,  6.02403858]), 'targetState': array([26,  2], dtype=int32), 'currentDistance': 8.45049941589556}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.926709022852402
{'scaleFactor': 20, 'currentTarget': array([26.,  2.]), 'previousTarget': array([26.,  2.]), 'currentState': array([25.53874181,  2.09979865,  6.11695693]), 'targetState': array([26,  2], dtype=int32), 'currentDistance': 0.47193102241916585}
episode index:2041
target Thresh 31.99999996498214
target distance 17.0
model initialize at round 2041
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 5.]), 'previousTarget': array([6., 5.]), 'currentState': array([17.44076802, 20.09470539,  4.21724755]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 18.940467357613574}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9267025626294007
{'scaleFactor': 20, 'currentTarget': array([6., 5.]), 'previousTarget': array([6., 5.]), 'currentState': array([6.6548146 , 5.84919201, 3.94294367]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 1.0723382090102582}
episode index:2042
target Thresh 31.999999965330574
target distance 6.0
model initialize at round 2042
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 13.]), 'previousTarget': array([15., 13.]), 'currentState': array([19.5987333 , 10.93211572,  3.56460953]), 'targetState': array([15, 13], dtype=int32), 'currentDistance': 5.042270654496916}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9267239020505317
{'scaleFactor': 20, 'currentTarget': array([15., 13.]), 'previousTarget': array([15., 13.]), 'currentState': array([14.50023006, 13.40591358,  2.57621928]), 'targetState': array([15, 13], dtype=int32), 'currentDistance': 0.6438445698561166}
episode index:2043
target Thresh 31.999999965675542
target distance 8.0
model initialize at round 2043
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([5.3717738 , 4.02500015, 0.39209002]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 6.91621333733911}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9267452205916029
{'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([11.14805941,  5.58315972,  0.21198606]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 0.9484506206864933}
episode index:2044
target Thresh 31.999999966017075
target distance 14.0
model initialize at round 2044
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 15.]), 'previousTarget': array([ 7., 15.]), 'currentState': array([19.45589703, 12.33057181,  2.54067326]), 'targetState': array([ 7, 15], dtype=int32), 'currentDistance': 12.738729042604206}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.926752425935764
{'scaleFactor': 20, 'currentTarget': array([ 7., 15.]), 'previousTarget': array([ 7., 15.]), 'currentState': array([ 7.80387726, 14.85500556,  2.84259952]), 'targetState': array([ 7, 15], dtype=int32), 'currentDistance': 0.8168488511014532}
episode index:2045
target Thresh 31.99999996635521
target distance 14.0
model initialize at round 2045
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 16.]), 'previousTarget': array([24., 16.]), 'currentState': array([10.99823535,  2.64504086,  0.07415598]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 18.638691408845844}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9267459571290915
{'scaleFactor': 20, 'currentTarget': array([24., 16.]), 'previousTarget': array([24., 16.]), 'currentState': array([23.12713851, 15.14712902,  0.53093913]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 1.2203590065307004}
episode index:2046
target Thresh 31.99999996668998
target distance 20.0
model initialize at round 2046
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  8.]), 'previousTarget': array([20.,  8.]), 'currentState': array([ 5.51827707, 27.27389888,  4.82709659]), 'targetState': array([20,  8], dtype=int32), 'currentDistance': 24.10816204576391}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.926726239940321
{'scaleFactor': 20, 'currentTarget': array([20.,  8.]), 'previousTarget': array([20.,  8.]), 'currentState': array([19.56800864,  8.77301648,  5.17396747]), 'targetState': array([20,  8], dtype=int32), 'currentDistance': 0.885534306259412}
episode index:2047
target Thresh 31.999999967021424
target distance 7.0
model initialize at round 2047
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 26.]), 'previousTarget': array([23., 26.]), 'currentState': array([17.6818281 , 19.06198641,  1.04683989]), 'targetState': array([23, 26], dtype=int32), 'currentDistance': 8.741795293894894}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.926742777913983
{'scaleFactor': 20, 'currentTarget': array([23., 26.]), 'previousTarget': array([23., 26.]), 'currentState': array([22.51302904, 25.4112681 ,  0.88594761]), 'targetState': array([23, 26], dtype=int32), 'currentDistance': 0.7640327049124582}
episode index:2048
target Thresh 31.999999967349563
target distance 6.0
model initialize at round 2048
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 25.]), 'previousTarget': array([11., 25.]), 'currentState': array([ 6.97089812, 26.77740562,  5.94523047]), 'targetState': array([11, 25], dtype=int32), 'currentDistance': 4.403729407516493}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9267688185299351
{'scaleFactor': 20, 'currentTarget': array([11., 25.]), 'previousTarget': array([11., 25.]), 'currentState': array([10.63578902, 25.19111828,  5.9965493 ]), 'targetState': array([11, 25], dtype=int32), 'currentDistance': 0.41130989994612205}
episode index:2049
target Thresh 31.99999996767444
target distance 8.0
model initialize at round 2049
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 13.]), 'previousTarget': array([11., 13.]), 'currentState': array([ 9.70702642, 19.92268509,  4.84625649]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 7.042396573771993}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9267900527647986
{'scaleFactor': 20, 'currentTarget': array([11., 13.]), 'previousTarget': array([11., 13.]), 'currentState': array([10.63811897, 13.99803032,  4.94500022]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 1.061613107302615}
episode index:2050
target Thresh 31.999999967996086
target distance 3.0
model initialize at round 2050
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 19.]), 'previousTarget': array([26., 19.]), 'currentState': array([24.91292471, 19.46345491,  5.80812249]), 'targetState': array([26, 19], dtype=int32), 'currentDistance': 1.1817457955645303}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9268208718517003
{'scaleFactor': 20, 'currentTarget': array([26., 19.]), 'previousTarget': array([26., 19.]), 'currentState': array([26.78848857, 18.7852251 ,  6.06686857]), 'targetState': array([26, 19], dtype=int32), 'currentDistance': 0.8172163023398222}
episode index:2051
target Thresh 31.99999996831453
target distance 13.0
model initialize at round 2051
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  3.]), 'previousTarget': array([11.,  3.]), 'currentState': array([23.39831238,  8.42826208,  3.33678293]), 'targetState': array([11,  3], dtype=int32), 'currentDistance': 13.534555000679106}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9268234276392515
{'scaleFactor': 20, 'currentTarget': array([11.,  3.]), 'previousTarget': array([11.,  3.]), 'currentState': array([10.64473688,  2.96753801,  3.7775347 ]), 'targetState': array([11,  3], dtype=int32), 'currentDistance': 0.35674314038762894}
episode index:2052
target Thresh 31.999999968629808
target distance 2.0
model initialize at round 2052
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 17.]), 'previousTarget': array([23., 17.]), 'currentState': array([23.6126721 , 18.95273784,  3.54981947]), 'targetState': array([23, 17], dtype=int32), 'currentDistance': 2.0465952630484527}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9268542004460517
{'scaleFactor': 20, 'currentTarget': array([23., 17.]), 'previousTarget': array([23., 17.]), 'currentState': array([23.2520953 , 17.28419027,  5.46848631]), 'targetState': array([23, 17], dtype=int32), 'currentDistance': 0.3798896533256256}
episode index:2053
target Thresh 31.999999968941946
target distance 14.0
model initialize at round 2053
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  4.]), 'previousTarget': array([19.,  4.]), 'currentState': array([6.65230189, 5.68017733, 0.81880253]), 'targetState': array([19,  4], dtype=int32), 'currentDistance': 12.461486444427512}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9268613211612197
{'scaleFactor': 20, 'currentTarget': array([19.,  4.]), 'previousTarget': array([19.,  4.]), 'currentState': array([18.17451175,  4.11710888,  6.06510992]), 'targetState': array([19,  4], dtype=int32), 'currentDistance': 0.833753767402559}
episode index:2054
target Thresh 31.99999996925098
target distance 5.0
model initialize at round 2054
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  5.]), 'previousTarget': array([10.,  5.]), 'currentState': array([7.55295638, 9.35137486, 4.87753857]), 'targetState': array([10,  5], dtype=int32), 'currentDistance': 4.992242547080494}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9268872280608978
{'scaleFactor': 20, 'currentTarget': array([10.,  5.]), 'previousTarget': array([10.,  5.]), 'currentState': array([9.24193462, 5.76736612, 5.3470013 ]), 'targetState': array([10,  5], dtype=int32), 'currentDistance': 1.0786630090561824}
episode index:2055
target Thresh 31.999999969556935
target distance 8.0
model initialize at round 2055
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 12.]), 'previousTarget': array([13., 12.]), 'currentState': array([9.84333399, 5.67566221, 0.65402031]), 'targetState': array([13, 12], dtype=int32), 'currentDistance': 7.068365355461251}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9269036233828527
{'scaleFactor': 20, 'currentTarget': array([13., 12.]), 'previousTarget': array([13., 12.]), 'currentState': array([13.36010242, 12.76947037,  1.37194294]), 'targetState': array([13, 12], dtype=int32), 'currentDistance': 0.8495636549438998}
episode index:2056
target Thresh 31.99999996985985
target distance 4.0
model initialize at round 2056
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 26.]), 'previousTarget': array([14., 26.]), 'currentState': array([ 9.72060104, 27.34038429,  5.55560136]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 4.484404697778521}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9269294845285101
{'scaleFactor': 20, 'currentTarget': array([14., 26.]), 'previousTarget': array([14., 26.]), 'currentState': array([13.44367127, 26.05609932,  6.11660635]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 0.5591500614741227}
episode index:2057
target Thresh 31.99999997015975
target distance 11.0
model initialize at round 2057
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 29.]), 'previousTarget': array([14., 29.]), 'currentState': array([22.64414881, 18.35939223,  1.22520369]), 'targetState': array([14, 29], dtype=int32), 'currentDistance': 13.709261191629546}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9269319800889466
{'scaleFactor': 20, 'currentTarget': array([14., 29.]), 'previousTarget': array([14., 29.]), 'currentState': array([14.14071705, 28.62728944,  2.31470124]), 'targetState': array([14, 29], dtype=int32), 'currentDistance': 0.39838982405371914}
episode index:2058
target Thresh 31.999999970456663
target distance 16.0
model initialize at round 2058
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 7.]), 'previousTarget': array([8., 7.]), 'currentState': array([16.40535801, 24.57441703,  2.9419235 ]), 'targetState': array([8, 7], dtype=int32), 'currentDistance': 19.48102095053774}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9269210282166396
{'scaleFactor': 20, 'currentTarget': array([8., 7.]), 'previousTarget': array([8., 7.]), 'currentState': array([8.03297402, 7.13917609, 4.58774645]), 'targetState': array([8, 7], dtype=int32), 'currentDistance': 0.14302891756957034}
episode index:2059
target Thresh 31.999999970750626
target distance 12.0
model initialize at round 2059
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([18.90083797, 12.27445319,  3.29247844]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 11.38201705436557}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9269280957511951
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([7.46445532, 8.83936304, 3.63563431]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.5591174626546381}
episode index:2060
target Thresh 31.99999997104166
target distance 12.0
model initialize at round 2060
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 16.]), 'previousTarget': array([23., 16.]), 'currentState': array([18.95748411,  5.98894353,  1.41188991]), 'targetState': array([23, 16], dtype=int32), 'currentDistance': 10.796443231352692}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9269397706440378
{'scaleFactor': 20, 'currentTarget': array([23., 16.]), 'previousTarget': array([23., 16.]), 'currentState': array([22.77447284, 15.18487925,  1.15913211]), 'targetState': array([23, 16], dtype=int32), 'currentDistance': 0.8457448375956929}
episode index:2061
target Thresh 31.999999971329803
target distance 17.0
model initialize at round 2061
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 20.]), 'previousTarget': array([ 6., 20.]), 'currentState': array([4.57299506, 4.95217587, 1.71464165]), 'targetState': array([ 6, 20], dtype=int32), 'currentDistance': 15.115335063745647}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9269377361744857
{'scaleFactor': 20, 'currentTarget': array([ 6., 20.]), 'previousTarget': array([ 6., 20.]), 'currentState': array([ 6.14399636, 20.75810967,  1.55263231]), 'targetState': array([ 6, 20], dtype=int32), 'currentDistance': 0.7716639297195825}
episode index:2062
target Thresh 31.999999971615075
target distance 8.0
model initialize at round 2062
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 12.]), 'previousTarget': array([ 4., 12.]), 'currentState': array([10.52861244, 12.18305035,  2.63843775]), 'targetState': array([ 4, 12], dtype=int32), 'currentDistance': 6.531178136132686}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9269587547221472
{'scaleFactor': 20, 'currentTarget': array([ 4., 12.]), 'previousTarget': array([ 4., 12.]), 'currentState': array([ 4.62529441, 11.98754356,  3.21697186]), 'targetState': array([ 4, 12], dtype=int32), 'currentDistance': 0.6254184712712523}
episode index:2063
target Thresh 31.99999997189751
target distance 8.0
model initialize at round 2063
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 12.]), 'previousTarget': array([15., 12.]), 'currentState': array([20.05096651, 21.68219812,  2.55050802]), 'targetState': array([15, 12], dtype=int32), 'currentDistance': 10.920495550972982}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9269657902815847
{'scaleFactor': 20, 'currentTarget': array([15., 12.]), 'previousTarget': array([15., 12.]), 'currentState': array([14.96614717, 11.74420966,  4.69603357]), 'targetState': array([15, 12], dtype=int32), 'currentDistance': 0.2580207582426655}
episode index:2064
target Thresh 31.999999972177136
target distance 15.0
model initialize at round 2064
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 27.]), 'previousTarget': array([21., 27.]), 'currentState': array([15.30829508, 13.66734331,  2.3765893 ]), 'targetState': array([21, 27], dtype=int32), 'currentDistance': 14.496732018852597}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9269637461673698
{'scaleFactor': 20, 'currentTarget': array([21., 27.]), 'previousTarget': array([21., 27.]), 'currentState': array([21.29581478, 27.5356759 ,  1.29066292]), 'targetState': array([21, 27], dtype=int32), 'currentDistance': 0.6119273285049738}
episode index:2065
target Thresh 31.999999972453978
target distance 14.0
model initialize at round 2065
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([ 9.03580627, 14.67354101,  0.10217732]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 13.071765700926264}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9269662154808933
{'scaleFactor': 20, 'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([22.86014976, 12.89239473,  6.253542  ]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 0.8668543740913811}
episode index:2066
target Thresh 31.999999972728062
target distance 9.0
model initialize at round 2066
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 28.]), 'previousTarget': array([19., 28.]), 'currentState': array([24.58024209, 18.42101553,  0.65879935]), 'targetState': array([19, 28], dtype=int32), 'currentDistance': 11.08584887488414}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9269732372196064
{'scaleFactor': 20, 'currentTarget': array([19., 28.]), 'previousTarget': array([19., 28.]), 'currentState': array([18.92353994, 28.22461152,  2.39433643]), 'targetState': array([19, 28], dtype=int32), 'currentDistance': 0.23726878048721953}
episode index:2067
target Thresh 31.999999972999426
target distance 24.0
model initialize at round 2067
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 23.]), 'previousTarget': array([ 3., 23.]), 'currentState': array([25.0216672 , 15.22479717,  3.19393969]), 'targetState': array([ 3, 23], dtype=int32), 'currentDistance': 23.353963370481235}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9269536103504075
{'scaleFactor': 20, 'currentTarget': array([ 3., 23.]), 'previousTarget': array([ 3., 23.]), 'currentState': array([ 2.77377988, 23.13602409,  2.89153954]), 'targetState': array([ 3, 23], dtype=int32), 'currentDistance': 0.2639660855852793}
episode index:2068
target Thresh 31.999999973268086
target distance 14.0
model initialize at round 2068
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 19.]), 'previousTarget': array([25., 19.]), 'currentState': array([14.51345816,  5.73609272,  1.46268004]), 'targetState': array([25, 19], dtype=int32), 'currentDistance': 16.9085421078027}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9269515760749495
{'scaleFactor': 20, 'currentTarget': array([25., 19.]), 'previousTarget': array([25., 19.]), 'currentState': array([24.20854979, 18.23106575,  0.78022766]), 'targetState': array([25, 19], dtype=int32), 'currentDistance': 1.1034732917955914}
episode index:2069
target Thresh 31.99999997353407
target distance 5.0
model initialize at round 2069
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 24.]), 'previousTarget': array([16., 24.]), 'currentState': array([21.20461002, 22.6704858 ,  2.45891806]), 'targetState': array([16, 24], dtype=int32), 'currentDistance': 5.371738406381728}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9269725168594544
{'scaleFactor': 20, 'currentTarget': array([16., 24.]), 'previousTarget': array([16., 24.]), 'currentState': array([15.45894482, 24.05996894,  3.24932358]), 'targetState': array([16, 24], dtype=int32), 'currentDistance': 0.5443684202434238}
episode index:2070
target Thresh 31.99999997379741
target distance 15.0
model initialize at round 2070
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  2.]), 'previousTarget': array([12.,  2.]), 'currentState': array([12.06886719, 15.70003422,  4.3905369 ]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 13.700207313361666}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.926974975976329
{'scaleFactor': 20, 'currentTarget': array([12.,  2.]), 'previousTarget': array([12.,  2.]), 'currentState': array([12.13090423,  1.85884916,  4.87253189]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 0.19250837735856985}
episode index:2071
target Thresh 31.99999997405813
target distance 5.0
model initialize at round 2071
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 23.]), 'previousTarget': array([22., 23.]), 'currentState': array([25.3315849 , 26.77914002,  4.28320456]), 'targetState': array([22, 23], dtype=int32), 'currentDistance': 5.037991385688112}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9270006154666879
{'scaleFactor': 20, 'currentTarget': array([22., 23.]), 'previousTarget': array([22., 23.]), 'currentState': array([22.72872459, 23.78695851,  3.85741843]), 'targetState': array([22, 23], dtype=int32), 'currentDistance': 1.0725405417787657}
episode index:2072
target Thresh 31.999999974316257
target distance 21.0
model initialize at round 2072
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  6.]), 'previousTarget': array([18.,  6.]), 'currentState': array([14.68181753, 27.06228026,  5.3102004 ]), 'targetState': array([18,  6], dtype=int32), 'currentDistance': 21.32205395244047}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9269853417758013
{'scaleFactor': 20, 'currentTarget': array([18.,  6.]), 'previousTarget': array([18.,  6.]), 'currentState': array([17.8944468 ,  5.80262881,  4.9030809 ]), 'targetState': array([18,  6], dtype=int32), 'currentDistance': 0.2238232896956324}
episode index:2073
target Thresh 31.999999974571814
target distance 14.0
model initialize at round 2073
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 25.]), 'previousTarget': array([22., 25.]), 'currentState': array([ 9.41870304, 15.36523952,  1.07540241]), 'targetState': array([22, 25], dtype=int32), 'currentDistance': 15.846691853947355}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9269832971049488
{'scaleFactor': 20, 'currentTarget': array([22., 25.]), 'previousTarget': array([22., 25.]), 'currentState': array([21.95420337, 24.99937059,  0.81760717]), 'targetState': array([22, 25], dtype=int32), 'currentDistance': 0.045800959328912774}
episode index:2074
target Thresh 31.99999997482483
target distance 7.0
model initialize at round 2074
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 17.]), 'previousTarget': array([ 3., 17.]), 'currentState': array([ 8.44576122, 10.35445379,  2.52525759]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 8.5918333150484}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9269994960027296
{'scaleFactor': 20, 'currentTarget': array([ 3., 17.]), 'previousTarget': array([ 3., 17.]), 'currentState': array([ 3.44794326, 16.53014715,  2.38924571]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 0.649164740109422}
episode index:2075
target Thresh 31.999999975075326
target distance 12.0
model initialize at round 2075
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 27.]), 'previousTarget': array([19., 27.]), 'currentState': array([ 8.96868125, 26.09127169,  0.34595584]), 'targetState': array([19, 27], dtype=int32), 'currentDistance': 10.072395096542234}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9270110521462253
{'scaleFactor': 20, 'currentTarget': array([19., 27.]), 'previousTarget': array([19., 27.]), 'currentState': array([18.8600857 , 27.12253056,  0.19416104]), 'targetState': array([19, 27], dtype=int32), 'currentDistance': 0.1859831977755202}
episode index:2076
target Thresh 31.99999997532333
target distance 22.0
model initialize at round 2076
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 24.]), 'previousTarget': array([ 3., 24.]), 'currentState': array([23.34538395, 12.3076261 ,  1.94777155]), 'targetState': array([ 3, 24], dtype=int32), 'currentDistance': 23.465852970392145}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9269914921171304
{'scaleFactor': 20, 'currentTarget': array([ 3., 24.]), 'previousTarget': array([ 3., 24.]), 'currentState': array([ 3.0913803 , 23.78390574,  2.63976401]), 'targetState': array([ 3, 24], dtype=int32), 'currentDistance': 0.2346211554117522}
episode index:2077
target Thresh 31.99999997556887
target distance 4.0
model initialize at round 2077
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 3.]), 'previousTarget': array([9., 3.]), 'currentState': array([3.91806959, 2.71088609, 5.02415013]), 'targetState': array([9, 3], dtype=int32), 'currentDistance': 5.090147689897801}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9270123330737632
{'scaleFactor': 20, 'currentTarget': array([9., 3.]), 'previousTarget': array([9., 3.]), 'currentState': array([9.30221988, 3.3701852 , 0.11678748]), 'targetState': array([9, 3], dtype=int32), 'currentDistance': 0.47788486322080276}
episode index:2078
target Thresh 31.999999975811964
target distance 10.0
model initialize at round 2078
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  2.]), 'previousTarget': array([23.,  2.]), 'currentState': array([24.71427457, 10.21308384,  4.60882056]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 8.39008245075746}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9270284868385185
{'scaleFactor': 20, 'currentTarget': array([23.,  2.]), 'previousTarget': array([23.,  2.]), 'currentState': array([22.89696454,  2.45916014,  4.69449476]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 0.47057872924817906}
episode index:2079
target Thresh 31.99999997605264
target distance 15.0
model initialize at round 2079
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 18.]), 'previousTarget': array([ 5., 18.]), 'currentState': array([18.3716603 , 22.42532093,  3.89610171]), 'targetState': array([ 5, 18], dtype=int32), 'currentDistance': 14.084912651286142}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9270309084063398
{'scaleFactor': 20, 'currentTarget': array([ 5., 18.]), 'previousTarget': array([ 5., 18.]), 'currentState': array([ 5.33603751, 17.93215977,  3.47832475]), 'targetState': array([ 5, 18], dtype=int32), 'currentDistance': 0.34281701588216085}
episode index:2080
target Thresh 31.99999997629092
target distance 9.0
model initialize at round 2080
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 9.47283374, 18.40172473,  5.40379071]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 7.546838297367587}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.927047037719936
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.98206456, 10.88576257,  4.68721426]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.11563680507037222}
episode index:2081
target Thresh 31.999999976526826
target distance 17.0
model initialize at round 2081
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([24.58263415, 23.16876211,  3.87202692]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 18.49549917105856}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9270405392616092
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.46220318,  8.57167781,  4.10931141]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.7351512117443525}
episode index:2082
target Thresh 31.99999997676039
target distance 6.0
model initialize at round 2082
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([10.84494215,  6.53024779,  2.72382617]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 5.959248684614315}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9270613066455452
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([5.98474569, 9.97577963, 2.74921057]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.02862376966829526}
episode index:2083
target Thresh 31.999999976991628
target distance 6.0
model initialize at round 2083
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 15.]), 'previousTarget': array([ 7., 15.]), 'currentState': array([ 9.69068932, 10.50779417,  2.18893139]), 'targetState': array([ 7, 15], dtype=int32), 'currentDistance': 5.23638446148984}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9270820540991702
{'scaleFactor': 20, 'currentTarget': array([ 7., 15.]), 'previousTarget': array([ 7., 15.]), 'currentState': array([ 6.51202437, 15.57008993,  2.37086212]), 'targetState': array([ 7, 15], dtype=int32), 'currentDistance': 0.7504150447480359}
episode index:2084
target Thresh 31.999999977220565
target distance 2.0
model initialize at round 2084
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([ 1.72393678, 12.09729246,  3.44137657]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 3.109570948496257}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9271074823705854
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.54901962, 8.95382514, 5.38749481]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.5509579528051801}
episode index:2085
target Thresh 31.999999977447224
target distance 2.0
model initialize at round 2085
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 22.]), 'previousTarget': array([23., 22.]), 'currentState': array([20.3519435 , 23.6589753 ,  4.35563827]), 'targetState': array([23, 22], dtype=int32), 'currentDistance': 3.124804362539582}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9271328862620665
{'scaleFactor': 20, 'currentTarget': array([23., 22.]), 'previousTarget': array([23., 22.]), 'currentState': array([23.3463012 , 22.36565362,  6.27327292]), 'targetState': array([23, 22], dtype=int32), 'currentDistance': 0.5036140320938424}
episode index:2086
target Thresh 31.999999977671628
target distance 13.0
model initialize at round 2086
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 22.]), 'previousTarget': array([24., 22.]), 'currentState': array([12.6150397 , 18.47332321,  1.29508894]), 'targetState': array([24, 22], dtype=int32), 'currentDistance': 11.918673176026385}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9271397608491
{'scaleFactor': 20, 'currentTarget': array([24., 22.]), 'previousTarget': array([24., 22.]), 'currentState': array([23.72288531, 21.78189848,  0.50882672]), 'targetState': array([24, 22], dtype=int32), 'currentDistance': 0.3526483053208517}
episode index:2087
target Thresh 31.999999977893797
target distance 5.0
model initialize at round 2087
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 12.]), 'previousTarget': array([16., 12.]), 'currentState': array([16.76001859, 18.68869464,  2.68331736]), 'targetState': array([16, 12], dtype=int32), 'currentDistance': 6.731735605376421}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9271557839569309
{'scaleFactor': 20, 'currentTarget': array([16., 12.]), 'previousTarget': array([16., 12.]), 'currentState': array([15.93350278, 11.92705113,  5.2143787 ]), 'targetState': array([16, 12], dtype=int32), 'currentDistance': 0.09870875773734825}
episode index:2088
target Thresh 31.99999997811376
target distance 18.0
model initialize at round 2088
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([16.66663762,  3.02690597,  1.47531331]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 19.370017401112083}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9271448822293348
{'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([26.14034982, 20.37048783,  1.23029059]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 0.3961808942372035}
episode index:2089
target Thresh 31.99999997833153
target distance 20.0
model initialize at round 2089
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  2.]), 'previousTarget': array([17.,  2.]), 'currentState': array([ 3.82270318, 20.7973528 ,  4.94764209]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 22.956080317667944}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.92712537983196
{'scaleFactor': 20, 'currentTarget': array([17.,  2.]), 'previousTarget': array([17.,  2.]), 'currentState': array([17.39704576,  1.57304295,  5.43042106]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 0.5830417347327375}
episode index:2090
target Thresh 31.999999978547134
target distance 8.0
model initialize at round 2090
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 10.]), 'previousTarget': array([17., 10.]), 'currentState': array([10.44993954, 12.85443749,  5.80569393]), 'targetState': array([17, 10], dtype=int32), 'currentDistance': 7.145005630827365}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9271413868286927
{'scaleFactor': 20, 'currentTarget': array([17., 10.]), 'previousTarget': array([17., 10.]), 'currentState': array([17.82168626,  9.83334581,  6.10238714]), 'targetState': array([17, 10], dtype=int32), 'currentDistance': 0.83841632175025}
episode index:2091
target Thresh 31.999999978760595
target distance 9.0
model initialize at round 2091
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 27.]), 'previousTarget': array([12., 27.]), 'currentState': array([ 4.47247201, 29.81499341,  5.77870137]), 'targetState': array([12, 27], dtype=int32), 'currentDistance': 8.03665760969419}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9271573785223692
{'scaleFactor': 20, 'currentTarget': array([12., 27.]), 'previousTarget': array([12., 27.]), 'currentState': array([11.81346332, 26.74443321,  6.24616802]), 'targetState': array([12, 27], dtype=int32), 'currentDistance': 0.3164021473441748}
episode index:2092
target Thresh 31.999999978971932
target distance 10.0
model initialize at round 2092
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'previousTarget': array([19.,  3.]), 'currentState': array([10.17044175, 11.54808568,  6.04048944]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 12.289461656028113}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9271642217000465
{'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'previousTarget': array([19.,  3.]), 'currentState': array([18.80336919,  3.47475388,  5.43395707]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 0.5138627502834952}
episode index:2093
target Thresh 31.999999979181162
target distance 5.0
model initialize at round 2093
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([14.3409735 ,  7.71712344,  2.30047607]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 4.683949449181972}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.927189501441355
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.5423417 , 10.54821397,  2.58728049]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.705864817927757}
episode index:2094
target Thresh 31.999999979388313
target distance 7.0
model initialize at round 2094
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([12.11281331, 10.64002554,  2.96352622]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 5.125469890004358}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9272100787676361
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.19106878, 10.79981788,  3.37924215]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.8333322258944618}
episode index:2095
target Thresh 31.999999979593404
target distance 22.0
model initialize at round 2095
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 28.]), 'previousTarget': array([21., 28.]), 'currentState': array([28.67447482,  6.1688851 ,  1.11051863]), 'targetState': array([21, 28], dtype=int32), 'currentDistance': 23.140767950182937}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9271863721570595
{'scaleFactor': 20, 'currentTarget': array([21., 28.]), 'previousTarget': array([21., 28.]), 'currentState': array([21.04104417, 28.90771499,  1.62582096]), 'targetState': array([21, 28], dtype=int32), 'currentDistance': 0.9086424623713394}
episode index:2096
target Thresh 31.999999979796453
target distance 13.0
model initialize at round 2096
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 26.]), 'previousTarget': array([15., 26.]), 'currentState': array([ 3.8314087 , 14.7586036 ,  0.71805394]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 15.846337899764787}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9271842540465542
{'scaleFactor': 20, 'currentTarget': array([15., 26.]), 'previousTarget': array([15., 26.]), 'currentState': array([14.99316556, 26.01233425,  0.95532293]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 0.014101179768372907}
episode index:2097
target Thresh 31.999999979997483
target distance 11.0
model initialize at round 2097
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 16.]), 'previousTarget': array([13., 16.]), 'currentState': array([2.85423213, 4.54993947, 6.25475597]), 'targetState': array([13, 16], dtype=int32), 'currentDistance': 15.298382002552131}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9271777397440935
{'scaleFactor': 20, 'currentTarget': array([13., 16.]), 'previousTarget': array([13., 16.]), 'currentState': array([13.01364921, 15.77112595,  0.94678622]), 'targetState': array([13, 16], dtype=int32), 'currentDistance': 0.22928068360783257}
episode index:2098
target Thresh 31.99999998019651
target distance 2.0
model initialize at round 2098
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 28.]), 'previousTarget': array([ 9., 28.]), 'currentState': array([ 8.45294362, 27.37435779,  0.74906191]), 'targetState': array([ 9, 28], dtype=int32), 'currentDistance': 0.8310829407769852}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9272124335317332
{'scaleFactor': 20, 'currentTarget': array([ 9., 28.]), 'previousTarget': array([ 9., 28.]), 'currentState': array([ 8.45294362, 27.37435779,  0.74906191]), 'targetState': array([ 9, 28], dtype=int32), 'currentDistance': 0.8310829407769852}
episode index:2099
target Thresh 31.999999980393557
target distance 11.0
model initialize at round 2099
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([12.74567129,  5.12207289,  3.42179096]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 9.785134890745116}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9272237562061942
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.89482299, 5.90143688, 3.10657448]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.1441419140950146}
episode index:2100
target Thresh 31.999999980588644
target distance 20.0
model initialize at round 2100
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  4.]), 'previousTarget': array([25.,  4.]), 'currentState': array([23.54904683, 23.3420927 ,  4.87155396]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 19.396438207055358}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9272128843922022
{'scaleFactor': 20, 'currentTarget': array([25.,  4.]), 'previousTarget': array([25.,  4.]), 'currentState': array([25.16333282,  3.50182972,  4.96247387]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 0.5242625631081413}
episode index:2101
target Thresh 31.99999998078179
target distance 4.0
model initialize at round 2101
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([24.07765836, 12.1450389 ,  2.59364265]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 2.7852369597992266}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9272427545708928
{'scaleFactor': 20, 'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([22.49656522, 13.36329244,  2.37434775]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 0.807448781589562}
episode index:2102
target Thresh 31.999999980973016
target distance 22.0
model initialize at round 2102
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([25.49938573, 26.60678979,  2.88282621]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 21.98740773514388}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9272275836244771
{'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([ 4.40116053, 21.86853326,  3.24401565]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 0.42215313929288334}
episode index:2103
target Thresh 31.999999981162336
target distance 14.0
model initialize at round 2103
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 25.]), 'previousTarget': array([14., 25.]), 'currentState': array([9.12229861, 9.74587324, 0.16918534]), 'targetState': array([14, 25], dtype=int32), 'currentDistance': 16.015004028331106}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9272254529737182
{'scaleFactor': 20, 'currentTarget': array([14., 25.]), 'previousTarget': array([14., 25.]), 'currentState': array([13.77776778, 24.40403619,  1.30708904]), 'targetState': array([14, 25], dtype=int32), 'currentDistance': 0.636050331143658}
episode index:2104
target Thresh 31.999999981349774
target distance 9.0
model initialize at round 2104
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([20.67451961,  9.16844036,  1.11025303]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 10.83051151410277}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9272322248009996
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.86956883, 10.95886809,  3.2577022 ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.13676302429706375}
episode index:2105
target Thresh 31.99999998153535
target distance 11.0
model initialize at round 2105
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 28.]), 'previousTarget': array([17., 28.]), 'currentState': array([20.33075739, 16.78548379,  2.25940323]), 'targetState': array([17, 28], dtype=int32), 'currentDistance': 11.69868875846441}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9272389901972959
{'scaleFactor': 20, 'currentTarget': array([17., 28.]), 'previousTarget': array([17., 28.]), 'currentState': array([16.96528033, 28.14972348,  2.09941972]), 'targetState': array([17, 28], dtype=int32), 'currentDistance': 0.15369637561552543}
episode index:2106
target Thresh 31.999999981719075
target distance 14.0
model initialize at round 2106
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  4.]), 'previousTarget': array([20.,  4.]), 'currentState': array([17.72929403, 16.3389442 ,  5.56083703]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 12.546140824825644}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9272457491717637
{'scaleFactor': 20, 'currentTarget': array([20.,  4.]), 'previousTarget': array([20.,  4.]), 'currentState': array([20.04073703,  4.77457269,  4.83263484]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 0.7756431924027323}
episode index:2107
target Thresh 31.999999981900974
target distance 19.0
model initialize at round 2107
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([10.58628031, 24.10726965,  4.774178  ]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 17.117312857538767}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9272392365998054
{'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([10.18377347,  6.16365987,  4.85053519]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 0.8562928846180201}
episode index:2108
target Thresh 31.99999998208106
target distance 4.0
model initialize at round 2108
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 20.]), 'previousTarget': array([ 9., 20.]), 'currentState': array([12.0115494 , 22.63772828,  3.07490885]), 'targetState': array([ 9, 20], dtype=int32), 'currentDistance': 4.003378602986162}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9272643009731579
{'scaleFactor': 20, 'currentTarget': array([ 9., 20.]), 'previousTarget': array([ 9., 20.]), 'currentState': array([ 9.2892901 , 20.4673392 ,  2.76964271]), 'targetState': array([ 9, 20], dtype=int32), 'currentDistance': 0.5496314095032557}
episode index:2109
target Thresh 31.999999982259357
target distance 15.0
model initialize at round 2109
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([ 8.05857715, 16.60496914,  5.12874913]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 14.893003133848207}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9272665763508516
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.62113015, 3.98270247, 4.12423637]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.1625432484850908}
episode index:2110
target Thresh 31.99999998243588
target distance 13.0
model initialize at round 2110
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 27.]), 'previousTarget': array([23., 27.]), 'currentState': array([11.84959943, 25.14804525,  0.75227326]), 'targetState': array([23, 27], dtype=int32), 'currentDistance': 11.303148643054907}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9272733094503544
{'scaleFactor': 20, 'currentTarget': array([23., 27.]), 'previousTarget': array([23., 27.]), 'currentState': array([23.48334234, 27.18327889,  0.3879631 ]), 'targetState': array([23, 27], dtype=int32), 'currentDistance': 0.5169245294488091}
episode index:2111
target Thresh 31.999999982610646
target distance 11.0
model initialize at round 2111
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 11.]), 'previousTarget': array([17., 11.]), 'currentState': array([21.61791362, 22.96032563,  3.54433978]), 'targetState': array([17, 11], dtype=int32), 'currentDistance': 12.820862501968056}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9272755784079568
{'scaleFactor': 20, 'currentTarget': array([17., 11.]), 'previousTarget': array([17., 11.]), 'currentState': array([16.9156409 , 10.79234039,  4.53874224]), 'targetState': array([17, 11], dtype=int32), 'currentDistance': 0.2241405132765459}
episode index:2112
target Thresh 31.999999982783674
target distance 9.0
model initialize at round 2112
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 29.]), 'previousTarget': array([22., 29.]), 'currentState': array([12.20085041, 18.51886934,  5.22760129]), 'targetState': array([22, 29], dtype=int32), 'currentDistance': 14.348429617015345}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9272734341183306
{'scaleFactor': 20, 'currentTarget': array([22., 29.]), 'previousTarget': array([22., 29.]), 'currentState': array([22.27671173, 29.56415139,  1.09380303]), 'targetState': array([22, 29], dtype=int32), 'currentDistance': 0.6283599094823931}
episode index:2113
target Thresh 31.99999998295498
target distance 10.0
model initialize at round 2113
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 18.]), 'previousTarget': array([10., 18.]), 'currentState': array([3.0387073 , 9.38141392, 1.16874909]), 'targetState': array([10, 18], dtype=int32), 'currentDistance': 11.078791541613873}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9272846529526645
{'scaleFactor': 20, 'currentTarget': array([10., 18.]), 'previousTarget': array([10., 18.]), 'currentState': array([ 9.2984639 , 17.12399724,  0.99028643]), 'targetState': array([10, 18], dtype=int32), 'currentDistance': 1.1222895097793786}
episode index:2114
target Thresh 31.999999983124578
target distance 1.0
model initialize at round 2114
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 20.]), 'previousTarget': array([19., 20.]), 'currentState': array([20.39426099, 18.05602982,  0.41352707]), 'targetState': array([19, 20], dtype=int32), 'currentDistance': 2.392275852090902}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9273096247479586
{'scaleFactor': 20, 'currentTarget': array([19., 20.]), 'previousTarget': array([19., 20.]), 'currentState': array([18.87350668, 20.50514868,  3.04959428]), 'targetState': array([19, 20], dtype=int32), 'currentDistance': 0.5207453814647898}
episode index:2115
target Thresh 31.999999983292494
target distance 21.0
model initialize at round 2115
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  2.]), 'previousTarget': array([13.,  2.]), 'currentState': array([12.39924674, 23.51959319,  3.83772898]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 21.527977047060503}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9272945154046273
{'scaleFactor': 20, 'currentTarget': array([13.,  2.]), 'previousTarget': array([13.,  2.]), 'currentState': array([12.92765767,  2.73077012,  4.82215899]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 0.7343421377791348}
episode index:2116
target Thresh 31.999999983458736
target distance 21.0
model initialize at round 2116
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([9.5247113 , 3.80988354, 1.62954354]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 20.19693354905914}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9272836923340576
{'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([ 8.80238017, 23.26034277,  1.39851568]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 0.7656019979409744}
episode index:2117
target Thresh 31.999999983623322
target distance 6.0
model initialize at round 2117
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  5.]), 'previousTarget': array([17.,  5.]), 'currentState': array([18.92421758,  9.59351148,  4.28374617]), 'targetState': array([17,  5], dtype=int32), 'currentDistance': 4.980257123838431}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9273086292120868
{'scaleFactor': 20, 'currentTarget': array([17.,  5.]), 'previousTarget': array([17.,  5.]), 'currentState': array([17.30285267,  5.9493001 ,  4.48029486]), 'targetState': array([17,  5], dtype=int32), 'currentDistance': 0.9964388714634479}
episode index:2118
target Thresh 31.999999983786275
target distance 7.0
model initialize at round 2118
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  5.]), 'previousTarget': array([11.,  5.]), 'currentState': array([14.01209225, 13.36250738,  3.20814776]), 'targetState': array([11,  5], dtype=int32), 'currentDistance': 8.88843233966192}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.927319804965125
{'scaleFactor': 20, 'currentTarget': array([11.,  5.]), 'previousTarget': array([11.,  5.]), 'currentState': array([10.84236254,  4.53751751,  4.46843442]), 'targetState': array([11,  5], dtype=int32), 'currentDistance': 0.4886098866118368}
episode index:2119
target Thresh 31.999999983947603
target distance 3.0
model initialize at round 2119
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 12.]), 'previousTarget': array([14., 12.]), 'currentState': array([13.63025934,  9.41790259,  1.26093787]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 2.6084353879883078}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9273493710948584
{'scaleFactor': 20, 'currentTarget': array([14., 12.]), 'previousTarget': array([14., 12.]), 'currentState': array([13.92258071, 11.38735795,  1.58923432]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 0.617514392683695}
episode index:2120
target Thresh 31.999999984107326
target distance 17.0
model initialize at round 2120
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 10.]), 'previousTarget': array([23., 10.]), 'currentState': array([12.40356193, 25.4262625 ,  5.36012077]), 'targetState': array([23, 10], dtype=int32), 'currentDistance': 18.71507612696982}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9273428495844336
{'scaleFactor': 20, 'currentTarget': array([23., 10.]), 'previousTarget': array([23., 10.]), 'currentState': array([22.63493794, 10.95845783,  5.02918816]), 'targetState': array([23, 10], dtype=int32), 'currentDistance': 1.0256274780335422}
episode index:2121
target Thresh 31.99999998426546
target distance 14.0
model initialize at round 2121
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 25.]), 'previousTarget': array([11., 25.]), 'currentState': array([11.76749257, 12.66683184,  2.71939266]), 'targetState': array([11, 25], dtype=int32), 'currentDistance': 12.35702560185942}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.927349511836939
{'scaleFactor': 20, 'currentTarget': array([11., 25.]), 'previousTarget': array([11., 25.]), 'currentState': array([10.81372485, 24.08213935,  1.52862039]), 'targetState': array([11, 25], dtype=int32), 'currentDistance': 0.9365717275440935}
episode index:2122
target Thresh 31.999999984422022
target distance 11.0
model initialize at round 2122
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 29.]), 'previousTarget': array([27., 29.]), 'currentState': array([17.4842393 , 22.33925517,  0.78532698]), 'targetState': array([27, 29], dtype=int32), 'currentDistance': 11.615301257431868}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9273561678131821
{'scaleFactor': 20, 'currentTarget': array([27., 29.]), 'previousTarget': array([27., 29.]), 'currentState': array([27.23788123, 29.22970796,  0.80725878]), 'targetState': array([27, 29], dtype=int32), 'currentDistance': 0.33068599174611085}
episode index:2123
target Thresh 31.999999984577027
target distance 15.0
model initialize at round 2123
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([3.2184456 , 4.66873293, 0.4306314 ]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 15.152283352207847}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9273539966863529
{'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([18.72725422,  8.30390535,  0.34840985]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 0.788198684390137}
episode index:2124
target Thresh 31.999999984730486
target distance 8.0
model initialize at round 2124
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 16.]), 'previousTarget': array([16., 16.]), 'currentState': array([25.65024464, 10.66972629,  0.81247347]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 11.024474561636694}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9273606442876303
{'scaleFactor': 20, 'currentTarget': array([16., 16.]), 'previousTarget': array([16., 16.]), 'currentState': array([16.04935702, 16.08376107,  3.06059874]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 0.09722156259349803}
episode index:2125
target Thresh 31.99999998488242
target distance 14.0
model initialize at round 2125
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 7.]), 'previousTarget': array([7., 7.]), 'currentState': array([ 6.41536313, 20.43315428,  4.49512243]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 13.445870529737368}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.927362857224422
{'scaleFactor': 20, 'currentTarget': array([7., 7.]), 'previousTarget': array([7., 7.]), 'currentState': array([7.05111525, 6.5341962 , 4.92813442]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 0.46859999159604193}
episode index:2126
target Thresh 31.999999985032844
target distance 2.0
model initialize at round 2126
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 15.]), 'previousTarget': array([ 5., 15.]), 'currentState': array([ 4.56201345, 14.62704802,  1.39140111]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 0.575261159068308}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9273970072680401
{'scaleFactor': 20, 'currentTarget': array([ 5., 15.]), 'previousTarget': array([ 5., 15.]), 'currentState': array([ 4.56201345, 14.62704802,  1.39140111]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 0.575261159068308}
episode index:2127
target Thresh 31.99999998518177
target distance 12.0
model initialize at round 2127
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 19.]), 'previousTarget': array([18., 19.]), 'currentState': array([18.99534828,  5.64291862,  0.0720269 ]), 'targetState': array([18, 19], dtype=int32), 'currentDistance': 13.394115921535406}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9273992010371372
{'scaleFactor': 20, 'currentTarget': array([18., 19.]), 'previousTarget': array([18., 19.]), 'currentState': array([18.02574005, 18.85364971,  1.85848686]), 'targetState': array([18, 19], dtype=int32), 'currentDistance': 0.14859662931106418}
episode index:2128
target Thresh 31.999999985329215
target distance 11.0
model initialize at round 2128
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 15.]), 'previousTarget': array([12., 15.]), 'currentState': array([23.11430542, 12.67908378,  2.51282513]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 11.354049367615799}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9274058149161245
{'scaleFactor': 20, 'currentTarget': array([12., 15.]), 'previousTarget': array([12., 15.]), 'currentState': array([11.42250266, 14.98651224,  3.14688551]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 0.5776548284053316}
episode index:2129
target Thresh 31.999999985475192
target distance 22.0
model initialize at round 2129
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 29.]), 'previousTarget': array([27., 29.]), 'currentState': array([ 3.70500241, 26.92511893,  4.84437561]), 'targetState': array([27, 29], dtype=int32), 'currentDistance': 23.387219677885913}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9273865562573451
{'scaleFactor': 20, 'currentTarget': array([27., 29.]), 'previousTarget': array([27., 29.]), 'currentState': array([26.55631558, 28.98802855,  0.19816593]), 'targetState': array([27, 29], dtype=int32), 'currentDistance': 0.4438458962001314}
episode index:2130
target Thresh 31.999999985619716
target distance 16.0
model initialize at round 2130
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([24.47422149, 16.61477617,  2.29515114]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 21.371993440120427}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9273715171667779
{'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([8.11484132, 3.11959611, 4.05853126]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 0.1658063889644302}
episode index:2131
target Thresh 31.999999985762802
target distance 12.0
model initialize at round 2131
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([ 3.55668423, 18.13442061,  4.69180322]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 13.745336812191475}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.927373718775943
{'scaleFactor': 20, 'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([16.33569667, 20.81195284,  0.49942769]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 0.6904061502536618}
episode index:2132
target Thresh 31.999999985904463
target distance 13.0
model initialize at round 2132
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  6.]), 'previousTarget': array([11.,  6.]), 'currentState': array([23.31397031,  5.53679906,  3.00065386]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 12.322679091885758}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9273803321986459
{'scaleFactor': 20, 'currentTarget': array([11.,  6.]), 'previousTarget': array([11.,  6.]), 'currentState': array([11.35187235,  5.84307644,  3.33303014]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 0.3852780251105223}
episode index:2133
target Thresh 31.999999986044717
target distance 4.0
model initialize at round 2133
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 23.]), 'previousTarget': array([19., 23.]), 'currentState': array([16.28853663, 25.91701212,  4.574644  ]), 'targetState': array([19, 23], dtype=int32), 'currentDistance': 3.982586259614642}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9274050368227327
{'scaleFactor': 20, 'currentTarget': array([19., 23.]), 'previousTarget': array([19., 23.]), 'currentState': array([18.82296706, 23.69527478,  4.55967322]), 'targetState': array([19, 23], dtype=int32), 'currentDistance': 0.7174591878605805}
episode index:2134
target Thresh 31.999999986183575
target distance 15.0
model initialize at round 2134
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 2.]), 'previousTarget': array([9., 2.]), 'currentState': array([22.58162973, 16.9058774 ,  3.58322012]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 20.16546174511711}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.927394253234061
{'scaleFactor': 20, 'currentTarget': array([9., 2.]), 'previousTarget': array([9., 2.]), 'currentState': array([9.28089835, 2.32921887, 4.18463427]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 0.4327689284411582}
episode index:2135
target Thresh 31.99999998632105
target distance 24.0
model initialize at round 2135
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 26.]), 'previousTarget': array([25., 26.]), 'currentState': array([26.54450247,  3.88918621,  2.22567225]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 22.16469205974469}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.927379245743904
{'scaleFactor': 20, 'currentTarget': array([25., 26.]), 'previousTarget': array([25., 26.]), 'currentState': array([24.75264548, 25.54458085,  1.51681046]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 0.5182575286206283}
episode index:2136
target Thresh 31.99999998645716
target distance 3.0
model initialize at round 2136
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 23.]), 'previousTarget': array([12., 23.]), 'currentState': array([13.00131357, 22.96932922,  3.21438812]), 'targetState': array([12, 23], dtype=int32), 'currentDistance': 1.0017831888175865}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9274085488577346
{'scaleFactor': 20, 'currentTarget': array([12., 23.]), 'previousTarget': array([12., 23.]), 'currentState': array([11.22093085, 23.70688236,  2.27390359]), 'targetState': array([12, 23], dtype=int32), 'currentDistance': 1.0519654983766604}
episode index:2137
target Thresh 31.99999998659191
target distance 19.0
model initialize at round 2137
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([2.53434549, 7.5958894 , 0.23770285]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 18.534488377054323}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.927402051523135
{'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([20.32559773,  6.23534864,  6.20848752]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 0.7142880445803338}
episode index:2138
target Thresh 31.999999986725324
target distance 11.0
model initialize at round 2138
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([12.33263205, 17.22863113,  4.01532149]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 16.281495807572167}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9273998741705892
{'scaleFactor': 20, 'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([24.26797346,  7.7295086 ,  5.5669038 ]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 1.0334629388368046}
episode index:2139
target Thresh 31.999999986857407
target distance 16.0
model initialize at round 2139
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 20.]), 'previousTarget': array([21., 20.]), 'currentState': array([ 6.73578407, 21.69441329,  6.02549285]), 'targetState': array([21, 20], dtype=int32), 'currentDistance': 14.364501123481135}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9274020542985034
{'scaleFactor': 20, 'currentTarget': array([21., 20.]), 'previousTarget': array([21., 20.]), 'currentState': array([20.39877122, 19.86374821,  6.20499128]), 'targetState': array([21, 20], dtype=int32), 'currentDistance': 0.6164743315400574}
episode index:2140
target Thresh 31.99999998698818
target distance 13.0
model initialize at round 2140
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([14.33590658, 21.25135868,  4.00167751]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 17.43850007990787}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9273955691014858
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.58771627, 7.78078739, 3.94144182]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.4669390129189808}
episode index:2141
target Thresh 31.99999998711765
target distance 6.0
model initialize at round 2141
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  4.]), 'previousTarget': array([18.,  4.]), 'currentState': array([22.58062295,  8.6087928 ,  4.11288092]), 'targetState': array([18,  4], dtype=int32), 'currentDistance': 6.49792872203013}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9274155987144169
{'scaleFactor': 20, 'currentTarget': array([18.,  4.]), 'previousTarget': array([18.,  4.]), 'currentState': array([18.57074039,  4.18955482,  3.84428517]), 'targetState': array([18,  4], dtype=int32), 'currentDistance': 0.6013947293976853}
episode index:2142
target Thresh 31.999999987245833
target distance 22.0
model initialize at round 2142
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([14.5300661 ,  4.59731588,  2.26038316]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 21.747905984596244}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9274006302848995
{'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([ 6.90017205, 25.04185909,  1.90041789]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 0.108248804676455}
episode index:2143
target Thresh 31.99999998737274
target distance 16.0
model initialize at round 2143
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 25.]), 'previousTarget': array([22., 25.]), 'currentState': array([20.62573927,  9.43515504,  1.27153509]), 'targetState': array([22, 25], dtype=int32), 'currentDistance': 15.625395716103444}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.927398458673026
{'scaleFactor': 20, 'currentTarget': array([22., 25.]), 'previousTarget': array([22., 25.]), 'currentState': array([21.83240664, 25.27107172,  1.45096136]), 'targetState': array([22, 25], dtype=int32), 'currentDistance': 0.31869643149331295}
episode index:2144
target Thresh 31.99999998749838
target distance 8.0
model initialize at round 2144
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([15.20166091, 13.8216965 ,  0.23441571]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 8.545890329305886}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9274139353869313
{'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([21.28534299, 18.86991219,  0.57075835]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 0.7264003602697201}
episode index:2145
target Thresh 31.999999987622775
target distance 13.0
model initialize at round 2145
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 11.]), 'previousTarget': array([23., 11.]), 'currentState': array([10.49232907, 10.39065225,  6.01926661]), 'targetState': array([23, 11], dtype=int32), 'currentDistance': 12.522505212742805}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9274204900066956
{'scaleFactor': 20, 'currentTarget': array([23., 11.]), 'previousTarget': array([23., 11.]), 'currentState': array([22.4083351 , 10.90039484,  0.14947353]), 'targetState': array([23, 11], dtype=int32), 'currentDistance': 0.5999904492554476}
episode index:2146
target Thresh 31.99999998774593
target distance 13.0
model initialize at round 2146
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 18.]), 'previousTarget': array([21., 18.]), 'currentState': array([ 9.92378729, 19.56862437,  0.07508885]), 'targetState': array([21, 18], dtype=int32), 'currentDistance': 11.186736357629087}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9274270385206194
{'scaleFactor': 20, 'currentTarget': array([21., 18.]), 'previousTarget': array([21., 18.]), 'currentState': array([21.7837633 , 17.90011091,  6.26343034]), 'targetState': array([21, 18], dtype=int32), 'currentDistance': 0.7901029975682476}
episode index:2147
target Thresh 31.99999998786786
target distance 10.0
model initialize at round 2147
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([23.93984712, 13.38949332,  3.65862656]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 10.438750156484103}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9274380082652093
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.53014342,  8.09605251,  3.56455639]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.5387746535447105}
episode index:2148
target Thresh 31.999999987988577
target distance 4.0
model initialize at round 2148
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 22.]), 'previousTarget': array([20., 22.]), 'currentState': array([22.86501065, 23.24265331,  3.32094359]), 'targetState': array([20, 22], dtype=int32), 'currentDistance': 3.1228950117577887}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9274625136126894
{'scaleFactor': 20, 'currentTarget': array([20., 22.]), 'previousTarget': array([20., 22.]), 'currentState': array([19.323655  , 21.44910237,  3.68884996]), 'targetState': array([20, 22], dtype=int32), 'currentDistance': 0.8723134534903068}
episode index:2149
target Thresh 31.99999998810809
target distance 5.0
model initialize at round 2149
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 27.]), 'previousTarget': array([ 8., 27.]), 'currentState': array([ 4.42879655, 28.11065839,  0.45324295]), 'targetState': array([ 8, 27], dtype=int32), 'currentDistance': 3.7399272891851596}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9274869961644975
{'scaleFactor': 20, 'currentTarget': array([ 8., 27.]), 'previousTarget': array([ 8., 27.]), 'currentState': array([ 7.97711492, 26.81217545,  6.10680044]), 'targetState': array([ 8, 27], dtype=int32), 'currentDistance': 0.18921360700867468}
episode index:2150
target Thresh 31.999999988226417
target distance 17.0
model initialize at round 2150
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([22.64854898, 27.00297967,  3.51313436]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 18.15589708802615}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9274805016276864
{'scaleFactor': 20, 'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([25.09937564,  9.8079049 ,  4.7555324 ]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 0.8139937632164126}
episode index:2151
target Thresh 31.999999988343568
target distance 9.0
model initialize at round 2151
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 28.]), 'previousTarget': array([17., 28.]), 'currentState': array([ 9.67419496, 26.17163714,  1.11216229]), 'targetState': array([17, 28], dtype=int32), 'currentDistance': 7.550518537262725}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9274958898750713
{'scaleFactor': 20, 'currentTarget': array([17., 28.]), 'previousTarget': array([17., 28.]), 'currentState': array([17.13895114, 28.12363706,  0.44598676]), 'targetState': array([17, 28], dtype=int32), 'currentDistance': 0.18599339168243154}
episode index:2152
target Thresh 31.99999998845955
target distance 6.0
model initialize at round 2152
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 19.]), 'previousTarget': array([18., 19.]), 'currentState': array([24.49683562, 22.39203779,  4.00206804]), 'targetState': array([18, 19], dtype=int32), 'currentDistance': 7.329037688938266}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9275112638277535
{'scaleFactor': 20, 'currentTarget': array([18., 19.]), 'previousTarget': array([18., 19.]), 'currentState': array([17.51820782, 18.62661469,  3.78274679]), 'targetState': array([18, 19], dtype=int32), 'currentDistance': 0.609541050194609}
episode index:2153
target Thresh 31.99999998857438
target distance 4.0
model initialize at round 2153
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 20.]), 'previousTarget': array([14., 20.]), 'currentState': array([19.34125967, 21.01656804,  1.65855091]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 5.437137613294735}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9275311281435252
{'scaleFactor': 20, 'currentTarget': array([14., 20.]), 'previousTarget': array([14., 20.]), 'currentState': array([14.30343418, 20.00284895,  3.78440265]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 0.3034475575055039}
episode index:2154
target Thresh 31.999999988688067
target distance 8.0
model initialize at round 2154
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'previousTarget': array([11., 21.]), 'currentState': array([ 5.68219185, 28.9488272 ,  5.24277437]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 9.56362573878789}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9275420139540851
{'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'previousTarget': array([11., 21.]), 'currentState': array([11.25230276, 20.69667514,  5.50056816]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 0.3945410658339721}
episode index:2155
target Thresh 31.999999988800624
target distance 4.0
model initialize at round 2155
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 16.]), 'previousTarget': array([ 9., 16.]), 'currentState': array([10.81667698, 13.19714252,  3.35988319]), 'targetState': array([ 9, 16], dtype=int32), 'currentDistance': 3.340108572847448}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9275663914986333
{'scaleFactor': 20, 'currentTarget': array([ 9., 16.]), 'previousTarget': array([ 9., 16.]), 'currentState': array([ 8.60488905, 15.84494779,  2.99530983]), 'targetState': array([ 9, 16], dtype=int32), 'currentDistance': 0.4244453419995091}
episode index:2156
target Thresh 31.99999998891206
target distance 20.0
model initialize at round 2156
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 25.]), 'previousTarget': array([22., 25.]), 'currentState': array([3.27956596, 3.90679414, 0.30298203]), 'targetState': array([22, 25], dtype=int32), 'currentDistance': 28.202446421819307}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9275391218747437
{'scaleFactor': 20, 'currentTarget': array([22., 25.]), 'previousTarget': array([22., 25.]), 'currentState': array([21.124507  , 24.24242669,  0.78977298]), 'targetState': array([22, 25], dtype=int32), 'currentDistance': 1.1577587501307052}
episode index:2157
target Thresh 31.999999989022385
target distance 14.0
model initialize at round 2157
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 4.]), 'previousTarget': array([8., 4.]), 'currentState': array([18.33391722, 16.11874563,  4.257228  ]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 15.92651376180227}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9275369001752781
{'scaleFactor': 20, 'currentTarget': array([8., 4.]), 'previousTarget': array([8., 4.]), 'currentState': array([8.04235464, 4.05453347, 4.16368199]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 0.06904936917783906}
episode index:2158
target Thresh 31.999999989131613
target distance 15.0
model initialize at round 2158
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 24.]), 'previousTarget': array([26., 24.]), 'currentState': array([10.68349822, 17.34705877,  5.53320098]), 'targetState': array([26, 24], dtype=int32), 'currentDistance': 16.699007565479018}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9275304065890384
{'scaleFactor': 20, 'currentTarget': array([26., 24.]), 'previousTarget': array([26., 24.]), 'currentState': array([26.48570559, 24.60152259,  0.70282939]), 'targetState': array([26, 24], dtype=int32), 'currentDistance': 0.7731360493043905}
episode index:2159
target Thresh 31.999999989239758
target distance 10.0
model initialize at round 2159
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 20.]), 'previousTarget': array([25., 20.]), 'currentState': array([21.04165271, 11.3218728 ,  1.9134075 ]), 'targetState': array([25, 20], dtype=int32), 'currentDistance': 9.538260055007747}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9275412675350156
{'scaleFactor': 20, 'currentTarget': array([25., 20.]), 'previousTarget': array([25., 20.]), 'currentState': array([25.00144446, 20.24081856,  1.38962462]), 'targetState': array([25, 20], dtype=int32), 'currentDistance': 0.24082288849889827}
episode index:2160
target Thresh 31.999999989346822
target distance 19.0
model initialize at round 2160
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 8.]), 'previousTarget': array([9., 8.]), 'currentState': array([ 2.39167802, 26.50433024,  4.45054674]), 'targetState': array([9, 8], dtype=int32), 'currentDistance': 19.64892254354535}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9275305506481455
{'scaleFactor': 20, 'currentTarget': array([9., 8.]), 'previousTarget': array([9., 8.]), 'currentState': array([8.95499742, 8.25019931, 5.13992114]), 'targetState': array([9, 8], dtype=int32), 'currentDistance': 0.25421433445705055}
episode index:2161
target Thresh 31.999999989452824
target distance 15.0
model initialize at round 2161
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 19.]), 'previousTarget': array([27., 19.]), 'currentState': array([24.34340648,  5.01372931,  1.65643614]), 'targetState': array([27, 19], dtype=int32), 'currentDistance': 14.23633579446982}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9275326481491903
{'scaleFactor': 20, 'currentTarget': array([27., 19.]), 'previousTarget': array([27., 19.]), 'currentState': array([26.76539631, 18.73431519,  1.50861536]), 'targetState': array([27, 19], dtype=int32), 'currentDistance': 0.3544394286151923}
episode index:2162
target Thresh 31.999999989557768
target distance 13.0
model initialize at round 2162
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 19.]), 'previousTarget': array([ 9., 19.]), 'currentState': array([11.0908994 ,  7.68051343,  2.52675873]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 11.510978956841674}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9275390963698338
{'scaleFactor': 20, 'currentTarget': array([ 9., 19.]), 'previousTarget': array([ 9., 19.]), 'currentState': array([ 8.89547933, 19.2207125 ,  1.90532599]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 0.2442101047711956}
episode index:2163
target Thresh 31.99999998966167
target distance 24.0
model initialize at round 2163
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 29.]), 'previousTarget': array([ 2., 29.]), 'currentState': array([7.40232322, 5.52897732, 1.81186104]), 'targetState': array([ 2, 29], dtype=int32), 'currentDistance': 24.084725490764136}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9275200787059457
{'scaleFactor': 20, 'currentTarget': array([ 2., 29.]), 'previousTarget': array([ 2., 29.]), 'currentState': array([ 1.86997484, 28.56950213,  1.76682005]), 'targetState': array([ 2, 29], dtype=int32), 'currentDistance': 0.44970541317487384}
episode index:2164
target Thresh 31.99999998976454
target distance 14.0
model initialize at round 2164
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 18.]), 'previousTarget': array([17., 18.]), 'currentState': array([ 4.66510413, 19.24472437,  5.4191497 ]), 'targetState': array([17, 18], dtype=int32), 'currentDistance': 12.397539867268613}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9275265267755508
{'scaleFactor': 20, 'currentTarget': array([17., 18.]), 'previousTarget': array([17., 18.]), 'currentState': array([16.34923674, 18.17028747,  6.2447656 ]), 'targetState': array([17, 18], dtype=int32), 'currentDistance': 0.6726742469883755}
episode index:2165
target Thresh 31.999999989866385
target distance 4.0
model initialize at round 2165
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 18.]), 'previousTarget': array([18., 18.]), 'currentState': array([15.63028304, 20.88503662,  5.40668738]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 3.7334963183566128}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.927550798923854
{'scaleFactor': 20, 'currentTarget': array([18., 18.]), 'previousTarget': array([18., 18.]), 'currentState': array([18.18896024, 17.8315182 ,  5.63268606]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 0.2531641546905564}
episode index:2166
target Thresh 31.999999989967215
target distance 9.0
model initialize at round 2166
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 16.]), 'previousTarget': array([17., 16.]), 'currentState': array([24.64303032, 15.99550056,  3.51865721]), 'targetState': array([17, 16], dtype=int32), 'currentDistance': 7.6430316410540895}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9275660482136906
{'scaleFactor': 20, 'currentTarget': array([17., 16.]), 'previousTarget': array([17., 16.]), 'currentState': array([16.71787094, 15.94470564,  3.33802616]), 'targetState': array([17, 16], dtype=int32), 'currentDistance': 0.2874965621238234}
episode index:2167
target Thresh 31.999999990067042
target distance 20.0
model initialize at round 2167
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  6.]), 'previousTarget': array([22.,  6.]), 'currentState': array([19.47035327, 24.23994125,  5.72220695]), 'targetState': array([22,  6], dtype=int32), 'currentDistance': 18.414520610328356}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9275595681395532
{'scaleFactor': 20, 'currentTarget': array([22.,  6.]), 'previousTarget': array([22.,  6.]), 'currentState': array([22.11856128,  6.87589377,  4.71271992]), 'targetState': array([22,  6], dtype=int32), 'currentDistance': 0.8838815993129363}
episode index:2168
target Thresh 31.99999999016588
target distance 8.0
model initialize at round 2168
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 15.]), 'previousTarget': array([23., 15.]), 'currentState': array([23.07512958,  8.69516698,  1.60848314]), 'targetState': array([23, 15], dtype=int32), 'currentDistance': 6.305280629126398}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9275792728107659
{'scaleFactor': 20, 'currentTarget': array([23., 15.]), 'previousTarget': array([23., 15.]), 'currentState': array([23.05798723, 14.68088917,  1.75697959]), 'targetState': array([23, 15], dtype=int32), 'currentDistance': 0.3243366185850589}
episode index:2169
target Thresh 31.999999990263728
target distance 14.0
model initialize at round 2169
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 18.]), 'previousTarget': array([13., 18.]), 'currentState': array([5.84613162, 5.77892469, 1.43102556]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 14.160950339777973}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9275813401264784
{'scaleFactor': 20, 'currentTarget': array([13., 18.]), 'previousTarget': array([13., 18.]), 'currentState': array([12.76281985, 17.75626485,  1.00573092]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 0.34009006085843435}
episode index:2170
target Thresh 31.999999990360607
target distance 10.0
model initialize at round 2170
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 21.]), 'previousTarget': array([15., 21.]), 'currentState': array([24.95261271, 15.68230275,  2.60895699]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 11.284166065328913}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9275877421574663
{'scaleFactor': 20, 'currentTarget': array([15., 21.]), 'previousTarget': array([15., 21.]), 'currentState': array([14.33516917, 21.22695772,  2.79433666]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 0.7025025501875455}
episode index:2171
target Thresh 31.99999999045652
target distance 8.0
model initialize at round 2171
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 17.]), 'previousTarget': array([17., 17.]), 'currentState': array([10.59138772, 24.54760679,  5.60460014]), 'targetState': array([17, 17], dtype=int32), 'currentDistance': 9.901347360199805}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9275985167006258
{'scaleFactor': 20, 'currentTarget': array([17., 17.]), 'previousTarget': array([17., 17.]), 'currentState': array([16.99162345, 16.92992726,  5.59010718]), 'targetState': array([17, 17], dtype=int32), 'currentDistance': 0.07057163634256013}
episode index:2172
target Thresh 31.99999999055148
target distance 16.0
model initialize at round 2172
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([13.74623061, 19.87730218,  4.88189268]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 14.930038777723668}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9275962830042278
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.2433031 ,  4.02847557,  4.80276668]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.0015268892771425}
episode index:2173
target Thresh 31.999999990645495
target distance 5.0
model initialize at round 2173
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 21.]), 'previousTarget': array([ 2., 21.]), 'currentState': array([ 5.6351721 , 23.01520288,  4.77654254]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 4.1563829024299555}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9276204337480162
{'scaleFactor': 20, 'currentTarget': array([ 2., 21.]), 'previousTarget': array([ 2., 21.]), 'currentState': array([ 2.8767771 , 21.05354396,  4.70921254]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 0.8784105205506778}
episode index:2174
target Thresh 31.999999990738573
target distance 14.0
model initialize at round 2174
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([18.68455206, 25.12214827,  5.12420465]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 12.567365424180982}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.927626806031075
{'scaleFactor': 20, 'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([21.90809182, 13.59174841,  5.14803914]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 0.598843294554865}
episode index:2175
target Thresh 31.999999990830727
target distance 16.0
model initialize at round 2175
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 27.]), 'previousTarget': array([22., 27.]), 'currentState': array([ 7.70386819, 24.95648013,  0.13489848]), 'targetState': array([22, 27], dtype=int32), 'currentDistance': 14.441445851093713}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9276288458021577
{'scaleFactor': 20, 'currentTarget': array([22., 27.]), 'previousTarget': array([22., 27.]), 'currentState': array([21.48734347, 26.93087919,  0.23447159]), 'targetState': array([22, 27], dtype=int32), 'currentDistance': 0.5172952737517481}
episode index:2176
target Thresh 31.99999999092196
target distance 18.0
model initialize at round 2176
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 23.]), 'previousTarget': array([ 4., 23.]), 'currentState': array([9.41012931, 5.55199571, 1.79741859]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 18.26752180680333}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9276223636715566
{'scaleFactor': 20, 'currentTarget': array([ 4., 23.]), 'previousTarget': array([ 4., 23.]), 'currentState': array([ 4.03335735, 22.49660429,  1.91202924]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 0.5044997058725612}
episode index:2177
target Thresh 31.999999991012288
target distance 8.0
model initialize at round 2177
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([ 7.33865801, 13.73105598,  4.31208396]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 9.395237936037155}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9276330926367671
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.82747919, 5.43103347, 4.31184137]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.594547168602368}
episode index:2178
target Thresh 31.999999991101717
target distance 15.0
model initialize at round 2178
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.07867729, 15.12641085,  5.37821269]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 13.126646632887415}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9276351267144496
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.96544272,  1.37698667,  4.71674034]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.6239710045247977}
episode index:2179
target Thresh 31.999999991190258
target distance 19.0
model initialize at round 2179
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  4.]), 'previousTarget': array([21.,  4.]), 'currentState': array([ 0.43283896, 13.38649014,  4.5247314 ]), 'targetState': array([21,  4], dtype=int32), 'currentDistance': 22.60783736845893}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9276162045791292
{'scaleFactor': 20, 'currentTarget': array([21.,  4.]), 'previousTarget': array([21.,  4.]), 'currentState': array([21.52126985,  3.78076983,  5.88587149]), 'targetState': array([21,  4], dtype=int32), 'currentDistance': 0.5654945812111408}
episode index:2180
target Thresh 31.999999991277917
target distance 10.0
model initialize at round 2180
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 19.]), 'previousTarget': array([16., 19.]), 'currentState': array([13.50535621, 27.39136219,  5.42407274]), 'targetState': array([16, 19], dtype=int32), 'currentDistance': 8.754325045423172}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9276313259938109
{'scaleFactor': 20, 'currentTarget': array([16., 19.]), 'previousTarget': array([16., 19.]), 'currentState': array([15.88863664, 19.85161394,  5.03693211]), 'targetState': array([16, 19], dtype=int32), 'currentDistance': 0.8588644239148325}
episode index:2181
target Thresh 31.999999991364703
target distance 12.0
model initialize at round 2181
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([12.84853894,  8.24080029,  5.96413903]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 11.42445401131079}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9276376728423019
{'scaleFactor': 20, 'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([23.55752549,  2.8996742 ,  5.9860983 ]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 0.5664803101562171}
episode index:2182
target Thresh 31.999999991450625
target distance 11.0
model initialize at round 2182
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 25.]), 'previousTarget': array([ 5., 25.]), 'currentState': array([ 8.23209171, 12.85355414,  0.26059407]), 'targetState': array([ 5, 25], dtype=int32), 'currentDistance': 12.56911149406689}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9276397010947365
{'scaleFactor': 20, 'currentTarget': array([ 5., 25.]), 'previousTarget': array([ 5., 25.]), 'currentState': array([ 4.67023439, 25.63795912,  2.0078281 ]), 'targetState': array([ 5, 25], dtype=int32), 'currentDistance': 0.7181484504157635}
episode index:2183
target Thresh 31.99999999153569
target distance 22.0
model initialize at round 2183
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 6.]), 'previousTarget': array([5., 6.]), 'currentState': array([ 7.32210581, 27.86938921,  4.22927809]), 'targetState': array([5, 6], dtype=int32), 'currentDistance': 21.99232502025079}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9276249110549764
{'scaleFactor': 20, 'currentTarget': array([5., 6.]), 'previousTarget': array([5., 6.]), 'currentState': array([5.18033835, 6.47378469, 4.61633698]), 'targetState': array([5, 6], dtype=int32), 'currentDistance': 0.5069456073325994}
episode index:2184
target Thresh 31.999999991619912
target distance 10.0
model initialize at round 2184
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 14.]), 'previousTarget': array([ 7., 14.]), 'currentState': array([18.23591441,  7.1423238 ,  1.75606554]), 'targetState': array([ 7, 14], dtype=int32), 'currentDistance': 13.16333906864388}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.927626943291522
{'scaleFactor': 20, 'currentTarget': array([ 7., 14.]), 'previousTarget': array([ 7., 14.]), 'currentState': array([ 7.12297349, 13.93011349,  2.76700284]), 'targetState': array([ 7, 14], dtype=int32), 'currentDistance': 0.14144470332623466}
episode index:2185
target Thresh 31.999999991703298
target distance 7.0
model initialize at round 2185
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 21.]), 'previousTarget': array([21., 21.]), 'currentState': array([15.66058908, 21.72644498,  0.84673243]), 'targetState': array([21, 21], dtype=int32), 'currentDistance': 5.388601976642079}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.927646463903008
{'scaleFactor': 20, 'currentTarget': array([21., 21.]), 'previousTarget': array([21., 21.]), 'currentState': array([21.27243192, 21.01661384,  6.22720466]), 'targetState': array([21, 21], dtype=int32), 'currentDistance': 0.2729380309554593}
episode index:2186
target Thresh 31.999999991785852
target distance 11.0
model initialize at round 2186
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 15.]), 'previousTarget': array([26., 15.]), 'currentState': array([21.45427665, 24.65880244,  5.28017479]), 'targetState': array([26, 15], dtype=int32), 'currentDistance': 10.675020620578861}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9276571376963307
{'scaleFactor': 20, 'currentTarget': array([26., 15.]), 'previousTarget': array([26., 15.]), 'currentState': array([25.84866889, 15.7105451 ,  5.20283599]), 'targetState': array([26, 15], dtype=int32), 'currentDistance': 0.7264815511696625}
episode index:2187
target Thresh 31.999999991867583
target distance 9.0
model initialize at round 2187
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 23.]), 'previousTarget': array([19., 23.]), 'currentState': array([ 9.03886168, 23.61847866,  5.11454797]), 'targetState': array([19, 23], dtype=int32), 'currentDistance': 9.980320254512675}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9276678017329869
{'scaleFactor': 20, 'currentTarget': array([19., 23.]), 'previousTarget': array([19., 23.]), 'currentState': array([18.40319216, 23.03428151,  0.09269422]), 'targetState': array([19, 23], dtype=int32), 'currentDistance': 0.5977916167691217}
episode index:2188
target Thresh 31.999999991948503
target distance 22.0
model initialize at round 2188
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([ 4.04860495, 23.12345837,  5.3572011 ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 20.227465466427716}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9276571641237021
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.23535163, 3.55492624, 4.62643937]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.6027715319663927}
episode index:2189
target Thresh 31.999999992028616
target distance 6.0
model initialize at round 2189
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 16.]), 'previousTarget': array([16., 16.]), 'currentState': array([ 8.40561045, 20.46119572,  4.47748137]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 8.807781780274865}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9276678184094447
{'scaleFactor': 20, 'currentTarget': array([16., 16.]), 'previousTarget': array([16., 16.]), 'currentState': array([16.65486442, 15.90885705,  5.96526185]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 0.6611765566964621}
episode index:2190
target Thresh 31.99999999210793
target distance 9.0
model initialize at round 2190
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([ 6.52716942, 14.06966297,  1.26166671]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 10.191799069236648}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9276784629696868
{'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([13.75805435, 20.73383067,  0.9609737 ]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 0.35969961203860623}
episode index:2191
target Thresh 31.99999999218646
target distance 10.0
model initialize at round 2191
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([11.38563808,  7.52436973,  4.43811011]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 11.62409682626419}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9276847593594821
{'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([22.31938039,  7.93302268,  0.37890404]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 0.6839071727165171}
episode index:2192
target Thresh 31.999999992264204
target distance 21.0
model initialize at round 2192
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([22.2558589 ,  5.31639155,  3.81126714]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 19.81727251436991}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9276741334204257
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.29816318, 9.90112395, 2.9975093 ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.31413015924659515}
episode index:2193
target Thresh 31.999999992341177
target distance 24.0
model initialize at round 2193
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  3.]), 'previousTarget': array([24.,  3.]), 'currentState': array([23.43890564, 25.11511399,  5.38830674]), 'targetState': array([24,  3], dtype=int32), 'currentDistance': 22.122230761506493}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9276593950981095
{'scaleFactor': 20, 'currentTarget': array([24.,  3.]), 'previousTarget': array([24.,  3.]), 'currentState': array([24.11928663,  3.49141634,  4.754837  ]), 'targetState': array([24,  3], dtype=int32), 'currentDistance': 0.5056869793224867}
episode index:2194
target Thresh 31.999999992417383
target distance 7.0
model initialize at round 2194
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 20.]), 'previousTarget': array([ 5., 20.]), 'currentState': array([ 9.13445242, 25.2017556 ,  4.1491565 ]), 'targetState': array([ 5, 20], dtype=int32), 'currentDistance': 6.644693985003493}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9276788208862197
{'scaleFactor': 20, 'currentTarget': array([ 5., 20.]), 'previousTarget': array([ 5., 20.]), 'currentState': array([ 5.44068019, 20.49601255,  4.18740628]), 'targetState': array([ 5, 20], dtype=int32), 'currentDistance': 0.6634964041314869}
episode index:2195
target Thresh 31.999999992492832
target distance 20.0
model initialize at round 2195
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 15.]), 'previousTarget': array([ 3., 15.]), 'currentState': array([22.37242232, 26.43841928,  3.32025766]), 'targetState': array([ 3, 15], dtype=int32), 'currentDistance': 22.497292778078766}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9276640938522362
{'scaleFactor': 20, 'currentTarget': array([ 3., 15.]), 'previousTarget': array([ 3., 15.]), 'currentState': array([ 3.89053045, 15.24899086,  3.57577655]), 'targetState': array([ 3, 15], dtype=int32), 'currentDistance': 0.9246842356654988}
episode index:2196
target Thresh 31.99999999256753
target distance 22.0
model initialize at round 2196
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([ 5.37459558, 19.35924827,  5.94684911]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 23.97857671373003}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9276453049482143
{'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([26.74275946,  9.16082732,  5.94218461]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 0.30337785087815045}
episode index:2197
target Thresh 31.999999992641484
target distance 18.0
model initialize at round 2197
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 23.]), 'previousTarget': array([17., 23.]), 'currentState': array([12.63953449,  5.3798878 ,  1.23768681]), 'targetState': array([17, 23], dtype=int32), 'currentDistance': 18.151639413410667}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9276388772605598
{'scaleFactor': 20, 'currentTarget': array([17., 23.]), 'previousTarget': array([17., 23.]), 'currentState': array([16.87551195, 22.61917158,  1.43482922]), 'targetState': array([17, 23], dtype=int32), 'currentDistance': 0.4006589029587221}
episode index:2198
target Thresh 31.999999992714702
target distance 11.0
model initialize at round 2198
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 27.]), 'previousTarget': array([ 4., 27.]), 'currentState': array([12.54128436, 17.35362175,  2.56852823]), 'targetState': array([ 4, 27], dtype=int32), 'currentDistance': 12.884337465553143}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9276451716089639
{'scaleFactor': 20, 'currentTarget': array([ 4., 27.]), 'previousTarget': array([ 4., 27.]), 'currentState': array([ 4.49011392, 26.18565092,  2.39217578]), 'targetState': array([ 4, 27], dtype=int32), 'currentDistance': 0.9504609854170588}
episode index:2199
target Thresh 31.999999992787192
target distance 20.0
model initialize at round 2199
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  7.]), 'previousTarget': array([22.,  7.]), 'currentState': array([11.32948849, 26.79559993,  4.27334523]), 'targetState': array([22,  7], dtype=int32), 'currentDistance': 22.488343478154697}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9276264169271944
{'scaleFactor': 20, 'currentTarget': array([22.,  7.]), 'previousTarget': array([22.,  7.]), 'currentState': array([22.36058288,  6.44785278,  5.17853551]), 'targetState': array([22,  7], dtype=int32), 'currentDistance': 0.659459297743178}
episode index:2200
target Thresh 31.99999999285896
target distance 21.0
model initialize at round 2200
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 3.]), 'previousTarget': array([5., 3.]), 'currentState': array([18.75539465, 25.13284845,  3.41316688]), 'targetState': array([5, 3], dtype=int32), 'currentDistance': 26.05904569821347}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9276036520957868
{'scaleFactor': 20, 'currentTarget': array([5., 3.]), 'previousTarget': array([5., 3.]), 'currentState': array([5.77176748, 3.72204671, 4.02802612]), 'targetState': array([5, 3], dtype=int32), 'currentDistance': 1.056871089088856}
episode index:2201
target Thresh 31.999999992930015
target distance 11.0
model initialize at round 2201
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 12.]), 'previousTarget': array([11., 12.]), 'currentState': array([ 5.68179184, 22.93703731,  5.23576492]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 12.161501687074349}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9276099538656801
{'scaleFactor': 20, 'currentTarget': array([11., 12.]), 'previousTarget': array([11., 12.]), 'currentState': array([10.94626235, 12.20807626,  5.32342609]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 0.2149033894113445}
episode index:2202
target Thresh 31.999999993000365
target distance 18.0
model initialize at round 2202
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([24.33043825,  9.17424296,  2.94558406]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 16.351302339760867}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9276077453956675
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([8.53862386, 9.82246389, 3.17938699]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.5671284964172346}
episode index:2203
target Thresh 31.999999993070013
target distance 14.0
model initialize at round 2203
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 15.]), 'previousTarget': array([ 6., 15.]), 'currentState': array([18.05295293, 27.23912293,  4.17595857]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 17.17759600856098}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9276013522477946
{'scaleFactor': 20, 'currentTarget': array([ 6., 15.]), 'previousTarget': array([ 6., 15.]), 'currentState': array([ 5.61690032, 14.3876874 ,  3.97480768]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 0.7222825492165379}
episode index:2204
target Thresh 31.999999993138967
target distance 17.0
model initialize at round 2204
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 18.]), 'previousTarget': array([21., 18.]), 'currentState': array([5.56087059, 1.3706582 , 0.62673872]), 'targetState': array([21, 18], dtype=int32), 'currentDistance': 22.691446091341614}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9275867204573233
{'scaleFactor': 20, 'currentTarget': array([21., 18.]), 'previousTarget': array([21., 18.]), 'currentState': array([20.11906334, 17.31360391,  0.82574899]), 'targetState': array([21, 18], dtype=int32), 'currentDistance': 1.116776157945438}
episode index:2205
target Thresh 31.999999993207233
target distance 1.0
model initialize at round 2205
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 14.]), 'previousTarget': array([ 6., 14.]), 'currentState': array([ 3.32370416, 14.85026252,  4.24068213]), 'targetState': array([ 6, 14], dtype=int32), 'currentDistance': 2.808114271225763}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.927610525207796
{'scaleFactor': 20, 'currentTarget': array([ 6., 14.]), 'previousTarget': array([ 6., 14.]), 'currentState': array([ 6.08412787, 13.82688961,  0.49551099]), 'targetState': array([ 6, 14], dtype=int32), 'currentDistance': 0.19247001750032353}
episode index:2206
target Thresh 31.999999993274823
target distance 5.0
model initialize at round 2206
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 23.]), 'previousTarget': array([27., 23.]), 'currentState': array([26.65236131, 26.35332595,  5.51432848]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 3.3712976131511376}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9276343083862246
{'scaleFactor': 20, 'currentTarget': array([27., 23.]), 'previousTarget': array([27., 23.]), 'currentState': array([27.46942323, 22.58411311,  4.63387263]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 0.6271523542541597}
episode index:2207
target Thresh 31.99999999334174
target distance 23.0
model initialize at round 2207
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([23.8797581 , 15.56034046,  3.30750871]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 25.741146978172225}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9276116121519007
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.50106767, 2.16657959, 3.75507843]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.5280317916217926}
episode index:2208
target Thresh 31.99999999340799
target distance 21.0
model initialize at round 2208
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 6.]), 'previousTarget': array([7., 6.]), 'currentState': array([11.18425806, 25.06293503,  4.39829683]), 'targetState': array([7, 6], dtype=int32), 'currentDistance': 19.516749403633806}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9276010962908129
{'scaleFactor': 20, 'currentTarget': array([7., 6.]), 'previousTarget': array([7., 6.]), 'currentState': array([6.91536545, 5.93229772, 4.4776431 ]), 'targetState': array([7, 6], dtype=int32), 'currentDistance': 0.10838175835309607}
episode index:2209
target Thresh 31.999999993473583
target distance 19.0
model initialize at round 2209
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([ 3.6413193 , 23.62789917,  5.05024541]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 22.06906050015738}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9275864977197577
{'scaleFactor': 20, 'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([20.53454675, 10.54734669,  5.62315273]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 0.7184950389011311}
episode index:2210
target Thresh 31.99999999353852
target distance 20.0
model initialize at round 2210
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([27.3912512 ,  9.94699957,  1.60764283]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 26.156805042637462}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9275598850173826
{'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([ 5.45415083, 25.22045015,  2.49189558]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 0.5886846210821849}
episode index:2211
target Thresh 31.999999993602813
target distance 13.0
model initialize at round 2211
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 28.]), 'previousTarget': array([13., 28.]), 'currentState': array([21.53095133, 16.61628631,  2.86323959]), 'targetState': array([13, 28], dtype=int32), 'currentDistance': 14.225542800515829}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9275619218450906
{'scaleFactor': 20, 'currentTarget': array([13., 28.]), 'previousTarget': array([13., 28.]), 'currentState': array([13.08695848, 27.47461997,  2.15621382]), 'targetState': array([13, 28], dtype=int32), 'currentDistance': 0.5325278921013356}
episode index:2212
target Thresh 31.999999993666467
target distance 13.0
model initialize at round 2212
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 24.]), 'previousTarget': array([12., 24.]), 'currentState': array([17.90035172, 12.68001735,  2.64004076]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 12.765428216577426}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9275682111480981
{'scaleFactor': 20, 'currentTarget': array([12., 24.]), 'previousTarget': array([12., 24.]), 'currentState': array([12.32653143, 23.10113928,  2.13032082]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 0.956333294824756}
episode index:2213
target Thresh 31.999999993729485
target distance 4.0
model initialize at round 2213
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 20.]), 'previousTarget': array([22., 20.]), 'currentState': array([21.25804515, 14.3369289 ,  5.87632096]), 'targetState': array([22, 20], dtype=int32), 'currentDistance': 5.7114684043470225}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9275875114140656
{'scaleFactor': 20, 'currentTarget': array([22., 20.]), 'previousTarget': array([22., 20.]), 'currentState': array([22.16161127, 19.19474083,  2.01809676]), 'targetState': array([22, 20], dtype=int32), 'currentDistance': 0.8213163391613925}
episode index:2214
target Thresh 31.999999993791878
target distance 16.0
model initialize at round 2214
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  5.]), 'previousTarget': array([26.,  5.]), 'currentState': array([11.87036971,  6.55504475,  6.18292374]), 'targetState': array([26,  5], dtype=int32), 'currentDistance': 14.21494341946244}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9275895330106765
{'scaleFactor': 20, 'currentTarget': array([26.,  5.]), 'previousTarget': array([26.,  5.]), 'currentState': array([25.72455001,  4.9170095 ,  0.14381506]), 'targetState': array([26,  5], dtype=int32), 'currentDistance': 0.28768058800171}
episode index:2215
target Thresh 31.999999993853653
target distance 5.0
model initialize at round 2215
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 21.]), 'previousTarget': array([ 6., 21.]), 'currentState': array([ 9.52882428, 19.27328635,  2.83764678]), 'targetState': array([ 6, 21], dtype=int32), 'currentDistance': 3.9286309070982077}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9276132290697873
{'scaleFactor': 20, 'currentTarget': array([ 6., 21.]), 'previousTarget': array([ 6., 21.]), 'currentState': array([ 5.91280774, 20.95567778,  2.84041619]), 'targetState': array([ 6, 21], dtype=int32), 'currentDistance': 0.09781078299214294}
episode index:2216
target Thresh 31.99999999391481
target distance 1.0
model initialize at round 2216
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 20.]), 'previousTarget': array([13., 20.]), 'currentState': array([12.51962919, 19.66659285,  4.02787647]), 'targetState': array([13, 20], dtype=int32), 'currentDistance': 0.5847362181121868}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.92764587984603
{'scaleFactor': 20, 'currentTarget': array([13., 20.]), 'previousTarget': array([13., 20.]), 'currentState': array([12.51962919, 19.66659285,  4.02787647]), 'targetState': array([13, 20], dtype=int32), 'currentDistance': 0.5847362181121868}
episode index:2217
target Thresh 31.999999993975358
target distance 9.0
model initialize at round 2217
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 23.]), 'previousTarget': array([21., 23.]), 'currentState': array([24.38746134, 15.05996785,  2.09381127]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 8.63243910022063}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9276607356305898
{'scaleFactor': 20, 'currentTarget': array([21., 23.]), 'previousTarget': array([21., 23.]), 'currentState': array([20.99306081, 22.26715313,  1.88333947]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 0.7328797263945842}
episode index:2218
target Thresh 31.999999994035303
target distance 16.0
model initialize at round 2218
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([23.33227034, 17.77402242,  4.28627253]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 15.788148913118569}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9276585201996738
{'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([23.94690835,  2.26364447,  4.74481922]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 0.26893704207167396}
episode index:2219
target Thresh 31.999999994094654
target distance 15.0
model initialize at round 2219
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 16.]), 'previousTarget': array([ 2., 16.]), 'currentState': array([15.38237292,  3.15032543,  2.73841599]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 18.552682860229545}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9276521502570089
{'scaleFactor': 20, 'currentTarget': array([ 2., 16.]), 'previousTarget': array([ 2., 16.]), 'currentState': array([ 2.37305288, 15.36454151,  2.3224491 ]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 0.7368690115429816}
episode index:2220
target Thresh 31.999999994153413
target distance 6.0
model initialize at round 2220
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 20.]), 'previousTarget': array([13., 20.]), 'currentState': array([ 7.69405441, 17.53319163,  0.13571095]), 'targetState': array([13, 20], dtype=int32), 'currentDistance': 5.851341912326532}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.927671351900297
{'scaleFactor': 20, 'currentTarget': array([13., 20.]), 'previousTarget': array([13., 20.]), 'currentState': array([13.07404257, 20.03338183,  0.75003924]), 'targetState': array([13, 20], dtype=int32), 'currentDistance': 0.08121975291134445}
episode index:2221
target Thresh 31.99999999421159
target distance 7.0
model initialize at round 2221
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([7.02907219, 2.97556027, 3.45215529]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 5.122820018000014}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9276905362603779
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.1357713 , 2.02132963, 3.02963506]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.8644918701835095}
episode index:2222
target Thresh 31.99999999426918
target distance 23.0
model initialize at round 2222
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  5.]), 'previousTarget': array([13.,  5.]), 'currentState': array([ 0.50304081, 28.7690912 ,  3.67699063]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 26.85411859603838}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9276640204153525
{'scaleFactor': 20, 'currentTarget': array([13.,  5.]), 'previousTarget': array([13.,  5.]), 'currentState': array([13.18024876,  4.93678617,  5.25338047]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 0.1910120483638468}
episode index:2223
target Thresh 31.999999994326206
target distance 14.0
model initialize at round 2223
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([20.43040059, 19.11469632,  5.38919413]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 12.384212045738437}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9276702327035654
{'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([23.17253931,  7.6011817 ,  4.85675438]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 0.6254512403920461}
episode index:2224
target Thresh 31.99999999438266
target distance 5.0
model initialize at round 2224
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.59154562, 13.42441694,  4.06155026]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 3.475134185084997}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9276937966439234
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.22860318,  9.58141352,  4.952378  ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.4769423975848758}
episode index:2225
target Thresh 31.999999994438554
target distance 10.0
model initialize at round 2225
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([13.99691084, 18.64863025,  5.08384967]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 10.524340582461942}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9277042621665004
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 8.74658691, 10.51174745,  3.98348002]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.9051394775635165}
episode index:2226
target Thresh 31.99999999449389
target distance 15.0
model initialize at round 2226
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 6.]), 'previousTarget': array([6., 6.]), 'currentState': array([ 7.41191469, 20.08409335,  4.69772572]), 'targetState': array([6, 6], dtype=int32), 'currentDistance': 14.15468786972056}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9277062204447853
{'scaleFactor': 20, 'currentTarget': array([6., 6.]), 'previousTarget': array([6., 6.]), 'currentState': array([6.06382506, 6.22907936, 4.71683258]), 'targetState': array([6, 6], dtype=int32), 'currentDistance': 0.23780452541386235}
episode index:2227
target Thresh 31.999999994548677
target distance 21.0
model initialize at round 2227
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  3.]), 'previousTarget': array([13.,  3.]), 'currentState': array([ 5.66548932, 22.45419534,  6.12892008]), 'targetState': array([13,  3], dtype=int32), 'currentDistance': 20.790881732185966}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.927691692632314
{'scaleFactor': 20, 'currentTarget': array([13.,  3.]), 'previousTarget': array([13.,  3.]), 'currentState': array([13.36911753,  2.44214864,  5.13267099]), 'targetState': array([13,  3], dtype=int32), 'currentDistance': 0.6689139648048301}
episode index:2228
target Thresh 31.99999999460292
target distance 4.0
model initialize at round 2228
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 23.]), 'previousTarget': array([ 7., 23.]), 'currentState': array([ 7.94184685, 25.69129836,  5.0424552 ]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 2.8513439517259567}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9277196461125147
{'scaleFactor': 20, 'currentTarget': array([ 7., 23.]), 'previousTarget': array([ 7., 23.]), 'currentState': array([ 7.85226077, 23.74107931,  4.28291911]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 1.1294011561270467}
episode index:2229
target Thresh 31.999999994656623
target distance 7.0
model initialize at round 2229
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  9.]), 'previousTarget': array([22.,  9.]), 'currentState': array([17.57866646,  2.94352702,  1.58478343]), 'targetState': array([22,  9], dtype=int32), 'currentDistance': 7.498603554791715}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9277343888765898
{'scaleFactor': 20, 'currentTarget': array([22.,  9.]), 'previousTarget': array([22.,  9.]), 'currentState': array([22.05743354,  9.37980942,  1.09192612]), 'targetState': array([22,  9], dtype=int32), 'currentDistance': 0.3841273320819655}
episode index:2230
target Thresh 31.99999999470979
target distance 23.0
model initialize at round 2230
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 29.]), 'previousTarget': array([18., 29.]), 'currentState': array([25.38587852,  6.47644508,  1.84457016]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 23.703622666286016}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9277158548034563
{'scaleFactor': 20, 'currentTarget': array([18., 29.]), 'previousTarget': array([18., 29.]), 'currentState': array([18.03247295, 28.81264418,  1.93993643]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 0.19014913630823665}
episode index:2231
target Thresh 31.999999994762426
target distance 9.0
model initialize at round 2231
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([16.65361005, 10.55086491,  0.16193748]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 10.547680716020334}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9277262823102201
{'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([24.4700186 , 16.5698921 ,  0.82828218]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 0.6825489664604293}
episode index:2232
target Thresh 31.99999999481454
target distance 7.0
model initialize at round 2232
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  3.]), 'previousTarget': array([10.,  3.]), 'currentState': array([4.0741815 , 2.70442206, 0.13144654]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 5.933185590201639}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9277453475666867
{'scaleFactor': 20, 'currentTarget': array([10.,  3.]), 'previousTarget': array([10.,  3.]), 'currentState': array([10.04635208,  3.03429999,  0.26444525]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 0.05766285266885605}
episode index:2233
target Thresh 31.999999994866137
target distance 2.0
model initialize at round 2233
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 26.]), 'previousTarget': array([16., 26.]), 'currentState': array([15.18972402, 28.19055183,  6.05916483]), 'targetState': array([16, 26], dtype=int32), 'currentDistance': 2.3356079436862327}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9277732144657167
{'scaleFactor': 20, 'currentTarget': array([16., 26.]), 'previousTarget': array([16., 26.]), 'currentState': array([15.7793935 , 26.61358224,  4.0612914 ]), 'targetState': array([16, 26], dtype=int32), 'currentDistance': 0.6520355794670167}
episode index:2234
target Thresh 31.99999999491722
target distance 10.0
model initialize at round 2234
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 25.]), 'previousTarget': array([20., 25.]), 'currentState': array([10.61184717, 16.56781093,  0.18871832]), 'targetState': array([20, 25], dtype=int32), 'currentDistance': 12.619002574319508}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9277793473225109
{'scaleFactor': 20, 'currentTarget': array([20., 25.]), 'previousTarget': array([20., 25.]), 'currentState': array([19.32025072, 24.4993107 ,  0.82555965]), 'targetState': array([20, 25], dtype=int32), 'currentDistance': 0.8442445464307549}
episode index:2235
target Thresh 31.999999994967794
target distance 15.0
model initialize at round 2235
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 17.]), 'previousTarget': array([11., 17.]), 'currentState': array([18.19659017,  3.4788241 ,  3.0784626 ]), 'targetState': array([11, 17], dtype=int32), 'currentDistance': 15.317085482943105}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9277770956888372
{'scaleFactor': 20, 'currentTarget': array([11., 17.]), 'previousTarget': array([11., 17.]), 'currentState': array([10.8151659 , 17.23091241,  2.15727329]), 'targetState': array([11, 17], dtype=int32), 'currentDistance': 0.2957772576098881}
episode index:2236
target Thresh 31.999999995017866
target distance 23.0
model initialize at round 2236
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 26.]), 'previousTarget': array([ 8., 26.]), 'currentState': array([20.32568029,  2.82958405,  2.23302555]), 'targetState': array([ 8, 26], dtype=int32), 'currentDistance': 26.244819867815924}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9277546298539289
{'scaleFactor': 20, 'currentTarget': array([ 8., 26.]), 'previousTarget': array([ 8., 26.]), 'currentState': array([ 8.37770683, 25.39719117,  2.13301113]), 'targetState': array([ 8, 26], dtype=int32), 'currentDistance': 0.7113655400860515}
episode index:2237
target Thresh 31.99999999506744
target distance 16.0
model initialize at round 2237
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([23.26336933, 26.66223484,  2.42365965]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 16.348094895034404}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9277523912768841
{'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([ 7.82598007, 24.89482142,  3.11909998]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 0.8326497473035271}
episode index:2238
target Thresh 31.999999995116518
target distance 23.0
model initialize at round 2238
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([21.9322191 , 27.02140115,  4.42887211]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 22.468186537376518}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9277379142170279
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.47652704,  6.53412862,  4.31736743]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.7158012313074761}
episode index:2239
target Thresh 31.999999995165112
target distance 9.0
model initialize at round 2239
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 15.]), 'previousTarget': array([25., 15.]), 'currentState': array([15.29984375,  4.46958525,  5.29332018]), 'targetState': array([25, 15], dtype=int32), 'currentDistance': 14.317215723213646}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9277398461070681
{'scaleFactor': 20, 'currentTarget': array([25., 15.]), 'previousTarget': array([25., 15.]), 'currentState': array([24.15792268, 14.07776585,  0.97215559]), 'targetState': array([25, 15], dtype=int32), 'currentDistance': 1.248843483530962}
episode index:2240
target Thresh 31.99999999521322
target distance 2.0
model initialize at round 2240
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([22.34514716, 13.69365025,  2.31464243]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 2.3320325340860153}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.927767628415811
{'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([21.47104708, 15.46194353,  1.73938179]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 0.7545170322859884}
episode index:2241
target Thresh 31.999999995260847
target distance 6.0
model initialize at round 2241
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 26.]), 'previousTarget': array([19., 26.]), 'currentState': array([14.07062748, 28.29851642,  6.15447885]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 5.438923712782691}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9277865986975167
{'scaleFactor': 20, 'currentTarget': array([19., 26.]), 'previousTarget': array([19., 26.]), 'currentState': array([19.50729365, 25.8515215 ,  5.89366668]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 0.5285761183170935}
episode index:2242
target Thresh 31.999999995308002
target distance 13.0
model initialize at round 2242
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 25.]), 'previousTarget': array([ 5., 25.]), 'currentState': array([19.11581577, 26.25989811,  1.85597199]), 'targetState': array([ 5, 25], dtype=int32), 'currentDistance': 14.171929936204881}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.927788506298591
{'scaleFactor': 20, 'currentTarget': array([ 5., 25.]), 'previousTarget': array([ 5., 25.]), 'currentState': array([ 5.816283  , 25.15341578,  3.44693273]), 'targetState': array([ 5, 25], dtype=int32), 'currentDistance': 0.830574701514858}
episode index:2243
target Thresh 31.99999999535469
target distance 17.0
model initialize at round 2243
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 19.]), 'previousTarget': array([16., 19.]), 'currentState': array([2.64239286, 3.55554476, 0.16915941]), 'targetState': array([16, 19], dtype=int32), 'currentDistance': 20.419521694335277}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9277780756251107
{'scaleFactor': 20, 'currentTarget': array([16., 19.]), 'previousTarget': array([16., 19.]), 'currentState': array([15.04715146, 18.30666191,  0.75373473]), 'targetState': array([16, 19], dtype=int32), 'currentDistance': 1.178404868652172}
episode index:2244
target Thresh 31.999999995400913
target distance 8.0
model initialize at round 2244
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 25.]), 'previousTarget': array([ 8., 25.]), 'currentState': array([14.02237031, 22.91497527,  2.93342221]), 'targetState': array([ 8, 25], dtype=int32), 'currentDistance': 6.373089693490881}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9277970159032287
{'scaleFactor': 20, 'currentTarget': array([ 8., 25.]), 'previousTarget': array([ 8., 25.]), 'currentState': array([ 8.32394643, 24.75052448,  2.90161425]), 'targetState': array([ 8, 25], dtype=int32), 'currentDistance': 0.4088756845374726}
episode index:2245
target Thresh 31.99999999544667
target distance 9.0
model initialize at round 2245
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([12.7660958 , 18.23883234,  5.38840466]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 10.331501334199938}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9278073422763351
{'scaleFactor': 20, 'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([18.82490224, 10.32518809,  5.45903645]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 0.36933253132722527}
episode index:2246
target Thresh 31.99999999549198
target distance 12.0
model initialize at round 2246
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 26.]), 'previousTarget': array([ 3., 26.]), 'currentState': array([13.30196498, 20.90239688,  3.14165246]), 'targetState': array([ 3, 26], dtype=int32), 'currentDistance': 11.494174182419403}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9278134271927234
{'scaleFactor': 20, 'currentTarget': array([ 3., 26.]), 'previousTarget': array([ 3., 26.]), 'currentState': array([ 2.60757788, 26.03032121,  2.73649848]), 'targetState': array([ 3, 26], dtype=int32), 'currentDistance': 0.39359178923277516}
episode index:2247
target Thresh 31.999999995536836
target distance 7.0
model initialize at round 2247
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 18.]), 'previousTarget': array([13., 18.]), 'currentState': array([ 8.46572283, 12.59591227,  0.883847  ]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 7.054348561671297}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9278323264688833
{'scaleFactor': 20, 'currentTarget': array([13., 18.]), 'previousTarget': array([13., 18.]), 'currentState': array([12.27462493, 17.22061377,  1.04124377]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 1.0647121182961965}
episode index:2248
target Thresh 31.999999995581245
target distance 8.0
model initialize at round 2248
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([11.630541  , 23.41680227,  5.52344829]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 9.483392450960324}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9278426233668073
{'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([16.33031162, 14.66062043,  5.2113347 ]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 0.47358658426487554}
episode index:2249
target Thresh 31.99999999562521
target distance 18.0
model initialize at round 2249
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([ 9.27967742, 12.52102711,  1.00308251]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 16.494708611997154}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9278403576206122
{'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([ 9.69783857, 28.24824733,  1.5063667 ]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 0.8102059089371422}
episode index:2250
target Thresh 31.99999999566874
target distance 14.0
model initialize at round 2250
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 5.]), 'previousTarget': array([8., 5.]), 'currentState': array([23.13417406,  9.24339748,  1.84130495]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 15.717813036155803}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.927838093887519
{'scaleFactor': 20, 'currentTarget': array([8., 5.]), 'previousTarget': array([8., 5.]), 'currentState': array([8.50535758, 4.97116546, 3.4631025 ]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 0.5061795288751398}
episode index:2251
target Thresh 31.99999999571184
target distance 22.0
model initialize at round 2251
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([ 3.86877181, 16.99682472,  5.64134562]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 21.312482368728176}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9278236623423908
{'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([24.40226222, 10.04889342,  5.8968691 ]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 0.4052227312878865}
episode index:2252
target Thresh 31.999999995754507
target distance 13.0
model initialize at round 2252
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([10.38013065, 16.9631342 ,  5.88247174]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 11.9917274822812}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9278297238102374
{'scaleFactor': 20, 'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([21.98669617, 14.14244031,  6.10563506]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 0.143060242776603}
episode index:2253
target Thresh 31.99999999579675
target distance 9.0
model initialize at round 2253
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 16.]), 'previousTarget': array([14., 16.]), 'currentState': array([24.43635214,  8.87708643,  1.55820244]), 'targetState': array([14, 16], dtype=int32), 'currentDistance': 12.635400423435545}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9278316029691092
{'scaleFactor': 20, 'currentTarget': array([14., 16.]), 'previousTarget': array([14., 16.]), 'currentState': array([13.29718161, 16.22394685,  2.47005432]), 'targetState': array([14, 16], dtype=int32), 'currentDistance': 0.7376353324017635}
episode index:2254
target Thresh 31.99999999583857
target distance 11.0
model initialize at round 2254
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 20.]), 'previousTarget': array([24., 20.]), 'currentState': array([13.83764919, 22.45970268,  0.03982735]), 'targetState': array([24, 20], dtype=int32), 'currentDistance': 10.455788408887729}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9278418727903646
{'scaleFactor': 20, 'currentTarget': array([24., 20.]), 'previousTarget': array([24., 20.]), 'currentState': array([23.54990445, 20.27704607,  6.11518422]), 'targetState': array([24, 20], dtype=int32), 'currentDistance': 0.5285267501025122}
episode index:2255
target Thresh 31.99999999587998
target distance 16.0
model initialize at round 2255
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([10.31586032, 16.20515815,  5.6339736 ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 14.675128726353622}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9278437448981292
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.06923586,  2.91832807,  4.97085702]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.9209343374689338}
episode index:2256
target Thresh 31.999999995920973
target distance 10.0
model initialize at round 2256
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([24.54418095,  2.84438083,  3.62600589]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 9.108290756046907}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9278540002392909
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.32726519,  6.03059797,  2.74557682]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.6734302895027001}
episode index:2257
target Thresh 31.999999995961563
target distance 22.0
model initialize at round 2257
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 26.]), 'previousTarget': array([26., 26.]), 'currentState': array([4.9346935 , 7.39966626, 6.25536079]), 'targetState': array([26, 26], dtype=int32), 'currentDistance': 28.10194927399574}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.9278239755955362
{'scaleFactor': 20, 'currentTarget': array([26., 26.]), 'previousTarget': array([26., 26.]), 'currentState': array([26.50678957, 26.49280318,  0.50162643]), 'targetState': array([26, 26], dtype=int32), 'currentDistance': 0.706887997383879}
episode index:2258
target Thresh 31.999999996001744
target distance 4.0
model initialize at round 2258
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 20.]), 'previousTarget': array([20., 20.]), 'currentState': array([16.6167129 , 17.80735577,  0.71256113]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 4.031664709973541}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9278471168192656
{'scaleFactor': 20, 'currentTarget': array([20., 20.]), 'previousTarget': array([20., 20.]), 'currentState': array([19.90110678, 20.07160038,  0.74707733]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 0.12209210860409055}
episode index:2259
target Thresh 31.999999996041527
target distance 16.0
model initialize at round 2259
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  4.]), 'previousTarget': array([21.,  4.]), 'currentState': array([6.91799713, 5.79105362, 0.35901004]), 'targetState': array([21,  4], dtype=int32), 'currentDistance': 14.195445671770965}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9278489832931983
{'scaleFactor': 20, 'currentTarget': array([21.,  4.]), 'previousTarget': array([21.,  4.]), 'currentState': array([20.56604108,  4.11780547,  6.21774654]), 'targetState': array([21,  4], dtype=int32), 'currentDistance': 0.44966484284981667}
episode index:2260
target Thresh 31.999999996080916
target distance 14.0
model initialize at round 2260
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 16.]), 'previousTarget': array([19., 16.]), 'currentState': array([28.45691599,  2.84248672,  1.5342862 ]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 16.20350010575701}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9278467257572118
{'scaleFactor': 20, 'currentTarget': array([19., 16.]), 'previousTarget': array([19., 16.]), 'currentState': array([19.19798067, 15.36841532,  2.1644824 ]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 0.6618878761532975}
episode index:2261
target Thresh 31.99999999611991
target distance 7.0
model initialize at round 2261
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([14.65973617, 17.27881815,  1.17639774]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 5.388740204843664}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9278654933408735
{'scaleFactor': 20, 'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([20.15213775, 18.0914008 ,  0.47195598]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 0.17748239551238787}
episode index:2262
target Thresh 31.999999996158518
target distance 4.0
model initialize at round 2262
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 16.]), 'previousTarget': array([ 9., 16.]), 'currentState': array([10.54216309, 13.8985212 ,  2.18827605]), 'targetState': array([ 9, 16], dtype=int32), 'currentDistance': 2.6066223627414837}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9278929500384692
{'scaleFactor': 20, 'currentTarget': array([ 9., 16.]), 'previousTarget': array([ 9., 16.]), 'currentState': array([ 9.23810553, 15.41093986,  2.37832373]), 'targetState': array([ 9, 16], dtype=int32), 'currentDistance': 0.6353629642352924}
episode index:2263
target Thresh 31.999999996196742
target distance 7.0
model initialize at round 2263
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  7.]), 'previousTarget': array([17.,  7.]), 'currentState': array([21.67256882, 15.65081099,  2.77660066]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 9.83206133694206}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9279031519377013
{'scaleFactor': 20, 'currentTarget': array([17.,  7.]), 'previousTarget': array([17.,  7.]), 'currentState': array([17.34539565,  7.36434443,  4.34287059]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 0.5020408594971468}
episode index:2264
target Thresh 31.999999996234585
target distance 14.0
model initialize at round 2264
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([ 1.00545593, 10.64232913,  5.09015822]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 15.084216812038374}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9279008744730171
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.34478856,  9.30055226,  6.16712566]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.4573956821943525}
episode index:2265
target Thresh 31.999999996272052
target distance 25.0
model initialize at round 2265
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([24.32529176, 27.83344549,  4.25071955]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 25.626340563385483}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9278786415288539
{'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([18.2386519 ,  3.20254937,  4.32857486]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 0.3130191286899142}
episode index:2266
target Thresh 31.999999996309146
target distance 16.0
model initialize at round 2266
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 28.]), 'previousTarget': array([ 5., 28.]), 'currentState': array([ 8.58966131, 12.5525984 ,  1.34455317]), 'targetState': array([ 5, 28], dtype=int32), 'currentDistance': 15.85900011089907}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9278763768852275
{'scaleFactor': 20, 'currentTarget': array([ 5., 28.]), 'previousTarget': array([ 5., 28.]), 'currentState': array([ 4.94616008, 27.75603202,  1.69228736]), 'targetState': array([ 5, 28], dtype=int32), 'currentDistance': 0.24983817053870389}
episode index:2267
target Thresh 31.99999999634587
target distance 18.0
model initialize at round 2267
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([19.38084939,  5.5409364 ,  2.40786362]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 16.445701365606247}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9278741142386413
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.77942313, 6.65060159, 3.05313198]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.854154352634764}
episode index:2268
target Thresh 31.99999999638223
target distance 9.0
model initialize at round 2268
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 14.]), 'previousTarget': array([15., 14.]), 'currentState': array([10.29456646, 21.92459927,  4.58000163]), 'targetState': array([15, 14], dtype=int32), 'currentDistance': 9.216310453441492}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9278843019581925
{'scaleFactor': 20, 'currentTarget': array([15., 14.]), 'previousTarget': array([15., 14.]), 'currentState': array([15.55749431, 13.63557727,  5.25175223]), 'targetState': array([15, 14], dtype=int32), 'currentDistance': 0.6660359076396847}
episode index:2269
target Thresh 31.999999996418225
target distance 12.0
model initialize at round 2269
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 23.]), 'previousTarget': array([26., 23.]), 'currentState': array([25.05274743, 12.98745343,  1.73484769]), 'targetState': array([26, 23], dtype=int32), 'currentDistance': 10.057254908097157}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9278944807017793
{'scaleFactor': 20, 'currentTarget': array([26., 23.]), 'previousTarget': array([26., 23.]), 'currentState': array([25.75899007, 22.8995213 ,  1.51410486]), 'targetState': array([26, 23], dtype=int32), 'currentDistance': 0.2611163674711674}
episode index:2270
target Thresh 31.999999996453866
target distance 6.0
model initialize at round 2270
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([11.16887893, 11.21083035,  6.07621044]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 4.8357192034067165}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.927917468601074
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.14714961, 11.18686231,  0.15010372]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.8730815001295015}
episode index:2271
target Thresh 31.99999999648915
target distance 3.0
model initialize at round 2271
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  2.]), 'previousTarget': array([25.,  2.]), 'currentState': array([24.51362449,  3.10216165,  4.81609893]), 'targetState': array([25,  2], dtype=int32), 'currentDistance': 1.2047080316492442}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9279447936589079
{'scaleFactor': 20, 'currentTarget': array([25.,  2.]), 'previousTarget': array([25.,  2.]), 'currentState': array([25.04170474,  1.18269852,  5.14896083]), 'targetState': array([25,  2], dtype=int32), 'currentDistance': 0.8183648253168004}
episode index:2272
target Thresh 31.999999996524082
target distance 10.0
model initialize at round 2272
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  9.]), 'previousTarget': array([19.,  9.]), 'currentState': array([23.6893243 , 20.05570701,  3.4735266 ]), 'targetState': array([19,  9], dtype=int32), 'currentDistance': 12.009097380378906}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9279507485008534
{'scaleFactor': 20, 'currentTarget': array([19.,  9.]), 'previousTarget': array([19.,  9.]), 'currentState': array([19.34347688,  9.2009733 ,  4.44305184]), 'targetState': array([19,  9], dtype=int32), 'currentDistance': 0.3979530556121509}
episode index:2273
target Thresh 31.99999999655867
target distance 7.0
model initialize at round 2273
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 12.]), 'previousTarget': array([11., 12.]), 'currentState': array([10.44988894,  6.93417186,  1.55091876]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 5.095609571548983}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9279693713027439
{'scaleFactor': 20, 'currentTarget': array([11., 12.]), 'previousTarget': array([11., 12.]), 'currentState': array([10.72169612, 12.91520113,  1.45034055]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 0.9565804544788518}
episode index:2274
target Thresh 31.999999996592912
target distance 9.0
model initialize at round 2274
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 16.]), 'previousTarget': array([ 7., 16.]), 'currentState': array([14.57495797, 10.36407877,  2.66782242]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 9.441588659425348}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9279794902823473
{'scaleFactor': 20, 'currentTarget': array([ 7., 16.]), 'previousTarget': array([ 7., 16.]), 'currentState': array([ 6.29518037, 15.91028072,  2.62737095]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 0.7105070453190366}
episode index:2275
target Thresh 31.999999996626812
target distance 4.0
model initialize at round 2275
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 8.]), 'previousTarget': array([8., 8.]), 'currentState': array([ 3.3999793 , 11.47815534,  4.46686316]), 'targetState': array([8, 8], dtype=int32), 'currentDistance': 5.766953701552493}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9279980840915378
{'scaleFactor': 20, 'currentTarget': array([8., 8.]), 'previousTarget': array([8., 8.]), 'currentState': array([7.97285086, 8.52023438, 5.56436137]), 'targetState': array([8, 8], dtype=int32), 'currentDistance': 0.5209423065435127}
episode index:2276
target Thresh 31.999999996660375
target distance 11.0
model initialize at round 2276
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([12.12768194,  5.10282441,  3.71084237]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 9.194063263382002}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.928008181573228
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.32783996, 3.68424579, 3.28678457]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.7426303560055606}
episode index:2277
target Thresh 31.999999996693607
target distance 13.0
model initialize at round 2277
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([13.28323774, 16.34219431,  4.21941042]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 12.484009529837321}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9280140955187186
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.79661896, 10.95924132,  3.47651619]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.7976609776995279}
episode index:2278
target Thresh 31.999999996726505
target distance 11.0
model initialize at round 2278
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 16.]), 'previousTarget': array([16., 16.]), 'currentState': array([25.55966465, 24.64605669,  4.16303283]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 12.889588220509287}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9280200042742615
{'scaleFactor': 20, 'currentTarget': array([16., 16.]), 'previousTarget': array([16., 16.]), 'currentState': array([16.77253693, 16.5460051 ,  4.10716907]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 0.9460099786361215}
episode index:2279
target Thresh 31.999999996759076
target distance 15.0
model initialize at round 2279
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 14.]), 'previousTarget': array([21., 14.]), 'currentState': array([7.52194506, 5.16473044, 1.16200972]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 16.115829276775425}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9280176905418727
{'scaleFactor': 20, 'currentTarget': array([21., 14.]), 'previousTarget': array([21., 14.]), 'currentState': array([20.53647161, 13.99647459,  0.48080811]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 0.4635417938907757}
episode index:2280
target Thresh 31.999999996791324
target distance 17.0
model initialize at round 2280
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 27.]), 'previousTarget': array([ 7., 27.]), 'currentState': array([13.01584614, 11.73331454,  2.23128584]), 'targetState': array([ 7, 27], dtype=int32), 'currentDistance': 16.409207471571342}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9280153788381839
{'scaleFactor': 20, 'currentTarget': array([ 7., 27.]), 'previousTarget': array([ 7., 27.]), 'currentState': array([ 6.81580597, 26.42945   ,  2.02822026]), 'targetState': array([ 7, 27], dtype=int32), 'currentDistance': 0.5995454418612566}
episode index:2281
target Thresh 31.999999996823252
target distance 15.0
model initialize at round 2281
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([17.25356199, 20.33624081,  3.8536284 ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 13.525304454665845}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9280171535836129
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.19685804,  6.84284577,  4.60610739]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.25189390392952143}
episode index:2282
target Thresh 31.99999999685486
target distance 18.0
model initialize at round 2282
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  2.]), 'previousTarget': array([18.,  2.]), 'currentState': array([15.5632504 , 19.12356479,  4.69934177]), 'targetState': array([18,  2], dtype=int32), 'currentDistance': 17.296075271260218}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9280108023325835
{'scaleFactor': 20, 'currentTarget': array([18.,  2.]), 'previousTarget': array([18.,  2.]), 'currentState': array([18.23934519,  1.66579286,  4.65282102]), 'targetState': array([18,  2], dtype=int32), 'currentDistance': 0.4110724146569832}
episode index:2283
target Thresh 31.999999996886157
target distance 12.0
model initialize at round 2283
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 12.]), 'previousTarget': array([ 8., 12.]), 'currentState': array([ 5.28800996, 22.9167556 ,  4.57392672]), 'targetState': array([ 8, 12], dtype=int32), 'currentDistance': 11.248575143772635}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9280166995948725
{'scaleFactor': 20, 'currentTarget': array([ 8., 12.]), 'previousTarget': array([ 8., 12.]), 'currentState': array([ 8.44750152, 11.4162088 ,  4.89511157]), 'targetState': array([ 8, 12], dtype=int32), 'currentDistance': 0.7355744546691921}
episode index:2284
target Thresh 31.99999999691714
target distance 15.0
model initialize at round 2284
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 12.]), 'previousTarget': array([ 9., 12.]), 'currentState': array([15.80621317, 25.81371964,  4.93383694]), 'targetState': array([ 9, 12], dtype=int32), 'currentDistance': 15.39946064136854}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9280143923716047
{'scaleFactor': 20, 'currentTarget': array([ 9., 12.]), 'previousTarget': array([ 9., 12.]), 'currentState': array([ 9.08425558, 11.53059738,  4.16833425]), 'targetState': array([ 9, 12], dtype=int32), 'currentDistance': 0.4769044158200447}
episode index:2285
target Thresh 31.999999996947814
target distance 13.0
model initialize at round 2285
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  7.]), 'previousTarget': array([24.,  7.]), 'currentState': array([26.62310926, 21.64022604,  2.80665553]), 'targetState': array([24,  7], dtype=int32), 'currentDistance': 14.873362788637706}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9280120871669049
{'scaleFactor': 20, 'currentTarget': array([24.,  7.]), 'previousTarget': array([24.,  7.]), 'currentState': array([24.3706452 ,  6.67154051,  4.6999534 ]), 'targetState': array([24,  7], dtype=int32), 'currentDistance': 0.49524084915542127}
episode index:2286
target Thresh 31.999999996978183
target distance 7.0
model initialize at round 2286
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 16.]), 'previousTarget': array([27., 16.]), 'currentState': array([22.46541265, 21.63987776,  5.49014096]), 'targetState': array([27, 16], dtype=int32), 'currentDistance': 7.236760571001164}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9280263346189525
{'scaleFactor': 20, 'currentTarget': array([27., 16.]), 'previousTarget': array([27., 16.]), 'currentState': array([27.82227745, 15.71362779,  5.43066657]), 'targetState': array([27, 16], dtype=int32), 'currentDistance': 0.8707176580989149}
episode index:2287
target Thresh 31.999999997008253
target distance 12.0
model initialize at round 2287
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 17.]), 'previousTarget': array([24., 17.]), 'currentState': array([13.75438244, 20.37759275,  0.31705516]), 'targetState': array([24, 17], dtype=int32), 'currentDistance': 10.787993876461318}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9280363712077991
{'scaleFactor': 20, 'currentTarget': array([24., 17.]), 'previousTarget': array([24., 17.]), 'currentState': array([23.13201382, 17.45568721,  6.11044979]), 'targetState': array([24, 17], dtype=int32), 'currentDistance': 0.9803320049966899}
episode index:2288
target Thresh 31.99999999703802
target distance 18.0
model initialize at round 2288
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 22.]), 'previousTarget': array([25., 22.]), 'currentState': array([8.22426482, 8.38862895, 1.53070998]), 'targetState': array([25, 22], dtype=int32), 'currentDistance': 21.60311812765343}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9280220863161657
{'scaleFactor': 20, 'currentTarget': array([25., 22.]), 'previousTarget': array([25., 22.]), 'currentState': array([24.76887053, 22.24016478,  0.69445909]), 'targetState': array([25, 22], dtype=int32), 'currentDistance': 0.33331659716150874}
episode index:2289
target Thresh 31.99999999706749
target distance 15.0
model initialize at round 2289
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([19.15907171,  4.54218385,  3.17917228]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 15.174553332314959}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9280197817782231
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([4.22003043, 9.88635101, 2.78966423]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.7882059491416774}
episode index:2290
target Thresh 31.99999999709667
target distance 13.0
model initialize at round 2290
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  4.]), 'previousTarget': array([10.,  4.]), 'currentState': array([13.43249196, 16.38737726,  4.52416539]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 12.854147828161897}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.928025657102371
{'scaleFactor': 20, 'currentTarget': array([10.,  4.]), 'previousTarget': array([10.,  4.]), 'currentState': array([10.34043495,  4.83973087,  4.59622863]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 0.906114718098329}
episode index:2291
target Thresh 31.99999999712556
target distance 2.0
model initialize at round 2291
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 12.]), 'previousTarget': array([23., 12.]), 'currentState': array([22.62978247, 11.37494458,  1.92808759]), 'targetState': array([23, 12], dtype=int32), 'currentDistance': 0.7264676802291299}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9280570595207381
{'scaleFactor': 20, 'currentTarget': array([23., 12.]), 'previousTarget': array([23., 12.]), 'currentState': array([22.62978247, 11.37494458,  1.92808759]), 'targetState': array([23, 12], dtype=int32), 'currentDistance': 0.7264676802291299}
episode index:2292
target Thresh 31.99999999715416
target distance 12.0
model initialize at round 2292
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([15.80068708, 11.3288739 ,  3.58368111]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 12.028294528669454}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9280629134631193
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.11646418, 8.71894842, 3.41822329]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.30422671566264387}
episode index:2293
target Thresh 31.999999997182478
target distance 19.0
model initialize at round 2293
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 28.]), 'previousTarget': array([23., 28.]), 'currentState': array([27.43068903,  9.88629396,  1.56462544]), 'targetState': array([23, 28], dtype=int32), 'currentDistance': 18.647717070289872}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9280565727194491
{'scaleFactor': 20, 'currentTarget': array([23., 28.]), 'previousTarget': array([23., 28.]), 'currentState': array([22.98143087, 27.06636945,  1.95241791]), 'targetState': array([23, 28], dtype=int32), 'currentDistance': 0.933815191651112}
episode index:2294
target Thresh 31.999999997210512
target distance 15.0
model initialize at round 2294
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 2.]), 'previousTarget': array([7., 2.]), 'currentState': array([23.0918493 ,  3.2807237 ,  1.87483805]), 'targetState': array([7, 2], dtype=int32), 'currentDistance': 16.142734184035646}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9280542581755311
{'scaleFactor': 20, 'currentTarget': array([7., 2.]), 'previousTarget': array([7., 2.]), 'currentState': array([7.79468561, 1.82354329, 3.2981531 ]), 'targetState': array([7, 2], dtype=int32), 'currentDistance': 0.8140406578953634}
episode index:2295
target Thresh 31.999999997238266
target distance 9.0
model initialize at round 2295
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 13.]), 'previousTarget': array([11., 13.]), 'currentState': array([16.98720504, 20.27991386,  4.07107835]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 9.425697323811244}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9280642476318571
{'scaleFactor': 20, 'currentTarget': array([11., 13.]), 'previousTarget': array([11., 13.]), 'currentState': array([10.99019314, 12.29468841,  4.0680021 ]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 0.7053797665234014}
episode index:2296
target Thresh 31.999999997265746
target distance 11.0
model initialize at round 2296
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 28.]), 'previousTarget': array([ 6., 28.]), 'currentState': array([15.15564503, 26.41752499,  2.99630409]), 'targetState': array([ 6, 28], dtype=int32), 'currentDistance': 9.291397259792891}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9280742283903545
{'scaleFactor': 20, 'currentTarget': array([ 6., 28.]), 'previousTarget': array([ 6., 28.]), 'currentState': array([ 5.28382489, 27.90812485,  2.91167482]), 'targetState': array([ 6, 28], dtype=int32), 'currentDistance': 0.7220442061218565}
episode index:2297
target Thresh 31.999999997292953
target distance 9.0
model initialize at round 2297
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 11.]), 'previousTarget': array([24., 11.]), 'currentState': array([16.68273899,  4.02788477,  1.02656954]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 10.1070618611419}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9280842004623778
{'scaleFactor': 20, 'currentTarget': array([24., 11.]), 'previousTarget': array([24., 11.]), 'currentState': array([23.70886816, 11.10566414,  0.8563951 ]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 0.30971383182223877}
episode index:2298
target Thresh 31.99999999731989
target distance 13.0
model initialize at round 2298
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 14.]), 'previousTarget': array([27., 14.]), 'currentState': array([14.45739167, 12.3803763 ,  5.9976275 ]), 'targetState': array([27, 14], dtype=int32), 'currentDistance': 12.646746798187351}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9280900273214203
{'scaleFactor': 20, 'currentTarget': array([27., 14.]), 'previousTarget': array([27., 14.]), 'currentState': array([26.17584811, 14.25046689,  0.20739917]), 'targetState': array([27, 14], dtype=int32), 'currentDistance': 0.8613710036308492}
episode index:2299
target Thresh 31.999999997346556
target distance 21.0
model initialize at round 2299
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 10.]), 'previousTarget': array([23., 10.]), 'currentState': array([ 3.78549008, 10.05651498,  0.85372799]), 'targetState': array([23, 10], dtype=int32), 'currentDistance': 19.2145930310688}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.928079719516067
{'scaleFactor': 20, 'currentTarget': array([23., 10.]), 'previousTarget': array([23., 10.]), 'currentState': array([23.53229492, 10.42347197,  6.23287648]), 'targetState': array([23, 10], dtype=int32), 'currentDistance': 0.6801958543836404}
episode index:2300
target Thresh 31.99999999737296
target distance 8.0
model initialize at round 2300
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  7.]), 'previousTarget': array([11.,  7.]), 'currentState': array([3.78334912, 7.51045234, 6.20653915]), 'targetState': array([11,  7], dtype=int32), 'currentDistance': 7.2346811639023825}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9280938508895932
{'scaleFactor': 20, 'currentTarget': array([11.,  7.]), 'previousTarget': array([11.,  7.]), 'currentState': array([11.77418449,  7.31124048,  6.1791506 ]), 'targetState': array([11,  7], dtype=int32), 'currentDistance': 0.8344053299507466}
episode index:2301
target Thresh 31.999999997399097
target distance 14.0
model initialize at round 2301
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([23.99180721, 17.34756646,  3.22311759]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 15.347568651429974}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9280915271900008
{'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([24.37052218,  2.05018825,  4.79474626]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 0.3739058030373916}
episode index:2302
target Thresh 31.99999999742498
target distance 18.0
model initialize at round 2302
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 29.]), 'previousTarget': array([26., 29.]), 'currentState': array([16.60815041,  9.4307514 ,  6.09211016]), 'targetState': array([26, 29], dtype=int32), 'currentDistance': 21.706273963953418}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.928077305186991
{'scaleFactor': 20, 'currentTarget': array([26., 29.]), 'previousTarget': array([26., 29.]), 'currentState': array([25.46914639, 28.71232405,  1.08289467]), 'targetState': array([26, 29], dtype=int32), 'currentDistance': 0.6037905332123454}
episode index:2303
target Thresh 31.9999999974506
target distance 18.0
model initialize at round 2303
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 11.]), 'previousTarget': array([25., 11.]), 'currentState': array([8.27895847, 3.90608349, 0.30242651]), 'targetState': array([25, 11], dtype=int32), 'currentDistance': 18.16361421638136}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9280709857175017
{'scaleFactor': 20, 'currentTarget': array([25., 11.]), 'previousTarget': array([25., 11.]), 'currentState': array([24.47379031, 11.07659304,  0.36970459]), 'targetState': array([25, 11], dtype=int32), 'currentDistance': 0.5317547657704919}
episode index:2304
target Thresh 31.999999997475967
target distance 13.0
model initialize at round 2304
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 20.]), 'previousTarget': array([13., 20.]), 'currentState': array([24.45454472, 20.66630028,  3.74453688]), 'targetState': array([13, 20], dtype=int32), 'currentDistance': 11.473907389604719}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9280768031420931
{'scaleFactor': 20, 'currentTarget': array([13., 20.]), 'previousTarget': array([13., 20.]), 'currentState': array([12.61509326, 19.42942942,  3.23316688]), 'targetState': array([13, 20], dtype=int32), 'currentDistance': 0.6882615685576781}
episode index:2305
target Thresh 31.99999999750108
target distance 10.0
model initialize at round 2305
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 26.]), 'previousTarget': array([ 9., 26.]), 'currentState': array([17.22480143, 26.16353275,  3.24172962]), 'targetState': array([ 9, 26], dtype=int32), 'currentDistance': 8.226427018869433}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9280909051398631
{'scaleFactor': 20, 'currentTarget': array([ 9., 26.]), 'previousTarget': array([ 9., 26.]), 'currentState': array([ 9.24611404, 25.71496047,  3.31457671]), 'targetState': array([ 9, 26], dtype=int32), 'currentDistance': 0.3765895029119544}
episode index:2306
target Thresh 31.999999997525947
target distance 9.0
model initialize at round 2306
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([15.64634974, 21.00000953,  3.51533031]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 12.007254290820317}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9280967088868337
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.40849457, 11.02442618,  4.27670788]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.40922420511375407}
episode index:2307
target Thresh 31.999999997550564
target distance 5.0
model initialize at round 2307
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  9.]), 'previousTarget': array([20.,  9.]), 'currentState': array([18.28571846, 12.48376069,  5.22370228]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 3.882698766113066}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9281192406420821
{'scaleFactor': 20, 'currentTarget': array([20.,  9.]), 'previousTarget': array([20.,  9.]), 'currentState': array([20.41510341,  9.1009319 ,  5.35062096]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 0.4271979475443638}
episode index:2308
target Thresh 31.999999997574935
target distance 5.0
model initialize at round 2308
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  4.]), 'previousTarget': array([20.,  4.]), 'currentState': array([16.62628224,  3.96628047,  6.19224298]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 3.373886269264925}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9281417528808686
{'scaleFactor': 20, 'currentTarget': array([20.,  4.]), 'previousTarget': array([20.,  4.]), 'currentState': array([20.57112205,  4.51078363,  0.11804509]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 0.7662116624433597}
episode index:2309
target Thresh 31.999999997599065
target distance 16.0
model initialize at round 2309
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  6.]), 'previousTarget': array([27.,  6.]), 'currentState': array([11.76674911, 20.33327204,  5.58334661]), 'targetState': array([27,  6], dtype=int32), 'currentDistance': 20.916372050692523}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9281275522321144
{'scaleFactor': 20, 'currentTarget': array([27.,  6.]), 'previousTarget': array([27.,  6.]), 'currentState': array([27.68608786,  5.40947617,  5.19972258]), 'targetState': array([27,  6], dtype=int32), 'currentDistance': 0.9052264585203146}
episode index:2310
target Thresh 31.999999997622954
target distance 17.0
model initialize at round 2310
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'previousTarget': array([11., 19.]), 'currentState': array([4.63941213, 2.38041549, 1.23800867]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 17.795158536025234}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9281212301616909
{'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'previousTarget': array([11., 19.]), 'currentState': array([10.60452923, 19.31794672,  1.21924281]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 0.50743200709689}
episode index:2311
target Thresh 31.999999997646608
target distance 15.0
model initialize at round 2311
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 23.]), 'previousTarget': array([ 4., 23.]), 'currentState': array([12.49404472,  9.26577282,  2.12526351]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 16.14861578997096}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9281189046704565
{'scaleFactor': 20, 'currentTarget': array([ 4., 23.]), 'previousTarget': array([ 4., 23.]), 'currentState': array([ 3.73472397, 22.6268024 ,  2.23199208]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 0.4578731501873016}
episode index:2312
target Thresh 31.999999997670024
target distance 20.0
model initialize at round 2312
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 14.]), 'previousTarget': array([ 5., 14.]), 'currentState': array([23.31667243, 10.0100421 ,  2.12627709]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 18.74620636056795}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9281086423143556
{'scaleFactor': 20, 'currentTarget': array([ 5., 14.]), 'previousTarget': array([ 5., 14.]), 'currentState': array([ 4.64530492, 14.55059692,  1.99741699]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 0.6549546313598696}
episode index:2313
target Thresh 31.99999999769321
target distance 1.0
model initialize at round 2313
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  7.]), 'previousTarget': array([27.,  7.]), 'currentState': array([27.04940842,  6.22256313,  5.57856715]), 'targetState': array([27,  7], dtype=int32), 'currentDistance': 0.7790053138402747}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9281397103168126
{'scaleFactor': 20, 'currentTarget': array([27.,  7.]), 'previousTarget': array([27.,  7.]), 'currentState': array([27.04940842,  6.22256313,  5.57856715]), 'targetState': array([27,  7], dtype=int32), 'currentDistance': 0.7790053138402747}
episode index:2314
target Thresh 31.99999999771616
target distance 13.0
model initialize at round 2314
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 23.]), 'previousTarget': array([ 4., 23.]), 'currentState': array([ 8.58184014, 10.57460408,  1.35843008]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 13.243251972488675}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9281414060565926
{'scaleFactor': 20, 'currentTarget': array([ 4., 23.]), 'previousTarget': array([ 4., 23.]), 'currentState': array([ 3.49044254, 23.31170901,  1.82672713]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 0.5973368442206937}
episode index:2315
target Thresh 31.999999997738886
target distance 17.0
model initialize at round 2315
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 13.]), 'previousTarget': array([23., 13.]), 'currentState': array([ 4.69245746, 23.9404149 ,  4.8326211 ]), 'targetState': array([23, 13], dtype=int32), 'currentDistance': 21.327418785037455}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9281272423468352
{'scaleFactor': 20, 'currentTarget': array([23., 13.]), 'previousTarget': array([23., 13.]), 'currentState': array([22.94101849, 13.37479958,  5.5793243 ]), 'targetState': array([23, 13], dtype=int32), 'currentDistance': 0.3794121075409132}
episode index:2316
target Thresh 31.999999997761382
target distance 18.0
model initialize at round 2316
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 10.]), 'previousTarget': array([23., 10.]), 'currentState': array([ 4.79328394, 26.82687408,  4.92287731]), 'targetState': array([23, 10], dtype=int32), 'currentDistance': 24.79169621255786}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9281054010782346
{'scaleFactor': 20, 'currentTarget': array([23., 10.]), 'previousTarget': array([23., 10.]), 'currentState': array([23.63298678,  9.90655692,  5.38949998]), 'targetState': array([23, 10], dtype=int32), 'currentDistance': 0.6398467550429686}
episode index:2317
target Thresh 31.999999997783657
target distance 15.0
model initialize at round 2317
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 21.]), 'previousTarget': array([ 3., 21.]), 'currentState': array([15.00177367,  7.68296908,  2.57974243]), 'targetState': array([ 3, 21], dtype=int32), 'currentDistance': 17.927238597211666}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9280991076556311
{'scaleFactor': 20, 'currentTarget': array([ 3., 21.]), 'previousTarget': array([ 3., 21.]), 'currentState': array([ 2.69050445, 20.74089044,  2.32634927]), 'targetState': array([ 3, 21], dtype=int32), 'currentDistance': 0.40364001404076666}
episode index:2318
target Thresh 31.99999999780571
target distance 5.0
model initialize at round 2318
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([13.44898899, 19.39274524,  3.39943278]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 4.730409170726465}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9281215314988155
{'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([ 9.65787958, 20.35854795,  2.82186949]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 0.918839632188173}
episode index:2319
target Thresh 31.999999997827544
target distance 12.0
model initialize at round 2319
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'previousTarget': array([16., 23.]), 'currentState': array([ 5.00417649, 19.35056198,  6.20463747]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 11.585617485973254}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9281272895237732
{'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'previousTarget': array([16., 23.]), 'currentState': array([16.17914326, 23.53678496,  0.32982468]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 0.5658890375134411}
episode index:2320
target Thresh 31.999999997849162
target distance 11.0
model initialize at round 2320
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([ 2.32865007, 19.8025728 ,  4.26917219]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 12.798920959622258}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9281289862313922
{'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([15.18080543, 18.31497505,  6.2332173 ]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 0.3631802402407493}
episode index:2321
target Thresh 31.99999999787056
target distance 22.0
model initialize at round 2321
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 4.]), 'previousTarget': array([5., 4.]), 'currentState': array([27.40070197, 22.36542788,  3.94279003]), 'targetState': array([5, 4], dtype=int32), 'currentDistance': 28.96688437004899}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.9280996707139114
{'scaleFactor': 20, 'currentTarget': array([5., 4.]), 'previousTarget': array([5., 4.]), 'currentState': array([4.42541764, 3.48481449, 3.54589098]), 'targetState': array([5, 4], dtype=int32), 'currentDistance': 0.7717259855068828}
episode index:2322
target Thresh 31.99999999789175
target distance 22.0
model initialize at round 2322
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 6.]), 'previousTarget': array([9., 6.]), 'currentState': array([21.28040913, 26.34055467,  3.86978531]), 'targetState': array([9, 6], dtype=int32), 'currentDistance': 23.760189657082446}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9280817134177436
{'scaleFactor': 20, 'currentTarget': array([9., 6.]), 'previousTarget': array([9., 6.]), 'currentState': array([9.20288345, 6.04072872, 4.01492722]), 'targetState': array([9, 6], dtype=int32), 'currentDistance': 0.2069312040718589}
episode index:2323
target Thresh 31.999999997912727
target distance 19.0
model initialize at round 2323
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 24.]), 'previousTarget': array([11., 24.]), 'currentState': array([18.74639465,  6.50840416,  2.12129804]), 'targetState': array([11, 24], dtype=int32), 'currentDistance': 19.130147809891223}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9280715156387381
{'scaleFactor': 20, 'currentTarget': array([11., 24.]), 'previousTarget': array([11., 24.]), 'currentState': array([10.57199531, 24.6306314 ,  1.72591408]), 'targetState': array([11, 24], dtype=int32), 'currentDistance': 0.7621574513154422}
episode index:2324
target Thresh 31.999999997933497
target distance 4.0
model initialize at round 2324
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 23.]), 'previousTarget': array([13., 23.]), 'currentState': array([10.63582401, 21.39556251,  1.24725705]), 'targetState': array([13, 23], dtype=int32), 'currentDistance': 2.857192287234967}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9280938934814742
{'scaleFactor': 20, 'currentTarget': array([13., 23.]), 'previousTarget': array([13., 23.]), 'currentState': array([13.62602654, 22.56543453,  5.31037444]), 'targetState': array([13, 23], dtype=int32), 'currentDistance': 0.762073738070769}
episode index:2325
target Thresh 31.99999999795406
target distance 2.0
model initialize at round 2325
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  6.]), 'previousTarget': array([18.,  6.]), 'currentState': array([16.71502818,  9.08684659,  3.44953406]), 'targetState': array([18,  6], dtype=int32), 'currentDistance': 3.343616970975513}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9281162520827291
{'scaleFactor': 20, 'currentTarget': array([18.,  6.]), 'previousTarget': array([18.,  6.]), 'currentState': array([17.80018348,  6.19967263,  5.74244982]), 'targetState': array([18,  6], dtype=int32), 'currentDistance': 0.282481506083802}
episode index:2326
target Thresh 31.999999997974417
target distance 12.0
model initialize at round 2326
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 16.]), 'previousTarget': array([10., 16.]), 'currentState': array([18.53960695,  5.61877309,  2.85788846]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 13.442275067602317}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9281179491587173
{'scaleFactor': 20, 'currentTarget': array([10., 16.]), 'previousTarget': array([10., 16.]), 'currentState': array([ 9.61508625, 16.15844904,  2.09765244]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 0.41625075737553957}
episode index:2327
target Thresh 31.99999999799457
target distance 12.0
model initialize at round 2327
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([ 3.9092874 , 10.13527812,  0.58691614]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 10.127695921418722}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.928127773944259
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.73465218, 11.10028102,  0.06902601]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.2836648529339222}
episode index:2328
target Thresh 31.999999998014523
target distance 15.0
model initialize at round 2328
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 16.]), 'previousTarget': array([18., 16.]), 'currentState': array([1.93444937, 3.69731434, 5.03678966]), 'targetState': array([18, 16], dtype=int32), 'currentDistance': 20.23506836250933}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9281136951466268
{'scaleFactor': 20, 'currentTarget': array([18., 16.]), 'previousTarget': array([18., 16.]), 'currentState': array([18.28253013, 16.57432433,  0.51314602]), 'targetState': array([18, 16], dtype=int32), 'currentDistance': 0.6400560188345368}
episode index:2329
target Thresh 31.99999999803428
target distance 13.0
model initialize at round 2329
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 25.]), 'previousTarget': array([14., 25.]), 'currentState': array([ 8.64480135, 11.64360611,  0.79661959]), 'targetState': array([14, 25], dtype=int32), 'currentDistance': 14.389976036447012}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.928115391134936
{'scaleFactor': 20, 'currentTarget': array([14., 25.]), 'previousTarget': array([14., 25.]), 'currentState': array([13.65885841, 24.62370324,  1.31677378]), 'targetState': array([14, 25], dtype=int32), 'currentDistance': 0.5079142019636628}
episode index:2330
target Thresh 31.99999999805384
target distance 11.0
model initialize at round 2330
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([5.4243319 , 8.10352539, 0.44824284]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 10.850778358479996}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9281252043733595
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.16833086,  3.82310568,  5.89667282]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.1701181608173006}
episode index:2331
target Thresh 31.999999998073203
target distance 11.0
model initialize at round 2331
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 15.]), 'previousTarget': array([22., 15.]), 'currentState': array([24.08745367, 24.00818167,  4.61775476]), 'targetState': array([22, 15], dtype=int32), 'currentDistance': 9.246880547858522}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9281350091956266
{'scaleFactor': 20, 'currentTarget': array([22., 15.]), 'previousTarget': array([22., 15.]), 'currentState': array([22.13374849, 14.22683672,  4.38655354]), 'targetState': array([22, 15], dtype=int32), 'currentDistance': 0.784646487054354}
episode index:2332
target Thresh 31.999999998092377
target distance 4.0
model initialize at round 2332
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 27.]), 'previousTarget': array([15., 27.]), 'currentState': array([13.87428114, 24.25105761,  1.29350853]), 'targetState': array([15, 27], dtype=int32), 'currentDistance': 2.9705095930579}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9281615265513078
{'scaleFactor': 20, 'currentTarget': array([15., 27.]), 'previousTarget': array([15., 27.]), 'currentState': array([14.41589911, 26.17632052,  1.29967828]), 'targetState': array([15, 27], dtype=int32), 'currentDistance': 1.0097632068238336}
episode index:2333
target Thresh 31.999999998111356
target distance 11.0
model initialize at round 2333
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  5.]), 'previousTarget': array([21.,  5.]), 'currentState': array([11.67657969, 11.1465218 ,  5.36035711]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 11.167179435111153}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9281713074096406
{'scaleFactor': 20, 'currentTarget': array([21.,  5.]), 'previousTarget': array([21.,  5.]), 'currentState': array([20.1078822 ,  5.88029476,  5.85521955]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 1.2533128193081282}
episode index:2334
target Thresh 31.99999999813015
target distance 15.0
model initialize at round 2334
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 13.]), 'previousTarget': array([12., 13.]), 'currentState': array([26.66981872, 15.65026313,  2.77826685]), 'targetState': array([12, 13], dtype=int32), 'currentDistance': 14.907296063192803}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9281689833783849
{'scaleFactor': 20, 'currentTarget': array([12., 13.]), 'previousTarget': array([12., 13.]), 'currentState': array([11.25252014, 12.73357809,  3.10101271]), 'targetState': array([12, 13], dtype=int32), 'currentDistance': 0.7935406607469976}
episode index:2335
target Thresh 31.999999998148756
target distance 24.0
model initialize at round 2335
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  6.]), 'previousTarget': array([27.,  6.]), 'currentState': array([ 4.97530746, 15.71330317,  6.02843732]), 'targetState': array([27,  6], dtype=int32), 'currentDistance': 24.071463189135326}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9281510963442829
{'scaleFactor': 20, 'currentTarget': array([27.,  6.]), 'previousTarget': array([27.,  6.]), 'currentState': array([26.77435573,  6.43368052,  5.86094185]), 'targetState': array([27,  6], dtype=int32), 'currentDistance': 0.4888702622744597}
episode index:2336
target Thresh 31.999999998167176
target distance 8.0
model initialize at round 2336
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 15.]), 'previousTarget': array([14., 15.]), 'currentState': array([19.18590366, 21.18074695,  4.14619398]), 'targetState': array([14, 15], dtype=int32), 'currentDistance': 8.06816147719949}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9281649794909049
{'scaleFactor': 20, 'currentTarget': array([14., 15.]), 'previousTarget': array([14., 15.]), 'currentState': array([14.25179947, 14.90151427,  4.11524244]), 'targetState': array([14, 15], dtype=int32), 'currentDistance': 0.2703745752720936}
episode index:2337
target Thresh 31.999999998185412
target distance 18.0
model initialize at round 2337
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([19.70951593, 22.91970422,  4.84856415]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 23.779914083656287}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9281471094704709
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.33290169, 6.07625357, 3.82732389]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.3415232661028793}
episode index:2338
target Thresh 31.999999998203467
target distance 6.0
model initialize at round 2338
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 16.]), 'previousTarget': array([16., 16.]), 'currentState': array([11.66482727, 11.81579385,  0.7670036 ]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 6.025056325582697}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9281651308003253
{'scaleFactor': 20, 'currentTarget': array([16., 16.]), 'previousTarget': array([16., 16.]), 'currentState': array([15.80562606, 16.14102529,  0.91574348]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 0.2401444578563942}
episode index:2339
target Thresh 31.999999998221345
target distance 6.0
model initialize at round 2339
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 28.]), 'previousTarget': array([ 8., 28.]), 'currentState': array([ 3.59922433, 24.19474445,  0.74908103]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 5.817799957091555}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9281831367273338
{'scaleFactor': 20, 'currentTarget': array([ 8., 28.]), 'previousTarget': array([ 8., 28.]), 'currentState': array([ 7.92567641, 28.34256368,  0.79291108]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 0.35053369925463235}
episode index:2340
target Thresh 31.99999999823904
target distance 1.0
model initialize at round 2340
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.47867508, 7.17976383, 2.70465839]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.5514478273880412}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9282138145843489
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.47867508, 7.17976383, 2.70465839]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.5514478273880412}
episode index:2341
target Thresh 31.999999998256563
target distance 15.0
model initialize at round 2341
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 18.]), 'previousTarget': array([17., 18.]), 'currentState': array([2.1108264 , 1.57109859, 5.16574919]), 'targetState': array([17, 18], dtype=int32), 'currentDistance': 22.171970863941524}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9281959542329962
{'scaleFactor': 20, 'currentTarget': array([17., 18.]), 'previousTarget': array([17., 18.]), 'currentState': array([17.1264083 , 18.60015897,  0.74996537]), 'targetState': array([17, 18], dtype=int32), 'currentDistance': 0.6133268639073632}
episode index:2342
target Thresh 31.99999999827391
target distance 26.0
model initialize at round 2342
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([11.01976324, 27.66116804,  4.35332428]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 25.159013635900205}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.928174326007971
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.40364372,  2.8466696 ,  4.85516845]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.431785202184356}
episode index:2343
target Thresh 31.999999998291084
target distance 14.0
model initialize at round 2343
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 19.]), 'previousTarget': array([27., 19.]), 'currentState': array([14.90993993, 26.68935   ,  5.67385444]), 'targetState': array([27, 19], dtype=int32), 'currentDistance': 14.328142092896943}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9281759860002489
{'scaleFactor': 20, 'currentTarget': array([27., 19.]), 'previousTarget': array([27., 19.]), 'currentState': array([26.71886493, 19.33013146,  5.71783573]), 'targetState': array([27, 19], dtype=int32), 'currentDistance': 0.4336170103051994}
episode index:2344
target Thresh 31.99999999830809
target distance 17.0
model initialize at round 2344
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 27.]), 'previousTarget': array([10., 27.]), 'currentState': array([27.91275785, 17.41395233,  2.00756529]), 'targetState': array([10, 27], dtype=int32), 'currentDistance': 20.316476169564027}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9281658393431097
{'scaleFactor': 20, 'currentTarget': array([10., 27.]), 'previousTarget': array([10., 27.]), 'currentState': array([10.57488406, 26.29740032,  2.64172478]), 'targetState': array([10, 27], dtype=int32), 'currentDistance': 0.9078204617580572}
episode index:2345
target Thresh 31.999999998324924
target distance 15.0
model initialize at round 2345
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 29.]), 'previousTarget': array([23., 29.]), 'currentState': array([ 7.51986043, 18.38697363,  5.43307686]), 'targetState': array([23, 29], dtype=int32), 'currentDistance': 18.768885152925733}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9281557013361471
{'scaleFactor': 20, 'currentTarget': array([23., 29.]), 'previousTarget': array([23., 29.]), 'currentState': array([23.26394891, 29.59682511,  0.53175309]), 'targetState': array([23, 29], dtype=int32), 'currentDistance': 0.6525865710168433}
episode index:2346
target Thresh 31.999999998341593
target distance 9.0
model initialize at round 2346
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 18.]), 'previousTarget': array([18., 18.]), 'currentState': array([28.4299498 , 15.88748614,  1.56545895]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 10.641737057969346}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9281613785615688
{'scaleFactor': 20, 'currentTarget': array([18., 18.]), 'previousTarget': array([18., 18.]), 'currentState': array([17.19154439, 17.78779763,  2.92993528]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 0.8358410806455557}
episode index:2347
target Thresh 31.999999998358092
target distance 18.0
model initialize at round 2347
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'previousTarget': array([ 5., 22.]), 'currentState': array([7.67060633, 5.54359167, 2.1709526 ]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 16.67169797521351}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9281590716262478
{'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'previousTarget': array([ 5., 22.]), 'currentState': array([ 4.8170923 , 21.11577368,  1.76627338]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 0.902945960121511}
episode index:2348
target Thresh 31.99999999837443
target distance 6.0
model initialize at round 2348
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 23.]), 'previousTarget': array([ 8., 23.]), 'currentState': array([ 3.61926282, 22.0875005 ,  0.20094651]), 'targetState': array([ 8, 23], dtype=int32), 'currentDistance': 4.474764084464303}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9281811835582929
{'scaleFactor': 20, 'currentTarget': array([ 8., 23.]), 'previousTarget': array([ 8., 23.]), 'currentState': array([ 7.49462368, 23.05085826,  0.38008412]), 'targetState': array([ 8, 23], dtype=int32), 'currentDistance': 0.5079289196873006}
episode index:2349
target Thresh 31.999999998390606
target distance 14.0
model initialize at round 2349
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 23.]), 'previousTarget': array([23., 23.]), 'currentState': array([10.54219974, 21.3261988 ,  0.59808558]), 'targetState': array([23, 23], dtype=int32), 'currentDistance': 12.569741361142128}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.928186842692694
{'scaleFactor': 20, 'currentTarget': array([23., 23.]), 'previousTarget': array([23., 23.]), 'currentState': array([22.23685567, 23.17785276,  0.16355962]), 'targetState': array([23, 23], dtype=int32), 'currentDistance': 0.7835948408292862}
episode index:2350
target Thresh 31.99999999840662
target distance 22.0
model initialize at round 2350
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([14.9752012, 27.6827873,  2.595532 ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 27.004263748724682}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9281615593962568
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.17874829, 3.74142432, 4.17969113]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.31434429120020885}
episode index:2351
target Thresh 31.999999998422474
target distance 7.0
model initialize at round 2351
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 28.]), 'previousTarget': array([10., 28.]), 'currentState': array([11.33436249, 21.24091447,  1.9879508 ]), 'targetState': array([10, 28], dtype=int32), 'currentDistance': 6.889539933875639}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9281794749747447
{'scaleFactor': 20, 'currentTarget': array([10., 28.]), 'previousTarget': array([10., 28.]), 'currentState': array([10.09431082, 27.0770703 ,  1.90700586]), 'targetState': array([10, 28], dtype=int32), 'currentDistance': 0.9277358276594683}
episode index:2352
target Thresh 31.99999999843817
target distance 24.0
model initialize at round 2352
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 26.]), 'previousTarget': array([14., 26.]), 'currentState': array([4.22476454, 3.15427029, 1.76577538]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 24.849197056207572}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9281579456708877
{'scaleFactor': 20, 'currentTarget': array([14., 26.]), 'previousTarget': array([14., 26.]), 'currentState': array([14.2281401 , 26.68526715,  0.81970469]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 0.7222457811593428}
episode index:2353
target Thresh 31.99999999845371
target distance 13.0
model initialize at round 2353
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 14.]), 'previousTarget': array([21., 14.]), 'currentState': array([9.42526379, 6.89499905, 1.57071417]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 13.581441633667907}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9281596055698836
{'scaleFactor': 20, 'currentTarget': array([21., 14.]), 'previousTarget': array([21., 14.]), 'currentState': array([20.93932298, 14.24068117,  0.52035492]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 0.2482118535992998}
episode index:2354
target Thresh 31.999999998469097
target distance 14.0
model initialize at round 2354
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 25.]), 'previousTarget': array([25., 25.]), 'currentState': array([24.53577057, 11.68832902,  1.43135422]), 'targetState': array([25, 25], dtype=int32), 'currentDistance': 13.319763257501988}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9281612640591989
{'scaleFactor': 20, 'currentTarget': array([25., 25.]), 'previousTarget': array([25., 25.]), 'currentState': array([24.80671802, 25.6131328 ,  1.42882181]), 'targetState': array([25, 25], dtype=int32), 'currentDistance': 0.6428761529497284}
episode index:2355
target Thresh 31.99999999848433
target distance 12.0
model initialize at round 2355
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  4.]), 'previousTarget': array([20.,  4.]), 'currentState': array([9.87663731, 8.25151934, 5.84061194]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 10.979885643882724}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9281709536966525
{'scaleFactor': 20, 'currentTarget': array([20.,  4.]), 'previousTarget': array([20.,  4.]), 'currentState': array([19.16205129,  4.61032832,  5.96513781]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 1.036657468451892}
episode index:2356
target Thresh 31.999999998499412
target distance 14.0
model initialize at round 2356
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([24.69471135, 21.65504892,  2.76320493]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 20.15530334787315}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9281608608333993
{'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([12.64083041,  6.49588298,  4.13662087]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 0.8102860940088638}
episode index:2357
target Thresh 31.99999999851434
target distance 12.0
model initialize at round 2357
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 19.]), 'previousTarget': array([10., 19.]), 'currentState': array([11.68291418,  6.98629007,  1.00185364]), 'targetState': array([10, 19], dtype=int32), 'currentDistance': 12.131010935669556}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.928166509386651
{'scaleFactor': 20, 'currentTarget': array([10., 19.]), 'previousTarget': array([10., 19.]), 'currentState': array([10.03218669, 18.56620749,  1.79924438]), 'targetState': array([10, 19], dtype=int32), 'currentDistance': 0.43498496976417683}
episode index:2358
target Thresh 31.999999998529123
target distance 14.0
model initialize at round 2358
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 18.]), 'previousTarget': array([18., 18.]), 'currentState': array([ 3.76762298, 25.33314997,  5.58387089]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 16.01048544256374}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9281642110335527
{'scaleFactor': 20, 'currentTarget': array([18., 18.]), 'previousTarget': array([18., 18.]), 'currentState': array([18.06382638, 18.23825246,  5.85825607]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 0.2466536880706084}
episode index:2359
target Thresh 31.99999999854376
target distance 13.0
model initialize at round 2359
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([19.47013971, 14.70136695,  3.72173262]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 11.491563021094079}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9281698533803185
{'scaleFactor': 20, 'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([ 7.67864142, 13.72292654,  3.28368211]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 0.4243124329078323}
episode index:2360
target Thresh 31.99999999855825
target distance 10.0
model initialize at round 2360
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 28.]), 'previousTarget': array([25., 28.]), 'currentState': array([26.04048747, 19.89721805,  2.11353117]), 'targetState': array([25, 28], dtype=int32), 'currentDistance': 8.169313896535172}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9281835874576669
{'scaleFactor': 20, 'currentTarget': array([25., 28.]), 'previousTarget': array([25., 28.]), 'currentState': array([24.75996298, 27.71386446,  1.79555119]), 'targetState': array([25, 28], dtype=int32), 'currentDistance': 0.3734853708712144}
episode index:2361
target Thresh 31.999999998572594
target distance 16.0
model initialize at round 2361
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  8.]), 'previousTarget': array([19.,  8.]), 'currentState': array([ 6.5448696 , 23.33234291,  4.86525143]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 19.75375944589745}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9281735106107368
{'scaleFactor': 20, 'currentTarget': array([19.,  8.]), 'previousTarget': array([19.,  8.]), 'currentState': array([19.14973229,  8.24985958,  5.48250103]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 0.2912894919864486}
episode index:2362
target Thresh 31.999999998586798
target distance 13.0
model initialize at round 2362
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([18.94552112, 13.3122432 ,  1.23846745]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 13.113080502633593}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9281751576007057
{'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([12.53806985, 25.36992423,  1.93880109]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 0.5917967568695677}
episode index:2363
target Thresh 31.99999999860086
target distance 18.0
model initialize at round 2363
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 23.]), 'previousTarget': array([19., 23.]), 'currentState': array([10.55533721,  6.91213236,  1.45289993]), 'targetState': array([19, 23], dtype=int32), 'currentDistance': 18.169529846007237}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.928168957131113
{'scaleFactor': 20, 'currentTarget': array([19., 23.]), 'previousTarget': array([19., 23.]), 'currentState': array([18.64403723, 22.63643403,  1.06167464]), 'targetState': array([19, 23], dtype=int32), 'currentDistance': 0.5088120597396287}
episode index:2364
target Thresh 31.999999998614783
target distance 12.0
model initialize at round 2364
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 11.]), 'previousTarget': array([27., 11.]), 'currentState': array([16.50073213,  4.31193065,  0.57540834]), 'targetState': array([27, 11], dtype=int32), 'currentDistance': 12.448489764037777}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9281745855422207
{'scaleFactor': 20, 'currentTarget': array([27., 11.]), 'previousTarget': array([27., 11.]), 'currentState': array([26.44958343, 10.9995342 ,  0.78343575]), 'targetState': array([27, 11], dtype=int32), 'currentDistance': 0.5504167627669183}
episode index:2365
target Thresh 31.999999998628564
target distance 13.0
model initialize at round 2365
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 23.]), 'previousTarget': array([11., 23.]), 'currentState': array([22.05972156, 19.46973874,  3.01007138]), 'targetState': array([11, 23], dtype=int32), 'currentDistance': 11.609486875595913}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9281802091955844
{'scaleFactor': 20, 'currentTarget': array([11., 23.]), 'previousTarget': array([11., 23.]), 'currentState': array([10.5737953 , 22.81831155,  2.73244251]), 'targetState': array([11, 23], dtype=int32), 'currentDistance': 0.4633153750193909}
episode index:2366
target Thresh 31.99999999864221
target distance 13.0
model initialize at round 2366
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 18.]), 'previousTarget': array([11., 18.]), 'currentState': array([14.83306627,  6.97679208,  1.87779063]), 'targetState': array([11, 18], dtype=int32), 'currentDistance': 11.670625939462349}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9281858280972343
{'scaleFactor': 20, 'currentTarget': array([11., 18.]), 'previousTarget': array([11., 18.]), 'currentState': array([10.54091241, 18.13966253,  2.03226525]), 'targetState': array([11, 18], dtype=int32), 'currentDistance': 0.47986147392300293}
episode index:2367
target Thresh 31.99999999865572
target distance 14.0
model initialize at round 2367
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  8.]), 'previousTarget': array([11.,  8.]), 'currentState': array([23.35473499,  9.98009343,  3.25089073]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 12.512403714991224}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9281914422531902
{'scaleFactor': 20, 'currentTarget': array([11.,  8.]), 'previousTarget': array([11.,  8.]), 'currentState': array([11.56011731,  7.87905664,  3.44759992]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 0.5730259158382359}
episode index:2368
target Thresh 31.999999998669097
target distance 11.0
model initialize at round 2368
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 25.]), 'previousTarget': array([ 8., 25.]), 'currentState': array([10.93683707, 15.39811458,  1.99044007]), 'targetState': array([ 8, 25], dtype=int32), 'currentDistance': 10.040976824221897}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.928201065979508
{'scaleFactor': 20, 'currentTarget': array([ 8., 25.]), 'previousTarget': array([ 8., 25.]), 'currentState': array([ 7.69827012, 24.84925253,  1.97540209]), 'targetState': array([ 8, 25], dtype=int32), 'currentDistance': 0.33729174476500934}
episode index:2369
target Thresh 31.999999998682338
target distance 11.0
model initialize at round 2369
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([27.52391669, 28.59934342,  2.26423055]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 16.350701016484916}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9281948702755013
{'scaleFactor': 20, 'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([15.42599414, 17.73628635,  2.03894283]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 0.9335953730550468}
episode index:2370
target Thresh 31.99999999869545
target distance 2.0
model initialize at round 2370
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  7.]), 'previousTarget': array([13.,  7.]), 'currentState': array([15.3101038 ,  7.65415347,  2.39547756]), 'targetState': array([13,  7], dtype=int32), 'currentDistance': 2.4009365544850545}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9282209373905262
{'scaleFactor': 20, 'currentTarget': array([13.,  7.]), 'previousTarget': array([13.,  7.]), 'currentState': array([13.67678274,  7.24799027,  4.39516369]), 'targetState': array([13,  7], dtype=int32), 'currentDistance': 0.7207871060487417}
episode index:2371
target Thresh 31.99999999870843
target distance 15.0
model initialize at round 2371
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 26.]), 'previousTarget': array([27., 26.]), 'currentState': array([13.63484739, 17.3995774 ,  1.24971312]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 15.893224127647892}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9282186286877595
{'scaleFactor': 20, 'currentTarget': array([27., 26.]), 'previousTarget': array([27., 26.]), 'currentState': array([26.78618783, 26.1478256 ,  0.50147664]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 0.25993855348850303}
episode index:2372
target Thresh 31.999999998721282
target distance 19.0
model initialize at round 2372
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 23.]), 'previousTarget': array([ 4., 23.]), 'currentState': array([11.35774287,  3.00555423,  0.37784165]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 21.305263238214966}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9282047726513376
{'scaleFactor': 20, 'currentTarget': array([ 4., 23.]), 'previousTarget': array([ 4., 23.]), 'currentState': array([ 3.82381643, 22.64205568,  1.74295728]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 0.39895461534455606}
episode index:2373
target Thresh 31.999999998734005
target distance 9.0
model initialize at round 2373
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 11.]), 'previousTarget': array([17., 11.]), 'currentState': array([11.40616797, 20.54045123,  3.82467079]), 'targetState': array([17, 11], dtype=int32), 'currentDistance': 11.05943789574906}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.928210364638174
{'scaleFactor': 20, 'currentTarget': array([17., 11.]), 'previousTarget': array([17., 11.]), 'currentState': array([16.98303516, 10.75503089,  4.81383401]), 'targetState': array([17, 11], dtype=int32), 'currentDistance': 0.24555584246872716}
episode index:2374
target Thresh 31.999999998746603
target distance 18.0
model initialize at round 2374
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([25.02777921, 24.62625382,  3.08650577]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 23.111146083885693}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.928192753904312
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([7.96764521, 8.76327887, 3.60608316]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.23892201147713296}
episode index:2375
target Thresh 31.999999998759073
target distance 19.0
model initialize at round 2375
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 19.]), 'previousTarget': array([ 2., 19.]), 'currentState': array([20.47001134, 20.40265843,  3.38202429]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 18.523195443120112}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9281865773443706
{'scaleFactor': 20, 'currentTarget': array([ 2., 19.]), 'previousTarget': array([ 2., 19.]), 'currentState': array([ 2.65498304, 18.25000352,  3.10368822]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 0.9957396758087438}
episode index:2376
target Thresh 31.999999998771422
target distance 10.0
model initialize at round 2376
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 21.]), 'previousTarget': array([18., 21.]), 'currentState': array([ 7.58619184, 10.36869657,  5.47396183]), 'targetState': array([18, 21], dtype=int32), 'currentDistance': 14.88193579953044}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9281842879531562
{'scaleFactor': 20, 'currentTarget': array([18., 21.]), 'previousTarget': array([18., 21.]), 'currentState': array([18.65588695, 20.2441338 ,  5.68210551]), 'targetState': array([18, 21], dtype=int32), 'currentDistance': 1.0007604092583433}
episode index:2377
target Thresh 31.999999998783647
target distance 22.0
model initialize at round 2377
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 24.]), 'previousTarget': array([ 5., 24.]), 'currentState': array([25.79505938, 15.17494944,  3.37879574]), 'targetState': array([ 5, 24], dtype=int32), 'currentDistance': 22.590175120056205}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9281704754915522
{'scaleFactor': 20, 'currentTarget': array([ 5., 24.]), 'previousTarget': array([ 5., 24.]), 'currentState': array([ 5.49454442, 23.16288573,  2.69684098]), 'targetState': array([ 5, 24], dtype=int32), 'currentDistance': 0.9722831289788849}
episode index:2378
target Thresh 31.999999998795747
target distance 11.0
model initialize at round 2378
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 27.]), 'previousTarget': array([23., 27.]), 'currentState': array([13.73471983, 28.42555737,  0.41685992]), 'targetState': array([23, 27], dtype=int32), 'currentDistance': 9.374306931017347}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.928180067578315
{'scaleFactor': 20, 'currentTarget': array([23., 27.]), 'previousTarget': array([23., 27.]), 'currentState': array([22.82777642, 26.37641781,  4.79825919]), 'targetState': array([23, 27], dtype=int32), 'currentDistance': 0.6469279031422619}
episode index:2379
target Thresh 31.99999999880773
target distance 5.0
model initialize at round 2379
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 15.]), 'previousTarget': array([22., 15.]), 'currentState': array([25.44954538, 16.34439342,  2.53230751]), 'targetState': array([22, 15], dtype=int32), 'currentDistance': 3.7022637645539214}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9282018826759711
{'scaleFactor': 20, 'currentTarget': array([22., 15.]), 'previousTarget': array([22., 15.]), 'currentState': array([22.33626611, 15.06447737,  2.53152812]), 'targetState': array([22, 15], dtype=int32), 'currentDistance': 0.3423919170699406}
episode index:2380
target Thresh 31.999999998819593
target distance 9.0
model initialize at round 2380
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'previousTarget': array([7., 3.]), 'currentState': array([11.74672104, 10.49175739,  4.16210365]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 8.86892267602911}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.928215487937342
{'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'previousTarget': array([7., 3.]), 'currentState': array([7.5768502 , 3.68866987, 4.41158698]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 0.8983442220106509}
episode index:2381
target Thresh 31.99999999883134
target distance 3.0
model initialize at round 2381
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 21.]), 'previousTarget': array([ 6., 21.]), 'currentState': array([ 4.26410507, 22.51511599,  5.80997097]), 'targetState': array([ 6, 21], dtype=int32), 'currentDistance': 2.3041067009461282}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.928241426019652
{'scaleFactor': 20, 'currentTarget': array([ 6., 21.]), 'previousTarget': array([ 6., 21.]), 'currentState': array([ 6.0217508 , 21.56123065,  5.76146221]), 'targetState': array([ 6, 21], dtype=int32), 'currentDistance': 0.5616519747274253}
episode index:2382
target Thresh 31.999999998842966
target distance 11.0
model initialize at round 2382
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 22.]), 'previousTarget': array([10., 22.]), 'currentState': array([20.78546449, 12.66924013,  2.70861846]), 'targetState': array([10, 22], dtype=int32), 'currentDistance': 14.261462902600643}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9282430306868309
{'scaleFactor': 20, 'currentTarget': array([10., 22.]), 'previousTarget': array([10., 22.]), 'currentState': array([ 9.96835343, 21.4417895 ,  2.47056154]), 'targetState': array([10, 22], dtype=int32), 'currentDistance': 0.5591068504695856}
episode index:2383
target Thresh 31.99999999885448
target distance 14.0
model initialize at round 2383
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 23.]), 'previousTarget': array([26., 23.]), 'currentState': array([11.30402824, 11.46767778,  5.29605269]), 'targetState': array([26, 23], dtype=int32), 'currentDistance': 18.680632797968382}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9282330218966975
{'scaleFactor': 20, 'currentTarget': array([26., 23.]), 'previousTarget': array([26., 23.]), 'currentState': array([26.3550675 , 23.3474181 ,  0.39217545]), 'targetState': array([26, 23], dtype=int32), 'currentDistance': 0.4967617799975913}
episode index:2384
target Thresh 31.999999998865878
target distance 7.0
model initialize at round 2384
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 10.]), 'previousTarget': array([22., 10.]), 'currentState': array([16.61921721, 14.82610344,  5.64703619]), 'targetState': array([22, 10], dtype=int32), 'currentDistance': 7.228007873732368}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9282465912837429
{'scaleFactor': 20, 'currentTarget': array([22., 10.]), 'previousTarget': array([22., 10.]), 'currentState': array([21.62451882,  9.32070409,  4.06809938]), 'targetState': array([22, 10], dtype=int32), 'currentDistance': 0.7761630292434201}
episode index:2385
target Thresh 31.999999998877165
target distance 12.0
model initialize at round 2385
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 3.]), 'previousTarget': array([6., 3.]), 'currentState': array([ 6.3270449 , 14.81667153,  4.26074076]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 11.821196402792872}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9282521376199194
{'scaleFactor': 20, 'currentTarget': array([6., 3.]), 'previousTarget': array([6., 3.]), 'currentState': array([6.23407513, 2.92355095, 4.8386049 ]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 0.24624301853595562}
episode index:2386
target Thresh 31.999999998888335
target distance 8.0
model initialize at round 2386
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  9.]), 'previousTarget': array([24.,  9.]), 'currentState': array([16.75670481, 11.49674157,  6.18873405]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 7.661530113462871}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9282656876292951
{'scaleFactor': 20, 'currentTarget': array([24.,  9.]), 'previousTarget': array([24.,  9.]), 'currentState': array([24.3618537 ,  9.11978065,  5.98133098]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 0.38116336157334446}
episode index:2387
target Thresh 31.999999998899398
target distance 3.0
model initialize at round 2387
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 11.]), 'previousTarget': array([24., 11.]), 'currentState': array([21.69065673, 10.46527482,  6.14525938]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 2.370442436355927}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9282915395188975
{'scaleFactor': 20, 'currentTarget': array([24., 11.]), 'previousTarget': array([24., 11.]), 'currentState': array([23.6399072 , 10.76205635,  0.44595229]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 0.4316063097487897}
episode index:2388
target Thresh 31.999999998910347
target distance 13.0
model initialize at round 2388
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([19.20199587,  7.75022644,  3.14585745]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 11.425681272114785}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9282970600755663
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([7.40478011, 9.72790221, 2.92541333]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.6544646111685245}
episode index:2389
target Thresh 31.99999999892119
target distance 11.0
model initialize at round 2389
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 14.]), 'previousTarget': array([ 6., 14.]), 'currentState': array([10.93682954, 23.04497221,  4.31210035]), 'targetState': array([ 6, 14], dtype=int32), 'currentDistance': 10.304552794776214}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9283065550503884
{'scaleFactor': 20, 'currentTarget': array([ 6., 14.]), 'previousTarget': array([ 6., 14.]), 'currentState': array([ 6.50620218, 14.15361845,  4.23076526]), 'targetState': array([ 6, 14], dtype=int32), 'currentDistance': 0.5289983677767349}
episode index:2390
target Thresh 31.999999998931926
target distance 17.0
model initialize at round 2390
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 15.]), 'previousTarget': array([10., 15.]), 'currentState': array([27.17628699, 20.67371174,  2.47585627]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 18.089108313889163}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9283003696436268
{'scaleFactor': 20, 'currentTarget': array([10., 15.]), 'previousTarget': array([10., 15.]), 'currentState': array([10.86420629, 14.76621851,  3.59628633]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 0.8952688370396712}
episode index:2391
target Thresh 31.999999998942553
target distance 11.0
model initialize at round 2391
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 25.]), 'previousTarget': array([17., 25.]), 'currentState': array([11.31030851, 15.05616272,  1.68841379]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 11.456547870254077}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9283058795849969
{'scaleFactor': 20, 'currentTarget': array([17., 25.]), 'previousTarget': array([17., 25.]), 'currentState': array([16.82562112, 25.5353112 ,  1.03167291]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 0.562997398961989}
episode index:2392
target Thresh 31.999999998953076
target distance 13.0
model initialize at round 2392
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 19.]), 'previousTarget': array([23., 19.]), 'currentState': array([11.87837918, 26.54477183,  5.5894363 ]), 'targetState': array([23, 19], dtype=int32), 'currentDistance': 13.439271987045457}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.928307450612294
{'scaleFactor': 20, 'currentTarget': array([23., 19.]), 'previousTarget': array([23., 19.]), 'currentState': array([23.66021057, 19.02712853,  5.74321366]), 'targetState': array([23, 19], dtype=int32), 'currentDistance': 0.6607677017592681}
episode index:2393
target Thresh 31.999999998963492
target distance 6.0
model initialize at round 2393
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 15.]), 'previousTarget': array([21., 15.]), 'currentState': array([17.93850373, 19.25715024,  5.45738566]), 'targetState': array([21, 15], dtype=int32), 'currentDistance': 5.243671188688484}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9283249909420299
{'scaleFactor': 20, 'currentTarget': array([21., 15.]), 'previousTarget': array([21., 15.]), 'currentState': array([21.57324686, 14.50657234,  5.18480682]), 'targetState': array([21, 15], dtype=int32), 'currentDistance': 0.7563615617596778}
episode index:2394
target Thresh 31.999999998973806
target distance 12.0
model initialize at round 2394
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 10.]), 'previousTarget': array([18., 10.]), 'currentState': array([ 7.5987784 , 15.52563837,  5.59082777]), 'targetState': array([18, 10], dtype=int32), 'currentDistance': 11.777864410404268}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.928330483701303
{'scaleFactor': 20, 'currentTarget': array([18., 10.]), 'previousTarget': array([18., 10.]), 'currentState': array([18.3133306 , 10.19087106,  5.90484326]), 'targetState': array([18, 10], dtype=int32), 'currentDistance': 0.3668893972089436}
episode index:2395
target Thresh 31.999999998984016
target distance 8.0
model initialize at round 2395
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 18.]), 'previousTarget': array([18., 18.]), 'currentState': array([14.22293294, 24.84378902,  4.51582468]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 7.8168845264671125}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9283439501146161
{'scaleFactor': 20, 'currentTarget': array([18., 18.]), 'previousTarget': array([18., 18.]), 'currentState': array([18.27684226, 18.18262018,  5.35444044]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 0.3316500676552332}
episode index:2396
target Thresh 31.999999998994124
target distance 9.0
model initialize at round 2396
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  5.]), 'previousTarget': array([26.,  5.]), 'currentState': array([20.68038661, 13.00521915,  5.36708957]), 'targetState': array([26,  5], dtype=int32), 'currentDistance': 9.611546194995764}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9283533977991325
{'scaleFactor': 20, 'currentTarget': array([26.,  5.]), 'previousTarget': array([26.,  5.]), 'currentState': array([26.47797611,  4.90950824,  5.32355702]), 'targetState': array([26,  5], dtype=int32), 'currentDistance': 0.4864667707254955}
episode index:2397
target Thresh 31.99999999900413
target distance 20.0
model initialize at round 2397
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 28.]), 'previousTarget': array([ 5., 28.]), 'currentState': array([8.25764688, 6.88164745, 0.28316038]), 'targetState': array([ 5, 28], dtype=int32), 'currentDistance': 21.368132287190686}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9283396300161715
{'scaleFactor': 20, 'currentTarget': array([ 5., 28.]), 'previousTarget': array([ 5., 28.]), 'currentState': array([ 4.64758077, 27.86674472,  1.77546346]), 'targetState': array([ 5, 28], dtype=int32), 'currentDistance': 0.37677086668413845}
episode index:2398
target Thresh 31.999999999014044
target distance 4.0
model initialize at round 2398
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  4.]), 'previousTarget': array([26.,  4.]), 'currentState': array([21.269409  ,  2.48387834,  5.27334094]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 4.967606673432983}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9283571203746475
{'scaleFactor': 20, 'currentTarget': array([26.,  4.]), 'previousTarget': array([26.,  4.]), 'currentState': array([26.26919732,  4.57446499,  0.50001327]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 0.6344109218728747}
episode index:2399
target Thresh 31.999999999023853
target distance 26.0
model initialize at round 2399
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 28.]), 'previousTarget': array([11., 28.]), 'currentState': array([23.66230001,  3.46735008,  2.52026129]), 'targetState': array([11, 28], dtype=int32), 'currentDistance': 27.60769374398415}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9283322823298117
{'scaleFactor': 20, 'currentTarget': array([11., 28.]), 'previousTarget': array([11., 28.]), 'currentState': array([10.90141106, 27.6756235 ,  1.86600889]), 'targetState': array([11, 28], dtype=int32), 'currentDistance': 0.33902786247389144}
episode index:2400
target Thresh 31.999999999033566
target distance 24.0
model initialize at round 2400
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 26.]), 'previousTarget': array([23., 26.]), 'currentState': array([24.02722508,  0.66688458,  0.09572428]), 'targetState': array([23, 26], dtype=int32), 'currentDistance': 25.35393319302517}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9283111197894823
{'scaleFactor': 20, 'currentTarget': array([23., 26.]), 'previousTarget': array([23., 26.]), 'currentState': array([22.63538705, 25.83858081,  1.63529949]), 'targetState': array([23, 26], dtype=int32), 'currentDistance': 0.39874647987797424}
episode index:2401
target Thresh 31.999999999043183
target distance 11.0
model initialize at round 2401
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([15.63017789, 25.02226002,  2.75150156]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 9.630203613857562}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9283205614756234
{'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([ 5.7070088 , 24.71209597,  3.21775314]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 0.4107707072458472}
episode index:2402
target Thresh 31.9999999990527
target distance 5.0
model initialize at round 2402
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 16.]), 'previousTarget': array([20., 16.]), 'currentState': array([17.59672655, 11.53183871,  1.33152312]), 'targetState': array([20, 16], dtype=int32), 'currentDistance': 5.073478946238105}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9283421093068862
{'scaleFactor': 20, 'currentTarget': array([20., 16.]), 'previousTarget': array([20., 16.]), 'currentState': array([19.32432064, 15.12179194,  1.18128272]), 'targetState': array([20, 16], dtype=int32), 'currentDistance': 1.1080577631313289}
episode index:2403
target Thresh 31.99999999906213
target distance 20.0
model initialize at round 2403
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([24.51207645, 16.39227863,  4.01802683]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 25.057592955726875}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9283209690879562
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.55536945, 1.43401956, 3.54771255]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.7197431399153401}
episode index:2404
target Thresh 31.99999999907146
target distance 13.0
model initialize at round 2404
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([12.53846345,  6.79958783,  1.89576346]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 11.46771096916434}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9283264406806019
{'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([14.80538053, 18.44030453,  1.39448159]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 0.4813988100364412}
episode index:2405
target Thresh 31.9999999990807
target distance 13.0
model initialize at round 2405
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 15.]), 'previousTarget': array([19., 15.]), 'currentState': array([6.89592737e+00, 3.57532381e+00, 5.76670970e-04]), 'targetState': array([19, 15], dtype=int32), 'currentDistance': 16.64427229697381}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9283202855712099
{'scaleFactor': 20, 'currentTarget': array([19., 15.]), 'previousTarget': array([19., 15.]), 'currentState': array([19.4777933 , 15.85211634,  0.52674775]), 'targetState': array([19, 15], dtype=int32), 'currentDistance': 0.9769281912832682}
episode index:2406
target Thresh 31.999999999089848
target distance 10.0
model initialize at round 2406
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 24.]), 'previousTarget': array([ 2., 24.]), 'currentState': array([ 5.59696072, 14.53113514,  1.33108252]), 'targetState': array([ 2, 24], dtype=int32), 'currentDistance': 10.129043792017118}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9283297038364068
{'scaleFactor': 20, 'currentTarget': array([ 2., 24.]), 'previousTarget': array([ 2., 24.]), 'currentState': array([ 1.99232862, 23.67193918,  2.09285976]), 'targetState': array([ 2, 24], dtype=int32), 'currentDistance': 0.32815050106148247}
episode index:2407
target Thresh 31.999999999098904
target distance 14.0
model initialize at round 2407
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 17.]), 'previousTarget': array([17., 17.]), 'currentState': array([ 1.8820656 , 25.74198142,  4.9958818 ]), 'targetState': array([17, 17], dtype=int32), 'currentDistance': 17.463509948971794}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9283235524841008
{'scaleFactor': 20, 'currentTarget': array([17., 17.]), 'previousTarget': array([17., 17.]), 'currentState': array([17.21108295, 17.34362412,  5.55388373]), 'targetState': array([17, 17], dtype=int32), 'currentDistance': 0.4032785038121659}
episode index:2408
target Thresh 31.999999999107867
target distance 11.0
model initialize at round 2408
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  5.]), 'previousTarget': array([19.,  5.]), 'currentState': array([14.53156965, 14.84141368,  5.13905662]), 'targetState': array([19,  5], dtype=int32), 'currentDistance': 10.808343676377659}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9283329615739372
{'scaleFactor': 20, 'currentTarget': array([19.,  5.]), 'previousTarget': array([19.,  5.]), 'currentState': array([19.01043137,  5.93069867,  5.25566228]), 'targetState': array([19,  5], dtype=int32), 'currentDistance': 0.9307571282111974}
episode index:2409
target Thresh 31.999999999116746
target distance 13.0
model initialize at round 2409
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'previousTarget': array([26., 11.]), 'currentState': array([21.45723427, 24.67250426,  3.74051905]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 14.40743187467738}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9283306427908891
{'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'previousTarget': array([26., 11.]), 'currentState': array([26.17096991, 10.16847938,  4.36682563]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 0.8489153351888062}
episode index:2410
target Thresh 31.999999999125535
target distance 9.0
model initialize at round 2410
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  2.]), 'previousTarget': array([26.,  2.]), 'currentState': array([22.34075633, 11.28159997,  3.98347867]), 'targetState': array([26,  2], dtype=int32), 'currentDistance': 9.976881387561255}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9283400411347751
{'scaleFactor': 20, 'currentTarget': array([26.,  2.]), 'previousTarget': array([26.,  2.]), 'currentState': array([25.95408005,  2.60880504,  5.23836232]), 'targetState': array([26,  2], dtype=int32), 'currentDistance': 0.6105343728987606}
episode index:2411
target Thresh 31.999999999134236
target distance 14.0
model initialize at round 2411
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 3.]), 'previousTarget': array([6., 3.]), 'currentState': array([ 5.40207037, 15.4268286 ,  5.35917282]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 12.441205285282368}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9283454889408556
{'scaleFactor': 20, 'currentTarget': array([6., 3.]), 'previousTarget': array([6., 3.]), 'currentState': array([6.58876688, 3.69569105, 4.61494107]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 0.911390408394723}
episode index:2412
target Thresh 31.99999999914285
target distance 11.0
model initialize at round 2412
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 11.]), 'previousTarget': array([23., 11.]), 'currentState': array([13.55154021, 15.34670546,  0.61063116]), 'targetState': array([23, 11], dtype=int32), 'currentDistance': 10.400348109883344}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9283548733424135
{'scaleFactor': 20, 'currentTarget': array([23., 11.]), 'previousTarget': array([23., 11.]), 'currentState': array([22.38840576, 11.48579986,  6.01013936]), 'targetState': array([23, 11], dtype=int32), 'currentDistance': 0.781056350709793}
episode index:2413
target Thresh 31.999999999151377
target distance 15.0
model initialize at round 2413
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([13.7282121 , 15.44416402,  1.07763749]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 14.031963352810301}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9283564104072705
{'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([26.57634985, 20.40693758,  0.26289722]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 0.5874331005695212}
episode index:2414
target Thresh 31.999999999159822
target distance 16.0
model initialize at round 2414
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 28.]), 'previousTarget': array([11., 28.]), 'currentState': array([26.59979004, 27.63469265,  2.82089639]), 'targetState': array([11, 28], dtype=int32), 'currentDistance': 15.604066733990761}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9283540867153535
{'scaleFactor': 20, 'currentTarget': array([11., 28.]), 'previousTarget': array([11., 28.]), 'currentState': array([10.6635274 , 27.69834816,  3.08302959]), 'targetState': array([11, 28], dtype=int32), 'currentDistance': 0.451893402388137}
episode index:2415
target Thresh 31.999999999168182
target distance 17.0
model initialize at round 2415
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 29.]), 'previousTarget': array([ 2., 29.]), 'currentState': array([ 7.37536848, 12.43927269,  1.86752391]), 'targetState': array([ 2, 29], dtype=int32), 'currentDistance': 17.411268629169847}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9283367153515293
{'scaleFactor': 20, 'currentTarget': array([ 2., 29.]), 'previousTarget': array([ 2., 29.]), 'currentState': array([ 1.52970409, 28.72306261,  4.7307286 ]), 'targetState': array([ 2, 29], dtype=int32), 'currentDistance': 0.5457770201800751}
episode index:2416
target Thresh 31.99999999917646
target distance 10.0
model initialize at round 2416
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 19.]), 'previousTarget': array([21., 19.]), 'currentState': array([12.77305713, 12.8957801 ,  0.6713427 ]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 10.24422225158978}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9283460878523769
{'scaleFactor': 20, 'currentTarget': array([21., 19.]), 'previousTarget': array([21., 19.]), 'currentState': array([20.57588278, 19.08438232,  0.70063054]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 0.43243010015613303}
episode index:2417
target Thresh 31.999999999184652
target distance 10.0
model initialize at round 2417
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  7.]), 'previousTarget': array([24.,  7.]), 'currentState': array([15.98085673,  8.23759005,  6.27948292]), 'targetState': array([24,  7], dtype=int32), 'currentDistance': 8.114079611000134}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9283594252891624
{'scaleFactor': 20, 'currentTarget': array([24.,  7.]), 'previousTarget': array([24.,  7.]), 'currentState': array([23.89726135,  7.26459025,  6.22076516]), 'targetState': array([24,  7], dtype=int32), 'currentDistance': 0.2838366290357387}
episode index:2418
target Thresh 31.999999999192767
target distance 18.0
model initialize at round 2418
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([18.67131436, 14.03294848,  3.49077165]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 17.152183420611053}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9283532896224382
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.15022096, 10.98427125,  1.33473676]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.9956687387083464}
episode index:2419
target Thresh 31.9999999992008
target distance 21.0
model initialize at round 2419
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([ 4.81407395, 13.82391825,  0.6502425 ]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 19.27242683961018}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9283433841618541
{'scaleFactor': 20, 'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([24.54291651, 12.25921602,  6.02257704]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 0.6016238742069498}
episode index:2420
target Thresh 31.99999999920875
target distance 14.0
model initialize at round 2420
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  9.]), 'previousTarget': array([23.,  9.]), 'currentState': array([ 7.72574395, 17.90060944,  4.8634541 ]), 'targetState': array([23,  9], dtype=int32), 'currentDistance': 17.678341162041978}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9283372601896616
{'scaleFactor': 20, 'currentTarget': array([23.,  9.]), 'previousTarget': array([23.,  9.]), 'currentState': array([22.91931301,  9.23378628,  5.81828702]), 'targetState': array([23,  9], dtype=int32), 'currentDistance': 0.2473184498085856}
episode index:2421
target Thresh 31.999999999216623
target distance 11.0
model initialize at round 2421
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([20.42565436, 14.89437002,  1.57028025]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 12.540498393173715}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9283426866509378
{'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([13.07061048, 24.08828151,  2.27231107]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 0.9144487111519644}
episode index:2422
target Thresh 31.999999999224418
target distance 11.0
model initialize at round 2422
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 19.]), 'previousTarget': array([16., 19.]), 'currentState': array([27.85261275, 25.4510133 ,  2.04954574]), 'targetState': array([16, 19], dtype=int32), 'currentDistance': 13.494443356809306}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9283442230361034
{'scaleFactor': 20, 'currentTarget': array([16., 19.]), 'previousTarget': array([16., 19.]), 'currentState': array([16.26773076, 18.96943895,  3.7973322 ]), 'targetState': array([16, 19], dtype=int32), 'currentDistance': 0.2694693627867931}
episode index:2423
target Thresh 31.999999999232134
target distance 23.0
model initialize at round 2423
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 24.]), 'previousTarget': array([ 2., 24.]), 'currentState': array([24.08414934,  6.41195101,  3.15621626]), 'targetState': array([ 2, 24], dtype=int32), 'currentDistance': 28.232058362433488}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.9283160522983168
{'scaleFactor': 20, 'currentTarget': array([ 2., 24.]), 'previousTarget': array([ 2., 24.]), 'currentState': array([ 1.40212527, 23.95390374,  2.77920186]), 'targetState': array([ 2, 24], dtype=int32), 'currentDistance': 0.5996491114950268}
episode index:2424
target Thresh 31.999999999239776
target distance 22.0
model initialize at round 2424
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([ 7.12298919, 27.03936894,  4.44445831]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 20.459114042868976}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9283061826169603
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.50986492, 7.43527466, 4.55827585]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.670392621554274}
episode index:2425
target Thresh 31.99999999924734
target distance 11.0
model initialize at round 2425
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([3.61555201, 3.04308211, 4.75638199]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 12.53810510637859}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9283077321492317
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.572709  ,  5.484878  ,  0.21651718]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.7504014130667123}
episode index:2426
target Thresh 31.99999999925483
target distance 20.0
model initialize at round 2426
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 27.]), 'previousTarget': array([25., 27.]), 'currentState': array([ 5.47661188, 10.38592776,  6.00951481]), 'targetState': array([25, 27], dtype=int32), 'currentDistance': 25.635718837757768}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9282868064347075
{'scaleFactor': 20, 'currentTarget': array([25., 27.]), 'previousTarget': array([25., 27.]), 'currentState': array([24.1857895 , 26.67325802,  0.62389029]), 'targetState': array([25, 27], dtype=int32), 'currentDistance': 0.8773249466578019}
episode index:2427
target Thresh 31.999999999262243
target distance 12.0
model initialize at round 2427
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  4.]), 'previousTarget': array([10.,  4.]), 'currentState': array([20.63026459, 15.02213856,  4.77159023]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 15.313068396902308}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9282845238515086
{'scaleFactor': 20, 'currentTarget': array([10.,  4.]), 'previousTarget': array([10.,  4.]), 'currentState': array([9.84726726, 3.55690651, 3.86777236]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 0.4686780685462677}
episode index:2428
target Thresh 31.999999999269583
target distance 13.0
model initialize at round 2428
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([ 1.33685086, 18.7420462 ,  4.30535805]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 15.02072586852484}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9282822431477524
{'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([15.3495011 , 21.91561025,  0.51234651]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 0.6559500356728287}
episode index:2429
target Thresh 31.999999999276852
target distance 3.0
model initialize at round 2429
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 15.]), 'previousTarget': array([11., 15.]), 'currentState': array([ 9.69430578, 10.34502594,  5.53973532]), 'targetState': array([11, 15], dtype=int32), 'currentDistance': 4.83462727260179}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9282995339941937
{'scaleFactor': 20, 'currentTarget': array([11., 15.]), 'previousTarget': array([11., 15.]), 'currentState': array([10.65642323, 14.61883552,  1.72666707]), 'targetState': array([11, 15], dtype=int32), 'currentDistance': 0.5131582182725587}
episode index:2430
target Thresh 31.999999999284046
target distance 15.0
model initialize at round 2430
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'previousTarget': array([ 2., 17.]), 'currentState': array([16.14943589, 29.45221513,  3.11063564]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 18.848453450186586}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9282896954672561
{'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'previousTarget': array([ 2., 17.]), 'currentState': array([ 1.6808382 , 16.41216554,  3.83871371]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 0.6688898322037997}
episode index:2431
target Thresh 31.999999999291173
target distance 25.0
model initialize at round 2431
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4.40908502, 24.23472299]), 'previousTarget': array([ 3.11513418, 25.15249803]), 'currentState': array([28.60786916,  6.50286336,  0.71013373]), 'targetState': array([ 2, 26], dtype=int32), 'currentDistance': 29.999999999999996}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.9282546023331759
{'scaleFactor': 20, 'currentTarget': array([ 2., 26.]), 'previousTarget': array([ 2., 26.]), 'currentState': array([ 2.20018555, 25.66130996,  2.51102857]), 'targetState': array([ 2, 26], dtype=int32), 'currentDistance': 0.39342750166160484}
episode index:2432
target Thresh 31.999999999298225
target distance 10.0
model initialize at round 2432
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 28.]), 'previousTarget': array([ 8., 28.]), 'currentState': array([ 6.28035451, 19.97996893,  1.40080283]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 8.202321563955314}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9282678951435609
{'scaleFactor': 20, 'currentTarget': array([ 8., 28.]), 'previousTarget': array([ 8., 28.]), 'currentState': array([ 7.70073526, 27.84059035,  1.58020826]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 0.3390734757109922}
episode index:2433
target Thresh 31.999999999305206
target distance 19.0
model initialize at round 2433
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 10.]), 'previousTarget': array([23., 10.]), 'currentState': array([5.76568862, 5.93681927, 0.42773491]), 'targetState': array([23, 10], dtype=int32), 'currentDistance': 17.706804519267866}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.928261834893906
{'scaleFactor': 20, 'currentTarget': array([23., 10.]), 'previousTarget': array([23., 10.]), 'currentState': array([23.15832203, 10.32218777,  0.18970144]), 'targetState': array([23, 10], dtype=int32), 'currentDistance': 0.3589858287391312}
episode index:2434
target Thresh 31.99999999931212
target distance 16.0
model initialize at round 2434
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 26.]), 'previousTarget': array([25., 26.]), 'currentState': array([15.55909563, 11.9092544 ,  1.10593897]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 16.961125758399668}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9282595691278008
{'scaleFactor': 20, 'currentTarget': array([25., 26.]), 'previousTarget': array([25., 26.]), 'currentState': array([24.32969723, 25.25880818,  1.22188379]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 0.9993353340257443}
episode index:2435
target Thresh 31.999999999318966
target distance 9.0
model initialize at round 2435
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 17.]), 'previousTarget': array([23., 17.]), 'currentState': array([17.72251446,  9.6599367 ,  0.72643113]), 'targetState': array([23, 17], dtype=int32), 'currentDistance': 9.040375148105248}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9282728435288156
{'scaleFactor': 20, 'currentTarget': array([23., 17.]), 'previousTarget': array([23., 17.]), 'currentState': array([22.20056469, 16.24642445,  1.16699407]), 'targetState': array([23, 17], dtype=int32), 'currentDistance': 1.0986231936833906}
episode index:2436
target Thresh 31.99999999932574
target distance 10.0
model initialize at round 2436
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 21.]), 'previousTarget': array([ 4., 21.]), 'currentState': array([4.76207818, 9.49945845, 6.19231176]), 'targetState': array([ 4, 21], dtype=int32), 'currentDistance': 11.525763274261893}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9282782630224028
{'scaleFactor': 20, 'currentTarget': array([ 4., 21.]), 'previousTarget': array([ 4., 21.]), 'currentState': array([ 3.86023237, 20.56826314,  1.75985917]), 'targetState': array([ 4, 21], dtype=int32), 'currentDistance': 0.453796991332012}
episode index:2437
target Thresh 31.999999999332452
target distance 20.0
model initialize at round 2437
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  5.]), 'previousTarget': array([19.,  5.]), 'currentState': array([27.20552642, 26.6703733 ,  2.45836946]), 'targetState': array([19,  5], dtype=int32), 'currentDistance': 23.171873964404107}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.928261079514894
{'scaleFactor': 20, 'currentTarget': array([19.,  5.]), 'previousTarget': array([19.,  5.]), 'currentState': array([19.40755576,  5.14490171,  4.35847588]), 'targetState': array([19,  5], dtype=int32), 'currentDistance': 0.43254849882650126}
episode index:2438
target Thresh 31.999999999339092
target distance 12.0
model initialize at round 2438
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 20.]), 'previousTarget': array([15., 20.]), 'currentState': array([4.01142598, 6.65485853, 0.08392638]), 'targetState': array([15, 20], dtype=int32), 'currentDistance': 17.287034440223227}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9282550344833108
{'scaleFactor': 20, 'currentTarget': array([15., 20.]), 'previousTarget': array([15., 20.]), 'currentState': array([15.05776529, 20.35363734,  0.84081961]), 'targetState': array([15, 20], dtype=int32), 'currentDistance': 0.35832414938789847}
episode index:2439
target Thresh 31.99999999934567
target distance 16.0
model initialize at round 2439
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  8.]), 'previousTarget': array([20.,  8.]), 'currentState': array([2.51269779, 2.2123959 , 4.63860154]), 'targetState': array([20,  8], dtype=int32), 'currentDistance': 18.420154717505437}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9282452504835262
{'scaleFactor': 20, 'currentTarget': array([20.,  8.]), 'previousTarget': array([20.,  8.]), 'currentState': array([20.24301589,  8.35313463,  0.24463057]), 'targetState': array([20,  8], dtype=int32), 'currentDistance': 0.4286732946991761}
episode index:2440
target Thresh 31.99999999935218
target distance 6.0
model initialize at round 2440
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 23.]), 'previousTarget': array([15., 23.]), 'currentState': array([ 9.03946427, 18.31749275,  5.74584031]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 7.579832476792853}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.928258503559936
{'scaleFactor': 20, 'currentTarget': array([15., 23.]), 'previousTarget': array([15., 23.]), 'currentState': array([14.72892594, 23.09527088,  0.78922569]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 0.2873285332750627}
episode index:2441
target Thresh 31.999999999358625
target distance 16.0
model initialize at round 2441
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 20.]), 'previousTarget': array([23., 20.]), 'currentState': array([21.42904444,  5.91105018,  1.73153874]), 'targetState': array([23, 20], dtype=int32), 'currentDistance': 14.176262149675908}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9282600624642551
{'scaleFactor': 20, 'currentTarget': array([23., 20.]), 'previousTarget': array([23., 20.]), 'currentState': array([22.48287233, 19.76777294,  1.40077799]), 'targetState': array([23, 20], dtype=int32), 'currentDistance': 0.566877795759358}
episode index:2442
target Thresh 31.999999999365006
target distance 4.0
model initialize at round 2442
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  9.]), 'previousTarget': array([18.,  9.]), 'currentState': array([13.33661793,  8.45328984,  5.31722069]), 'targetState': array([18,  9], dtype=int32), 'currentDistance': 4.695319406165811}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9282772703797425
{'scaleFactor': 20, 'currentTarget': array([18.,  9.]), 'previousTarget': array([18.,  9.]), 'currentState': array([18.39011411,  8.70926295,  1.07393211]), 'targetState': array([18,  9], dtype=int32), 'currentDistance': 0.4865357656081025}
episode index:2443
target Thresh 31.999999999371326
target distance 16.0
model initialize at round 2443
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 19.]), 'previousTarget': array([17., 19.]), 'currentState': array([19.36890541,  2.02097698,  0.38915413]), 'targetState': array([17, 19], dtype=int32), 'currentDistance': 17.143480845019607}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9282712310905051
{'scaleFactor': 20, 'currentTarget': array([17., 19.]), 'previousTarget': array([17., 19.]), 'currentState': array([16.64748735, 19.15124409,  1.63281685]), 'targetState': array([17, 19], dtype=int32), 'currentDistance': 0.3835882505440068}
episode index:2444
target Thresh 31.999999999377582
target distance 24.0
model initialize at round 2444
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4.29402876, 24.01333964]), 'previousTarget': array([ 4.34035385, 23.7571609 ]), 'currentState': array([26.97202833,  4.373881  ,  1.96505087]), 'targetState': array([ 2, 26], dtype=int32), 'currentDistance': 30.000000000000004}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.9282363320975782
{'scaleFactor': 20, 'currentTarget': array([ 2., 26.]), 'previousTarget': array([ 2., 26.]), 'currentState': array([ 1.58617419, 26.04425536,  2.17123681]), 'targetState': array([ 2, 26], dtype=int32), 'currentDistance': 0.416185462057781}
episode index:2445
target Thresh 31.999999999383775
target distance 7.0
model initialize at round 2445
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([23.90141432, 17.42121086,  2.01556728]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 7.922584494613967}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9282495617287728
{'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([16.48393246, 17.37867445,  3.07609898]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 0.7875506763440645}
episode index:2446
target Thresh 31.999999999389907
target distance 10.0
model initialize at round 2446
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([14.98399625, 10.85009501,  6.02966394]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 8.892668197725811}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9282627805470283
{'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([22.37972226,  7.8716167 ,  5.91023622]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 1.0697944402018074}
episode index:2447
target Thresh 31.999999999395975
target distance 2.0
model initialize at round 2447
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 29.]), 'previousTarget': array([ 6., 29.]), 'currentState': array([ 6.38010429, 27.45642752,  1.85694945]), 'targetState': array([ 6, 29], dtype=int32), 'currentDistance': 1.5896840161603079}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9282879999994191
{'scaleFactor': 20, 'currentTarget': array([ 6., 29.]), 'previousTarget': array([ 6., 29.]), 'currentState': array([ 5.59825843, 29.2922655 ,  2.09222677]), 'targetState': array([ 6, 29], dtype=int32), 'currentDistance': 0.4968052050936936}
episode index:2448
target Thresh 31.999999999401986
target distance 4.0
model initialize at round 2448
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 11.]), 'previousTarget': array([17., 11.]), 'currentState': array([19.31810583, 13.06016712,  4.11583447]), 'targetState': array([17, 11], dtype=int32), 'currentDistance': 3.101274446657015}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9283091563897828
{'scaleFactor': 20, 'currentTarget': array([17., 11.]), 'previousTarget': array([17., 11.]), 'currentState': array([16.3426005 , 11.47191166,  2.26829458]), 'targetState': array([17, 11], dtype=int32), 'currentDistance': 0.809243297249507}
episode index:2449
target Thresh 31.999999999407937
target distance 14.0
model initialize at round 2449
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  4.]), 'previousTarget': array([25.,  4.]), 'currentState': array([11.96617777,  3.37800167,  6.23248822]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 13.048655255052545}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9283106895291777
{'scaleFactor': 20, 'currentTarget': array([25.,  4.]), 'previousTarget': array([25.,  4.]), 'currentState': array([25.1111525 ,  3.15407131,  4.59841382]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 0.8531999887761664}
episode index:2450
target Thresh 31.999999999413827
target distance 15.0
model initialize at round 2450
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 22.]), 'previousTarget': array([ 7., 22.]), 'currentState': array([14.82344828,  8.67368383,  2.68589455]), 'targetState': array([ 7, 22], dtype=int32), 'currentDistance': 15.453059425270164}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9283084186213436
{'scaleFactor': 20, 'currentTarget': array([ 7., 22.]), 'previousTarget': array([ 7., 22.]), 'currentState': array([ 6.6570244 , 22.01978972,  1.9068363 ]), 'targetState': array([ 7, 22], dtype=int32), 'currentDistance': 0.3435460558665232}
episode index:2451
target Thresh 31.99999999941966
target distance 18.0
model initialize at round 2451
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([19.56957002, 21.60307826,  3.95870405]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 23.456616515025498}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9282913209268472
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.09103249, 4.62979326, 3.69208841]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.38123476545559226}
episode index:2452
target Thresh 31.999999999425434
target distance 9.0
model initialize at round 2452
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([12.51512531, 15.38839067,  5.43014002]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 13.586734376728545}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.928292859462102
{'scaleFactor': 20, 'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([20.43190522, 25.79481012,  1.07837944]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 0.6040153738536735}
episode index:2453
target Thresh 31.99999999943115
target distance 5.0
model initialize at round 2453
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([6.76510908, 3.3288901 , 3.57042623]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 4.812136017484508}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9283139707663148
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.83492533, 3.36375287, 3.00611525]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.0497193518069143}
episode index:2454
target Thresh 31.999999999436813
target distance 17.0
model initialize at round 2454
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([ 5.20127308, 24.32910885,  5.8422699 ]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 20.26193311134947}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.928304222539937
{'scaleFactor': 20, 'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([21.78551707, 13.85392067,  5.44036615]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 0.8804450189411177}
episode index:2455
target Thresh 31.999999999442416
target distance 11.0
model initialize at round 2455
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 16.]), 'previousTarget': array([25., 16.]), 'currentState': array([16.77126428, 25.16526683,  5.28320351]), 'targetState': array([25, 16], dtype=int32), 'currentDistance': 12.31723213923529}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9283095873310042
{'scaleFactor': 20, 'currentTarget': array([25., 16.]), 'previousTarget': array([25., 16.]), 'currentState': array([25.06522326, 16.56812203,  5.48481547]), 'targetState': array([25, 16], dtype=int32), 'currentDistance': 0.5718537492757628}
episode index:2456
target Thresh 31.999999999447965
target distance 13.0
model initialize at round 2456
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([13.36205462,  5.01146811,  0.3821904 ]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 14.084100593392959}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9283111159270873
{'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([14.27617533, 18.30760728,  1.27750691]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 1.0016635345966212}
episode index:2457
target Thresh 31.999999999453458
target distance 21.0
model initialize at round 2457
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 27.]), 'previousTarget': array([17., 27.]), 'currentState': array([3.68142164, 5.92782407, 0.9671008 ]), 'targetState': array([17, 27], dtype=int32), 'currentDistance': 24.928319802402687}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9282904527485162
{'scaleFactor': 20, 'currentTarget': array([17., 27.]), 'previousTarget': array([17., 27.]), 'currentState': array([17.57995466, 26.85121136,  6.07188035]), 'targetState': array([17, 27], dtype=int32), 'currentDistance': 0.5987365560594564}
episode index:2458
target Thresh 31.999999999458893
target distance 20.0
model initialize at round 2458
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 16.]), 'previousTarget': array([26., 16.]), 'currentState': array([5.46077696, 0.40575203, 5.39623761]), 'targetState': array([26, 16], dtype=int32), 'currentDistance': 25.788374373707576}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9282662377668245
{'scaleFactor': 20, 'currentTarget': array([26., 16.]), 'previousTarget': array([26., 16.]), 'currentState': array([26.35272027, 15.35306316,  4.88493271]), 'targetState': array([26, 16], dtype=int32), 'currentDistance': 0.7368438543877993}
episode index:2459
target Thresh 31.99999999946428
target distance 14.0
model initialize at round 2459
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 13.]), 'previousTarget': array([ 2., 13.]), 'currentState': array([14.42367727,  5.21032551,  2.68350127]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 14.663791646174477}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9282677821205401
{'scaleFactor': 20, 'currentTarget': array([ 2., 13.]), 'previousTarget': array([ 2., 13.]), 'currentState': array([ 2.33509359, 12.16913176,  2.72523928]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 0.8958960559212539}
episode index:2460
target Thresh 31.99999999946961
target distance 5.0
model initialize at round 2460
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 13.]), 'previousTarget': array([ 7., 13.]), 'currentState': array([ 4.66357382, 16.97708595,  4.80488348]), 'targetState': array([ 7, 13], dtype=int32), 'currentDistance': 4.612602290747974}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.928288843566245
{'scaleFactor': 20, 'currentTarget': array([ 7., 13.]), 'previousTarget': array([ 7., 13.]), 'currentState': array([ 7.07738937, 13.92791446,  5.38622782]), 'targetState': array([ 7, 13], dtype=int32), 'currentDistance': 0.9311360608921221}
episode index:2461
target Thresh 31.999999999474888
target distance 15.0
model initialize at round 2461
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 19.]), 'previousTarget': array([ 6., 19.]), 'currentState': array([3.98253374, 2.63361223, 0.06261652]), 'targetState': array([ 6, 19], dtype=int32), 'currentDistance': 16.49026435756426}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9282828437303056
{'scaleFactor': 20, 'currentTarget': array([ 6., 19.]), 'previousTarget': array([ 6., 19.]), 'currentState': array([ 6.87997222, 18.82680095,  5.88542409]), 'targetState': array([ 6, 19], dtype=int32), 'currentDistance': 0.8968550684706446}
episode index:2462
target Thresh 31.999999999480114
target distance 10.0
model initialize at round 2462
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 13.]), 'previousTarget': array([16., 13.]), 'currentState': array([25.48035531,  3.80058493,  1.50575703]), 'targetState': array([16, 13], dtype=int32), 'currentDistance': 13.210086090327199}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9282843794607875
{'scaleFactor': 20, 'currentTarget': array([16., 13.]), 'previousTarget': array([16., 13.]), 'currentState': array([16.0994989 , 13.37196467,  1.46099615]), 'targetState': array([16, 13], dtype=int32), 'currentDistance': 0.38504252192243715}
episode index:2463
target Thresh 31.999999999485286
target distance 9.0
model initialize at round 2463
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  5.]), 'previousTarget': array([26.,  5.]), 'currentState': array([19.75911448, 13.29268168,  5.33218296]), 'targetState': array([26,  5], dtype=int32), 'currentDistance': 10.378690737238692}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9282935944244398
{'scaleFactor': 20, 'currentTarget': array([26.,  5.]), 'previousTarget': array([26.,  5.]), 'currentState': array([26.17878266,  5.70090627,  5.35901208]), 'targetState': array([26,  5], dtype=int32), 'currentDistance': 0.7233483530005552}
episode index:2464
target Thresh 31.999999999490406
target distance 23.0
model initialize at round 2464
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 16.]), 'previousTarget': array([ 4., 16.]), 'currentState': array([27.41482514, 29.63104512,  2.33174571]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 27.093531099394802}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9282694371093665
{'scaleFactor': 20, 'currentTarget': array([ 4., 16.]), 'previousTarget': array([ 4., 16.]), 'currentState': array([ 4.05680016, 15.5306795 ,  3.5843297 ]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 0.47274516753386026}
episode index:2465
target Thresh 31.999999999495476
target distance 20.0
model initialize at round 2465
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  2.]), 'previousTarget': array([10.,  2.]), 'currentState': array([18.99106616, 23.34701172,  3.22366762]), 'targetState': array([10,  2], dtype=int32), 'currentDistance': 23.16320746587709}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9282524522896611
{'scaleFactor': 20, 'currentTarget': array([10.,  2.]), 'previousTarget': array([10.,  2.]), 'currentState': array([10.43079077,  1.68723524,  4.45472277]), 'targetState': array([10,  2], dtype=int32), 'currentDistance': 0.5323555922001171}
episode index:2466
target Thresh 31.999999999500496
target distance 18.0
model initialize at round 2466
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  7.]), 'previousTarget': array([21.,  7.]), 'currentState': array([ 4.43184943, 24.38902385,  4.52311516]), 'targetState': array([21,  7], dtype=int32), 'currentDistance': 24.018363053673703}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9282354812395706
{'scaleFactor': 20, 'currentTarget': array([21.,  7.]), 'previousTarget': array([21.,  7.]), 'currentState': array([20.87575045,  7.94081996,  5.4154424 ]), 'targetState': array([21,  7], dtype=int32), 'currentDistance': 0.9489890149955865}
episode index:2467
target Thresh 31.999999999505466
target distance 16.0
model initialize at round 2467
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([20.25077441, 25.49299997,  3.24101305]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 21.03884902149412}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9282221517310694
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.56481003, 11.04591088,  2.88139697]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.43760497994051356}
episode index:2468
target Thresh 31.999999999510386
target distance 13.0
model initialize at round 2468
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 14.]), 'previousTarget': array([21., 14.]), 'currentState': array([ 8.56539211, 26.12006348,  4.70178056]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 17.364199152629933}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9282161959172793
{'scaleFactor': 20, 'currentTarget': array([21., 14.]), 'previousTarget': array([21., 14.]), 'currentState': array([21.31077881, 14.04420139,  5.40437625]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 0.31390640651023066}
episode index:2469
target Thresh 31.99999999951526
target distance 4.0
model initialize at round 2469
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 28.]), 'previousTarget': array([16., 28.]), 'currentState': array([21.55118879, 28.65284104,  1.40836304]), 'targetState': array([16, 28], dtype=int32), 'currentDistance': 5.589445268688346}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9282332334897825
{'scaleFactor': 20, 'currentTarget': array([16., 28.]), 'previousTarget': array([16., 28.]), 'currentState': array([16.79427098, 27.93028581,  3.73508901]), 'targetState': array([16, 28], dtype=int32), 'currentDistance': 0.7973245675416643}
episode index:2470
target Thresh 31.99999999952008
target distance 21.0
model initialize at round 2470
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 17.]), 'previousTarget': array([26., 17.]), 'currentState': array([ 6.56343432, 13.94677835,  6.09194297]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 19.674914175620636}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9282235810581836
{'scaleFactor': 20, 'currentTarget': array([26., 17.]), 'previousTarget': array([26., 17.]), 'currentState': array([26.05043105, 17.38297843,  0.08560776]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 0.386284568528211}
episode index:2471
target Thresh 31.999999999524857
target distance 6.0
model initialize at round 2471
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 22.]), 'previousTarget': array([22., 22.]), 'currentState': array([20.55069193, 17.62188481,  0.8310473 ]), 'targetState': array([22, 22], dtype=int32), 'currentDistance': 4.611766090987246}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9282445666645517
{'scaleFactor': 20, 'currentTarget': array([22., 22.]), 'previousTarget': array([22., 22.]), 'currentState': array([21.31712928, 21.44710822,  1.41509052]), 'targetState': array([22, 22], dtype=int32), 'currentDistance': 0.8786362984585399}
episode index:2472
target Thresh 31.999999999529585
target distance 11.0
model initialize at round 2472
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 22.]), 'previousTarget': array([24., 22.]), 'currentState': array([12.25759032, 16.48963052,  5.26553082]), 'targetState': array([24, 22], dtype=int32), 'currentDistance': 12.971058432263849}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9282461116630323
{'scaleFactor': 20, 'currentTarget': array([24., 22.]), 'previousTarget': array([24., 22.]), 'currentState': array([24.27387084, 22.09341981,  6.21083296]), 'targetState': array([24, 22], dtype=int32), 'currentDistance': 0.28936568408772834}
episode index:2473
target Thresh 31.999999999534268
target distance 4.0
model initialize at round 2473
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([19.34007537,  2.98187128,  0.36028498]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 5.679508336035945}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9282631095968792
{'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([21.8614342 ,  8.22115689,  1.2988723 ]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 0.2609805581956355}
episode index:2474
target Thresh 31.9999999995389
target distance 7.0
model initialize at round 2474
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 19.]), 'previousTarget': array([ 6., 19.]), 'currentState': array([ 5.8879175 , 24.01627687,  4.85647944]), 'targetState': array([ 6, 19], dtype=int32), 'currentDistance': 5.017528883844751}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9282800937950219
{'scaleFactor': 20, 'currentTarget': array([ 6., 19.]), 'previousTarget': array([ 6., 19.]), 'currentState': array([ 5.12659385, 18.74548671,  3.02838202]), 'targetState': array([ 6, 19], dtype=int32), 'currentDistance': 0.909733647356973}
episode index:2475
target Thresh 31.999999999543487
target distance 21.0
model initialize at round 2475
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 25.]), 'previousTarget': array([26., 25.]), 'currentState': array([11.25459938,  2.87822975,  0.28043955]), 'targetState': array([26, 25], dtype=int32), 'currentDistance': 26.58570214518071}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9282560492550274
{'scaleFactor': 20, 'currentTarget': array([26., 25.]), 'previousTarget': array([26., 25.]), 'currentState': array([26.36589825, 24.91539716,  6.19860731]), 'targetState': array([26, 25], dtype=int32), 'currentDistance': 0.3755518239105353}
episode index:2476
target Thresh 31.99999999954803
target distance 16.0
model initialize at round 2476
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 20.]), 'previousTarget': array([ 6., 20.]), 'currentState': array([19.41468836,  5.06812283,  2.02494431]), 'targetState': array([ 6, 20], dtype=int32), 'currentDistance': 20.07273821714483}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9282464109933213
{'scaleFactor': 20, 'currentTarget': array([ 6., 20.]), 'previousTarget': array([ 6., 20.]), 'currentState': array([ 5.78766597, 19.55156482,  2.35980015]), 'targetState': array([ 6, 20], dtype=int32), 'currentDistance': 0.4961651510695309}
episode index:2477
target Thresh 31.99999999955253
target distance 14.0
model initialize at round 2477
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([ 6.3296566 , 19.52868694,  5.6748673 ]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 15.853500454951186}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9282441907687186
{'scaleFactor': 20, 'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([19.13053697, 10.03783417,  5.31460873]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 0.13590924960956077}
episode index:2478
target Thresh 31.99999999955698
target distance 2.0
model initialize at round 2478
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 20.]), 'previousTarget': array([ 5., 20.]), 'currentState': array([ 4.11751507, 20.3211378 ,  5.79227185]), 'targetState': array([ 5, 20], dtype=int32), 'currentDistance': 0.9391001701510467}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9282731362343222
{'scaleFactor': 20, 'currentTarget': array([ 5., 20.]), 'previousTarget': array([ 5., 20.]), 'currentState': array([ 4.11751507, 20.3211378 ,  5.79227185]), 'targetState': array([ 5, 20], dtype=int32), 'currentDistance': 0.9391001701510467}
episode index:2479
target Thresh 31.99999999956139
target distance 5.0
model initialize at round 2479
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 19.]), 'previousTarget': array([19., 19.]), 'currentState': array([25.66009343, 14.27654635,  1.17506903]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 8.165038821026663}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9282861696511631
{'scaleFactor': 20, 'currentTarget': array([19., 19.]), 'previousTarget': array([19., 19.]), 'currentState': array([19.4626862 , 18.34449849,  2.81403141]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 0.8023470263515674}
episode index:2480
target Thresh 31.999999999565752
target distance 20.0
model initialize at round 2480
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 13.]), 'previousTarget': array([ 5., 13.]), 'currentState': array([23.76205512, 26.43232518,  3.95695366]), 'targetState': array([ 5, 13], dtype=int32), 'currentDistance': 23.074706327038534}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9282692807765419
{'scaleFactor': 20, 'currentTarget': array([ 5., 13.]), 'previousTarget': array([ 5., 13.]), 'currentState': array([ 4.45238756, 13.62305583,  2.16137717]), 'targetState': array([ 5, 13], dtype=int32), 'currentDistance': 0.8295046360202006}
episode index:2481
target Thresh 31.999999999570075
target distance 4.0
model initialize at round 2481
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 14.]), 'previousTarget': array([17., 14.]), 'currentState': array([17.34997841,  8.99503912,  0.37007492]), 'targetState': array([17, 14], dtype=int32), 'currentDistance': 5.017182307302847}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9282862145876715
{'scaleFactor': 20, 'currentTarget': array([17., 14.]), 'previousTarget': array([17., 14.]), 'currentState': array([16.87533458, 14.43513824,  1.87592968]), 'targetState': array([17, 14], dtype=int32), 'currentDistance': 0.45264418274540014}
episode index:2482
target Thresh 31.999999999574353
target distance 11.0
model initialize at round 2482
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 25.]), 'previousTarget': array([24., 25.]), 'currentState': array([13.56624749, 15.41514926,  6.06554127]), 'targetState': array([24, 25], dtype=int32), 'currentDistance': 14.168011688432632}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9282877365906193
{'scaleFactor': 20, 'currentTarget': array([24., 25.]), 'previousTarget': array([24., 25.]), 'currentState': array([23.18995714, 24.69173461,  0.87779568]), 'targetState': array([24, 25], dtype=int32), 'currentDistance': 0.8667162077378985}
episode index:2483
target Thresh 31.999999999578588
target distance 13.0
model initialize at round 2483
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 14.]), 'previousTarget': array([11., 14.]), 'currentState': array([22.59992606, 15.93390634,  3.56333113]), 'targetState': array([11, 14], dtype=int32), 'currentDistance': 11.76002884395499}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9282930475458568
{'scaleFactor': 20, 'currentTarget': array([11., 14.]), 'previousTarget': array([11., 14.]), 'currentState': array([10.87045179, 13.57436888,  3.34528179]), 'targetState': array([11, 14], dtype=int32), 'currentDistance': 0.4449096402317815}
episode index:2484
target Thresh 31.99999999958278
target distance 10.0
model initialize at round 2484
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 28.]), 'previousTarget': array([ 8., 28.]), 'currentState': array([ 8.20429305, 19.8178352 ,  1.76433054]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 8.184714803451891}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9283060467259188
{'scaleFactor': 20, 'currentTarget': array([ 8., 28.]), 'previousTarget': array([ 8., 28.]), 'currentState': array([ 7.69765711, 27.7652365 ,  1.70801041]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 0.3827860040647451}
episode index:2485
target Thresh 31.999999999586933
target distance 10.0
model initialize at round 2485
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([15.42009096, 11.09682241,  0.44352978]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 8.62731527787549}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9283190354480724
{'scaleFactor': 20, 'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([23.25861037, 12.27481281,  0.19288238]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 0.7906836732414881}
episode index:2486
target Thresh 31.99999999959104
target distance 8.0
model initialize at round 2486
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'previousTarget': array([11., 19.]), 'currentState': array([20.68187014, 15.93916498,  0.9738447 ]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 10.154177492689902}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9283243274118652
{'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'previousTarget': array([11., 19.]), 'currentState': array([10.88075572, 19.78977949,  1.49685677]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 0.7987307720363434}
episode index:2487
target Thresh 31.99999999959511
target distance 17.0
model initialize at round 2487
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  5.]), 'previousTarget': array([17.,  5.]), 'currentState': array([21.4637335 , 20.08193316,  4.59998497]), 'targetState': array([17,  5], dtype=int32), 'currentDistance': 15.728624370495957}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9283220847941064
{'scaleFactor': 20, 'currentTarget': array([17.,  5.]), 'previousTarget': array([17.,  5.]), 'currentState': array([17.22800734,  4.69688945,  4.40639402]), 'targetState': array([17,  5], dtype=int32), 'currentDistance': 0.37929322335811827}
episode index:2488
target Thresh 31.99999999959914
target distance 10.0
model initialize at round 2488
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 11.]), 'previousTarget': array([18., 11.]), 'currentState': array([25.02269773, 19.31718305,  3.7158761 ]), 'targetState': array([18, 11], dtype=int32), 'currentDistance': 10.885486543915547}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9283311920520839
{'scaleFactor': 20, 'currentTarget': array([18., 11.]), 'previousTarget': array([18., 11.]), 'currentState': array([18.80320589, 11.55564202,  4.24257345]), 'targetState': array([18, 11], dtype=int32), 'currentDistance': 0.9766666532460179}
episode index:2489
target Thresh 31.99999999960313
target distance 12.0
model initialize at round 2489
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 15.]), 'previousTarget': array([ 7., 15.]), 'currentState': array([13.66002679,  3.27694609,  1.17530983]), 'targetState': array([ 7, 15], dtype=int32), 'currentDistance': 13.482801999825563}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9283326917130699
{'scaleFactor': 20, 'currentTarget': array([ 7., 15.]), 'previousTarget': array([ 7., 15.]), 'currentState': array([ 6.51137956, 14.90008895,  2.19415244]), 'targetState': array([ 7, 15], dtype=int32), 'currentDistance': 0.49873054090135666}
episode index:2490
target Thresh 31.999999999607077
target distance 8.0
model initialize at round 2490
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 16.]), 'previousTarget': array([ 9., 16.]), 'currentState': array([2.91287171, 9.41406477, 6.27099198]), 'targetState': array([ 9, 16], dtype=int32), 'currentDistance': 8.968147728889523}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9283417874008206
{'scaleFactor': 20, 'currentTarget': array([ 9., 16.]), 'previousTarget': array([ 9., 16.]), 'currentState': array([ 9.62694374, 16.0568138 ,  6.06560678]), 'targetState': array([ 9, 16], dtype=int32), 'currentDistance': 0.6295127188008994}
episode index:2491
target Thresh 31.999999999610985
target distance 2.0
model initialize at round 2491
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 23.]), 'previousTarget': array([26., 23.]), 'currentState': array([25.11596577, 23.25977316,  6.11905128]), 'targetState': array([26, 23], dtype=int32), 'currentDistance': 0.921411206898769}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9283705427028267
{'scaleFactor': 20, 'currentTarget': array([26., 23.]), 'previousTarget': array([26., 23.]), 'currentState': array([25.11596577, 23.25977316,  6.11905128]), 'targetState': array([26, 23], dtype=int32), 'currentDistance': 0.921411206898769}
episode index:2492
target Thresh 31.999999999614857
target distance 9.0
model initialize at round 2492
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 16.]), 'previousTarget': array([24., 16.]), 'currentState': array([16.75685543, 19.05970035,  5.94170058]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 7.862881756286756}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9283834690836117
{'scaleFactor': 20, 'currentTarget': array([24., 16.]), 'previousTarget': array([24., 16.]), 'currentState': array([24.24303183, 16.26489371,  6.03024415]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 0.3594901227120689}
episode index:2493
target Thresh 31.99999999961869
target distance 8.0
model initialize at round 2493
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  7.]), 'previousTarget': array([12.,  7.]), 'currentState': array([5.55202031, 8.977921  , 0.08329719]), 'targetState': array([12,  7], dtype=int32), 'currentDistance': 6.744524711934584}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9284002756316937
{'scaleFactor': 20, 'currentTarget': array([12.,  7.]), 'previousTarget': array([12.,  7.]), 'currentState': array([11.29636595,  7.47540688,  6.08890982]), 'targetState': array([12,  7], dtype=int32), 'currentDistance': 0.8491834780945307}
episode index:2494
target Thresh 31.999999999622485
target distance 2.0
model initialize at round 2494
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([14.73563121, 21.51368253,  2.1284212 ]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 2.778521691847466}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9284249648999775
{'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([12.9917199 , 22.12704321,  3.49199523]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 0.9998241516965235}
episode index:2495
target Thresh 31.99999999962624
target distance 13.0
model initialize at round 2495
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 27.]), 'previousTarget': array([21., 27.]), 'currentState': array([ 6.63466255, 27.01600725,  4.77607369]), 'targetState': array([21, 27], dtype=int32), 'currentDistance': 14.365346371116805}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9284226891505896
{'scaleFactor': 20, 'currentTarget': array([21., 27.]), 'previousTarget': array([21., 27.]), 'currentState': array([21.7823216 , 26.93983641,  5.71202493]), 'targetState': array([21, 27], dtype=int32), 'currentDistance': 0.7846316017122943}
episode index:2496
target Thresh 31.99999999962996
target distance 14.0
model initialize at round 2496
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([ 9.55778535, 25.13258719,  4.69307399]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 17.418068846259732}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9284167198107149
{'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([21.30731895, 12.08126409,  5.49628769]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 0.31788172101212514}
episode index:2497
target Thresh 31.99999999963364
target distance 8.0
model initialize at round 2497
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([16.59585702, 20.36627526,  5.47988224]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 8.35070188838203}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9284296018324079
{'scaleFactor': 20, 'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([21.96224436, 14.46930738,  5.64882664]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 0.47082364122889436}
episode index:2498
target Thresh 31.999999999637286
target distance 23.0
model initialize at round 2498
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 25.]), 'previousTarget': array([16., 25.]), 'currentState': array([1.96543544, 0.31738496, 5.7018497 ]), 'targetState': array([16, 25], dtype=int32), 'currentDistance': 28.393669853791117}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.9284022423897543
{'scaleFactor': 20, 'currentTarget': array([16., 25.]), 'previousTarget': array([16., 25.]), 'currentState': array([16.02582213, 25.70123551,  0.85325465]), 'targetState': array([16, 25], dtype=int32), 'currentDistance': 0.7017107870002001}
episode index:2499
target Thresh 31.999999999640895
target distance 5.0
model initialize at round 2499
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([12.89739523, 19.70022757,  1.5749805 ]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 3.47911416225074}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9284229214927986
{'scaleFactor': 20, 'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([13.7251388 , 23.59714414,  1.40731642]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 0.6573658036273126}
episode index:2500
target Thresh 31.99999999964447
target distance 23.0
model initialize at round 2500
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  2.]), 'previousTarget': array([19.,  2.]), 'currentState': array([11.64928105, 24.66494761,  5.0727622 ]), 'targetState': array([19,  2], dtype=int32), 'currentDistance': 23.827146689061017}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9284061129962865
{'scaleFactor': 20, 'currentTarget': array([19.,  2.]), 'previousTarget': array([19.,  2.]), 'currentState': array([19.55750695,  2.1461502 ,  5.20736899]), 'targetState': array([19,  2], dtype=int32), 'currentDistance': 0.5763452762235263}
episode index:2501
target Thresh 31.999999999648008
target distance 8.0
model initialize at round 2501
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  5.]), 'previousTarget': array([27.,  5.]), 'currentState': array([19.40382319,  4.36619618,  5.96469998]), 'targetState': array([27,  5], dtype=int32), 'currentDistance': 7.622572356999792}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9284189786625549
{'scaleFactor': 20, 'currentTarget': array([27.,  5.]), 'previousTarget': array([27.,  5.]), 'currentState': array([27.1919819 ,  5.53881945,  0.21251205]), 'targetState': array([27,  5], dtype=int32), 'currentDistance': 0.5719995160878498}
episode index:2502
target Thresh 31.999999999651507
target distance 25.0
model initialize at round 2502
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5.63776222, 27.062335  ]), 'previousTarget': array([ 5., 28.]), 'currentState': array([22.50977668,  2.25638555,  0.55234402]), 'targetState': array([ 5, 28], dtype=int32), 'currentDistance': 29.999999999999996}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.9283848293276453
{'scaleFactor': 20, 'currentTarget': array([ 5., 28.]), 'previousTarget': array([ 5., 28.]), 'currentState': array([ 5.42006944, 28.60705971,  0.45396638]), 'targetState': array([ 5, 28], dtype=int32), 'currentDistance': 0.7382274855652171}
episode index:2503
target Thresh 31.999999999654975
target distance 2.0
model initialize at round 2503
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  3.]), 'previousTarget': array([10.,  3.]), 'currentState': array([10.17068065,  3.10891501,  4.24147689]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 0.2024706477462981}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9284134296354218
{'scaleFactor': 20, 'currentTarget': array([10.,  3.]), 'previousTarget': array([10.,  3.]), 'currentState': array([10.17068065,  3.10891501,  4.24147689]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 0.2024706477462981}
episode index:2504
target Thresh 31.99999999965841
target distance 11.0
model initialize at round 2504
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 28.]), 'previousTarget': array([21., 28.]), 'currentState': array([11.68235955, 26.07768895,  0.49022794]), 'targetState': array([21, 28], dtype=int32), 'currentDistance': 9.513868999062508}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9284224422582821
{'scaleFactor': 20, 'currentTarget': array([21., 28.]), 'previousTarget': array([21., 28.]), 'currentState': array([21.20365685, 28.59082479,  0.13668055]), 'targetState': array([21, 28], dtype=int32), 'currentDistance': 0.6249400281589585}
episode index:2505
target Thresh 31.999999999661807
target distance 13.0
model initialize at round 2505
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'previousTarget': array([25.,  3.]), 'currentState': array([23.96439004, 14.6207466 ,  4.31258565]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 11.666800740979143}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9284276528357531
{'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'previousTarget': array([25.,  3.]), 'currentState': array([25.35509625,  2.9048586 ,  4.87866327]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 0.3676210425177568}
episode index:2506
target Thresh 31.999999999665174
target distance 14.0
model initialize at round 2506
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 16.]), 'previousTarget': array([18., 16.]), 'currentState': array([14.83106827,  3.95160133,  1.30136579]), 'targetState': array([18, 16], dtype=int32), 'currentDistance': 12.45817157143593}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9284328592564014
{'scaleFactor': 20, 'currentTarget': array([18., 16.]), 'previousTarget': array([18., 16.]), 'currentState': array([17.5693917 , 15.59078036,  1.62077452]), 'targetState': array([18, 16], dtype=int32), 'currentDistance': 0.5940405886517474}
episode index:2507
target Thresh 31.999999999668503
target distance 9.0
model initialize at round 2507
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([10.01861335,  5.63278812,  3.07981539]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 8.027017170730804}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9284456834791858
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.03970017, 5.39185841, 3.36748206]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.609436050340783}
episode index:2508
target Thresh 31.999999999671804
target distance 17.0
model initialize at round 2508
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 15.]), 'previousTarget': array([22., 15.]), 'currentState': array([6.64770806, 9.65729572, 0.80493515]), 'targetState': array([22, 15], dtype=int32), 'currentDistance': 16.2553793149183}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9284434112635416
{'scaleFactor': 20, 'currentTarget': array([22., 15.]), 'previousTarget': array([22., 15.]), 'currentState': array([21.34363884, 15.23626561,  0.33074105]), 'targetState': array([22, 15], dtype=int32), 'currentDistance': 0.6975897202684156}
episode index:2509
target Thresh 31.99999999967507
target distance 7.0
model initialize at round 2509
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 17.]), 'previousTarget': array([15., 17.]), 'currentState': array([14.12391884, 22.32159349,  5.79607844]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 5.393224964399267}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9284600867969027
{'scaleFactor': 20, 'currentTarget': array([15., 17.]), 'previousTarget': array([15., 17.]), 'currentState': array([15.28104496, 16.71033191,  4.68819091]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 0.40360113240937723}
episode index:2510
target Thresh 31.999999999678302
target distance 9.0
model initialize at round 2510
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 19.]), 'previousTarget': array([ 9., 19.]), 'currentState': array([ 6.03906635, 29.38166371,  3.1884892 ]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 10.795650489383947}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9284652720070198
{'scaleFactor': 20, 'currentTarget': array([ 9., 19.]), 'previousTarget': array([ 9., 19.]), 'currentState': array([ 9.55114586, 18.97652404,  5.16823137]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 0.5516456078385084}
episode index:2511
target Thresh 31.999999999681503
target distance 11.0
model initialize at round 2511
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 28.]), 'previousTarget': array([16., 28.]), 'currentState': array([ 3.95489187, 27.68085744,  5.05238342]), 'targetState': array([16, 28], dtype=int32), 'currentDistance': 12.049335327069391}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9284704530887848
{'scaleFactor': 20, 'currentTarget': array([16., 28.]), 'previousTarget': array([16., 28.]), 'currentState': array([15.36811181, 28.27533589,  0.26179289]), 'targetState': array([16, 28], dtype=int32), 'currentDistance': 0.6892695683837329}
episode index:2512
target Thresh 31.999999999684672
target distance 20.0
model initialize at round 2512
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 27.]), 'previousTarget': array([ 8., 27.]), 'currentState': array([8.48835339, 6.21438414, 0.52432745]), 'targetState': array([ 8, 27], dtype=int32), 'currentDistance': 20.791351942047687}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9284572687677223
{'scaleFactor': 20, 'currentTarget': array([ 8., 27.]), 'previousTarget': array([ 8., 27.]), 'currentState': array([ 7.95240389, 27.50540467,  1.10233713]), 'targetState': array([ 8, 27], dtype=int32), 'currentDistance': 0.5076408842216733}
episode index:2513
target Thresh 31.99999999968781
target distance 3.0
model initialize at round 2513
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([20.73296238, 14.0185895 ,  4.62391136]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 1.2548937909860693}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9284817487721901
{'scaleFactor': 20, 'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([19.49534573, 12.62377729,  3.33648441]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 0.6294596553441411}
episode index:2514
target Thresh 31.999999999690917
target distance 8.0
model initialize at round 2514
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  7.]), 'previousTarget': array([13.,  7.]), 'currentState': array([ 3.3206634 , 11.88947094,  4.21731496]), 'targetState': array([13,  7], dtype=int32), 'currentDistance': 10.84419121375162}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9284869171223407
{'scaleFactor': 20, 'currentTarget': array([13.,  7.]), 'previousTarget': array([13.,  7.]), 'currentState': array([13.33897572,  6.92423857,  5.51900487]), 'targetState': array([13,  7], dtype=int32), 'currentDistance': 0.34733892750068474}
episode index:2515
target Thresh 31.99999999969399
target distance 10.0
model initialize at round 2515
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  5.]), 'previousTarget': array([17.,  5.]), 'currentState': array([ 5.65475838, 16.01129278,  3.50696898]), 'targetState': array([17,  5], dtype=int32), 'currentDistance': 15.810220616040752}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9284846348398706
{'scaleFactor': 20, 'currentTarget': array([17.,  5.]), 'previousTarget': array([17.,  5.]), 'currentState': array([16.29065476,  5.76694212,  5.91006016]), 'targetState': array([17,  5], dtype=int32), 'currentDistance': 1.0446869819177884}
episode index:2516
target Thresh 31.999999999697035
target distance 15.0
model initialize at round 2516
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 25.]), 'previousTarget': array([ 8., 25.]), 'currentState': array([21.00362901, 23.98363043,  3.25423066]), 'targetState': array([ 8, 25], dtype=int32), 'currentDistance': 13.04328848925998}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9284860574513396
{'scaleFactor': 20, 'currentTarget': array([ 8., 25.]), 'previousTarget': array([ 8., 25.]), 'currentState': array([ 7.91749134, 25.75596935,  1.36817384]), 'targetState': array([ 8, 25], dtype=int32), 'currentDistance': 0.7604586327681139}
episode index:2517
target Thresh 31.99999999970005
target distance 15.0
model initialize at round 2517
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([13.46709707, 22.24986207,  3.68482399]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 17.522932410585305}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9284801127293507
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.7183744 , 8.86641118, 3.59862502]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.3117033084120264}
episode index:2518
target Thresh 31.999999999703036
target distance 20.0
model initialize at round 2518
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'previousTarget': array([7., 3.]), 'currentState': array([25.31594404, 13.93772519,  3.86951029]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 21.33325193900925}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9284669559772782
{'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'previousTarget': array([7., 3.]), 'currentState': array([6.52748515, 2.86065263, 3.05949843]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 0.49263371180608173}
episode index:2519
target Thresh 31.99999999970599
target distance 14.0
model initialize at round 2519
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([ 3.96531489, 13.64561027,  6.00912009]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 13.747646395830936}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9284683839105837
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.34121777,  7.33973383,  5.82290919]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.48150664213968597}
episode index:2520
target Thresh 31.999999999708916
target distance 11.0
model initialize at round 2520
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([25.04378165, 19.68240044,  2.55477887]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 12.419935104747893}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9284698107110584
{'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([13.24292247, 14.22465174,  2.41050496]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 0.7897055056165267}
episode index:2521
target Thresh 31.99999999971181
target distance 18.0
model initialize at round 2521
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 22.]), 'previousTarget': array([23., 22.]), 'currentState': array([ 6.97955288, 17.06067162,  0.27540728]), 'targetState': array([23, 22], dtype=int32), 'currentDistance': 16.764596347359728}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9284675406411601
{'scaleFactor': 20, 'currentTarget': array([23., 22.]), 'previousTarget': array([23., 22.]), 'currentState': array([22.06389321, 22.15098404,  0.3753302 ]), 'targetState': array([23, 22], dtype=int32), 'currentDistance': 0.9482046764797208}
episode index:2522
target Thresh 31.999999999714678
target distance 12.0
model initialize at round 2522
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 28.]), 'previousTarget': array([ 9., 28.]), 'currentState': array([21.99335173, 25.3585435 ,  1.94944352]), 'targetState': array([ 9, 28], dtype=int32), 'currentDistance': 13.25912823463664}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9284689666448328
{'scaleFactor': 20, 'currentTarget': array([ 9., 28.]), 'previousTarget': array([ 9., 28.]), 'currentState': array([ 8.87618252, 27.70649115,  2.95406776]), 'targetState': array([ 9, 28], dtype=int32), 'currentDistance': 0.31855645143382805}
episode index:2523
target Thresh 31.999999999717517
target distance 22.0
model initialize at round 2523
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([ 3.77030096, 12.9842726 ,  0.84455865]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 20.253629697884296}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9284594235023462
{'scaleFactor': 20, 'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([23.30285919, 12.66154753,  6.13363535]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 0.9610673461683167}
episode index:2524
target Thresh 31.99999999972033
target distance 8.0
model initialize at round 2524
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 24.]), 'previousTarget': array([11., 24.]), 'currentState': array([ 9.74069237, 17.97477434,  1.54213104]), 'targetState': array([11, 24], dtype=int32), 'currentDistance': 6.155420377910365}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9284759936316523
{'scaleFactor': 20, 'currentTarget': array([11., 24.]), 'previousTarget': array([11., 24.]), 'currentState': array([10.64135024, 23.88229477,  1.61892678]), 'targetState': array([11, 24], dtype=int32), 'currentDistance': 0.3774707530753534}
episode index:2525
target Thresh 31.999999999723112
target distance 3.0
model initialize at round 2525
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([11.30883929, 12.05798295,  1.68980366]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 5.283687702690979}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9284925506412993
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([7.94829055, 9.36909099, 4.23365153]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 1.0175869126553407}
episode index:2526
target Thresh 31.999999999725866
target distance 11.0
model initialize at round 2526
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([10.67923026, 10.1121331 ,  1.0766775 ]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 12.58958997938528}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9284976901738515
{'scaleFactor': 20, 'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([16.3878847 , 20.55522215,  1.1054748 ]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 0.7566455442109409}
episode index:2527
target Thresh 31.999999999728594
target distance 19.0
model initialize at round 2527
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([11.48195441,  4.91805597,  1.58642608]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 17.08979763542422}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9284917643658254
{'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([12.35038555, 22.71404339,  1.02015014]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 0.7953791513604136}
episode index:2528
target Thresh 31.999999999731294
target distance 11.0
model initialize at round 2528
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 27.]), 'previousTarget': array([ 5., 27.]), 'currentState': array([11.82911857, 17.2121082 ,  1.33277583]), 'targetState': array([ 5, 27], dtype=int32), 'currentDistance': 11.934809862390514}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9284969001448032
{'scaleFactor': 20, 'currentTarget': array([ 5., 27.]), 'previousTarget': array([ 5., 27.]), 'currentState': array([ 5.0801284 , 26.67876477,  2.20127992]), 'targetState': array([ 5, 27], dtype=int32), 'currentDistance': 0.33107798768119406}
episode index:2529
target Thresh 31.99999999973397
target distance 13.0
model initialize at round 2529
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 18.]), 'previousTarget': array([12., 18.]), 'currentState': array([25.75946018, 27.50186827,  2.11261747]), 'targetState': array([12, 18], dtype=int32), 'currentDistance': 16.72149052120574}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9284909793334746
{'scaleFactor': 20, 'currentTarget': array([12., 18.]), 'previousTarget': array([12., 18.]), 'currentState': array([11.6306709 , 17.64830701,  3.66491867]), 'targetState': array([12, 18], dtype=int32), 'currentDistance': 0.5099921063136232}
episode index:2530
target Thresh 31.999999999736616
target distance 13.0
model initialize at round 2530
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([5.22722936, 5.15164933, 6.02682287]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 11.828965494152703}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9284961113643191
{'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([17.06752646,  4.20751635,  6.15908019]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 0.2182266253288282}
episode index:2531
target Thresh 31.999999999739238
target distance 13.0
model initialize at round 2531
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 19.]), 'previousTarget': array([ 4., 19.]), 'currentState': array([15.18548921, 15.59661748,  3.35213137]), 'targetState': array([ 4, 19], dtype=int32), 'currentDistance': 11.691799746926204}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9285012393414267
{'scaleFactor': 20, 'currentTarget': array([ 4., 19.]), 'previousTarget': array([ 4., 19.]), 'currentState': array([ 3.89917333, 18.90784905,  2.77818533]), 'targetState': array([ 4, 19], dtype=int32), 'currentDistance': 0.1365936122611372}
episode index:2532
target Thresh 31.99999999974183
target distance 4.0
model initialize at round 2532
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 15.]), 'previousTarget': array([ 9., 15.]), 'currentState': array([ 8.00172612, 12.68296992,  0.55977201]), 'targetState': array([ 9, 15], dtype=int32), 'currentDistance': 2.5229306614014044}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9285255183626104
{'scaleFactor': 20, 'currentTarget': array([ 9., 15.]), 'previousTarget': array([ 9., 15.]), 'currentState': array([ 9.22867648, 14.21394264,  1.23738581]), 'targetState': array([ 9, 15], dtype=int32), 'currentDistance': 0.8186446728958466}
episode index:2533
target Thresh 31.9999999997444
target distance 18.0
model initialize at round 2533
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([ 3.80964012, 25.68174754,  5.39480025]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 17.92029570692022}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9285195956037788
{'scaleFactor': 20, 'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([19.97710242, 18.20368652,  5.84953288]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 0.20496950531382271}
episode index:2534
target Thresh 31.999999999746944
target distance 21.0
model initialize at round 2534
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 5.]), 'previousTarget': array([9., 5.]), 'currentState': array([27.05923697, 24.31807281,  3.73759413]), 'targetState': array([9, 5], dtype=int32), 'currentDistance': 26.44473439218565}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9284994778236587
{'scaleFactor': 20, 'currentTarget': array([9., 5.]), 'previousTarget': array([9., 5.]), 'currentState': array([9.95866884, 5.30571676, 3.93471859]), 'targetState': array([9, 5], dtype=int32), 'currentDistance': 1.0062348998886157}
episode index:2535
target Thresh 31.999999999749463
target distance 5.0
model initialize at round 2535
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([8.44891968, 9.14383422, 0.47629898]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 3.5539920832770417}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9285198250327189
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([12.38237783,  9.48645354,  0.04615251]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.6187486162196397}
episode index:2536
target Thresh 31.999999999751953
target distance 7.0
model initialize at round 2536
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 23.]), 'previousTarget': array([21., 23.]), 'currentState': array([12.97506266, 14.6651249 ,  5.06758332]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 11.570210121336677}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9285249335563168
{'scaleFactor': 20, 'currentTarget': array([21., 23.]), 'previousTarget': array([21., 23.]), 'currentState': array([20.17983879, 22.54676264,  1.02675766]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 0.937063776284084}
episode index:2537
target Thresh 31.999999999754422
target distance 16.0
model initialize at round 2537
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 10.]), 'previousTarget': array([20., 10.]), 'currentState': array([ 9.05695237, 24.60606698,  5.12758398]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 18.250684485226213}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9285190203624347
{'scaleFactor': 20, 'currentTarget': array([20., 10.]), 'previousTarget': array([20., 10.]), 'currentState': array([20.0729392 , 10.56326256,  5.39282313]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 0.5679655271460768}
episode index:2538
target Thresh 31.999999999756866
target distance 16.0
model initialize at round 2538
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  4.]), 'previousTarget': array([19.,  4.]), 'currentState': array([4.97833813, 4.22561482, 0.27813437]), 'targetState': array([19,  4], dtype=int32), 'currentDistance': 14.023476877309767}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9285204171042798
{'scaleFactor': 20, 'currentTarget': array([19.,  4.]), 'previousTarget': array([19.,  4.]), 'currentState': array([18.92036678,  4.44471353,  0.19818518]), 'targetState': array([19,  4], dtype=int32), 'currentDistance': 0.4517870909145954}
episode index:2539
target Thresh 31.999999999759286
target distance 20.0
model initialize at round 2539
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 22.]), 'previousTarget': array([ 7., 22.]), 'currentState': array([10.09727525,  3.96273551,  1.84875238]), 'targetState': array([ 7, 22], dtype=int32), 'currentDistance': 18.301257448000023}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9285145103445867
{'scaleFactor': 20, 'currentTarget': array([ 7., 22.]), 'previousTarget': array([ 7., 22.]), 'currentState': array([ 6.77428621, 21.53209422,  1.85690128]), 'targetState': array([ 7, 22], dtype=int32), 'currentDistance': 0.5195021963193263}
episode index:2540
target Thresh 31.99999999976168
target distance 6.0
model initialize at round 2540
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 22.]), 'previousTarget': array([22., 22.]), 'currentState': array([17.75379392, 27.16637271,  5.41568816]), 'targetState': array([22, 22], dtype=int32), 'currentDistance': 6.687426487034463}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9285309544570052
{'scaleFactor': 20, 'currentTarget': array([22., 22.]), 'previousTarget': array([22., 22.]), 'currentState': array([21.8016223 , 22.76083313,  5.68836612]), 'targetState': array([22, 22], dtype=int32), 'currentDistance': 0.7862701578871815}
episode index:2541
target Thresh 31.99999999976405
target distance 16.0
model initialize at round 2541
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 24.]), 'previousTarget': array([22., 24.]), 'currentState': array([4.66836768, 9.97085123, 4.8095547 ]), 'targetState': array([22, 24], dtype=int32), 'currentDistance': 22.298037898977654}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9285143745660763
{'scaleFactor': 20, 'currentTarget': array([22., 24.]), 'previousTarget': array([22., 24.]), 'currentState': array([21.6375274 , 24.2421976 ,  0.72374706]), 'targetState': array([22, 24], dtype=int32), 'currentDistance': 0.43594272716439036}
episode index:2542
target Thresh 31.9999999997664
target distance 18.0
model initialize at round 2542
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'previousTarget': array([11., 21.]), 'currentState': array([13.68107097,  3.07992795,  1.05751007]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 18.119523274907962}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9285084771507863
{'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'previousTarget': array([11., 21.]), 'currentState': array([10.73780761, 20.50772235,  1.91530118]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 0.5577473782315355}
episode index:2543
target Thresh 31.999999999768722
target distance 21.0
model initialize at round 2543
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 17.]), 'previousTarget': array([27., 17.]), 'currentState': array([6.5996876 , 7.4274979 , 6.08672285]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 22.534541095037266}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9284954385411589
{'scaleFactor': 20, 'currentTarget': array([27., 17.]), 'previousTarget': array([27., 17.]), 'currentState': array([26.06468751, 16.98681816,  0.39808376]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 0.9354053725221186}
episode index:2544
target Thresh 31.999999999771024
target distance 16.0
model initialize at round 2544
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([ 6.94115715, 28.35891386,  6.18369678]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 14.255368702252232}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9284968412560375
{'scaleFactor': 20, 'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([20.73373638, 26.21151387,  0.05860061]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 0.3400506339329526}
episode index:2545
target Thresh 31.999999999773305
target distance 22.0
model initialize at round 2545
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 24.]), 'previousTarget': array([11., 24.]), 'currentState': array([2.33308613, 3.95944725, 1.59775335]), 'targetState': array([11, 24], dtype=int32), 'currentDistance': 21.834357113037132}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9284838174591021
{'scaleFactor': 20, 'currentTarget': array([11., 24.]), 'previousTarget': array([11., 24.]), 'currentState': array([10.61973029, 23.95000874,  1.25259409]), 'targetState': array([11, 24], dtype=int32), 'currentDistance': 0.38354162753199167}
episode index:2546
target Thresh 31.999999999775557
target distance 18.0
model initialize at round 2546
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 23.]), 'previousTarget': array([27., 23.]), 'currentState': array([22.39061513,  6.95786454,  1.47813086]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 16.691211432313715}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9284815641716928
{'scaleFactor': 20, 'currentTarget': array([27., 23.]), 'previousTarget': array([27., 23.]), 'currentState': array([26.29515587, 22.43514082,  1.49573017]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 0.903255854758098}
episode index:2547
target Thresh 31.999999999777792
target distance 16.0
model initialize at round 2547
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 12.]), 'previousTarget': array([ 3., 12.]), 'currentState': array([20.14683746, 12.23172721,  1.83107251]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 17.14840320603982}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9284756912059596
{'scaleFactor': 20, 'currentTarget': array([ 3., 12.]), 'previousTarget': array([ 3., 12.]), 'currentState': array([ 2.76031405, 11.46378525,  3.33234388]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 0.5873462500382485}
episode index:2548
target Thresh 31.99999999978
target distance 2.0
model initialize at round 2548
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 27.]), 'previousTarget': array([ 9., 27.]), 'currentState': array([ 8.17153791, 26.61252674,  1.08710164]), 'targetState': array([ 9, 27], dtype=int32), 'currentDistance': 0.9145955180370859}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9285037509583308
{'scaleFactor': 20, 'currentTarget': array([ 9., 27.]), 'previousTarget': array([ 9., 27.]), 'currentState': array([ 8.17153791, 26.61252674,  1.08710164]), 'targetState': array([ 9, 27], dtype=int32), 'currentDistance': 0.9145955180370859}
episode index:2549
target Thresh 31.999999999782194
target distance 6.0
model initialize at round 2549
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 14.]), 'previousTarget': array([ 6., 14.]), 'currentState': array([13.28629175, 13.08528412,  1.71084326]), 'targetState': array([ 6, 14], dtype=int32), 'currentDistance': 7.343483683491972}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9285163361579549
{'scaleFactor': 20, 'currentTarget': array([ 6., 14.]), 'previousTarget': array([ 6., 14.]), 'currentState': array([ 5.87800023, 13.57669351,  3.29320843]), 'targetState': array([ 6, 14], dtype=int32), 'currentDistance': 0.44053640971058733}
episode index:2550
target Thresh 31.99999999978436
target distance 5.0
model initialize at round 2550
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([6.3311064 , 8.4554202 , 3.29408979]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 4.168211340689695}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9285365571159487
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.4297948 , 5.28923307, 4.58734266]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.5180534156404023}
episode index:2551
target Thresh 31.999999999786503
target distance 8.0
model initialize at round 2551
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([18.29657892, 16.02288076,  2.40068653]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 8.681754464280285}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9285491195974863
{'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([12.50013947, 21.48225974,  2.71695272]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 0.7198572585876138}
episode index:2552
target Thresh 31.999999999788628
target distance 5.0
model initialize at round 2552
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([3.66140121, 8.21261356, 3.77078629]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 3.6167858487257982}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9285693118733981
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.12227494, 4.62502239, 4.29748863]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.3944101552528353}
episode index:2553
target Thresh 31.99999999979073
target distance 19.0
model initialize at round 2553
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 28.]), 'previousTarget': array([ 8., 28.]), 'currentState': array([25.31125308, 14.6627343 ,  2.00248575]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 21.8531951818521}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9285563004961017
{'scaleFactor': 20, 'currentTarget': array([ 8., 28.]), 'previousTarget': array([ 8., 28.]), 'currentState': array([ 7.91110543, 27.22885299,  2.21602696]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 0.7762537916802346}
episode index:2554
target Thresh 31.999999999792816
target distance 10.0
model initialize at round 2554
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 19.]), 'previousTarget': array([ 5., 19.]), 'currentState': array([13.60843912, 25.05314496,  4.74876797]), 'targetState': array([ 5, 19], dtype=int32), 'currentDistance': 10.523582468601173}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9285650808285495
{'scaleFactor': 20, 'currentTarget': array([ 5., 19.]), 'previousTarget': array([ 5., 19.]), 'currentState': array([ 5.8134819 , 19.31941514,  3.89485893]), 'targetState': array([ 5, 19], dtype=int32), 'currentDistance': 0.873944412081783}
episode index:2555
target Thresh 31.999999999794877
target distance 14.0
model initialize at round 2555
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 26.]), 'previousTarget': array([ 6., 26.]), 'currentState': array([18.1778531 , 28.58539879,  4.00559497]), 'targetState': array([ 6, 26], dtype=int32), 'currentDistance': 12.449272793449618}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9285701336722789
{'scaleFactor': 20, 'currentTarget': array([ 6., 26.]), 'previousTarget': array([ 6., 26.]), 'currentState': array([ 6.67563218, 25.73391486,  3.37838416]), 'targetState': array([ 6, 26], dtype=int32), 'currentDistance': 0.7261405775998165}
episode index:2556
target Thresh 31.999999999796916
target distance 11.0
model initialize at round 2556
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 13.]), 'previousTarget': array([ 9., 13.]), 'currentState': array([18.88348052, 14.74072547,  2.97700596]), 'targetState': array([ 9, 13], dtype=int32), 'currentDistance': 10.03560224197903}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9285789017271197
{'scaleFactor': 20, 'currentTarget': array([ 9., 13.]), 'previousTarget': array([ 9., 13.]), 'currentState': array([ 9.24433336, 12.48320027,  3.31333131]), 'targetState': array([ 9, 13], dtype=int32), 'currentDistance': 0.5716474004205881}
episode index:2557
target Thresh 31.999999999798938
target distance 13.0
model initialize at round 2557
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 5.]), 'previousTarget': array([7., 5.]), 'currentState': array([ 8.16142885, 16.3244557 ,  3.79899466]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 11.383857602170258}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9285839452172189
{'scaleFactor': 20, 'currentTarget': array([7., 5.]), 'previousTarget': array([7., 5.]), 'currentState': array([7.23566533, 4.7430811 , 4.68735781]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 0.3486337165959899}
episode index:2558
target Thresh 31.999999999800938
target distance 6.0
model initialize at round 2558
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'previousTarget': array([ 5., 22.]), 'currentState': array([10.01551787, 22.63501538,  3.07754946]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 5.055557735015437}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.928600246528193
{'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'previousTarget': array([ 5., 22.]), 'currentState': array([ 4.12399464, 21.92810557,  2.89498836]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 0.8789506250561069}
episode index:2559
target Thresh 31.99999999980292
target distance 19.0
model initialize at round 2559
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 7.]), 'previousTarget': array([7., 7.]), 'currentState': array([24.44613148,  5.35356307,  2.52583075]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 17.523648538816698}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9285943547316912
{'scaleFactor': 20, 'currentTarget': array([7., 7.]), 'previousTarget': array([7., 7.]), 'currentState': array([6.66206394, 6.54399019, 3.09977153]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 0.5675788278754741}
episode index:2560
target Thresh 31.99999999980488
target distance 9.0
model initialize at round 2560
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 19.]), 'previousTarget': array([16., 19.]), 'currentState': array([23.09661308, 15.75900683,  2.76525348]), 'targetState': array([16, 19], dtype=int32), 'currentDistance': 7.801663534167602}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9286068504971219
{'scaleFactor': 20, 'currentTarget': array([16., 19.]), 'previousTarget': array([16., 19.]), 'currentState': array([15.72075894, 18.80525158,  2.91885624]), 'targetState': array([16, 19], dtype=int32), 'currentDistance': 0.34044458536471217}
episode index:2561
target Thresh 31.99999999980682
target distance 14.0
model initialize at round 2561
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 28.]), 'previousTarget': array([24., 28.]), 'currentState': array([19.92188969, 15.40801544,  2.00109334]), 'targetState': array([24, 28], dtype=int32), 'currentDistance': 13.235900378146821}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9286082004180469
{'scaleFactor': 20, 'currentTarget': array([24., 28.]), 'previousTarget': array([24., 28.]), 'currentState': array([23.98044414, 28.6031787 ,  1.43747589]), 'targetState': array([24, 28], dtype=int32), 'currentDistance': 0.6034956282363539}
episode index:2562
target Thresh 31.999999999808743
target distance 10.0
model initialize at round 2562
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 26.]), 'previousTarget': array([12., 26.]), 'currentState': array([20.14506238, 23.72530707,  2.92835113]), 'targetState': array([12, 26], dtype=int32), 'currentDistance': 8.45672921656216}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9286206810304471
{'scaleFactor': 20, 'currentTarget': array([12., 26.]), 'previousTarget': array([12., 26.]), 'currentState': array([12.46516778, 25.87804232,  3.00918192]), 'targetState': array([12, 26], dtype=int32), 'currentDistance': 0.48088952686317404}
episode index:2563
target Thresh 31.999999999810647
target distance 8.0
model initialize at round 2563
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 24.]), 'previousTarget': array([10., 24.]), 'currentState': array([17.61984055, 22.63947152,  2.80864882]), 'targetState': array([10, 24], dtype=int32), 'currentDistance': 7.740349331461592}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9286331519075803
{'scaleFactor': 20, 'currentTarget': array([10., 24.]), 'previousTarget': array([10., 24.]), 'currentState': array([ 9.74384981, 23.9122235 ,  3.19301886]), 'targetState': array([10, 24], dtype=int32), 'currentDistance': 0.2707722902318678}
episode index:2564
target Thresh 31.99999999981253
target distance 15.0
model initialize at round 2564
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 20.]), 'previousTarget': array([ 4., 20.]), 'currentState': array([18.37889248, 21.56416544,  2.95878774]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 14.463718831834742}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9286344899956893
{'scaleFactor': 20, 'currentTarget': array([ 4., 20.]), 'previousTarget': array([ 4., 20.]), 'currentState': array([ 4.69168914, 19.57040521,  3.04979553]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 0.8142392488734249}
episode index:2565
target Thresh 31.999999999814396
target distance 13.0
model initialize at round 2565
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 16.]), 'previousTarget': array([13., 16.]), 'currentState': array([24.34903021, 16.32662952,  3.95627391]), 'targetState': array([13, 16], dtype=int32), 'currentDistance': 11.35372950057282}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9286394960983413
{'scaleFactor': 20, 'currentTarget': array([13., 16.]), 'previousTarget': array([13., 16.]), 'currentState': array([12.56714496, 15.64719332,  3.37911068]), 'targetState': array([13, 16], dtype=int32), 'currentDistance': 0.5584228120316789}
episode index:2566
target Thresh 31.999999999816243
target distance 11.0
model initialize at round 2566
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 7.]), 'previousTarget': array([6., 7.]), 'currentState': array([ 7.79206679, 16.82812737,  4.92183924]), 'targetState': array([6, 7], dtype=int32), 'currentDistance': 9.990174725095311}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9286482029755527
{'scaleFactor': 20, 'currentTarget': array([6., 7.]), 'previousTarget': array([6., 7.]), 'currentState': array([6.49345959, 6.97772005, 4.65294453]), 'targetState': array([6, 7], dtype=int32), 'currentDistance': 0.4939623079874182}
episode index:2567
target Thresh 31.999999999818073
target distance 18.0
model initialize at round 2567
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  5.]), 'previousTarget': array([21.,  5.]), 'currentState': array([4.26508672, 3.89007042, 0.28983801]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 16.771680477335863}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9286459041015077
{'scaleFactor': 20, 'currentTarget': array([21.,  5.]), 'previousTarget': array([21.,  5.]), 'currentState': array([20.08079722,  5.46955699,  6.23828088]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 1.0321906398976692}
episode index:2568
target Thresh 31.99999999981988
target distance 10.0
model initialize at round 2568
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 7.]), 'previousTarget': array([7., 7.]), 'currentState': array([16.03975385, 15.2614408 ,  4.00141692]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 12.246164862463525}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9286508999151704
{'scaleFactor': 20, 'currentTarget': array([7., 7.]), 'previousTarget': array([7., 7.]), 'currentState': array([7.37479066, 7.02664483, 4.01938865]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 0.37573658985168634}
episode index:2569
target Thresh 31.999999999821675
target distance 15.0
model initialize at round 2569
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([ 4.92758017, 26.70296346,  5.03150511]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 16.08778532693192}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9286486017807395
{'scaleFactor': 20, 'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([20.34136822, 26.0697408 ,  0.09637575]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 0.6623138190697069}
episode index:2570
target Thresh 31.999999999823448
target distance 21.0
model initialize at round 2570
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  7.]), 'previousTarget': array([11.,  7.]), 'currentState': array([16.23846889, 26.24101002,  4.81175423]), 'targetState': array([11,  7], dtype=int32), 'currentDistance': 19.94136462243179}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9286391632250134
{'scaleFactor': 20, 'currentTarget': array([11.,  7.]), 'previousTarget': array([11.,  7.]), 'currentState': array([10.92683037,  7.11789636,  4.54942113]), 'targetState': array([11,  7], dtype=int32), 'currentDistance': 0.13875642528075796}
episode index:2571
target Thresh 31.999999999825206
target distance 18.0
model initialize at round 2571
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 15.]), 'previousTarget': array([ 7., 15.]), 'currentState': array([23.33176448, 23.77778763,  4.28401518]), 'targetState': array([ 7, 15], dtype=int32), 'currentDistance': 18.541199712757965}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9286332837865446
{'scaleFactor': 20, 'currentTarget': array([ 7., 15.]), 'previousTarget': array([ 7., 15.]), 'currentState': array([ 7.64509204, 15.35766481,  3.68234077]), 'targetState': array([ 7, 15], dtype=int32), 'currentDistance': 0.7376095596306881}
episode index:2572
target Thresh 31.999999999826944
target distance 17.0
model initialize at round 2572
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 24.]), 'previousTarget': array([ 8., 24.]), 'currentState': array([23.98819205, 14.34485418,  3.22580302]), 'targetState': array([ 8, 24], dtype=int32), 'currentDistance': 18.67736935354442}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9286274089181796
{'scaleFactor': 20, 'currentTarget': array([ 8., 24.]), 'previousTarget': array([ 8., 24.]), 'currentState': array([ 8.64367522, 23.36065941,  2.5442968 ]), 'targetState': array([ 8, 24], dtype=int32), 'currentDistance': 0.9072343533842555}
episode index:2573
target Thresh 31.999999999828667
target distance 18.0
model initialize at round 2573
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 28.]), 'previousTarget': array([25., 28.]), 'currentState': array([20.76219443, 11.83530255,  1.37468293]), 'targetState': array([25, 28], dtype=int32), 'currentDistance': 16.710967642936005}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9286251234813148
{'scaleFactor': 20, 'currentTarget': array([25., 28.]), 'previousTarget': array([25., 28.]), 'currentState': array([24.3646929 , 27.34953286,  1.25164806]), 'targetState': array([25, 28], dtype=int32), 'currentDistance': 0.9092428761631499}
episode index:2574
target Thresh 31.999999999830372
target distance 14.0
model initialize at round 2574
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([ 9.9981878 , 16.07220445,  6.27986934]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 12.179389452176208}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9286301157243902
{'scaleFactor': 20, 'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([21.81120508, 14.57059585,  6.07843164]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 0.6010184222179955}
episode index:2575
target Thresh 31.99999999983206
target distance 8.0
model initialize at round 2575
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 20.]), 'previousTarget': array([10., 20.]), 'currentState': array([15.14227192, 13.23596236,  1.83477276]), 'targetState': array([10, 20], dtype=int32), 'currentDistance': 8.496773839275363}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9286425248448388
{'scaleFactor': 20, 'currentTarget': array([10., 20.]), 'previousTarget': array([10., 20.]), 'currentState': array([10.07726217, 19.3323087 ,  2.40497781]), 'targetState': array([10, 20], dtype=int32), 'currentDistance': 0.6721466426855125}
episode index:2576
target Thresh 31.99999999983373
target distance 9.0
model initialize at round 2576
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([ 2.88743003, 12.57001537,  4.25780869]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 7.5708523127865055}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9286549243346156
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.33864022, 4.74144682, 4.95185552]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.4260597901815057}
episode index:2577
target Thresh 31.999999999835385
target distance 6.0
model initialize at round 2577
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 23.]), 'previousTarget': array([18., 23.]), 'currentState': array([24.49467058, 18.60862957,  2.28246343]), 'targetState': array([18, 23], dtype=int32), 'currentDistance': 7.839954095277609}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.928667314204928
{'scaleFactor': 20, 'currentTarget': array([18., 23.]), 'previousTarget': array([18., 23.]), 'currentState': array([17.72576114, 22.73104422,  2.93679654]), 'targetState': array([18, 23], dtype=int32), 'currentDistance': 0.3841147818755489}
episode index:2578
target Thresh 31.999999999837023
target distance 5.0
model initialize at round 2578
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 12.]), 'previousTarget': array([13., 12.]), 'currentState': array([18.66094191,  6.72859613,  0.84802788]), 'targetState': array([13, 12], dtype=int32), 'currentDistance': 7.73524156180631}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9286796944669655
{'scaleFactor': 20, 'currentTarget': array([13., 12.]), 'previousTarget': array([13., 12.]), 'currentState': array([13.13099509, 11.45895961,  2.6823021 ]), 'targetState': array([13, 12], dtype=int32), 'currentDistance': 0.5566726322049687}
episode index:2579
target Thresh 31.999999999838643
target distance 25.0
model initialize at round 2579
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 26.]), 'previousTarget': array([ 2., 26.]), 'currentState': array([27.72545564, 29.51858559,  2.1351327 ]), 'targetState': array([ 2, 26], dtype=int32), 'currentDistance': 25.964967022353935}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9286598655245362
{'scaleFactor': 20, 'currentTarget': array([ 2., 26.]), 'previousTarget': array([ 2., 26.]), 'currentState': array([ 2.82793896, 25.64138966,  3.23109166]), 'targetState': array([ 2, 26], dtype=int32), 'currentDistance': 0.9022662015851057}
episode index:2580
target Thresh 31.99999999984025
target distance 25.0
model initialize at round 2580
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 19.]), 'previousTarget': array([ 2., 19.]), 'currentState': array([28.38390081, 22.95770905,  1.61536091]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 26.679086997473174}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.92863665202095
{'scaleFactor': 20, 'currentTarget': array([ 2., 19.]), 'previousTarget': array([ 2., 19.]), 'currentState': array([ 1.91165773, 18.70081449,  3.21112411]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 0.31195564385068075}
episode index:2581
target Thresh 31.99999999984184
target distance 13.0
model initialize at round 2581
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 24.]), 'previousTarget': array([10., 24.]), 'currentState': array([15.80803084, 12.59561645,  2.37208378]), 'targetState': array([10, 24], dtype=int32), 'currentDistance': 12.798171215472648}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9286416262647068
{'scaleFactor': 20, 'currentTarget': array([10., 24.]), 'previousTarget': array([10., 24.]), 'currentState': array([10.15555149, 23.06104099,  2.02738278]), 'targetState': array([10, 24], dtype=int32), 'currentDistance': 0.951756418985176}
episode index:2582
target Thresh 31.999999999843414
target distance 21.0
model initialize at round 2582
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  6.]), 'previousTarget': array([24.,  6.]), 'currentState': array([ 4.1270221 , 14.75011629,  0.17295807]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 21.7140458210443}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9286287329731829
{'scaleFactor': 20, 'currentTarget': array([24.,  6.]), 'previousTarget': array([24.,  6.]), 'currentState': array([23.92007366,  6.57444063,  5.70635373]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 0.5799743554336895}
episode index:2583
target Thresh 31.99999999984497
target distance 15.0
model initialize at round 2583
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 12.]), 'previousTarget': array([25., 12.]), 'currentState': array([20.37893033, 26.03514808,  4.66265372]), 'targetState': array([25, 12], dtype=int32), 'currentDistance': 14.776321142502841}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9286300629325227
{'scaleFactor': 20, 'currentTarget': array([25., 12.]), 'previousTarget': array([25., 12.]), 'currentState': array([25.1501261 , 12.95947473,  5.22631399]), 'targetState': array([25, 12], dtype=int32), 'currentDistance': 0.9711486026577663}
episode index:2584
target Thresh 31.999999999846512
target distance 7.0
model initialize at round 2584
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 16.]), 'previousTarget': array([ 9., 16.]), 'currentState': array([14.51651557, 12.20522831,  2.62342691]), 'targetState': array([ 9, 16], dtype=int32), 'currentDistance': 6.695687886890796}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9286461824439608
{'scaleFactor': 20, 'currentTarget': array([ 9., 16.]), 'previousTarget': array([ 9., 16.]), 'currentState': array([ 9.36712757, 15.26633271,  2.72554418]), 'targetState': array([ 9, 16], dtype=int32), 'currentDistance': 0.8203964526085731}
episode index:2585
target Thresh 31.99999999984804
target distance 10.0
model initialize at round 2585
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([18.41759614, 10.95387258,  1.28198977]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 8.807598080958524}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9286585373656762
{'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([21.41159115, 18.34018437,  1.28748975]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 0.8840710604946692}
episode index:2586
target Thresh 31.999999999849553
target distance 9.0
model initialize at round 2586
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 24.]), 'previousTarget': array([25., 24.]), 'currentState': array([17.99522734, 24.12621699,  0.11217672]), 'targetState': array([25, 24], dtype=int32), 'currentDistance': 7.005909703634462}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9286708827358479
{'scaleFactor': 20, 'currentTarget': array([25., 24.]), 'previousTarget': array([25., 24.]), 'currentState': array([25.86596213, 23.66425703,  5.64243125]), 'targetState': array([25, 24], dtype=int32), 'currentDistance': 0.9287700256975562}
episode index:2587
target Thresh 31.99999999985105
target distance 17.0
model initialize at round 2587
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([17.84515707, 29.67583166,  2.67293239]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 20.664546735064487}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9286580030494193
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.07692456, 10.56473528,  4.21817043]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.44200991975810244}
episode index:2588
target Thresh 31.99999999985253
target distance 5.0
model initialize at round 2588
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 23.]), 'previousTarget': array([ 7., 23.]), 'currentState': array([ 2.00217559, 17.31703139,  5.72368169]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 7.567983952562448}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9286703390891837
{'scaleFactor': 20, 'currentTarget': array([ 7., 23.]), 'previousTarget': array([ 7., 23.]), 'currentState': array([ 6.63488092, 23.04283306,  1.23137545]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 0.36762292713506467}
episode index:2589
target Thresh 31.999999999853998
target distance 9.0
model initialize at round 2589
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([18.23608428, 13.5003931 ,  3.23124123]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 8.949069005974001}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9286789567381455
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([9.12947799, 9.687603  , 3.00141961]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.9248786169719782}
episode index:2590
target Thresh 31.99999999985545
target distance 15.0
model initialize at round 2590
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 14.]), 'previousTarget': array([17., 14.]), 'currentState': array([ 3.02585528, 15.66583021,  0.09469717]), 'targetState': array([17, 14], dtype=int32), 'currentDistance': 14.073084626691587}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.928680263720457
{'scaleFactor': 20, 'currentTarget': array([17., 14.]), 'previousTarget': array([17., 14.]), 'currentState': array([16.92215523, 14.49180243,  6.28298484]), 'targetState': array([17, 14], dtype=int32), 'currentDistance': 0.4979251310182824}
episode index:2591
target Thresh 31.99999999985689
target distance 9.0
model initialize at round 2591
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 26.]), 'previousTarget': array([15., 26.]), 'currentState': array([ 7.61899496, 21.45961225,  1.28660982]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 8.665699995651192}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9286925768941758
{'scaleFactor': 20, 'currentTarget': array([15., 26.]), 'previousTarget': array([15., 26.]), 'currentState': array([14.01658178, 26.00316007,  0.54944883]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 0.9834232966499887}
episode index:2592
target Thresh 31.999999999858314
target distance 5.0
model initialize at round 2592
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 18.]), 'previousTarget': array([ 7., 18.]), 'currentState': array([ 3.45810752, 16.35875283,  0.60432023]), 'targetState': array([ 7, 18], dtype=int32), 'currentDistance': 3.9036770632198734}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9287124023562299
{'scaleFactor': 20, 'currentTarget': array([ 7., 18.]), 'previousTarget': array([ 7., 18.]), 'currentState': array([ 6.89222247, 18.38279576,  0.72472058]), 'targetState': array([ 7, 18], dtype=int32), 'currentDistance': 0.39767900647340115}
episode index:2593
target Thresh 31.999999999859725
target distance 14.0
model initialize at round 2593
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 19.]), 'previousTarget': array([25., 19.]), 'currentState': array([25.29779833,  3.92850218,  0.31982678]), 'targetState': array([25, 19], dtype=int32), 'currentDistance': 15.074439634744992}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9287101017749159
{'scaleFactor': 20, 'currentTarget': array([25., 19.]), 'previousTarget': array([25., 19.]), 'currentState': array([24.49675442, 19.29561283,  1.70231489]), 'targetState': array([25, 19], dtype=int32), 'currentDistance': 0.5836463510590146}
episode index:2594
target Thresh 31.99999999986112
target distance 16.0
model initialize at round 2594
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([ 3.69188651, 20.31247082,  5.15812943]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 18.876415161417597}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9287042471104491
{'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([17.39464939,  8.96212715,  5.52349839]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 1.136722490208016}
episode index:2595
target Thresh 31.999999999862503
target distance 20.0
model initialize at round 2595
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  2.]), 'previousTarget': array([12.,  2.]), 'currentState': array([ 5.99481604, 20.40056099,  4.66655302]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 19.355693714600125}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9286948780148784
{'scaleFactor': 20, 'currentTarget': array([12.,  2.]), 'previousTarget': array([12.,  2.]), 'currentState': array([12.41461729,  1.81248655,  4.98309041]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 0.45504811810054036}
episode index:2596
target Thresh 31.99999999986387
target distance 13.0
model initialize at round 2596
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  3.]), 'previousTarget': array([22.,  3.]), 'currentState': array([10.83258316, 13.22672712,  5.70065957]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 15.142560763094748}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9286925868390651
{'scaleFactor': 20, 'currentTarget': array([22.,  3.]), 'previousTarget': array([22.,  3.]), 'currentState': array([22.58733174,  2.56201556,  5.03903768]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 0.7326588176271233}
episode index:2597
target Thresh 31.999999999865224
target distance 3.0
model initialize at round 2597
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  7.]), 'previousTarget': array([27.,  7.]), 'currentState': array([26.58812021,  8.42267074,  4.06001055]), 'targetState': array([27,  7], dtype=int32), 'currentDistance': 1.4810931746962355}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9287161847656089
{'scaleFactor': 20, 'currentTarget': array([27.,  7.]), 'previousTarget': array([27.,  7.]), 'currentState': array([26.73578954,  6.60668058,  5.54186118]), 'targetState': array([27,  7], dtype=int32), 'currentDistance': 0.4738220492859383}
episode index:2598
target Thresh 31.999999999866564
target distance 18.0
model initialize at round 2598
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([22.12575486, 27.32173488,  3.77718067]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 22.494701807832534}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9287033421605658
{'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([ 4.91466868, 13.94019044,  3.74454319]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 0.9166220453552779}
episode index:2599
target Thresh 31.999999999867892
target distance 2.0
model initialize at round 2599
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  4.]), 'previousTarget': array([10.,  4.]), 'currentState': array([9.65881757, 4.88304227, 5.71420303]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 0.9466620869596424}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9287307639520425
{'scaleFactor': 20, 'currentTarget': array([10.,  4.]), 'previousTarget': array([10.,  4.]), 'currentState': array([9.65881757, 4.88304227, 5.71420303]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 0.9466620869596424}
episode index:2600
target Thresh 31.999999999869207
target distance 3.0
model initialize at round 2600
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 23.]), 'previousTarget': array([25., 23.]), 'currentState': array([25.09700748, 21.78449674,  2.0508499 ]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 1.2193681248648645}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9287543199828182
{'scaleFactor': 20, 'currentTarget': array([25., 23.]), 'previousTarget': array([25., 23.]), 'currentState': array([24.76499239, 23.72365756,  1.42361365]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 0.7608605935547492}
episode index:2601
target Thresh 31.999999999870507
target distance 20.0
model initialize at round 2601
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 12.]), 'previousTarget': array([23., 12.]), 'currentState': array([ 1.56636811, 23.11847423,  4.70288873]), 'targetState': array([23, 12], dtype=int32), 'currentDistance': 24.145828736737446}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9287380365668817
{'scaleFactor': 20, 'currentTarget': array([23., 12.]), 'previousTarget': array([23., 12.]), 'currentState': array([22.38831817, 12.96709432,  5.67101495]), 'targetState': array([23, 12], dtype=int32), 'currentDistance': 1.1443015677938315}
episode index:2602
target Thresh 31.999999999871797
target distance 5.0
model initialize at round 2602
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 21.]), 'previousTarget': array([15., 21.]), 'currentState': array([11.00972222, 22.65357914,  0.08266037]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 4.319333368586871}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9287577684007017
{'scaleFactor': 20, 'currentTarget': array([15., 21.]), 'previousTarget': array([15., 21.]), 'currentState': array([14.81423331, 21.57116884,  5.9561874 ]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 0.6006189346603075}
episode index:2603
target Thresh 31.999999999873072
target distance 13.0
model initialize at round 2603
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([12.68244341, 15.04209832,  5.29820222]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 17.267988184337458}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9287519156660945
{'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([24.44014581,  1.54637668,  5.00930926]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 0.6320620676577555}
episode index:2604
target Thresh 31.999999999874337
target distance 21.0
model initialize at round 2604
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  5.]), 'previousTarget': array([24.,  5.]), 'currentState': array([11.36178335, 25.01109444,  4.64510134]), 'targetState': array([24,  5], dtype=int32), 'currentDistance': 23.66787740786185}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9287356519256146
{'scaleFactor': 20, 'currentTarget': array([24.,  5.]), 'previousTarget': array([24.,  5.]), 'currentState': array([24.26891157,  5.35583602,  5.27978457]), 'targetState': array([24,  5], dtype=int32), 'currentDistance': 0.4460187277700114}
episode index:2605
target Thresh 31.999999999875588
target distance 13.0
model initialize at round 2605
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 17.]), 'previousTarget': array([12., 17.]), 'currentState': array([26.21441853,  7.16515051,  1.77469652]), 'targetState': array([12, 17], dtype=int32), 'currentDistance': 17.28507907306682}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9287298121694971
{'scaleFactor': 20, 'currentTarget': array([12., 17.]), 'previousTarget': array([12., 17.]), 'currentState': array([11.58776715, 17.00675005,  2.3713072 ]), 'targetState': array([12, 17], dtype=int32), 'currentDistance': 0.41228811092739576}
episode index:2606
target Thresh 31.999999999876824
target distance 10.0
model initialize at round 2606
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([1.3743912 , 9.43762946, 5.34151816]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 15.703247275836857}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.928727516382101
{'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([11.22448016, 20.75884899,  0.90074355]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 0.8121482804582892}
episode index:2607
target Thresh 31.99999999987805
target distance 10.0
model initialize at round 2607
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 23.]), 'previousTarget': array([ 5., 23.]), 'currentState': array([14.05847383, 23.39496113,  3.17451024]), 'targetState': array([ 5, 23], dtype=int32), 'currentDistance': 9.067080150764331}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9287360526296156
{'scaleFactor': 20, 'currentTarget': array([ 5., 23.]), 'previousTarget': array([ 5., 23.]), 'currentState': array([ 4.18042326, 23.1858448 ,  2.56605466]), 'targetState': array([ 5, 23], dtype=int32), 'currentDistance': 0.8403834390040034}
episode index:2608
target Thresh 31.999999999879265
target distance 15.0
model initialize at round 2608
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 19.]), 'previousTarget': array([13., 19.]), 'currentState': array([10.68558304,  5.89527873,  1.24109316]), 'targetState': array([13, 19], dtype=int32), 'currentDistance': 13.307525889984168}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9287373287105959
{'scaleFactor': 20, 'currentTarget': array([13., 19.]), 'previousTarget': array([13., 19.]), 'currentState': array([13.0955227 , 19.50732408,  0.97892645]), 'targetState': array([13, 19], dtype=int32), 'currentDistance': 0.5162386172883834}
episode index:2609
target Thresh 31.999999999880465
target distance 3.0
model initialize at round 2609
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 27.]), 'previousTarget': array([24., 27.]), 'currentState': array([24.79348032, 25.5898247 ,  2.33333378]), 'targetState': array([24, 27], dtype=int32), 'currentDistance': 1.6180869556641377}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9287608009984462
{'scaleFactor': 20, 'currentTarget': array([24., 27.]), 'previousTarget': array([24., 27.]), 'currentState': array([23.61129515, 27.19611704,  2.07459582]), 'targetState': array([24, 27], dtype=int32), 'currentDistance': 0.43537725371718494}
episode index:2610
target Thresh 31.999999999881656
target distance 15.0
model initialize at round 2610
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 12.]), 'previousTarget': array([ 2., 12.]), 'currentState': array([11.61570146, 26.04286592,  4.75653815]), 'targetState': array([ 2, 12], dtype=int32), 'currentDistance': 17.019512270109914}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9287549627933466
{'scaleFactor': 20, 'currentTarget': array([ 2., 12.]), 'previousTarget': array([ 2., 12.]), 'currentState': array([ 1.65045283, 11.51176225,  3.53719703]), 'targetState': array([ 2, 12], dtype=int32), 'currentDistance': 0.6004659232381848}
episode index:2611
target Thresh 31.99999999988283
target distance 15.0
model initialize at round 2611
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 27.]), 'previousTarget': array([22., 27.]), 'currentState': array([ 6.37739319, 16.43643073,  5.34343886]), 'targetState': array([22, 27], dtype=int32), 'currentDistance': 18.858813300130187}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.928745631672449
{'scaleFactor': 20, 'currentTarget': array([22., 27.]), 'previousTarget': array([22., 27.]), 'currentState': array([22.10928159, 27.59982182,  0.69807015]), 'targetState': array([22, 27], dtype=int32), 'currentDistance': 0.6096955656293105}
episode index:2612
target Thresh 31.999999999883997
target distance 16.0
model initialize at round 2612
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([23.33596044, 26.25171496,  4.00146341]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 14.700116049226823}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9287469021340773
{'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([ 9.95456014, 22.62924822,  3.40622713]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 1.024032201076841}
episode index:2613
target Thresh 31.99999999988515
target distance 24.0
model initialize at round 2613
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 13.]), 'previousTarget': array([27., 13.]), 'currentState': array([ 4.73025512, 26.20284699,  5.30848547]), 'targetState': array([27, 13], dtype=int32), 'currentDistance': 25.889316437350224}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9287273053937808
{'scaleFactor': 20, 'currentTarget': array([27., 13.]), 'previousTarget': array([27., 13.]), 'currentState': array([26.87292287, 13.76981847,  5.91595651]), 'targetState': array([27, 13], dtype=int32), 'currentDistance': 0.7802365463124702}
episode index:2614
target Thresh 31.999999999886295
target distance 14.0
model initialize at round 2614
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  5.]), 'previousTarget': array([17.,  5.]), 'currentState': array([ 2.69771083, 16.34440061,  5.54179239]), 'targetState': array([17,  5], dtype=int32), 'currentDistance': 18.255160931551913}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9287214889280407
{'scaleFactor': 20, 'currentTarget': array([17.,  5.]), 'previousTarget': array([17.,  5.]), 'currentState': array([16.88850769,  5.68872792,  5.57623487]), 'targetState': array([17,  5], dtype=int32), 'currentDistance': 0.6976938275424703}
episode index:2615
target Thresh 31.999999999887425
target distance 13.0
model initialize at round 2615
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([ 7.32896093, 11.47179302,  0.60799727]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 13.37275736063008}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9287227671615955
{'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([19.36198837, 18.48667739,  0.27206689]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 0.6065397444952804}
episode index:2616
target Thresh 31.999999999888544
target distance 17.0
model initialize at round 2616
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 16.]), 'previousTarget': array([ 7., 16.]), 'currentState': array([24.03266637, 16.68265296,  2.56138515]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 17.0463409184931}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9287169568751308
{'scaleFactor': 20, 'currentTarget': array([ 7., 16.]), 'previousTarget': array([ 7., 16.]), 'currentState': array([ 6.48228899, 15.76532242,  2.8108018 ]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 0.5684173283909215}
episode index:2617
target Thresh 31.999999999889656
target distance 20.0
model initialize at round 2617
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 26.]), 'previousTarget': array([18., 26.]), 'currentState': array([9.65592572, 7.64288822, 2.04703826]), 'targetState': array([18, 26], dtype=int32), 'currentDistance': 20.16450169627281}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9287076616566945
{'scaleFactor': 20, 'currentTarget': array([18., 26.]), 'previousTarget': array([18., 26.]), 'currentState': array([17.36745153, 25.62636075,  1.18330395]), 'targetState': array([18, 26], dtype=int32), 'currentDistance': 0.7346590046224873}
episode index:2618
target Thresh 31.999999999890754
target distance 16.0
model initialize at round 2618
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 24.]), 'previousTarget': array([13., 24.]), 'currentState': array([8.50485663, 9.91220606, 1.57418603]), 'targetState': array([13, 24], dtype=int32), 'currentDistance': 14.787570866347897}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9287089437056637
{'scaleFactor': 20, 'currentTarget': array([13., 24.]), 'previousTarget': array([13., 24.]), 'currentState': array([12.0156508 , 23.36552069,  1.16182114]), 'targetState': array([13, 24], dtype=int32), 'currentDistance': 1.1711137198494042}
episode index:2619
target Thresh 31.99999999989184
target distance 8.0
model initialize at round 2619
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 8.]), 'previousTarget': array([8., 8.]), 'currentState': array([ 5.28031634, 14.69633553,  4.75182104]), 'targetState': array([8, 8], dtype=int32), 'currentDistance': 7.227557586100805}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9287211143416538
{'scaleFactor': 20, 'currentTarget': array([8., 8.]), 'previousTarget': array([8., 8.]), 'currentState': array([8.43357677, 7.47492015, 4.74452157]), 'targetState': array([8, 8], dtype=int32), 'currentDistance': 0.6809534982755155}
episode index:2620
target Thresh 31.999999999892918
target distance 6.0
model initialize at round 2620
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 18.]), 'previousTarget': array([24., 18.]), 'currentState': array([26.15897613, 13.69374825,  2.6139251 ]), 'targetState': array([24, 18], dtype=int32), 'currentDistance': 4.817154974858726}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9287369777089406
{'scaleFactor': 20, 'currentTarget': array([24., 18.]), 'previousTarget': array([24., 18.]), 'currentState': array([24.20978673, 18.70883914,  0.71174394]), 'targetState': array([24, 18], dtype=int32), 'currentDistance': 0.7392316248905184}
episode index:2621
target Thresh 31.99999999989398
target distance 7.0
model initialize at round 2621
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  5.]), 'previousTarget': array([13.,  5.]), 'currentState': array([10.79713815, 10.5117512 ,  4.20307422]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 5.93565511050676}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9287528289760234
{'scaleFactor': 20, 'currentTarget': array([13.,  5.]), 'previousTarget': array([13.,  5.]), 'currentState': array([13.23912625,  5.35215952,  5.45685497]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 0.4256732213388992}
episode index:2622
target Thresh 31.999999999895035
target distance 8.0
model initialize at round 2622
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 11.]), 'previousTarget': array([19., 11.]), 'currentState': array([21.27464999, 17.2727564 ,  4.94342077]), 'targetState': array([19, 11], dtype=int32), 'currentDistance': 6.672443731814074}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9287686681567416
{'scaleFactor': 20, 'currentTarget': array([19., 11.]), 'previousTarget': array([19., 11.]), 'currentState': array([19.52314475, 11.66716044,  4.58791377]), 'targetState': array([19, 11], dtype=int32), 'currentDistance': 0.8478109932631356}
episode index:2623
target Thresh 31.99999999989608
target distance 15.0
model initialize at round 2623
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([25.33527537, 27.89243699,  3.76774234]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 14.579107755962962}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9287699245133538
{'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([12.780028  , 21.88910868,  3.64049849]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 0.7878709025504976}
episode index:2624
target Thresh 31.999999999897113
target distance 10.0
model initialize at round 2624
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 8.]), 'previousTarget': array([8., 8.]), 'currentState': array([13.18558842, 16.17333482,  4.28835757]), 'targetState': array([8, 8], dtype=int32), 'currentDistance': 9.679552126685898}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.928778389323025
{'scaleFactor': 20, 'currentTarget': array([8., 8.]), 'previousTarget': array([8., 8.]), 'currentState': array([8.0442494 , 7.63188418, 4.09332324]), 'targetState': array([8, 8], dtype=int32), 'currentDistance': 0.37076578598313215}
episode index:2625
target Thresh 31.99999999989814
target distance 11.0
model initialize at round 2625
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([16.57672464,  3.58106824,  2.23102535]), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 11.663358036929155}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9287832262461315
{'scaleFactor': 20, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.96017959, 4.50806084, 3.15029878]), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.4935481765985835}
episode index:2626
target Thresh 31.999999999899153
target distance 9.0
model initialize at round 2626
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  4.]), 'previousTarget': array([26.,  4.]), 'currentState': array([17.3904767 ,  3.36295509,  5.95653915]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 8.633059576418848}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9287953361752346
{'scaleFactor': 20, 'currentTarget': array([26.,  4.]), 'previousTarget': array([26.,  4.]), 'currentState': array([25.2401976 ,  4.34727245,  0.25157824]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 0.8354027981143246}
episode index:2627
target Thresh 31.999999999900155
target distance 19.0
model initialize at round 2627
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 24.]), 'previousTarget': array([11., 24.]), 'currentState': array([2.8612213 , 6.44592045, 2.04360253]), 'targetState': array([11, 24], dtype=int32), 'currentDistance': 19.349042030011255}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9287860465020358
{'scaleFactor': 20, 'currentTarget': array([11., 24.]), 'previousTarget': array([11., 24.]), 'currentState': array([10.64114718, 24.41361709,  1.13214046]), 'targetState': array([11, 24], dtype=int32), 'currentDistance': 0.5475896643505677}
episode index:2628
target Thresh 31.99999999990115
target distance 2.0
model initialize at round 2628
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  6.]), 'previousTarget': array([19.,  6.]), 'currentState': array([18.28758627,  6.15709624,  5.51615381]), 'targetState': array([19,  6], dtype=int32), 'currentDistance': 0.7295289979869986}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9288131343504565
{'scaleFactor': 20, 'currentTarget': array([19.,  6.]), 'previousTarget': array([19.,  6.]), 'currentState': array([18.28758627,  6.15709624,  5.51615381]), 'targetState': array([19,  6], dtype=int32), 'currentDistance': 0.7295289979869986}
episode index:2629
target Thresh 31.999999999902133
target distance 9.0
model initialize at round 2629
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 14.]), 'previousTarget': array([24., 14.]), 'currentState': array([16.40771315, 17.92235123,  5.85321612]), 'targetState': array([24, 14], dtype=int32), 'currentDistance': 8.54562220106391}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9288252190940494
{'scaleFactor': 20, 'currentTarget': array([24., 14.]), 'previousTarget': array([24., 14.]), 'currentState': array([23.59373702, 14.48581579,  6.12344113]), 'targetState': array([24, 14], dtype=int32), 'currentDistance': 0.6332981856954406}
episode index:2630
target Thresh 31.999999999903107
target distance 22.0
model initialize at round 2630
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([5.34856366, 8.17350103, 6.17973859]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 23.798065065292285}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.928809088213252
{'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([25.55191296, 20.26386916,  0.54709397]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 0.5200085866436518}
episode index:2631
target Thresh 31.99999999990407
target distance 4.0
model initialize at round 2631
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([21.98218587,  7.58003832,  2.75600898]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 3.8405269335843393}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9288285756417425
{'scaleFactor': 20, 'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([18.59713522,  9.681648  ,  2.71925312]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 0.5134666747154646}
episode index:2632
target Thresh 31.999999999905025
target distance 15.0
model initialize at round 2632
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 17.]), 'previousTarget': array([21., 17.]), 'currentState': array([23.49608924,  1.22921788,  0.53426808]), 'targetState': array([21, 17], dtype=int32), 'currentDistance': 15.967092103947797}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9288262650146198
{'scaleFactor': 20, 'currentTarget': array([21., 17.]), 'previousTarget': array([21., 17.]), 'currentState': array([20.62510844, 16.36091418,  1.86924451]), 'targetState': array([21, 17], dtype=int32), 'currentDistance': 0.7409280410978194}
episode index:2633
target Thresh 31.99999999990597
target distance 19.0
model initialize at round 2633
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([ 9.98245456, 19.65948168,  5.07310915]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 18.656584531731646}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9288204529350712
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.05740126,  2.86758971,  5.17898747]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.8694865239476552}
episode index:2634
target Thresh 31.999999999906905
target distance 8.0
model initialize at round 2634
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  6.]), 'previousTarget': array([17.,  6.]), 'currentState': array([21.31983716, 12.34770091,  3.89359522]), 'targetState': array([17,  6], dtype=int32), 'currentDistance': 7.678170349857759}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9288325119700104
{'scaleFactor': 20, 'currentTarget': array([17.,  6.]), 'previousTarget': array([17.,  6.]), 'currentState': array([17.2229501 ,  5.54009359,  4.21553593]), 'targetState': array([17,  6], dtype=int32), 'currentDistance': 0.5110975035595584}
episode index:2635
target Thresh 31.999999999907832
target distance 10.0
model initialize at round 2635
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 14.]), 'previousTarget': array([21., 14.]), 'currentState': array([20.81566157, 22.0325494 ,  4.89009953]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 8.034664301982545}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9288445618554542
{'scaleFactor': 20, 'currentTarget': array([21., 14.]), 'previousTarget': array([21., 14.]), 'currentState': array([21.49390646, 14.09019485,  5.00240492]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 0.5020743978720164}
episode index:2636
target Thresh 31.99999999990875
target distance 20.0
model initialize at round 2636
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 26.]), 'previousTarget': array([25., 26.]), 'currentState': array([ 6.68331514, 28.78639959,  0.86035078]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 18.52741122831158}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9288387494495489
{'scaleFactor': 20, 'currentTarget': array([25., 26.]), 'previousTarget': array([25., 26.]), 'currentState': array([24.18740931, 26.68681402,  6.20976192]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 1.0639629371539672}
episode index:2637
target Thresh 31.999999999909658
target distance 16.0
model initialize at round 2637
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([8.42507217, 9.89529737, 5.83411616]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 14.85971964834303}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9288399725725427
{'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([22.15172788,  7.83166127,  6.06970787]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 1.1879503584016937}
episode index:2638
target Thresh 31.999999999910557
target distance 12.0
model initialize at round 2638
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 25.]), 'previousTarget': array([16., 25.]), 'currentState': array([13.63423225, 13.98339541,  1.50754905]), 'targetState': array([16, 25], dtype=int32), 'currentDistance': 11.267760817304465}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.928844762332614
{'scaleFactor': 20, 'currentTarget': array([16., 25.]), 'previousTarget': array([16., 25.]), 'currentState': array([16.10899276, 25.60032178,  0.85542041]), 'targetState': array([16, 25], dtype=int32), 'currentDistance': 0.6101357745223973}
episode index:2639
target Thresh 31.999999999911445
target distance 4.0
model initialize at round 2639
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 14.]), 'previousTarget': array([15., 14.]), 'currentState': array([12.36144417,  8.35630106,  5.93884087]), 'targetState': array([15, 14], dtype=int32), 'currentDistance': 6.230033272379457}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9288604646953669
{'scaleFactor': 20, 'currentTarget': array([15., 14.]), 'previousTarget': array([15., 14.]), 'currentState': array([14.23656789, 13.31724894,  1.43964025]), 'targetState': array([15, 14], dtype=int32), 'currentDistance': 1.0241960717169736}
episode index:2640
target Thresh 31.999999999912326
target distance 2.0
model initialize at round 2640
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  7.]), 'previousTarget': array([11.,  7.]), 'currentState': array([8.34452256, 9.3029563 , 3.97059309]), 'targetState': array([11,  7], dtype=int32), 'currentDistance': 3.5149919176067876}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9288798662611771
{'scaleFactor': 20, 'currentTarget': array([11.,  7.]), 'previousTarget': array([11.,  7.]), 'currentState': array([10.73161647,  7.44407573,  0.08707964]), 'targetState': array([11,  7], dtype=int32), 'currentDistance': 0.5188766436250614}
episode index:2641
target Thresh 31.9999999999132
target distance 13.0
model initialize at round 2641
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 9.81045385, 24.19053268,  3.36578   ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 15.33403376791161}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9288775440916717
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.50583949, 9.72703389, 4.47940064]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.5747904766529649}
episode index:2642
target Thresh 31.999999999914063
target distance 8.0
model initialize at round 2642
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([17.32592324, 13.03649198,  1.6734907 ]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 10.133224568424952}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9288859105335213
{'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([ 8.61584881, 16.47753045,  2.94431677]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 0.8076163639226641}
episode index:2643
target Thresh 31.99999999991492
target distance 11.0
model initialize at round 2643
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 24.]), 'previousTarget': array([20., 24.]), 'currentState': array([23.91861297, 14.67857917,  2.0434033 ]), 'targetState': array([20, 24], dtype=int32), 'currentDistance': 10.111597985037228}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9288942706467462
{'scaleFactor': 20, 'currentTarget': array([20., 24.]), 'previousTarget': array([20., 24.]), 'currentState': array([19.61566504, 23.67076476,  2.26664048]), 'targetState': array([20, 24], dtype=int32), 'currentDistance': 0.5060723285427657}
episode index:2644
target Thresh 31.999999999915765
target distance 6.0
model initialize at round 2644
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([10.65945732,  2.28033815,  1.17735356]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 5.716309562802162}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9289099246086945
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.79312876,  6.54877531,  0.86609596]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.5864725490629205}
episode index:2645
target Thresh 31.999999999916604
target distance 9.0
model initialize at round 2645
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 14.]), 'previousTarget': array([17., 14.]), 'currentState': array([24.09807467, 17.4066522 ,  3.59729302]), 'targetState': array([17, 14], dtype=int32), 'currentDistance': 7.873242232778595}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9289218996976556
{'scaleFactor': 20, 'currentTarget': array([17., 14.]), 'previousTarget': array([17., 14.]), 'currentState': array([17.16167693, 13.51506016,  3.91127298]), 'targetState': array([17, 14], dtype=int32), 'currentDistance': 0.511181061090166}
episode index:2646
target Thresh 31.99999999991743
target distance 8.0
model initialize at round 2646
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 13.]), 'previousTarget': array([ 9., 13.]), 'currentState': array([4.4795288 , 5.80211134, 1.50678843]), 'targetState': array([ 9, 13], dtype=int32), 'currentDistance': 8.499662404494735}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9289338657385706
{'scaleFactor': 20, 'currentTarget': array([ 9., 13.]), 'previousTarget': array([ 9., 13.]), 'currentState': array([ 8.45414024, 12.61744901,  1.26482456]), 'targetState': array([ 9, 13], dtype=int32), 'currentDistance': 0.6665644284878185}
episode index:2647
target Thresh 31.999999999918256
target distance 13.0
model initialize at round 2647
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 16.]), 'previousTarget': array([10., 16.]), 'currentState': array([23.9207958 , 30.40873105,  2.00187005]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 20.03497158222916}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.928924593914277
{'scaleFactor': 20, 'currentTarget': array([10., 16.]), 'previousTarget': array([10., 16.]), 'currentState': array([10.92638047, 16.73361123,  4.33670804]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 1.1816793994385555}
episode index:2648
target Thresh 31.99999999991907
target distance 11.0
model initialize at round 2648
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 18.]), 'previousTarget': array([21., 18.]), 'currentState': array([ 9.31929917, 29.08736596,  4.09965754]), 'targetState': array([21, 18], dtype=int32), 'currentDistance': 16.1049202344341}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9289222609963885
{'scaleFactor': 20, 'currentTarget': array([21., 18.]), 'previousTarget': array([21., 18.]), 'currentState': array([20.55948795, 18.63936267,  5.85211748]), 'targetState': array([21, 18], dtype=int32), 'currentDistance': 0.7764248097553402}
episode index:2649
target Thresh 31.999999999919872
target distance 22.0
model initialize at round 2649
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  6.]), 'previousTarget': array([27.,  6.]), 'currentState': array([19.52220454, 26.38627772,  5.43452978]), 'targetState': array([27,  6], dtype=int32), 'currentDistance': 21.714459339821023}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9289095877862988
{'scaleFactor': 20, 'currentTarget': array([27.,  6.]), 'previousTarget': array([27.,  6.]), 'currentState': array([27.20751843,  6.11231855,  4.97558091]), 'targetState': array([27,  6], dtype=int32), 'currentDistance': 0.23596473186429237}
episode index:2650
target Thresh 31.99999999992067
target distance 5.0
model initialize at round 2650
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([4.6836557 , 4.46190668, 6.14028239]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 6.751396950805355}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9289215404163303
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.84678643, 10.85735026,  2.40014068]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.20934025830338093}
episode index:2651
target Thresh 31.99999999992146
target distance 11.0
model initialize at round 2651
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  8.]), 'previousTarget': array([24.,  8.]), 'currentState': array([11.36712144, 12.5729023 ,  3.7317915 ]), 'targetState': array([24,  8], dtype=int32), 'currentDistance': 13.435068147654112}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9289227258641022
{'scaleFactor': 20, 'currentTarget': array([24.,  8.]), 'previousTarget': array([24.,  8.]), 'currentState': array([23.01051339,  8.22762924,  6.22292545]), 'targetState': array([24,  8], dtype=int32), 'currentDistance': 1.0153318776523572}
episode index:2652
target Thresh 31.99999999992224
target distance 20.0
model initialize at round 2652
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([12.39600711, 25.50950456,  3.84402502]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 20.572130915977393}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9289100668095958
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.70272238,  4.50129974,  4.17917716]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.5805824106917551}
episode index:2653
target Thresh 31.999999999923016
target distance 4.0
model initialize at round 2653
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 14.]), 'previousTarget': array([ 5., 14.]), 'currentState': array([ 5.67559348, 11.03842932,  1.46664   ]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 3.037651631579975}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9289293546517926
{'scaleFactor': 20, 'currentTarget': array([ 5., 14.]), 'previousTarget': array([ 5., 14.]), 'currentState': array([ 4.92639491, 14.87881426,  1.34245086]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 0.8818912705313344}
episode index:2654
target Thresh 31.99999999992378
target distance 18.0
model initialize at round 2654
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([2.77864985, 5.81980951, 1.41801181]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 21.541712201251865}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9289167026365787
{'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([16.69645606, 22.13843208,  0.81719648]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 0.3336200873379184}
episode index:2655
target Thresh 31.99999999992454
target distance 16.0
model initialize at round 2655
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 9.55037287, 19.40956679,  4.03554249]), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 15.110973114137112}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9289143788383073
{'scaleFactor': 20, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.79381422, 4.25731532, 4.01881544]), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.7707743561716167}
episode index:2656
target Thresh 31.99999999992529
target distance 5.0
model initialize at round 2656
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 25.]), 'previousTarget': array([ 4., 25.]), 'currentState': array([ 7.06211602, 24.54793714,  3.19493535]), 'targetState': array([ 4, 25], dtype=int32), 'currentDistance': 3.095305375276881}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9289336432798435
{'scaleFactor': 20, 'currentTarget': array([ 4., 25.]), 'previousTarget': array([ 4., 25.]), 'currentState': array([ 3.31767252, 25.24327719,  2.22028435]), 'targetState': array([ 4, 25], dtype=int32), 'currentDistance': 0.7243994631039766}
episode index:2657
target Thresh 31.999999999926033
target distance 5.0
model initialize at round 2657
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 23.]), 'previousTarget': array([22., 23.]), 'currentState': array([15.43373508, 22.38420598,  4.52618933]), 'targetState': array([22, 23], dtype=int32), 'currentDistance': 6.59507674854908}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9289455553816945
{'scaleFactor': 20, 'currentTarget': array([22., 23.]), 'previousTarget': array([22., 23.]), 'currentState': array([22.16381305, 23.52777681,  0.41865343]), 'targetState': array([22, 23], dtype=int32), 'currentDistance': 0.5526147643009207}
episode index:2658
target Thresh 31.999999999926768
target distance 18.0
model initialize at round 2658
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 25.]), 'previousTarget': array([21., 25.]), 'currentState': array([14.61685863,  5.36122278,  5.49271727]), 'targetState': array([21, 25], dtype=int32), 'currentDistance': 20.650086305665255}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.928932916306432
{'scaleFactor': 20, 'currentTarget': array([21., 25.]), 'previousTarget': array([21., 25.]), 'currentState': array([20.30761331, 24.85854139,  1.12110093]), 'targetState': array([21, 25], dtype=int32), 'currentDistance': 0.7066893683534999}
episode index:2659
target Thresh 31.9999999999275
target distance 11.0
model initialize at round 2659
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 8.]), 'previousTarget': array([8., 8.]), 'currentState': array([ 5.08035995, 20.40948581,  3.15890241]), 'targetState': array([8, 8], dtype=int32), 'currentDistance': 12.748318955479023}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.928934093912297
{'scaleFactor': 20, 'currentTarget': array([8., 8.]), 'previousTarget': array([8., 8.]), 'currentState': array([8.61539641, 7.87972343, 5.29907758]), 'targetState': array([8, 8], dtype=int32), 'currentDistance': 0.6270400286942531}
episode index:2660
target Thresh 31.99999999992822
target distance 22.0
model initialize at round 2660
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 15.]), 'previousTarget': array([27., 15.]), 'currentState': array([ 5.65011337, 25.44766605,  6.11899424]), 'targetState': array([27, 15], dtype=int32), 'currentDistance': 23.76912671670695}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9289181039753573
{'scaleFactor': 20, 'currentTarget': array([27., 15.]), 'previousTarget': array([27., 15.]), 'currentState': array([26.70747067, 15.32292264,  5.58623004]), 'targetState': array([27, 15], dtype=int32), 'currentDistance': 0.43572059719595563}
episode index:2661
target Thresh 31.999999999928935
target distance 7.0
model initialize at round 2661
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 21.]), 'previousTarget': array([ 4., 21.]), 'currentState': array([ 9.30764671, 21.03414064,  3.26193231]), 'targetState': array([ 4, 21], dtype=int32), 'currentDistance': 5.307756512879169}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9289336490151863
{'scaleFactor': 20, 'currentTarget': array([ 4., 21.]), 'previousTarget': array([ 4., 21.]), 'currentState': array([ 3.31487882, 20.79679673,  3.13705954]), 'targetState': array([ 4, 21], dtype=int32), 'currentDistance': 0.7146205960934866}
episode index:2662
target Thresh 31.999999999929642
target distance 9.0
model initialize at round 2662
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([10.73949884, 15.19760912,  5.50147653]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 7.9016735431587755}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9289455387489395
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.58418156,  8.28320612,  5.16288425]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.6492101330520578}
episode index:2663
target Thresh 31.999999999930342
target distance 17.0
model initialize at round 2663
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 16.]), 'previousTarget': array([27., 16.]), 'currentState': array([11.68270496, 14.97012808,  0.99224943]), 'targetState': array([27, 16], dtype=int32), 'currentDistance': 15.351878171348778}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9289432111046748
{'scaleFactor': 20, 'currentTarget': array([27., 16.]), 'previousTarget': array([27., 16.]), 'currentState': array([27.16285173, 16.54003626,  0.07780518]), 'targetState': array([27, 16], dtype=int32), 'currentDistance': 0.5640566028064349}
episode index:2664
target Thresh 31.999999999931035
target distance 16.0
model initialize at round 2664
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 16.]), 'previousTarget': array([ 3., 16.]), 'currentState': array([19.52600932, 29.5986564 ,  2.26292184]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 21.40169245153656}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.92893060136477
{'scaleFactor': 20, 'currentTarget': array([ 3., 16.]), 'previousTarget': array([ 3., 16.]), 'currentState': array([ 3.31967051, 15.92838711,  4.00074178]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 0.32759371702115503}
episode index:2665
target Thresh 31.99999999993172
target distance 18.0
model initialize at round 2665
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([ 9.66701655, 23.76882064,  5.1353857 ]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 22.020091086756622}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9289146427264922
{'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([25.95180372,  8.16585395,  3.92516056]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 0.8355372625834249}
episode index:2666
target Thresh 31.9999999999324
target distance 6.0
model initialize at round 2666
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 24.]), 'previousTarget': array([16., 24.]), 'currentState': array([20.06601776, 23.50161819,  2.9665323 ]), 'targetState': array([16, 24], dtype=int32), 'currentDistance': 4.0964478349424525}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9289338348364561
{'scaleFactor': 20, 'currentTarget': array([16., 24.]), 'previousTarget': array([16., 24.]), 'currentState': array([16.09645008, 23.88535442,  3.2187555 ]), 'targetState': array([16, 24], dtype=int32), 'currentDistance': 0.14982064413218388}
episode index:2667
target Thresh 31.999999999933074
target distance 7.0
model initialize at round 2667
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  2.]), 'previousTarget': array([22.,  2.]), 'currentState': array([14.05233771,  7.60920014,  5.12426972]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 9.727716181356485}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9289421017836313
{'scaleFactor': 20, 'currentTarget': array([22.,  2.]), 'previousTarget': array([22.,  2.]), 'currentState': array([22.11730231,  2.07842378,  5.83961231]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 0.1411032316759557}
episode index:2668
target Thresh 31.99999999993374
target distance 14.0
model initialize at round 2668
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 20.]), 'previousTarget': array([ 3., 20.]), 'currentState': array([18.33563575, 16.0239459 ,  1.66406315]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 15.842686961692726}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9289397797876194
{'scaleFactor': 20, 'currentTarget': array([ 3., 20.]), 'previousTarget': array([ 3., 20.]), 'currentState': array([ 3.4165675 , 19.7071003 ,  2.98640872]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 0.5092334594987694}
episode index:2669
target Thresh 31.9999999999344
target distance 12.0
model initialize at round 2669
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  2.]), 'previousTarget': array([11.,  2.]), 'currentState': array([ 7.48585085, 12.39748986,  5.41192436]), 'targetState': array([11,  2], dtype=int32), 'currentDistance': 10.975292238402618}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9289444765552649
{'scaleFactor': 20, 'currentTarget': array([11.,  2.]), 'previousTarget': array([11.,  2.]), 'currentState': array([11.42990443,  1.38714167,  4.50729714]), 'targetState': array([11,  2], dtype=int32), 'currentDistance': 0.7486074746151434}
episode index:2670
target Thresh 31.99999999993505
target distance 17.0
model initialize at round 2670
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 25.]), 'previousTarget': array([ 2., 25.]), 'currentState': array([18.54983783, 19.62164796,  2.85157394]), 'targetState': array([ 2, 25], dtype=int32), 'currentDistance': 17.40183332041716}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9289387007300789
{'scaleFactor': 20, 'currentTarget': array([ 2., 25.]), 'previousTarget': array([ 2., 25.]), 'currentState': array([ 1.56220285, 24.89336062,  3.08098735]), 'targetState': array([ 2, 25], dtype=int32), 'currentDistance': 0.45059771797819864}
episode index:2671
target Thresh 31.999999999935696
target distance 19.0
model initialize at round 2671
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 4.]), 'previousTarget': array([8., 4.]), 'currentState': array([28.04313583,  8.32070274,  1.91228503]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 20.503554958258135}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9289261257126868
{'scaleFactor': 20, 'currentTarget': array([8., 4.]), 'previousTarget': array([8., 4.]), 'currentState': array([7.21265847, 3.84868779, 3.02997633]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 0.801749381531196}
episode index:2672
target Thresh 31.999999999936335
target distance 4.0
model initialize at round 2672
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 27.]), 'previousTarget': array([23., 27.]), 'currentState': array([21.5851123 , 22.43448513,  0.66730994]), 'targetState': array([23, 27], dtype=int32), 'currentDistance': 4.779731503519363}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9289452704468012
{'scaleFactor': 20, 'currentTarget': array([23., 27.]), 'previousTarget': array([23., 27.]), 'currentState': array([22.34805563, 26.19450206,  1.38113418]), 'targetState': array([23, 27], dtype=int32), 'currentDistance': 1.0362713849085208}
episode index:2673
target Thresh 31.99999999993697
target distance 14.0
model initialize at round 2673
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([13.51802765,  7.60126058,  0.2479105 ]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 17.593418456377833}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9289395008047057
{'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([26.33573839, 19.69774647,  0.92889039]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 0.4517493387345196}
episode index:2674
target Thresh 31.999999999937597
target distance 11.0
model initialize at round 2674
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 24.]), 'previousTarget': array([13., 24.]), 'currentState': array([ 5.33246038, 14.48964332,  0.77607684]), 'targetState': array([13, 24], dtype=int32), 'currentDistance': 12.216302547813571}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9289441888976389
{'scaleFactor': 20, 'currentTarget': array([13., 24.]), 'previousTarget': array([13., 24.]), 'currentState': array([12.91679862, 23.64217105,  1.212051  ]), 'targetState': array([13, 24], dtype=int32), 'currentDistance': 0.367374501953467}
episode index:2675
target Thresh 31.99999999993822
target distance 9.0
model initialize at round 2675
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 17.]), 'previousTarget': array([22., 17.]), 'currentState': array([21.46948075,  8.82037456,  1.51917427]), 'targetState': array([22, 17], dtype=int32), 'currentDistance': 8.19681175776614}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9289560169324304
{'scaleFactor': 20, 'currentTarget': array([22., 17.]), 'previousTarget': array([22., 17.]), 'currentState': array([21.8281233 , 16.79196946,  1.61868104]), 'targetState': array([22, 17], dtype=int32), 'currentDistance': 0.2698486692523888}
episode index:2676
target Thresh 31.999999999938833
target distance 7.0
model initialize at round 2676
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 14.]), 'previousTarget': array([18., 14.]), 'currentState': array([ 9.32032305, 19.10523036,  4.08902526]), 'targetState': array([18, 14], dtype=int32), 'currentDistance': 10.069765091099729}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9289642478001807
{'scaleFactor': 20, 'currentTarget': array([18., 14.]), 'previousTarget': array([18., 14.]), 'currentState': array([17.38831703, 14.87049841,  5.74014654]), 'targetState': array([18, 14], dtype=int32), 'currentDistance': 1.0639189512467444}
episode index:2677
target Thresh 31.99999999993944
target distance 11.0
model initialize at round 2677
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 12.]), 'previousTarget': array([26., 12.]), 'currentState': array([21.81746119, 21.80250677,  4.94327402]), 'targetState': array([26, 12], dtype=int32), 'currentDistance': 10.657521747056457}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9289724725209051
{'scaleFactor': 20, 'currentTarget': array([26., 12.]), 'previousTarget': array([26., 12.]), 'currentState': array([25.99087314, 12.79965045,  5.07801452]), 'targetState': array([26, 12], dtype=int32), 'currentDistance': 0.7997025359890494}
episode index:2678
target Thresh 31.999999999940044
target distance 16.0
model initialize at round 2678
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 23.]), 'previousTarget': array([ 7., 23.]), 'currentState': array([14.42750643,  7.89141095,  1.56820601]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 16.835596661681034}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9289701478556968
{'scaleFactor': 20, 'currentTarget': array([ 7., 23.]), 'previousTarget': array([ 7., 23.]), 'currentState': array([ 7.57794953, 22.22912166,  2.2884753 ]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 0.963472406109453}
episode index:2679
target Thresh 31.99999999994064
target distance 15.0
model initialize at round 2679
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 25.]), 'previousTarget': array([10., 25.]), 'currentState': array([24.03411454, 29.37820657,  3.19207764]), 'targetState': array([10, 25], dtype=int32), 'currentDistance': 14.701192592901322}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9289713027810892
{'scaleFactor': 20, 'currentTarget': array([10., 25.]), 'previousTarget': array([10., 25.]), 'currentState': array([10.75477546, 25.09336116,  3.6590816 ]), 'targetState': array([10, 25], dtype=int32), 'currentDistance': 0.7605276520500232}
episode index:2680
target Thresh 31.99999999994123
target distance 10.0
model initialize at round 2680
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 27.]), 'previousTarget': array([21., 27.]), 'currentState': array([15.92935118, 15.31851351,  5.68039799]), 'targetState': array([21, 27], dtype=int32), 'currentDistance': 12.734543813070356}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9289724568449184
{'scaleFactor': 20, 'currentTarget': array([21., 27.]), 'previousTarget': array([21., 27.]), 'currentState': array([20.81683503, 27.42322862,  1.38800994]), 'targetState': array([21, 27], dtype=int32), 'currentDistance': 0.46116360652903027}
episode index:2681
target Thresh 31.999999999941817
target distance 10.0
model initialize at round 2681
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 9.38978712, 20.0508531 ,  4.67399821]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 10.648588184034384}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9289806662383021
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.72300537, 11.99386828,  5.31610995]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.031746183052029}
episode index:2682
target Thresh 31.999999999942396
target distance 6.0
model initialize at round 2682
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 19.]), 'previousTarget': array([23., 19.]), 'currentState': array([23.67244338, 14.54279227,  2.16976218]), 'targetState': array([23, 19], dtype=int32), 'currentDistance': 4.507646932561681}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9289997192885302
{'scaleFactor': 20, 'currentTarget': array([23., 19.]), 'previousTarget': array([23., 19.]), 'currentState': array([22.57774725, 18.35120198,  1.75539223]), 'targetState': array([23, 19], dtype=int32), 'currentDistance': 0.774103519349543}
episode index:2683
target Thresh 31.99999999994297
target distance 13.0
model initialize at round 2683
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 10.]), 'previousTarget': array([20., 10.]), 'currentState': array([ 8.96507667, 16.67203068,  5.96378425]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 12.895174534898269}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9290043692252337
{'scaleFactor': 20, 'currentTarget': array([20., 10.]), 'previousTarget': array([20., 10.]), 'currentState': array([19.40857899, 10.85421412,  5.81707552]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 1.0389709249592096}
episode index:2684
target Thresh 31.999999999943537
target distance 8.0
model initialize at round 2684
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 17.]), 'previousTarget': array([14., 17.]), 'currentState': array([15.66684286,  8.76757159,  0.87145108]), 'targetState': array([14, 17], dtype=int32), 'currentDistance': 8.399478711144852}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9290161351994514
{'scaleFactor': 20, 'currentTarget': array([14., 17.]), 'previousTarget': array([14., 17.]), 'currentState': array([13.69725974, 16.10542936,  1.90212921]), 'targetState': array([14, 17], dtype=int32), 'currentDistance': 0.944408966622658}
episode index:2685
target Thresh 31.999999999944098
target distance 12.0
model initialize at round 2685
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 17.]), 'previousTarget': array([19., 17.]), 'currentState': array([15.6864589 , 29.05477433,  5.313016  ]), 'targetState': array([19, 17], dtype=int32), 'currentDistance': 12.50188540898794}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9290207755621473
{'scaleFactor': 20, 'currentTarget': array([19., 17.]), 'previousTarget': array([19., 17.]), 'currentState': array([19.33433214, 17.70334166,  5.09036731]), 'targetState': array([19, 17], dtype=int32), 'currentDistance': 0.7787602131894349}
episode index:2686
target Thresh 31.999999999944652
target distance 16.0
model initialize at round 2686
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 16.]), 'previousTarget': array([20., 16.]), 'currentState': array([ 5.58625825, 20.43767398,  0.66930455]), 'targetState': array([20, 16], dtype=int32), 'currentDistance': 15.081409138912832}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9290184398415913
{'scaleFactor': 20, 'currentTarget': array([20., 16.]), 'previousTarget': array([20., 16.]), 'currentState': array([20.75812753, 16.3908228 ,  6.16753337]), 'targetState': array([20, 16], dtype=int32), 'currentDistance': 0.8529359952583738}
episode index:2687
target Thresh 31.999999999945203
target distance 5.0
model initialize at round 2687
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 19.]), 'previousTarget': array([ 9., 19.]), 'currentState': array([13.68244894, 14.04187656,  1.0348851 ]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 6.819700587704235}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.929030187449537
{'scaleFactor': 20, 'currentTarget': array([ 9., 19.]), 'previousTarget': array([ 9., 19.]), 'currentState': array([ 8.17632127, 19.01781041,  2.5577145 ]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 0.8238712634800538}
episode index:2688
target Thresh 31.99999999994575
target distance 13.0
model initialize at round 2688
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 19.]), 'previousTarget': array([16., 19.]), 'currentState': array([7.33508909, 7.54605354, 0.96695328]), 'targetState': array([16, 19], dtype=int32), 'currentDistance': 14.362227213691114}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9290313161815777
{'scaleFactor': 20, 'currentTarget': array([16., 19.]), 'previousTarget': array([16., 19.]), 'currentState': array([15.40815006, 18.8992551 ,  1.08060414]), 'targetState': array([16, 19], dtype=int32), 'currentDistance': 0.6003631241316533}
episode index:2689
target Thresh 31.99999999994629
target distance 5.0
model initialize at round 2689
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 7.]), 'previousTarget': array([6., 7.]), 'currentState': array([5.51126636, 3.93349154, 1.29939577]), 'targetState': array([6, 7], dtype=int32), 'currentDistance': 3.1052108952693747}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9290503008224026
{'scaleFactor': 20, 'currentTarget': array([6., 7.]), 'previousTarget': array([6., 7.]), 'currentState': array([6.04967032, 7.8396052 , 1.03773496]), 'targetState': array([6, 7], dtype=int32), 'currentDistance': 0.8410731429337847}
episode index:2690
target Thresh 31.999999999946823
target distance 14.0
model initialize at round 2690
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.1439305 , 21.32059668,  3.79223359]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 12.321437354603331}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.929054919866839
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.08111665,  9.8249428 ,  4.92758376]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.8289213102078499}
episode index:2691
target Thresh 31.999999999947352
target distance 12.0
model initialize at round 2691
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  6.]), 'previousTarget': array([13.,  6.]), 'currentState': array([ 4.00895026, 16.65299057,  0.08207017]), 'targetState': array([13,  6], dtype=int32), 'currentDistance': 13.940056797394828}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9290560381536296
{'scaleFactor': 20, 'currentTarget': array([13.,  6.]), 'previousTarget': array([13.,  6.]), 'currentState': array([12.94172667,  6.26283243,  5.63521332]), 'targetState': array([13,  6], dtype=int32), 'currentDistance': 0.2692149127101351}
episode index:2692
target Thresh 31.999999999947878
target distance 18.0
model initialize at round 2692
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  5.]), 'previousTarget': array([19.,  5.]), 'currentState': array([21.9927141 , 24.34824449,  3.22244477]), 'targetState': array([19,  5], dtype=int32), 'currentDistance': 19.578327367303334}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9290468758947567
{'scaleFactor': 20, 'currentTarget': array([19.,  5.]), 'previousTarget': array([19.,  5.]), 'currentState': array([19.29031951,  5.25520763,  4.65227646]), 'targetState': array([19,  5], dtype=int32), 'currentDistance': 0.3865441154486028}
episode index:2693
target Thresh 31.999999999948397
target distance 10.0
model initialize at round 2693
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 28.]), 'previousTarget': array([10., 28.]), 'currentState': array([10.18715117, 19.80059199,  2.24046743]), 'targetState': array([10, 28], dtype=int32), 'currentDistance': 8.20154359124438}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9290585867834371
{'scaleFactor': 20, 'currentTarget': array([10., 28.]), 'previousTarget': array([10., 28.]), 'currentState': array([ 9.78447897, 27.61756698,  1.83824101]), 'targetState': array([10, 28], dtype=int32), 'currentDistance': 0.4389810097423934}
episode index:2694
target Thresh 31.99999999994891
target distance 20.0
model initialize at round 2694
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 10.]), 'previousTarget': array([23., 10.]), 'currentState': array([ 4.29344949, 24.92325507,  0.31577581]), 'targetState': array([23, 10], dtype=int32), 'currentDistance': 23.929867819866224}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9290427523808147
{'scaleFactor': 20, 'currentTarget': array([23., 10.]), 'previousTarget': array([23., 10.]), 'currentState': array([22.79236271, 10.51803268,  5.84352376]), 'targetState': array([23, 10], dtype=int32), 'currentDistance': 0.5580959630879146}
episode index:2695
target Thresh 31.999999999949416
target distance 23.0
model initialize at round 2695
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 19.]), 'previousTarget': array([27., 19.]), 'currentState': array([ 5.98197334, 20.2593255 ,  0.18900307]), 'targetState': array([27, 19], dtype=int32), 'currentDistance': 21.055720019052202}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9290302507123718
{'scaleFactor': 20, 'currentTarget': array([27., 19.]), 'previousTarget': array([27., 19.]), 'currentState': array([27.81747833, 19.2614233 ,  6.09305194]), 'targetState': array([27, 19], dtype=int32), 'currentDistance': 0.8582615957997445}
episode index:2696
target Thresh 31.99999999994992
target distance 3.0
model initialize at round 2696
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([17.65777919,  8.45089877,  4.10393763]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 2.2030295251976018}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9290528572193377
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.60309179,  6.75270564,  4.2103762 ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.6518237537651598}
episode index:2697
target Thresh 31.99999999995042
target distance 1.0
model initialize at round 2697
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 15.]), 'previousTarget': array([ 3., 15.]), 'currentState': array([ 2.20282317, 15.51889837,  3.49611878]), 'targetState': array([ 3, 15], dtype=int32), 'currentDistance': 0.9511815900007629}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9290791534175515
{'scaleFactor': 20, 'currentTarget': array([ 3., 15.]), 'previousTarget': array([ 3., 15.]), 'currentState': array([ 2.20282317, 15.51889837,  3.49611878]), 'targetState': array([ 3, 15], dtype=int32), 'currentDistance': 0.9511815900007629}
episode index:2698
target Thresh 31.999999999950912
target distance 9.0
model initialize at round 2698
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 12.]), 'previousTarget': array([ 3., 12.]), 'currentState': array([10.04376899, 12.8662696 ,  3.55726361]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 7.096837644934495}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9290908306522986
{'scaleFactor': 20, 'currentTarget': array([ 3., 12.]), 'previousTarget': array([ 3., 12.]), 'currentState': array([ 2.19426888, 11.61704286,  3.10897416]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 0.8921091910322625}
episode index:2699
target Thresh 31.9999999999514
target distance 13.0
model initialize at round 2699
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 17.]), 'previousTarget': array([26., 17.]), 'currentState': array([14.20443708,  3.82453439,  0.23677462]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 17.684122784379788}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9290850626585323
{'scaleFactor': 20, 'currentTarget': array([26., 17.]), 'previousTarget': array([26., 17.]), 'currentState': array([25.59531614, 17.24588572,  1.07193684]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 0.47352804741437543}
episode index:2700
target Thresh 31.999999999951886
target distance 21.0
model initialize at round 2700
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 16.]), 'previousTarget': array([ 2., 16.]), 'currentState': array([21.42497912, 22.08440034,  2.91946959]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 20.355582554332635}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9290759167912055
{'scaleFactor': 20, 'currentTarget': array([ 2., 16.]), 'previousTarget': array([ 2., 16.]), 'currentState': array([ 2.63777122, 15.916374  ,  3.6942015 ]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 0.6432304659869557}
episode index:2701
target Thresh 31.99999999995236
target distance 22.0
model initialize at round 2701
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 13.]), 'previousTarget': array([24., 13.]), 'currentState': array([ 1.29360068, 15.47245684,  5.28923702]), 'targetState': array([24, 13], dtype=int32), 'currentDistance': 22.8406132296232}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9290601169965811
{'scaleFactor': 20, 'currentTarget': array([24., 13.]), 'previousTarget': array([24., 13.]), 'currentState': array([24.00802658, 13.30621628,  0.03152714]), 'targetState': array([24, 13], dtype=int32), 'currentDistance': 0.306321458952615}
episode index:2702
target Thresh 31.999999999952838
target distance 11.0
model initialize at round 2702
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([ 4.91302111, 11.6807209 ,  0.61250114]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 14.483441020674562}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9290612288097185
{'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([15.33807715, 20.87847451,  0.87456704]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 0.6729861098184875}
episode index:2703
target Thresh 31.999999999953307
target distance 6.0
model initialize at round 2703
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 27.]), 'previousTarget': array([ 9., 27.]), 'currentState': array([16.67770081, 22.86692831,  0.93084782]), 'targetState': array([ 9, 27], dtype=int32), 'currentDistance': 8.719482287558721}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.929069338580832
{'scaleFactor': 20, 'currentTarget': array([ 9., 27.]), 'previousTarget': array([ 9., 27.]), 'currentState': array([ 8.55383957, 27.03745112,  2.6312181 ]), 'targetState': array([ 9, 27], dtype=int32), 'currentDistance': 0.44772951158276325}
episode index:2704
target Thresh 31.999999999953772
target distance 17.0
model initialize at round 2704
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 12.]), 'previousTarget': array([11., 12.]), 'currentState': array([20.00199113, 27.64781589,  4.340116  ]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 18.052423285256054}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9290635891941046
{'scaleFactor': 20, 'currentTarget': array([11., 12.]), 'previousTarget': array([11., 12.]), 'currentState': array([11.47989273, 11.89922556,  4.4394255 ]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 0.49035958257069406}
episode index:2705
target Thresh 31.99999999995423
target distance 14.0
model initialize at round 2705
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  4.]), 'previousTarget': array([24.,  4.]), 'currentState': array([10.65858275,  3.45124022,  6.12445641]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 13.352698291397095}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9290646984914857
{'scaleFactor': 20, 'currentTarget': array([24.,  4.]), 'previousTarget': array([24.,  4.]), 'currentState': array([24.54012901,  4.11019525,  5.9472181 ]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 0.5512552419194872}
episode index:2706
target Thresh 31.999999999954685
target distance 25.0
model initialize at round 2706
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 27.]), 'previousTarget': array([14., 27.]), 'currentState': array([3.68367272, 3.53785168, 2.16246855]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 25.63004120739835}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9290456576065605
{'scaleFactor': 20, 'currentTarget': array([14., 27.]), 'previousTarget': array([14., 27.]), 'currentState': array([13.51352338, 27.00827891,  1.38251245]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 0.48654705890098243}
episode index:2707
target Thresh 31.999999999955136
target distance 18.0
model initialize at round 2707
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 8.]), 'previousTarget': array([8., 8.]), 'currentState': array([20.66075334, 24.45216502,  4.10585833]), 'targetState': array([8, 8], dtype=int32), 'currentDistance': 20.75977863487325}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.929033210264113
{'scaleFactor': 20, 'currentTarget': array([8., 8.]), 'previousTarget': array([8., 8.]), 'currentState': array([7.42171237, 7.66506317, 3.23275578]), 'targetState': array([8, 8], dtype=int32), 'currentDistance': 0.6682808237234114}
episode index:2708
target Thresh 31.999999999955584
target distance 4.0
model initialize at round 2708
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 16.]), 'previousTarget': array([11., 16.]), 'currentState': array([16.32348132, 19.04148249,  1.67477423]), 'targetState': array([11, 16], dtype=int32), 'currentDistance': 6.131074055141801}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9290448613529781
{'scaleFactor': 20, 'currentTarget': array([11., 16.]), 'previousTarget': array([11., 16.]), 'currentState': array([10.69665369, 15.49812838,  3.94050613]), 'targetState': array([11, 16], dtype=int32), 'currentDistance': 0.5864248524949329}
episode index:2709
target Thresh 31.999999999956025
target distance 17.0
model initialize at round 2709
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([18.34747854, 19.11255089,  4.28465502]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 16.804006525582217}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9290425365681348
{'scaleFactor': 20, 'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([11.52302585,  4.69708316,  4.53048847]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 0.8714820575263811}
episode index:2710
target Thresh 31.99999999995646
target distance 19.0
model initialize at round 2710
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([18.40854753, 24.57561799,  2.93989843]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 26.317227965349293}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9290202950617538
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.47539024, 3.42824257, 3.30986347]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.7759651734686974}
episode index:2711
target Thresh 31.999999999956895
target distance 13.0
model initialize at round 2711
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([18.67003316, 22.03129835,  3.49201298]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 14.421026754119241}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9290179810497206
{'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([22.21427986,  7.19339945,  4.60530282]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 0.8345779243971497}
episode index:2712
target Thresh 31.999999999957325
target distance 17.0
model initialize at round 2712
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([22.37542817,  4.5605066 ,  2.39579725]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 16.30925748750953}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9290156687435569
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([7.32467352, 9.63938601, 3.12225463]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.4852374084390852}
episode index:2713
target Thresh 31.99999999995775
target distance 16.0
model initialize at round 2713
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 28.]), 'previousTarget': array([14., 28.]), 'currentState': array([21.55648806, 11.3598961 ,  0.61983412]), 'targetState': array([14, 28], dtype=int32), 'currentDistance': 18.275490955151128}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9290099581977721
{'scaleFactor': 20, 'currentTarget': array([14., 28.]), 'previousTarget': array([14., 28.]), 'currentState': array([14.01852872, 27.01149746,  2.25108936]), 'targetState': array([14, 28], dtype=int32), 'currentDistance': 0.9886761809292446}
episode index:2714
target Thresh 31.99999999995817
target distance 21.0
model initialize at round 2714
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 23.]), 'previousTarget': array([11., 23.]), 'currentState': array([4.71962104, 3.85499511, 1.37851754]), 'targetState': array([11, 23], dtype=int32), 'currentDistance': 20.148805724371623}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9290008871542402
{'scaleFactor': 20, 'currentTarget': array([11., 23.]), 'previousTarget': array([11., 23.]), 'currentState': array([10.41545098, 22.79896315,  1.18003644]), 'targetState': array([11, 23], dtype=int32), 'currentDistance': 0.618153199564732}
episode index:2715
target Thresh 31.999999999958586
target distance 8.0
model initialize at round 2715
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 23.]), 'previousTarget': array([12., 23.]), 'currentState': array([19.37625976, 26.56311746,  2.96047145]), 'targetState': array([12, 23], dtype=int32), 'currentDistance': 8.19176501270019}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9290125201155236
{'scaleFactor': 20, 'currentTarget': array([12., 23.]), 'previousTarget': array([12., 23.]), 'currentState': array([12.55153198, 22.87433413,  3.68062088]), 'targetState': array([12, 23], dtype=int32), 'currentDistance': 0.5656672453397863}
episode index:2716
target Thresh 31.999999999958998
target distance 11.0
model initialize at round 2716
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 26.]), 'previousTarget': array([10., 26.]), 'currentState': array([19.11638958, 25.35372856,  3.30972162]), 'targetState': array([10, 26], dtype=int32), 'currentDistance': 9.139268333401429}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9290206090112854
{'scaleFactor': 20, 'currentTarget': array([10., 26.]), 'previousTarget': array([10., 26.]), 'currentState': array([ 9.44819424, 26.47956872,  2.34004278]), 'targetState': array([10, 26], dtype=int32), 'currentDistance': 0.7310784920326994}
episode index:2717
target Thresh 31.999999999959407
target distance 18.0
model initialize at round 2717
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 21.]), 'previousTarget': array([24., 21.]), 'currentState': array([7.03433313, 5.7025729 , 1.18011969]), 'targetState': array([24, 21], dtype=int32), 'currentDistance': 22.84392978958235}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9290049225737227
{'scaleFactor': 20, 'currentTarget': array([24., 21.]), 'previousTarget': array([24., 21.]), 'currentState': array([24.4376726 , 21.40095157,  0.09745342]), 'targetState': array([24, 21], dtype=int32), 'currentDistance': 0.5935650460908966}
episode index:2718
target Thresh 31.99999999995981
target distance 8.0
model initialize at round 2718
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 16.]), 'previousTarget': array([12., 16.]), 'currentState': array([ 5.07074337, 11.29842183,  6.15439123]), 'targetState': array([12, 16], dtype=int32), 'currentDistance': 8.373734811202052}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9290165412156595
{'scaleFactor': 20, 'currentTarget': array([12., 16.]), 'previousTarget': array([12., 16.]), 'currentState': array([11.02657949, 16.20016735,  0.67067644]), 'targetState': array([12, 16], dtype=int32), 'currentDistance': 0.9937879327325987}
episode index:2719
target Thresh 31.99999999996021
target distance 21.0
model initialize at round 2719
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 10.]), 'previousTarget': array([25., 10.]), 'currentState': array([5.87875421, 2.61073281, 0.58847835]), 'targetState': array([25, 10], dtype=int32), 'currentDistance': 20.499349016277748}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9290074844266129
{'scaleFactor': 20, 'currentTarget': array([25., 10.]), 'previousTarget': array([25., 10.]), 'currentState': array([24.31243665, 10.10534058,  0.56552207]), 'targetState': array([25, 10], dtype=int32), 'currentDistance': 0.6955860778130016}
episode index:2720
target Thresh 31.999999999960608
target distance 9.0
model initialize at round 2720
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 19.]), 'previousTarget': array([19., 19.]), 'currentState': array([11.98456024, 17.98951981,  0.21215823]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 7.087839246228785}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9290190935870587
{'scaleFactor': 20, 'currentTarget': array([19., 19.]), 'previousTarget': array([19., 19.]), 'currentState': array([19.82270977, 19.18083869,  5.95393633]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 0.8423502793568157}
episode index:2721
target Thresh 31.999999999961
target distance 5.0
model initialize at round 2721
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([8.47460612, 7.18229503, 5.01039362]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 3.225374452234564}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9290378595335735
{'scaleFactor': 20, 'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([9.21262176, 3.32325534, 4.44591098]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 0.7093598122418716}
episode index:2722
target Thresh 31.999999999961386
target distance 13.0
model initialize at round 2722
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 11.]), 'previousTarget': array([17., 11.]), 'currentState': array([22.93320907, 22.69832985,  5.03583717]), 'targetState': array([17, 11], dtype=int32), 'currentDistance': 13.116931463886594}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9290389713544965
{'scaleFactor': 20, 'currentTarget': array([17., 11.]), 'previousTarget': array([17., 11.]), 'currentState': array([16.7666774 , 10.55701129,  3.70875494]), 'targetState': array([17, 11], dtype=int32), 'currentDistance': 0.5006779706549211}
episode index:2723
target Thresh 31.99999999996177
target distance 5.0
model initialize at round 2723
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 5.]), 'previousTarget': array([7., 5.]), 'currentState': array([10.12150276, 11.4354897 ,  3.12998128]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 7.152573470371286}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9290505561704456
{'scaleFactor': 20, 'currentTarget': array([7., 5.]), 'previousTarget': array([7., 5.]), 'currentState': array([7.05112351, 4.65212461, 4.05319491]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 0.35161185793579885}
episode index:2724
target Thresh 31.99999999996215
target distance 22.0
model initialize at round 2724
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 28.]), 'previousTarget': array([25., 28.]), 'currentState': array([23.7181082 ,  7.97022811,  1.88540703]), 'targetState': array([25, 28], dtype=int32), 'currentDistance': 20.07075007178637}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9290415035168084
{'scaleFactor': 20, 'currentTarget': array([25., 28.]), 'previousTarget': array([25., 28.]), 'currentState': array([24.31863541, 27.72425965,  1.48934892]), 'targetState': array([25, 28], dtype=int32), 'currentDistance': 0.7350445204442496}
episode index:2725
target Thresh 31.999999999962526
target distance 6.0
model initialize at round 2725
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([11.12853758, 25.21013731,  3.63296771]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 4.1338819845525565}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9290602337062741
{'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([ 7.2521686 , 24.37141729,  3.20097623]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 0.6772778054724355}
episode index:2726
target Thresh 31.9999999999629
target distance 18.0
model initialize at round 2726
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 14.]), 'previousTarget': array([ 5., 14.]), 'currentState': array([21.48773149, 15.73853371,  3.69729912]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 16.57913716720628}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9290579177769457
{'scaleFactor': 20, 'currentTarget': array([ 5., 14.]), 'previousTarget': array([ 5., 14.]), 'currentState': array([ 5.79984961, 13.49198796,  3.275771  ]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 0.9475418939839246}
episode index:2727
target Thresh 31.99999999996327
target distance 11.0
model initialize at round 2727
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 18.]), 'previousTarget': array([24., 18.]), 'currentState': array([23.99811438,  8.35226229,  1.19844532]), 'targetState': array([24, 18], dtype=int32), 'currentDistance': 9.647737896674844}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9290659574148208
{'scaleFactor': 20, 'currentTarget': array([24., 18.]), 'previousTarget': array([24., 18.]), 'currentState': array([23.46287008, 18.26481451,  1.82398798]), 'targetState': array([24, 18], dtype=int32), 'currentDistance': 0.5988616522752613}
episode index:2728
target Thresh 31.999999999963634
target distance 7.0
model initialize at round 2728
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 22.]), 'previousTarget': array([ 6., 22.]), 'currentState': array([ 8.23391646, 27.23925262,  4.79925263]), 'targetState': array([ 6, 22], dtype=int32), 'currentDistance': 5.695625583930668}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9290810666279338
{'scaleFactor': 20, 'currentTarget': array([ 6., 22.]), 'previousTarget': array([ 6., 22.]), 'currentState': array([ 6.45045085, 21.59879085,  4.51109879]), 'targetState': array([ 6, 22], dtype=int32), 'currentDistance': 0.6032203193748961}
episode index:2729
target Thresh 31.999999999963997
target distance 23.0
model initialize at round 2729
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 15.]), 'previousTarget': array([ 2., 15.]), 'currentState': array([23.34191257, 22.71167008,  4.32376409]), 'targetState': array([ 2, 15], dtype=int32), 'currentDistance': 22.69244560058932}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9290654269960978
{'scaleFactor': 20, 'currentTarget': array([ 2., 15.]), 'previousTarget': array([ 2., 15.]), 'currentState': array([ 1.29209016, 15.21584361,  2.21579844]), 'targetState': array([ 2, 15], dtype=int32), 'currentDistance': 0.7400843204941105}
episode index:2730
target Thresh 31.999999999964356
target distance 7.0
model initialize at round 2730
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  2.]), 'previousTarget': array([17.,  2.]), 'currentState': array([10.31944987,  3.34762599,  5.91336083]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 6.815118912624014}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9290805253384647
{'scaleFactor': 20, 'currentTarget': array([17.,  2.]), 'previousTarget': array([17.,  2.]), 'currentState': array([16.2021678 ,  2.48953653,  0.21141167]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 0.9360460689474132}
episode index:2731
target Thresh 31.99999999996471
target distance 7.0
model initialize at round 2731
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 21.]), 'previousTarget': array([19., 21.]), 'currentState': array([10.39645721, 21.48908062,  4.46004248]), 'targetState': array([19, 21], dtype=int32), 'currentDistance': 8.617432817187533}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9290885449301785
{'scaleFactor': 20, 'currentTarget': array([19., 21.]), 'previousTarget': array([19., 21.]), 'currentState': array([19.40234551, 21.23114885,  6.06587283]), 'targetState': array([19, 21], dtype=int32), 'currentDistance': 0.4640169177678126}
episode index:2732
target Thresh 31.99999999996506
target distance 20.0
model initialize at round 2732
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([20.99337938,  4.64147673,  0.07057684]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 21.358524300154357}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9290761957568628
{'scaleFactor': 20, 'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([20.18242656, 25.52404664,  1.3414177 ]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 0.9460221587426694}
episode index:2733
target Thresh 31.999999999965407
target distance 20.0
model initialize at round 2733
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 5.]), 'previousTarget': array([6., 5.]), 'currentState': array([24.22964489, 16.24718217,  4.02948594]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 21.42006208539831}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9290638556173243
{'scaleFactor': 20, 'currentTarget': array([6., 5.]), 'previousTarget': array([6., 5.]), 'currentState': array([6.07522636, 4.5578914 , 3.64942134]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 0.4484629510677041}
episode index:2734
target Thresh 31.999999999965752
target distance 14.0
model initialize at round 2734
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 24.]), 'previousTarget': array([16., 24.]), 'currentState': array([ 3.85166655, 23.90806706,  0.62651461]), 'targetState': array([16, 24], dtype=int32), 'currentDistance': 12.148681297802051}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9290683953956729
{'scaleFactor': 20, 'currentTarget': array([16., 24.]), 'previousTarget': array([16., 24.]), 'currentState': array([15.68261977, 24.08846021,  0.22792807]), 'targetState': array([16, 24], dtype=int32), 'currentDistance': 0.3294774906666647}
episode index:2735
target Thresh 31.999999999966093
target distance 6.0
model initialize at round 2735
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 8.9678954 , 11.37679577,  1.9680559 ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 7.10261450879195}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9290799186466248
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.73551983, 9.77518123, 3.41895733]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.3471213671790021}
episode index:2736
target Thresh 31.99999999996643
target distance 2.0
model initialize at round 2736
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 19.]), 'previousTarget': array([ 8., 19.]), 'currentState': array([ 7.46102146, 18.28182204,  1.13733965]), 'targetState': array([ 8, 19], dtype=int32), 'currentDistance': 0.8979295342631167}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9291058302583723
{'scaleFactor': 20, 'currentTarget': array([ 8., 19.]), 'previousTarget': array([ 8., 19.]), 'currentState': array([ 7.46102146, 18.28182204,  1.13733965]), 'targetState': array([ 8, 19], dtype=int32), 'currentDistance': 0.8979295342631167}
episode index:2737
target Thresh 31.999999999966764
target distance 7.0
model initialize at round 2737
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 13.]), 'previousTarget': array([18., 13.]), 'currentState': array([21.30988689, 18.46474487,  3.28037894]), 'targetState': array([18, 13], dtype=int32), 'currentDistance': 6.388958265706421}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9291173314197096
{'scaleFactor': 20, 'currentTarget': array([18., 13.]), 'previousTarget': array([18., 13.]), 'currentState': array([17.39023768, 12.78817692,  2.43928775]), 'targetState': array([18, 13], dtype=int32), 'currentDistance': 0.6455068618836459}
episode index:2738
target Thresh 31.999999999967095
target distance 5.0
model initialize at round 2738
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 18.]), 'previousTarget': array([17., 18.]), 'currentState': array([11.96534878, 18.31738674,  5.7017982 ]), 'targetState': array([17, 18], dtype=int32), 'currentDistance': 5.04464540020572}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9291323667130942
{'scaleFactor': 20, 'currentTarget': array([17., 18.]), 'previousTarget': array([17., 18.]), 'currentState': array([17.72961738, 18.02304691,  5.75593218]), 'targetState': array([17, 18], dtype=int32), 'currentDistance': 0.7299812915570276}
episode index:2739
target Thresh 31.99999999996742
target distance 5.0
model initialize at round 2739
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 14.]), 'previousTarget': array([24., 14.]), 'currentState': array([20.64143006, 17.62838805,  5.05054325]), 'targetState': array([24, 14], dtype=int32), 'currentDistance': 4.9442079107833115}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9291473910318121
{'scaleFactor': 20, 'currentTarget': array([24., 14.]), 'previousTarget': array([24., 14.]), 'currentState': array([24.5631743 , 13.36701664,  4.85881716]), 'targetState': array([24, 14], dtype=int32), 'currentDistance': 0.8472503870187855}
episode index:2740
target Thresh 31.99999999996775
target distance 15.0
model initialize at round 2740
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([ 9.28261509, 23.65907106,  2.41207051]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 17.476574918682065}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9291416886810101
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.17018928, 7.6793732 , 4.73530902]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.7003658630668749}
episode index:2741
target Thresh 31.999999999968068
target distance 21.0
model initialize at round 2741
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  4.]), 'previousTarget': array([25.,  4.]), 'currentState': array([ 9.4663831 , 23.7619757 ,  5.08139479]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 25.136207701955463}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9291228627635477
{'scaleFactor': 20, 'currentTarget': array([25.,  4.]), 'previousTarget': array([25.,  4.]), 'currentState': array([25.67870798,  3.62957744,  5.0298079 ]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 0.7732123852296782}
episode index:2742
target Thresh 31.999999999968384
target distance 24.0
model initialize at round 2742
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 16.]), 'previousTarget': array([ 3., 16.]), 'currentState': array([25.42888475, 25.76301527,  3.84204972]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 24.461629913904247}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9291072820158089
{'scaleFactor': 20, 'currentTarget': array([ 3., 16.]), 'previousTarget': array([ 3., 16.]), 'currentState': array([ 3.67206282, 16.11948844,  3.71511737]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 0.6826023155794427}
episode index:2743
target Thresh 31.9999999999687
target distance 14.0
model initialize at round 2743
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'previousTarget': array([19.,  3.]), 'currentState': array([25.11975292, 15.32129594,  3.77360487]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 13.757387450130627}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.92910836002816
{'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'previousTarget': array([19.,  3.]), 'currentState': array([19.31126231,  2.70610541,  4.45693615]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 0.4280867400158177}
episode index:2744
target Thresh 31.999999999969013
target distance 8.0
model initialize at round 2744
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  2.]), 'previousTarget': array([20.,  2.]), 'currentState': array([18.92102235,  8.22986562,  5.07357776]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 6.322611678678334}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9291233657257818
{'scaleFactor': 20, 'currentTarget': array([20.,  2.]), 'previousTarget': array([20.,  2.]), 'currentState': array([20.34284865,  2.42526573,  5.14919546]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 0.5462564759445102}
episode index:2745
target Thresh 31.99999999996932
target distance 22.0
model initialize at round 2745
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([ 5.73249719, 18.48479838,  6.17269468]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 26.307520914337164}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9291045739039586
{'scaleFactor': 20, 'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([26.81581779,  3.77364432,  5.66297816]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 0.7952663879238228}
episode index:2746
target Thresh 31.999999999969624
target distance 8.0
model initialize at round 2746
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 8.69639596, 14.94216999,  5.77538238]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 8.010022977412694}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9291160378413797
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.30349679, 10.48862605,  5.86984388]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.5752092818213804}
episode index:2747
target Thresh 31.999999999969926
target distance 10.0
model initialize at round 2747
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 16.]), 'previousTarget': array([15., 16.]), 'currentState': array([6.34949953, 9.05510116, 1.6221537 ]), 'targetState': array([15, 16], dtype=int32), 'currentDistance': 11.09336641262128}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9291205371541743
{'scaleFactor': 20, 'currentTarget': array([15., 16.]), 'previousTarget': array([15., 16.]), 'currentState': array([1.53971732e+01, 1.62037033e+01, 1.25817185e-03]), 'targetState': array([15, 16], dtype=int32), 'currentDistance': 0.44636490104653703}
episode index:2748
target Thresh 31.99999999997023
target distance 10.0
model initialize at round 2748
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 20.]), 'previousTarget': array([16., 20.]), 'currentState': array([26.34091218, 12.64807978,  2.37681898]), 'targetState': array([16, 20], dtype=int32), 'currentDistance': 12.687994155167273}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.929125033193551
{'scaleFactor': 20, 'currentTarget': array([16., 20.]), 'previousTarget': array([16., 20.]), 'currentState': array([16.3249277 , 19.08818878,  2.50073373]), 'targetState': array([16, 20], dtype=int32), 'currentDistance': 0.9679760900771475}
episode index:2749
target Thresh 31.999999999970523
target distance 13.0
model initialize at round 2749
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([19.41774209,  8.57345281,  3.80389023]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 11.672744569651861}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.929129525963081
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.86469572, 10.71798564,  3.14138626]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.31279282089371113}
episode index:2750
target Thresh 31.999999999970814
target distance 5.0
model initialize at round 2750
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([13.01455116, 28.18729365,  2.91497472]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 3.1221803752028703}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9291480539434652
{'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([ 9.34148735, 29.48292271,  2.24200665]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 0.8166108337265671}
episode index:2751
target Thresh 31.999999999971106
target distance 4.0
model initialize at round 2751
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([24.61399196, 10.52311416,  4.98589087]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 2.8307085908764145}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9291665684587475
{'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([27.82398613,  8.48207272,  5.96390891]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 0.9732429348800735}
episode index:2752
target Thresh 31.999999999971394
target distance 3.0
model initialize at round 2752
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  7.]), 'previousTarget': array([27.,  7.]), 'currentState': array([22.43482057,  7.38145217,  4.52794814]), 'targetState': array([27,  7], dtype=int32), 'currentDistance': 4.581088192026744}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.929181509407364
{'scaleFactor': 20, 'currentTarget': array([27.,  7.]), 'previousTarget': array([27.,  7.]), 'currentState': array([27.52332498,  6.76181472,  5.9147253 ]), 'targetState': array([27,  7], dtype=int32), 'currentDistance': 0.5749793591152897}
episode index:2753
target Thresh 31.999999999971678
target distance 10.0
model initialize at round 2753
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  3.]), 'previousTarget': array([17.,  3.]), 'currentState': array([25.47827677,  8.71143503,  3.97977301]), 'targetState': array([17,  3], dtype=int32), 'currentDistance': 10.22260568908554}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9291894282673832
{'scaleFactor': 20, 'currentTarget': array([17.,  3.]), 'previousTarget': array([17.,  3.]), 'currentState': array([17.37202122,  3.04683997,  3.60788749]), 'targetState': array([17,  3], dtype=int32), 'currentDistance': 0.37495836315389103}
episode index:2754
target Thresh 31.999999999971962
target distance 6.0
model initialize at round 2754
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 12.]), 'previousTarget': array([12., 12.]), 'currentState': array([7.84815483, 9.74495292, 0.53328449]), 'targetState': array([12, 12], dtype=int32), 'currentDistance': 4.724728107503398}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9292079076037653
{'scaleFactor': 20, 'currentTarget': array([12., 12.]), 'previousTarget': array([12., 12.]), 'currentState': array([11.21574985, 11.87297913,  0.78566547]), 'targetState': array([12, 12], dtype=int32), 'currentDistance': 0.794470014525637}
episode index:2755
target Thresh 31.99999999997224
target distance 4.0
model initialize at round 2755
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([13.7096324 , 17.18027896,  1.05291336]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 2.4326377255823646}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9292299656924431
{'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([15.42697155, 17.45664761,  5.53134513]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 0.7896793186068816}
episode index:2756
target Thresh 31.999999999972516
target distance 8.0
model initialize at round 2756
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 23.]), 'previousTarget': array([20., 23.]), 'currentState': array([13.28044488, 20.09217626,  5.97939891]), 'targetState': array([20, 23], dtype=int32), 'currentDistance': 7.321738855414652}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9292413425674185
{'scaleFactor': 20, 'currentTarget': array([20., 23.]), 'previousTarget': array([20., 23.]), 'currentState': array([20.26474822, 23.23692182,  0.05620483]), 'targetState': array([20, 23], dtype=int32), 'currentDistance': 0.3552795614828444}
episode index:2757
target Thresh 31.99999999997279
target distance 24.0
model initialize at round 2757
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  4.]), 'previousTarget': array([20.,  4.]), 'currentState': array([17.48704621, 27.21191266,  4.78585148]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 23.34754432698351}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9292258036004674
{'scaleFactor': 20, 'currentTarget': array([20.,  4.]), 'previousTarget': array([20.,  4.]), 'currentState': array([20.51077261,  3.68394539,  4.9632808 ]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 0.6006489648629691}
episode index:2758
target Thresh 31.99999999997306
target distance 4.0
model initialize at round 2758
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([11.69827867,  3.6557122 ,  0.74106419]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 2.3636245555407056}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9292478312178647
{'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([12.36618407,  5.46014727,  1.70111132]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 0.6523279425516841}
episode index:2759
target Thresh 31.99999999997333
target distance 9.0
model initialize at round 2759
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 29.]), 'previousTarget': array([ 6., 29.]), 'currentState': array([ 3.43436091, 19.11966094,  0.45953148]), 'targetState': array([ 6, 29], dtype=int32), 'currentDistance': 10.208016642902848}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9292557088333293
{'scaleFactor': 20, 'currentTarget': array([ 6., 29.]), 'previousTarget': array([ 6., 29.]), 'currentState': array([ 5.29249508, 28.61457298,  1.50638076]), 'targetState': array([ 6, 29], dtype=int32), 'currentDistance': 0.8056780964092809}
episode index:2760
target Thresh 31.999999999973593
target distance 17.0
model initialize at round 2760
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 18.]), 'previousTarget': array([ 7., 18.]), 'currentState': array([22.02298257,  9.27360647,  2.89147685]), 'targetState': array([ 7, 18], dtype=int32), 'currentDistance': 17.37354164826431}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9292500085575779
{'scaleFactor': 20, 'currentTarget': array([ 7., 18.]), 'previousTarget': array([ 7., 18.]), 'currentState': array([ 6.46926685, 17.84833335,  2.52471992]), 'targetState': array([ 7, 18], dtype=int32), 'currentDistance': 0.5519786643108603}
episode index:2761
target Thresh 31.999999999973856
target distance 14.0
model initialize at round 2761
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 24.]), 'previousTarget': array([16., 24.]), 'currentState': array([ 3.39808015, 19.3654179 ,  0.39798373]), 'targetState': array([16, 24], dtype=int32), 'currentDistance': 13.427126839133317}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.929251027869435
{'scaleFactor': 20, 'currentTarget': array([16., 24.]), 'previousTarget': array([16., 24.]), 'currentState': array([16.42756641, 24.30371176,  0.22221213]), 'targetState': array([16, 24], dtype=int32), 'currentDistance': 0.5244557800414734}
episode index:2762
target Thresh 31.999999999974115
target distance 3.0
model initialize at round 2762
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  9.]), 'previousTarget': array([18.,  9.]), 'currentState': array([19.26732893,  7.99804008,  2.58222533]), 'targetState': array([18,  9], dtype=int32), 'currentDistance': 1.6155637657801536}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.929273014468107
{'scaleFactor': 20, 'currentTarget': array([18.,  9.]), 'previousTarget': array([18.,  9.]), 'currentState': array([17.54439345,  9.01317885,  2.6366359 ]), 'targetState': array([18,  9], dtype=int32), 'currentDistance': 0.45579711259522615}
episode index:2763
target Thresh 31.999999999974374
target distance 24.0
model initialize at round 2763
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 29.]), 'previousTarget': array([ 8., 29.]), 'currentState': array([12.57304224,  6.58240661,  2.23335342]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 22.87927465832301}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.929257497773913
{'scaleFactor': 20, 'currentTarget': array([ 8., 29.]), 'previousTarget': array([ 8., 29.]), 'currentState': array([ 8.31642798, 29.82931318,  1.12629949]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 0.8876300028477614}
episode index:2764
target Thresh 31.99999999997463
target distance 3.0
model initialize at round 2764
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 4.]), 'previousTarget': array([7., 4.]), 'currentState': array([9.08225407, 5.41071985, 3.15755916]), 'targetState': array([7, 4], dtype=int32), 'currentDistance': 2.515136675455105}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9292794661291484
{'scaleFactor': 20, 'currentTarget': array([7., 4.]), 'previousTarget': array([7., 4.]), 'currentState': array([7.30668792, 4.61709798, 3.974455  ]), 'targetState': array([7, 4], dtype=int32), 'currentDistance': 0.689106224883473}
episode index:2765
target Thresh 31.999999999974882
target distance 13.0
model initialize at round 2765
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 14.]), 'previousTarget': array([25., 14.]), 'currentState': array([13.8207374 , 15.61188366,  6.11209178]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 11.29486968689747}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9292838770775473
{'scaleFactor': 20, 'currentTarget': array([25., 14.]), 'previousTarget': array([25., 14.]), 'currentState': array([25.54561971, 13.98891851,  5.67314178]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 0.5457322312584905}
episode index:2766
target Thresh 31.99999999997513
target distance 8.0
model initialize at round 2766
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 25.]), 'previousTarget': array([ 9., 25.]), 'currentState': array([18.31291375, 18.05292239,  1.6859433 ]), 'targetState': array([ 9, 25], dtype=int32), 'currentDistance': 11.618616522000941}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9292882848376931
{'scaleFactor': 20, 'currentTarget': array([ 9., 25.]), 'previousTarget': array([ 9., 25.]), 'currentState': array([ 8.75661775, 24.58788278,  2.57989683]), 'targetState': array([ 9, 25], dtype=int32), 'currentDistance': 0.4786183479001256}
episode index:2767
target Thresh 31.99999999997538
target distance 24.0
model initialize at round 2767
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7.63274368, 25.11616063]), 'previousTarget': array([ 7., 26.]), 'currentState': array([25.09601011,  0.7228352 ,  0.14841526]), 'targetState': array([ 7, 26], dtype=int32), 'currentDistance': 29.999999999999996}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.9292601668775259
{'scaleFactor': 20, 'currentTarget': array([ 7., 26.]), 'previousTarget': array([ 7., 26.]), 'currentState': array([ 6.76616242, 25.11672947,  2.20370562]), 'targetState': array([ 7, 26], dtype=int32), 'currentDistance': 0.913699539258837}
episode index:2768
target Thresh 31.999999999975625
target distance 9.0
model initialize at round 2768
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([18.28320771, 19.47456435,  5.55413613]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 9.410144858376112}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9292680144336916
{'scaleFactor': 20, 'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([24.59175857, 11.82646732,  5.16844457]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 0.6166780306414962}
episode index:2769
target Thresh 31.999999999975866
target distance 16.0
model initialize at round 2769
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 20.]), 'previousTarget': array([ 7., 20.]), 'currentState': array([18.74756301,  5.66393018,  2.73135948]), 'targetState': array([ 7, 20], dtype=int32), 'currentDistance': 18.53451198576966}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9292623282362366
{'scaleFactor': 20, 'currentTarget': array([ 7., 20.]), 'previousTarget': array([ 7., 20.]), 'currentState': array([ 7.08774567, 19.01618468,  2.19817656]), 'targetState': array([ 7, 20], dtype=int32), 'currentDistance': 0.9877205473750786}
episode index:2770
target Thresh 31.999999999976108
target distance 10.0
model initialize at round 2770
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([ 3.48471939, 14.73233378,  3.7013948 ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 10.834545674142026}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9292667374102404
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.89155914, 3.22594448, 4.21014148]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.7816145955482715}
episode index:2771
target Thresh 31.999999999976342
target distance 4.0
model initialize at round 2771
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 21.]), 'previousTarget': array([20., 21.]), 'currentState': array([22.05021342, 26.38935004,  3.18044376]), 'targetState': array([20, 21], dtype=int32), 'currentDistance': 5.76614853593941}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.929281539813772
{'scaleFactor': 20, 'currentTarget': array([20., 21.]), 'previousTarget': array([20., 21.]), 'currentState': array([20.16858972, 21.28075597,  4.66964918]), 'targetState': array([20, 21], dtype=int32), 'currentDistance': 0.32748497576529284}
episode index:2772
target Thresh 31.99999999997658
target distance 11.0
model initialize at round 2772
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 20.]), 'previousTarget': array([19., 20.]), 'currentState': array([ 9.58410707, 27.43167537,  4.92872209]), 'targetState': array([19, 20], dtype=int32), 'currentDistance': 11.995367376995338}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9292859388796166
{'scaleFactor': 20, 'currentTarget': array([19., 20.]), 'previousTarget': array([19., 20.]), 'currentState': array([19.01028428, 20.59949441,  5.77442937]), 'targetState': array([19, 20], dtype=int32), 'currentDistance': 0.5995826191477424}
episode index:2773
target Thresh 31.99999999997681
target distance 12.0
model initialize at round 2773
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 7.]), 'previousTarget': array([5., 7.]), 'currentState': array([12.42420989, 17.41859119,  3.35320902]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 12.793198772401283}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9292869408295185
{'scaleFactor': 20, 'currentTarget': array([5., 7.]), 'previousTarget': array([5., 7.]), 'currentState': array([4.47941869, 6.49210414, 3.40327461]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 0.7272984938517896}
episode index:2774
target Thresh 31.999999999977042
target distance 21.0
model initialize at round 2774
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 25.]), 'previousTarget': array([11., 25.]), 'currentState': array([6.56656376, 5.88285329, 1.60073227]), 'targetState': array([11, 25], dtype=int32), 'currentDistance': 19.624491210545116}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9292779661030967
{'scaleFactor': 20, 'currentTarget': array([11., 25.]), 'previousTarget': array([11., 25.]), 'currentState': array([10.85125277, 25.28247157,  1.2458082 ]), 'targetState': array([11, 25], dtype=int32), 'currentDistance': 0.319242740533898}
episode index:2775
target Thresh 31.999999999977273
target distance 7.0
model initialize at round 2775
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 23.]), 'previousTarget': array([27., 23.]), 'currentState': array([25.27175669, 17.97878951,  1.34446262]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 5.310308817122732}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9292927431325984
{'scaleFactor': 20, 'currentTarget': array([27., 23.]), 'previousTarget': array([27., 23.]), 'currentState': array([27.1837405 , 23.6173485 ,  0.86843944]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 0.6441115911213551}
episode index:2776
target Thresh 31.999999999977497
target distance 8.0
model initialize at round 2776
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([23.14961248, 20.13647542,  3.7406137 ]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 6.151126664952778}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.929307509519659
{'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([17.29879584, 19.52795766,  3.3782952 ]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 0.5586617281264743}
episode index:2777
target Thresh 31.99999999997772
target distance 13.0
model initialize at round 2777
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([ 5.79501427, 21.51663847,  6.2143743 ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 13.58472602146526}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9293085022620592
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13.43585749, 10.15241116,  5.56126441]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.46173684758874994}
episode index:2778
target Thresh 31.999999999977945
target distance 8.0
model initialize at round 2778
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 13.]), 'previousTarget': array([12., 13.]), 'currentState': array([3.20130246, 6.51862553, 5.22790647]), 'targetState': array([12, 13], dtype=int32), 'currentDistance': 10.928188024082383}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9293128821278882
{'scaleFactor': 20, 'currentTarget': array([12., 13.]), 'previousTarget': array([12., 13.]), 'currentState': array([11.77377156, 13.57165363,  1.0353522 ]), 'targetState': array([12, 13], dtype=int32), 'currentDistance': 0.6147903549898284}
episode index:2779
target Thresh 31.99999999997816
target distance 21.0
model initialize at round 2779
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([19.72112366, 25.09401252,  3.4439441 ]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 22.16094889046188}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9293006610387265
{'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([18.59504612,  3.71302746,  4.76118952]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 0.9287023477837731}
episode index:2780
target Thresh 31.99999999997838
target distance 5.0
model initialize at round 2780
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 15.]), 'previousTarget': array([18., 15.]), 'currentState': array([18.56866142, 11.67745703,  0.8976562 ]), 'targetState': array([18, 15], dtype=int32), 'currentDistance': 3.3708556191152526}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9293189276115282
{'scaleFactor': 20, 'currentTarget': array([18., 15.]), 'previousTarget': array([18., 15.]), 'currentState': array([17.53642184, 15.07948635,  2.53152323]), 'targetState': array([18, 15], dtype=int32), 'currentDistance': 0.4703432677619742}
episode index:2781
target Thresh 31.999999999978595
target distance 3.0
model initialize at round 2781
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 20.]), 'previousTarget': array([25., 20.]), 'currentState': array([28.6170601 , 17.46637399,  1.29078883]), 'targetState': array([25, 20], dtype=int32), 'currentDistance': 4.41615042081466}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9293371810523581
{'scaleFactor': 20, 'currentTarget': array([25., 20.]), 'previousTarget': array([25., 20.]), 'currentState': array([25.77839376, 19.49356243,  2.16794592]), 'targetState': array([25, 20], dtype=int32), 'currentDistance': 0.9286419444813386}
episode index:2782
target Thresh 31.999999999978808
target distance 24.0
model initialize at round 2782
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.47121979,  2.7050999 ]), 'previousTarget': array([20.,  2.]), 'currentState': array([ 1.47219391, 26.70583046,  3.71881306]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 29.999999999999996}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.9293091970746515
{'scaleFactor': 20, 'currentTarget': array([20.,  2.]), 'previousTarget': array([20.,  2.]), 'currentState': array([20.32303403,  1.91415288,  5.21622392]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 0.3342464814106961}
episode index:2783
target Thresh 31.999999999979018
target distance 25.0
model initialize at round 2783
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 14.]), 'previousTarget': array([27., 14.]), 'currentState': array([ 2.86904312, 17.44123285,  0.01818419]), 'targetState': array([27, 14], dtype=int32), 'currentDistance': 24.3750930922069}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9292937788543358
{'scaleFactor': 20, 'currentTarget': array([27., 14.]), 'previousTarget': array([27., 14.]), 'currentState': array([26.67770331, 14.89359046,  6.11139712]), 'targetState': array([27, 14], dtype=int32), 'currentDistance': 0.9499363523027154}
episode index:2784
target Thresh 31.999999999979227
target distance 6.0
model initialize at round 2784
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([22.22186788, 27.66828137,  2.44858029]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 9.277415768336278}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9293015692568658
{'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([17.17404521, 20.02291119,  4.3740524 ]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 0.17554673770617316}
episode index:2785
target Thresh 31.999999999979433
target distance 14.0
model initialize at round 2785
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'previousTarget': array([22., 18.]), 'currentState': array([8.26805225, 6.3385139 , 5.88234353]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 18.01545578482299}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9292959036711611
{'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'previousTarget': array([22., 18.]), 'currentState': array([21.56254497, 17.54883458,  0.78992288]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 0.6284243304190523}
episode index:2786
target Thresh 31.99999999997964
target distance 11.0
model initialize at round 2786
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 14.]), 'previousTarget': array([11., 14.]), 'currentState': array([23.52123091,  3.71989209,  1.45200318]), 'targetState': array([11, 14], dtype=int32), 'currentDistance': 16.200674124899706}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9292935530399291
{'scaleFactor': 20, 'currentTarget': array([11., 14.]), 'previousTarget': array([11., 14.]), 'currentState': array([11.51738914, 13.20033993,  2.21571513]), 'targetState': array([11, 14], dtype=int32), 'currentDistance': 0.9524430402600302}
episode index:2787
target Thresh 31.999999999979842
target distance 5.0
model initialize at round 2787
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 28.]), 'previousTarget': array([12., 28.]), 'currentState': array([12.17298955, 24.99238915,  1.46542485]), 'targetState': array([12, 28], dtype=int32), 'currentDistance': 3.012581689185266}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9293117762992406
{'scaleFactor': 20, 'currentTarget': array([12., 28.]), 'previousTarget': array([12., 28.]), 'currentState': array([11.99161567, 28.98051591,  1.62524268]), 'targetState': array([12, 28], dtype=int32), 'currentDistance': 0.9805517564208543}
episode index:2788
target Thresh 31.99999999998004
target distance 16.0
model initialize at round 2788
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'previousTarget': array([11., 21.]), 'currentState': array([27.01767634, 21.68287719,  2.57029307]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 16.032226192536243}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9293094216624993
{'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'previousTarget': array([11., 21.]), 'currentState': array([11.49249485, 20.93229924,  3.035405  ]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 0.49712631497350884}
episode index:2789
target Thresh 31.99999999998024
target distance 15.0
model initialize at round 2789
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 15.]), 'previousTarget': array([24., 15.]), 'currentState': array([ 8.29304388, 17.47271446,  5.28887248]), 'targetState': array([24, 15], dtype=int32), 'currentDistance': 15.900402114542752}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9293070687136696
{'scaleFactor': 20, 'currentTarget': array([24., 15.]), 'previousTarget': array([24., 15.]), 'currentState': array([23.63476554, 15.27733644,  6.04618875]), 'targetState': array([24, 15], dtype=int32), 'currentDistance': 0.4585975485343115}
episode index:2790
target Thresh 31.99999999998044
target distance 6.0
model initialize at round 2790
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([ 8.16264541, 17.63794179,  2.39523578]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 6.0295247457630445}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9293217558979356
{'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([ 4.01815207, 21.94706924,  2.56052392]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 0.05595679331643974}
episode index:2791
target Thresh 31.99999999998063
target distance 6.0
model initialize at round 2791
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 21.]), 'previousTarget': array([ 8., 21.]), 'currentState': array([15.23274877, 22.1457393 ,  1.75883263]), 'targetState': array([ 8, 21], dtype=int32), 'currentDistance': 7.322934749259386}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9293329572783446
{'scaleFactor': 20, 'currentTarget': array([ 8., 21.]), 'previousTarget': array([ 8., 21.]), 'currentState': array([ 8.02710368, 20.95425811,  3.54274174]), 'targetState': array([ 8, 21], dtype=int32), 'currentDistance': 0.05316888004098382}
episode index:2792
target Thresh 31.999999999980826
target distance 8.0
model initialize at round 2792
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([ 7.40920413, 15.68115972,  5.06583065]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 7.165899698318612}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.929344150637715
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.53507326,  8.35824268,  5.39078803]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.8355572085782552}
episode index:2793
target Thresh 31.999999999981014
target distance 13.0
model initialize at round 2793
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 14.]), 'previousTarget': array([20., 14.]), 'currentState': array([ 8.77888868, 23.08943909,  5.8802724 ]), 'targetState': array([20, 14], dtype=int32), 'currentDistance': 14.440610870953526}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9293451245809038
{'scaleFactor': 20, 'currentTarget': array([20., 14.]), 'previousTarget': array([20., 14.]), 'currentState': array([19.23565878, 14.10324402,  5.56340384]), 'targetState': array([20, 14], dtype=int32), 'currentDistance': 0.7712825889937139}
episode index:2794
target Thresh 31.999999999981206
target distance 15.0
model initialize at round 2794
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 13.]), 'previousTarget': array([ 3., 13.]), 'currentState': array([16.54911436,  9.14717003,  2.66299295]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 14.086262769284271}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9293460978271744
{'scaleFactor': 20, 'currentTarget': array([ 3., 13.]), 'previousTarget': array([ 3., 13.]), 'currentState': array([ 3.23897119, 13.02518075,  2.91344261]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 0.24029419237120098}
episode index:2795
target Thresh 31.99999999998139
target distance 4.0
model initialize at round 2795
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 19.]), 'previousTarget': array([21., 19.]), 'currentState': array([24.77213017, 20.66747217,  2.71661091]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 4.124248959062671}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9293642501527012
{'scaleFactor': 20, 'currentTarget': array([21., 19.]), 'previousTarget': array([21., 19.]), 'currentState': array([21.47345448, 18.92382852,  3.61052844]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 0.47954274387227663}
episode index:2796
target Thresh 31.999999999981576
target distance 3.0
model initialize at round 2796
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  7.]), 'previousTarget': array([26.,  7.]), 'currentState': array([23.71329544,  5.08479823,  1.43113208]), 'targetState': array([26,  7], dtype=int32), 'currentDistance': 2.9827865386333303}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9293859290049883
{'scaleFactor': 20, 'currentTarget': array([26.,  7.]), 'previousTarget': array([26.,  7.]), 'currentState': array([25.04830222,  6.36996208,  0.08819389]), 'targetState': array([26,  7], dtype=int32), 'currentDistance': 1.141348520045354}
episode index:2797
target Thresh 31.99999999998176
target distance 8.0
model initialize at round 2797
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([13.42591844,  9.59552944,  1.76990175]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 9.179693630599878}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9293936502776456
{'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([ 7.54032385, 17.42489234,  2.24279885]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 0.6259677830884322}
episode index:2798
target Thresh 31.99999999998194
target distance 22.0
model initialize at round 2798
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 10.]), 'previousTarget': array([26., 10.]), 'currentState': array([ 2.46841707, 22.69759717,  3.72419536]), 'targetState': array([26, 10], dtype=int32), 'currentDistance': 26.73881764584817}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9293719825972209
{'scaleFactor': 20, 'currentTarget': array([26., 10.]), 'previousTarget': array([26., 10.]), 'currentState': array([25.19563524,  9.75687039,  5.80284163]), 'targetState': array([26, 10], dtype=int32), 'currentDistance': 0.8403062979416884}
episode index:2799
target Thresh 31.999999999982123
target distance 5.0
model initialize at round 2799
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 27.]), 'previousTarget': array([15., 27.]), 'currentState': array([14.69756246, 23.53170887,  2.15328074]), 'targetState': array([15, 27], dtype=int32), 'currentDistance': 3.481452547485502}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9293900997462934
{'scaleFactor': 20, 'currentTarget': array([15., 27.]), 'previousTarget': array([15., 27.]), 'currentState': array([15.40606786, 27.26892255,  1.31248564]), 'targetState': array([15, 27], dtype=int32), 'currentDistance': 0.48704254780515094}
episode index:2800
target Thresh 31.9999999999823
target distance 22.0
model initialize at round 2800
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.60972275, 23.52706301]), 'previousTarget': array([26., 24.]), 'currentState': array([6.51519985, 0.38836825, 5.43018627]), 'targetState': array([26, 24], dtype=int32), 'currentDistance': 30.000000000000004}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.9293622767085742
{'scaleFactor': 20, 'currentTarget': array([26., 24.]), 'previousTarget': array([26., 24.]), 'currentState': array([25.33853965, 23.75314677,  0.34330269]), 'targetState': array([26, 24], dtype=int32), 'currentDistance': 0.706021461877467}
episode index:2801
target Thresh 31.999999999982474
target distance 12.0
model initialize at round 2801
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 19.]), 'previousTarget': array([14., 19.]), 'currentState': array([ 1.52015772, 17.38688517,  5.43326116]), 'targetState': array([14, 19], dtype=int32), 'currentDistance': 12.58366412937538}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9293632414020784
{'scaleFactor': 20, 'currentTarget': array([14., 19.]), 'previousTarget': array([14., 19.]), 'currentState': array([14.78470399, 18.96803562,  0.12844408]), 'targetState': array([14, 19], dtype=int32), 'currentDistance': 0.7853547399877777}
episode index:2802
target Thresh 31.99999999998265
target distance 11.0
model initialize at round 2802
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 24.]), 'previousTarget': array([11., 24.]), 'currentState': array([20.42249731, 25.86404178,  3.34971404]), 'targetState': array([11, 24], dtype=int32), 'currentDistance': 9.605108398967245}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.929370956995549
{'scaleFactor': 20, 'currentTarget': array([11., 24.]), 'previousTarget': array([11., 24.]), 'currentState': array([10.60501157, 24.13198541,  3.52399805]), 'targetState': array([11, 24], dtype=int32), 'currentDistance': 0.41645649474823343}
episode index:2803
target Thresh 31.999999999982823
target distance 8.0
model initialize at round 2803
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 17.]), 'previousTarget': array([15., 17.]), 'currentState': array([21.2430004 , 16.30686751,  2.92892885]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 6.2813602538614015}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9293855533018986
{'scaleFactor': 20, 'currentTarget': array([15., 17.]), 'previousTarget': array([15., 17.]), 'currentState': array([15.36420675, 17.37919494,  3.2257985 ]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 0.5257712040378691}
episode index:2804
target Thresh 31.999999999982993
target distance 23.0
model initialize at round 2804
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([25.38610062, 25.5228031 ,  4.43908095]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 23.702977489519192}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9293702232906381
{'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([18.10032443,  2.92931977,  4.27821179]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 0.12272198762817868}
episode index:2805
target Thresh 31.999999999983164
target distance 22.0
model initialize at round 2805
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([25.16456008, 16.53903175,  3.18293285]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 23.64338794924213}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9293549042059714
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.17683991, 5.94040717, 3.36331497]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.18661098609352456}
episode index:2806
target Thresh 31.99999999998333
target distance 13.0
model initialize at round 2806
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([23.93236635, 13.30097905,  3.26799572]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 14.535735959074437}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.929355869807575
{'scaleFactor': 20, 'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([12.75636129,  5.26076372,  3.51468559]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 0.8000500750491006}
episode index:2807
target Thresh 31.999999999983494
target distance 9.0
model initialize at round 2807
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([18.00786397, 10.07805731,  3.24174371]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 7.008298674491778}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9293669952136263
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([10.02405539,  9.99095529,  3.20841592]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.9759865170873886}
episode index:2808
target Thresh 31.99999999998366
target distance 11.0
model initialize at round 2808
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 21.]), 'previousTarget': array([ 7., 21.]), 'currentState': array([18.61542236, 13.56641099,  2.20643693]), 'targetState': array([ 7, 21], dtype=int32), 'currentDistance': 13.790441694013955}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9293679558233428
{'scaleFactor': 20, 'currentTarget': array([ 7., 21.]), 'previousTarget': array([ 7., 21.]), 'currentState': array([ 6.98440287, 21.02776648,  2.56967465]), 'targetState': array([ 7, 21], dtype=int32), 'currentDistance': 0.03184726006568468}
episode index:2809
target Thresh 31.99999999998382
target distance 11.0
model initialize at round 2809
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 14.]), 'previousTarget': array([21., 14.]), 'currentState': array([11.37470023,  9.9708748 ,  5.88809532]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 10.434569735215804}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9293756505187438
{'scaleFactor': 20, 'currentTarget': array([21., 14.]), 'previousTarget': array([21., 14.]), 'currentState': array([20.27389577, 13.64774467,  0.38312332]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 0.8070385227971432}
episode index:2810
target Thresh 31.999999999983984
target distance 3.0
model initialize at round 2810
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 20.]), 'previousTarget': array([16., 20.]), 'currentState': array([12.84326707, 18.22411471,  1.3199439 ]), 'targetState': array([16, 20], dtype=int32), 'currentDistance': 3.621978931957516}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9293936954669763
{'scaleFactor': 20, 'currentTarget': array([16., 20.]), 'previousTarget': array([16., 20.]), 'currentState': array([16.24246288, 19.76476904,  0.58694875]), 'targetState': array([16, 20], dtype=int32), 'currentDistance': 0.33781926222532915}
episode index:2811
target Thresh 31.999999999984144
target distance 13.0
model initialize at round 2811
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 17.]), 'previousTarget': array([15., 17.]), 'currentState': array([3.35939587, 5.4269056 , 1.10927546]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 16.41463305936164}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9293913309573606
{'scaleFactor': 20, 'currentTarget': array([15., 17.]), 'previousTarget': array([15., 17.]), 'currentState': array([14.43352986, 16.73671386,  0.70114579]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 0.6246663246064073}
episode index:2812
target Thresh 31.9999999999843
target distance 13.0
model initialize at round 2812
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 23.]), 'previousTarget': array([ 3., 23.]), 'currentState': array([11.68410418, 11.04919315,  1.45848656]), 'targetState': array([ 3, 23], dtype=int32), 'currentDistance': 14.772794240957534}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9293889681288752
{'scaleFactor': 20, 'currentTarget': array([ 3., 23.]), 'previousTarget': array([ 3., 23.]), 'currentState': array([ 2.5128085 , 23.53935483,  2.24883003]), 'targetState': array([ 3, 23], dtype=int32), 'currentDistance': 0.7268144136868103}
episode index:2813
target Thresh 31.999999999984457
target distance 18.0
model initialize at round 2813
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 16.]), 'previousTarget': array([ 8., 16.]), 'currentState': array([24.14199373, 18.66570234,  3.91054463]), 'targetState': array([ 8, 16], dtype=int32), 'currentDistance': 16.360621335877777}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9293866069797277
{'scaleFactor': 20, 'currentTarget': array([ 8., 16.]), 'previousTarget': array([ 8., 16.]), 'currentState': array([ 8.84219111, 15.76715329,  3.00055939]), 'targetState': array([ 8, 16], dtype=int32), 'currentDistance': 0.8737868519830827}
episode index:2814
target Thresh 31.999999999984613
target distance 16.0
model initialize at round 2814
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 5.]), 'previousTarget': array([6., 5.]), 'currentState': array([21.03324315,  8.37759547,  3.19271004]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 15.4080028107101}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9293842475081284
{'scaleFactor': 20, 'currentTarget': array([6., 5.]), 'previousTarget': array([6., 5.]), 'currentState': array([5.80162451, 4.97724223, 3.27560525]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 0.19967661828596409}
episode index:2815
target Thresh 31.999999999984766
target distance 21.0
model initialize at round 2815
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 23.]), 'previousTarget': array([ 4., 23.]), 'currentState': array([5.84339674, 3.45638938, 2.0558854 ]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 19.630354748837032}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9293753688957352
{'scaleFactor': 20, 'currentTarget': array([ 4., 23.]), 'previousTarget': array([ 4., 23.]), 'currentState': array([ 4.01132809, 22.82883369,  1.56560373]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 0.17154075224773377}
episode index:2816
target Thresh 31.999999999984915
target distance 6.0
model initialize at round 2816
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([4.82198834, 8.46857865, 0.05052352]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 5.382241912533262}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.929389896276319
{'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([10.52989246,  6.87400069,  6.20250556]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 0.5446667284284735}
episode index:2817
target Thresh 31.999999999985068
target distance 6.0
model initialize at round 2817
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 28.]), 'previousTarget': array([14., 28.]), 'currentState': array([18.13408589, 24.68188636,  2.5885554 ]), 'targetState': array([14, 28], dtype=int32), 'currentDistance': 5.300994653587236}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9294044133464835
{'scaleFactor': 20, 'currentTarget': array([14., 28.]), 'previousTarget': array([14., 28.]), 'currentState': array([13.39502075, 28.29421624,  2.81205123]), 'targetState': array([14, 28], dtype=int32), 'currentDistance': 0.672728096067276}
episode index:2818
target Thresh 31.999999999985214
target distance 16.0
model initialize at round 2818
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 17.]), 'previousTarget': array([19., 17.]), 'currentState': array([ 2.54900805, 16.37858261,  5.45109963]), 'targetState': array([19, 17], dtype=int32), 'currentDistance': 16.462724435225642}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9293987776012324
{'scaleFactor': 20, 'currentTarget': array([19., 17.]), 'previousTarget': array([19., 17.]), 'currentState': array([19.9387663 , 17.28323809,  0.32850037]), 'targetState': array([19, 17], dtype=int32), 'currentDistance': 0.9805641170798384}
episode index:2819
target Thresh 31.999999999985363
target distance 19.0
model initialize at round 2819
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 27.]), 'previousTarget': array([19., 27.]), 'currentState': array([24.46311246,  8.83167904,  1.52688378]), 'targetState': array([19, 27], dtype=int32), 'currentDistance': 18.97191303983037}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9293899064301003
{'scaleFactor': 20, 'currentTarget': array([19., 27.]), 'previousTarget': array([19., 27.]), 'currentState': array([19.07568012, 27.58572903,  1.93813691]), 'targetState': array([19, 27], dtype=int32), 'currentDistance': 0.5905979828581797}
episode index:2820
target Thresh 31.99999999998551
target distance 21.0
model initialize at round 2820
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 18.]), 'previousTarget': array([24., 18.]), 'currentState': array([ 4.95567529, 18.01091779,  0.37501055]), 'targetState': array([24, 18], dtype=int32), 'currentDistance': 19.04432783786576}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9293810415483488
{'scaleFactor': 20, 'currentTarget': array([24., 18.]), 'previousTarget': array([24., 18.]), 'currentState': array([24.69720744, 18.08068224,  0.21125121]), 'targetState': array([24, 18], dtype=int32), 'currentDistance': 0.7018602711721726}
episode index:2821
target Thresh 31.99999999998565
target distance 14.0
model initialize at round 2821
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 19.]), 'previousTarget': array([13., 19.]), 'currentState': array([11.30593653,  6.06156388,  1.69254273]), 'targetState': array([13, 19], dtype=int32), 'currentDistance': 13.048868930049432}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9293819927554213
{'scaleFactor': 20, 'currentTarget': array([13., 19.]), 'previousTarget': array([13., 19.]), 'currentState': array([12.95415846, 19.80212011,  1.69457918]), 'targetState': array([13, 19], dtype=int32), 'currentDistance': 0.8034289733913229}
episode index:2822
target Thresh 31.999999999985796
target distance 14.0
model initialize at round 2822
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 20.]), 'previousTarget': array([ 6., 20.]), 'currentState': array([21.65963833, 12.72073545,  0.84329336]), 'targetState': array([ 6, 20], dtype=int32), 'currentDistance': 17.268814814989582}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9293763729377551
{'scaleFactor': 20, 'currentTarget': array([ 6., 20.]), 'previousTarget': array([ 6., 20.]), 'currentState': array([ 6.45561572, 19.91840554,  2.86949689]), 'targetState': array([ 6, 20], dtype=int32), 'currentDistance': 0.4628642751678684}
episode index:2823
target Thresh 31.999999999985935
target distance 5.0
model initialize at round 2823
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 17.]), 'previousTarget': array([ 3., 17.]), 'currentState': array([ 9.4132174 , 11.08610467,  0.43596428]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 8.723732879990594}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9293840265060845
{'scaleFactor': 20, 'currentTarget': array([ 3., 17.]), 'previousTarget': array([ 3., 17.]), 'currentState': array([ 2.98305924, 17.07149661,  2.94786427]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 0.07347621677748231}
episode index:2824
target Thresh 31.999999999986077
target distance 22.0
model initialize at round 2824
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 25.]), 'previousTarget': array([ 4., 25.]), 'currentState': array([25.92563858, 18.6813264 ,  2.62499535]), 'targetState': array([ 4, 25], dtype=int32), 'currentDistance': 22.81795922962312}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9293688055663358
{'scaleFactor': 20, 'currentTarget': array([ 4., 25.]), 'previousTarget': array([ 4., 25.]), 'currentState': array([ 3.57711628, 25.27692622,  2.84924086]), 'targetState': array([ 4, 25], dtype=int32), 'currentDistance': 0.5054886479561164}
episode index:2825
target Thresh 31.999999999986215
target distance 6.0
model initialize at round 2825
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 16.]), 'previousTarget': array([23., 16.]), 'currentState': array([18.99858858, 15.94266283,  6.21203984]), 'targetState': array([23, 16], dtype=int32), 'currentDistance': 4.001822200322792}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9293867571567229
{'scaleFactor': 20, 'currentTarget': array([23., 16.]), 'previousTarget': array([23., 16.]), 'currentState': array([22.95205275, 15.81277938,  0.3351531 ]), 'targetState': array([23, 16], dtype=int32), 'currentDistance': 0.19326277024686306}
episode index:2826
target Thresh 31.99999999998635
target distance 9.0
model initialize at round 2826
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([14.63691596, 18.15573936,  4.65679193]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 7.340578969185365}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9293977968641312
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.9361635 , 10.40915406,  4.84390034]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.5942844663484603}
episode index:2827
target Thresh 31.99999999998649
target distance 15.0
model initialize at round 2827
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 12.]), 'previousTarget': array([25., 12.]), 'currentState': array([16.68295461, 26.99279768,  5.26890576]), 'targetState': array([25, 12], dtype=int32), 'currentDistance': 17.145180846133066}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9293921813940531
{'scaleFactor': 20, 'currentTarget': array([25., 12.]), 'previousTarget': array([25., 12.]), 'currentState': array([25.35753403, 11.47000202,  5.43088626]), 'targetState': array([25, 12], dtype=int32), 'currentDistance': 0.6393187320501044}
episode index:2828
target Thresh 31.99999999998662
target distance 17.0
model initialize at round 2828
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 22.]), 'previousTarget': array([20., 22.]), 'currentState': array([17.856652  ,  6.99454965,  1.6731357 ]), 'targetState': array([20, 22], dtype=int32), 'currentDistance': 15.157753157485105}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9293898316284235
{'scaleFactor': 20, 'currentTarget': array([20., 22.]), 'previousTarget': array([20., 22.]), 'currentState': array([20.01956654, 22.67076175,  1.66347601]), 'targetState': array([20, 22], dtype=int32), 'currentDistance': 0.6710470689413829}
episode index:2829
target Thresh 31.999999999986755
target distance 18.0
model initialize at round 2829
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([15.13894248,  1.76096888,  0.18253678]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 19.258290124359892}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.929380994965307
{'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([16.12242158, 20.92420906,  1.75610424]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 0.14398371144417837}
episode index:2830
target Thresh 31.999999999986887
target distance 8.0
model initialize at round 2830
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 26.]), 'previousTarget': array([10., 26.]), 'currentState': array([ 7.45718213, 19.62710274,  0.2989136 ]), 'targetState': array([10, 26], dtype=int32), 'currentDistance': 6.861467933391859}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9293920211097911
{'scaleFactor': 20, 'currentTarget': array([10., 26.]), 'previousTarget': array([10., 26.]), 'currentState': array([10.16620935, 26.80808262,  1.52319536]), 'targetState': array([10, 26], dtype=int32), 'currentDistance': 0.8249988326660733}
episode index:2831
target Thresh 31.99999999998702
target distance 14.0
model initialize at round 2831
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 28.]), 'previousTarget': array([23., 28.]), 'currentState': array([19.13076777, 15.24649602,  1.84404105]), 'targetState': array([23, 28], dtype=int32), 'currentDistance': 13.327521215285975}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9293929650811179
{'scaleFactor': 20, 'currentTarget': array([23., 28.]), 'previousTarget': array([23., 28.]), 'currentState': array([23.17528325, 28.3429962 ,  1.49050018]), 'targetState': array([23, 28], dtype=int32), 'currentDistance': 0.3851890585922444}
episode index:2832
target Thresh 31.999999999987146
target distance 3.0
model initialize at round 2832
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 15.]), 'previousTarget': array([13., 15.]), 'currentState': array([11.64965235, 17.33321982,  5.47249753]), 'targetState': array([13, 15], dtype=int32), 'currentDistance': 2.6958029454402967}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9294143583161757
{'scaleFactor': 20, 'currentTarget': array([13., 15.]), 'previousTarget': array([13., 15.]), 'currentState': array([12.7308986 , 15.66454208,  5.0984276 ]), 'targetState': array([13, 15], dtype=int32), 'currentDistance': 0.7169600710036577}
episode index:2833
target Thresh 31.999999999987274
target distance 10.0
model initialize at round 2833
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  6.]), 'previousTarget': array([13.,  6.]), 'currentState': array([ 4.35962385, 11.99224213,  0.35627168]), 'targetState': array([13,  6], dtype=int32), 'currentDistance': 10.514897325182726}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9294219714748151
{'scaleFactor': 20, 'currentTarget': array([13.,  6.]), 'previousTarget': array([13.,  6.]), 'currentState': array([12.1909483 ,  6.46266715,  5.60276044]), 'targetState': array([13,  6], dtype=int32), 'currentDistance': 0.9320008275737125}
episode index:2834
target Thresh 31.999999999987402
target distance 14.0
model initialize at round 2834
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([16.03977972, 14.68928963,  3.51549074]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 12.0594948434851}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9294262248003622
{'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([ 4.2462881 , 14.20661786,  3.35853267]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 0.321479029411779}
episode index:2835
target Thresh 31.999999999987526
target distance 8.0
model initialize at round 2835
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 8.]), 'previousTarget': array([7., 8.]), 'currentState': array([10.66996571, 14.96878864,  4.81110692]), 'targetState': array([7, 8], dtype=int32), 'currentDistance': 7.876081729862586}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9294372155567794
{'scaleFactor': 20, 'currentTarget': array([7., 8.]), 'previousTarget': array([7., 8.]), 'currentState': array([6.91895695, 8.180962  , 4.49709436]), 'targetState': array([7, 8], dtype=int32), 'currentDistance': 0.19828066081633552}
episode index:2836
target Thresh 31.99999999998765
target distance 22.0
model initialize at round 2836
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 20.]), 'previousTarget': array([25., 20.]), 'currentState': array([ 2.06141852, 13.60305588,  5.13078451]), 'targetState': array([25, 20], dtype=int32), 'currentDistance': 23.813849216902426}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9294220402505261
{'scaleFactor': 20, 'currentTarget': array([25., 20.]), 'previousTarget': array([25., 20.]), 'currentState': array([24.21157671, 19.95541709,  0.11121838]), 'targetState': array([25, 20], dtype=int32), 'currentDistance': 0.7896827945758004}
episode index:2837
target Thresh 31.999999999987775
target distance 13.0
model initialize at round 2837
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 13.]), 'previousTarget': array([15., 13.]), 'currentState': array([ 0.57337977, 23.89282854,  3.59239328]), 'targetState': array([15, 13], dtype=int32), 'currentDistance': 18.077087290217076}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9294132171479039
{'scaleFactor': 20, 'currentTarget': array([15., 13.]), 'previousTarget': array([15., 13.]), 'currentState': array([15.44434534, 12.86391835,  6.0196571 ]), 'targetState': array([15, 13], dtype=int32), 'currentDistance': 0.4647160387333749}
episode index:2838
target Thresh 31.999999999987896
target distance 14.0
model initialize at round 2838
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 25.]), 'previousTarget': array([ 4., 25.]), 'currentState': array([ 6.12835481, 12.79251186,  1.8791098 ]), 'targetState': array([ 4, 25], dtype=int32), 'currentDistance': 12.391636733863226}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9294174675643369
{'scaleFactor': 20, 'currentTarget': array([ 4., 25.]), 'previousTarget': array([ 4., 25.]), 'currentState': array([ 3.84113287, 24.44446821,  1.73380749]), 'targetState': array([ 4, 25], dtype=int32), 'currentDistance': 0.5778012988527286}
episode index:2839
target Thresh 31.999999999988017
target distance 9.0
model initialize at round 2839
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 15.]), 'previousTarget': array([25., 15.]), 'currentState': array([23.82338304,  7.99178163,  1.69436503]), 'targetState': array([25, 15], dtype=int32), 'currentDistance': 7.106303694563661}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9294284459243493
{'scaleFactor': 20, 'currentTarget': array([25., 15.]), 'previousTarget': array([25., 15.]), 'currentState': array([24.70143155, 15.87251293,  1.70293439]), 'targetState': array([25, 15], dtype=int32), 'currentDistance': 0.9221832404846539}
episode index:2840
target Thresh 31.999999999988134
target distance 21.0
model initialize at round 2840
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 6.]), 'previousTarget': array([5., 6.]), 'currentState': array([24.89115596, 13.2660383 ,  3.30010056]), 'targetState': array([5, 6], dtype=int32), 'currentDistance': 21.17671829989566}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9294164465608626
{'scaleFactor': 20, 'currentTarget': array([5., 6.]), 'previousTarget': array([5., 6.]), 'currentState': array([4.73287118, 5.54356452, 3.7089325 ]), 'targetState': array([5, 6], dtype=int32), 'currentDistance': 0.5288583525178397}
episode index:2841
target Thresh 31.99999999998825
target distance 13.0
model initialize at round 2841
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([18.40460415, 14.73245831,  2.17788452]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 11.5625937971982}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9294206913542616
{'scaleFactor': 20, 'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([20.6522132 , 26.18754386,  1.48240868]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 0.39513081320874527}
episode index:2842
target Thresh 31.99999999998837
target distance 13.0
model initialize at round 2842
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([18.28571486, 13.46394277,  4.2471199 ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 11.551552882059656}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9294249331615239
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.01924988, 10.80304884,  3.41064589]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.19788966168965771}
episode index:2843
target Thresh 31.999999999988486
target distance 14.0
model initialize at round 2843
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 20.]), 'previousTarget': array([23., 20.]), 'currentState': array([13.76716756,  4.33321353,  5.58359766]), 'targetState': array([23, 20], dtype=int32), 'currentDistance': 18.18497712842723}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9294193397418058
{'scaleFactor': 20, 'currentTarget': array([23., 20.]), 'previousTarget': array([23., 20.]), 'currentState': array([22.22305564, 19.14420633,  1.00477436]), 'targetState': array([23, 20], dtype=int32), 'currentDistance': 1.1558656271590837}
episode index:2844
target Thresh 31.9999999999886
target distance 8.0
model initialize at round 2844
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 16.]), 'previousTarget': array([ 9., 16.]), 'currentState': array([ 5.51387012, 24.78981393,  3.66309738]), 'targetState': array([ 9, 16], dtype=int32), 'currentDistance': 9.455893958984339}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.929426921713742
{'scaleFactor': 20, 'currentTarget': array([ 9., 16.]), 'previousTarget': array([ 9., 16.]), 'currentState': array([ 9.00910003, 16.13912523,  5.43470415]), 'targetState': array([ 9, 16], dtype=int32), 'currentDistance': 0.13942251940873093}
episode index:2845
target Thresh 31.999999999988713
target distance 12.0
model initialize at round 2845
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 2.]), 'previousTarget': array([7., 2.]), 'currentState': array([14.75745102, 15.13510357,  3.41135347]), 'targetState': array([7, 2], dtype=int32), 'currentDistance': 15.254802264567205}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9294245737772395
{'scaleFactor': 20, 'currentTarget': array([7., 2.]), 'previousTarget': array([7., 2.]), 'currentState': array([7.17841501, 1.8142757 , 4.17750579]), 'targetState': array([7, 2], dtype=int32), 'currentDistance': 0.2575372467856195}
episode index:2846
target Thresh 31.999999999988827
target distance 19.0
model initialize at round 2846
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([23.86797628, 23.6777836 ,  2.65932387]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 22.731623922862344}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9294094562141693
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.83126463, 10.55291871,  3.86334093]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.47786326616695907}
episode index:2847
target Thresh 31.999999999988937
target distance 10.0
model initialize at round 2847
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 21.]), 'previousTarget': array([19., 21.]), 'currentState': array([23.1471962 , 12.759769  ,  2.39226139]), 'targetState': array([19, 21], dtype=int32), 'currentDistance': 9.22500098859805}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9294170336698174
{'scaleFactor': 20, 'currentTarget': array([19., 21.]), 'previousTarget': array([19., 21.]), 'currentState': array([18.42623594, 21.42814102,  2.1227314 ]), 'targetState': array([19, 21], dtype=int32), 'currentDistance': 0.7158979872398695}
episode index:2848
target Thresh 31.999999999989047
target distance 4.0
model initialize at round 2848
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 8.00636962, 13.26948426,  4.0736613 ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 3.029204159693217}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9294382983122639
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.85321682, 11.63564004,  4.12239989]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 1.063962964842027}
episode index:2849
target Thresh 31.999999999989157
target distance 18.0
model initialize at round 2849
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([15.36796351, 22.4109075 ,  3.90494323]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 21.637747328164565}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9294263333845257
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.1548858 , 3.92118535, 4.30461587]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.17378538356551898}
episode index:2850
target Thresh 31.999999999989264
target distance 4.0
model initialize at round 2850
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 23.]), 'previousTarget': array([13., 23.]), 'currentState': array([ 7.33056972, 25.78694926,  4.27852535]), 'targetState': array([13, 23], dtype=int32), 'currentDistance': 6.317398665546001}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9294406696407921
{'scaleFactor': 20, 'currentTarget': array([13., 23.]), 'previousTarget': array([13., 23.]), 'currentState': array([12.09865071, 23.4577671 ,  6.01659233]), 'targetState': array([13, 23], dtype=int32), 'currentDistance': 1.010930891295639}
episode index:2851
target Thresh 31.99999999998937
target distance 13.0
model initialize at round 2851
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([23.00034029,  4.01571472,  3.16293244]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 11.177874330527493}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9294448910572578
{'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([11.22940337,  5.82473453,  3.14632708]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 0.7902766327258781}
episode index:2852
target Thresh 31.999999999989477
target distance 14.0
model initialize at round 2852
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([19.18046456, 19.53005111,  5.21378565]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 13.099276000050267}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9294458095489683
{'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([23.49377337,  6.36844413,  5.01099136]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 0.8016701067548588}
episode index:2853
target Thresh 31.99999999998958
target distance 5.0
model initialize at round 2853
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  2.]), 'previousTarget': array([22.,  2.]), 'currentState': array([22.42028036,  6.09712029,  4.70692483]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 4.118619941308233}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9294635580389652
{'scaleFactor': 20, 'currentTarget': array([22.,  2.]), 'previousTarget': array([22.,  2.]), 'currentState': array([22.22840913,  2.13728017,  4.99174109]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 0.2664893547679633}
episode index:2854
target Thresh 31.999999999989686
target distance 11.0
model initialize at round 2854
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 16.]), 'previousTarget': array([21., 16.]), 'currentState': array([11.91514708, 17.50126508,  6.28222066]), 'targetState': array([21, 16], dtype=int32), 'currentDistance': 9.208058943944122}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9294710979660619
{'scaleFactor': 20, 'currentTarget': array([21., 16.]), 'previousTarget': array([21., 16.]), 'currentState': array([21.76649497, 16.18233317,  0.02938919]), 'targetState': array([21, 16], dtype=int32), 'currentDistance': 0.7878831884166205}
episode index:2855
target Thresh 31.999999999989786
target distance 11.0
model initialize at round 2855
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 18.]), 'previousTarget': array([ 8., 18.]), 'currentState': array([ 5.32075779, 28.88804601,  4.21816349]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 11.212844632054281}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9294753028160041
{'scaleFactor': 20, 'currentTarget': array([ 8., 18.]), 'previousTarget': array([ 8., 18.]), 'currentState': array([ 8.26981315, 17.74038503,  5.23281684]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 0.37443165837268005}
episode index:2856
target Thresh 31.99999999998989
target distance 9.0
model initialize at round 2856
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([12.65459714, 21.69227084,  5.08930236]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 8.221180399078332}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9294861956081579
{'scaleFactor': 20, 'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([19.57740302, 18.4296129 ,  5.78332299]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 0.6026238087268041}
episode index:2857
target Thresh 31.99999999998999
target distance 4.0
model initialize at round 2857
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 19.]), 'previousTarget': array([24., 19.]), 'currentState': array([21.10157079, 21.32009778,  5.78277779]), 'targetState': array([24, 19], dtype=int32), 'currentDistance': 3.7126467128800322}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9295039051268396
{'scaleFactor': 20, 'currentTarget': array([24., 19.]), 'previousTarget': array([24., 19.]), 'currentState': array([24.31844221, 18.99482927,  5.93657303]), 'targetState': array([24, 19], dtype=int32), 'currentDistance': 0.3184841823751268}
episode index:2858
target Thresh 31.999999999990088
target distance 21.0
model initialize at round 2858
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([23.28096855, 25.3406493 ,  3.87012243]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 21.038896246906184}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9294919549166724
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.90625957,  5.23233825,  4.42314318]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.7733639713729914}
episode index:2859
target Thresh 31.999999999990187
target distance 5.0
model initialize at round 2859
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 19.]), 'previousTarget': array([ 4., 19.]), 'currentState': array([ 3.41100957, 22.3178273 ,  4.02653456]), 'targetState': array([ 4, 19], dtype=int32), 'currentDistance': 3.369701425352552}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9295096500373309
{'scaleFactor': 20, 'currentTarget': array([ 4., 19.]), 'previousTarget': array([ 4., 19.]), 'currentState': array([ 4.54711157, 18.90641014,  5.52629542]), 'targetState': array([ 4, 19], dtype=int32), 'currentDistance': 0.5550586763088965}
episode index:2860
target Thresh 31.999999999990287
target distance 5.0
model initialize at round 2860
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 29.]), 'previousTarget': array([12., 29.]), 'currentState': array([14.6546094 , 23.69233812,  0.82615727]), 'targetState': array([12, 29], dtype=int32), 'currentDistance': 5.934494560379264}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9295239070628334
{'scaleFactor': 20, 'currentTarget': array([12., 29.]), 'previousTarget': array([12., 29.]), 'currentState': array([11.77532235, 28.31502906,  2.04830877]), 'targetState': array([12, 29], dtype=int32), 'currentDistance': 0.720878096085045}
episode index:2861
target Thresh 31.999999999990383
target distance 16.0
model initialize at round 2861
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 23.]), 'previousTarget': array([17., 23.]), 'currentState': array([26.06760009,  8.74445284,  2.32157165]), 'targetState': array([17, 23], dtype=int32), 'currentDistance': 16.89502874664937}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9295215383651971
{'scaleFactor': 20, 'currentTarget': array([17., 23.]), 'previousTarget': array([17., 23.]), 'currentState': array([17.36327264, 22.02321504,  2.23509177]), 'targetState': array([17, 23], dtype=int32), 'currentDistance': 1.0421496404451707}
episode index:2862
target Thresh 31.99999999999048
target distance 9.0
model initialize at round 2862
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'previousTarget': array([10., 12.]), 'currentState': array([11.10054564,  4.42245209,  1.1246469 ]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 7.657051199782073}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.929532392179949
{'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'previousTarget': array([10., 12.]), 'currentState': array([ 9.53719752, 11.9541484 ,  1.85595262]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 0.46506827871893985}
episode index:2863
target Thresh 31.99999999999057
target distance 13.0
model initialize at round 2863
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 29.]), 'previousTarget': array([23., 29.]), 'currentState': array([10.82082063, 18.53076837,  6.23186684]), 'targetState': array([23, 29], dtype=int32), 'currentDistance': 16.06042405854861}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9295300221737507
{'scaleFactor': 20, 'currentTarget': array([23., 29.]), 'previousTarget': array([23., 29.]), 'currentState': array([22.3717878 , 28.76109291,  0.86509503]), 'targetState': array([23, 29], dtype=int32), 'currentDistance': 0.6721065084489996}
episode index:2864
target Thresh 31.999999999990667
target distance 16.0
model initialize at round 2864
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 12.]), 'previousTarget': array([20., 12.]), 'currentState': array([23.49323261, 29.60485978,  2.88665795]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 17.9480851845542}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9295244330726371
{'scaleFactor': 20, 'currentTarget': array([20., 12.]), 'previousTarget': array([20., 12.]), 'currentState': array([20.34871539, 12.72362417,  4.64977981]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 0.8032648162311746}
episode index:2865
target Thresh 31.99999999999076
target distance 15.0
model initialize at round 2865
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 17.]), 'previousTarget': array([23., 17.]), 'currentState': array([9.41389521, 7.41445982, 0.77311178]), 'targetState': array([23, 17], dtype=int32), 'currentDistance': 16.627231394194517}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9295220674973947
{'scaleFactor': 20, 'currentTarget': array([23., 17.]), 'previousTarget': array([23., 17.]), 'currentState': array([22.31045454, 16.67844449,  0.74090118]), 'targetState': array([23, 17], dtype=int32), 'currentDistance': 0.7608356522953679}
episode index:2866
target Thresh 31.99999999999085
target distance 15.0
model initialize at round 2866
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([24.46481729,  2.31036086,  2.55380011]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 16.02532786165421}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9295197035723617
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.18074441, 10.88202456,  2.76919389]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.21583962932149509}
episode index:2867
target Thresh 31.99999999999094
target distance 14.0
model initialize at round 2867
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 12.]), 'previousTarget': array([ 8., 12.]), 'currentState': array([ 9.56351037, 24.0661285 ,  4.72201031]), 'targetState': array([ 8, 12], dtype=int32), 'currentDistance': 12.167005455237799}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9295238738812279
{'scaleFactor': 20, 'currentTarget': array([ 8., 12.]), 'previousTarget': array([ 8., 12.]), 'currentState': array([ 7.95872139, 12.30233825,  4.87904992]), 'targetState': array([ 8, 12], dtype=int32), 'currentDistance': 0.3051431473761864}
episode index:2868
target Thresh 31.999999999991033
target distance 7.0
model initialize at round 2868
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  6.]), 'previousTarget': array([24.,  6.]), 'currentState': array([18.62688197,  7.43086324,  5.53208143]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 5.560374715311467}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9295380861942705
{'scaleFactor': 20, 'currentTarget': array([24.,  6.]), 'previousTarget': array([24.,  6.]), 'currentState': array([24.28441271,  5.94721616,  5.76542451]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 0.28926928775719213}
episode index:2869
target Thresh 31.99999999999112
target distance 20.0
model initialize at round 2869
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 10.]), 'previousTarget': array([26., 10.]), 'currentState': array([4.31758992, 7.95659009, 4.17738914]), 'targetState': array([26, 10], dtype=int32), 'currentDistance': 21.77848550203246}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9295230502310377
{'scaleFactor': 20, 'currentTarget': array([26., 10.]), 'previousTarget': array([26., 10.]), 'currentState': array([26.44476453, 10.23934874,  0.25835401]), 'targetState': array([26, 10], dtype=int32), 'currentDistance': 0.5050775256190227}
episode index:2870
target Thresh 31.99999999999121
target distance 24.0
model initialize at round 2870
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 12.]), 'previousTarget': array([27., 12.]), 'currentState': array([4.67062831, 6.073418  , 0.36207968]), 'targetState': array([27, 12], dtype=int32), 'currentDistance': 23.10249368634906}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9295080247421783
{'scaleFactor': 20, 'currentTarget': array([27., 12.]), 'previousTarget': array([27., 12.]), 'currentState': array([27.36190744, 12.47380477,  0.44772092]), 'targetState': array([27, 12], dtype=int32), 'currentDistance': 0.5962113349627481}
episode index:2871
target Thresh 31.999999999991296
target distance 3.0
model initialize at round 2871
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 27.]), 'previousTarget': array([19., 27.]), 'currentState': array([17.38727749, 26.95281611,  5.87501842]), 'targetState': array([19, 27], dtype=int32), 'currentDistance': 1.613412596067188}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9295290874076579
{'scaleFactor': 20, 'currentTarget': array([19., 27.]), 'previousTarget': array([19., 27.]), 'currentState': array([19.30723583, 27.08087362,  0.55095696]), 'targetState': array([19, 27], dtype=int32), 'currentDistance': 0.31770174454672473}
episode index:2872
target Thresh 31.999999999991385
target distance 17.0
model initialize at round 2872
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 23.]), 'previousTarget': array([20., 23.]), 'currentState': array([4.96288507, 7.7490732 , 0.9654986 ]), 'targetState': array([20, 23], dtype=int32), 'currentDistance': 21.41741332732044}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9295171866651766
{'scaleFactor': 20, 'currentTarget': array([20., 23.]), 'previousTarget': array([20., 23.]), 'currentState': array([19.99748224, 23.46175882,  1.07572175]), 'targetState': array([20, 23], dtype=int32), 'currentDistance': 0.4617656837304537}
episode index:2873
target Thresh 31.99999999999147
target distance 25.0
model initialize at round 2873
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 27.]), 'previousTarget': array([ 6., 27.]), 'currentState': array([4.51221522, 3.61073092, 0.85485148]), 'targetState': array([ 6, 27], dtype=int32), 'currentDistance': 23.43654009731776}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9295021789007545
{'scaleFactor': 20, 'currentTarget': array([ 6., 27.]), 'previousTarget': array([ 6., 27.]), 'currentState': array([ 5.70639011, 26.96806352,  1.81167226]), 'targetState': array([ 6, 27], dtype=int32), 'currentDistance': 0.2953416776716311}
episode index:2874
target Thresh 31.999999999991555
target distance 12.0
model initialize at round 2874
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 15.]), 'previousTarget': array([13., 15.]), 'currentState': array([18.32527122, 27.16634782,  4.05258918]), 'targetState': array([13, 15], dtype=int32), 'currentDistance': 13.280757993597994}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9295030704378002
{'scaleFactor': 20, 'currentTarget': array([13., 15.]), 'previousTarget': array([13., 15.]), 'currentState': array([13.29058319, 14.22171623,  4.59609337]), 'targetState': array([13, 15], dtype=int32), 'currentDistance': 0.8307612284960583}
episode index:2875
target Thresh 31.99999999999164
target distance 10.0
model initialize at round 2875
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 7.]), 'previousTarget': array([7., 7.]), 'currentState': array([15.92595445,  8.70430936,  3.01025105]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 9.087207120369317}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.929510541571132
{'scaleFactor': 20, 'currentTarget': array([7., 7.]), 'previousTarget': array([7., 7.]), 'currentState': array([6.37412882, 6.3206154 , 3.5744596 ]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 0.9237305706564897}
episode index:2876
target Thresh 31.999999999991722
target distance 10.0
model initialize at round 2876
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 4.41169828, 18.36816281,  3.94952297]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 8.708756367391587}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9295213463915799
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.37514989, 10.84371627,  4.60138676]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.9233604844178135}
episode index:2877
target Thresh 31.999999999991804
target distance 17.0
model initialize at round 2877
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  5.]), 'previousTarget': array([26.,  5.]), 'currentState': array([ 7.32689719, 15.81802453,  4.25993204]), 'targetState': array([26,  5], dtype=int32), 'currentDistance': 21.580417590775056}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9295094690141883
{'scaleFactor': 20, 'currentTarget': array([26.,  5.]), 'previousTarget': array([26.,  5.]), 'currentState': array([25.73576434,  5.24454308,  6.11695729]), 'targetState': array([26,  5], dtype=int32), 'currentDistance': 0.3600302786023288}
episode index:2878
target Thresh 31.999999999991886
target distance 17.0
model initialize at round 2878
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([ 7.02018163, 19.3537037 ,  4.82511806]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 15.480823561091256}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.929507119318257
{'scaleFactor': 20, 'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([9.28877267, 3.6348903 , 5.04921131]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 0.4655048279728832}
episode index:2879
target Thresh 31.999999999991967
target distance 3.0
model initialize at round 2879
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 25.]), 'previousTarget': array([22., 25.]), 'currentState': array([21.97949091, 26.63142859,  4.32357745]), 'targetState': array([22, 25], dtype=int32), 'currentDistance': 1.6315574992376705}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9295281237907158
{'scaleFactor': 20, 'currentTarget': array([22., 25.]), 'previousTarget': array([22., 25.]), 'currentState': array([22.07466683, 24.69837702,  5.2084419 ]), 'targetState': array([22, 25], dtype=int32), 'currentDistance': 0.3107274648819243}
episode index:2880
target Thresh 31.999999999992045
target distance 6.0
model initialize at round 2880
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 19.]), 'previousTarget': array([21., 19.]), 'currentState': array([17.27084654, 23.48318646,  5.27428889]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 5.83142746994391}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9295422754311912
{'scaleFactor': 20, 'currentTarget': array([21., 19.]), 'previousTarget': array([21., 19.]), 'currentState': array([21.29129788, 19.14104592,  5.74508378]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 0.32364858296325505}
episode index:2881
target Thresh 31.999999999992127
target distance 17.0
model initialize at round 2881
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.51220185, 19.78666686,  3.66521382]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 17.794040258382164}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9295367150467541
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.96140759,  2.58533208,  4.99953888]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.5866029462471326}
episode index:2882
target Thresh 31.999999999992205
target distance 7.0
model initialize at round 2882
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([ 9.54493207, 13.4076944 ,  6.05212355]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 10.74688873679686}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.929540857757248
{'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([16.17764913, 22.6442765 ,  0.99341005]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 0.6683198527731721}
episode index:2883
target Thresh 31.99999999999228
target distance 19.0
model initialize at round 2883
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  3.]), 'previousTarget': array([11.,  3.]), 'currentState': array([22.56293326, 22.87591511,  3.60420549]), 'targetState': array([11,  3], dtype=int32), 'currentDistance': 22.99463909599375}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9295258938231145
{'scaleFactor': 20, 'currentTarget': array([11.,  3.]), 'previousTarget': array([11.,  3.]), 'currentState': array([10.77504892,  2.3983213 ,  4.08627751]), 'targetState': array([11,  3], dtype=int32), 'currentDistance': 0.6423552366517732}
episode index:2884
target Thresh 31.999999999992358
target distance 9.0
model initialize at round 2884
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([ 8.39259593, 24.50136162,  4.45239305]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 12.441250907483841}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9295300374125695
{'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([18.18307543, 18.7725643 ,  5.79268475]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 1.1243759837619842}
episode index:2885
target Thresh 31.999999999992433
target distance 25.0
model initialize at round 2885
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.85907499,  2.24332367]), 'previousTarget': array([21.,  2.]), 'currentState': array([ 5.82371931, 28.20364107,  3.35469866]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 30.000000000000004}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.9295029853452382
{'scaleFactor': 20, 'currentTarget': array([21.,  2.]), 'previousTarget': array([21.,  2.]), 'currentState': array([21.02080443,  2.23755552,  5.39683613]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 0.23846477747849387}
episode index:2886
target Thresh 31.99999999999251
target distance 6.0
model initialize at round 2886
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 12.]), 'previousTarget': array([ 5., 12.]), 'currentState': array([9.13137438, 9.6925959 , 2.63863543]), 'targetState': array([ 5, 12], dtype=int32), 'currentDistance': 4.732057477082876}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9295205111556487
{'scaleFactor': 20, 'currentTarget': array([ 5., 12.]), 'previousTarget': array([ 5., 12.]), 'currentState': array([ 5.75861752, 11.73205143,  3.02123451]), 'targetState': array([ 5, 12], dtype=int32), 'currentDistance': 0.8045476851495296}
episode index:2887
target Thresh 31.999999999992585
target distance 17.0
model initialize at round 2887
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 25.]), 'previousTarget': array([24., 25.]), 'currentState': array([10.84764375,  9.65155923,  0.42627466]), 'targetState': array([24, 25], dtype=int32), 'currentDistance': 20.21284515146292}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9295118067109996
{'scaleFactor': 20, 'currentTarget': array([24., 25.]), 'previousTarget': array([24., 25.]), 'currentState': array([23.60925925, 24.84177819,  1.20947471]), 'targetState': array([24, 25], dtype=int32), 'currentDistance': 0.4215595787658026}
episode index:2888
target Thresh 31.999999999992657
target distance 16.0
model initialize at round 2888
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 17.]), 'previousTarget': array([24., 17.]), 'currentState': array([9.21564054, 9.8361245 , 0.24635285]), 'targetState': array([24, 17], dtype=int32), 'currentDistance': 16.428584747888305}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9295094643391466
{'scaleFactor': 20, 'currentTarget': array([24., 17.]), 'previousTarget': array([24., 17.]), 'currentState': array([23.43104149, 16.75219694,  0.82258073]), 'targetState': array([24, 17], dtype=int32), 'currentDistance': 0.6205804857994869}
episode index:2889
target Thresh 31.99999999999273
target distance 13.0
model initialize at round 2889
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 21.]), 'previousTarget': array([19., 21.]), 'currentState': array([ 7.34420784, 17.98733354,  0.36435479]), 'targetState': array([19, 21], dtype=int32), 'currentDistance': 12.038839230747085}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9295136064447044
{'scaleFactor': 20, 'currentTarget': array([19., 21.]), 'previousTarget': array([19., 21.]), 'currentState': array([18.90977803, 20.86626225,  0.52305276]), 'targetState': array([19, 21], dtype=int32), 'currentDistance': 0.16132510461482186}
episode index:2890
target Thresh 31.999999999992802
target distance 2.0
model initialize at round 2890
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  5.]), 'previousTarget': array([22.,  5.]), 'currentState': array([24.64981806,  2.66760194,  0.81118601]), 'targetState': array([22,  5], dtype=int32), 'currentDistance': 3.530101475648527}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9295311043324787
{'scaleFactor': 20, 'currentTarget': array([22.,  5.]), 'previousTarget': array([22.,  5.]), 'currentState': array([22.43355781,  5.10315706,  2.64961783]), 'targetState': array([22,  5], dtype=int32), 'currentDistance': 0.44566102770757426}
episode index:2891
target Thresh 31.999999999992873
target distance 9.0
model initialize at round 2891
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 8.44806305, 12.0924829 ,  4.22176802]), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 7.886219144884836}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9295418460011049
{'scaleFactor': 20, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.64061886, 5.17480977, 4.31527795]), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.39964141526575153}
episode index:2892
target Thresh 31.999999999992944
target distance 10.0
model initialize at round 2892
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([22.19186816, 10.55882385,  4.03971624]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 8.582207376243579}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9295525802437592
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.77793476,  8.15806434,  3.46234734]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.7938304801870829}
episode index:2893
target Thresh 31.999999999993015
target distance 8.0
model initialize at round 2893
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 25.]), 'previousTarget': array([ 5., 25.]), 'currentState': array([11.56925132, 27.60731992,  3.81218176]), 'targetState': array([ 5, 25], dtype=int32), 'currentDistance': 7.067756370925408}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9295633070681393
{'scaleFactor': 20, 'currentTarget': array([ 5., 25.]), 'previousTarget': array([ 5., 25.]), 'currentState': array([ 4.23089118, 24.7765109 ,  3.36427862]), 'targetState': array([ 5, 25], dtype=int32), 'currentDistance': 0.8009218185842628}
episode index:2894
target Thresh 31.999999999993086
target distance 10.0
model initialize at round 2894
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 8.]), 'previousTarget': array([8., 8.]), 'currentState': array([16.52388161, 18.80837032,  3.65056896]), 'targetState': array([8, 8], dtype=int32), 'currentDistance': 13.76507997257883}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9295641713309507
{'scaleFactor': 20, 'currentTarget': array([8., 8.]), 'previousTarget': array([8., 8.]), 'currentState': array([8.29285516, 7.98056657, 3.79938951]), 'targetState': array([8, 8], dtype=int32), 'currentDistance': 0.2934992359935728}
episode index:2895
target Thresh 31.999999999993154
target distance 22.0
model initialize at round 2895
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6.44097509, 26.68368823]), 'previousTarget': array([ 5.29933368, 27.7142724 ]), 'currentState': array([28.59064944,  6.45025237,  0.67723959]), 'targetState': array([ 5, 28], dtype=int32), 'currentDistance': 29.999999999999996}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.9295372008888803
{'scaleFactor': 20, 'currentTarget': array([ 5., 28.]), 'previousTarget': array([ 5., 28.]), 'currentState': array([ 5.98350148, 27.32206943,  2.39769186]), 'targetState': array([ 5, 28], dtype=int32), 'currentDistance': 1.1945145540971513}
episode index:2896
target Thresh 31.99999999999322
target distance 13.0
model initialize at round 2896
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 16.]), 'previousTarget': array([ 5., 16.]), 'currentState': array([10.5227375 ,  2.28330022,  0.57009428]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 14.786767141388038}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9295348562197532
{'scaleFactor': 20, 'currentTarget': array([ 5., 16.]), 'previousTarget': array([ 5., 16.]), 'currentState': array([ 4.57614148, 16.31810611,  2.39769556]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 0.5299505050121238}
episode index:2897
target Thresh 31.99999999999329
target distance 5.0
model initialize at round 2897
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 5.]), 'previousTarget': array([8., 5.]), 'currentState': array([11.05448375,  3.74753851,  2.92784983]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 3.3012922898810224}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9295523045095325
{'scaleFactor': 20, 'currentTarget': array([8., 5.]), 'previousTarget': array([8., 5.]), 'currentState': array([7.35028337, 5.10403597, 3.17875254]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 0.6579933033444945}
episode index:2898
target Thresh 31.999999999993356
target distance 7.0
model initialize at round 2898
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 29.]), 'previousTarget': array([18., 29.]), 'currentState': array([12.6719252 , 21.80750526,  0.89537114]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 8.95099780461516}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9295630129281217
{'scaleFactor': 20, 'currentTarget': array([18., 29.]), 'previousTarget': array([18., 29.]), 'currentState': array([17.53388974, 28.11270492,  1.19918815]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 1.002273079414052}
episode index:2899
target Thresh 31.999999999993424
target distance 11.0
model initialize at round 2899
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'previousTarget': array([27.,  8.]), 'currentState': array([17.31083246,  4.94448764,  0.33208245]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 10.159533619388283}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9295704015615603
{'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'previousTarget': array([27.,  8.]), 'currentState': array([26.82080241,  7.83481121,  0.64191078]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 0.24371933536669277}
episode index:2900
target Thresh 31.999999999993488
target distance 9.0
model initialize at round 2900
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 16.]), 'previousTarget': array([21., 16.]), 'currentState': array([10.32169794, 14.87117458,  4.22775054]), 'targetState': array([21, 16], dtype=int32), 'currentDistance': 10.737801525578002}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.929574506955507
{'scaleFactor': 20, 'currentTarget': array([21., 16.]), 'previousTarget': array([21., 16.]), 'currentState': array([20.70780616, 15.88634183,  0.76480962]), 'targetState': array([21, 16], dtype=int32), 'currentDistance': 0.3135210075152731}
episode index:2901
target Thresh 31.999999999993552
target distance 9.0
model initialize at round 2901
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 26.]), 'previousTarget': array([ 7., 26.]), 'currentState': array([14.0333391 , 26.66759521,  3.43818063]), 'targetState': array([ 7, 26], dtype=int32), 'currentDistance': 7.064951682832537}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.929585196653317
{'scaleFactor': 20, 'currentTarget': array([ 7., 26.]), 'previousTarget': array([ 7., 26.]), 'currentState': array([ 6.27549168, 26.56451387,  2.50091062]), 'targetState': array([ 7, 26], dtype=int32), 'currentDistance': 0.9184705902269404}
episode index:2902
target Thresh 31.999999999993616
target distance 10.0
model initialize at round 2902
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 18.]), 'previousTarget': array([27., 18.]), 'currentState': array([23.64567234, 26.33061721,  5.89651   ]), 'targetState': array([27, 18], dtype=int32), 'currentDistance': 8.980573321383812}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9295925700095852
{'scaleFactor': 20, 'currentTarget': array([27., 18.]), 'previousTarget': array([27., 18.]), 'currentState': array([27.24897138, 17.52997027,  4.94280191]), 'targetState': array([27, 18], dtype=int32), 'currentDistance': 0.531897262212761}
episode index:2903
target Thresh 31.99999999999368
target distance 13.0
model initialize at round 2903
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 28.]), 'previousTarget': array([12., 28.]), 'currentState': array([ 3.15810191, 13.32447266,  5.81646991]), 'targetState': array([12, 28], dtype=int32), 'currentDistance': 17.13330863096169}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.929587034430203
{'scaleFactor': 20, 'currentTarget': array([12., 28.]), 'previousTarget': array([12., 28.]), 'currentState': array([11.95283538, 27.80248464,  1.19766088]), 'targetState': array([12, 28], dtype=int32), 'currentDistance': 0.20306850905564772}
episode index:2904
target Thresh 31.999999999993744
target distance 12.0
model initialize at round 2904
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 13.]), 'previousTarget': array([ 9., 13.]), 'currentState': array([11.28105842, 23.90854345,  4.56753365]), 'targetState': array([ 9, 13], dtype=int32), 'currentDistance': 11.144485076771387}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9295911284456834
{'scaleFactor': 20, 'currentTarget': array([ 9., 13.]), 'previousTarget': array([ 9., 13.]), 'currentState': array([ 8.40345304, 12.40779054,  3.93039586]), 'targetState': array([ 9, 13], dtype=int32), 'currentDistance': 0.8405833221442742}
episode index:2905
target Thresh 31.999999999993804
target distance 15.0
model initialize at round 2905
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 28.]), 'previousTarget': array([25., 28.]), 'currentState': array([17.43208715, 14.94357772,  1.18621981]), 'targetState': array([25, 28], dtype=int32), 'currentDistance': 15.091171843576925}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.929588772480777
{'scaleFactor': 20, 'currentTarget': array([25., 28.]), 'previousTarget': array([25., 28.]), 'currentState': array([25.58267362, 28.5171    ,  0.60894099]), 'targetState': array([25, 28], dtype=int32), 'currentDistance': 0.7790384820244026}
episode index:2906
target Thresh 31.999999999993868
target distance 15.0
model initialize at round 2906
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([24.50834895,  2.74650621,  1.46957224]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 16.510295096598476}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9295832439204064
{'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([7.44955821, 2.50962068, 3.56325758]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 0.7371960659624102}
episode index:2907
target Thresh 31.99999999999393
target distance 8.0
model initialize at round 2907
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 8.]), 'previousTarget': array([7., 8.]), 'currentState': array([ 7.70014338, 14.16783253,  4.73262864]), 'targetState': array([7, 8], dtype=int32), 'currentDistance': 6.207443823617675}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9295972452120431
{'scaleFactor': 20, 'currentTarget': array([7., 8.]), 'previousTarget': array([7., 8.]), 'currentState': array([6.96091506, 8.29853192, 5.05736628]), 'targetState': array([7, 8], dtype=int32), 'currentDistance': 0.30107962769601754}
episode index:2908
target Thresh 31.99999999999399
target distance 13.0
model initialize at round 2908
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 20.]), 'previousTarget': array([25., 20.]), 'currentState': array([12.62921042, 18.56092355,  0.17761922]), 'targetState': array([25, 20], dtype=int32), 'currentDistance': 12.454211170249815}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9296013300880104
{'scaleFactor': 20, 'currentTarget': array([25., 20.]), 'previousTarget': array([25., 20.]), 'currentState': array([24.39875542, 19.92872872,  0.29805925]), 'targetState': array([25, 20], dtype=int32), 'currentDistance': 0.6054540825608666}
episode index:2909
target Thresh 31.99999999999405
target distance 16.0
model initialize at round 2909
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([23.58666328,  7.57740745,  2.22473206]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 17.318062497342005}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9295958029118576
{'scaleFactor': 20, 'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([13.90655195, 22.17111457,  2.37025279]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 0.19496854428462956}
episode index:2910
target Thresh 31.99999999999411
target distance 16.0
model initialize at round 2910
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([3.72535209, 7.93632454, 2.08112514]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 14.066356986374226}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9295966512612205
{'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([ 4.18422897, 21.4579967 ,  1.76844513]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 0.5724577606169058}
episode index:2911
target Thresh 31.999999999994166
target distance 5.0
model initialize at round 2911
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([11.55211126, 11.85790808,  3.61668873]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 7.355188736864347}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9296072966454028
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.57680256,  6.01763742,  5.98105849]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.4235648159529938}
episode index:2912
target Thresh 31.999999999994223
target distance 17.0
model initialize at round 2912
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 7.]), 'previousTarget': array([9., 7.]), 'currentState': array([12.28099759, 22.02996241,  4.6787886 ]), 'targetState': array([9, 7], dtype=int32), 'currentDistance': 15.383910916868917}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.929604940791569
{'scaleFactor': 20, 'currentTarget': array([9., 7.]), 'previousTarget': array([9., 7.]), 'currentState': array([8.77756361, 6.8176937 , 4.71273949]), 'targetState': array([9, 7], dtype=int32), 'currentDistance': 0.2875996112013228}
episode index:2913
target Thresh 31.999999999994284
target distance 17.0
model initialize at round 2913
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([ 3.05707009, 17.69042339,  0.11831015]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 17.70083687069707}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9295994199633919
{'scaleFactor': 20, 'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([18.7297591 ,  9.9460657 ,  6.08253658]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 0.2755704168482816}
episode index:2914
target Thresh 31.99999999999434
target distance 5.0
model initialize at round 2914
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([8.59048434, 7.95734532, 1.74807791]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 3.4332765625165114}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9296167443476241
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.63405635, 11.35924252,  2.42325877]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.512805947844804}
episode index:2915
target Thresh 31.999999999994394
target distance 13.0
model initialize at round 2915
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 20.]), 'previousTarget': array([18., 20.]), 'currentState': array([17.67069975,  5.45644892,  6.13229322]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 14.54727866820202}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9296143876775556
{'scaleFactor': 20, 'currentTarget': array([18., 20.]), 'previousTarget': array([18., 20.]), 'currentState': array([18.13717502, 20.33890836,  1.90559538]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 0.3656171039200325}
episode index:2916
target Thresh 31.99999999999445
target distance 6.0
model initialize at round 2916
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([19.09140591, 17.76407329,  2.26578981]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 5.244031426471563}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9296316950523662
{'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([16.92639425, 21.12143643,  2.15324311]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 1.2767459653284339}
episode index:2917
target Thresh 31.999999999994508
target distance 7.0
model initialize at round 2917
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 22.]), 'previousTarget': array([ 9., 22.]), 'currentState': array([ 8.31526554, 13.95001683,  0.3362934 ]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 8.07905256288686}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9296423065379548
{'scaleFactor': 20, 'currentTarget': array([ 9., 22.]), 'previousTarget': array([ 9., 22.]), 'currentState': array([ 8.88292175, 21.2486485 ,  1.24959961]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 0.7604185696781166}
episode index:2918
target Thresh 31.99999999999456
target distance 16.0
model initialize at round 2918
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 29.]), 'previousTarget': array([25., 29.]), 'currentState': array([10.23985292, 21.56646456,  0.81845461]), 'targetState': array([25, 29], dtype=int32), 'currentDistance': 16.526324186730758}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9296399435327783
{'scaleFactor': 20, 'currentTarget': array([25., 29.]), 'previousTarget': array([25., 29.]), 'currentState': array([24.08968219, 28.60859368,  0.26784984]), 'targetState': array([25, 29], dtype=int32), 'currentDistance': 0.990897286566003}
episode index:2919
target Thresh 31.999999999994614
target distance 4.0
model initialize at round 2919
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'previousTarget': array([13., 22.]), 'currentState': array([ 9.3976689 , 23.36468733,  5.96093488]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 3.85216313885731}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9296572243740343
{'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'previousTarget': array([13., 22.]), 'currentState': array([13.03094799, 21.94365534,  0.1791954 ]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 0.06428451579648209}
episode index:2920
target Thresh 31.999999999994667
target distance 6.0
model initialize at round 2920
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([26.13899269, 14.32277935,  3.7850709 ]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 4.823039604933613}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9296744933831497
{'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([24.68923915, 10.718986  ,  4.10645336]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 0.9959876888692308}
episode index:2921
target Thresh 31.99999999999472
target distance 18.0
model initialize at round 2921
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 26.]), 'previousTarget': array([ 4., 26.]), 'currentState': array([20.29656377, 21.23022433,  3.95543766]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 16.980245899922544}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9296689638670992
{'scaleFactor': 20, 'currentTarget': array([ 4., 26.]), 'previousTarget': array([ 4., 26.]), 'currentState': array([ 4.28947921, 25.99137013,  3.16124049]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 0.2896078204001944}
episode index:2922
target Thresh 31.999999999994774
target distance 18.0
model initialize at round 2922
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  9.]), 'previousTarget': array([23.,  9.]), 'currentState': array([3.91901529, 0.71009297, 5.02488351]), 'targetState': array([23,  9], dtype=int32), 'currentDistance': 20.804002885245342}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9296572188415746
{'scaleFactor': 20, 'currentTarget': array([23.,  9.]), 'previousTarget': array([23.,  9.]), 'currentState': array([22.9502604 ,  8.89467402,  0.70602483]), 'targetState': array([23,  9], dtype=int32), 'currentDistance': 0.11647999864208755}
episode index:2923
target Thresh 31.999999999994827
target distance 17.0
model initialize at round 2923
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([25.71368901, 25.91473869,  4.85241818]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 22.365127536645744}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9296424198172498
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([9.72772281, 9.3344835 , 4.15003618]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.7190598539902086}
episode index:2924
target Thresh 31.999999999994877
target distance 15.0
model initialize at round 2924
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  4.]), 'previousTarget': array([25.,  4.]), 'currentState': array([18.44489227, 17.83218898,  4.93654871]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 15.306824927725813}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9296400616205355
{'scaleFactor': 20, 'currentTarget': array([25.,  4.]), 'previousTarget': array([25.,  4.]), 'currentState': array([25.51573851,  3.72024243,  5.57896142]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 0.5867286537193194}
episode index:2925
target Thresh 31.99999999999493
target distance 15.0
model initialize at round 2925
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 2.]), 'previousTarget': array([7., 2.]), 'currentState': array([17.38145692, 16.53879896,  4.42918396]), 'targetState': array([7, 2], dtype=int32), 'currentDistance': 17.864806831404234}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9296345514311516
{'scaleFactor': 20, 'currentTarget': array([7., 2.]), 'previousTarget': array([7., 2.]), 'currentState': array([6.95075505, 2.18175942, 4.36227041]), 'targetState': array([7, 2], dtype=int32), 'currentDistance': 0.1883123785920164}
episode index:2926
target Thresh 31.99999999999498
target distance 18.0
model initialize at round 2926
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([ 6.42558027, 25.06344783,  5.15868968]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 16.069084443151475}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9296321975339862
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([5.880717  , 9.52663873, 4.72028007]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.5399785076851676}
episode index:2927
target Thresh 31.99999999999503
target distance 9.0
model initialize at round 2927
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 19.]), 'previousTarget': array([ 9., 19.]), 'currentState': array([15.70785387, 26.92169278,  4.8470242 ]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 10.38019845805246}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9296394918824719
{'scaleFactor': 20, 'currentTarget': array([ 9., 19.]), 'previousTarget': array([ 9., 19.]), 'currentState': array([ 9.63685937, 19.49037595,  3.80743536]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 0.8037775968378994}
episode index:2928
target Thresh 31.99999999999508
target distance 3.0
model initialize at round 2928
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 19.]), 'previousTarget': array([19., 19.]), 'currentState': array([20.08174795, 18.5036789 ,  2.65828964]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 1.1901736275885035}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.929660099771894
{'scaleFactor': 20, 'currentTarget': array([19., 19.]), 'previousTarget': array([19., 19.]), 'currentState': array([18.14697837, 18.88276674,  3.2437841 ]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 0.8610398042003066}
episode index:2929
target Thresh 31.99999999999513
target distance 7.0
model initialize at round 2929
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 13.]), 'previousTarget': array([24., 13.]), 'currentState': array([21.61815814, 19.53745019,  4.99476058]), 'targetState': array([24, 13], dtype=int32), 'currentDistance': 6.957831963890716}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9296739697037125
{'scaleFactor': 20, 'currentTarget': array([24., 13.]), 'previousTarget': array([24., 13.]), 'currentState': array([23.744904  , 13.93888957,  5.06945586]), 'targetState': array([24, 13], dtype=int32), 'currentDistance': 0.9729273291600155}
episode index:2930
target Thresh 31.999999999995175
target distance 14.0
model initialize at round 2930
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 15.]), 'previousTarget': array([ 7., 15.]), 'currentState': array([21.52498201, 27.59899405,  2.26356438]), 'targetState': array([ 7, 15], dtype=int32), 'currentDistance': 19.227837982392483}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9296653406028271
{'scaleFactor': 20, 'currentTarget': array([ 7., 15.]), 'previousTarget': array([ 7., 15.]), 'currentState': array([ 7.20067996, 14.91401415,  4.25192575]), 'targetState': array([ 7, 15], dtype=int32), 'currentDistance': 0.21832547427871418}
episode index:2931
target Thresh 31.999999999995225
target distance 16.0
model initialize at round 2931
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 26.]), 'previousTarget': array([23., 26.]), 'currentState': array([ 8.87470028, 22.68119493,  0.47684455]), 'targetState': array([23, 26], dtype=int32), 'currentDistance': 14.509946903168414}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9296661591592066
{'scaleFactor': 20, 'currentTarget': array([23., 26.]), 'previousTarget': array([23., 26.]), 'currentState': array([22.330205  , 25.79576737,  0.69094482]), 'targetState': array([23, 26], dtype=int32), 'currentDistance': 0.7002401779287596}
episode index:2932
target Thresh 31.99999999999527
target distance 15.0
model initialize at round 2932
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([ 6.99607171, 16.98246059,  6.16579879]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 13.154174276138859}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9296669771574159
{'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([20.56614927, 15.06631192,  6.23622371]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 0.570019530277945}
episode index:2933
target Thresh 31.999999999995318
target distance 20.0
model initialize at round 2933
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 25.]), 'previousTarget': array([ 5., 25.]), 'currentState': array([12.4426754 ,  6.62370766,  2.31463253]), 'targetState': array([ 5, 25], dtype=int32), 'currentDistance': 19.826283999749286}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9296583592630231
{'scaleFactor': 20, 'currentTarget': array([ 5., 25.]), 'previousTarget': array([ 5., 25.]), 'currentState': array([ 4.66958098, 24.86384797,  2.46302818]), 'targetState': array([ 5, 25], dtype=int32), 'currentDistance': 0.35737110533118266}
episode index:2934
target Thresh 31.999999999995364
target distance 7.0
model initialize at round 2934
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 14.]), 'previousTarget': array([13., 14.]), 'currentState': array([18.44456438,  7.86349397,  1.54876631]), 'targetState': array([13, 14], dtype=int32), 'currentDistance': 8.203656957151567}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9296689002002417
{'scaleFactor': 20, 'currentTarget': array([13., 14.]), 'previousTarget': array([13., 14.]), 'currentState': array([13.10119516, 13.13554602,  2.22189077]), 'targetState': array([13, 14], dtype=int32), 'currentDistance': 0.8703569099997224}
episode index:2935
target Thresh 31.99999999999541
target distance 4.0
model initialize at round 2935
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 7.]), 'previousTarget': array([5., 7.]), 'currentState': array([7.41072281, 6.44629717, 2.46684098]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 2.473493743839214}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9296894489399554
{'scaleFactor': 20, 'currentTarget': array([5., 7.]), 'previousTarget': array([5., 7.]), 'currentState': array([5.52716938, 6.67656335, 3.58422351]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 0.618481060467916}
episode index:2936
target Thresh 31.999999999995456
target distance 18.0
model initialize at round 2936
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 16.]), 'previousTarget': array([21., 16.]), 'currentState': array([ 4.47121376, 23.8172626 ,  5.7802431 ]), 'targetState': array([21, 16], dtype=int32), 'currentDistance': 18.28415623447007}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9296839425724184
{'scaleFactor': 20, 'currentTarget': array([21., 16.]), 'previousTarget': array([21., 16.]), 'currentState': array([20.58505271, 16.2175468 ,  6.15506087]), 'targetState': array([21, 16], dtype=int32), 'currentDistance': 0.46851666750219434}
episode index:2937
target Thresh 31.999999999995502
target distance 5.0
model initialize at round 2937
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  4.]), 'previousTarget': array([19.,  4.]), 'currentState': array([21.12390314, 10.43695594,  3.12830997]), 'targetState': array([19,  4], dtype=int32), 'currentDistance': 6.778301143713991}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9296944640385271
{'scaleFactor': 20, 'currentTarget': array([19.,  4.]), 'previousTarget': array([19.,  4.]), 'currentState': array([18.75320568,  3.50240378,  4.8989515 ]), 'targetState': array([19,  4], dtype=int32), 'currentDistance': 0.5554362587070926}
episode index:2938
target Thresh 31.99999999999555
target distance 17.0
model initialize at round 2938
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 20.]), 'previousTarget': array([11., 20.]), 'currentState': array([15.60392984,  4.6357042 ,  2.81835836]), 'targetState': array([11, 20], dtype=int32), 'currentDistance': 16.03925669612427}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9296920993670025
{'scaleFactor': 20, 'currentTarget': array([11., 20.]), 'previousTarget': array([11., 20.]), 'currentState': array([11.17962273, 19.64542723,  2.22835258]), 'targetState': array([11, 20], dtype=int32), 'currentDistance': 0.3974747422485519}
episode index:2939
target Thresh 31.99999999999559
target distance 16.0
model initialize at round 2939
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([16.82918618, 21.37877185,  4.06197135]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 19.270108928187287}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9296834905151801
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.23773298, 6.37365087, 4.61210585]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.6699479087111672}
episode index:2940
target Thresh 31.999999999995634
target distance 14.0
model initialize at round 2940
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 18.]), 'previousTarget': array([ 5., 18.]), 'currentState': array([20.53828253, 10.31730319,  0.59231013]), 'targetState': array([ 5, 18], dtype=int32), 'currentDistance': 17.333841303869935}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9296779936627381
{'scaleFactor': 20, 'currentTarget': array([ 5., 18.]), 'previousTarget': array([ 5., 18.]), 'currentState': array([ 5.81812967, 18.14317844,  3.10449069]), 'targetState': array([ 5, 18], dtype=int32), 'currentDistance': 0.830563801835175}
episode index:2941
target Thresh 31.99999999999568
target distance 13.0
model initialize at round 2941
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 13.]), 'previousTarget': array([15., 13.]), 'currentState': array([ 3.66973974, 17.21061171,  5.39865715]), 'targetState': array([15, 13], dtype=int32), 'currentDistance': 12.087350768373001}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9296820052724384
{'scaleFactor': 20, 'currentTarget': array([15., 13.]), 'previousTarget': array([15., 13.]), 'currentState': array([14.66975585, 12.95503123,  6.21822767]), 'targetState': array([15, 13], dtype=int32), 'currentDistance': 0.33329174528713357}
episode index:2942
target Thresh 31.999999999995723
target distance 16.0
model initialize at round 2942
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  5.]), 'previousTarget': array([10.,  5.]), 'currentState': array([ 4.39610086, 21.50979959,  3.84384108]), 'targetState': array([10,  5], dtype=int32), 'currentDistance': 17.434941011105142}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9296765126602097
{'scaleFactor': 20, 'currentTarget': array([10.,  5.]), 'previousTarget': array([10.,  5.]), 'currentState': array([10.01047084,  5.11084864,  5.51543313]), 'targetState': array([10,  5], dtype=int32), 'currentDistance': 0.11134208423074113}
episode index:2943
target Thresh 31.999999999995765
target distance 18.0
model initialize at round 2943
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 27.]), 'previousTarget': array([25., 27.]), 'currentState': array([ 5.53806817, 13.16488759,  4.66946495]), 'targetState': array([25, 27], dtype=int32), 'currentDistance': 23.87838198084581}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9296587968009498
{'scaleFactor': 20, 'currentTarget': array([25., 27.]), 'previousTarget': array([25., 27.]), 'currentState': array([24.79937989, 27.51155003,  1.25033831]), 'targetState': array([25, 27], dtype=int32), 'currentDistance': 0.5494832671044706}
episode index:2944
target Thresh 31.999999999995808
target distance 9.0
model initialize at round 2944
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  4.]), 'previousTarget': array([18.,  4.]), 'currentState': array([10.99648207,  2.04513546,  0.11857153]), 'targetState': array([18,  4], dtype=int32), 'currentDistance': 7.271228148198031}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9296693017969426
{'scaleFactor': 20, 'currentTarget': array([18.,  4.]), 'previousTarget': array([18.,  4.]), 'currentState': array([18.09976621,  4.78447364,  0.96036438]), 'targetState': array([18,  4], dtype=int32), 'currentDistance': 0.7907921315559159}
episode index:2945
target Thresh 31.999999999995847
target distance 10.0
model initialize at round 2945
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 19.]), 'previousTarget': array([23., 19.]), 'currentState': array([14.98348922, 18.76274365,  6.2493438 ]), 'targetState': array([23, 19], dtype=int32), 'currentDistance': 8.020020930483055}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9296797996612342
{'scaleFactor': 20, 'currentTarget': array([23., 19.]), 'previousTarget': array([23., 19.]), 'currentState': array([22.89433143, 19.0139899 ,  0.47072382]), 'targetState': array([23, 19], dtype=int32), 'currentDistance': 0.10659063992664758}
episode index:2946
target Thresh 31.99999999999589
target distance 1.0
model initialize at round 2946
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 11.]), 'previousTarget': array([20., 11.]), 'currentState': array([21.68836682, 11.53575363,  2.1594175 ]), 'targetState': array([20, 11], dtype=int32), 'currentDistance': 1.771331267613095}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9297002680020344
{'scaleFactor': 20, 'currentTarget': array([20., 11.]), 'previousTarget': array([20., 11.]), 'currentState': array([20.00388137, 11.52683833,  4.15430175]), 'targetState': array([20, 11], dtype=int32), 'currentDistance': 0.526852625924499}
episode index:2947
target Thresh 31.99999999999593
target distance 20.0
model initialize at round 2947
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([21.00442894,  5.98311155,  3.03453383]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 18.03312295338772}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.929694778510678
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.14278806, 6.98510381, 3.59626722]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.14356297127686665}
episode index:2948
target Thresh 31.99999999999597
target distance 16.0
model initialize at round 2948
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([26.94497384,  8.39262792,  1.98460883]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 17.021039438081996}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9296892927422729
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([ 9.79808552, 10.01362759,  3.44890874]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.20237383079798496}
episode index:2949
target Thresh 31.99999999999601
target distance 19.0
model initialize at round 2949
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([20.00421216, 25.35193873,  3.68878448]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 24.294711540196236}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9296746132775181
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.37014812, 8.38546762, 4.33497427]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.5344108154423858}
episode index:2950
target Thresh 31.99999999999605
target distance 16.0
model initialize at round 2950
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 12.]), 'previousTarget': array([ 3., 12.]), 'currentState': array([20.61558481, 13.47145923,  1.293935  ]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 17.676934703246026}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9296691380603734
{'scaleFactor': 20, 'currentTarget': array([ 3., 12.]), 'previousTarget': array([ 3., 12.]), 'currentState': array([ 3.90518967, 12.06468449,  3.20717818]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 0.9074978891669607}
episode index:2951
target Thresh 31.99999999999609
target distance 10.0
model initialize at round 2951
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([21.65317634, 20.0091978 ,  3.50851583]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 9.989272610562649}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9296763605914845
{'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([13.55366769, 24.92939305,  3.07131942]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 0.5581516432616549}
episode index:2952
target Thresh 31.999999999996128
target distance 10.0
model initialize at round 2952
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([ 8.54588341, 23.84730929,  3.62399316]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 14.473136096823007}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9296740132612563
{'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([20.22715147, 14.98791906,  6.10298676]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 0.22747250614214995}
episode index:2953
target Thresh 31.999999999996167
target distance 19.0
model initialize at round 2953
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  7.]), 'previousTarget': array([18.,  7.]), 'currentState': array([21.8616634 , 24.26547074,  5.54310489]), 'targetState': array([18,  7], dtype=int32), 'currentDistance': 17.692058223462638}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9296685438077094
{'scaleFactor': 20, 'currentTarget': array([18.,  7.]), 'previousTarget': array([18.,  7.]), 'currentState': array([18.03853272,  7.17083165,  5.02732187]), 'targetState': array([18,  7], dtype=int32), 'currentDistance': 0.17512344836616964}
episode index:2954
target Thresh 31.999999999996206
target distance 21.0
model initialize at round 2954
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 18.]), 'previousTarget': array([ 5., 18.]), 'currentState': array([24.52512241,  8.81063195,  3.64903617]), 'targetState': array([ 5, 18], dtype=int32), 'currentDistance': 21.57950161659271}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9296569261124304
{'scaleFactor': 20, 'currentTarget': array([ 5., 18.]), 'previousTarget': array([ 5., 18.]), 'currentState': array([ 5.18870824, 17.96618742,  2.9827752 ]), 'targetState': array([ 5, 18], dtype=int32), 'currentDistance': 0.19171356550259924}
episode index:2955
target Thresh 31.999999999996245
target distance 23.0
model initialize at round 2955
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([24.08097433, 21.40988649,  3.15846658]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 28.748797327083313}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.9296333812641655
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.84947354, 3.57773248, 3.42239966]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.0273169458400493}
episode index:2956
target Thresh 31.99999999999628
target distance 13.0
model initialize at round 2956
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  9.]), 'previousTarget': array([21.,  9.]), 'currentState': array([ 6.54908951, 16.85278769,  3.6202215 ]), 'targetState': array([21,  9], dtype=int32), 'currentDistance': 16.4467348858733}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9296279311005603
{'scaleFactor': 20, 'currentTarget': array([21.,  9.]), 'previousTarget': array([21.,  9.]), 'currentState': array([20.77857126,  9.12254985,  6.03978015]), 'targetState': array([21,  9], dtype=int32), 'currentDistance': 0.25307934369589696}
episode index:2957
target Thresh 31.999999999996316
target distance 4.0
model initialize at round 2957
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 25.]), 'previousTarget': array([14., 25.]), 'currentState': array([16.58489108, 26.08880735,  2.70387757]), 'targetState': array([14, 25], dtype=int32), 'currentDistance': 2.804846399620839}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9296483408601611
{'scaleFactor': 20, 'currentTarget': array([14., 25.]), 'previousTarget': array([14., 25.]), 'currentState': array([14.77893779, 25.67257482,  4.04577136]), 'targetState': array([14, 25], dtype=int32), 'currentDistance': 1.029126310484242}
episode index:2958
target Thresh 31.999999999996355
target distance 10.0
model initialize at round 2958
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 19.]), 'previousTarget': array([13., 19.]), 'currentState': array([21.64284087, 24.77337306,  4.60879159]), 'targetState': array([13, 19], dtype=int32), 'currentDistance': 10.393773849333773}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9296555533336455
{'scaleFactor': 20, 'currentTarget': array([13., 19.]), 'previousTarget': array([13., 19.]), 'currentState': array([13.70420032, 19.3745523 ,  3.75616807]), 'targetState': array([13, 19], dtype=int32), 'currentDistance': 0.7976136376204448}
episode index:2959
target Thresh 31.99999999999639
target distance 12.0
model initialize at round 2959
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([14.31727727, 15.02884963,  4.13444972]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 14.388329825673937}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9296563674534337
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.46031039, 5.40388631, 4.27500885]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.6123804411853838}
episode index:2960
target Thresh 31.999999999996426
target distance 14.0
model initialize at round 2960
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([22.25252676, 25.50786997,  3.04100978]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 18.59054426417482}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9296478317248136
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([11.96962125,  9.2424999 ,  4.53452121]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.7581090050531253}
episode index:2961
target Thresh 31.99999999999646
target distance 12.0
model initialize at round 2961
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([18.43619385, 10.78438042,  4.04568595]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 13.01962828099347}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9296518264303084
{'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([8.92321169, 3.6969823 , 3.81493977]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 1.1567645186072593}
episode index:2962
target Thresh 31.999999999996497
target distance 7.0
model initialize at round 2962
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([10.58424768, 18.34486407,  1.99999893]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 5.872853067092586}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9296655446799101
{'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([ 8.97995157, 23.98983343,  2.45208599]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 0.022478850199592986}
episode index:2963
target Thresh 31.999999999996533
target distance 15.0
model initialize at round 2963
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 5.]), 'previousTarget': array([7., 5.]), 'currentState': array([22.16075946,  3.67527445,  2.48512918]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 15.218525727883991}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9296632097101895
{'scaleFactor': 20, 'currentTarget': array([7., 5.]), 'previousTarget': array([7., 5.]), 'currentState': array([6.70618931, 4.77829496, 3.43168028]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 0.3680731536901636}
episode index:2964
target Thresh 31.999999999996568
target distance 14.0
model initialize at round 2964
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 15.]), 'previousTarget': array([ 9., 15.]), 'currentState': array([20.29679724, 27.21305384,  3.8406862 ]), 'targetState': array([ 9, 15], dtype=int32), 'currentDistance': 16.636595563059323}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9296608763154905
{'scaleFactor': 20, 'currentTarget': array([ 9., 15.]), 'previousTarget': array([ 9., 15.]), 'currentState': array([ 9.58289557, 15.66726784,  4.34220052]), 'targetState': array([ 9, 15], dtype=int32), 'currentDistance': 0.8860099465383703}
episode index:2965
target Thresh 31.9999999999966
target distance 25.0
model initialize at round 2965
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.95317949,  4.07026262]), 'previousTarget': array([25.,  4.]), 'currentState': array([ 8.31739927, 29.03525444,  4.13064337]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 30.000000000000004}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.929637409517893
{'scaleFactor': 20, 'currentTarget': array([25.,  4.]), 'previousTarget': array([25.,  4.]), 'currentState': array([24.4915484 ,  4.96988849,  5.45175682]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 1.0950829728426255}
episode index:2966
target Thresh 31.999999999996636
target distance 12.0
model initialize at round 2966
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  7.]), 'previousTarget': array([21.,  7.]), 'currentState': array([10.99908741,  9.02094055,  6.24408457]), 'targetState': array([21,  7], dtype=int32), 'currentDistance': 10.203060983911225}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9296446062285039
{'scaleFactor': 20, 'currentTarget': array([21.,  7.]), 'previousTarget': array([21.,  7.]), 'currentState': array([20.61250499,  7.04462243,  0.35061398]), 'targetState': array([21,  7], dtype=int32), 'currentDistance': 0.390055821308906}
episode index:2967
target Thresh 31.999999999996668
target distance 10.0
model initialize at round 2967
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([14.9287972 ,  9.84350291,  5.75447035]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 10.581958548672453}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9296517980895792
{'scaleFactor': 20, 'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([22.41648705,  3.47285251,  5.94154393]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 0.7510505047002644}
episode index:2968
target Thresh 31.9999999999967
target distance 2.0
model initialize at round 2968
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 22.]), 'previousTarget': array([24., 22.]), 'currentState': array([22.37469924, 21.35927194,  5.94691229]), 'targetState': array([24, 22], dtype=int32), 'currentDistance': 1.7470360636311684}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9296721241932876
{'scaleFactor': 20, 'currentTarget': array([24., 22.]), 'previousTarget': array([24., 22.]), 'currentState': array([24.21877012, 21.78812687,  0.80467707]), 'targetState': array([24, 22], dtype=int32), 'currentDistance': 0.3045498161741065}
episode index:2969
target Thresh 31.999999999996735
target distance 4.0
model initialize at round 2969
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([23.38854467,  8.51461375,  2.42415905]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 4.225288505324875}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9296891032760508
{'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([21.09376953, 11.7498561 ,  2.39928272]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 0.26714171653865926}
episode index:2970
target Thresh 31.999999999996767
target distance 9.0
model initialize at round 2970
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 16.]), 'previousTarget': array([ 7., 16.]), 'currentState': array([17.07587142,  5.69964587,  0.12240988]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 14.409041605328152}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.929686765878256
{'scaleFactor': 20, 'currentTarget': array([ 7., 16.]), 'previousTarget': array([ 7., 16.]), 'currentState': array([ 7.22962041, 15.9415647 ,  2.89778359]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 0.23693926485123576}
episode index:2971
target Thresh 31.9999999999968
target distance 4.0
model initialize at round 2971
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([18.33542058, 21.75188021,  4.29956174]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 4.104562065426209}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9297037286084452
{'scaleFactor': 20, 'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([19.85049821, 18.30976465,  4.90886319]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 0.3439548278738134}
episode index:2972
target Thresh 31.99999999999683
target distance 17.0
model initialize at round 2972
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  4.]), 'previousTarget': array([27.,  4.]), 'currentState': array([11.93797486, 12.66509055,  5.79201523]), 'targetState': array([27,  4], dtype=int32), 'currentDistance': 17.37666237851974}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9296982841142895
{'scaleFactor': 20, 'currentTarget': array([27.,  4.]), 'previousTarget': array([27.,  4.]), 'currentState': array([27.46153396,  4.06623431,  6.09095271]), 'targetState': array([27,  4], dtype=int32), 'currentDistance': 0.4662623511843022}
episode index:2973
target Thresh 31.999999999996863
target distance 19.0
model initialize at round 2973
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 24.]), 'previousTarget': array([ 6., 24.]), 'currentState': array([22.3565024 ,  6.07942424,  2.23837221]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 24.262774088353776}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9296837200886007
{'scaleFactor': 20, 'currentTarget': array([ 6., 24.]), 'previousTarget': array([ 6., 24.]), 'currentState': array([ 6.39322652, 23.03669227,  2.080213  ]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 1.040475314032686}
episode index:2974
target Thresh 31.99999999999689
target distance 2.0
model initialize at round 2974
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 8.]), 'previousTarget': array([7., 8.]), 'currentState': array([7.68157888, 5.93158532, 0.96933764]), 'targetState': array([7, 8], dtype=int32), 'currentDistance': 2.177817496638}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9297039944684028
{'scaleFactor': 20, 'currentTarget': array([7., 8.]), 'previousTarget': array([7., 8.]), 'currentState': array([7.55639885, 7.7785265 , 2.32111972]), 'targetState': array([7, 8], dtype=int32), 'currentDistance': 0.5988574096526796}
episode index:2975
target Thresh 31.999999999996923
target distance 19.0
model initialize at round 2975
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 24.]), 'previousTarget': array([ 5., 24.]), 'currentState': array([22.71025452,  8.4771327 ,  2.63525945]), 'targetState': array([ 5, 24], dtype=int32), 'currentDistance': 23.550212834485627}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9296894383115639
{'scaleFactor': 20, 'currentTarget': array([ 5., 24.]), 'previousTarget': array([ 5., 24.]), 'currentState': array([ 5.26560685, 23.75480863,  2.72024958]), 'targetState': array([ 5, 24], dtype=int32), 'currentDistance': 0.3614772551156986}
episode index:2976
target Thresh 31.999999999996955
target distance 14.0
model initialize at round 2976
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 15.]), 'previousTarget': array([21., 15.]), 'currentState': array([ 7.39557331, 24.36417915,  5.95965362]), 'targetState': array([21, 15], dtype=int32), 'currentDistance': 16.515697887703837}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9296871055121405
{'scaleFactor': 20, 'currentTarget': array([21., 15.]), 'previousTarget': array([21., 15.]), 'currentState': array([20.21265439, 15.3713907 ,  5.97416146]), 'targetState': array([21, 15], dtype=int32), 'currentDistance': 0.8705424539517423}
episode index:2977
target Thresh 31.999999999996984
target distance 20.0
model initialize at round 2977
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 14.]), 'previousTarget': array([24., 14.]), 'currentState': array([3.72752989, 9.33923267, 5.55977488]), 'targetState': array([24, 14], dtype=int32), 'currentDistance': 20.80134121883565}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9296755713109136
{'scaleFactor': 20, 'currentTarget': array([24., 14.]), 'previousTarget': array([24., 14.]), 'currentState': array([23.79104575, 13.85403221,  0.49583701]), 'targetState': array([24, 14], dtype=int32), 'currentDistance': 0.254889143891116}
episode index:2978
target Thresh 31.999999999997016
target distance 19.0
model initialize at round 2978
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 7.]), 'previousTarget': array([6., 7.]), 'currentState': array([25.46798272, 10.61659527,  2.29901251]), 'targetState': array([6, 7], dtype=int32), 'currentDistance': 19.801063419466505}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9296670807112822
{'scaleFactor': 20, 'currentTarget': array([6., 7.]), 'previousTarget': array([6., 7.]), 'currentState': array([6.8955342 , 7.36105729, 3.80207187]), 'targetState': array([6, 7], dtype=int32), 'currentDistance': 0.9655795509448082}
episode index:2979
target Thresh 31.999999999997044
target distance 22.0
model initialize at round 2979
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([ 9.57899374, 28.09826317,  4.71704865]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 22.05370785798572}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9296555609708619
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.97555729,  7.57974304,  5.2912056 ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.5802580751228329}
episode index:2980
target Thresh 31.999999999997073
target distance 20.0
model initialize at round 2980
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 16.]), 'previousTarget': array([ 6., 16.]), 'currentState': array([24.5144456 , 14.79089582,  3.66236925]), 'targetState': array([ 6, 16], dtype=int32), 'currentDistance': 18.553884462848455}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9296470827803345
{'scaleFactor': 20, 'currentTarget': array([ 6., 16.]), 'previousTarget': array([ 6., 16.]), 'currentState': array([ 5.0396098 , 15.64130721,  3.35139905]), 'targetState': array([ 6, 16], dtype=int32), 'currentDistance': 1.0251877139409762}
episode index:2981
target Thresh 31.999999999997105
target distance 17.0
model initialize at round 2981
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([13.32693228, 27.96480011,  4.61066872]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 17.713012470341216}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.929641673714172
{'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([20.8361377 , 12.21251189,  5.59808988]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 0.2683508082027935}
episode index:2982
target Thresh 31.999999999997133
target distance 4.0
model initialize at round 2982
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 26.]), 'previousTarget': array([22., 26.]), 'currentState': array([16.44694757, 28.64841003,  3.75607622]), 'targetState': array([22, 26], dtype=int32), 'currentDistance': 6.152273318275579}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9296520506287833
{'scaleFactor': 20, 'currentTarget': array([22., 26.]), 'previousTarget': array([22., 26.]), 'currentState': array([22.15860737, 25.68687352,  0.56332549]), 'targetState': array([22, 26], dtype=int32), 'currentDistance': 0.3510049731126083}
episode index:2983
target Thresh 31.99999999999716
target distance 20.0
model initialize at round 2983
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([19.7869445 , 26.83343049,  4.9174583 ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 19.702457472700594}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.929643582138294
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.06752071,  8.22047107,  4.83933207]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.23057870458789814}
episode index:2984
target Thresh 31.99999999999719
target distance 16.0
model initialize at round 2984
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 22.]), 'previousTarget': array([ 2., 22.]), 'currentState': array([19.67991111, 23.10142367,  1.07030123]), 'targetState': array([ 2, 22], dtype=int32), 'currentDistance': 17.714186146013187}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9296351193218353
{'scaleFactor': 20, 'currentTarget': array([ 2., 22.]), 'previousTarget': array([ 2., 22.]), 'currentState': array([ 1.5149555 , 21.7541151 ,  3.48287591]), 'targetState': array([ 2, 22], dtype=int32), 'currentDistance': 0.543808378006415}
episode index:2985
target Thresh 31.99999999999722
target distance 8.0
model initialize at round 2985
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([15.70848568,  5.1684028 ,  3.31364346]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 8.8888304059912}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9296454880059203
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.96914577, 10.1352461 ,  2.64013623]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.2988621283170125}
episode index:2986
target Thresh 31.999999999997243
target distance 11.0
model initialize at round 2986
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 24.]), 'previousTarget': array([27., 24.]), 'currentState': array([17.60712736, 20.80592221,  1.26374656]), 'targetState': array([27, 24], dtype=int32), 'currentDistance': 9.921098188134053}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9296526338251015
{'scaleFactor': 20, 'currentTarget': array([27., 24.]), 'previousTarget': array([27., 24.]), 'currentState': array([26.5303534 , 23.89923459,  0.87394206]), 'targetState': array([27, 24], dtype=int32), 'currentDistance': 0.4803348752151314}
episode index:2987
target Thresh 31.99999999999727
target distance 14.0
model initialize at round 2987
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 19.]), 'previousTarget': array([13., 19.]), 'currentState': array([23.20838848,  5.82822528,  2.42476118]), 'targetState': array([13, 19], dtype=int32), 'currentDistance': 16.664538536836886}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9296503219310596
{'scaleFactor': 20, 'currentTarget': array([13., 19.]), 'previousTarget': array([13., 19.]), 'currentState': array([13.70094032, 18.08305902,  2.6759248 ]), 'targetState': array([13, 19], dtype=int32), 'currentDistance': 1.1541655393833345}
episode index:2988
target Thresh 31.9999999999973
target distance 6.0
model initialize at round 2988
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 25.]), 'previousTarget': array([ 2., 25.]), 'currentState': array([ 9.67679298, 23.14406038,  1.09570378]), 'targetState': array([ 2, 25], dtype=int32), 'currentDistance': 7.897953045691629}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9296606751221164
{'scaleFactor': 20, 'currentTarget': array([ 2., 25.]), 'previousTarget': array([ 2., 25.]), 'currentState': array([ 2.98228186, 25.33412197,  3.51938369]), 'targetState': array([ 2, 25], dtype=int32), 'currentDistance': 1.0375524803568525}
episode index:2989
target Thresh 31.999999999997325
target distance 22.0
model initialize at round 2989
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 14.]), 'previousTarget': array([ 3., 14.]), 'currentState': array([24.73171834, 21.66144908,  2.74088895]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 23.042686127051628}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9296462016092716
{'scaleFactor': 20, 'currentTarget': array([ 3., 14.]), 'previousTarget': array([ 3., 14.]), 'currentState': array([ 3.0363709 , 13.85423168,  3.81086186]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 0.1502372955870047}
episode index:2990
target Thresh 31.999999999997353
target distance 17.0
model initialize at round 2990
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 29.]), 'previousTarget': array([ 3., 29.]), 'currentState': array([18.87292914, 13.24983974,  3.31458974]), 'targetState': array([ 3, 29], dtype=int32), 'currentDistance': 22.36106947105538}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9296347312156404
{'scaleFactor': 20, 'currentTarget': array([ 3., 29.]), 'previousTarget': array([ 3., 29.]), 'currentState': array([ 3.83786392, 28.08053591,  2.80638521]), 'targetState': array([ 3, 29], dtype=int32), 'currentDistance': 1.243957460378592}
episode index:2991
target Thresh 31.999999999997378
target distance 17.0
model initialize at round 2991
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 15.]), 'previousTarget': array([ 8., 15.]), 'currentState': array([23.67960341,  8.95647667,  2.8003974 ]), 'targetState': array([ 8, 15], dtype=int32), 'currentDistance': 16.803991712308978}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9296324283958584
{'scaleFactor': 20, 'currentTarget': array([ 8., 15.]), 'previousTarget': array([ 8., 15.]), 'currentState': array([ 8.92259984, 14.41349529,  2.87372596]), 'targetState': array([ 8, 15], dtype=int32), 'currentDistance': 1.093242074322654}
episode index:2992
target Thresh 31.999999999997407
target distance 22.0
model initialize at round 2992
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 21.]), 'previousTarget': array([ 2., 21.]), 'currentState': array([23.37978984,  7.56452147,  2.9582141 ]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 25.250891011264507}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.929615017301506
{'scaleFactor': 20, 'currentTarget': array([ 2., 21.]), 'previousTarget': array([ 2., 21.]), 'currentState': array([ 2.39114761, 20.77072096,  2.88166222]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 0.45339312890262823}
episode index:2993
target Thresh 31.99999999999743
target distance 4.0
model initialize at round 2993
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([5.02993479, 4.13372195, 2.79445887]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 2.034334485951961}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9296351859664019
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.08437983, 4.04284719, 3.59003216]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.09463528641476728}
episode index:2994
target Thresh 31.999999999997456
target distance 3.0
model initialize at round 2994
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 23.]), 'previousTarget': array([19., 23.]), 'currentState': array([20.3926151 , 23.49871677,  3.85073614]), 'targetState': array([19, 23], dtype=int32), 'currentDistance': 1.4792211552069976}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9296553411630741
{'scaleFactor': 20, 'currentTarget': array([19., 23.]), 'previousTarget': array([19., 23.]), 'currentState': array([18.5307535 , 22.95856004,  2.9885571 ]), 'targetState': array([19, 23], dtype=int32), 'currentDistance': 0.47107276733324627}
episode index:2995
target Thresh 31.99999999999748
target distance 13.0
model initialize at round 2995
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 24.]), 'previousTarget': array([ 8., 24.]), 'currentState': array([12.39938455, 12.09438775,  2.10772419]), 'targetState': array([ 8, 24], dtype=int32), 'currentDistance': 12.692446082004494}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.929659288028307
{'scaleFactor': 20, 'currentTarget': array([ 8., 24.]), 'previousTarget': array([ 8., 24.]), 'currentState': array([ 8.22848634, 23.12756756,  2.26654508]), 'targetState': array([ 8, 24], dtype=int32), 'currentDistance': 0.9018560701085858}
episode index:2996
target Thresh 31.999999999997506
target distance 13.0
model initialize at round 2996
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 16.]), 'previousTarget': array([23., 16.]), 'currentState': array([14.66279015,  4.88007157,  1.09052637]), 'targetState': array([23, 16], dtype=int32), 'currentDistance': 13.898268826381551}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9296600908510895
{'scaleFactor': 20, 'currentTarget': array([23., 16.]), 'previousTarget': array([23., 16.]), 'currentState': array([22.69932371, 15.98182783,  1.47640236]), 'targetState': array([23, 16], dtype=int32), 'currentDistance': 0.30122493077675233}
episode index:2997
target Thresh 31.99999999999753
target distance 7.0
model initialize at round 2997
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 17.]), 'previousTarget': array([22., 17.]), 'currentState': array([16.6756892 , 20.84362294,  5.18013376]), 'targetState': array([22, 17], dtype=int32), 'currentDistance': 6.566713239668064}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9296736461910324
{'scaleFactor': 20, 'currentTarget': array([22., 17.]), 'previousTarget': array([22., 17.]), 'currentState': array([21.4451103 , 17.68598061,  5.53157249]), 'targetState': array([22, 17], dtype=int32), 'currentDistance': 0.8823105910052604}
episode index:2998
target Thresh 31.999999999997556
target distance 10.0
model initialize at round 2998
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 16.]), 'previousTarget': array([12., 16.]), 'currentState': array([10.38420646, 25.52925664,  4.43508458]), 'targetState': array([12, 16], dtype=int32), 'currentDistance': 9.665273964267177}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9296807540282145
{'scaleFactor': 20, 'currentTarget': array([12., 16.]), 'previousTarget': array([12., 16.]), 'currentState': array([12.34952936, 15.98864079,  5.34018497]), 'targetState': array([12, 16], dtype=int32), 'currentDistance': 0.3497138940103659}
episode index:2999
target Thresh 31.99999999999758
target distance 2.0
model initialize at round 2999
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  5.]), 'previousTarget': array([26.,  5.]), 'currentState': array([25.64540091,  3.99872751,  1.49628162]), 'targetState': array([26,  5], dtype=int32), 'currentDistance': 1.0622085981970029}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9297008604435384
{'scaleFactor': 20, 'currentTarget': array([26.,  5.]), 'previousTarget': array([26.,  5.]), 'currentState': array([25.87591102,  5.98482309,  1.41339217]), 'targetState': array([26,  5], dtype=int32), 'currentDistance': 0.9926099857116351}
episode index:3000
target Thresh 31.999999999997605
target distance 9.0
model initialize at round 3000
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([10.03961721,  9.61208937,  3.40707964]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 7.221844819863462}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9297111553950732
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.49660765, 7.52692043, 3.31352565]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.6908025325018304}
episode index:3001
target Thresh 31.999999999997627
target distance 15.0
model initialize at round 3001
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([23.23329169,  8.14515488,  1.75835865]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 16.649636891683777}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9297088347884885
{'scaleFactor': 20, 'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([14.43395478, 21.08154658,  2.40459673]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 1.0158117149429047}
episode index:3002
target Thresh 31.99999999999765
target distance 7.0
model initialize at round 3002
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 12.]), 'previousTarget': array([14., 12.]), 'currentState': array([8.4172597 , 9.09238607, 0.44040268]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 6.294537954234973}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9297223513270205
{'scaleFactor': 20, 'currentTarget': array([14., 12.]), 'previousTarget': array([14., 12.]), 'currentState': array([13.65335921, 11.84396004,  0.91965359]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 0.3801424855608255}
episode index:3003
target Thresh 31.999999999997677
target distance 25.0
model initialize at round 3003
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 17.]), 'previousTarget': array([27., 17.]), 'currentState': array([ 0.35688151, 12.63592621,  4.36964464]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 26.998164802769907}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.9296991609153409
{'scaleFactor': 20, 'currentTarget': array([27., 17.]), 'previousTarget': array([27., 17.]), 'currentState': array([27.85960746, 17.11464891,  5.97626651]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 0.86721932720242}
episode index:3004
target Thresh 31.999999999997698
target distance 9.0
model initialize at round 3004
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 27.]), 'previousTarget': array([18., 27.]), 'currentState': array([13.53392265, 16.40396908,  6.04521751]), 'targetState': array([18, 27], dtype=int32), 'currentDistance': 11.498770287917907}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9297030813773993
{'scaleFactor': 20, 'currentTarget': array([18., 27.]), 'previousTarget': array([18., 27.]), 'currentState': array([17.73723258, 26.82459427,  1.741868  ]), 'targetState': array([18, 27], dtype=int32), 'currentDistance': 0.3159333537106304}
episode index:3005
target Thresh 31.999999999997723
target distance 12.0
model initialize at round 3005
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([ 5.41194923, 17.55720987,  3.8141371 ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 12.786751695300186}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9297038672278749
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.99913652, 4.45884496, 4.18812287]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.5411557250035188}
episode index:3006
target Thresh 31.999999999997744
target distance 18.0
model initialize at round 3006
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([21.43312294, 27.61423479,  3.77799129]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 24.830168473277492}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9296865134386403
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([4.87093238, 8.27251287, 4.24818254]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 0.7388477346602979}
episode index:3007
target Thresh 31.999999999997765
target distance 18.0
model initialize at round 3007
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([5.89291862, 7.57343615, 6.2816515 ]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 17.112398721959682}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9296811380177775
{'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([23.42419918,  8.3345547 ,  0.30260987]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 0.5402515951602437}
episode index:3008
target Thresh 31.99999999999779
target distance 13.0
model initialize at round 3008
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 26.]), 'previousTarget': array([10., 26.]), 'currentState': array([24.5804063 , 14.4214639 ,  0.65908306]), 'targetState': array([10, 26], dtype=int32), 'currentDistance': 18.618559183407175}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9296727302201674
{'scaleFactor': 20, 'currentTarget': array([10., 26.]), 'previousTarget': array([10., 26.]), 'currentState': array([ 9.91876513, 25.63555256,  2.81467436]), 'targetState': array([10, 26], dtype=int32), 'currentDistance': 0.37339126715375154}
episode index:3009
target Thresh 31.99999999999781
target distance 19.0
model initialize at round 3009
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  6.]), 'previousTarget': array([10.,  6.]), 'currentState': array([11.60814312, 23.1231137 ,  4.73742497]), 'targetState': array([10,  6], dtype=int32), 'currentDistance': 17.19846351084884}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9296673629501552
{'scaleFactor': 20, 'currentTarget': array([10.,  6.]), 'previousTarget': array([10.,  6.]), 'currentState': array([10.24229434,  5.68607548,  4.90945148]), 'targetState': array([10,  6], dtype=int32), 'currentDistance': 0.39655409783029855}
episode index:3010
target Thresh 31.999999999997833
target distance 21.0
model initialize at round 3010
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 23.]), 'previousTarget': array([26., 23.]), 'currentState': array([23.53545258,  3.61758579,  0.84045577]), 'targetState': array([26, 23], dtype=int32), 'currentDistance': 19.538474215802903}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9296589653121806
{'scaleFactor': 20, 'currentTarget': array([26., 23.]), 'previousTarget': array([26., 23.]), 'currentState': array([25.85526502, 22.70541301,  1.93988258]), 'targetState': array([26, 23], dtype=int32), 'currentDistance': 0.32822204497514523}
episode index:3011
target Thresh 31.999999999997854
target distance 10.0
model initialize at round 3011
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 12.]), 'previousTarget': array([14., 12.]), 'currentState': array([6.51853765, 0.38736796, 5.4322567 ]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 13.813960392614353}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9296597642439851
{'scaleFactor': 20, 'currentTarget': array([14., 12.]), 'previousTarget': array([14., 12.]), 'currentState': array([13.30121747, 11.26340447,  1.2169826 ]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 1.0153176802333261}
episode index:3012
target Thresh 31.999999999997875
target distance 9.0
model initialize at round 3012
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'previousTarget': array([13., 22.]), 'currentState': array([20.60294637, 13.93845382,  1.54012048]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 11.081214746900582}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9296636873721487
{'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'previousTarget': array([13., 22.]), 'currentState': array([12.50636446, 21.95881073,  2.65333427]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 0.4953509894320388}
episode index:3013
target Thresh 31.999999999997897
target distance 11.0
model initialize at round 3013
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([20.55076633, 10.85563418,  1.5982585 ]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 12.075624099923534}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9296676078970421
{'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([14.165182  , 20.49312638,  2.57095256]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 0.5331097041159275}
episode index:3014
target Thresh 31.999999999997918
target distance 17.0
model initialize at round 3014
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 24.]), 'previousTarget': array([26., 24.]), 'currentState': array([10.53606146,  8.31232038,  0.58906859]), 'targetState': array([26, 24], dtype=int32), 'currentDistance': 22.028088590802316}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9296562217100973
{'scaleFactor': 20, 'currentTarget': array([26., 24.]), 'previousTarget': array([26., 24.]), 'currentState': array([25.10137844, 23.78613784,  0.12739381]), 'targetState': array([26, 24], dtype=int32), 'currentDistance': 0.9237195047873316}
episode index:3015
target Thresh 31.99999999999794
target distance 11.0
model initialize at round 3015
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 17.]), 'previousTarget': array([ 6., 17.]), 'currentState': array([15.30756591, 11.90239078,  3.15591097]), 'targetState': array([ 6, 17], dtype=int32), 'currentDistance': 10.612087587943162}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9296632952605582
{'scaleFactor': 20, 'currentTarget': array([ 6., 17.]), 'previousTarget': array([ 6., 17.]), 'currentState': array([ 6.76237554, 16.33993792,  2.30776814]), 'targetState': array([ 6, 17], dtype=int32), 'currentDistance': 1.0084138119052084}
episode index:3016
target Thresh 31.999999999997957
target distance 3.0
model initialize at round 3016
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  4.]), 'previousTarget': array([21.,  4.]), 'currentState': array([19.49952855,  4.6960528 ,  5.36912474]), 'targetState': array([21,  4], dtype=int32), 'currentDistance': 1.6540568482293179}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9296832941683273
{'scaleFactor': 20, 'currentTarget': array([21.,  4.]), 'previousTarget': array([21.,  4.]), 'currentState': array([21.25873748,  3.94630067,  0.11863616]), 'targetState': array([21,  4], dtype=int32), 'currentDistance': 0.2642512091427405}
episode index:3017
target Thresh 31.99999999999798
target distance 3.0
model initialize at round 3017
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([24.58832383,  9.44356897,  0.67303484]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 5.805249113604919}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9296967519900078
{'scaleFactor': 20, 'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([20.88054788, 12.74121948,  3.25425747]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 0.9177864280030645}
episode index:3018
target Thresh 31.999999999998
target distance 20.0
model initialize at round 3018
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([ 7.795765  , 23.82432736,  4.92498994]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 19.696349724502085}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9296883668701068
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.53227551, 5.33840291, 3.66695747]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.6307406358058116}
episode index:3019
target Thresh 31.999999999998018
target distance 20.0
model initialize at round 3019
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 21.]), 'previousTarget': array([22., 21.]), 'currentState': array([ 3.87793938, 29.19904465,  5.79950613]), 'targetState': array([22, 21], dtype=int32), 'currentDistance': 19.8905357939533}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9296799873032653
{'scaleFactor': 20, 'currentTarget': array([22., 21.]), 'previousTarget': array([22., 21.]), 'currentState': array([21.1049971 , 21.22922651,  5.92763801]), 'targetState': array([22, 21], dtype=int32), 'currentDistance': 0.9238912208095853}
episode index:3020
target Thresh 31.99999999999804
target distance 11.0
model initialize at round 3020
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 14.]), 'previousTarget': array([24., 14.]), 'currentState': array([14.25741077, 22.87545263,  0.27368718]), 'targetState': array([24, 14], dtype=int32), 'currentDistance': 13.179214858414708}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9296807768963152
{'scaleFactor': 20, 'currentTarget': array([24., 14.]), 'previousTarget': array([24., 14.]), 'currentState': array([24.31395307, 13.86507419,  5.97105626]), 'targetState': array([24, 14], dtype=int32), 'currentDistance': 0.3417184587115159}
episode index:3021
target Thresh 31.999999999998057
target distance 8.0
model initialize at round 3021
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'previousTarget': array([10., 12.]), 'currentState': array([ 0.33019593, 18.78989894,  4.27675867]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 11.81557609220838}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9296846813875478
{'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'previousTarget': array([10., 12.]), 'currentState': array([ 9.61462531, 12.47364157,  5.60960366]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 0.6106144308209925}
episode index:3022
target Thresh 31.999999999998078
target distance 12.0
model initialize at round 3022
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 24.]), 'previousTarget': array([19., 24.]), 'currentState': array([ 7.73096944, 24.48406077,  6.17168665]), 'targetState': array([19, 24], dtype=int32), 'currentDistance': 11.279422172034364}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9296885832955905
{'scaleFactor': 20, 'currentTarget': array([19., 24.]), 'previousTarget': array([19., 24.]), 'currentState': array([19.41804756, 24.1441979 ,  0.52217128]), 'targetState': array([19, 24], dtype=int32), 'currentDistance': 0.44221803836414947}
episode index:3023
target Thresh 31.999999999998096
target distance 6.0
model initialize at round 3023
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([12.21226537, 10.30190735,  4.01422447]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 4.408870866605098}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9297052537376225
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([8.54357125, 9.06628242, 3.1706894 ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.5475975343674961}
episode index:3024
target Thresh 31.999999999998117
target distance 19.0
model initialize at round 3024
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 14.]), 'previousTarget': array([21., 14.]), 'currentState': array([ 3.62363717, 18.44293387,  5.53950829]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 17.935374166453197}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9296999023305963
{'scaleFactor': 20, 'currentTarget': array([21., 14.]), 'previousTarget': array([21., 14.]), 'currentState': array([20.75087831, 13.84641343,  0.0452038 ]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 0.29266098891112496}
episode index:3025
target Thresh 31.999999999998135
target distance 5.0
model initialize at round 3025
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 10.]), 'previousTarget': array([23., 10.]), 'currentState': array([19.46332937, 14.83129733,  5.78980822]), 'targetState': array([23, 10], dtype=int32), 'currentDistance': 5.987442949229509}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.929713319084618
{'scaleFactor': 20, 'currentTarget': array([23., 10.]), 'previousTarget': array([23., 10.]), 'currentState': array([22.84063555, 10.129533  ,  5.94466073]), 'targetState': array([23, 10], dtype=int32), 'currentDistance': 0.20536753903975802}
episode index:3026
target Thresh 31.999999999998153
target distance 7.0
model initialize at round 3026
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 18.]), 'previousTarget': array([26., 18.]), 'currentState': array([20.17964976, 21.79966031,  0.21590834]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 6.950819763992959}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9297235214932453
{'scaleFactor': 20, 'currentTarget': array([26., 18.]), 'previousTarget': array([26., 18.]), 'currentState': array([26.63128507, 17.90781773,  5.44292825]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 0.6379799455150446}
episode index:3027
target Thresh 31.99999999999817
target distance 13.0
model initialize at round 3027
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 20.]), 'previousTarget': array([18., 20.]), 'currentState': array([24.96866697,  8.3995062 ,  3.1190424 ]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 13.532692850507846}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9297242948837388
{'scaleFactor': 20, 'currentTarget': array([18., 20.]), 'previousTarget': array([18., 20.]), 'currentState': array([17.67604059, 19.66268697,  2.38674468]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 0.46768555993698246}
episode index:3028
target Thresh 31.999999999998188
target distance 8.0
model initialize at round 3028
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 19.]), 'previousTarget': array([ 6., 19.]), 'currentState': array([ 6.98209671, 25.31851793,  4.83771187]), 'targetState': array([ 6, 19], dtype=int32), 'currentDistance': 6.394386820049843}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9297376902964546
{'scaleFactor': 20, 'currentTarget': array([ 6., 19.]), 'previousTarget': array([ 6., 19.]), 'currentState': array([ 6.3494168, 19.5001738,  5.1941914]), 'targetState': array([ 6, 19], dtype=int32), 'currentDistance': 0.6101359971772482}
episode index:3029
target Thresh 31.999999999998206
target distance 15.0
model initialize at round 3029
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  4.]), 'previousTarget': array([13.,  4.]), 'currentState': array([13.3212027 , 17.02837671,  4.95931822]), 'targetState': array([13,  4], dtype=int32), 'currentDistance': 13.032335584391594}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9297384585002865
{'scaleFactor': 20, 'currentTarget': array([13.,  4.]), 'previousTarget': array([13.,  4.]), 'currentState': array([13.28065739,  3.1783773 ,  4.75905995]), 'targetState': array([13,  4], dtype=int32), 'currentDistance': 0.8682352425571052}
episode index:3030
target Thresh 31.999999999998224
target distance 18.0
model initialize at round 3030
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([ 6.95437608, 23.01547818,  4.9035776 ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 16.285695540226598}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9297361510888472
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.13658097, 7.53195909, 5.1353353 ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.5492129264733632}
episode index:3031
target Thresh 31.99999999999824
target distance 8.0
model initialize at round 3031
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 27.]), 'previousTarget': array([ 2., 27.]), 'currentState': array([ 8.20879048, 24.8341917 ,  2.97764862]), 'targetState': array([ 2, 27], dtype=int32), 'currentDistance': 6.575698048038518}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9297495293371688
{'scaleFactor': 20, 'currentTarget': array([ 2., 27.]), 'previousTarget': array([ 2., 27.]), 'currentState': array([ 2.62527548, 26.73559822,  3.3044461 ]), 'targetState': array([ 2, 27], dtype=int32), 'currentDistance': 0.6788797586996267}
episode index:3032
target Thresh 31.99999999999826
target distance 8.0
model initialize at round 3032
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'previousTarget': array([11., 19.]), 'currentState': array([17.3185717 , 17.92795681,  2.17441452]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 6.40887080146274}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.929762898763698
{'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'previousTarget': array([11., 19.]), 'currentState': array([11.77593567, 18.76506636,  3.02394006]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 0.8107218907866987}
episode index:3033
target Thresh 31.999999999998277
target distance 11.0
model initialize at round 3033
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([24.52738949, 13.3847516 ,  3.4177413 ]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 10.534418032852406}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9297698951879354
{'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([14.7361933 , 12.9722836 ,  3.80348368]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 0.7367148487453882}
episode index:3034
target Thresh 31.999999999998295
target distance 12.0
model initialize at round 3034
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([ 6.67283826, 21.0500562 ,  0.28394741]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 10.370759990790344}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9297768870016792
{'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([16.43155274, 22.18220208,  0.24964105]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 0.5969337318517697}
episode index:3035
target Thresh 31.999999999998312
target distance 10.0
model initialize at round 3035
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 16.]), 'previousTarget': array([26., 16.]), 'currentState': array([24.03479176,  7.99959361,  1.57121716]), 'targetState': array([26, 16], dtype=int32), 'currentDistance': 8.238236817234739}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9297870382279632
{'scaleFactor': 20, 'currentTarget': array([26., 16.]), 'previousTarget': array([26., 16.]), 'currentState': array([25.80678058, 15.59598861,  1.92139852]), 'targetState': array([26, 16], dtype=int32), 'currentDistance': 0.44783807575596035}
episode index:3036
target Thresh 31.99999999999833
target distance 11.0
model initialize at round 3036
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([ 6.66176286, 21.26633114,  5.43210406]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 12.926850633276421}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.92978778841225
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13.32055743,  9.16665779,  4.81616485]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.8928697008528333}
episode index:3037
target Thresh 31.999999999998344
target distance 16.0
model initialize at round 3037
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([18.92059772,  6.68109588,  2.6279937 ]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 15.903637569223335}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9297854700797996
{'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([11.95773283, 20.69972946,  2.48080163]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 0.30323078939439324}
episode index:3038
target Thresh 31.999999999998362
target distance 19.0
model initialize at round 3038
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([ 4.39543197, 27.58709037,  5.6999943 ]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 24.187133506183105}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9297711888694133
{'scaleFactor': 20, 'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([20.66875232, 10.18472777,  5.19906475]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 0.3792748070078676}
episode index:3039
target Thresh 31.999999999998376
target distance 10.0
model initialize at round 3039
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([12.43674843, 18.61651795,  5.3875829 ]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 12.147989916629612}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9297750405011671
{'scaleFactor': 20, 'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([20.91296258, 10.24387706,  5.42132436]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 0.25894310998512493}
episode index:3040
target Thresh 31.999999999998394
target distance 26.0
model initialize at round 3040
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 28.]), 'previousTarget': array([17.9916736 , 27.98556758]), 'currentState': array([3.70849121, 3.68859694, 0.44537151]), 'targetState': array([18, 28], dtype=int32), 'currentDistance': 28.200913859477968}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.9297521149221273
{'scaleFactor': 20, 'currentTarget': array([18., 28.]), 'previousTarget': array([18., 28.]), 'currentState': array([18.73404819, 28.14984721,  6.05878816]), 'targetState': array([18, 28], dtype=int32), 'currentDistance': 0.7491868479702997}
episode index:3041
target Thresh 31.999999999998412
target distance 23.0
model initialize at round 3041
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 16.]), 'previousTarget': array([25., 16.]), 'currentState': array([ 3.34237295, 10.01509751,  5.92064029]), 'targetState': array([25, 16], dtype=int32), 'currentDistance': 22.469353959960184}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9297378587606526
{'scaleFactor': 20, 'currentTarget': array([25., 16.]), 'previousTarget': array([25., 16.]), 'currentState': array([25.51540168, 15.25695807,  5.06111804]), 'targetState': array([25, 16], dtype=int32), 'currentDistance': 0.9042954108688024}
episode index:3042
target Thresh 31.999999999998426
target distance 9.0
model initialize at round 3042
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([11.7550314 , 11.50409962,  0.09556413]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 8.379571893528539}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9297479994610269
{'scaleFactor': 20, 'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([19.58609479, 12.71610415,  0.23268882]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 0.5019107257765791}
episode index:3043
target Thresh 31.99999999999844
target distance 23.0
model initialize at round 3043
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([11.65579064, 24.69876018,  5.0932225 ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 22.943788921655432}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9297337540182723
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.44743251,  1.35172551,  5.20640363]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.7876900817960165}
episode index:3044
target Thresh 31.999999999998458
target distance 23.0
model initialize at round 3044
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([14.13450263, 26.55663505,  5.18223524]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 24.26243317101443}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9297166069801709
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.24182798, 4.72339373, 3.1843993 ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.8070538059494619}
episode index:3045
target Thresh 31.999999999998472
target distance 10.0
model initialize at round 3045
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 29.]), 'previousTarget': array([20., 29.]), 'currentState': array([21.64646117, 19.34864553,  1.21867197]), 'targetState': array([20, 29], dtype=int32), 'currentDistance': 9.790785339828755}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9297235910389102
{'scaleFactor': 20, 'currentTarget': array([20., 29.]), 'previousTarget': array([20., 29.]), 'currentState': array([20.44641962, 28.95011924,  1.20244363]), 'targetState': array([20, 29], dtype=int32), 'currentDistance': 0.44919769504746904}
episode index:3046
target Thresh 31.999999999998487
target distance 13.0
model initialize at round 3046
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 18.]), 'previousTarget': array([ 8., 18.]), 'currentState': array([16.41292143,  4.08564715,  0.43564051]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 16.259965007448095}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9297182722520525
{'scaleFactor': 20, 'currentTarget': array([ 8., 18.]), 'previousTarget': array([ 8., 18.]), 'currentState': array([ 7.66815676, 18.15827991,  2.30047962]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 0.3676580800851196}
episode index:3047
target Thresh 31.999999999998504
target distance 14.0
model initialize at round 3047
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 20.]), 'previousTarget': array([ 2., 20.]), 'currentState': array([17.58593908, 16.56319209,  1.35122507]), 'targetState': array([ 2, 20], dtype=int32), 'currentDistance': 15.96036169567404}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9297159843328188
{'scaleFactor': 20, 'currentTarget': array([ 2., 20.]), 'previousTarget': array([ 2., 20.]), 'currentState': array([ 2.87444878, 19.67849002,  3.51392861]), 'targetState': array([ 2, 20], dtype=int32), 'currentDistance': 0.9316809160079598}
episode index:3048
target Thresh 31.99999999999852
target distance 17.0
model initialize at round 3048
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 22.]), 'previousTarget': array([21., 22.]), 'currentState': array([ 5.63471914, 16.15125626,  0.65555169]), 'targetState': array([21, 22], dtype=int32), 'currentDistance': 16.440792539217274}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9297136979143521
{'scaleFactor': 20, 'currentTarget': array([21., 22.]), 'previousTarget': array([21., 22.]), 'currentState': array([20.1226959 , 22.14507321,  0.66587913]), 'targetState': array([21, 22], dtype=int32), 'currentDistance': 0.8892180338456871}
episode index:3049
target Thresh 31.999999999998533
target distance 8.0
model initialize at round 3049
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([10.66514762,  9.56321012,  3.61801445]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 7.557821061066396}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9297238232625769
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.84884576, 5.71667775, 3.31391466]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.3211216276837577}
episode index:3050
target Thresh 31.999999999998547
target distance 8.0
model initialize at round 3050
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([1.44577033, 3.64556814, 1.73791361]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 7.375285654761667}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9297339419734051
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.38724177, 11.08342859,  0.31093983]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.3961269012706933}
episode index:3051
target Thresh 31.99999999999856
target distance 13.0
model initialize at round 3051
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'previousTarget': array([16., 23.]), 'currentState': array([9.50754956, 8.39538742, 6.02873802]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 15.982697559584478}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9297316519185082
{'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'previousTarget': array([16., 23.]), 'currentState': array([15.39704822, 22.39920542,  1.82901818]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 0.851178580248279}
episode index:3052
target Thresh 31.999999999998575
target distance 25.0
model initialize at round 3052
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([ 8.01180256, 27.00003601,  4.71638769]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 23.347295693402298}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9297174538247635
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.34417203, 3.51030146, 4.52679303]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.5985474443172173}
episode index:3053
target Thresh 31.99999999999859
target distance 14.0
model initialize at round 3053
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 12.]), 'previousTarget': array([ 7., 12.]), 'currentState': array([ 1.44627257, 24.41073161,  5.38712549]), 'targetState': array([ 7, 12], dtype=int32), 'currentDistance': 13.596696195032722}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9297182226178488
{'scaleFactor': 20, 'currentTarget': array([ 7., 12.]), 'previousTarget': array([ 7., 12.]), 'currentState': array([ 7.44461442, 12.02828668,  5.49295028]), 'targetState': array([ 7, 12], dtype=int32), 'currentDistance': 0.44551331938550937}
episode index:3054
target Thresh 31.999999999998604
target distance 21.0
model initialize at round 3054
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([ 5.16114081, 18.78174711,  0.20060366]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 19.918708708731867}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9297099292798425
{'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([24.72865689, 17.21960718,  0.10062278]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 0.34907649161144255}
episode index:3055
target Thresh 31.999999999998618
target distance 7.0
model initialize at round 3055
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([14.4799625 , 29.60060897,  2.89493757]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 8.95101607191065}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9297168926700979
{'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([12.04839063, 20.71732198,  4.19399586]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 0.28679001730943593}
episode index:3056
target Thresh 31.999999999998632
target distance 16.0
model initialize at round 3056
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 26.]), 'previousTarget': array([14., 26.]), 'currentState': array([21.10429217, 11.42500107,  3.14169085]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 16.214239450998953}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9297115934731117
{'scaleFactor': 20, 'currentTarget': array([14., 26.]), 'previousTarget': array([14., 26.]), 'currentState': array([14.75286246, 25.16927217,  0.77107775]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 1.121120250204261}
episode index:3057
target Thresh 31.999999999998646
target distance 17.0
model initialize at round 3057
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([14.46188578, 20.59462258,  2.90625226]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 22.384337964725372}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9297003530090128
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.98074369, 2.82774036, 4.32076448]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 1.2833597638976284}
episode index:3058
target Thresh 31.999999999998657
target distance 17.0
model initialize at round 3058
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 25.]), 'previousTarget': array([ 3., 25.]), 'currentState': array([3.22222945, 9.8237617 , 1.74339202]), 'targetState': array([ 3, 25], dtype=int32), 'currentDistance': 15.177865291313223}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9296980791748902
{'scaleFactor': 20, 'currentTarget': array([ 3., 25.]), 'previousTarget': array([ 3., 25.]), 'currentState': array([ 2.55133904, 25.4604817 ,  1.85228605]), 'targetState': array([ 3, 25], dtype=int32), 'currentDistance': 0.6429152792909044}
episode index:3059
target Thresh 31.99999999999867
target distance 6.0
model initialize at round 3059
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.49477303, 18.22666768,  4.79574877]), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.541520942955605}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9297113474496697
{'scaleFactor': 20, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([1.44392337e+01, 1.48953149e+01, 1.06966496e-02]), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.570454064453498}
episode index:3060
target Thresh 31.999999999998685
target distance 5.0
model initialize at round 3060
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 18.]), 'previousTarget': array([14., 18.]), 'currentState': array([19.89922415, 15.42259763,  2.01710759]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 6.437689686421112}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9297214371793495
{'scaleFactor': 20, 'currentTarget': array([14., 18.]), 'previousTarget': array([14., 18.]), 'currentState': array([13.89448183, 18.66631273,  1.4492136 ]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 0.6746159929354995}
episode index:3061
target Thresh 31.9999999999987
target distance 6.0
model initialize at round 3061
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([17.09151525,  3.74897192,  1.75052285]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 4.388923016528782}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9297378900084876
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.07580271,  7.36937832,  2.71429911]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.6351611992172}
episode index:3062
target Thresh 31.99999999999871
target distance 14.0
model initialize at round 3062
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  2.]), 'previousTarget': array([10.,  2.]), 'currentState': array([ 6.76552725, 14.46120053,  6.1225965 ]), 'targetState': array([10,  2], dtype=int32), 'currentDistance': 12.874134245766554}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9297386498706811
{'scaleFactor': 20, 'currentTarget': array([10.,  2.]), 'previousTarget': array([10.,  2.]), 'currentState': array([9.29031627, 2.93882248, 3.71029866]), 'targetState': array([10,  2], dtype=int32), 'currentDistance': 1.176876652953406}
episode index:3063
target Thresh 31.999999999998725
target distance 21.0
model initialize at round 3063
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 6.]), 'previousTarget': array([5., 6.]), 'currentState': array([15.31728777, 27.02945583,  4.13408947]), 'targetState': array([5, 6], dtype=int32), 'currentDistance': 23.424014162119548}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9297245004652782
{'scaleFactor': 20, 'currentTarget': array([5., 6.]), 'previousTarget': array([5., 6.]), 'currentState': array([5.13950094, 6.08616826, 4.59288236]), 'targetState': array([5, 6], dtype=int32), 'currentDistance': 0.16396792791286352}
episode index:3064
target Thresh 31.999999999998735
target distance 19.0
model initialize at round 3064
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'previousTarget': array([22., 18.]), 'currentState': array([ 4.59509241, 19.46328007,  0.6854183 ]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 17.466310334445076}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9297192126176496
{'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'previousTarget': array([22., 18.]), 'currentState': array([21.94106103, 18.13858007,  0.55983926]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 0.15059295079794774}
episode index:3065
target Thresh 31.99999999999875
target distance 14.0
model initialize at round 3065
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 17.]), 'previousTarget': array([27., 17.]), 'currentState': array([14.46330217, 11.85521631,  1.51875442]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 13.551294837666925}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9297169378237193
{'scaleFactor': 20, 'currentTarget': array([27., 17.]), 'previousTarget': array([27., 17.]), 'currentState': array([27.23669957, 16.18623047,  0.88756495]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 0.8474948610874004}
episode index:3066
target Thresh 31.99999999999876
target distance 9.0
model initialize at round 3066
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([6.25247685, 8.49215478, 5.26214242]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 11.196006726412689}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9297207732366888
{'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([15.80286124, 14.04721504,  1.40108775]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 0.20271396101651087}
episode index:3067
target Thresh 31.999999999998774
target distance 21.0
model initialize at round 3067
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  8.]), 'previousTarget': array([10.,  8.]), 'currentState': array([19.45477921, 29.66684391,  3.74418509]), 'targetState': array([10,  8], dtype=int32), 'currentDistance': 23.639902174116845}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9297008987384919
{'scaleFactor': 20, 'currentTarget': array([10.,  8.]), 'previousTarget': array([10.,  8.]), 'currentState': array([9.42807476, 8.32628812, 3.11787844]), 'targetState': array([10,  8], dtype=int32), 'currentDistance': 0.658454568324339}
episode index:3068
target Thresh 31.999999999998785
target distance 21.0
model initialize at round 3068
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 23.]), 'previousTarget': array([ 2., 23.]), 'currentState': array([21.0473847 , 14.43042115,  2.88520303]), 'targetState': array([ 2, 23], dtype=int32), 'currentDistance': 20.88637224369843}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9296897020475567
{'scaleFactor': 20, 'currentTarget': array([ 2., 23.]), 'previousTarget': array([ 2., 23.]), 'currentState': array([ 1.68534503, 23.11679777,  3.17340281]), 'targetState': array([ 2, 23], dtype=int32), 'currentDistance': 0.3356329404862946}
episode index:3069
target Thresh 31.9999999999988
target distance 13.0
model initialize at round 3069
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 17.]), 'previousTarget': array([ 6., 17.]), 'currentState': array([17.33378057, 16.23685653,  4.01038659]), 'targetState': array([ 6, 17], dtype=int32), 'currentDistance': 11.359444086518359}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9296935425841539
{'scaleFactor': 20, 'currentTarget': array([ 6., 17.]), 'previousTarget': array([ 6., 17.]), 'currentState': array([ 5.82730617, 16.91365093,  3.64553514]), 'targetState': array([ 6, 17], dtype=int32), 'currentDistance': 0.19307853775316744}
episode index:3070
target Thresh 31.99999999999881
target distance 13.0
model initialize at round 3070
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([15.79836348, 20.4595397 ,  4.42768395]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 14.447568997339443}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9296943149076066
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([7.55009873, 9.5457364 , 4.55275155]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.7748785885109575}
episode index:3071
target Thresh 31.99999999999882
target distance 8.0
model initialize at round 3071
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 18.]), 'previousTarget': array([10., 18.]), 'currentState': array([ 3.31403241, 14.47293994,  0.5590986 ]), 'targetState': array([10, 18], dtype=int32), 'currentDistance': 7.559253623021062}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9297043740531444
{'scaleFactor': 20, 'currentTarget': array([10., 18.]), 'previousTarget': array([10., 18.]), 'currentState': array([10.15753527, 18.38979461,  1.00907326]), 'targetState': array([10, 18], dtype=int32), 'currentDistance': 0.4204250169315112}
episode index:3072
target Thresh 31.999999999998835
target distance 22.0
model initialize at round 3072
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 24.]), 'previousTarget': array([10., 24.]), 'currentState': array([8.6641216 , 3.02362931, 1.47776651]), 'targetState': array([10, 24], dtype=int32), 'currentDistance': 21.01886529411048}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9296931908055706
{'scaleFactor': 20, 'currentTarget': array([10., 24.]), 'previousTarget': array([10., 24.]), 'currentState': array([ 9.6217558 , 24.65432879,  1.65042095]), 'targetState': array([10, 24], dtype=int32), 'currentDistance': 0.7557875648479929}
episode index:3073
target Thresh 31.999999999998845
target distance 18.0
model initialize at round 3073
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 24.]), 'previousTarget': array([ 4., 24.]), 'currentState': array([20.21548183, 12.62177768,  2.22049236]), 'targetState': array([ 4, 24], dtype=int32), 'currentDistance': 19.809235074610186}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9296849568706985
{'scaleFactor': 20, 'currentTarget': array([ 4., 24.]), 'previousTarget': array([ 4., 24.]), 'currentState': array([ 4.13846898, 23.61758417,  3.08424003]), 'targetState': array([ 4, 24], dtype=int32), 'currentDistance': 0.40671307929269673}
episode index:3074
target Thresh 31.999999999998856
target distance 13.0
model initialize at round 3074
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 20.]), 'previousTarget': array([ 7., 20.]), 'currentState': array([18.5768873 , 15.10159122,  2.69470835]), 'targetState': array([ 7, 20], dtype=int32), 'currentDistance': 12.570550037978187}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9296887927056676
{'scaleFactor': 20, 'currentTarget': array([ 7., 20.]), 'previousTarget': array([ 7., 20.]), 'currentState': array([ 7.80617787, 19.58000514,  3.43460163]), 'targetState': array([ 7, 20], dtype=int32), 'currentDistance': 0.9090205928460959}
episode index:3075
target Thresh 31.999999999998867
target distance 9.0
model initialize at round 3075
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 19.]), 'previousTarget': array([ 8., 19.]), 'currentState': array([15.05010878, 15.17331363,  2.69145906]), 'targetState': array([ 8, 19], dtype=int32), 'currentDistance': 8.021693236928742}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9296988405656462
{'scaleFactor': 20, 'currentTarget': array([ 8., 19.]), 'previousTarget': array([ 8., 19.]), 'currentState': array([ 8.06506838, 18.87592149,  3.11260181]), 'targetState': array([ 8, 19], dtype=int32), 'currentDistance': 0.14010485352291274}
episode index:3076
target Thresh 31.99999999999888
target distance 13.0
model initialize at round 3076
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 25.]), 'previousTarget': array([23., 25.]), 'currentState': array([24.60378429, 11.4898392 ,  0.7020232 ]), 'targetState': array([23, 25], dtype=int32), 'currentDistance': 13.605019990740645}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9296996096613048
{'scaleFactor': 20, 'currentTarget': array([23., 25.]), 'previousTarget': array([23., 25.]), 'currentState': array([23.03755521, 24.47593986,  2.46423624]), 'targetState': array([23, 25], dtype=int32), 'currentDistance': 0.5254040535005321}
episode index:3077
target Thresh 31.99999999999889
target distance 15.0
model initialize at round 3077
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 18.]), 'previousTarget': array([11., 18.]), 'currentState': array([24.76960871, 14.8517294 ,  2.88248062]), 'targetState': array([11, 18], dtype=int32), 'currentDistance': 14.124932985651787}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9297003782572262
{'scaleFactor': 20, 'currentTarget': array([11., 18.]), 'previousTarget': array([11., 18.]), 'currentState': array([11.4334402 , 17.93480202,  3.58366982]), 'targetState': array([11, 18], dtype=int32), 'currentDistance': 0.43831630772541685}
episode index:3078
target Thresh 31.999999999998902
target distance 10.0
model initialize at round 3078
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  6.]), 'previousTarget': array([17.,  6.]), 'currentState': array([25.36357937,  2.60691525,  2.36733627]), 'targetState': array([17,  6], dtype=int32), 'currentDistance': 9.02565698642994}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9297072927332388
{'scaleFactor': 20, 'currentTarget': array([17.,  6.]), 'previousTarget': array([17.,  6.]), 'currentState': array([16.09816631,  5.82077003,  3.03916774]), 'targetState': array([17,  6], dtype=int32), 'currentDistance': 0.9194712595388079}
episode index:3079
target Thresh 31.999999999998913
target distance 9.0
model initialize at round 3079
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 13.]), 'previousTarget': array([10., 13.]), 'currentState': array([17.48494473, 16.69459946,  3.83252383]), 'targetState': array([10, 13], dtype=int32), 'currentDistance': 8.347123020971011}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.929717321537546
{'scaleFactor': 20, 'currentTarget': array([10., 13.]), 'previousTarget': array([10., 13.]), 'currentState': array([10.58927033, 13.29394608,  4.43605703]), 'targetState': array([10, 13], dtype=int32), 'currentDistance': 0.6585163801190778}
episode index:3080
target Thresh 31.999999999998924
target distance 15.0
model initialize at round 3080
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 23.]), 'previousTarget': array([24., 23.]), 'currentState': array([ 9.56622633, 16.4151417 ,  6.06552792]), 'targetState': array([24, 23], dtype=int32), 'currentDistance': 15.864872557605812}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9297120634804042
{'scaleFactor': 20, 'currentTarget': array([24., 23.]), 'previousTarget': array([24., 23.]), 'currentState': array([23.62899587, 22.42135809,  0.997482  ]), 'targetState': array([24, 23], dtype=int32), 'currentDistance': 0.6873649157677982}
episode index:3081
target Thresh 31.999999999998934
target distance 5.0
model initialize at round 3081
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([15.63694622, 20.39089299,  1.2444033 ]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 7.554780510005959}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9297220842287882
{'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([ 9.29411838, 23.73517217,  3.5808396 ]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 0.3957769568970984}
episode index:3082
target Thresh 31.999999999998945
target distance 11.0
model initialize at round 3082
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 12.]), 'previousTarget': array([ 8., 12.]), 'currentState': array([17.41101462, 12.86198305,  3.3885259 ]), 'targetState': array([ 8, 12], dtype=int32), 'currentDistance': 9.450407977747572}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9297289826931643
{'scaleFactor': 20, 'currentTarget': array([ 8., 12.]), 'previousTarget': array([ 8., 12.]), 'currentState': array([ 7.69305563, 11.62249876,  3.93417189]), 'targetState': array([ 8, 12], dtype=int32), 'currentDistance': 0.4865408890714942}
episode index:3083
target Thresh 31.999999999998956
target distance 9.0
model initialize at round 3083
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 19.]), 'previousTarget': array([ 2., 19.]), 'currentState': array([ 9.7099286 , 19.08078855,  3.45423925]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 7.7103518645833455}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9297389914568823
{'scaleFactor': 20, 'currentTarget': array([ 2., 19.]), 'previousTarget': array([ 2., 19.]), 'currentState': array([ 2.05041789, 19.00239485,  4.00508554]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 0.050474736751983384}
episode index:3084
target Thresh 31.999999999998966
target distance 19.0
model initialize at round 3084
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  8.]), 'previousTarget': array([20.,  8.]), 'currentState': array([25.40713273, 26.45671192,  4.48029494]), 'targetState': array([20,  8], dtype=int32), 'currentDistance': 19.232454322728685}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9297307720350192
{'scaleFactor': 20, 'currentTarget': array([20.,  8.]), 'previousTarget': array([20.,  8.]), 'currentState': array([20.23744522,  7.31401787,  4.56635228]), 'targetState': array([20,  8], dtype=int32), 'currentDistance': 0.7259143990132507}
episode index:3085
target Thresh 31.999999999998977
target distance 13.0
model initialize at round 3085
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 19.]), 'previousTarget': array([14., 19.]), 'currentState': array([5.87343441, 7.79919377, 1.12213612]), 'targetState': array([14, 19], dtype=int32), 'currentDistance': 13.83832101738667}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9297315285404865
{'scaleFactor': 20, 'currentTarget': array([14., 19.]), 'previousTarget': array([14., 19.]), 'currentState': array([13.73081434, 19.02674032,  0.855363  ]), 'targetState': array([14, 19], dtype=int32), 'currentDistance': 0.27051056429437276}
episode index:3086
target Thresh 31.999999999998987
target distance 3.0
model initialize at round 3086
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 18.]), 'previousTarget': array([ 8., 18.]), 'currentState': array([ 5.8884185 , 14.57060946,  6.27846563]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 4.0273435343608535}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9297478448577717
{'scaleFactor': 20, 'currentTarget': array([ 8., 18.]), 'previousTarget': array([ 8., 18.]), 'currentState': array([ 7.6322526 , 17.68717636,  0.60553163]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 0.48280097151384926}
episode index:3087
target Thresh 31.999999999998998
target distance 24.0
model initialize at round 3087
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  2.]), 'previousTarget': array([19.,  2.]), 'currentState': array([25.47804822, 27.59998576,  2.89613378]), 'targetState': array([19,  2], dtype=int32), 'currentDistance': 26.406900223748263}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9297280903137015
{'scaleFactor': 20, 'currentTarget': array([19.,  2.]), 'previousTarget': array([19.,  2.]), 'currentState': array([18.91576173,  1.84820224,  5.06401469]), 'targetState': array([19,  2], dtype=int32), 'currentDistance': 0.17360486025641653}
episode index:3088
target Thresh 31.999999999999005
target distance 25.0
model initialize at round 3088
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([11.24563255, 27.49556727,  5.25759816]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 23.602638558246625}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9297140588411869
{'scaleFactor': 20, 'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([9.4704151 , 4.63396868, 3.82080555]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 0.7894343879862045}
episode index:3089
target Thresh 31.999999999999016
target distance 11.0
model initialize at round 3089
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  3.]), 'previousTarget': array([26.,  3.]), 'currentState': array([19.59409822, 12.36671134,  5.47880554]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 11.347724837158552}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9297178666374845
{'scaleFactor': 20, 'currentTarget': array([26.,  3.]), 'previousTarget': array([26.,  3.]), 'currentState': array([26.08330007,  2.94335147,  6.00081989]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 0.10073706764142848}
episode index:3090
target Thresh 31.999999999999027
target distance 18.0
model initialize at round 3090
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([19.6067048 , 16.05601029,  4.74707365]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 19.950388664690134}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9297096700047998
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.55447422, 5.09153031, 4.13129885]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.5619781631642795}
episode index:3091
target Thresh 31.999999999999037
target distance 12.0
model initialize at round 3091
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  4.]), 'previousTarget': array([20.,  4.]), 'currentState': array([9.19983512, 4.81983739, 0.23286742]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 10.831236997021795}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9297134767575151
{'scaleFactor': 20, 'currentTarget': array([20.,  4.]), 'previousTarget': array([20.,  4.]), 'currentState': array([20.85281699,  4.31059073,  0.47401808]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 0.9076141361208806}
episode index:3092
target Thresh 31.999999999999044
target distance 17.0
model initialize at round 3092
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  2.]), 'previousTarget': array([22.,  2.]), 'currentState': array([15.36228223, 17.05297478,  5.14136851]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 16.451484640432852}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.929711223675611
{'scaleFactor': 20, 'currentTarget': array([22.,  2.]), 'previousTarget': array([22.,  2.]), 'currentState': array([21.87048048,  2.79289099,  5.57186836]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 0.8033999168301781}
episode index:3093
target Thresh 31.999999999999055
target distance 14.0
model initialize at round 3093
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([17.90096081, 14.38936922,  3.72043651]), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 15.956034728465392}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9297089720501269
{'scaleFactor': 20, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.22611428, 5.02134856, 4.41631528]), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.2271198545281952}
episode index:3094
target Thresh 31.999999999999066
target distance 9.0
model initialize at round 3094
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 17.]), 'previousTarget': array([18., 17.]), 'currentState': array([18.96282711, 24.23328578,  5.55106759]), 'targetState': array([18, 17], dtype=int32), 'currentDistance': 7.297085668522759}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9297189517069765
{'scaleFactor': 20, 'currentTarget': array([18., 17.]), 'previousTarget': array([18., 17.]), 'currentState': array([18.28684545, 16.66583169,  4.55212519]), 'targetState': array([18, 17], dtype=int32), 'currentDistance': 0.4403961568953058}
episode index:3095
target Thresh 31.999999999999073
target distance 5.0
model initialize at round 3095
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 19.]), 'previousTarget': array([20., 19.]), 'currentState': array([17.1776356 , 22.42475621,  5.67401361]), 'targetState': array([20, 19], dtype=int32), 'currentDistance': 4.437870647778939}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9297352246553916
{'scaleFactor': 20, 'currentTarget': array([20., 19.]), 'previousTarget': array([20., 19.]), 'currentState': array([19.79671133, 19.43059085,  5.50899738]), 'targetState': array([20, 19], dtype=int32), 'currentDistance': 0.47616673714326374}
episode index:3096
target Thresh 31.999999999999083
target distance 14.0
model initialize at round 3096
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 27.]), 'previousTarget': array([21., 27.]), 'currentState': array([ 5.97355867, 15.33371896,  1.21672916]), 'targetState': array([21, 27], dtype=int32), 'currentDistance': 19.023565710087222}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9297270382977402
{'scaleFactor': 20, 'currentTarget': array([21., 27.]), 'previousTarget': array([21., 27.]), 'currentState': array([20.9813985 , 27.08415001,  0.98923875]), 'targetState': array([21, 27], dtype=int32), 'currentDistance': 0.08618143348505572}
episode index:3097
target Thresh 31.99999999999909
target distance 7.0
model initialize at round 3097
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 20.]), 'previousTarget': array([15., 20.]), 'currentState': array([ 8.8725252 , 14.43912747,  0.01576638]), 'targetState': array([15, 20], dtype=int32), 'currentDistance': 8.2746148422309}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9297370024590385
{'scaleFactor': 20, 'currentTarget': array([15., 20.]), 'previousTarget': array([15., 20.]), 'currentState': array([14.27481604, 19.43664525,  1.15821654]), 'targetState': array([15, 20], dtype=int32), 'currentDistance': 0.9182920794374269}
episode index:3098
target Thresh 31.9999999999991
target distance 5.0
model initialize at round 3098
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 24.]), 'previousTarget': array([10., 24.]), 'currentState': array([13.61486278, 27.63455296,  4.33180207]), 'targetState': array([10, 24], dtype=int32), 'currentDistance': 5.126129933052319}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9297500911965477
{'scaleFactor': 20, 'currentTarget': array([10., 24.]), 'previousTarget': array([10., 24.]), 'currentState': array([ 9.63704204, 23.47303517,  4.67421237]), 'targetState': array([10, 24], dtype=int32), 'currentDistance': 0.6398674961658981}
episode index:3099
target Thresh 31.99999999999911
target distance 7.0
model initialize at round 3099
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 25.]), 'previousTarget': array([ 5., 25.]), 'currentState': array([10.62498686, 18.02957383,  2.74617243]), 'targetState': array([ 5, 25], dtype=int32), 'currentDistance': 8.956970362391477}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9297600414929359
{'scaleFactor': 20, 'currentTarget': array([ 5., 25.]), 'previousTarget': array([ 5., 25.]), 'currentState': array([ 5.6772709 , 24.08116683,  2.42554016]), 'targetState': array([ 5, 25], dtype=int32), 'currentDistance': 1.1414684663777508}
episode index:3100
target Thresh 31.99999999999912
target distance 11.0
model initialize at round 3100
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 20.]), 'previousTarget': array([ 4., 20.]), 'currentState': array([9.68287411, 8.98203299, 0.99932402]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 12.397203525854463}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9297607849003574
{'scaleFactor': 20, 'currentTarget': array([ 4., 20.]), 'previousTarget': array([ 4., 20.]), 'currentState': array([ 3.30999575, 20.56686295,  2.00512664]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 0.8929946647579593}
episode index:3101
target Thresh 31.999999999999126
target distance 4.0
model initialize at round 3101
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([19.62599263, 14.56579263,  5.01223308]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 3.821357524987763}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9297770128871723
{'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([21.13741897, 11.120682  ,  5.98945097]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 0.18288826854162882}
episode index:3102
target Thresh 31.999999999999137
target distance 10.0
model initialize at round 3102
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 17.]), 'previousTarget': array([ 7., 17.]), 'currentState': array([2.66226909, 8.74226196, 1.85974461]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 9.327708565430559}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9297838491865642
{'scaleFactor': 20, 'currentTarget': array([ 7., 17.]), 'previousTarget': array([ 7., 17.]), 'currentState': array([ 6.88047823, 17.45186689,  1.44575607]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 0.46740682469804956}
episode index:3103
target Thresh 31.999999999999144
target distance 23.0
model initialize at round 3103
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  4.]), 'previousTarget': array([27.,  4.]), 'currentState': array([20.22184488, 25.50773237,  5.24172258]), 'targetState': array([27,  4], dtype=int32), 'currentDistance': 22.550519692833124}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9297727520232497
{'scaleFactor': 20, 'currentTarget': array([27.,  4.]), 'previousTarget': array([27.,  4.]), 'currentState': array([26.6944809 ,  4.79292856,  5.54401359]), 'targetState': array([27,  4], dtype=int32), 'currentDistance': 0.8497515052925539}
episode index:3104
target Thresh 31.999999999999154
target distance 12.0
model initialize at round 3104
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 15.]), 'previousTarget': array([ 5., 15.]), 'currentState': array([13.6820793 ,  2.94525237,  0.9774639 ]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 14.855821799429535}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9297704885586457
{'scaleFactor': 20, 'currentTarget': array([ 5., 15.]), 'previousTarget': array([ 5., 15.]), 'currentState': array([ 4.58144864, 15.20352919,  2.4026226 ]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 0.4654131172164626}
episode index:3105
target Thresh 31.99999999999916
target distance 4.0
model initialize at round 3105
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  7.]), 'previousTarget': array([24.,  7.]), 'currentState': array([24.25423545,  4.71200453,  1.33802617]), 'targetState': array([24,  7], dtype=int32), 'currentDistance': 2.302077088195296}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9297898799016725
{'scaleFactor': 20, 'currentTarget': array([24.,  7.]), 'previousTarget': array([24.,  7.]), 'currentState': array([23.88899008,  6.61724775,  2.1909101 ]), 'targetState': array([24,  7], dtype=int32), 'currentDistance': 0.3985253866049542}
episode index:3106
target Thresh 31.99999999999917
target distance 11.0
model initialize at round 3106
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  6.]), 'previousTarget': array([26.,  6.]), 'currentState': array([14.82267373,  6.32639805,  5.61682796]), 'targetState': array([26,  6], dtype=int32), 'currentDistance': 11.182090957431992}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9297936424602496
{'scaleFactor': 20, 'currentTarget': array([26.,  6.]), 'previousTarget': array([26.,  6.]), 'currentState': array([26.2329575 ,  6.2770942 ,  0.24798272]), 'targetState': array([26,  6], dtype=int32), 'currentDistance': 0.36200882694340886}
episode index:3107
target Thresh 31.99999999999918
target distance 18.0
model initialize at round 3107
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 21.]), 'previousTarget': array([23., 21.]), 'currentState': array([ 4.92673942, 25.31862527,  5.67884469]), 'targetState': array([23, 21], dtype=int32), 'currentDistance': 18.58206856532191}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9297884055249289
{'scaleFactor': 20, 'currentTarget': array([23., 21.]), 'previousTarget': array([23., 21.]), 'currentState': array([22.10878774, 21.39634888,  0.3908174 ]), 'targetState': array([23, 21], dtype=int32), 'currentDistance': 0.9753726090249062}
episode index:3108
target Thresh 31.999999999999186
target distance 5.0
model initialize at round 3108
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([8.09020716, 9.27515735, 3.83465004]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 5.2398902892891535}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9298014356292953
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.44131361, 6.09089973, 2.70384556]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.5660328992011431}
episode index:3109
target Thresh 31.999999999999194
target distance 16.0
model initialize at round 3109
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([14.50227004, 26.24133484,  4.80553574]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 17.45241769914511}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9297961995559365
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.18228288, 10.72416825,  4.61276583]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.3306209311790019}
episode index:3110
target Thresh 31.999999999999204
target distance 13.0
model initialize at round 3110
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  2.]), 'previousTarget': array([17.,  2.]), 'currentState': array([18.93838539, 16.30589529,  3.2633779 ]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 14.436619336611066}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9297939329197655
{'scaleFactor': 20, 'currentTarget': array([17.,  2.]), 'previousTarget': array([17.,  2.]), 'currentState': array([16.99936889,  1.21308846,  4.37950498]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 0.7869117940331397}
episode index:3111
target Thresh 31.99999999999921
target distance 8.0
model initialize at round 3111
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([17.56817698, 11.62662748,  2.84028244]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 8.011022081470944}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9298038307594442
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.57470118,  9.0327668 ,  3.94301438]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.5756345316396201}
episode index:3112
target Thresh 31.99999999999922
target distance 7.0
model initialize at round 3112
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 19.]), 'previousTarget': array([16., 19.]), 'currentState': array([19.81153342, 13.46967974,  2.83569026]), 'targetState': array([16, 19], dtype=int32), 'currentDistance': 6.716563788396194}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9298168391658819
{'scaleFactor': 20, 'currentTarget': array([16., 19.]), 'previousTarget': array([16., 19.]), 'currentState': array([16.43492965, 18.09932359,  2.6869317 ]), 'targetState': array([16, 19], dtype=int32), 'currentDistance': 1.0001908824941008}
episode index:3113
target Thresh 31.999999999999226
target distance 24.0
model initialize at round 3113
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  4.]), 'previousTarget': array([23.,  4.]), 'currentState': array([11.61801629, 26.43461061,  6.09840488]), 'targetState': array([23,  4], dtype=int32), 'currentDistance': 25.15673480979626}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9298000453906197
{'scaleFactor': 20, 'currentTarget': array([23.,  4.]), 'previousTarget': array([23.,  4.]), 'currentState': array([23.52704799,  3.98373298,  5.26982163]), 'targetState': array([23,  4], dtype=int32), 'currentDistance': 0.527298966876357}
episode index:3114
target Thresh 31.999999999999233
target distance 11.0
model initialize at round 3114
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([ 4.5316454 , 11.69746   ,  5.70049304]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 13.553245708678292}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9298007726145415
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.46725705,  2.10674449,  5.67479025]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.47929483331102596}
episode index:3115
target Thresh 31.99999999999924
target distance 13.0
model initialize at round 3115
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([10.74695505, 14.0316365 ,  5.86637352]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 11.65426275903042}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9298045208099158
{'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([22.06303817, 10.99232286,  5.35642303]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 0.06350393188914595}
episode index:3116
target Thresh 31.99999999999925
target distance 7.0
model initialize at round 3116
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 12.]), 'previousTarget': array([14., 12.]), 'currentState': array([ 8.4213351 , 11.0987816 ,  0.44490879]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 5.650990767635215}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.929817512301475
{'scaleFactor': 20, 'currentTarget': array([14., 12.]), 'previousTarget': array([14., 12.]), 'currentState': array([14.14827343, 12.46364707,  0.72288183]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 0.4867788157529498}
episode index:3117
target Thresh 31.999999999999257
target distance 18.0
model initialize at round 3117
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 10.]), 'previousTarget': array([27., 10.]), 'currentState': array([24.40549695, 26.36659707,  5.96570027]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 16.570966950418374}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9298122845064725
{'scaleFactor': 20, 'currentTarget': array([27., 10.]), 'previousTarget': array([27., 10.]), 'currentState': array([27.3696077 ,  9.68423255,  4.95516549]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 0.48612646012659577}
episode index:3118
target Thresh 31.999999999999265
target distance 4.0
model initialize at round 3118
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 12.]), 'previousTarget': array([23., 12.]), 'currentState': array([27.98155397, 11.36709185,  1.95810026]), 'targetState': array([23, 12], dtype=int32), 'currentDistance': 5.021598620362382}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9298252651783203
{'scaleFactor': 20, 'currentTarget': array([23., 12.]), 'previousTarget': array([23., 12.]), 'currentState': array([22.45964188, 11.81360061,  3.0666823 ]), 'targetState': array([23, 12], dtype=int32), 'currentDistance': 0.5716044320480403}
episode index:3119
target Thresh 31.99999999999927
target distance 18.0
model initialize at round 3119
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 24.]), 'previousTarget': array([27., 24.]), 'currentState': array([18.93630602,  7.90092168,  1.05248845]), 'targetState': array([27, 24], dtype=int32), 'currentDistance': 18.005651433550327}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.929820038249572
{'scaleFactor': 20, 'currentTarget': array([27., 24.]), 'previousTarget': array([27., 24.]), 'currentState': array([26.79849855, 23.66814014,  1.73611964]), 'targetState': array([27, 24], dtype=int32), 'currentDistance': 0.3882445136405608}
episode index:3120
target Thresh 31.99999999999928
target distance 23.0
model initialize at round 3120
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([25.32613616, 23.17483687,  4.04751897]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 23.215979737868032}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9298061211824353
{'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([ 3.7523656 , 13.67344555,  4.01657761]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 0.4098299707471086}
episode index:3121
target Thresh 31.999999999999286
target distance 13.0
model initialize at round 3121
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 28.]), 'previousTarget': array([ 8., 28.]), 'currentState': array([ 6.27938818, 13.9065861 ,  0.30281943]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 14.198056926027535}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.929803859354519
{'scaleFactor': 20, 'currentTarget': array([ 8., 28.]), 'previousTarget': array([ 8., 28.]), 'currentState': array([ 7.55866839, 28.91813085,  1.26980004]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 1.0186941904361917}
episode index:3122
target Thresh 31.999999999999293
target distance 5.0
model initialize at round 3122
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 18.]), 'previousTarget': array([11., 18.]), 'currentState': array([14.52860153, 14.816959  ,  1.62476945]), 'targetState': array([11, 18], dtype=int32), 'currentDistance': 4.752134130762565}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9298168260982416
{'scaleFactor': 20, 'currentTarget': array([11., 18.]), 'previousTarget': array([11., 18.]), 'currentState': array([10.19598526, 18.39784213,  2.65425003]), 'targetState': array([11, 18], dtype=int32), 'currentDistance': 0.8970607925228427}
episode index:3123
target Thresh 31.9999999999993
target distance 10.0
model initialize at round 3123
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 15.]), 'previousTarget': array([ 6., 15.]), 'currentState': array([9.31750929, 6.5891885 , 2.90573013]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 9.041438936549365}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9298236036986903
{'scaleFactor': 20, 'currentTarget': array([ 6., 15.]), 'previousTarget': array([ 6., 15.]), 'currentState': array([ 5.90182066, 15.26002125,  2.56470338]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 0.27793926129027596}
episode index:3124
target Thresh 31.999999999999307
target distance 17.0
model initialize at round 3124
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 23.]), 'previousTarget': array([23., 23.]), 'currentState': array([ 7.68296932, 13.00153185,  1.01091021]), 'targetState': array([23, 23], dtype=int32), 'currentDistance': 18.291549799211765}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9298183856647015
{'scaleFactor': 20, 'currentTarget': array([23., 23.]), 'previousTarget': array([23., 23.]), 'currentState': array([22.4014255 , 22.79620626,  0.53324252]), 'targetState': array([23, 23], dtype=int32), 'currentDistance': 0.6323158403498299}
episode index:3125
target Thresh 31.999999999999314
target distance 11.0
model initialize at round 3125
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([12.57522753, 11.0926142 ,  5.15952715]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 9.410374832493476}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9298251584299719
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.39879875,  1.77817864,  5.35002352]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.4563388611345763}
episode index:3126
target Thresh 31.99999999999932
target distance 4.0
model initialize at round 3126
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 24.]), 'previousTarget': array([12., 24.]), 'currentState': array([11.32167137, 20.12490437,  2.05730772]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 3.9340177807309646}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9298412360895723
{'scaleFactor': 20, 'currentTarget': array([12., 24.]), 'previousTarget': array([12., 24.]), 'currentState': array([11.70874776, 23.9300602 ,  1.03962144]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 0.29953203990665445}
episode index:3127
target Thresh 31.99999999999933
target distance 5.0
model initialize at round 3127
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 23.]), 'previousTarget': array([18., 23.]), 'currentState': array([14.22065508, 18.84138476,  0.25067108]), 'targetState': array([18, 23], dtype=int32), 'currentDistance': 5.619388642234017}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9298541701573186
{'scaleFactor': 20, 'currentTarget': array([18., 23.]), 'previousTarget': array([18., 23.]), 'currentState': array([17.75935609, 23.23279484,  1.10300707]), 'targetState': array([18, 23], dtype=int32), 'currentDistance': 0.33481775754961857}
episode index:3128
target Thresh 31.999999999999336
target distance 19.0
model initialize at round 3128
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 23.]), 'previousTarget': array([ 2., 23.]), 'currentState': array([22.58412898, 13.56826354,  1.35442465]), 'targetState': array([ 2, 23], dtype=int32), 'currentDistance': 22.64208512389128}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9298402777640807
{'scaleFactor': 20, 'currentTarget': array([ 2., 23.]), 'previousTarget': array([ 2., 23.]), 'currentState': array([ 1.26791707, 22.91972139,  2.79661615]), 'targetState': array([ 2, 23], dtype=int32), 'currentDistance': 0.736471363656194}
episode index:3129
target Thresh 31.99999999999934
target distance 11.0
model initialize at round 3129
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 25.]), 'previousTarget': array([17., 25.]), 'currentState': array([12.42276146, 14.89896492,  1.57350653]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 11.08972598468645}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9298439965729104
{'scaleFactor': 20, 'currentTarget': array([17., 25.]), 'previousTarget': array([17., 25.]), 'currentState': array([16.93700091, 25.5325819 ,  1.5473707 ]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 0.5362950400924245}
episode index:3130
target Thresh 31.999999999999346
target distance 3.0
model initialize at round 3130
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 28.]), 'previousTarget': array([22., 28.]), 'currentState': array([23.25959478, 25.91517541,  2.33578175]), 'targetState': array([22, 28], dtype=int32), 'currentDistance': 2.4357899320520717}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9298632096049854
{'scaleFactor': 20, 'currentTarget': array([22., 28.]), 'previousTarget': array([22., 28.]), 'currentState': array([22.13344087, 27.55695219,  2.00495178]), 'targetState': array([22, 28], dtype=int32), 'currentDistance': 0.4627070645093956}
episode index:3131
target Thresh 31.999999999999353
target distance 5.0
model initialize at round 3131
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([22.4917998 , 14.3111382 ,  5.28637457]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 3.347462209680649}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9298792494486621
{'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([22.19665618, 10.47575394,  4.83337051]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 0.5599174851824875}
episode index:3132
target Thresh 31.99999999999936
target distance 15.0
model initialize at round 3132
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([17.69058001, 12.9427359 ,  2.81084728]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 14.555501337397432}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9298769722207588
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.26311748, 7.63706864, 3.16471571]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.8214103863415754}
episode index:3133
target Thresh 31.999999999999368
target distance 9.0
model initialize at round 3133
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 12.]), 'previousTarget': array([17., 12.]), 'currentState': array([ 7.03546375, 17.62084885,  5.1120863 ]), 'targetState': array([17, 12], dtype=int32), 'currentDistance': 11.4405386442234}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9298806745746772
{'scaleFactor': 20, 'currentTarget': array([17., 12.]), 'previousTarget': array([17., 12.]), 'currentState': array([17.19857769, 11.99544748,  5.80697697]), 'targetState': array([17, 12], dtype=int32), 'currentDistance': 0.19862987028867174}
episode index:3134
target Thresh 31.99999999999937
target distance 6.0
model initialize at round 3134
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 16.]), 'previousTarget': array([21., 16.]), 'currentState': array([14.984959  , 14.68290281,  0.56973362]), 'targetState': array([21, 16], dtype=int32), 'currentDistance': 6.157553345286151}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9298935671824683
{'scaleFactor': 20, 'currentTarget': array([21., 16.]), 'previousTarget': array([21., 16.]), 'currentState': array([20.66577991, 15.99957748,  0.76704034]), 'targetState': array([21, 16], dtype=int32), 'currentDistance': 0.33422035395101946}
episode index:3135
target Thresh 31.99999999999938
target distance 16.0
model initialize at round 3135
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([ 1.38020843, 26.45679684,  3.87672138]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 18.57759319889369}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9298854321403212
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.63138638,  9.54694943,  5.53812356]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.7771123337778008}
episode index:3136
target Thresh 31.999999999999385
target distance 13.0
model initialize at round 3136
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([10.73624691, 15.28056088,  0.22033947]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 12.049702694808317}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9298891282567574
{'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([21.67129757, 11.31198169,  0.3014699 ]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 0.453186349067647}
episode index:3137
target Thresh 31.999999999999392
target distance 22.0
model initialize at round 3137
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 29.]), 'previousTarget': array([18., 29.]), 'currentState': array([11.0396907 ,  8.89462598,  0.97852051]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 21.276089160469237}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9298781177806585
{'scaleFactor': 20, 'currentTarget': array([18., 29.]), 'previousTarget': array([18., 29.]), 'currentState': array([17.50932379, 28.21090224,  1.50835372]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 0.929213874070261}
episode index:3138
target Thresh 31.999999999999396
target distance 16.0
model initialize at round 3138
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 17.]), 'previousTarget': array([ 6., 17.]), 'currentState': array([20.06914978, 13.51896341,  2.83533034]), 'targetState': array([ 6, 17], dtype=int32), 'currentDistance': 14.493398197423362}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9298788145726709
{'scaleFactor': 20, 'currentTarget': array([ 6., 17.]), 'previousTarget': array([ 6., 17.]), 'currentState': array([ 6.57644298, 16.7224738 ,  3.48207117]), 'targetState': array([ 6, 17], dtype=int32), 'currentDistance': 0.6397712861031957}
episode index:3139
target Thresh 31.999999999999403
target distance 21.0
model initialize at round 3139
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([14.60786082, 26.36335253,  5.48722315]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 19.413332341064862}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9298706945919181
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.15724946,  6.89212583,  5.18321359]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.190694072045011}
episode index:3140
target Thresh 31.99999999999941
target distance 14.0
model initialize at round 3140
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([15.42860006,  1.41699962,  3.35598373]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 14.955373403062291}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9298684258876314
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.5679426 , 8.16241557, 3.13808924]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.4615760087634854}
episode index:3141
target Thresh 31.999999999999414
target distance 15.0
model initialize at round 3141
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([18.35304024, 25.34628262,  3.94435561]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 17.472311566155952}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9298632218206664
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.16128182,  9.71311494,  4.71628227]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.3291122343961918}
episode index:3142
target Thresh 31.99999999999942
target distance 22.0
model initialize at round 3142
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  3.]), 'previousTarget': array([10.,  3.]), 'currentState': array([10.21096778, 23.19288514,  5.5825336 ]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 20.193987163787046}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9298551145515568
{'scaleFactor': 20, 'currentTarget': array([10.,  3.]), 'previousTarget': array([10.,  3.]), 'currentState': array([10.22543101,  3.75986634,  5.38011299]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 0.7926007763114219}
episode index:3143
target Thresh 31.999999999999428
target distance 27.0
model initialize at round 3143
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  2.]), 'previousTarget': array([20.,  2.]), 'currentState': array([ 8.86207   , 27.55458267,  6.2601651 ]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 27.876337280753646}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.9298301790097449
{'scaleFactor': 20, 'currentTarget': array([20.,  2.]), 'previousTarget': array([20.,  2.]), 'currentState': array([20.10289736,  2.64143203,  5.6392802 ]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 0.6496329124693955}
episode index:3144
target Thresh 31.99999999999943
target distance 5.0
model initialize at round 3144
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 25.]), 'previousTarget': array([12., 25.]), 'currentState': array([ 8.40151499, 22.05112644,  1.53195029]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 4.652413316806755}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9298461630545748
{'scaleFactor': 20, 'currentTarget': array([12., 25.]), 'previousTarget': array([12., 25.]), 'currentState': array([11.05634643, 24.67417461,  1.28066295]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 0.9983207094896699}
episode index:3145
target Thresh 31.99999999999944
target distance 19.0
model initialize at round 3145
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([ 4.48724475, 25.83708442,  5.02886117]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 20.215194604802544}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9298380689388579
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.99145398,  8.5922059 ,  5.96607978]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.592267564378168}
episode index:3146
target Thresh 31.999999999999442
target distance 19.0
model initialize at round 3146
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 29.]), 'previousTarget': array([17., 29.]), 'currentState': array([15.07488528, 11.40589859,  1.14279151]), 'targetState': array([17, 29], dtype=int32), 'currentDistance': 17.69910933264498}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9298328827865047
{'scaleFactor': 20, 'currentTarget': array([17., 29.]), 'previousTarget': array([17., 29.]), 'currentState': array([16.71321607, 28.90493714,  2.07250326]), 'targetState': array([17, 29], dtype=int32), 'currentDistance': 0.3021290568910011}
episode index:3147
target Thresh 31.99999999999945
target distance 15.0
model initialize at round 3147
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 13.]), 'previousTarget': array([15., 13.]), 'currentState': array([10.62360575, 28.44304904,  5.53957922]), 'targetState': array([15, 13], dtype=int32), 'currentDistance': 16.051186564100604}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9298306311383603
{'scaleFactor': 20, 'currentTarget': array([15., 13.]), 'previousTarget': array([15., 13.]), 'currentState': array([15.10993498, 13.51924018,  5.40579522]), 'targetState': array([15, 13], dtype=int32), 'currentDistance': 0.5307504731072683}
episode index:3148
target Thresh 31.999999999999456
target distance 11.0
model initialize at round 3148
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 18.]), 'previousTarget': array([26., 18.]), 'currentState': array([26.24331022,  5.86573028,  0.27043169]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 12.136708837656583}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9298343305725497
{'scaleFactor': 20, 'currentTarget': array([26., 18.]), 'previousTarget': array([26., 18.]), 'currentState': array([25.82433588, 17.11500307,  1.72101865]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 0.9022624096699974}
episode index:3149
target Thresh 31.99999999999946
target distance 17.0
model initialize at round 3149
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 11.]), 'previousTarget': array([27., 11.]), 'currentState': array([16.56716723, 28.61349406,  5.64631397]), 'targetState': array([27, 11], dtype=int32), 'currentDistance': 20.471423319046142}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9298262504914183
{'scaleFactor': 20, 'currentTarget': array([27., 11.]), 'previousTarget': array([27., 11.]), 'currentState': array([26.8027147 , 11.75532101,  5.81216364]), 'targetState': array([27, 11], dtype=int32), 'currentDistance': 0.7806608257984042}
episode index:3150
target Thresh 31.999999999999467
target distance 14.0
model initialize at round 3150
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([22.84070028, 14.77999499,  2.94249916]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 13.701530412429301}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9298269610904079
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.33548237, 10.00890961,  4.20398606]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.3356006574896551}
episode index:3151
target Thresh 31.99999999999947
target distance 14.0
model initialize at round 3151
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'previousTarget': array([ 2., 17.]), 'currentState': array([15.67542624, 16.65137517,  2.77487004]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 13.6798692305417}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9298276712385096
{'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'previousTarget': array([ 2., 17.]), 'currentState': array([ 2.04285236, 16.81890965,  3.45844832]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 0.1860914790456972}
episode index:3152
target Thresh 31.999999999999478
target distance 8.0
model initialize at round 3152
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 17.]), 'previousTarget': array([22., 17.]), 'currentState': array([22.14179719, 23.4431987 ,  4.88199157]), 'targetState': array([22, 17], dtype=int32), 'currentDistance': 6.444758793990812}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9298405070547993
{'scaleFactor': 20, 'currentTarget': array([22., 17.]), 'previousTarget': array([22., 17.]), 'currentState': array([22.15425816, 17.55352088,  5.23198257]), 'targetState': array([22, 17], dtype=int32), 'currentDistance': 0.5746137355777569}
episode index:3153
target Thresh 31.99999999999948
target distance 15.0
model initialize at round 3153
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 26.]), 'previousTarget': array([ 8., 26.]), 'currentState': array([20.07183773, 12.75016634,  2.29956979]), 'targetState': array([ 8, 26], dtype=int32), 'currentDistance': 17.92449045836064}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9298353316395896
{'scaleFactor': 20, 'currentTarget': array([ 8., 26.]), 'previousTarget': array([ 8., 26.]), 'currentState': array([ 8.10687271, 25.6492923 ,  2.87899514]), 'targetState': array([ 8, 26], dtype=int32), 'currentDistance': 0.3666301476745208}
episode index:3154
target Thresh 31.999999999999485
target distance 7.0
model initialize at round 3154
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 20.]), 'previousTarget': array([16., 20.]), 'currentState': array([21.44919905, 20.6537618 ,  3.75263596]), 'targetState': array([16, 20], dtype=int32), 'currentDistance': 5.488276116846277}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9298481568910509
{'scaleFactor': 20, 'currentTarget': array([16., 20.]), 'previousTarget': array([16., 20.]), 'currentState': array([15.81222309, 19.90174176,  4.02082139]), 'targetState': array([16, 20], dtype=int32), 'currentDistance': 0.2119312403386227}
episode index:3155
target Thresh 31.999999999999492
target distance 8.0
model initialize at round 3155
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 6.]), 'previousTarget': array([8., 6.]), 'currentState': array([10.32986816, 12.11720668,  4.43829419]), 'targetState': array([8, 6], dtype=int32), 'currentDistance': 6.5458768123566005}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9298609740149765
{'scaleFactor': 20, 'currentTarget': array([8., 6.]), 'previousTarget': array([8., 6.]), 'currentState': array([8.35212224, 6.58910634, 4.9898115 ]), 'targetState': array([8, 6], dtype=int32), 'currentDistance': 0.6863208821043298}
episode index:3156
target Thresh 31.999999999999496
target distance 19.0
model initialize at round 3156
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  6.]), 'previousTarget': array([10.,  6.]), 'currentState': array([21.45851437, 24.16028343,  4.75079945]), 'targetState': array([10,  6], dtype=int32), 'currentDistance': 21.473086549865716}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.929850038722054
{'scaleFactor': 20, 'currentTarget': array([10.,  6.]), 'previousTarget': array([10.,  6.]), 'currentState': array([10.32194622,  6.07398117,  4.67304181]), 'targetState': array([10,  6], dtype=int32), 'currentDistance': 0.3303370773339957}
episode index:3157
target Thresh 31.999999999999503
target distance 3.0
model initialize at round 3157
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 26.]), 'previousTarget': array([ 6., 26.]), 'currentState': array([ 4.90540386, 27.30010409,  5.63356662]), 'targetState': array([ 6, 26], dtype=int32), 'currentDistance': 1.699532682885866}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9298690855748969
{'scaleFactor': 20, 'currentTarget': array([ 6., 26.]), 'previousTarget': array([ 6., 26.]), 'currentState': array([ 6.39664871, 25.97051534,  5.47491173]), 'targetState': array([ 6, 26], dtype=int32), 'currentDistance': 0.39774306257477016}
episode index:3158
target Thresh 31.999999999999506
target distance 8.0
model initialize at round 3158
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([18.6726071 , 13.81352323,  0.89896983]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 8.611691502071361}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9298757715401786
{'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([15.75449205, 22.54871073,  2.65637755]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 0.6011302817206866}
episode index:3159
target Thresh 31.99999999999951
target distance 7.0
model initialize at round 3159
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 21.]), 'previousTarget': array([ 4., 21.]), 'currentState': array([10.4516648 , 15.62123009,  2.30909199]), 'targetState': array([ 4, 21], dtype=int32), 'currentDistance': 8.399710971441477}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9298854931346279
{'scaleFactor': 20, 'currentTarget': array([ 4., 21.]), 'previousTarget': array([ 4., 21.]), 'currentState': array([ 4.40749982, 20.55618664,  3.2503481 ]), 'targetState': array([ 4, 21], dtype=int32), 'currentDistance': 0.6025167201527422}
episode index:3160
target Thresh 31.999999999999517
target distance 7.0
model initialize at round 3160
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([20.34137748,  5.71475412,  4.32190132]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 5.991677834885711}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9298982781731807
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.44518147,  2.98119385,  4.38025835]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.4455785150958063}
episode index:3161
target Thresh 31.99999999999952
target distance 20.0
model initialize at round 3161
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'previousTarget': array([16., 23.]), 'currentState': array([19.68283665,  3.02118699,  1.02258938]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 20.31541914435138}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9298845168175649
{'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'previousTarget': array([16., 23.]), 'currentState': array([16.30477442, 23.15265871,  0.48089665]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 0.34086966813640446}
episode index:3162
target Thresh 31.999999999999527
target distance 1.0
model initialize at round 3162
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  5.]), 'previousTarget': array([19.,  5.]), 'currentState': array([18.67474316,  5.93861798,  2.06180215]), 'targetState': array([19,  5], dtype=int32), 'currentDistance': 0.9933759268776825}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9299066842166109
{'scaleFactor': 20, 'currentTarget': array([19.,  5.]), 'previousTarget': array([19.,  5.]), 'currentState': array([18.67474316,  5.93861798,  2.06180215]), 'targetState': array([19,  5], dtype=int32), 'currentDistance': 0.9933759268776825}
episode index:3163
target Thresh 31.99999999999953
target distance 16.0
model initialize at round 3163
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 29.]), 'previousTarget': array([16., 29.]), 'currentState': array([ 6.3495518 , 11.9944663 ,  0.36965054]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 19.552987672022308}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9298929289029255
{'scaleFactor': 20, 'currentTarget': array([16., 29.]), 'previousTarget': array([16., 29.]), 'currentState': array([15.8772629 , 29.89948732,  1.64948936]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 0.9078225750089188}
episode index:3164
target Thresh 31.999999999999535
target distance 15.0
model initialize at round 3164
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 17.]), 'previousTarget': array([ 4., 17.]), 'currentState': array([20.58913623,  8.55410659,  1.34550207]), 'targetState': array([ 4, 17], dtype=int32), 'currentDistance': 18.61538494034901}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9298848686015372
{'scaleFactor': 20, 'currentTarget': array([ 4., 17.]), 'previousTarget': array([ 4., 17.]), 'currentState': array([ 3.66723692, 17.34715877,  2.06030769]), 'targetState': array([ 4, 17], dtype=int32), 'currentDistance': 0.4808850976315815}
episode index:3165
target Thresh 31.99999999999954
target distance 7.0
model initialize at round 3165
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([25.36564656, 10.40159299,  1.89064646]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 5.961701269790259}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9298976336461987
{'scaleFactor': 20, 'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([20.97820571, 13.15741631,  3.28419673]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 0.9907907495682134}
episode index:3166
target Thresh 31.999999999999545
target distance 16.0
model initialize at round 3166
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 16.]), 'previousTarget': array([21., 16.]), 'currentState': array([4.73163878, 3.33856377, 5.56224847]), 'targetState': array([21, 16], dtype=int32), 'currentDistance': 20.614837960378214}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9298867213066384
{'scaleFactor': 20, 'currentTarget': array([21., 16.]), 'previousTarget': array([21., 16.]), 'currentState': array([21.28594568, 16.35694162,  0.6533341 ]), 'targetState': array([21, 16], dtype=int32), 'currentDistance': 0.4573535298206402}
episode index:3167
target Thresh 31.99999999999955
target distance 11.0
model initialize at round 3167
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  3.]), 'previousTarget': array([21.,  3.]), 'currentState': array([22.33382499, 13.76283147,  4.29298592]), 'targetState': array([21,  3], dtype=int32), 'currentDistance': 10.845166223341739}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9298903808483349
{'scaleFactor': 20, 'currentTarget': array([21.,  3.]), 'previousTarget': array([21.,  3.]), 'currentState': array([20.49372863,  2.48156707,  3.27954343]), 'targetState': array([21,  3], dtype=int32), 'currentDistance': 0.7246263887203647}
episode index:3168
target Thresh 31.999999999999552
target distance 13.0
model initialize at round 3168
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([ 9.54841189, 18.34059975,  4.87058998]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 13.925278855480437}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9298910671743237
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.11587631,  6.38944292,  5.46924308]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.4063165141918578}
episode index:3169
target Thresh 31.99999999999956
target distance 23.0
model initialize at round 3169
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'previousTarget': array([ 2., 17.]), 'currentState': array([23.43116084, 18.39079414,  2.5019865 ]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 21.476241836671736}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9298801672333409
{'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'previousTarget': array([ 2., 17.]), 'currentState': array([ 2.47190778, 16.54000253,  2.9580358 ]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 0.6590103391985542}
episode index:3170
target Thresh 31.999999999999563
target distance 13.0
model initialize at round 3170
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 15.]), 'previousTarget': array([26., 15.]), 'currentState': array([14.95374041, 21.14837736,  6.0052509 ]), 'targetState': array([26, 15], dtype=int32), 'currentDistance': 12.642088239268901}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9298838253797197
{'scaleFactor': 20, 'currentTarget': array([26., 15.]), 'previousTarget': array([26., 15.]), 'currentState': array([25.34237192, 15.71118896,  6.09116779]), 'targetState': array([26, 15], dtype=int32), 'currentDistance': 0.9686405088220448}
episode index:3171
target Thresh 31.999999999999567
target distance 11.0
model initialize at round 3171
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([26.31422248,  1.34662396,  3.89019823]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 11.434390350588195}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9298874812195751
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.13043949,  3.55059295,  1.97682674]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.5658330665463304}
episode index:3172
target Thresh 31.99999999999957
target distance 12.0
model initialize at round 3172
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 18.]), 'previousTarget': array([ 8., 18.]), 'currentState': array([14.94310899,  7.32944303,  1.25855589]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 12.730575338088954}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9298881675942009
{'scaleFactor': 20, 'currentTarget': array([ 8., 18.]), 'previousTarget': array([ 8., 18.]), 'currentState': array([ 8.65070876, 18.47309009,  0.56612765]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 0.8045098681698123}
episode index:3173
target Thresh 31.999999999999574
target distance 15.0
model initialize at round 3173
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 23.]), 'previousTarget': array([ 4., 23.]), 'currentState': array([15.48460343,  7.26790621,  2.58163214]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 19.478061782974223}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9298801316482067
{'scaleFactor': 20, 'currentTarget': array([ 4., 23.]), 'previousTarget': array([ 4., 23.]), 'currentState': array([ 3.82456651, 22.83453454,  2.52978299]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 0.24115498292054402}
episode index:3174
target Thresh 31.99999999999958
target distance 7.0
model initialize at round 3174
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([ 5.45706772, 13.38028484,  3.97742748]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 6.395215570856058}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9298928620004435
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.4381584 , 8.45615668, 5.00612947]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.6325043061771211}
episode index:3175
target Thresh 31.999999999999584
target distance 11.0
model initialize at round 3175
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 20.]), 'previousTarget': array([11., 20.]), 'currentState': array([ 3.46906294, 10.3426268 ,  0.9140054 ]), 'targetState': array([11, 20], dtype=int32), 'currentDistance': 12.246626887855669}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.929896510390683
{'scaleFactor': 20, 'currentTarget': array([11., 20.]), 'previousTarget': array([11., 20.]), 'currentState': array([10.40307233, 19.82156445,  1.32414678]), 'targetState': array([11, 20], dtype=int32), 'currentDistance': 0.6230263961582133}
episode index:3176
target Thresh 31.999999999999588
target distance 15.0
model initialize at round 3176
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 10.]), 'previousTarget': array([27., 10.]), 'currentState': array([12.45484572, 14.62034054,  0.2871294 ]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 15.261358380931945}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9298942592682521
{'scaleFactor': 20, 'currentTarget': array([27., 10.]), 'previousTarget': array([27., 10.]), 'currentState': array([26.33726981,  9.26250113,  4.14826079]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 0.9915220044068653}
episode index:3177
target Thresh 31.99999999999959
target distance 16.0
model initialize at round 3177
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  4.]), 'previousTarget': array([19.,  4.]), 'currentState': array([3.47158155, 4.61554911, 0.27678561]), 'targetState': array([19,  4], dtype=int32), 'currentDistance': 15.540613892816134}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9298920095625125
{'scaleFactor': 20, 'currentTarget': array([19.,  4.]), 'previousTarget': array([19.,  4.]), 'currentState': array([18.71146951,  3.35887265,  4.7460132 ]), 'targetState': array([19,  4], dtype=int32), 'currentDistance': 0.7030605401786013}
episode index:3178
target Thresh 31.999999999999595
target distance 20.0
model initialize at round 3178
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 15.]), 'previousTarget': array([24., 15.]), 'currentState': array([ 5.24206082, 26.13563771,  6.01385445]), 'targetState': array([24, 15], dtype=int32), 'currentDistance': 21.814277650051903}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9298811401836815
{'scaleFactor': 20, 'currentTarget': array([24., 15.]), 'previousTarget': array([24., 15.]), 'currentState': array([23.78498603, 15.33843286,  0.17992207]), 'targetState': array([24, 15], dtype=int32), 'currentDistance': 0.4009586109428071}
episode index:3179
target Thresh 31.9999999999996
target distance 8.0
model initialize at round 3179
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 19.]), 'previousTarget': array([ 6., 19.]), 'currentState': array([12.97987248, 12.36829748,  1.95932978]), 'targetState': array([ 6, 19], dtype=int32), 'currentDistance': 9.627985152449167}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9298877782056049
{'scaleFactor': 20, 'currentTarget': array([ 6., 19.]), 'previousTarget': array([ 6., 19.]), 'currentState': array([ 5.93703778, 18.85571926,  3.18427488]), 'targetState': array([ 6, 19], dtype=int32), 'currentDistance': 0.15742036819564623}
episode index:3180
target Thresh 31.999999999999606
target distance 10.0
model initialize at round 3180
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([13.11177537, 15.4294912 ,  3.1367718 ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 11.756776343735561}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9298914224593601
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.28802074, 8.09298   , 4.14844407]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.3026569462481748}
episode index:3181
target Thresh 31.99999999999961
target distance 13.0
model initialize at round 3181
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 19.]), 'previousTarget': array([23., 19.]), 'currentState': array([24.52237984,  5.28254081,  0.56959551]), 'targetState': array([23, 19], dtype=int32), 'currentDistance': 13.801678416171262}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9298921056540326
{'scaleFactor': 20, 'currentTarget': array([23., 19.]), 'previousTarget': array([23., 19.]), 'currentState': array([22.82430539, 18.24289422,  2.63948168]), 'targetState': array([23, 19], dtype=int32), 'currentDistance': 0.7772243939905399}
episode index:3182
target Thresh 31.999999999999613
target distance 12.0
model initialize at round 3182
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([ 9.39475981, 19.49443892,  4.45670271]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 12.382663822402156}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9298927884194278
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.98923964,  7.34713093,  3.64283633]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.6529577429562071}
episode index:3183
target Thresh 31.999999999999616
target distance 17.0
model initialize at round 3183
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([21.13590071, 21.48745198,  3.36185563]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 15.538130001891586}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9298905434150334
{'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([ 5.8731677 , 24.41182521,  3.19753445]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 0.6016942900083274}
episode index:3184
target Thresh 31.99999999999962
target distance 8.0
model initialize at round 3184
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 17.]), 'previousTarget': array([27., 17.]), 'currentState': array([19.7525091 , 12.80086056,  0.78934258]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 8.37609075129635}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9299001840638826
{'scaleFactor': 20, 'currentTarget': array([27., 17.]), 'previousTarget': array([27., 17.]), 'currentState': array([26.33361929, 17.09034316,  0.70660301]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 0.6724768626902171}
episode index:3185
target Thresh 31.999999999999623
target distance 24.0
model initialize at round 3185
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 27.]), 'previousTarget': array([ 4., 27.]), 'currentState': array([21.82215254,  4.46848673,  2.0704117 ]), 'targetState': array([ 4, 27], dtype=int32), 'currentDistance': 28.728003959427582}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.9298782625857211
{'scaleFactor': 20, 'currentTarget': array([ 4., 27.]), 'previousTarget': array([ 4., 27.]), 'currentState': array([ 4.54460024, 27.43680766,  0.49844893]), 'targetState': array([ 4, 27], dtype=int32), 'currentDistance': 0.6981334761659138}
episode index:3186
target Thresh 31.999999999999627
target distance 3.0
model initialize at round 3186
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 14.]), 'previousTarget': array([26., 14.]), 'currentState': array([25.72350259, 15.86821039,  4.83776677]), 'targetState': array([26, 14], dtype=int32), 'currentDistance': 1.8885605304449102}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9298971272664284
{'scaleFactor': 20, 'currentTarget': array([26., 14.]), 'previousTarget': array([26., 14.]), 'currentState': array([26.3002946 , 13.96327542,  5.17844376]), 'targetState': array([26, 14], dtype=int32), 'currentDistance': 0.3025318885812399}
episode index:3187
target Thresh 31.99999999999963
target distance 15.0
model initialize at round 3187
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'previousTarget': array([ 5., 22.]), 'currentState': array([4.44714694, 7.85915879, 1.54576796]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 14.151644311944864}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9298978073858264
{'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'previousTarget': array([ 5., 22.]), 'currentState': array([ 4.50769189, 21.43866862,  1.41482682]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 0.7466325660066736}
episode index:3188
target Thresh 31.999999999999634
target distance 19.0
model initialize at round 3188
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 8.]), 'previousTarget': array([5., 8.]), 'currentState': array([ 9.77036757, 25.85091681,  4.90314126]), 'targetState': array([5, 8], dtype=int32), 'currentDistance': 18.477327666910664}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9298926708038564
{'scaleFactor': 20, 'currentTarget': array([5., 8.]), 'previousTarget': array([5., 8.]), 'currentState': array([5.62441491, 8.84307668, 4.8119972 ]), 'targetState': array([5, 8], dtype=int32), 'currentDistance': 1.0491292879367797}
episode index:3189
target Thresh 31.999999999999638
target distance 10.0
model initialize at round 3189
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 15.]), 'previousTarget': array([10., 15.]), 'currentState': array([7.66330411, 4.74347024, 0.85697668]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 10.519341723521329}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9298992844023192
{'scaleFactor': 20, 'currentTarget': array([10., 15.]), 'previousTarget': array([10., 15.]), 'currentState': array([ 9.57541332, 14.24715968,  2.03650957]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 0.8643161431303102}
episode index:3190
target Thresh 31.99999999999964
target distance 10.0
model initialize at round 3190
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 22.]), 'previousTarget': array([ 3., 22.]), 'currentState': array([ 6.63836542, 13.6436575 ,  2.79736328]), 'targetState': array([ 3, 22], dtype=int32), 'currentDistance': 9.114064019488891}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9299058938556246
{'scaleFactor': 20, 'currentTarget': array([ 3., 22.]), 'previousTarget': array([ 3., 22.]), 'currentState': array([ 2.55617159, 22.19460197,  2.68975799]), 'targetState': array([ 3, 22], dtype=int32), 'currentDistance': 0.48461694798180344}
episode index:3191
target Thresh 31.999999999999645
target distance 15.0
model initialize at round 3191
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 28.]), 'previousTarget': array([19., 28.]), 'currentState': array([25.71583871, 14.65880693,  2.75045419]), 'targetState': array([19, 28], dtype=int32), 'currentDistance': 14.93619502911405}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9299036503720947
{'scaleFactor': 20, 'currentTarget': array([19., 28.]), 'previousTarget': array([19., 28.]), 'currentState': array([18.7559719 , 28.13179804,  2.56208041]), 'targetState': array([19, 28], dtype=int32), 'currentDistance': 0.27734533830747277}
episode index:3192
target Thresh 31.99999999999965
target distance 20.0
model initialize at round 3192
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 17.]), 'previousTarget': array([ 3., 17.]), 'currentState': array([22.25553954,  1.4906403 ,  3.24417257]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 24.72480619755458}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9298872449141012
{'scaleFactor': 20, 'currentTarget': array([ 3., 17.]), 'previousTarget': array([ 3., 17.]), 'currentState': array([ 2.7833389 , 17.0051067 ,  3.07616069]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 0.21672127226808047}
episode index:3193
target Thresh 31.999999999999652
target distance 9.0
model initialize at round 3193
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 21.]), 'previousTarget': array([ 4., 21.]), 'currentState': array([ 6.37290539, 13.79436481,  2.46223235]), 'targetState': array([ 4, 21], dtype=int32), 'currentDistance': 7.586294120321071}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9298968594304086
{'scaleFactor': 20, 'currentTarget': array([ 4., 21.]), 'previousTarget': array([ 4., 21.]), 'currentState': array([ 3.86491906, 20.98287551,  2.70114093]), 'targetState': array([ 4, 21], dtype=int32), 'currentDistance': 0.13616206801286215}
episode index:3194
target Thresh 31.999999999999655
target distance 18.0
model initialize at round 3194
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([ 9.98189756, 21.26072288,  5.13540819]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 16.749813607519073}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9298917327913016
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.0506598 ,  4.35050943,  5.25890031]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.6514632876939935}
episode index:3195
target Thresh 31.99999999999966
target distance 7.0
model initialize at round 3195
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  4.]), 'previousTarget': array([18.,  4.]), 'currentState': array([23.87860599, 11.25493568,  3.31005692]), 'targetState': array([18,  4], dtype=int32), 'currentDistance': 9.337671024975243}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.929898334267243
{'scaleFactor': 20, 'currentTarget': array([18.,  4.]), 'previousTarget': array([18.,  4.]), 'currentState': array([18.13848793,  3.84143962,  4.83123574]), 'targetState': array([18,  4], dtype=int32), 'currentDistance': 0.21052387282969348}
episode index:3196
target Thresh 31.999999999999662
target distance 19.0
model initialize at round 3196
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 23.]), 'previousTarget': array([15., 23.]), 'currentState': array([25.32159852,  4.12392159,  2.05789328]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 21.513756812670927}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9298875241077158
{'scaleFactor': 20, 'currentTarget': array([15., 23.]), 'previousTarget': array([15., 23.]), 'currentState': array([14.75487042, 22.81719301,  2.80578536]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 0.30578899371461654}
episode index:3197
target Thresh 31.999999999999666
target distance 11.0
model initialize at round 3197
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 15.]), 'previousTarget': array([10., 15.]), 'currentState': array([7.24237373, 5.13529538, 1.75039357]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 10.242895090948652}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9298941227711905
{'scaleFactor': 20, 'currentTarget': array([10., 15.]), 'previousTarget': array([10., 15.]), 'currentState': array([ 9.60499626, 14.44689753,  2.14519021]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 0.6796692569282524}
episode index:3198
target Thresh 31.99999999999967
target distance 8.0
model initialize at round 3198
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 16.]), 'previousTarget': array([11., 16.]), 'currentState': array([14.76028505, 22.43150552,  4.09090914]), 'targetState': array([11, 16], dtype=int32), 'currentDistance': 7.450101130990296}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9299037201101179
{'scaleFactor': 20, 'currentTarget': array([11., 16.]), 'previousTarget': array([11., 16.]), 'currentState': array([10.47595577, 16.24325902,  2.92766051]), 'targetState': array([11, 16], dtype=int32), 'currentDistance': 0.5777519439051656}
episode index:3199
target Thresh 31.999999999999673
target distance 13.0
model initialize at round 3199
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 13.]), 'previousTarget': array([ 6., 13.]), 'currentState': array([10.75172647, 27.12880524,  3.41641068]), 'targetState': array([ 6, 13], dtype=int32), 'currentDistance': 14.906442971464022}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9299014829145922
{'scaleFactor': 20, 'currentTarget': array([ 6., 13.]), 'previousTarget': array([ 6., 13.]), 'currentState': array([ 6.01782334, 12.66851784,  4.64198798]), 'targetState': array([ 6, 13], dtype=int32), 'currentDistance': 0.3319609847826209}
episode index:3200
target Thresh 31.999999999999677
target distance 10.0
model initialize at round 3200
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  9.]), 'previousTarget': array([24.,  9.]), 'currentState': array([24.73222422, 17.24247822,  5.40020478]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 8.274938035307489}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9299110719577304
{'scaleFactor': 20, 'currentTarget': array([24.,  9.]), 'previousTarget': array([24.,  9.]), 'currentState': array([24.20027908,  9.76751521,  5.41974968]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 0.7932158040281659}
episode index:3201
target Thresh 31.99999999999968
target distance 12.0
model initialize at round 3201
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([21.37555121,  7.35946674,  3.92743158]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 12.899912797080642}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9299117447484704
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.34440054, 10.97112012,  2.52044716]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.6562352446138217}
episode index:3202
target Thresh 31.999999999999684
target distance 14.0
model initialize at round 3202
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 14.]), 'previousTarget': array([19., 14.]), 'currentState': array([ 6.00536274, 22.65032084,  0.07942646]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 15.610530037043365}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9299095071430002
{'scaleFactor': 20, 'currentTarget': array([19., 14.]), 'previousTarget': array([19., 14.]), 'currentState': array([19.16014819, 14.13187786,  6.07464406]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 0.2074589451972487}
episode index:3203
target Thresh 31.999999999999684
target distance 9.0
model initialize at round 3203
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([7.25999157, 6.88428976, 0.28525942]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 7.820008942269448}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9299190847031927
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.06995246,  8.16755061,  0.55263555]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.18156694300325932}
episode index:3204
target Thresh 31.999999999999687
target distance 6.0
model initialize at round 3204
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 22.]), 'previousTarget': array([20., 22.]), 'currentState': array([24.67018376, 26.80473997,  4.69255471]), 'targetState': array([20, 22], dtype=int32), 'currentDistance': 6.700458380458658}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9299316837407269
{'scaleFactor': 20, 'currentTarget': array([20., 22.]), 'previousTarget': array([20., 22.]), 'currentState': array([20.74561979, 22.6150136 ,  4.13046065]), 'targetState': array([20, 22], dtype=int32), 'currentDistance': 0.966535355674868}
episode index:3205
target Thresh 31.99999999999969
target distance 6.0
model initialize at round 3205
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([12.76301932,  7.15506928,  5.00085291]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 4.335287983543504}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9299473319990735
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.93076668,  3.5119737 ,  5.76429766]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.5166336466188023}
episode index:3206
target Thresh 31.999999999999694
target distance 14.0
model initialize at round 3206
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 14.]), 'previousTarget': array([18., 14.]), 'currentState': array([ 5.34505006, 19.01154754,  5.91799838]), 'targetState': array([18, 14], dtype=int32), 'currentDistance': 13.611148615730116}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9299479924343427
{'scaleFactor': 20, 'currentTarget': array([18., 14.]), 'previousTarget': array([18., 14.]), 'currentState': array([17.98664199, 14.02475544,  0.29899037]), 'targetState': array([18, 14], dtype=int32), 'currentDistance': 0.02812948425332462}
episode index:3207
target Thresh 31.999999999999698
target distance 16.0
model initialize at round 3207
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 21.]), 'previousTarget': array([ 8., 21.]), 'currentState': array([6.14116978, 3.76301996, 0.18433588]), 'targetState': array([ 8, 21], dtype=int32), 'currentDistance': 17.336918139949546}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9299428706310537
{'scaleFactor': 20, 'currentTarget': array([ 8., 21.]), 'previousTarget': array([ 8., 21.]), 'currentState': array([ 7.75460859, 20.7396164 ,  1.71100795]), 'targetState': array([ 8, 21], dtype=int32), 'currentDistance': 0.3577940266404593}
episode index:3208
target Thresh 31.9999999999997
target distance 13.0
model initialize at round 3208
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 27.]), 'previousTarget': array([16., 27.]), 'currentState': array([18.81590746, 15.59663884,  2.01546249]), 'targetState': array([16, 27], dtype=int32), 'currentDistance': 11.745892073272607}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9299464659189222
{'scaleFactor': 20, 'currentTarget': array([16., 27.]), 'previousTarget': array([16., 27.]), 'currentState': array([15.81941737, 26.85985   ,  2.55472516]), 'targetState': array([16, 27], dtype=int32), 'currentDistance': 0.22858720587780784}
episode index:3209
target Thresh 31.999999999999705
target distance 7.0
model initialize at round 3209
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 26.]), 'previousTarget': array([ 4., 26.]), 'currentState': array([ 2.47666587, 19.71543078,  1.69252014]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 6.466556820862645}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9299590368018136
{'scaleFactor': 20, 'currentTarget': array([ 4., 26.]), 'previousTarget': array([ 4., 26.]), 'currentState': array([ 3.60043032, 25.44548475,  1.92429951]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 0.6834786694462119}
episode index:3210
target Thresh 31.999999999999705
target distance 13.0
model initialize at round 3210
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([12.60629463, 10.56996653,  2.21225747]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 11.514689737314233}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.929962624815703
{'scaleFactor': 20, 'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([13.69059522, 22.15264289,  1.92643982]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 0.3450089464713386}
episode index:3211
target Thresh 31.99999999999971
target distance 9.0
model initialize at round 3211
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 22.]), 'previousTarget': array([20., 22.]), 'currentState': array([10.42293599, 25.5809446 ,  0.91078222]), 'targetState': array([20, 22], dtype=int32), 'currentDistance': 10.22464274462162}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9299691713365885
{'scaleFactor': 20, 'currentTarget': array([20., 22.]), 'previousTarget': array([20., 22.]), 'currentState': array([19.12609186, 22.66805086,  0.16744408]), 'targetState': array([20, 22], dtype=int32), 'currentDistance': 1.1000033611658087}
episode index:3212
target Thresh 31.999999999999712
target distance 4.0
model initialize at round 3212
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([4.44936412, 4.34584677, 2.53080177]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 2.9556061145962325}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9299878550678874
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.862731  , 5.56241554, 2.44322318]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.9673597762355218}
episode index:3213
target Thresh 31.999999999999716
target distance 1.0
model initialize at round 3213
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([19.1584592 , 14.24618627,  3.70650092]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 2.8521854410971477}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9299973784515003
{'scaleFactor': 20, 'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([21.7827807 , 14.2636419 ,  5.76289487]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 0.3416010483261555}
episode index:3214
target Thresh 31.99999999999972
target distance 24.0
model initialize at round 3214
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  3.]), 'previousTarget': array([12.,  3.]), 'currentState': array([ 3.48538152, 26.26629779,  4.6026938 ]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 24.77537771373912}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9299810561014373
{'scaleFactor': 20, 'currentTarget': array([12.,  3.]), 'previousTarget': array([12.,  3.]), 'currentState': array([12.52106072,  2.28852799,  5.16176185]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 0.8818711337156105}
episode index:3215
target Thresh 31.99999999999972
target distance 11.0
model initialize at round 3215
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 14.]), 'previousTarget': array([15., 14.]), 'currentState': array([4.74499373, 5.80049499, 0.78005636]), 'targetState': array([15, 14], dtype=int32), 'currentDistance': 13.130005176836924}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9299817042021232
{'scaleFactor': 20, 'currentTarget': array([15., 14.]), 'previousTarget': array([15., 14.]), 'currentState': array([15.39645108, 14.52456018,  0.77216754]), 'targetState': array([15, 14], dtype=int32), 'currentDistance': 0.6575232646697504}
episode index:3216
target Thresh 31.999999999999723
target distance 17.0
model initialize at round 3216
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 25.]), 'previousTarget': array([19., 25.]), 'currentState': array([3.97148478, 9.65009487, 0.52483582]), 'targetState': array([19, 25], dtype=int32), 'currentDistance': 21.481989136433953}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9299709353336297
{'scaleFactor': 20, 'currentTarget': array([19., 25.]), 'previousTarget': array([19., 25.]), 'currentState': array([18.85775408, 25.09441392,  1.51281371]), 'targetState': array([19, 25], dtype=int32), 'currentDistance': 0.17072753503035257}
episode index:3217
target Thresh 31.999999999999726
target distance 13.0
model initialize at round 3217
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 26.]), 'previousTarget': array([20., 26.]), 'currentState': array([22.32112924, 12.88260725,  2.20140266]), 'targetState': array([20, 26], dtype=int32), 'currentDistance': 13.321172376997163}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9299715861765675
{'scaleFactor': 20, 'currentTarget': array([20., 26.]), 'previousTarget': array([20., 26.]), 'currentState': array([19.75005938, 26.34624993,  2.1093424 ]), 'targetState': array([20, 26], dtype=int32), 'currentDistance': 0.4270355102547127}
episode index:3218
target Thresh 31.99999999999973
target distance 13.0
model initialize at round 3218
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 16.]), 'previousTarget': array([20., 16.]), 'currentState': array([ 8.3505834 , 20.99585232,  0.37067717]), 'targetState': array([20, 16], dtype=int32), 'currentDistance': 12.675466362804462}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9299751613748354
{'scaleFactor': 20, 'currentTarget': array([20., 16.]), 'previousTarget': array([20., 16.]), 'currentState': array([19.15601007, 16.63829508,  5.93520578]), 'targetState': array([20, 16], dtype=int32), 'currentDistance': 1.058177493151408}
episode index:3219
target Thresh 31.999999999999734
target distance 6.0
model initialize at round 3219
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 25.]), 'previousTarget': array([18., 25.]), 'currentState': array([14.01067545, 20.36147898,  1.189188  ]), 'targetState': array([18, 25], dtype=int32), 'currentDistance': 6.11805422877775}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9299876843060854
{'scaleFactor': 20, 'currentTarget': array([18., 25.]), 'previousTarget': array([18., 25.]), 'currentState': array([17.55593602, 24.93348873,  1.6235956 ]), 'targetState': array([18, 25], dtype=int32), 'currentDistance': 0.449017330069935}
episode index:3220
target Thresh 31.999999999999734
target distance 16.0
model initialize at round 3220
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 23.]), 'previousTarget': array([18., 23.]), 'currentState': array([11.78613955,  8.66932675,  0.68821406]), 'targetState': array([18, 23], dtype=int32), 'currentDistance': 15.61986739942443}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9299854356286938
{'scaleFactor': 20, 'currentTarget': array([18., 23.]), 'previousTarget': array([18., 23.]), 'currentState': array([17.63509975, 22.98619762,  1.87221569]), 'targetState': array([18, 23], dtype=int32), 'currentDistance': 0.36516119289545773}
episode index:3221
target Thresh 31.999999999999737
target distance 13.0
model initialize at round 3221
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'previousTarget': array([11., 19.]), 'currentState': array([22.05674924, 13.80495838,  2.86106205]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 12.216389043090787}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.929989003199697
{'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'previousTarget': array([11., 19.]), 'currentState': array([11.32149509, 18.42916085,  3.2342234 ]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 0.6551461174117532}
episode index:3222
target Thresh 31.99999999999974
target distance 8.0
model initialize at round 3222
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 17.]), 'previousTarget': array([27., 17.]), 'currentState': array([19.21164432, 20.33039083,  5.84847927]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 8.470536421223176}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9299984996337026
{'scaleFactor': 20, 'currentTarget': array([27., 17.]), 'previousTarget': array([27., 17.]), 'currentState': array([26.42057826, 17.45379913,  0.48751756]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 0.7359777204009238}
episode index:3223
target Thresh 31.99999999999974
target distance 4.0
model initialize at round 3223
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 18.]), 'previousTarget': array([ 8., 18.]), 'currentState': array([10.82943596, 16.79079865,  2.93322515]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 3.0769913471703534}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.93001403980131
{'scaleFactor': 20, 'currentTarget': array([ 8., 18.]), 'previousTarget': array([ 8., 18.]), 'currentState': array([ 7.22395762, 18.08078898,  3.51805276]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 0.7802362721894489}
episode index:3224
target Thresh 31.999999999999744
target distance 20.0
model initialize at round 3224
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([ 4.87932632, 11.3125125 ,  5.90522057]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 18.626770484188626}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9300089245168704
{'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([22.06846075,  7.63913171,  0.06944564]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 1.1297144390640306}
episode index:3225
target Thresh 31.999999999999748
target distance 6.0
model initialize at round 3225
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 21.]), 'previousTarget': array([18., 21.]), 'currentState': array([10.12402269, 16.68356112,  2.75829887]), 'targetState': array([18, 21], dtype=int32), 'currentDistance': 8.981239517656652}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9300154282755138
{'scaleFactor': 20, 'currentTarget': array([18., 21.]), 'previousTarget': array([18., 21.]), 'currentState': array([17.03825843, 20.74283758,  1.18145749]), 'targetState': array([18, 21], dtype=int32), 'currentDistance': 0.9955296859471229}
episode index:3226
target Thresh 31.99999999999975
target distance 17.0
model initialize at round 3226
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 19.]), 'previousTarget': array([21., 19.]), 'currentState': array([6.63251838, 1.59101122, 0.76452607]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 22.572049491558317}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9300046823275693
{'scaleFactor': 20, 'currentTarget': array([21., 19.]), 'previousTarget': array([21., 19.]), 'currentState': array([20.14065325, 18.4522905 ,  1.55603126]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 1.019049816593458}
episode index:3227
target Thresh 31.99999999999975
target distance 10.0
model initialize at round 3227
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([14.59789791, 25.42674815,  4.06567776]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 8.542597655879025}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9300111833708073
{'scaleFactor': 20, 'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([15.84533184, 16.47910246,  3.59480488]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 0.5433750853308589}
episode index:3228
target Thresh 31.999999999999755
target distance 6.0
model initialize at round 3228
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 17.]), 'previousTarget': array([18., 17.]), 'currentState': array([13.8105221 , 19.15847673,  5.74508688]), 'targetState': array([18, 17], dtype=int32), 'currentDistance': 4.712827908130653}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9300266955469081
{'scaleFactor': 20, 'currentTarget': array([18., 17.]), 'previousTarget': array([18., 17.]), 'currentState': array([17.16539197, 17.5257483 ,  0.47253432]), 'targetState': array([18, 17], dtype=int32), 'currentDistance': 0.9863984175732156}
episode index:3229
target Thresh 31.99999999999976
target distance 12.0
model initialize at round 3229
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 19.]), 'previousTarget': array([16., 19.]), 'currentState': array([19.05807707,  8.39469326,  1.15479469]), 'targetState': array([16, 19], dtype=int32), 'currentDistance': 11.037407598602163}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9300302415078536
{'scaleFactor': 20, 'currentTarget': array([16., 19.]), 'previousTarget': array([16., 19.]), 'currentState': array([16.22533094, 19.17876506,  0.66682919]), 'targetState': array([16, 19], dtype=int32), 'currentDistance': 0.2876299329564841}
episode index:3230
target Thresh 31.99999999999976
target distance 12.0
model initialize at round 3230
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 13.]), 'previousTarget': array([ 4., 13.]), 'currentState': array([ 4.66226581, 23.15901911,  4.69153875]), 'targetState': array([ 4, 13], dtype=int32), 'currentDistance': 10.180582752495116}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.93003672860423
{'scaleFactor': 20, 'currentTarget': array([ 4., 13.]), 'previousTarget': array([ 4., 13.]), 'currentState': array([ 4.15164822, 13.2408267 ,  4.93780287]), 'targetState': array([ 4, 13], dtype=int32), 'currentDistance': 0.28459564512080404}
episode index:3231
target Thresh 31.999999999999762
target distance 16.0
model initialize at round 3231
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 13.]), 'previousTarget': array([19., 13.]), 'currentState': array([20.13967907, 27.76164638,  4.44631674]), 'targetState': array([19, 13], dtype=int32), 'currentDistance': 14.805575709390865}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9300344724055368
{'scaleFactor': 20, 'currentTarget': array([19., 13.]), 'previousTarget': array([19., 13.]), 'currentState': array([18.75668944, 12.55728052,  3.25186411]), 'targetState': array([19, 13], dtype=int32), 'currentDistance': 0.5051737946081688}
episode index:3232
target Thresh 31.999999999999766
target distance 10.0
model initialize at round 3232
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 25.]), 'previousTarget': array([15., 25.]), 'currentState': array([ 5.84977739, 17.77091287,  0.79169267]), 'targetState': array([15, 25], dtype=int32), 'currentDistance': 11.661315302320347}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9300380126706143
{'scaleFactor': 20, 'currentTarget': array([15., 25.]), 'previousTarget': array([15., 25.]), 'currentState': array([14.97414444, 25.37224939,  0.74325506]), 'targetState': array([15, 25], dtype=int32), 'currentDistance': 0.3731462398868018}
episode index:3233
target Thresh 31.999999999999766
target distance 5.0
model initialize at round 3233
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 18.]), 'previousTarget': array([17., 18.]), 'currentState': array([19.37485202, 13.43735814,  1.86870217]), 'targetState': array([17, 18], dtype=int32), 'currentDistance': 5.143697393934512}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9300504619555028
{'scaleFactor': 20, 'currentTarget': array([17., 18.]), 'previousTarget': array([17., 18.]), 'currentState': array([16.050413  , 18.27430403,  2.5531176 ]), 'targetState': array([17, 18], dtype=int32), 'currentDistance': 0.9884119453636784}
episode index:3234
target Thresh 31.99999999999977
target distance 15.0
model initialize at round 3234
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 12.]), 'previousTarget': array([27., 12.]), 'currentState': array([11.19489089,  6.47789966,  1.05961204]), 'targetState': array([27, 12], dtype=int32), 'currentDistance': 16.742014995864363}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9300453512245995
{'scaleFactor': 20, 'currentTarget': array([27., 12.]), 'previousTarget': array([27., 12.]), 'currentState': array([27.39425759, 11.89443575,  5.18807758]), 'targetState': array([27, 12], dtype=int32), 'currentDistance': 0.40814562938818816}
episode index:3235
target Thresh 31.999999999999773
target distance 11.0
model initialize at round 3235
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([12.43823282, 12.12600089,  0.46394556]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 9.62783826592435}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.930051823628393
{'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([22.04966877, 11.30284264,  0.58030428]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 0.3068886575578204}
episode index:3236
target Thresh 31.999999999999773
target distance 10.0
model initialize at round 3236
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([14.67452234, 18.96293825,  4.81551313]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 10.970229247514956}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9300553541584431
{'scaleFactor': 20, 'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([21.54425583,  9.54095444,  5.44419444]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 0.7119952480249631}
episode index:3237
target Thresh 31.999999999999776
target distance 9.0
model initialize at round 3237
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 23.]), 'previousTarget': array([ 6., 23.]), 'currentState': array([13.33909487, 20.10488161,  2.6803572 ]), 'targetState': array([ 6, 23], dtype=int32), 'currentDistance': 7.889488193546764}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9300647861089809
{'scaleFactor': 20, 'currentTarget': array([ 6., 23.]), 'previousTarget': array([ 6., 23.]), 'currentState': array([ 5.91682304, 22.98030384,  2.9341076 ]), 'targetState': array([ 6, 23], dtype=int32), 'currentDistance': 0.08547716429218816}
episode index:3238
target Thresh 31.99999999999978
target distance 17.0
model initialize at round 3238
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 24.]), 'previousTarget': array([10., 24.]), 'currentState': array([3.673414  , 8.63495432, 2.03768662]), 'targetState': array([10, 24], dtype=int32), 'currentDistance': 16.616567612569487}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9300625261238987
{'scaleFactor': 20, 'currentTarget': array([10., 24.]), 'previousTarget': array([10., 24.]), 'currentState': array([ 9.24569763, 23.11855802,  1.4405086 ]), 'targetState': array([10, 24], dtype=int32), 'currentDistance': 1.1601344922765202}
episode index:3239
target Thresh 31.99999999999978
target distance 14.0
model initialize at round 3239
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 28.]), 'previousTarget': array([19., 28.]), 'currentState': array([10.33873963, 12.98011555,  0.35897415]), 'targetState': array([19, 28], dtype=int32), 'currentDistance': 17.338234057449085}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9300574195564172
{'scaleFactor': 20, 'currentTarget': array([19., 28.]), 'previousTarget': array([19., 28.]), 'currentState': array([18.5136629 , 28.04084491,  1.79609908]), 'targetState': array([19, 28], dtype=int32), 'currentDistance': 0.48804926468331755}
episode index:3240
target Thresh 31.999999999999783
target distance 19.0
model initialize at round 3240
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'previousTarget': array([11., 21.]), 'currentState': array([21.58092548,  2.57711586,  1.36001843]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 21.24520284388341}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9300467070709812
{'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'previousTarget': array([11., 21.]), 'currentState': array([11.0484859 , 21.10895323,  1.35097372]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 0.11925472470994791}
episode index:3241
target Thresh 31.999999999999783
target distance 14.0
model initialize at round 3241
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 20.]), 'previousTarget': array([19., 20.]), 'currentState': array([ 6.49525778, 28.77239384,  5.74999482]), 'targetState': array([19, 20], dtype=int32), 'currentDistance': 15.274929515050133}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9300444547536946
{'scaleFactor': 20, 'currentTarget': array([19., 20.]), 'previousTarget': array([19., 20.]), 'currentState': array([18.83700196, 19.60311593,  4.26828909]), 'targetState': array([19, 20], dtype=int32), 'currentDistance': 0.4290516603474953}
episode index:3242
target Thresh 31.999999999999787
target distance 16.0
model initialize at round 3242
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.89295275, 25.32043761,  5.65873945]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 14.348250694715436}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9300450779091536
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.38934986, 11.82103064,  4.75347578]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.9086719013514503}
episode index:3243
target Thresh 31.99999999999979
target distance 12.0
model initialize at round 3243
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 29.]), 'previousTarget': array([ 8., 29.]), 'currentState': array([18.32754023, 24.81220648,  2.24341011]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 11.144312531607145}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9300486029003656
{'scaleFactor': 20, 'currentTarget': array([ 8., 29.]), 'previousTarget': array([ 8., 29.]), 'currentState': array([ 7.39697174, 29.11934809,  2.65678166]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 0.6147251835288512}
episode index:3244
target Thresh 31.99999999999979
target distance 14.0
model initialize at round 3244
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 17.]), 'previousTarget': array([ 7., 17.]), 'currentState': array([16.67149177,  3.19622269,  1.12685889]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 16.854732902362294}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9300435084919166
{'scaleFactor': 20, 'currentTarget': array([ 7., 17.]), 'previousTarget': array([ 7., 17.]), 'currentState': array([ 7.30698496, 17.31323175,  1.01477283]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 0.43858168524667296}
episode index:3245
target Thresh 31.999999999999794
target distance 10.0
model initialize at round 3245
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([5.66407779, 1.74853791, 0.86002272]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 8.947603296607086}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9300499615237737
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.65097711,  5.79518854,  0.55152303]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.0276653261469872}
episode index:3246
target Thresh 31.999999999999794
target distance 2.0
model initialize at round 3246
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 21.]), 'previousTarget': array([ 3., 21.]), 'currentState': array([ 5.69017227, 20.4650569 ,  4.12494373]), 'targetState': array([ 3, 21], dtype=int32), 'currentDistance': 2.7428435868957317}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9300653757641422
{'scaleFactor': 20, 'currentTarget': array([ 3., 21.]), 'previousTarget': array([ 3., 21.]), 'currentState': array([ 2.26681712, 21.10750858,  3.47584903]), 'targetState': array([ 3, 21], dtype=int32), 'currentDistance': 0.7410230927094189}
episode index:3247
target Thresh 31.999999999999797
target distance 4.0
model initialize at round 3247
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 12.]), 'previousTarget': array([11., 12.]), 'currentState': array([ 8.49702562, 13.08934539,  0.34127336]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 2.7297534913240193}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9300838285425399
{'scaleFactor': 20, 'currentTarget': array([11., 12.]), 'previousTarget': array([11., 12.]), 'currentState': array([10.26034233, 12.5107113 ,  5.2944535 ]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 0.8988434279851599}
episode index:3248
target Thresh 31.9999999999998
target distance 11.0
model initialize at round 3248
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 29.]), 'previousTarget': array([16., 29.]), 'currentState': array([25.49949568, 22.00990581,  3.32574606]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 11.794144091438218}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9300873361820777
{'scaleFactor': 20, 'currentTarget': array([16., 29.]), 'previousTarget': array([16., 29.]), 'currentState': array([15.98524315, 28.44841163,  2.3811266 ]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 0.5517857304743102}
episode index:3249
target Thresh 31.9999999999998
target distance 6.0
model initialize at round 3249
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  2.]), 'previousTarget': array([19.,  2.]), 'currentState': array([24.50783117,  5.60939676,  2.87757438]), 'targetState': array([19,  2], dtype=int32), 'currentDistance': 6.585130917042924}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9300967234663293
{'scaleFactor': 20, 'currentTarget': array([19.,  2.]), 'previousTarget': array([19.,  2.]), 'currentState': array([18.29802949,  2.02205329,  2.53534162]), 'targetState': array([19,  2], dtype=int32), 'currentDistance': 0.7023168431537058}
episode index:3250
target Thresh 31.999999999999805
target distance 4.0
model initialize at round 3250
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 21.]), 'previousTarget': array([ 2., 21.]), 'currentState': array([ 4.39927684, 22.27171314,  4.44442868]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 2.715471164478431}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9301151495741525
{'scaleFactor': 20, 'currentTarget': array([ 2., 21.]), 'previousTarget': array([ 2., 21.]), 'currentState': array([ 2.80073388, 21.6131762 ,  2.60188043]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 1.0085434057530132}
episode index:3251
target Thresh 31.999999999999805
target distance 7.0
model initialize at round 3251
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([17.67342739, 12.82103397,  5.16664446]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 8.872107008343397}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9301215686701938
{'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([12.10705885,  5.68818145,  4.6387204 ]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 0.3296853100362253}
episode index:3252
target Thresh 31.999999999999808
target distance 9.0
model initialize at round 3252
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 15.]), 'previousTarget': array([21., 15.]), 'currentState': array([16.98649193, 22.6564266 ,  5.07611752]), 'targetState': array([21, 15], dtype=int32), 'currentDistance': 8.64460035999393}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9301309367738918
{'scaleFactor': 20, 'currentTarget': array([21., 15.]), 'previousTarget': array([21., 15.]), 'currentState': array([20.75324991, 15.94560602,  6.23412645]), 'targetState': array([21, 15], dtype=int32), 'currentDistance': 0.9772698444778727}
episode index:3253
target Thresh 31.999999999999808
target distance 16.0
model initialize at round 3253
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 10.]), 'previousTarget': array([17., 10.]), 'currentState': array([22.19549661, 24.09792045,  4.29265308]), 'targetState': array([17, 10], dtype=int32), 'currentDistance': 15.024797705215732}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9301286668776576
{'scaleFactor': 20, 'currentTarget': array([17., 10.]), 'previousTarget': array([17., 10.]), 'currentState': array([17.20943868,  9.25015538,  4.98083242]), 'targetState': array([17, 10], dtype=int32), 'currentDistance': 0.7785444862130647}
episode index:3254
target Thresh 31.99999999999981
target distance 13.0
model initialize at round 3254
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'previousTarget': array([19.,  3.]), 'currentState': array([6.42989977, 4.62713683, 0.30249238]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 12.674975110938396}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9301321542762822
{'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'previousTarget': array([19.,  3.]), 'currentState': array([18.11579916,  3.51648865,  6.00160847]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 1.023997874770599}
episode index:3255
target Thresh 31.99999999999981
target distance 5.0
model initialize at round 3255
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  5.]), 'previousTarget': array([11.,  5.]), 'currentState': array([15.68922108,  4.65402677,  2.76652324]), 'targetState': array([11,  5], dtype=int32), 'currentDistance': 4.701966802585592}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9301475006662465
{'scaleFactor': 20, 'currentTarget': array([11.,  5.]), 'previousTarget': array([11.,  5.]), 'currentState': array([11.81885267,  4.57729597,  2.67544383]), 'targetState': array([11,  5], dtype=int32), 'currentDistance': 0.9215196077808234}
episode index:3256
target Thresh 31.999999999999815
target distance 12.0
model initialize at round 3256
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 28.]), 'previousTarget': array([21., 28.]), 'currentState': array([ 9.83721299, 15.54004709,  6.24305916]), 'targetState': array([21, 28], dtype=int32), 'currentDistance': 16.728964112029864}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9301423946628131
{'scaleFactor': 20, 'currentTarget': array([21., 28.]), 'previousTarget': array([21., 28.]), 'currentState': array([21.05909334, 27.95482512,  0.53882534]), 'targetState': array([21, 28], dtype=int32), 'currentDistance': 0.07438274184828345}
episode index:3257
target Thresh 31.999999999999815
target distance 13.0
model initialize at round 3257
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 16.]), 'previousTarget': array([21., 16.]), 'currentState': array([15.6717155 ,  4.03346426,  1.47038341]), 'targetState': array([21, 16], dtype=int32), 'currentDistance': 13.09918292471913}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.930145874636643
{'scaleFactor': 20, 'currentTarget': array([21., 16.]), 'previousTarget': array([21., 16.]), 'currentState': array([20.26419413, 15.00982442,  1.22698081]), 'targetState': array([21, 16], dtype=int32), 'currentDistance': 1.233636074082335}
episode index:3258
target Thresh 31.99999999999982
target distance 15.0
model initialize at round 3258
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 12.]), 'previousTarget': array([12., 12.]), 'currentState': array([25.4676424 , 20.91900693,  4.37972403]), 'targetState': array([12, 12], dtype=int32), 'currentDistance': 16.153206377683418}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9301436036393406
{'scaleFactor': 20, 'currentTarget': array([12., 12.]), 'previousTarget': array([12., 12.]), 'currentState': array([12.86498112, 11.93621928,  3.54987264]), 'targetState': array([12, 12], dtype=int32), 'currentDistance': 0.8673294149922001}
episode index:3259
target Thresh 31.99999999999982
target distance 7.0
model initialize at round 3259
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  5.]), 'previousTarget': array([18.,  5.]), 'currentState': array([18.64093758, 10.44385505,  4.09309053]), 'targetState': array([18,  5], dtype=int32), 'currentDistance': 5.4814559016431765}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9301559212455862
{'scaleFactor': 20, 'currentTarget': array([18.,  5.]), 'previousTarget': array([18.,  5.]), 'currentState': array([17.66839891,  5.30536219,  5.16725349]), 'targetState': array([18,  5], dtype=int32), 'currentDistance': 0.45078303957641636}
episode index:3260
target Thresh 31.999999999999822
target distance 11.0
model initialize at round 3260
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([4.09086461, 4.68051531, 0.50677943]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 11.23362796522927}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9301593938699821
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.54045179,  2.26648487,  6.09017291]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.6025797284789602}
episode index:3261
target Thresh 31.999999999999822
target distance 17.0
model initialize at round 3261
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 4.53797875, 27.31661911,  4.85505069]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 16.915723238757277}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9301542920470556
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.86504345, 10.68412642,  5.2539587 ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.9209105772618833}
episode index:3262
target Thresh 31.999999999999826
target distance 18.0
model initialize at round 3262
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([ 2.52318043, 23.6140109 ,  0.84805083]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 18.630239002706936}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9301381617163637
{'scaleFactor': 20, 'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([20.39233573, 25.25673963,  3.41767523]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 0.9600478389863235}
episode index:3263
target Thresh 31.999999999999826
target distance 7.0
model initialize at round 3263
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 16.]), 'previousTarget': array([ 7., 16.]), 'currentState': array([ 8.7635656 , 10.55276957,  2.02810529]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 5.725598920526738}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9301504658947594
{'scaleFactor': 20, 'currentTarget': array([ 7., 16.]), 'previousTarget': array([ 7., 16.]), 'currentState': array([ 6.56170048, 16.08870047,  2.19617566]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 0.44718480138428274}
episode index:3264
target Thresh 31.99999999999983
target distance 14.0
model initialize at round 3264
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 21.]), 'previousTarget': array([10., 21.]), 'currentState': array([22.53109054, 16.82139703,  3.64172244]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 13.209426667880814}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9301510523823588
{'scaleFactor': 20, 'currentTarget': array([10., 21.]), 'previousTarget': array([10., 21.]), 'currentState': array([10.07478467, 21.12958723,  1.55664419]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 0.14961816593241223}
episode index:3265
target Thresh 31.99999999999983
target distance 3.0
model initialize at round 3265
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 14.]), 'previousTarget': array([23., 14.]), 'currentState': array([26.63507103, 15.44145168,  4.08932352]), 'targetState': array([23, 14], dtype=int32), 'currentDistance': 3.9104378681498213}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9301663459976736
{'scaleFactor': 20, 'currentTarget': array([23., 14.]), 'previousTarget': array([23., 14.]), 'currentState': array([23.09618152, 13.7764876 ,  3.67032135]), 'targetState': array([23, 14], dtype=int32), 'currentDistance': 0.24332833247340582}
episode index:3266
target Thresh 31.999999999999833
target distance 2.0
model initialize at round 3266
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.8030213 , 10.8827572 ,  0.71554416]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.22923019485894633}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9301877214656877
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.8030213 , 10.8827572 ,  0.71554416]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.22923019485894633}
episode index:3267
target Thresh 31.999999999999833
target distance 14.0
model initialize at round 3267
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([6.17321893, 3.53411421, 5.20884967]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 15.03043676977078}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9301854439176346
{'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([20.87214232,  5.66589689,  5.26521724]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 0.35773240676637513}
episode index:3268
target Thresh 31.999999999999837
target distance 18.0
model initialize at round 3268
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  2.]), 'previousTarget': array([11.,  2.]), 'currentState': array([ 5.50966525, 20.78185064,  3.66844821]), 'targetState': array([11,  2], dtype=int32), 'currentDistance': 19.567873903085676}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9301775505652611
{'scaleFactor': 20, 'currentTarget': array([11.,  2.]), 'previousTarget': array([11.,  2.]), 'currentState': array([11.01740973,  2.20795526,  5.26185511]), 'targetState': array([11,  2], dtype=int32), 'currentDistance': 0.20868275052271132}
episode index:3269
target Thresh 31.999999999999837
target distance 17.0
model initialize at round 3269
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 16.]), 'previousTarget': array([ 8., 16.]), 'currentState': array([26.66049453,  7.27412773,  1.17361229]), 'targetState': array([ 8, 16], dtype=int32), 'currentDistance': 20.599876280674547}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9301668963462071
{'scaleFactor': 20, 'currentTarget': array([ 8., 16.]), 'previousTarget': array([ 8., 16.]), 'currentState': array([ 7.75325363, 15.92472281,  3.14625736]), 'targetState': array([ 8, 16], dtype=int32), 'currentDistance': 0.2579736893221542}
episode index:3270
target Thresh 31.99999999999984
target distance 10.0
model initialize at round 3270
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([23.29785679, 19.52950417,  3.01116383]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 11.361538874333112}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9301703549989294
{'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([13.7131012 , 13.08648614,  3.48049844]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 0.29965108909909743}
episode index:3271
target Thresh 31.99999999999984
target distance 12.0
model initialize at round 3271
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  8.]), 'previousTarget': array([10.,  8.]), 'currentState': array([22.49379309, 18.39110085,  4.00017643]), 'targetState': array([10,  8], dtype=int32), 'currentDistance': 16.250225925884358}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9301680855427646
{'scaleFactor': 20, 'currentTarget': array([10.,  8.]), 'previousTarget': array([10.,  8.]), 'currentState': array([10.566605  ,  8.45077888,  4.93290366]), 'targetState': array([10,  8], dtype=int32), 'currentDistance': 0.7240461488944314}
episode index:3272
target Thresh 31.999999999999844
target distance 13.0
model initialize at round 3272
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([13.33059243,  9.9388362 ,  3.44865876]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 12.360195302894635}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9301715417187066
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.59695267, 5.05679604, 4.36428997]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.5996484663118344}
episode index:3273
target Thresh 31.999999999999844
target distance 20.0
model initialize at round 3273
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  4.]), 'previousTarget': array([22.,  4.]), 'currentState': array([ 0.32710671, 20.18390578,  4.04209697]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 27.048717338571198}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9301527800421795
{'scaleFactor': 20, 'currentTarget': array([22.,  4.]), 'previousTarget': array([22.,  4.]), 'currentState': array([22.08262555,  4.2071454 ,  6.19711646]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 0.2230161400509603}
episode index:3274
target Thresh 31.999999999999844
target distance 18.0
model initialize at round 3274
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 23.]), 'previousTarget': array([15., 23.]), 'currentState': array([12.63723597,  6.8686053 ,  1.52329051]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 16.30351338926569}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9301505180313049
{'scaleFactor': 20, 'currentTarget': array([15., 23.]), 'previousTarget': array([15., 23.]), 'currentState': array([14.69282888, 22.57328943,  1.7470149 ]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 0.5257718179259997}
episode index:3275
target Thresh 31.999999999999847
target distance 7.0
model initialize at round 3275
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([9.66672163, 9.54527355, 0.15346789]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 6.4982031314337805}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9301627733676812
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.26713347, 10.97772148,  1.09893988]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.7332050761440186}
episode index:3276
target Thresh 31.999999999999847
target distance 15.0
model initialize at round 3276
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 20.]), 'previousTarget': array([ 5., 20.]), 'currentState': array([20.66047803, 27.45204753,  4.10568047]), 'targetState': array([ 5, 20], dtype=int32), 'currentDistance': 17.343113455913485}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9301576938663433
{'scaleFactor': 20, 'currentTarget': array([ 5., 20.]), 'previousTarget': array([ 5., 20.]), 'currentState': array([ 4.65487427, 19.60410355,  3.70991779]), 'targetState': array([ 5, 20], dtype=int32), 'currentDistance': 0.525210215040788}
episode index:3277
target Thresh 31.99999999999985
target distance 7.0
model initialize at round 3277
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 12.]), 'previousTarget': array([ 5., 12.]), 'currentState': array([10.48080281,  9.29006274,  2.58357036]), 'targetState': array([ 5, 12], dtype=int32), 'currentDistance': 6.1141605620965}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9301699395363048
{'scaleFactor': 20, 'currentTarget': array([ 5., 12.]), 'previousTarget': array([ 5., 12.]), 'currentState': array([ 5.13009913, 11.78263091,  3.18188845]), 'targetState': array([ 5, 12], dtype=int32), 'currentDistance': 0.25332805684741}
episode index:3278
target Thresh 31.99999999999985
target distance 9.0
model initialize at round 3278
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 26.]), 'previousTarget': array([15., 26.]), 'currentState': array([23.16042227, 16.5409454 ,  3.18090391]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 12.49264605442687}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9301733888226312
{'scaleFactor': 20, 'currentTarget': array([15., 26.]), 'previousTarget': array([15., 26.]), 'currentState': array([15.42192198, 25.22340937,  2.70215101]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 0.8838049342097384}
episode index:3279
target Thresh 31.999999999999854
target distance 22.0
model initialize at round 3279
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([ 5.64418046, 25.64075043,  0.79488235]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 21.12291301737566}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9301627683547763
{'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([26.23367559, 20.22421224,  0.51042716]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 0.32384472872921033}
episode index:3280
target Thresh 31.999999999999854
target distance 3.0
model initialize at round 3280
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([12.572411  ,  4.10805934,  4.70945144]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 4.023963286584307}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9301779884802397
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.73105822,  2.35179591,  5.50254285]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.44282055633111017}
episode index:3281
target Thresh 31.999999999999854
target distance 20.0
model initialize at round 3281
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 26.]), 'previousTarget': array([13., 26.]), 'currentState': array([13.94126599,  7.88156339,  2.20714498]), 'targetState': array([13, 26], dtype=int32), 'currentDistance': 18.142869861258077}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9301729120813985
{'scaleFactor': 20, 'currentTarget': array([13., 26.]), 'previousTarget': array([13., 26.]), 'currentState': array([12.76363458, 25.5255165 ,  2.38014185]), 'targetState': array([13, 26], dtype=int32), 'currentDistance': 0.5300973513804772}
episode index:3282
target Thresh 31.999999999999858
target distance 2.0
model initialize at round 3282
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  4.]), 'previousTarget': array([24.,  4.]), 'currentState': array([24.18813079,  4.35420301,  4.97349441]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 0.4010647884942861}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9301941813740938
{'scaleFactor': 20, 'currentTarget': array([24.,  4.]), 'previousTarget': array([24.,  4.]), 'currentState': array([24.18813079,  4.35420301,  4.97349441]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 0.4010647884942861}
episode index:3283
target Thresh 31.999999999999858
target distance 16.0
model initialize at round 3283
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([4.31103147, 4.27295749, 1.5073115 ]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 20.104663456116135}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9301863214147865
{'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([18.42464173, 17.65698188,  1.43671879]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 0.669849659606592}
episode index:3284
target Thresh 31.99999999999986
target distance 14.0
model initialize at round 3284
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([ 9.10566523, 16.57432353,  5.16213346]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 12.574767486202974}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9301897594141735
{'scaleFactor': 20, 'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([9.18089209, 4.75505452, 5.11063297]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 0.7764208072621138}
episode index:3285
target Thresh 31.99999999999986
target distance 18.0
model initialize at round 3285
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 25.]), 'previousTarget': array([22., 25.]), 'currentState': array([ 5.61887147, 26.460047  ,  5.55006367]), 'targetState': array([22, 25], dtype=int32), 'currentDistance': 16.44606667453884}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9301874937218465
{'scaleFactor': 20, 'currentTarget': array([22., 25.]), 'previousTarget': array([22., 25.]), 'currentState': array([21.16427449, 25.26221818,  6.08450404]), 'targetState': array([22, 25], dtype=int32), 'currentDistance': 0.8758969736873989}
episode index:3286
target Thresh 31.99999999999986
target distance 4.0
model initialize at round 3286
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 19.]), 'previousTarget': array([ 9., 19.]), 'currentState': array([ 3.21324506, 18.4400038 ,  2.19199753]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 5.813787791196751}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9301967448676567
{'scaleFactor': 20, 'currentTarget': array([ 9., 19.]), 'previousTarget': array([ 9., 19.]), 'currentState': array([ 9.52678579, 19.1787919 ,  0.56826976]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 0.5563001062705482}
episode index:3287
target Thresh 31.999999999999865
target distance 17.0
model initialize at round 3287
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 21.]), 'previousTarget': array([ 4., 21.]), 'currentState': array([16.0994107 ,  5.27423867,  1.86891907]), 'targetState': array([ 4, 21], dtype=int32), 'currentDistance': 19.841756693811952}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9301888936906924
{'scaleFactor': 20, 'currentTarget': array([ 4., 21.]), 'previousTarget': array([ 4., 21.]), 'currentState': array([ 3.98944632, 20.71351275,  3.05621149]), 'targetState': array([ 4, 21], dtype=int32), 'currentDistance': 0.2866815779518845}
episode index:3288
target Thresh 31.999999999999865
target distance 9.0
model initialize at round 3288
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 23.]), 'previousTarget': array([ 8., 23.]), 'currentState': array([15.25375319, 22.25045067,  3.86456788]), 'targetState': array([ 8, 23], dtype=int32), 'currentDistance': 7.2923768126047825}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.930198138785344
{'scaleFactor': 20, 'currentTarget': array([ 8., 23.]), 'previousTarget': array([ 8., 23.]), 'currentState': array([ 7.59079758, 22.79007892,  3.46958842]), 'targetState': array([ 8, 23], dtype=int32), 'currentDistance': 0.4599059476420605}
episode index:3289
target Thresh 31.99999999999987
target distance 6.0
model initialize at round 3289
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 29.]), 'previousTarget': array([12., 29.]), 'currentState': array([17.01140739, 24.36202594,  3.20862424]), 'targetState': array([12, 29], dtype=int32), 'currentDistance': 6.828250682867264}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9302103274969593
{'scaleFactor': 20, 'currentTarget': array([12., 29.]), 'previousTarget': array([12., 29.]), 'currentState': array([12.72883628, 28.17900741,  2.49844104]), 'targetState': array([12, 29], dtype=int32), 'currentDistance': 1.0978302010773497}
episode index:3290
target Thresh 31.99999999999987
target distance 18.0
model initialize at round 3290
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 11.]), 'previousTarget': array([27., 11.]), 'currentState': array([ 9.9884127 , 11.36214112,  6.21625954]), 'targetState': array([27, 11], dtype=int32), 'currentDistance': 17.015441479083687}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9302052551542023
{'scaleFactor': 20, 'currentTarget': array([27., 11.]), 'previousTarget': array([27., 11.]), 'currentState': array([27.82445277, 11.61312603,  0.31501806]), 'targetState': array([27, 11], dtype=int32), 'currentDistance': 1.0274462993143745}
episode index:3291
target Thresh 31.99999999999987
target distance 9.0
model initialize at round 3291
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 26.]), 'previousTarget': array([17., 26.]), 'currentState': array([17.05918885, 18.71624147,  1.70911652]), 'targetState': array([17, 26], dtype=int32), 'currentDistance': 7.2839990146552624}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9302144868537301
{'scaleFactor': 20, 'currentTarget': array([17., 26.]), 'previousTarget': array([17., 26.]), 'currentState': array([16.75529639, 26.51658004,  2.31349238]), 'targetState': array([17, 26], dtype=int32), 'currentDistance': 0.5716072041385463}
episode index:3292
target Thresh 31.999999999999872
target distance 15.0
model initialize at round 3292
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 11.]), 'previousTarget': array([24., 11.]), 'currentState': array([9.29977482, 2.3439435 , 5.90146708]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 17.059423627228895}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9302094163285645
{'scaleFactor': 20, 'currentTarget': array([24., 11.]), 'previousTarget': array([24., 11.]), 'currentState': array([23.89046051, 11.12387389,  1.43366781]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 0.16535912781924933}
episode index:3293
target Thresh 31.999999999999872
target distance 19.0
model initialize at round 3293
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 7.]), 'previousTarget': array([8., 7.]), 'currentState': array([23.03737051, 24.61948664,  5.11343646]), 'targetState': array([8, 7], dtype=int32), 'currentDistance': 23.163955218139197}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9301961119737945
{'scaleFactor': 20, 'currentTarget': array([8., 7.]), 'previousTarget': array([8., 7.]), 'currentState': array([8.01263253, 6.78107164, 4.70340809]), 'targetState': array([8, 7], dtype=int32), 'currentDistance': 0.21929252086693152}
episode index:3294
target Thresh 31.999999999999872
target distance 12.0
model initialize at round 3294
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 28.]), 'previousTarget': array([11., 28.]), 'currentState': array([21.02903273, 23.15988056,  2.79744422]), 'targetState': array([11, 28], dtype=int32), 'currentDistance': 11.13589932548611}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9301995365678543
{'scaleFactor': 20, 'currentTarget': array([11., 28.]), 'previousTarget': array([11., 28.]), 'currentState': array([10.37043546, 28.12841667,  3.33139739]), 'targetState': array([11, 28], dtype=int32), 'currentDistance': 0.642528091436322}
episode index:3295
target Thresh 31.999999999999876
target distance 11.0
model initialize at round 3295
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 24.]), 'previousTarget': array([ 8., 24.]), 'currentState': array([11.35220491, 13.34228561,  1.92678189]), 'targetState': array([ 8, 24], dtype=int32), 'currentDistance': 11.17247304265956}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9302029590838837
{'scaleFactor': 20, 'currentTarget': array([ 8., 24.]), 'previousTarget': array([ 8., 24.]), 'currentState': array([ 7.71590848, 24.45084324,  2.48739302]), 'targetState': array([ 8, 24], dtype=int32), 'currentDistance': 0.5328861204034484}
episode index:3296
target Thresh 31.999999999999876
target distance 11.0
model initialize at round 3296
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 14.]), 'previousTarget': array([ 3., 14.]), 'currentState': array([3.34984189, 2.66929426, 2.32938099]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 11.33610523867195}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9302063795237736
{'scaleFactor': 20, 'currentTarget': array([ 3., 14.]), 'previousTarget': array([ 3., 14.]), 'currentState': array([ 2.74482348, 14.23030857,  2.16196236]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 0.3437398635516764}
episode index:3297
target Thresh 31.999999999999876
target distance 5.0
model initialize at round 3297
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 19.]), 'previousTarget': array([ 9., 19.]), 'currentState': array([12.0133507 , 19.78118513,  3.18732738]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 3.11296203597579}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.930221507971462
{'scaleFactor': 20, 'currentTarget': array([ 9., 19.]), 'previousTarget': array([ 9., 19.]), 'currentState': array([ 8.52982189, 19.67987898,  1.85082763]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 0.8266213610264933}
episode index:3298
target Thresh 31.99999999999988
target distance 11.0
model initialize at round 3298
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 27.]), 'previousTarget': array([ 2., 27.]), 'currentState': array([11.98526422, 22.65735359,  3.05520344]), 'targetState': array([ 2, 27], dtype=int32), 'currentDistance': 10.88871339487326}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9302278033767147
{'scaleFactor': 20, 'currentTarget': array([ 2., 27.]), 'previousTarget': array([ 2., 27.]), 'currentState': array([ 2.82624031, 26.40593343,  2.77146982]), 'targetState': array([ 2, 27], dtype=int32), 'currentDistance': 1.0176385061492494}
episode index:3299
target Thresh 31.99999999999988
target distance 15.0
model initialize at round 3299
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 22.]), 'previousTarget': array([15., 22.]), 'currentState': array([19.4110401 ,  8.91095922,  1.90203374]), 'targetState': array([15, 22], dtype=int32), 'currentDistance': 13.81232287988917}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9302283602083906
{'scaleFactor': 20, 'currentTarget': array([15., 22.]), 'previousTarget': array([15., 22.]), 'currentState': array([14.76915454, 21.97666232,  2.48676497]), 'targetState': array([15, 22], dtype=int32), 'currentDistance': 0.2320221422246773}
episode index:3300
target Thresh 31.99999999999988
target distance 9.0
model initialize at round 3300
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'previousTarget': array([26., 11.]), 'currentState': array([24.65487954, 19.69379453,  5.09022272]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 8.797238910795814}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9302375597387728
{'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'previousTarget': array([26., 11.]), 'currentState': array([26.08640266, 11.92639051,  5.29238422]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 0.9304110905257542}
episode index:3301
target Thresh 31.999999999999883
target distance 23.0
model initialize at round 3301
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 29.]), 'previousTarget': array([ 3., 29.]), 'currentState': array([24.77676705, 17.15589326,  3.39449131]), 'targetState': array([ 3, 29], dtype=int32), 'currentDistance': 24.78932123951557}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9302215947064469
{'scaleFactor': 20, 'currentTarget': array([ 3., 29.]), 'previousTarget': array([ 3., 29.]), 'currentState': array([ 2.24928112, 29.08621914,  3.05492979]), 'targetState': array([ 3, 29], dtype=int32), 'currentDistance': 0.7556537440142487}
episode index:3302
target Thresh 31.999999999999883
target distance 8.0
model initialize at round 3302
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([12.71723143, 27.65904491,  2.74961466]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 11.76513266522613}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9302250032909745
{'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([ 6.5526345 , 18.28124223,  4.18874916]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 0.6200823166783185}
episode index:3303
target Thresh 31.999999999999883
target distance 13.0
model initialize at round 3303
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 5.]), 'previousTarget': array([8., 5.]), 'currentState': array([20.77759242, 16.66820951,  2.71333587]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 17.303582897143865}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9302199464641563
{'scaleFactor': 20, 'currentTarget': array([8., 5.]), 'previousTarget': array([8., 5.]), 'currentState': array([8.47398241, 4.86303217, 4.59554558]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 0.49337563422332614}
episode index:3304
target Thresh 31.999999999999886
target distance 14.0
model initialize at round 3304
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 29.]), 'previousTarget': array([ 4., 29.]), 'currentState': array([19.22045966, 23.15882109,  1.76949757]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 16.302814579850388}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.930214892697445
{'scaleFactor': 20, 'currentTarget': array([ 4., 29.]), 'previousTarget': array([ 4., 29.]), 'currentState': array([ 3.0722283 , 29.08326439,  2.66579964]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 0.9315005521844305}
episode index:3305
target Thresh 31.999999999999886
target distance 16.0
model initialize at round 3305
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 23.]), 'previousTarget': array([27., 23.]), 'currentState': array([13.12435073,  5.32163026,  5.79634404]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 22.47350438670067}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9302016349778499
{'scaleFactor': 20, 'currentTarget': array([27., 23.]), 'previousTarget': array([27., 23.]), 'currentState': array([27.61518979, 23.01122803,  5.82185983]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 0.6152922491518655}
episode index:3306
target Thresh 31.999999999999886
target distance 3.0
model initialize at round 3306
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([20.40299549,  6.42191421,  3.3478961 ]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 1.477919279430991}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9302197173380017
{'scaleFactor': 20, 'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([19.45061995,  4.90872385,  4.96960986]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 0.556910918641695}
episode index:3307
target Thresh 31.99999999999989
target distance 21.0
model initialize at round 3307
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 15.]), 'previousTarget': array([27., 15.]), 'currentState': array([7.7364412 , 6.98475304, 0.62321655]), 'targetState': array([27, 15], dtype=int32), 'currentDistance': 20.86453645863447}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9302091727602873
{'scaleFactor': 20, 'currentTarget': array([27., 15.]), 'previousTarget': array([27., 15.]), 'currentState': array([27.59517959, 14.85203002,  5.26014452]), 'targetState': array([27, 15], dtype=int32), 'currentDistance': 0.6132975273641892}
episode index:3308
target Thresh 31.99999999999989
target distance 19.0
model initialize at round 3308
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([11.21851619,  4.66938713,  0.43174386]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 18.26952921154824}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9302041283585717
{'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([16.40827609, 21.34843428,  1.67336642]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 0.8801562724292658}
episode index:3309
target Thresh 31.99999999999989
target distance 2.0
model initialize at round 3309
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 24.]), 'previousTarget': array([24., 24.]), 'currentState': array([24.56978209, 25.39322798,  4.90434236]), 'targetState': array([24, 24], dtype=int32), 'currentDistance': 1.5052361374887657}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9302221935765902
{'scaleFactor': 20, 'currentTarget': array([24., 24.]), 'previousTarget': array([24., 24.]), 'currentState': array([24.04206608, 23.54106785,  3.95582152]), 'targetState': array([24, 24], dtype=int32), 'currentDistance': 0.46085602049409496}
episode index:3310
target Thresh 31.999999999999893
target distance 21.0
model initialize at round 3310
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([23.58783406, 20.57697153,  2.22398974]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 26.048467867880255}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9302036262613356
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.40467542, 5.40394282, 3.63253041]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.8424342795371054}
episode index:3311
target Thresh 31.999999999999893
target distance 8.0
model initialize at round 3311
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 23.]), 'previousTarget': array([13., 23.]), 'currentState': array([16.04213043, 16.32149622,  1.91304606]), 'targetState': array([13, 23], dtype=int32), 'currentDistance': 7.338730835758396}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9302128027057011
{'scaleFactor': 20, 'currentTarget': array([13., 23.]), 'previousTarget': array([13., 23.]), 'currentState': array([13.20574639, 23.28841919,  0.86514495]), 'targetState': array([13, 23], dtype=int32), 'currentDistance': 0.3542840716955255}
episode index:3312
target Thresh 31.999999999999893
target distance 19.0
model initialize at round 3312
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 8.]), 'previousTarget': array([5., 8.]), 'currentState': array([23.62875125, 19.64151225,  2.80321711]), 'targetState': array([5, 8], dtype=int32), 'currentDistance': 21.967138653224698}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9302022761290494
{'scaleFactor': 20, 'currentTarget': array([5., 8.]), 'previousTarget': array([5., 8.]), 'currentState': array([5.92297693, 8.33108881, 4.63106544]), 'targetState': array([5, 8], dtype=int32), 'currentDistance': 0.9805642356070389}
episode index:3313
target Thresh 31.999999999999897
target distance 12.0
model initialize at round 3313
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([ 4.59136945, 16.54765991,  5.60463351]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 13.299616254642952}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9302028383112395
{'scaleFactor': 20, 'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([9.66393478, 3.73256032, 4.89260334]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 0.7157746675293839}
episode index:3314
target Thresh 31.999999999999897
target distance 3.0
model initialize at round 3314
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 14.]), 'previousTarget': array([12., 14.]), 'currentState': array([10.56044428, 15.11324006,  0.28840869]), 'targetState': array([12, 14], dtype=int32), 'currentDistance': 1.8197868262461478}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9302208766707233
{'scaleFactor': 20, 'currentTarget': array([12., 14.]), 'previousTarget': array([12., 14.]), 'currentState': array([11.8560807 , 14.032902  ,  4.58484876]), 'targetState': array([12, 14], dtype=int32), 'currentDistance': 0.147632334356972}
episode index:3315
target Thresh 31.999999999999897
target distance 13.0
model initialize at round 3315
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 23.]), 'previousTarget': array([23., 23.]), 'currentState': array([11.78406105, 22.0634394 ,  0.86000686]), 'targetState': array([23, 23], dtype=int32), 'currentDistance': 11.254973665351486}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9302242721088204
{'scaleFactor': 20, 'currentTarget': array([23., 23.]), 'previousTarget': array([23., 23.]), 'currentState': array([23.41861104, 23.49838838,  6.19748025]), 'targetState': array([23, 23], dtype=int32), 'currentDistance': 0.65086571696752}
episode index:3316
target Thresh 31.999999999999897
target distance 4.0
model initialize at round 3316
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  8.]), 'previousTarget': array([19.,  8.]), 'currentState': array([21.3861225 ,  8.49848429,  2.44473767]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 2.437635571871436}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9302422931301924
{'scaleFactor': 20, 'currentTarget': array([19.,  8.]), 'previousTarget': array([19.,  8.]), 'currentState': array([19.62310923,  8.27404855,  4.10832608]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 0.680711188529895}
episode index:3317
target Thresh 31.9999999999999
target distance 24.0
model initialize at round 3317
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 16.]), 'previousTarget': array([ 2., 16.]), 'currentState': array([24.31704121, 29.00614743,  4.14793992]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 25.830412292245875}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.930226403657579
{'scaleFactor': 20, 'currentTarget': array([ 2., 16.]), 'previousTarget': array([ 2., 16.]), 'currentState': array([ 2.04843823, 15.97144269,  3.32964907]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 0.05622972454498431}
episode index:3318
target Thresh 31.9999999999999
target distance 21.0
model initialize at round 3318
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([12.63729543, 27.04233792,  4.69547087]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 19.0910344759729}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.930218616875823
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.61155752,  7.69987974,  3.33144607]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.4908764906822618}
episode index:3319
target Thresh 31.9999999999999
target distance 12.0
model initialize at round 3319
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 19.]), 'previousTarget': array([17., 19.]), 'currentState': array([ 4.38735693, 16.5675001 ,  0.93338203]), 'targetState': array([17, 19], dtype=int32), 'currentDistance': 12.845069910610526}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9302191731201095
{'scaleFactor': 20, 'currentTarget': array([17., 19.]), 'previousTarget': array([17., 19.]), 'currentState': array([17.40801299, 18.93525098,  5.22411446]), 'targetState': array([17, 19], dtype=int32), 'currentDistance': 0.4131186717396501}
episode index:3320
target Thresh 31.999999999999904
target distance 5.0
model initialize at round 3320
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([ 2.93963895, 26.60376691,  0.03156524]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 4.365615668548608}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9302341929415127
{'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([6.65621792e+00, 2.53612272e+01, 2.30764111e-02]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 0.4986694191969445}
episode index:3321
target Thresh 31.999999999999904
target distance 19.0
model initialize at round 3321
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  6.]), 'previousTarget': array([17.,  6.]), 'currentState': array([ 9.542713  , 25.67262522,  5.68433732]), 'targetState': array([17,  6], dtype=int32), 'currentDistance': 21.03861479670218}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9302236884446183
{'scaleFactor': 20, 'currentTarget': array([17.,  6.]), 'previousTarget': array([17.,  6.]), 'currentState': array([17.34317821,  5.90202   ,  6.21960644]), 'targetState': array([17,  6], dtype=int32), 'currentDistance': 0.3568912448802686}
episode index:3322
target Thresh 31.999999999999904
target distance 10.0
model initialize at round 3322
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([22.70635292, 22.65715403,  2.75617546]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 13.448583926157488}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9302242426605265
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.49676105, 11.03497733,  4.7608462 ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.4979909198527573}
episode index:3323
target Thresh 31.999999999999904
target distance 18.0
model initialize at round 3323
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([3.67961399, 6.89376944, 0.94683712]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 18.254308431675234}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9302192164886922
{'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([ 5.42422633, 24.45000272,  1.65041683]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 0.7962489131499331}
episode index:3324
target Thresh 31.999999999999908
target distance 16.0
model initialize at round 3324
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 25.]), 'previousTarget': array([22., 25.]), 'currentState': array([ 5.46681982, 10.4037209 ,  5.40002561]), 'targetState': array([22, 25], dtype=int32), 'currentDistance': 22.054419298434507}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9302087259737358
{'scaleFactor': 20, 'currentTarget': array([22., 25.]), 'previousTarget': array([22., 25.]), 'currentState': array([21.59485051, 24.04827854,  0.8068977 ]), 'targetState': array([22, 25], dtype=int32), 'currentDistance': 1.0343692929215438}
episode index:3325
target Thresh 31.999999999999908
target distance 20.0
model initialize at round 3325
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([21.7502851 ,  6.12720927,  3.41768849]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 18.870564657951387}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9302009608952737
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.57342325, 4.22222394, 2.05151233]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.4809898168852515}
episode index:3326
target Thresh 31.999999999999908
target distance 6.0
model initialize at round 3326
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([19.43014851, 12.00255002,  4.36943936]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 4.86173039354421}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9302130132063963
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.26728648,  9.37527282,  3.77773571]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.9628879244101267}
episode index:3327
target Thresh 31.999999999999908
target distance 20.0
model initialize at round 3327
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([21.52573696, 24.28970793,  4.83748347]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 20.70578539843872}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9302025340120009
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.71176861,  4.56786978,  3.33817917]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.5194361051626019}
episode index:3328
target Thresh 31.99999999999991
target distance 2.0
model initialize at round 3328
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 18.]), 'previousTarget': array([14., 18.]), 'currentState': array([14.33064816, 15.78633549,  2.25889301]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 2.2382222336685733}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9302204966031657
{'scaleFactor': 20, 'currentTarget': array([14., 18.]), 'previousTarget': array([14., 18.]), 'currentState': array([14.11998789, 17.66349051,  1.09456754]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 0.35726143213725825}
episode index:3329
target Thresh 31.99999999999991
target distance 6.0
model initialize at round 3329
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 16.]), 'previousTarget': array([ 8., 16.]), 'currentState': array([13.47939982, 21.19765078,  4.77623609]), 'targetState': array([ 8, 16], dtype=int32), 'currentDistance': 7.552443046837885}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9302296183789606
{'scaleFactor': 20, 'currentTarget': array([ 8., 16.]), 'previousTarget': array([ 8., 16.]), 'currentState': array([ 7.97550851, 16.3031126 ,  2.81625051]), 'targetState': array([ 8, 16], dtype=int32), 'currentDistance': 0.304100448646749}
episode index:3330
target Thresh 31.99999999999991
target distance 7.0
model initialize at round 3330
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 23.]), 'previousTarget': array([10., 23.]), 'currentState': array([18.64819732, 16.34034348,  1.21363228]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 10.915234394451836}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9302329959025336
{'scaleFactor': 20, 'currentTarget': array([10., 23.]), 'previousTarget': array([10., 23.]), 'currentState': array([ 9.98182106, 23.30543423,  1.53520674]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 0.30597474183001033}
episode index:3331
target Thresh 31.99999999999991
target distance 10.0
model initialize at round 3331
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 18.]), 'previousTarget': array([14., 18.]), 'currentState': array([9.67724367, 8.13871462, 1.09251612]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 10.767133866230173}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9302392255105761
{'scaleFactor': 20, 'currentTarget': array([14., 18.]), 'previousTarget': array([14., 18.]), 'currentState': array([13.46118828, 17.28867297,  1.74064655]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 0.8923587905245033}
episode index:3332
target Thresh 31.999999999999915
target distance 14.0
model initialize at round 3332
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 21.]), 'previousTarget': array([10., 21.]), 'currentState': array([7.49505492, 8.71687063, 2.1112836 ]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 12.535948986612874}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9302425981250046
{'scaleFactor': 20, 'currentTarget': array([10., 21.]), 'previousTarget': array([10., 21.]), 'currentState': array([ 9.62068634, 20.16938529,  1.21764366]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 0.9131263022754372}
episode index:3333
target Thresh 31.999999999999915
target distance 2.0
model initialize at round 3333
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 24.]), 'previousTarget': array([17., 24.]), 'currentState': array([19.41881117, 21.36794308,  3.95688701]), 'targetState': array([17, 24], dtype=int32), 'currentDistance': 3.5746847568707643}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9302575523547213
{'scaleFactor': 20, 'currentTarget': array([17., 24.]), 'previousTarget': array([17., 24.]), 'currentState': array([17.26352501, 23.59122686,  1.68679857]), 'targetState': array([17, 24], dtype=int32), 'currentDistance': 0.48635471648981976}
episode index:3334
target Thresh 31.999999999999915
target distance 9.0
model initialize at round 3334
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([23.46789861, 15.30354223,  2.55824614]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 7.474064995995214}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9302666493435203
{'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([15.92864162, 14.91373855,  3.62537581]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 0.11195113802971392}
episode index:3335
target Thresh 31.999999999999915
target distance 7.0
model initialize at round 3335
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 26.]), 'previousTarget': array([20., 26.]), 'currentState': array([14.26200318, 22.88656565,  0.28706425]), 'targetState': array([20, 26], dtype=int32), 'currentDistance': 6.52825251984765}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9302786494486333
{'scaleFactor': 20, 'currentTarget': array([20., 26.]), 'previousTarget': array([20., 26.]), 'currentState': array([19.55140312, 25.60520215,  0.6273553 ]), 'targetState': array([20, 26], dtype=int32), 'currentDistance': 0.597582212498979}
episode index:3336
target Thresh 31.99999999999992
target distance 18.0
model initialize at round 3336
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 14.]), 'previousTarget': array([25., 14.]), 'currentState': array([ 8.78060642, 27.17666734,  5.50690448]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 20.897207713530957}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9302708890127808
{'scaleFactor': 20, 'currentTarget': array([25., 14.]), 'previousTarget': array([25., 14.]), 'currentState': array([24.06832656, 14.79982459,  6.09010878]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 1.2278985193651635}
episode index:3337
target Thresh 31.99999999999992
target distance 12.0
model initialize at round 3337
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  8.]), 'previousTarget': array([11.,  8.]), 'currentState': array([19.27581057, 18.98987208,  3.59932551]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 13.757409958634485}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.93027142659783
{'scaleFactor': 20, 'currentTarget': array([11.,  8.]), 'previousTarget': array([11.,  8.]), 'currentState': array([11.13275215,  7.77471081,  4.20910067]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 0.26149254648344405}
episode index:3338
target Thresh 31.99999999999992
target distance 5.0
model initialize at round 3338
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  7.]), 'previousTarget': array([17.,  7.]), 'currentState': array([15.64633788,  2.34922726,  1.21902531]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 4.843767979089317}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9302863498004064
{'scaleFactor': 20, 'currentTarget': array([17.,  7.]), 'previousTarget': array([17.,  7.]), 'currentState': array([16.59988224,  6.05661151,  2.04827059]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 1.0247321904638378}
episode index:3339
target Thresh 31.99999999999992
target distance 13.0
model initialize at round 3339
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 19.]), 'previousTarget': array([23., 19.]), 'currentState': array([25.10822476,  7.97982979,  1.74601993]), 'targetState': array([23, 19], dtype=int32), 'currentDistance': 11.220016180046814}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9302897012374125
{'scaleFactor': 20, 'currentTarget': array([23., 19.]), 'previousTarget': array([23., 19.]), 'currentState': array([22.66462302, 19.42734733,  2.41895708]), 'targetState': array([23, 19], dtype=int32), 'currentDistance': 0.5432342619452816}
episode index:3340
target Thresh 31.99999999999992
target distance 22.0
model initialize at round 3340
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 27.]), 'previousTarget': array([17., 27.]), 'currentState': array([8.190139  , 3.32780526, 5.83560896]), 'targetState': array([17, 27], dtype=int32), 'currentDistance': 25.25839374687413}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9302739069607773
{'scaleFactor': 20, 'currentTarget': array([17., 27.]), 'previousTarget': array([17., 27.]), 'currentState': array([16.82559531, 26.82130142,  2.01330685]), 'targetState': array([17, 27], dtype=int32), 'currentDistance': 0.2497001813378016}
episode index:3341
target Thresh 31.999999999999922
target distance 22.0
model initialize at round 3341
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([25.47240282, 14.40186693,  3.38352108]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 21.476163059611753}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.93026345344411
{'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([ 3.58256361, 13.82053612,  3.24573771]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 0.4543791591892134}
episode index:3342
target Thresh 31.999999999999922
target distance 10.0
model initialize at round 3342
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 16.]), 'previousTarget': array([12., 16.]), 'currentState': array([14.93028079, 24.59751444,  4.28806319]), 'targetState': array([12, 16], dtype=int32), 'currentDistance': 9.083160248950836}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9302696534430498
{'scaleFactor': 20, 'currentTarget': array([12., 16.]), 'previousTarget': array([12., 16.]), 'currentState': array([12.27143223, 15.2200126 ,  4.3199384 ]), 'targetState': array([12, 16], dtype=int32), 'currentDistance': 0.8258666985829948}
episode index:3343
target Thresh 31.999999999999922
target distance 25.0
model initialize at round 3343
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([ 3.20204009, 27.16613577,  4.31440512]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 23.197300420748892}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9302565300035381
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.88781839, 4.23206748, 5.07346101]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.25775963170374383}
episode index:3344
target Thresh 31.999999999999922
target distance 14.0
model initialize at round 3344
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 12.]), 'previousTarget': array([23., 12.]), 'currentState': array([10.7633587 , 15.7343203 ,  6.14977163]), 'targetState': array([23, 12], dtype=int32), 'currentDistance': 12.793769510689922}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.93025988534566
{'scaleFactor': 20, 'currentTarget': array([23., 12.]), 'previousTarget': array([23., 12.]), 'currentState': array([22.06856792, 12.29515977,  0.45790257]), 'targetState': array([23, 12], dtype=int32), 'currentDistance': 0.9770798362081544}
episode index:3345
target Thresh 31.999999999999925
target distance 16.0
model initialize at round 3345
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 13.]), 'previousTarget': array([24., 13.]), 'currentState': array([ 9.27016109, 26.10411906,  5.98876352]), 'targetState': array([24, 13], dtype=int32), 'currentDistance': 19.715123399255845}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9302521513915843
{'scaleFactor': 20, 'currentTarget': array([24., 13.]), 'previousTarget': array([24., 13.]), 'currentState': array([23.72309351, 13.042733  ,  5.56559919]), 'targetState': array([24, 13], dtype=int32), 'currentDistance': 0.2801844276381232}
episode index:3346
target Thresh 31.999999999999925
target distance 9.0
model initialize at round 3346
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([21.51821934,  9.38746303,  3.41205931]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 8.669506112597603}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.93026121737862
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.94535875, 10.77706829,  3.66804507]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.9712886856614389}
episode index:3347
target Thresh 31.999999999999925
target distance 8.0
model initialize at round 3347
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  5.]), 'previousTarget': array([27.,  5.]), 'currentState': array([20.75732115,  3.91175974,  0.22938442]), 'targetState': array([27,  5], dtype=int32), 'currentDistance': 6.336821451700863}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9302731760950541
{'scaleFactor': 20, 'currentTarget': array([27.,  5.]), 'previousTarget': array([27.,  5.]), 'currentState': array([26.573755  ,  4.88804282,  0.69794713]), 'targetState': array([27,  5], dtype=int32), 'currentDistance': 0.44070308342542464}
episode index:3348
target Thresh 31.999999999999925
target distance 10.0
model initialize at round 3348
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 14.]), 'previousTarget': array([17., 14.]), 'currentState': array([25.21655262, 16.48280889,  4.09288359]), 'targetState': array([17, 14], dtype=int32), 'currentDistance': 8.58347697041786}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9302822303900391
{'scaleFactor': 20, 'currentTarget': array([17., 14.]), 'previousTarget': array([17., 14.]), 'currentState': array([17.95922848, 14.1923324 ,  4.16579856]), 'targetState': array([17, 14], dtype=int32), 'currentDistance': 0.9783205185271202}
episode index:3349
target Thresh 31.999999999999925
target distance 15.0
model initialize at round 3349
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 14.]), 'previousTarget': array([ 2., 14.]), 'currentState': array([15.48259115,  8.72791377,  3.7043097 ]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 14.476710857407966}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9302827626639248
{'scaleFactor': 20, 'currentTarget': array([ 2., 14.]), 'previousTarget': array([ 2., 14.]), 'currentState': array([ 2.95472709, 13.39373851,  3.4118628 ]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 1.1309539387324803}
episode index:3350
target Thresh 31.99999999999993
target distance 23.0
model initialize at round 3350
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 27.]), 'previousTarget': array([14., 27.]), 'currentState': array([22.24523218,  5.66500729,  2.43456176]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 22.87281722370738}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9302643989068687
{'scaleFactor': 20, 'currentTarget': array([14., 27.]), 'previousTarget': array([14., 27.]), 'currentState': array([14.65951536, 27.78392915,  4.43783611]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 1.0244537192082763}
episode index:3351
target Thresh 31.99999999999993
target distance 15.0
model initialize at round 3351
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 8.]), 'previousTarget': array([9., 8.]), 'currentState': array([20.72810052, 21.07330122,  4.1631912 ]), 'targetState': array([9, 8], dtype=int32), 'currentDistance': 17.563016438773925}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9302594027399763
{'scaleFactor': 20, 'currentTarget': array([9., 8.]), 'previousTarget': array([9., 8.]), 'currentState': array([9.23076574, 7.97205659, 3.75058872]), 'targetState': array([9, 8], dtype=int32), 'currentDistance': 0.23245141171364334}
episode index:3352
target Thresh 31.99999999999993
target distance 7.0
model initialize at round 3352
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 24.]), 'previousTarget': array([14., 24.]), 'currentState': array([ 7.01260978, 18.36288247,  1.18776798]), 'targetState': array([14, 24], dtype=int32), 'currentDistance': 8.977790157335052}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9302655854560992
{'scaleFactor': 20, 'currentTarget': array([14., 24.]), 'previousTarget': array([14., 24.]), 'currentState': array([14.4483453 , 24.49458007,  0.02717668]), 'targetState': array([14, 24], dtype=int32), 'currentDistance': 0.6675499635240513}
episode index:3353
target Thresh 31.99999999999993
target distance 13.0
model initialize at round 3353
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  3.]), 'previousTarget': array([10.,  3.]), 'currentState': array([24.10985819,  4.26514935,  1.86069077]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 14.166463953542642}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9302661220579034
{'scaleFactor': 20, 'currentTarget': array([10.,  3.]), 'previousTarget': array([10.,  3.]), 'currentState': array([10.9726474 ,  3.03112658,  4.03157792]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 0.9731453324235448}
episode index:3354
target Thresh 31.99999999999993
target distance 2.0
model initialize at round 3354
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 6.87206872, 13.43941114,  0.01609576]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 3.619637820062128}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9302809756727892
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 8.14917715, 10.19300752,  5.49083144]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.2439379522430002}
episode index:3355
target Thresh 31.999999999999932
target distance 12.0
model initialize at round 3355
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 28.]), 'previousTarget': array([18., 28.]), 'currentState': array([21.02273348, 17.33656437,  1.92764061]), 'targetState': array([18, 28], dtype=int32), 'currentDistance': 11.083581418903933}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9302843127328989
{'scaleFactor': 20, 'currentTarget': array([18., 28.]), 'previousTarget': array([18., 28.]), 'currentState': array([17.56004826, 28.48544126,  2.59206381]), 'targetState': array([18, 28], dtype=int32), 'currentDistance': 0.6551417793278228}
episode index:3356
target Thresh 31.999999999999932
target distance 17.0
model initialize at round 3356
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 11.]), 'previousTarget': array([24., 11.]), 'currentState': array([ 6.15776641, 15.54293766,  5.19827652]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 18.41150678582526}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9302793180753924
{'scaleFactor': 20, 'currentTarget': array([24., 11.]), 'previousTarget': array([24., 11.]), 'currentState': array([23.2180031 , 11.4568874 ,  6.15259533]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 0.9056849549391502}
episode index:3357
target Thresh 31.999999999999932
target distance 11.0
model initialize at round 3357
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([11.06302728,  3.45497911,  2.73311529]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 11.17925587546787}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9302854856548518
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.9314347 , 9.16636926, 2.34177671]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.250004322552628}
episode index:3358
target Thresh 31.999999999999932
target distance 6.0
model initialize at round 3358
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 12.]), 'previousTarget': array([ 4., 12.]), 'currentState': array([ 4.68125105, 16.46107665,  4.11913919]), 'targetState': array([ 4, 12], dtype=int32), 'currentDistance': 4.512793795424282}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9303003158169076
{'scaleFactor': 20, 'currentTarget': array([ 4., 12.]), 'previousTarget': array([ 4., 12.]), 'currentState': array([ 4.6302098 , 12.63084139,  4.27927548]), 'targetState': array([ 4, 12], dtype=int32), 'currentDistance': 0.8916979583888883}
episode index:3359
target Thresh 31.999999999999932
target distance 11.0
model initialize at round 3359
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 23.]), 'previousTarget': array([15., 23.]), 'currentState': array([17.50270224, 13.84270503,  2.36689699]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 9.493132765121182}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9303064734758609
{'scaleFactor': 20, 'currentTarget': array([15., 23.]), 'previousTarget': array([15., 23.]), 'currentState': array([14.83454008, 23.28618417,  2.06507646]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 0.33057278344707697}
episode index:3360
target Thresh 31.999999999999936
target distance 27.0
model initialize at round 3360
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([22.88788851, 27.29322463,  4.70670307]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 25.31766190595618}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9302907681945528
{'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([24.07469593,  2.09341397,  5.01767017]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 0.11960623851799863}
episode index:3361
target Thresh 31.999999999999936
target distance 16.0
model initialize at round 3361
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([ 8.05130482, 14.99565668,  1.43888   ]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 14.139273133867365}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9302912960290896
{'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([ 9.81532293, 28.78253931,  1.94340473]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 0.2852976874859204}
episode index:3362
target Thresh 31.999999999999936
target distance 10.0
model initialize at round 3362
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 12.]), 'previousTarget': array([12., 12.]), 'currentState': array([11.1235101 , 20.56328376,  5.17460179]), 'targetState': array([12, 12], dtype=int32), 'currentDistance': 8.608023198300499}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9303003072434728
{'scaleFactor': 20, 'currentTarget': array([12., 12.]), 'previousTarget': array([12., 12.]), 'currentState': array([12.06769811, 12.75299782,  5.02857734]), 'targetState': array([12, 12], dtype=int32), 'currentDistance': 0.7560348905152321}
episode index:3363
target Thresh 31.999999999999936
target distance 22.0
model initialize at round 3363
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 6.]), 'previousTarget': array([7., 6.]), 'currentState': array([ 7.38719202, 26.43256437,  5.34969807]), 'targetState': array([7, 6], dtype=int32), 'currentDistance': 20.436232633003346}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.93028991424318
{'scaleFactor': 20, 'currentTarget': array([7., 6.]), 'previousTarget': array([7., 6.]), 'currentState': array([7.08239819, 5.39714249, 5.38187483]), 'targetState': array([7, 6], dtype=int32), 'currentDistance': 0.6084625183796616}
episode index:3364
target Thresh 31.999999999999936
target distance 11.0
model initialize at round 3364
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([23.91847046,  5.31867298,  3.65449357]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 11.24288716626649}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9302932397216815
{'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([12.59166692,  7.92214347,  3.39465789]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 0.4156892430593745}
episode index:3365
target Thresh 31.999999999999936
target distance 3.0
model initialize at round 3365
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([11.52899699, 10.86961412,  3.57356191]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.534546276257315}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9303109779154659
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.1002859 , 11.75995034,  1.57506752]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.7665388351608532}
episode index:3366
target Thresh 31.99999999999994
target distance 23.0
model initialize at round 3366
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 5.]), 'previousTarget': array([9., 5.]), 'currentState': array([ 1.03549696, 26.62082562,  5.11211038]), 'targetState': array([9, 5], dtype=int32), 'currentDistance': 23.041124304506344}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9302979318488785
{'scaleFactor': 20, 'currentTarget': array([9., 5.]), 'previousTarget': array([9., 5.]), 'currentState': array([8.03529994, 4.64640729, 3.262959  ]), 'targetState': array([9, 5], dtype=int32), 'currentDistance': 1.02745997988943}
episode index:3367
target Thresh 31.99999999999994
target distance 3.0
model initialize at round 3367
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 15.]), 'previousTarget': array([ 2., 15.]), 'currentState': array([ 1.60968645, 12.94837562,  1.53296041]), 'targetState': array([ 2, 15], dtype=int32), 'currentDistance': 2.0884221895820496}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9303156581161443
{'scaleFactor': 20, 'currentTarget': array([ 2., 15.]), 'previousTarget': array([ 2., 15.]), 'currentState': array([ 2.45759093, 14.69575941,  0.69672614]), 'targetState': array([ 2, 15], dtype=int32), 'currentDistance': 0.5495014034295775}
episode index:3368
target Thresh 31.99999999999994
target distance 16.0
model initialize at round 3368
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 19.]), 'previousTarget': array([ 2., 19.]), 'currentState': array([17.13712555,  7.44493452,  3.11913383]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 19.04342689967562}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9303079604067032
{'scaleFactor': 20, 'currentTarget': array([ 2., 19.]), 'previousTarget': array([ 2., 19.]), 'currentState': array([ 2.41812515, 19.60150779,  1.2037152 ]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 0.7325573466370354}
episode index:3369
target Thresh 31.99999999999994
target distance 5.0
model initialize at round 3369
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 22.]), 'previousTarget': array([15., 22.]), 'currentState': array([10.52017387, 25.38688036,  5.43327117]), 'targetState': array([15, 22], dtype=int32), 'currentDistance': 5.616030693831257}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9303198271840305
{'scaleFactor': 20, 'currentTarget': array([15., 22.]), 'previousTarget': array([15., 22.]), 'currentState': array([14.25516306, 21.41112819,  4.117581  ]), 'targetState': array([15, 22], dtype=int32), 'currentDistance': 0.9495009568759186}
episode index:3370
target Thresh 31.99999999999994
target distance 12.0
model initialize at round 3370
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 16.]), 'previousTarget': array([19., 16.]), 'currentState': array([24.28837527,  2.91719012,  0.31107872]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 14.1112305437118}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9303203449890508
{'scaleFactor': 20, 'currentTarget': array([19., 16.]), 'previousTarget': array([19., 16.]), 'currentState': array([19.84654758, 15.34642426,  1.83380624]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 1.0694877546533486}
episode index:3371
target Thresh 31.99999999999994
target distance 14.0
model initialize at round 3371
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([4.74992562, 0.8731894 , 4.88517785]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 15.798586467857644}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9303180983548393
{'scaleFactor': 20, 'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([19.22334191,  4.78074514,  1.03113828]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 0.8070133057309423}
episode index:3372
target Thresh 31.999999999999943
target distance 5.0
model initialize at round 3372
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  3.]), 'previousTarget': array([11.,  3.]), 'currentState': array([10.1858225 ,  6.52707665,  5.2174269 ]), 'targetState': array([11,  3], dtype=int32), 'currentDistance': 3.6198279891669523}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9303328572939573
{'scaleFactor': 20, 'currentTarget': array([11.,  3.]), 'previousTarget': array([11.,  3.]), 'currentState': array([11.46120954,  2.76620233,  5.27429369]), 'targetState': array([11,  3], dtype=int32), 'currentDistance': 0.5170837362360011}
episode index:3373
target Thresh 31.999999999999943
target distance 16.0
model initialize at round 3373
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 10.]), 'previousTarget': array([27., 10.]), 'currentState': array([18.43029496, 24.37296763,  5.98093581]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 16.73385917625586}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9303278734143454
{'scaleFactor': 20, 'currentTarget': array([27., 10.]), 'previousTarget': array([27., 10.]), 'currentState': array([26.92170048,  9.37415952,  3.87997567]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 0.6307195245286727}
episode index:3374
target Thresh 31.999999999999943
target distance 5.0
model initialize at round 3374
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  2.]), 'previousTarget': array([25.,  2.]), 'currentState': array([22.64522831,  5.92931618,  5.3685382 ]), 'targetState': array([25,  2], dtype=int32), 'currentDistance': 4.58088150620869}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9303426207111116
{'scaleFactor': 20, 'currentTarget': array([25.,  2.]), 'previousTarget': array([25.,  2.]), 'currentState': array([24.80566758,  2.61576395,  5.65116942]), 'targetState': array([25,  2], dtype=int32), 'currentDistance': 0.6457014293219556}
episode index:3375
target Thresh 31.999999999999943
target distance 15.0
model initialize at round 3375
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([25.16351626, 10.67245919,  3.96642196]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 13.269336084115455}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9303431309976034
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([11.67677707,  8.97894347,  3.96325716]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.3239080774317777}
episode index:3376
target Thresh 31.999999999999943
target distance 14.0
model initialize at round 3376
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  5.]), 'previousTarget': array([11.,  5.]), 'currentState': array([23.52141862,  9.19614347,  2.62956107]), 'targetState': array([11,  5], dtype=int32), 'currentDistance': 13.205814794481286}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.9303098363001203
{'scaleFactor': 20, 'currentTarget': array([11.,  5.]), 'previousTarget': array([11.,  5.]), 'currentState': array([10.39711526,  4.7930784 ,  4.17954161]), 'targetState': array([11,  5], dtype=int32), 'currentDistance': 0.6374061142921071}
episode index:3377
target Thresh 31.999999999999943
target distance 9.0
model initialize at round 3377
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 15.]), 'previousTarget': array([12., 15.]), 'currentState': array([ 4.56871074, 18.38722258,  0.63562721]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 8.166843755357316}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9303159583290131
{'scaleFactor': 20, 'currentTarget': array([12., 15.]), 'previousTarget': array([12., 15.]), 'currentState': array([12.23619991, 14.84372426,  4.77178919]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 0.28321812364157245}
episode index:3378
target Thresh 31.999999999999947
target distance 7.0
model initialize at round 3378
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 14.]), 'previousTarget': array([11., 14.]), 'currentState': array([16.57077799,  8.39899963,  2.37663979]), 'targetState': array([11, 14], dtype=int32), 'currentDistance': 7.899669143468118}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9303249195754384
{'scaleFactor': 20, 'currentTarget': array([11., 14.]), 'previousTarget': array([11., 14.]), 'currentState': array([11.51817804, 14.53119036,  2.26716341]), 'targetState': array([11, 14], dtype=int32), 'currentDistance': 0.7420725578267199}
episode index:3379
target Thresh 31.999999999999947
target distance 24.0
model initialize at round 3379
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 12.]), 'previousTarget': array([26., 12.]), 'currentState': array([1.33879024, 9.45235995, 5.31862473]), 'targetState': array([26, 12], dtype=int32), 'currentDistance': 24.79245322283453}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.930294104103326
{'scaleFactor': 20, 'currentTarget': array([26., 12.]), 'previousTarget': array([26., 12.]), 'currentState': array([25.57200149, 11.19200494,  5.58008753]), 'targetState': array([26, 12], dtype=int32), 'currentDistance': 0.9143515445565094}
episode index:3380
target Thresh 31.999999999999947
target distance 13.0
model initialize at round 3380
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 13.]), 'previousTarget': array([11., 13.]), 'currentState': array([ 3.49630586, 25.22963849,  4.7977345 ]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 14.348152607882245}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9302891420043553
{'scaleFactor': 20, 'currentTarget': array([11., 13.]), 'previousTarget': array([11., 13.]), 'currentState': array([11.66973226, 13.01053849,  1.64577105]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 0.6698151688911315}
episode index:3381
target Thresh 31.999999999999947
target distance 8.0
model initialize at round 3381
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([18.11283281,  7.79105893,  1.9685725 ]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 6.55858305291736}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9303009722403092
{'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([16.52744539, 13.51772619,  2.21122622]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 0.7146934107861314}
episode index:3382
target Thresh 31.999999999999947
target distance 7.0
model initialize at round 3382
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 13.]), 'previousTarget': array([10., 13.]), 'currentState': array([6.81088656, 7.82664423, 1.08617789]), 'targetState': array([10, 13], dtype=int32), 'currentDistance': 6.077339420621584}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9303127954823309
{'scaleFactor': 20, 'currentTarget': array([10., 13.]), 'previousTarget': array([10., 13.]), 'currentState': array([10.40228456, 12.5771702 ,  1.21466559]), 'targetState': array([10, 13], dtype=int32), 'currentDistance': 0.5836247964449424}
episode index:3383
target Thresh 31.999999999999947
target distance 3.0
model initialize at round 3383
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 22.]), 'previousTarget': array([11., 22.]), 'currentState': array([13.6774305 , 18.85232693,  0.92060011]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 4.132369758902249}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9303217444227911
{'scaleFactor': 20, 'currentTarget': array([11., 22.]), 'previousTarget': array([11., 22.]), 'currentState': array([10.74836085, 22.44904395,  4.90987867]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 0.5147453090196182}
episode index:3384
target Thresh 31.99999999999995
target distance 8.0
model initialize at round 3384
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 17.]), 'previousTarget': array([ 7., 17.]), 'currentState': array([15.74193039, 22.48939503,  4.15892982]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 10.32253867127198}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9303114095660219
{'scaleFactor': 20, 'currentTarget': array([ 7., 17.]), 'previousTarget': array([ 7., 17.]), 'currentState': array([ 6.03570349, 16.75612651,  4.05742866]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 0.9946567431921249}
episode index:3385
target Thresh 31.99999999999995
target distance 4.0
model initialize at round 3385
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([17.35728781,  5.63409735,  4.37075782]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 4.331682051646242}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9303175166659433
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.1395011 ,  1.28498185,  2.49272466]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.7284994927021449}
episode index:3386
target Thresh 31.99999999999995
target distance 20.0
model initialize at round 3386
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 26.]), 'previousTarget': array([22., 26.]), 'currentState': array([3.16548997, 8.21413187, 6.07906896]), 'targetState': array([22, 26], dtype=int32), 'currentDistance': 25.905132181202383}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.9302917197001087
{'scaleFactor': 20, 'currentTarget': array([22., 26.]), 'previousTarget': array([22., 26.]), 'currentState': array([22.65076923, 25.7739766 ,  4.03008408]), 'targetState': array([22, 26], dtype=int32), 'currentDistance': 0.6889028690285338}
episode index:3387
target Thresh 31.99999999999995
target distance 9.0
model initialize at round 3387
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 20.]), 'previousTarget': array([20., 20.]), 'currentState': array([14.95717172, 27.6790544 ,  5.05411053]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 9.186838059074823}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9302978290065432
{'scaleFactor': 20, 'currentTarget': array([20., 20.]), 'previousTarget': array([20., 20.]), 'currentState': array([19.87515266, 19.68212006,  4.08080899]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 0.341517959200712}
episode index:3388
target Thresh 31.99999999999995
target distance 13.0
model initialize at round 3388
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  6.]), 'previousTarget': array([25.,  6.]), 'currentState': array([1.28993744e+01, 5.57749737e+00, 2.99805006e-03]), 'targetState': array([25,  6], dtype=int32), 'currentDistance': 12.107999337262925}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9302956002858059
{'scaleFactor': 20, 'currentTarget': array([25.,  6.]), 'previousTarget': array([25.,  6.]), 'currentState': array([25.64311267,  5.81346919,  3.85130943]), 'targetState': array([25,  6], dtype=int32), 'currentDistance': 0.6696175383029359}
episode index:3389
target Thresh 31.99999999999995
target distance 12.0
model initialize at round 3389
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([25.2916948 ,  8.81111264,  2.11613107]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 13.796906642283613}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9302906509191976
{'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([14.62974985, 18.64277367,  4.8990379 ]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 0.7417837729234553}
episode index:3390
target Thresh 31.99999999999995
target distance 10.0
model initialize at round 3390
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([20.90424822, 15.72245953,  2.9936353 ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 11.124295198043843}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9302911742742516
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.13411939,  6.86776633,  4.9926182 ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.8759195028661627}
episode index:3391
target Thresh 31.99999999999995
target distance 12.0
model initialize at round 3391
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([11.49255374, 21.25167264,  4.61237264]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 11.263109642223027}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9302944729107865
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([11.34564483,  9.87922193,  3.43844912]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.6654081696252747}
episode index:3392
target Thresh 31.999999999999954
target distance 12.0
model initialize at round 3392
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'previousTarget': array([13., 22.]), 'currentState': array([12.66394809, 11.02340285,  1.47793603]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 10.981740114095523}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9302977696029439
{'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'previousTarget': array([13., 22.]), 'currentState': array([13.25235907, 22.42509145,  0.353997  ]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 0.4943559885456468}
episode index:3393
target Thresh 31.999999999999954
target distance 24.0
model initialize at round 3393
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 27.]), 'previousTarget': array([14., 27.]), 'currentState': array([6.43081806, 4.88554362, 0.89493793]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 23.373953375676873}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9302796340823681
{'scaleFactor': 20, 'currentTarget': array([14., 27.]), 'previousTarget': array([14., 27.]), 'currentState': array([14.97239676, 27.69952521,  3.74037241]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 1.1978693485431078}
episode index:3394
target Thresh 31.999999999999954
target distance 6.0
model initialize at round 3394
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 19.]), 'previousTarget': array([ 5., 19.]), 'currentState': array([ 7.3280438 , 24.80777472,  4.26606035]), 'targetState': array([ 5, 19], dtype=int32), 'currentDistance': 6.2569988934848375}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9302914218190155
{'scaleFactor': 20, 'currentTarget': array([ 5., 19.]), 'previousTarget': array([ 5., 19.]), 'currentState': array([ 4.74436971, 19.40368048,  4.43511624]), 'targetState': array([ 5, 19], dtype=int32), 'currentDistance': 0.4778124845743103}
episode index:3395
target Thresh 31.999999999999954
target distance 16.0
model initialize at round 3395
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  9.]), 'previousTarget': array([23.,  9.]), 'currentState': array([ 8.49549595, 13.67415667,  5.62350686]), 'targetState': array([23,  9], dtype=int32), 'currentDistance': 15.2390412529575}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.930289199578912
{'scaleFactor': 20, 'currentTarget': array([23.,  9.]), 'previousTarget': array([23.,  9.]), 'currentState': array([22.59303884,  8.12973762,  4.49451991]), 'targetState': array([23,  9], dtype=int32), 'currentDistance': 0.9607153568754312}
episode index:3396
target Thresh 31.999999999999954
target distance 7.0
model initialize at round 3396
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  9.]), 'previousTarget': array([17.,  9.]), 'currentState': array([21.19117831,  3.67207624,  2.4669548 ]), 'targetState': array([17,  9], dtype=int32), 'currentDistance': 6.778845572683304}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9303009775596071
{'scaleFactor': 20, 'currentTarget': array([17.,  9.]), 'previousTarget': array([17.,  9.]), 'currentState': array([17.33320765,  8.09234074,  2.87227961]), 'targetState': array([17,  9], dtype=int32), 'currentDistance': 0.9668881389938055}
episode index:3397
target Thresh 31.999999999999954
target distance 15.0
model initialize at round 3397
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([ 8.65575638, 10.30142811,  1.19007653]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 15.92902981202074}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.930293349866096
{'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([22.75057681, 19.09335602,  4.36545567]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 0.7563602896355589}
episode index:3398
target Thresh 31.999999999999954
target distance 6.0
model initialize at round 3398
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 28.]), 'previousTarget': array([13., 28.]), 'currentState': array([18.68634969, 30.65348467,  2.76825953]), 'targetState': array([13, 28], dtype=int32), 'currentDistance': 6.274994320371275}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9303022650353028
{'scaleFactor': 20, 'currentTarget': array([13., 28.]), 'previousTarget': array([13., 28.]), 'currentState': array([12.97197498, 27.04252431,  2.6680162 ]), 'targetState': array([13, 28], dtype=int32), 'currentDistance': 0.9578857490614312}
episode index:3399
target Thresh 31.999999999999957
target distance 10.0
model initialize at round 3399
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  9.]), 'previousTarget': array([21.,  9.]), 'currentState': array([14.949177  , 17.32514922,  5.70945483]), 'targetState': array([21,  9], dtype=int32), 'currentDistance': 10.291771882304449}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9303027835890886
{'scaleFactor': 20, 'currentTarget': array([21.,  9.]), 'previousTarget': array([21.,  9.]), 'currentState': array([20.7168757 ,  8.2478486 ,  2.44232237]), 'targetState': array([21,  9], dtype=int32), 'currentDistance': 0.8036735030926562}
episode index:3400
target Thresh 31.999999999999957
target distance 20.0
model initialize at round 3400
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  4.]), 'previousTarget': array([24.,  4.]), 'currentState': array([ 3.52701669, 10.38486073,  5.43751049]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 21.44550050855184}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.9302770971468054
{'scaleFactor': 20, 'currentTarget': array([24.,  4.]), 'previousTarget': array([24.,  4.]), 'currentState': array([24.35210739,  3.33350341,  3.77389681]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 0.7537886435947871}
episode index:3401
target Thresh 31.999999999999957
target distance 12.0
model initialize at round 3401
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([ 6.67541764, 12.34862652,  5.52831006]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 12.54636907082656}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9302668270577731
{'scaleFactor': 20, 'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([18.17449457,  9.65375286,  1.63660599]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 0.8951794794812311}
episode index:3402
target Thresh 31.999999999999957
target distance 8.0
model initialize at round 3402
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 14.]), 'previousTarget': array([15., 14.]), 'currentState': array([18.67520657,  5.8385347 ,  0.91391152]), 'targetState': array([15, 14], dtype=int32), 'currentDistance': 8.950790978507841}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9302729167500571
{'scaleFactor': 20, 'currentTarget': array([15., 14.]), 'previousTarget': array([15., 14.]), 'currentState': array([15.86022517, 13.76815564,  2.91390425]), 'targetState': array([15, 14], dtype=int32), 'currentDistance': 0.8909204003722756}
episode index:3403
target Thresh 31.999999999999957
target distance 17.0
model initialize at round 3403
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  5.]), 'previousTarget': array([19.,  5.]), 'currentState': array([ 1.47281161, 21.29283402,  4.58524656]), 'targetState': array([19,  5], dtype=int32), 'currentDistance': 23.93028986742821}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9302548418076418
{'scaleFactor': 20, 'currentTarget': array([19.,  5.]), 'previousTarget': array([19.,  5.]), 'currentState': array([19.03644213,  5.56410612,  4.45441628]), 'targetState': array([19,  5], dtype=int32), 'currentDistance': 0.565282002338256}
episode index:3404
target Thresh 31.999999999999957
target distance 17.0
model initialize at round 3404
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([ 4.24058365, 21.02083691,  4.69478202]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 15.071980268443996}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9302526361843292
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.94999943, 5.61850145, 3.47134501]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.38476122122662826}
episode index:3405
target Thresh 31.999999999999957
target distance 7.0
model initialize at round 3405
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 20.]), 'previousTarget': array([23., 20.]), 'currentState': array([15.17203931, 15.53478016,  5.20804477]), 'targetState': array([23, 20], dtype=int32), 'currentDistance': 9.011945227820128}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9302587246792545
{'scaleFactor': 20, 'currentTarget': array([23., 20.]), 'previousTarget': array([23., 20.]), 'currentState': array([22.61929602, 20.58056251,  1.51690226]), 'targetState': array([23, 20], dtype=int32), 'currentDistance': 0.6942538065640652}
episode index:3406
target Thresh 31.999999999999957
target distance 19.0
model initialize at round 3406
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([15.53571484, 22.82961258,  3.63612068]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 21.187946769075097}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9302484750548281
{'scaleFactor': 20, 'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([23.33647125,  3.03376059,  5.049642  ]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 0.33816073386314405}
episode index:3407
target Thresh 31.999999999999957
target distance 14.0
model initialize at round 3407
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 24.]), 'previousTarget': array([ 5., 24.]), 'currentState': array([ 6.66138082, 11.02004253,  1.48044872]), 'targetState': array([ 5, 24], dtype=int32), 'currentDistance': 13.085850453909389}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9302490081747965
{'scaleFactor': 20, 'currentTarget': array([ 5., 24.]), 'previousTarget': array([ 5., 24.]), 'currentState': array([ 4.85099991, 24.56978753,  1.11537798]), 'targetState': array([ 5, 24], dtype=int32), 'currentDistance': 0.5889472487091038}
episode index:3408
target Thresh 31.99999999999996
target distance 6.0
model initialize at round 3408
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 15.]), 'previousTarget': array([19., 15.]), 'currentState': array([13.3182892 , 10.34740202,  5.91265845]), 'targetState': array([19, 15], dtype=int32), 'currentDistance': 7.3436030357607995}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9302579101993859
{'scaleFactor': 20, 'currentTarget': array([19., 15.]), 'previousTarget': array([19., 15.]), 'currentState': array([18.71605211, 15.18798486,  1.37455096]), 'targetState': array([19, 15], dtype=int32), 'currentDistance': 0.34053591607925554}
episode index:3409
target Thresh 31.99999999999996
target distance 21.0
model initialize at round 3409
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 14.]), 'previousTarget': array([ 6., 14.]), 'currentState': array([26.69853982, 23.34424946,  3.52229309]), 'targetState': array([ 6, 14], dtype=int32), 'currentDistance': 22.710009882617854}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9302450442056958
{'scaleFactor': 20, 'currentTarget': array([ 6., 14.]), 'previousTarget': array([ 6., 14.]), 'currentState': array([ 5.93895019, 13.96488053,  3.06661652]), 'targetState': array([ 6, 14], dtype=int32), 'currentDistance': 0.07043050417452482}
episode index:3410
target Thresh 31.99999999999996
target distance 8.0
model initialize at round 3410
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([20.55348733, 14.86022623,  3.61508691]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 8.791498598445548}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9302539421728004
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.65233278,  9.88904409,  4.5624113 ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.1026955346858738}
episode index:3411
target Thresh 31.99999999999996
target distance 8.0
model initialize at round 3411
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([10.28613335,  3.90920503,  3.08395654]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 6.6247185706343314}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9302656787079198
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.71268362, 5.61593445, 3.49334455]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.8095827850743273}
episode index:3412
target Thresh 31.99999999999996
target distance 11.0
model initialize at round 3412
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  5.]), 'previousTarget': array([18.,  5.]), 'currentState': array([14.6841426 , 17.04924134,  3.47844994]), 'targetState': array([18,  5], dtype=int32), 'currentDistance': 12.497164761253726}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9302662060062494
{'scaleFactor': 20, 'currentTarget': array([18.,  5.]), 'previousTarget': array([18.,  5.]), 'currentState': array([18.33121118,  4.49567076,  4.26456704]), 'targetState': array([18,  5], dtype=int32), 'currentDistance': 0.6033645858894356}
episode index:3413
target Thresh 31.99999999999996
target distance 7.0
model initialize at round 3413
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 25.]), 'previousTarget': array([27., 25.]), 'currentState': array([20.68708944, 19.76834016,  0.63468528]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 8.198969714432511}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9302750899558668
{'scaleFactor': 20, 'currentTarget': array([27., 25.]), 'previousTarget': array([27., 25.]), 'currentState': array([26.7148112 , 24.94557706,  0.86930113]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 0.29033516427146927}
episode index:3414
target Thresh 31.99999999999996
target distance 12.0
model initialize at round 3414
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 28.]), 'previousTarget': array([23., 28.]), 'currentState': array([13.38406527, 17.4382996 ,  0.91415268]), 'targetState': array([23, 28], dtype=int32), 'currentDistance': 14.283407017413051}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9302756141895275
{'scaleFactor': 20, 'currentTarget': array([23., 28.]), 'previousTarget': array([23., 28.]), 'currentState': array([22.94031732, 27.54390328,  0.79560868]), 'targetState': array([23, 28], dtype=int32), 'currentDistance': 0.45998504267670387}
episode index:3415
target Thresh 31.99999999999996
target distance 7.0
model initialize at round 3415
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 26.]), 'previousTarget': array([24., 26.]), 'currentState': array([1.78579791e+01, 3.04478467e+01, 2.58433819e-02]), 'targetState': array([24, 26], dtype=int32), 'currentDistance': 7.583387198954444}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9302844901836171
{'scaleFactor': 20, 'currentTarget': array([24., 26.]), 'previousTarget': array([24., 26.]), 'currentState': array([24.23145341, 25.97034954,  5.55239186]), 'targetState': array([24, 26], dtype=int32), 'currentDistance': 0.23334487811407092}
episode index:3416
target Thresh 31.99999999999996
target distance 16.0
model initialize at round 3416
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 14.]), 'previousTarget': array([ 6., 14.]), 'currentState': array([20.14964543, 14.32885071,  3.17738241]), 'targetState': array([ 6, 14], dtype=int32), 'currentDistance': 14.153466308626454}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9302850113594215
{'scaleFactor': 20, 'currentTarget': array([ 6., 14.]), 'previousTarget': array([ 6., 14.]), 'currentState': array([ 6.31057731, 13.73640518,  3.73154172]), 'targetState': array([ 6, 14], dtype=int32), 'currentDistance': 0.407357944579835}
episode index:3417
target Thresh 31.999999999999964
target distance 16.0
model initialize at round 3417
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  9.]), 'previousTarget': array([22.,  9.]), 'currentState': array([ 7.36492793, 20.01543921,  0.3851028 ]), 'targetState': array([22,  9], dtype=int32), 'currentDistance': 18.317347937320754}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9302801056356427
{'scaleFactor': 20, 'currentTarget': array([22.,  9.]), 'previousTarget': array([22.,  9.]), 'currentState': array([21.43075168,  9.78932514,  5.53124799]), 'targetState': array([22,  9], dtype=int32), 'currentDistance': 0.9731792368744143}
episode index:3418
target Thresh 31.999999999999964
target distance 2.0
model initialize at round 3418
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 17.]), 'previousTarget': array([21., 17.]), 'currentState': array([21.28202343, 17.18770423,  4.73048413]), 'targetState': array([21, 17], dtype=int32), 'currentDistance': 0.33877735259175273}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9303004975322102
{'scaleFactor': 20, 'currentTarget': array([21., 17.]), 'previousTarget': array([21., 17.]), 'currentState': array([21.28202343, 17.18770423,  4.73048413]), 'targetState': array([21, 17], dtype=int32), 'currentDistance': 0.33877735259175273}
episode index:3419
target Thresh 31.999999999999964
target distance 12.0
model initialize at round 3419
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 20.]), 'previousTarget': array([23., 20.]), 'currentState': array([24.02652727,  9.71030725,  1.7742573 ]), 'targetState': array([23, 20], dtype=int32), 'currentDistance': 10.340770529115403}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.930306547108926
{'scaleFactor': 20, 'currentTarget': array([23., 20.]), 'previousTarget': array([23., 20.]), 'currentState': array([23.0105783 , 19.53231919,  2.25824711]), 'targetState': array([23, 20], dtype=int32), 'currentDistance': 0.46780042862517185}
episode index:3420
target Thresh 31.999999999999964
target distance 23.0
model initialize at round 3420
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([24.69459091, 25.46705135,  4.12782431]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 22.804407976700546}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9302963254506827
{'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([16.95499501,  4.96657343,  4.40881743]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 0.9676206079699967}
episode index:3421
target Thresh 31.999999999999964
target distance 7.0
model initialize at round 3421
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 12.]), 'previousTarget': array([ 4., 12.]), 'currentState': array([ 6.84698002, 17.77405832,  4.96763396]), 'targetState': array([ 4, 12], dtype=int32), 'currentDistance': 6.437782592097099}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9303080153029765
{'scaleFactor': 20, 'currentTarget': array([ 4., 12.]), 'previousTarget': array([ 4., 12.]), 'currentState': array([ 4.35392766, 12.61138182,  4.58075264]), 'targetState': array([ 4, 12], dtype=int32), 'currentDistance': 0.7064364950954595}
episode index:3422
target Thresh 31.999999999999964
target distance 9.0
model initialize at round 3422
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 14.]), 'previousTarget': array([ 6., 14.]), 'currentState': array([3.67575515, 5.1556688 , 1.10262888]), 'targetState': array([ 6, 14], dtype=int32), 'currentDistance': 9.144632770759443}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9303140573814449
{'scaleFactor': 20, 'currentTarget': array([ 6., 14.]), 'previousTarget': array([ 6., 14.]), 'currentState': array([ 5.7143921 , 14.68156192,  1.67124173]), 'targetState': array([ 6, 14], dtype=int32), 'currentDistance': 0.738984797752595}
episode index:3423
target Thresh 31.999999999999964
target distance 4.0
model initialize at round 3423
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([19.45128054,  9.80827545,  6.21277905]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 6.234207336134087}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9303229014096629
{'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([15.94583739, 14.88627968,  2.54969573]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 0.12595990924162004}
episode index:3424
target Thresh 31.999999999999964
target distance 3.0
model initialize at round 3424
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([ 9.25525354, 10.45095531,  5.25548202]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 2.269231481242801}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9303403253800541
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.82044599,  9.28990217,  6.04229736]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.3410027996014836}
episode index:3425
target Thresh 31.999999999999964
target distance 10.0
model initialize at round 3425
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  8.]), 'previousTarget': array([25.,  8.]), 'currentState': array([19.5372457 , 17.31497172,  4.85397929]), 'targetState': array([25,  8], dtype=int32), 'currentDistance': 10.798628740418605}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9303463527368901
{'scaleFactor': 20, 'currentTarget': array([25.,  8.]), 'previousTarget': array([25.,  8.]), 'currentState': array([24.54981966,  8.82186784,  5.79518173]), 'targetState': array([25,  8], dtype=int32), 'currentDistance': 0.9370854203541755}
episode index:3426
target Thresh 31.999999999999964
target distance 7.0
model initialize at round 3426
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.2016136 , 14.82164825,  4.49756086]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 5.876138972286293}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9303580109356829
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.11052321,  8.97082979,  5.05301693]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.11430783263232604}
episode index:3427
target Thresh 31.999999999999968
target distance 26.0
model initialize at round 3427
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 29.]), 'previousTarget': array([22., 29.]), 'currentState': array([22.23555188,  4.82076718,  2.15974927]), 'targetState': array([22, 29], dtype=int32), 'currentDistance': 24.18038015647792}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9303451832988044
{'scaleFactor': 20, 'currentTarget': array([22., 29.]), 'previousTarget': array([22., 29.]), 'currentState': array([22.00180173, 28.61754474,  2.05064945]), 'targetState': array([22, 29], dtype=int32), 'currentDistance': 0.3824595062151422}
episode index:3428
target Thresh 31.999999999999968
target distance 12.0
model initialize at round 3428
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 24.]), 'previousTarget': array([ 3., 24.]), 'currentState': array([ 4.29045883, 10.91967408,  0.31300515]), 'targetState': array([ 3, 24], dtype=int32), 'currentDistance': 13.143827836147578}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9303456849507754
{'scaleFactor': 20, 'currentTarget': array([ 3., 24.]), 'previousTarget': array([ 3., 24.]), 'currentState': array([ 2.86526797, 24.1340937 ,  2.11176591]), 'targetState': array([ 3, 24], dtype=int32), 'currentDistance': 0.1900890342706278}
episode index:3429
target Thresh 31.999999999999968
target distance 12.0
model initialize at round 3429
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  9.]), 'previousTarget': array([19.,  9.]), 'currentState': array([21.794575  , 22.17445249,  3.37920809]), 'targetState': array([19,  9], dtype=int32), 'currentDistance': 13.467585074742434}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9303461863102379
{'scaleFactor': 20, 'currentTarget': array([19.,  9.]), 'previousTarget': array([19.,  9.]), 'currentState': array([19.13144017,  9.13683031,  4.95828062]), 'targetState': array([19,  9], dtype=int32), 'currentDistance': 0.18973416282544814}
episode index:3430
target Thresh 31.999999999999968
target distance 23.0
model initialize at round 3430
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 28.]), 'previousTarget': array([15., 28.]), 'currentState': array([23.41757444,  4.42701011,  2.47900248]), 'targetState': array([15, 28], dtype=int32), 'currentDistance': 25.03080925276557}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9303307898767459
{'scaleFactor': 20, 'currentTarget': array([15., 28.]), 'previousTarget': array([15., 28.]), 'currentState': array([14.6567208 , 28.01520749,  2.34061265]), 'targetState': array([15, 28], dtype=int32), 'currentDistance': 0.34361589047738317}
episode index:3431
target Thresh 31.999999999999968
target distance 13.0
model initialize at round 3431
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  4.]), 'previousTarget': array([24.,  4.]), 'currentState': array([12.39819094, 15.42830857,  5.35670567]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 16.28521447075416}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.930328579475974
{'scaleFactor': 20, 'currentTarget': array([24.,  4.]), 'previousTarget': array([24.,  4.]), 'currentState': array([23.66010556,  4.25683742,  5.99963252]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 0.4260207612010558}
episode index:3432
target Thresh 31.999999999999968
target distance 7.0
model initialize at round 3432
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([18.16002999,  8.45494986,  5.81476575]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 7.292584428395624}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.930337396088419
{'scaleFactor': 20, 'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([23.44613648,  2.5964194 ,  5.67635059]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 0.6015937666472285}
episode index:3433
target Thresh 31.999999999999968
target distance 23.0
model initialize at round 3433
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 29.]), 'previousTarget': array([ 7., 29.]), 'currentState': array([4.28425674, 7.08769146, 1.71271628]), 'targetState': array([ 7, 29], dtype=int32), 'currentDistance': 22.079957586922724}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9303272041426329
{'scaleFactor': 20, 'currentTarget': array([ 7., 29.]), 'previousTarget': array([ 7., 29.]), 'currentState': array([ 7.08241155, 28.84081497,  1.69891343]), 'targetState': array([ 7, 29], dtype=int32), 'currentDistance': 0.1792527138823764}
episode index:3434
target Thresh 31.999999999999968
target distance 13.0
model initialize at round 3434
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 20.]), 'previousTarget': array([ 7., 20.]), 'currentState': array([18.67798616, 29.04147371,  3.48433924]), 'targetState': array([ 7, 20], dtype=int32), 'currentDistance': 14.769008347329452}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9303249967162239
{'scaleFactor': 20, 'currentTarget': array([ 7., 20.]), 'previousTarget': array([ 7., 20.]), 'currentState': array([ 6.87410934, 19.18879122,  4.1913625 ]), 'targetState': array([ 7, 20], dtype=int32), 'currentDistance': 0.8209190827407683}
episode index:3435
target Thresh 31.999999999999968
target distance 9.0
model initialize at round 3435
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 25.]), 'previousTarget': array([27., 25.]), 'currentState': array([21.99016876, 17.73756402,  1.07112141]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 8.822776512010622}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9303338066735242
{'scaleFactor': 20, 'currentTarget': array([27., 25.]), 'previousTarget': array([27., 25.]), 'currentState': array([26.42820714, 24.30033445,  0.67690363]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 0.9035922475675833}
episode index:3436
target Thresh 31.999999999999968
target distance 16.0
model initialize at round 3436
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([17.7959394 , 27.32944699,  5.60083961]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 14.933424809993113}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9303315986106071
{'scaleFactor': 20, 'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([21.86472536, 12.36322744,  5.08063041]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 0.6509827398266964}
episode index:3437
target Thresh 31.999999999999968
target distance 14.0
model initialize at round 3437
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 5.38962421, 25.48895592,  3.85681033]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 14.57817388549756}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9303293918321944
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.36649918, 10.44280228,  5.00253458]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.6669264954856188}
episode index:3438
target Thresh 31.99999999999997
target distance 10.0
model initialize at round 3438
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 25.]), 'previousTarget': array([22., 25.]), 'currentState': array([20.45101665, 16.94845197,  1.33378929]), 'targetState': array([22, 25], dtype=int32), 'currentDistance': 8.199193567191923}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9303381928261367
{'scaleFactor': 20, 'currentTarget': array([22., 25.]), 'previousTarget': array([22., 25.]), 'currentState': array([21.91315457, 24.71498776,  1.94607284]), 'targetState': array([22, 25], dtype=int32), 'currentDistance': 0.29794984153094634}
episode index:3439
target Thresh 31.99999999999997
target distance 21.0
model initialize at round 3439
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 25.]), 'previousTarget': array([ 4., 25.]), 'currentState': array([13.54209057,  4.674051  ,  1.4220764 ]), 'targetState': array([ 4, 25], dtype=int32), 'currentDistance': 22.45430238144581}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9303280184253904
{'scaleFactor': 20, 'currentTarget': array([ 4., 25.]), 'previousTarget': array([ 4., 25.]), 'currentState': array([ 4.71066154, 24.27148262,  2.67363283]), 'targetState': array([ 4, 25], dtype=int32), 'currentDistance': 1.017731495349307}
episode index:3440
target Thresh 31.99999999999997
target distance 9.0
model initialize at round 3440
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 24.]), 'previousTarget': array([ 7., 24.]), 'currentState': array([ 9.03128075, 16.99927731,  1.59340138]), 'targetState': array([ 7, 24], dtype=int32), 'currentDistance': 7.289459488172342}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9303368147030929
{'scaleFactor': 20, 'currentTarget': array([ 7., 24.]), 'previousTarget': array([ 7., 24.]), 'currentState': array([ 6.60220796, 24.47460092,  2.05308922]), 'targetState': array([ 7, 24], dtype=int32), 'currentDistance': 0.619261286297523}
episode index:3441
target Thresh 31.99999999999997
target distance 17.0
model initialize at round 3441
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 25.]), 'previousTarget': array([19., 25.]), 'currentState': array([14.03715001,  9.3254086 ,  1.91680926]), 'targetState': array([19, 25], dtype=int32), 'currentDistance': 16.441493110716625}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9303346089737857
{'scaleFactor': 20, 'currentTarget': array([19., 25.]), 'previousTarget': array([19., 25.]), 'currentState': array([18.91879444, 24.12123697,  1.83839227]), 'targetState': array([19, 25], dtype=int32), 'currentDistance': 0.8825071146150557}
episode index:3442
target Thresh 31.99999999999997
target distance 9.0
model initialize at round 3442
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 18.]), 'previousTarget': array([ 4., 18.]), 'currentState': array([ 4.37180675, 25.10177568,  4.43305412]), 'targetState': array([ 4, 18], dtype=int32), 'currentDistance': 7.111501814544311}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9303433982276417
{'scaleFactor': 20, 'currentTarget': array([ 4., 18.]), 'previousTarget': array([ 4., 18.]), 'currentState': array([ 4.38667454, 17.25106087,  4.86506144]), 'targetState': array([ 4, 18], dtype=int32), 'currentDistance': 0.8428683322632091}
episode index:3443
target Thresh 31.99999999999997
target distance 12.0
model initialize at round 3443
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  2.]), 'previousTarget': array([17.,  2.]), 'currentState': array([21.1284214 , 12.56029905,  5.17801666]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 11.338596882324664}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9303466318952297
{'scaleFactor': 20, 'currentTarget': array([17.,  2.]), 'previousTarget': array([17.,  2.]), 'currentState': array([16.86483861,  1.70004966,  4.63451021]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 0.3289966723286706}
episode index:3444
target Thresh 31.99999999999997
target distance 17.0
model initialize at round 3444
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 12.]), 'previousTarget': array([ 6., 12.]), 'currentState': array([21.41557934, 14.92533237,  3.22269475]), 'targetState': array([ 6, 12], dtype=int32), 'currentDistance': 15.690686916435391}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9303444252370389
{'scaleFactor': 20, 'currentTarget': array([ 6., 12.]), 'previousTarget': array([ 6., 12.]), 'currentState': array([ 5.77957589, 11.8196339 ,  3.74715725]), 'targetState': array([ 6, 12], dtype=int32), 'currentDistance': 0.2848134771475253}
episode index:3445
target Thresh 31.99999999999997
target distance 10.0
model initialize at round 3445
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([ 7.85388723, 15.54973601,  6.25451756]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 10.529610057852683}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9303504164223735
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.6246896 ,  7.79139837,  5.20692546]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.8758819947219645}
episode index:3446
target Thresh 31.99999999999997
target distance 19.0
model initialize at round 3446
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([21.95917845, 20.67747265,  5.05562878]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 19.386633564313698}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9303428828159293
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.98368443,  2.61679278,  4.34304137]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.3835543895887698}
episode index:3447
target Thresh 31.99999999999997
target distance 9.0
model initialize at round 3447
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 13.]), 'previousTarget': array([19., 13.]), 'currentState': array([12.2155335 , 20.5110405 ,  5.23748851]), 'targetState': array([19, 13], dtype=int32), 'currentDistance': 10.121497671963033}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9303488709734362
{'scaleFactor': 20, 'currentTarget': array([19., 13.]), 'previousTarget': array([19., 13.]), 'currentState': array([18.77402982, 13.15813814,  5.8832367 ]), 'targetState': array([19, 13], dtype=int32), 'currentDistance': 0.27580825911341894}
episode index:3448
target Thresh 31.99999999999997
target distance 19.0
model initialize at round 3448
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 21.]), 'previousTarget': array([20., 21.]), 'currentState': array([22.52411279,  3.94245218,  1.82878852]), 'targetState': array([20, 21], dtype=int32), 'currentDistance': 17.24329095617902}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9303439908274548
{'scaleFactor': 20, 'currentTarget': array([20., 21.]), 'previousTarget': array([20., 21.]), 'currentState': array([19.77365357, 21.67379937,  1.96539415]), 'targetState': array([20, 21], dtype=int32), 'currentDistance': 0.7108011608155917}
episode index:3449
target Thresh 31.99999999999997
target distance 18.0
model initialize at round 3449
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([ 1.47467242, 10.59888055,  0.8782444 ]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 19.040945730725326}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9303364656344639
{'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([20.72577048, 15.36032025,  0.31417533]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 0.8102922184537855}
episode index:3450
target Thresh 31.999999999999975
target distance 20.0
model initialize at round 3450
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  8.]), 'previousTarget': array([25.,  8.]), 'currentState': array([ 6.42208445, 28.90003551,  5.83744449]), 'targetState': array([25,  8], dtype=int32), 'currentDistance': 27.963376592711903}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9303186184444131
{'scaleFactor': 20, 'currentTarget': array([25.,  8.]), 'previousTarget': array([25.,  8.]), 'currentState': array([24.91444445,  8.05754283,  5.67128058]), 'targetState': array([25,  8], dtype=int32), 'currentDistance': 0.10310639887440846}
episode index:3451
target Thresh 31.999999999999975
target distance 15.0
model initialize at round 3451
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 16.]), 'previousTarget': array([21., 16.]), 'currentState': array([ 7.91576901, 16.50468126,  0.01691875]), 'targetState': array([21, 16], dtype=int32), 'currentDistance': 13.093960587905439}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9303191244494717
{'scaleFactor': 20, 'currentTarget': array([21., 16.]), 'previousTarget': array([21., 16.]), 'currentState': array([21.62857389, 16.50408187,  0.2116195 ]), 'targetState': array([21, 16], dtype=int32), 'currentDistance': 0.805731758709023}
episode index:3452
target Thresh 31.999999999999975
target distance 10.0
model initialize at round 3452
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 16.]), 'previousTarget': array([23., 16.]), 'currentState': array([12.23516634, 10.49913894,  1.0325563 ]), 'targetState': array([23, 16], dtype=int32), 'currentDistance': 12.088883991881017}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.930322356718499
{'scaleFactor': 20, 'currentTarget': array([23., 16.]), 'previousTarget': array([23., 16.]), 'currentState': array([22.82046429, 15.73305251,  0.48259374]), 'targetState': array([23, 16], dtype=int32), 'currentDistance': 0.3217048850332669}
episode index:3453
target Thresh 31.999999999999975
target distance 23.0
model initialize at round 3453
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 18.]), 'previousTarget': array([ 4., 18.]), 'currentState': array([27.70633255, 14.52757403,  2.14768808]), 'targetState': array([ 4, 18], dtype=int32), 'currentDistance': 23.959297675343727}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9303096359643004
{'scaleFactor': 20, 'currentTarget': array([ 4., 18.]), 'previousTarget': array([ 4., 18.]), 'currentState': array([ 4.81683051, 18.1701271 ,  3.73053032]), 'targetState': array([ 4, 18], dtype=int32), 'currentDistance': 0.8343592223543488}
episode index:3454
target Thresh 31.999999999999975
target distance 11.0
model initialize at round 3454
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([17.15329325, 21.77431539,  0.19418162]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 10.005290046750094}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9303156216123281
{'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([26.81307284, 20.07313445,  0.22363428]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 0.20072471606173792}
episode index:3455
target Thresh 31.999999999999975
target distance 14.0
model initialize at round 3455
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 16.]), 'previousTarget': array([15., 16.]), 'currentState': array([18.7123841 ,  3.34275686,  1.67446136]), 'targetState': array([15, 16], dtype=int32), 'currentDistance': 13.190435915374772}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9303161278988717
{'scaleFactor': 20, 'currentTarget': array([15., 16.]), 'previousTarget': array([15., 16.]), 'currentState': array([14.55223549, 16.33180432,  2.17867643]), 'targetState': array([15, 16], dtype=int32), 'currentDistance': 0.5573034763054827}
episode index:3456
target Thresh 31.999999999999975
target distance 8.0
model initialize at round 3456
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([19.03308065, 25.62251858,  3.0903573 ]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 8.416169049468511}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9303248869043971
{'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([12.51552687, 21.33208393,  3.93878248]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 0.6132272779270659}
episode index:3457
target Thresh 31.999999999999975
target distance 6.0
model initialize at round 3457
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 23.]), 'previousTarget': array([10., 23.]), 'currentState': array([ 5.21924231, 26.56805068,  6.02752221]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 5.965453020354748}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.930336446798294
{'scaleFactor': 20, 'currentTarget': array([10., 23.]), 'previousTarget': array([10., 23.]), 'currentState': array([ 9.93118879, 23.08629096,  6.07226223]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 0.11036807633288788}
episode index:3458
target Thresh 31.999999999999975
target distance 10.0
model initialize at round 3458
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([15.97408496,  9.98927561,  1.40394026]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 8.563169220485428}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9303451948651346
{'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([18.75472564, 17.37483101,  1.6925538 ]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 0.6715621944748166}
episode index:3459
target Thresh 31.999999999999975
target distance 14.0
model initialize at round 3459
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 17.]), 'previousTarget': array([13., 17.]), 'currentState': array([18.70191258,  4.52961002,  2.1505796 ]), 'targetState': array([13, 17], dtype=int32), 'currentDistance': 13.71212723832067}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9303456920191929
{'scaleFactor': 20, 'currentTarget': array([13., 17.]), 'previousTarget': array([13., 17.]), 'currentState': array([12.8988641 , 17.06306443,  2.56547196]), 'targetState': array([13, 17], dtype=int32), 'currentDistance': 0.11918721565689916}
episode index:3460
target Thresh 31.999999999999975
target distance 6.0
model initialize at round 3460
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 10.]), 'previousTarget': array([20., 10.]), 'currentState': array([15.06015761, 14.60390388,  5.12988162]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 6.7526271750517495}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9303572358816549
{'scaleFactor': 20, 'currentTarget': array([20., 10.]), 'previousTarget': array([20., 10.]), 'currentState': array([19.38734688, 10.55016588,  5.74044907]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 0.8234235533451962}
episode index:3461
target Thresh 31.999999999999975
target distance 8.0
model initialize at round 3461
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 24.]), 'previousTarget': array([27., 24.]), 'currentState': array([21.60902804, 14.4310918 ,  6.09266949]), 'targetState': array([27, 24], dtype=int32), 'currentDistance': 10.9830133705083}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9303604487394016
{'scaleFactor': 20, 'currentTarget': array([27., 24.]), 'previousTarget': array([27., 24.]), 'currentState': array([26.95785811, 24.39980143,  0.96506725]), 'targetState': array([27, 24], dtype=int32), 'currentDistance': 0.4020163197490327}
episode index:3462
target Thresh 31.999999999999975
target distance 23.0
model initialize at round 3462
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 27.]), 'previousTarget': array([23.67925571, 25.73667938]), 'currentState': array([3.42036481, 6.40466925, 0.69468708]), 'targetState': array([25, 27], dtype=int32), 'currentDistance': 29.830325239914306}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.9303401478170517
{'scaleFactor': 20, 'currentTarget': array([25., 27.]), 'previousTarget': array([25., 27.]), 'currentState': array([25.00521749, 27.03008875,  1.06214413]), 'targetState': array([25, 27], dtype=int32), 'currentDistance': 0.03053776780904088}
episode index:3463
target Thresh 31.999999999999975
target distance 10.0
model initialize at round 3463
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  5.]), 'previousTarget': array([17.,  5.]), 'currentState': array([25.79618868, 12.40957181,  3.93645141]), 'targetState': array([17,  5], dtype=int32), 'currentDistance': 11.501073414443312}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9303433637528437
{'scaleFactor': 20, 'currentTarget': array([17.,  5.]), 'previousTarget': array([17.,  5.]), 'currentState': array([16.69745343,  4.62006153,  4.03545628]), 'targetState': array([17,  5], dtype=int32), 'currentDistance': 0.485682677435248}
episode index:3464
target Thresh 31.999999999999975
target distance 11.0
model initialize at round 3464
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'previousTarget': array([25.,  3.]), 'currentState': array([15.25119788,  4.12556295,  6.00577289]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 9.813563813934667}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9303493223924244
{'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'previousTarget': array([25.,  3.]), 'currentState': array([25.06475542,  2.99883504,  0.12867067]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 0.06476590015614597}
episode index:3465
target Thresh 31.99999999999998
target distance 14.0
model initialize at round 3465
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 12.]), 'previousTarget': array([ 8., 12.]), 'currentState': array([14.54871201, 25.34130493,  4.87104535]), 'targetState': array([ 8, 12], dtype=int32), 'currentDistance': 14.861899145624388}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9303471283278069
{'scaleFactor': 20, 'currentTarget': array([ 8., 12.]), 'previousTarget': array([ 8., 12.]), 'currentState': array([ 8.14245709, 11.08089105,  4.35346464]), 'targetState': array([ 8, 12], dtype=int32), 'currentDistance': 0.9300834859277306}
episode index:3466
target Thresh 31.99999999999998
target distance 3.0
model initialize at round 3466
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 20.]), 'previousTarget': array([ 3., 20.]), 'currentState': array([ 3.96536546, 18.63303078,  2.58999038]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 1.673480004387587}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9303643342325291
{'scaleFactor': 20, 'currentTarget': array([ 3., 20.]), 'previousTarget': array([ 3., 20.]), 'currentState': array([ 2.63450626, 20.09040519,  2.02607226]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 0.37650865829007757}
episode index:3467
target Thresh 31.99999999999998
target distance 17.0
model initialize at round 3467
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 29.]), 'previousTarget': array([21., 29.]), 'currentState': array([ 5.46245299, 10.38181419,  6.00075388]), 'targetState': array([21, 29], dtype=int32), 'currentDistance': 24.24978783020839}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9303516527266131
{'scaleFactor': 20, 'currentTarget': array([21., 29.]), 'previousTarget': array([21., 29.]), 'currentState': array([20.6647387 , 28.13842387,  0.98071876]), 'targetState': array([21, 29], dtype=int32), 'currentDistance': 0.9245071988303918}
episode index:3468
target Thresh 31.99999999999998
target distance 13.0
model initialize at round 3468
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 11.]), 'previousTarget': array([18., 11.]), 'currentState': array([ 6.21599232, 21.43807913,  5.62505606]), 'targetState': array([18, 11], dtype=int32), 'currentDistance': 15.742183230218759}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9303494598876686
{'scaleFactor': 20, 'currentTarget': array([18., 11.]), 'previousTarget': array([18., 11.]), 'currentState': array([18.27721549, 10.99249251,  5.72377459]), 'targetState': array([18, 11], dtype=int32), 'currentDistance': 0.27731712944731773}
episode index:3469
target Thresh 31.99999999999998
target distance 10.0
model initialize at round 3469
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 26.]), 'previousTarget': array([27., 26.]), 'currentState': array([18.75198695, 20.96427517,  0.52653508]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 9.66375930476136}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9303554081845021
{'scaleFactor': 20, 'currentTarget': array([27., 26.]), 'previousTarget': array([27., 26.]), 'currentState': array([27.14877097, 26.27099452,  0.61855854]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 0.30914532316634447}
episode index:3470
target Thresh 31.99999999999998
target distance 19.0
model initialize at round 3470
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 27.]), 'previousTarget': array([21., 27.]), 'currentState': array([9.51636689, 8.7944941 , 1.63994551]), 'targetState': array([21, 27], dtype=int32), 'currentDistance': 21.524736342776194}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9303453196930225
{'scaleFactor': 20, 'currentTarget': array([21., 27.]), 'previousTarget': array([21., 27.]), 'currentState': array([21.03187354, 27.12117921,  1.319413  ]), 'targetState': array([21, 27], dtype=int32), 'currentDistance': 0.12530093124227232}
episode index:3471
target Thresh 31.99999999999998
target distance 7.0
model initialize at round 3471
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 12.]), 'previousTarget': array([20., 12.]), 'currentState': array([12.26771488, 10.48469588,  5.27222323]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 7.879364171880327}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9303540324494473
{'scaleFactor': 20, 'currentTarget': array([20., 12.]), 'previousTarget': array([20., 12.]), 'currentState': array([19.48974294, 11.91036848,  0.57321643]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 0.5180695675240745}
episode index:3472
target Thresh 31.99999999999998
target distance 5.0
model initialize at round 3472
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'previousTarget': array([10., 12.]), 'currentState': array([ 6.32450841, 16.84151967,  4.24589944]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 6.07861424250441}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9303655340237491
{'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'previousTarget': array([10., 12.]), 'currentState': array([ 9.6158686 , 12.53087349,  5.1738291 ]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 0.655273674323671}
episode index:3473
target Thresh 31.99999999999998
target distance 11.0
model initialize at round 3473
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  5.]), 'previousTarget': array([13.,  5.]), 'currentState': array([22.16634371,  9.59979913,  3.97629118]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 10.2557305418761}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9303714708446692
{'scaleFactor': 20, 'currentTarget': array([13.,  5.]), 'previousTarget': array([13.,  5.]), 'currentState': array([13.35701144,  5.11144417,  3.92578093]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 0.3740013008496083}
episode index:3474
target Thresh 31.99999999999998
target distance 6.0
model initialize at round 3474
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.09369025, 15.00524032,  4.85582381]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 4.1065006231440915}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9303857812127716
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.06833255, 11.17846919,  5.34361189]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.19110361407835647}
episode index:3475
target Thresh 31.99999999999998
target distance 22.0
model initialize at round 3475
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 24.]), 'previousTarget': array([ 6., 24.]), 'currentState': array([20.67251656,  1.81271293,  0.89848536]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 26.599970855449513}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9303680481953827
{'scaleFactor': 20, 'currentTarget': array([ 6., 24.]), 'previousTarget': array([ 6., 24.]), 'currentState': array([ 5.77708982, 24.52809845,  2.2327031 ]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 0.573216294296405}
episode index:3476
target Thresh 31.99999999999998
target distance 22.0
model initialize at round 3476
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  4.]), 'previousTarget': array([20.,  4.]), 'currentState': array([22.31703239, 25.99714978,  4.15328622]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 22.118843497248985}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9303579734775406
{'scaleFactor': 20, 'currentTarget': array([20.,  4.]), 'previousTarget': array([20.,  4.]), 'currentState': array([20.14718867,  4.55390343,  4.85117725]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 0.5731260923338276}
episode index:3477
target Thresh 31.99999999999998
target distance 11.0
model initialize at round 3477
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([7.09711868, 9.72378737, 0.14928358]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 15.00733253372098}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9303584643845071
{'scaleFactor': 20, 'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([16.2285437 , 20.01718718,  1.15772028]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 1.2494262105565852}
episode index:3478
target Thresh 31.99999999999998
target distance 12.0
model initialize at round 3478
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 17.]), 'previousTarget': array([15., 17.]), 'currentState': array([15.6708419 , 27.07690088,  4.9313066 ]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 10.099205919153823}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9303643947051498
{'scaleFactor': 20, 'currentTarget': array([15., 17.]), 'previousTarget': array([15., 17.]), 'currentState': array([15.01826926, 17.21041714,  4.99017681]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 0.21120875970214115}
episode index:3479
target Thresh 31.99999999999998
target distance 1.0
model initialize at round 3479
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 22.]), 'previousTarget': array([ 9., 22.]), 'currentState': array([10.11752642, 22.6810509 ,  2.50732946]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 1.30869997322118}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9303815313733378
{'scaleFactor': 20, 'currentTarget': array([ 9., 22.]), 'previousTarget': array([ 9., 22.]), 'currentState': array([ 8.48267048, 22.16757625,  4.40346348]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 0.5437937424044348}
episode index:3480
target Thresh 31.99999999999998
target distance 7.0
model initialize at round 3480
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  3.]), 'previousTarget': array([21.,  3.]), 'currentState': array([15.00850767,  4.34733083,  6.20142669]), 'targetState': array([21,  3], dtype=int32), 'currentDistance': 6.14111396564975}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9303929986151152
{'scaleFactor': 20, 'currentTarget': array([21.,  3.]), 'previousTarget': array([21.,  3.]), 'currentState': array([20.80985927,  3.04649048,  0.12462978]), 'targetState': array([21,  3], dtype=int32), 'currentDistance': 0.19574182056650166}
episode index:3481
target Thresh 31.999999999999982
target distance 14.0
model initialize at round 3481
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([19.12155548, 19.34470132,  5.26780631]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 13.672884136987765}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9303934788992312
{'scaleFactor': 20, 'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([25.13229654,  6.73492513,  5.32353116]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 0.2962550633959915}
episode index:3482
target Thresh 31.999999999999982
target distance 9.0
model initialize at round 3482
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([19.32172552, 24.12562985,  4.07687545]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 7.960981507421847}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9304021503121225
{'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([12.12173351, 21.09411618,  3.8873384 ]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 0.15387300900566248}
episode index:3483
target Thresh 31.999999999999982
target distance 4.0
model initialize at round 3483
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 15.]), 'previousTarget': array([ 4., 15.]), 'currentState': array([ 6.51633098, 11.99365622,  1.72797132]), 'targetState': array([ 4, 15], dtype=int32), 'currentDistance': 3.92046228010653}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9304164149073257
{'scaleFactor': 20, 'currentTarget': array([ 4., 15.]), 'previousTarget': array([ 4., 15.]), 'currentState': array([ 3.95260482, 14.89637146,  2.0456714 ]), 'targetState': array([ 4, 15], dtype=int32), 'currentDistance': 0.113952523611999}
episode index:3484
target Thresh 31.999999999999982
target distance 22.0
model initialize at round 3484
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 4.]), 'previousTarget': array([5., 4.]), 'currentState': array([26.22752716, 15.49521697,  3.05765867]), 'targetState': array([5, 4], dtype=int32), 'currentDistance': 24.14017237675293}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9304037803181747
{'scaleFactor': 20, 'currentTarget': array([5., 4.]), 'previousTarget': array([5., 4.]), 'currentState': array([5.45529666, 4.17910548, 4.02395472]), 'targetState': array([5, 4], dtype=int32), 'currentDistance': 0.4892584410426257}
episode index:3485
target Thresh 31.999999999999982
target distance 19.0
model initialize at round 3485
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 26.]), 'previousTarget': array([20., 26.]), 'currentState': array([23.46485411,  8.61749768,  2.30094728]), 'targetState': array([20, 26], dtype=int32), 'currentDistance': 17.724463348177824}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9303989362181074
{'scaleFactor': 20, 'currentTarget': array([20., 26.]), 'previousTarget': array([20., 26.]), 'currentState': array([19.9455959 , 26.12931127,  1.97764093]), 'targetState': array([20, 26], dtype=int32), 'currentDistance': 0.14028973569815195}
episode index:3486
target Thresh 31.999999999999982
target distance 22.0
model initialize at round 3486
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 28.]), 'previousTarget': array([25., 28.]), 'currentState': array([20.12875874,  7.99120516,  1.3870614 ]), 'targetState': array([25, 28], dtype=int32), 'currentDistance': 20.593223698365822}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9303914751165274
{'scaleFactor': 20, 'currentTarget': array([25., 28.]), 'previousTarget': array([25., 28.]), 'currentState': array([24.89786763, 27.27390181,  1.87848413]), 'targetState': array([25, 28], dtype=int32), 'currentDistance': 0.733245938676393}
episode index:3487
target Thresh 31.999999999999982
target distance 20.0
model initialize at round 3487
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([8.66687223, 4.02720905, 1.47508407]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 18.97571528319483}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9303840182931021
{'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([ 8.40655111, 23.7901033 ,  1.76518417]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 0.9881522221059001}
episode index:3488
target Thresh 31.999999999999982
target distance 14.0
model initialize at round 3488
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  8.]), 'previousTarget': array([24.,  8.]), 'currentState': array([11.68143059, 13.07196697,  5.31596023]), 'targetState': array([24,  8], dtype=int32), 'currentDistance': 13.321861774012397}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9303845001875172
{'scaleFactor': 20, 'currentTarget': array([24.,  8.]), 'previousTarget': array([24.,  8.]), 'currentState': array([24.4639845 ,  7.8868201 ,  5.98503328]), 'targetState': array([24,  8], dtype=int32), 'currentDistance': 0.47758906010256835}
episode index:3489
target Thresh 31.999999999999982
target distance 12.0
model initialize at round 3489
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([ 5.93987321, 11.66663977,  0.14200258]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 10.082190216198326}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9303904043564892
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.84168982, 11.08548262,  0.3487967 ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.1799149516562436}
episode index:3490
target Thresh 31.999999999999982
target distance 12.0
model initialize at round 3490
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 17.]), 'previousTarget': array([27., 17.]), 'currentState': array([17.15313732, 27.32401162,  5.81350732]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 14.26695204240269}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9303908841455327
{'scaleFactor': 20, 'currentTarget': array([27., 17.]), 'previousTarget': array([27., 17.]), 'currentState': array([26.74256862, 17.29926758,  5.95097471]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 0.3947556185311196}
episode index:3491
target Thresh 31.999999999999982
target distance 22.0
model initialize at round 3491
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 6.]), 'previousTarget': array([8., 6.]), 'currentState': array([17.25645519, 26.20623642,  4.7434783 ]), 'targetState': array([8, 6], dtype=int32), 'currentDistance': 22.225524809385764}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9303808461644655
{'scaleFactor': 20, 'currentTarget': array([8., 6.]), 'previousTarget': array([8., 6.]), 'currentState': array([8.11865982, 6.35989236, 4.63606346]), 'targetState': array([8, 6], dtype=int32), 'currentDistance': 0.3789494252969798}
episode index:3492
target Thresh 31.999999999999982
target distance 20.0
model initialize at round 3492
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  4.]), 'previousTarget': array([26.,  4.]), 'currentState': array([14.81764608, 22.29203651,  5.73420382]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 21.439301263036477}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9303708139308823
{'scaleFactor': 20, 'currentTarget': array([26.,  4.]), 'previousTarget': array([26.,  4.]), 'currentState': array([26.41482147,  3.76775185,  5.37575961]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 0.47541145790125244}
episode index:3493
target Thresh 31.999999999999982
target distance 22.0
model initialize at round 3493
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 17.]), 'previousTarget': array([ 3., 17.]), 'currentState': array([25.33674212, 18.35106312,  3.90383673]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 22.3775651089835}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9303607874398484
{'scaleFactor': 20, 'currentTarget': array([ 3., 17.]), 'previousTarget': array([ 3., 17.]), 'currentState': array([ 3.60310708, 17.12807254,  3.60751057]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 0.6165555354492613}
episode index:3494
target Thresh 31.999999999999982
target distance 5.0
model initialize at round 3494
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 25.]), 'previousTarget': array([19., 25.]), 'currentState': array([25.04082742, 23.32252272,  1.91403168]), 'targetState': array([19, 25], dtype=int32), 'currentDistance': 6.2694119327986675}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.930372214682355
{'scaleFactor': 20, 'currentTarget': array([19., 25.]), 'previousTarget': array([19., 25.]), 'currentState': array([19.68711647, 24.77358391,  2.76445691]), 'targetState': array([19, 25], dtype=int32), 'currentDistance': 0.7234592551936742}
episode index:3495
target Thresh 31.999999999999982
target distance 19.0
model initialize at round 3495
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 23.]), 'previousTarget': array([27., 23.]), 'currentState': array([20.06065829,  5.65301768,  1.33295429]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 18.683475563833092}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9303673934674811
{'scaleFactor': 20, 'currentTarget': array([27., 23.]), 'previousTarget': array([27., 23.]), 'currentState': array([26.80428397, 22.22151576,  1.72146374]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 0.8027094623264418}
episode index:3496
target Thresh 31.999999999999982
target distance 13.0
model initialize at round 3496
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([13.66506944, 11.75519018,  0.86401909]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 12.489542863346289}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9303705712644309
{'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([24.39269488, 16.85379665,  0.53574981]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 0.6246558477356989}
episode index:3497
target Thresh 31.999999999999982
target distance 22.0
model initialize at round 3497
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 15.]), 'previousTarget': array([ 3., 15.]), 'currentState': array([23.12543262, 13.47481365,  3.00940847]), 'targetState': array([ 3, 15], dtype=int32), 'currentDistance': 20.183142265386063}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9303631417343408
{'scaleFactor': 20, 'currentTarget': array([ 3., 15.]), 'previousTarget': array([ 3., 15.]), 'currentState': array([ 3.46910404, 14.83790166,  2.98179739]), 'targetState': array([ 3, 15], dtype=int32), 'currentDistance': 0.4963209360926173}
episode index:3498
target Thresh 31.999999999999982
target distance 16.0
model initialize at round 3498
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([10.80395393,  4.52147162,  6.22042322]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 17.768333198537242}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9303583272461295
{'scaleFactor': 20, 'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([13.8875024 , 21.52627794,  1.89564364]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 0.4868965983876413}
episode index:3499
target Thresh 31.999999999999982
target distance 16.0
model initialize at round 3499
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 10.]), 'previousTarget': array([27., 10.]), 'currentState': array([20.67463523, 25.83271307,  5.17362106]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 17.04948804270752}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9303535155090545
{'scaleFactor': 20, 'currentTarget': array([27., 10.]), 'previousTarget': array([27., 10.]), 'currentState': array([27.8681462 ,  9.53767659,  5.43055571]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 0.9835754951852748}
episode index:3500
target Thresh 31.999999999999982
target distance 4.0
model initialize at round 3500
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 24.]), 'previousTarget': array([17., 24.]), 'currentState': array([21.30374879, 22.34466778,  3.88386726]), 'targetState': array([17, 24], dtype=int32), 'currentDistance': 4.611114655996784}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9303649252446989
{'scaleFactor': 20, 'currentTarget': array([17., 24.]), 'previousTarget': array([17., 24.]), 'currentState': array([16.10346625, 23.40592958,  3.07419279]), 'targetState': array([17, 24], dtype=int32), 'currentDistance': 1.0754963601448668}
episode index:3501
target Thresh 31.999999999999986
target distance 3.0
model initialize at round 3501
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 15.]), 'previousTarget': array([19., 15.]), 'currentState': array([17.43564464, 13.94604529,  5.91836387]), 'targetState': array([19, 15], dtype=int32), 'currentDistance': 1.8862736342878461}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9303819541067078
{'scaleFactor': 20, 'currentTarget': array([19., 15.]), 'previousTarget': array([19., 15.]), 'currentState': array([18.89292081, 14.86161353,  1.50542378]), 'targetState': array([19, 15], dtype=int32), 'currentDistance': 0.17497647504348912}
episode index:3502
target Thresh 31.999999999999986
target distance 12.0
model initialize at round 3502
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 15.]), 'previousTarget': array([19., 15.]), 'currentState': array([24.3216498 , 26.87538585,  4.22570467]), 'targetState': array([19, 15], dtype=int32), 'currentDistance': 13.013252691170342}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9303851223040512
{'scaleFactor': 20, 'currentTarget': array([19., 15.]), 'previousTarget': array([19., 15.]), 'currentState': array([19.38723353, 15.96358819,  4.5538575 ]), 'targetState': array([19, 15], dtype=int32), 'currentDistance': 1.0384854375541392}
episode index:3503
target Thresh 31.999999999999986
target distance 12.0
model initialize at round 3503
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'previousTarget': array([7., 3.]), 'currentState': array([17.24164221,  2.35570533,  3.7488476 ]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 10.261888268222755}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9303910027057625
{'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'previousTarget': array([7., 3.]), 'currentState': array([7.46241292, 2.88924404, 3.31287225]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 0.47549194592130484}
episode index:3504
target Thresh 31.999999999999986
target distance 6.0
model initialize at round 3504
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([9.82271646, 2.53182913, 4.21315765]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 6.843413042934441}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9303996203968592
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.23226356, 1.96407726, 3.26709345]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.7685764051189873}
episode index:3505
target Thresh 31.999999999999986
target distance 7.0
model initialize at round 3505
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 27.]), 'previousTarget': array([ 7., 27.]), 'currentState': array([12.7169667 , 22.10353767,  3.42547488]), 'targetState': array([ 7, 27], dtype=int32), 'currentDistance': 7.527220708931486}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9304082331719884
{'scaleFactor': 20, 'currentTarget': array([ 7., 27.]), 'previousTarget': array([ 7., 27.]), 'currentState': array([ 6.79930675, 26.9771855 ,  2.73301313]), 'targetState': array([ 7, 27], dtype=int32), 'currentDistance': 0.20198584253336693}
episode index:3506
target Thresh 31.999999999999986
target distance 24.0
model initialize at round 3506
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 13.]), 'previousTarget': array([26., 13.]), 'currentState': array([ 2.6578029 , 22.54909116,  0.15923238]), 'targetState': array([26, 13], dtype=int32), 'currentDistance': 25.21989904045854}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9303931527014515
{'scaleFactor': 20, 'currentTarget': array([26., 13.]), 'previousTarget': array([26., 13.]), 'currentState': array([26.62898037, 12.89126092,  6.07987997]), 'targetState': array([26, 13], dtype=int32), 'currentDistance': 0.6383106571674642}
episode index:3507
target Thresh 31.999999999999986
target distance 13.0
model initialize at round 3507
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 27.]), 'previousTarget': array([ 4., 27.]), 'currentState': array([ 9.37986026, 13.54443958,  2.40570068]), 'targetState': array([ 4, 27], dtype=int32), 'currentDistance': 14.491204317316106}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9303936293819548
{'scaleFactor': 20, 'currentTarget': array([ 4., 27.]), 'previousTarget': array([ 4., 27.]), 'currentState': array([ 4.05548068, 26.31432799,  1.9517312 ]), 'targetState': array([ 4, 27], dtype=int32), 'currentDistance': 0.6879129421648686}
episode index:3508
target Thresh 31.999999999999986
target distance 20.0
model initialize at round 3508
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 12.]), 'previousTarget': array([ 2., 12.]), 'currentState': array([20.04721313,  3.24054339,  2.7030943 ]), 'targetState': array([ 2, 12], dtype=int32), 'currentDistance': 20.060657566655586}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9303862165707912
{'scaleFactor': 20, 'currentTarget': array([ 2., 12.]), 'previousTarget': array([ 2., 12.]), 'currentState': array([ 2.21065906, 11.71256449,  2.67334795]), 'targetState': array([ 2, 12], dtype=int32), 'currentDistance': 0.3563655544866511}
episode index:3509
target Thresh 31.999999999999986
target distance 15.0
model initialize at round 3509
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 29.]), 'previousTarget': array([ 2., 29.]), 'currentState': array([15.78625357, 27.83414937,  2.89686584]), 'targetState': array([ 2, 29], dtype=int32), 'currentDistance': 13.83546151503966}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9303866949557873
{'scaleFactor': 20, 'currentTarget': array([ 2., 29.]), 'previousTarget': array([ 2., 29.]), 'currentState': array([ 1.88589539, 28.95186585,  3.39252533]), 'targetState': array([ 2, 29], dtype=int32), 'currentDistance': 0.12384166994670207}
episode index:3510
target Thresh 31.999999999999986
target distance 17.0
model initialize at round 3510
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 26.]), 'previousTarget': array([ 9., 26.]), 'currentState': array([8.64073473, 8.62532998, 0.7854945 ]), 'targetState': array([ 9, 26], dtype=int32), 'currentDistance': 17.378383979403605}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9303818902142686
{'scaleFactor': 20, 'currentTarget': array([ 9., 26.]), 'previousTarget': array([ 9., 26.]), 'currentState': array([ 8.97315787, 26.3216309 ,  1.75348976]), 'targetState': array([ 9, 26], dtype=int32), 'currentDistance': 0.3227490246385516}
episode index:3511
target Thresh 31.999999999999986
target distance 11.0
model initialize at round 3511
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 16.]), 'previousTarget': array([16., 16.]), 'currentState': array([27.57452049, 22.5818705 ,  2.23241909]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 13.315049525722753}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9303823695587142
{'scaleFactor': 20, 'currentTarget': array([16., 16.]), 'previousTarget': array([16., 16.]), 'currentState': array([15.89261344, 16.06443937,  3.72930332]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 0.12523699926298926}
episode index:3512
target Thresh 31.999999999999986
target distance 13.0
model initialize at round 3512
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 16.]), 'previousTarget': array([ 3., 16.]), 'currentState': array([ 8.72422857, 27.90236831,  4.86207461]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 13.207314791602808}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9303828486302623
{'scaleFactor': 20, 'currentTarget': array([ 3., 16.]), 'previousTarget': array([ 3., 16.]), 'currentState': array([ 2.75227654, 15.44718051,  4.36701173]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 0.605785694214333}
episode index:3513
target Thresh 31.999999999999986
target distance 18.0
model initialize at round 3513
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([20.31749416, 25.96047543,  4.17507982]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 18.159694803263772}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9303780490852575
{'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([22.9144138 ,  8.40216204,  5.24864233]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 0.41116821635603396}
episode index:3514
target Thresh 31.999999999999986
target distance 13.0
model initialize at round 3514
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 12.]), 'previousTarget': array([12., 12.]), 'currentState': array([24.61096527,  9.63738818,  2.81406575]), 'targetState': array([12, 12], dtype=int32), 'currentDistance': 12.830369428107756}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9303812075775236
{'scaleFactor': 20, 'currentTarget': array([12., 12.]), 'previousTarget': array([12., 12.]), 'currentState': array([12.90540904, 11.89156218,  3.34832903]), 'targetState': array([12, 12], dtype=int32), 'currentDistance': 0.9118795371039936}
episode index:3515
target Thresh 31.999999999999986
target distance 7.0
model initialize at round 3515
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([ 8.63965825, 13.62064673,  0.7826392 ]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 9.120752075811124}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.930389801093571
{'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([13.31806884, 20.09078812,  0.96227096]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 1.1365281994364047}
episode index:3516
target Thresh 31.999999999999986
target distance 12.0
model initialize at round 3516
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.67546879,  2.8412788 ,  0.91554945]), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 12.378931563356318}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9303929544482219
{'scaleFactor': 20, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.75701198, 14.47893828,  1.44293035]), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.5749334690034981}
episode index:3517
target Thresh 31.999999999999986
target distance 22.0
model initialize at round 3517
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 20.]), 'previousTarget': array([ 5., 20.]), 'currentState': array([27.94444851, 15.39298424,  1.984986  ]), 'targetState': array([ 5, 20], dtype=int32), 'currentDistance': 23.402399694819508}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9303804450443753
{'scaleFactor': 20, 'currentTarget': array([ 5., 20.]), 'previousTarget': array([ 5., 20.]), 'currentState': array([ 5.16122197, 19.86547155,  2.97809997]), 'targetState': array([ 5, 20], dtype=int32), 'currentDistance': 0.20997720770726452}
episode index:3518
target Thresh 31.999999999999986
target distance 5.0
model initialize at round 3518
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'previousTarget': array([16., 23.]), 'currentState': array([19.14897446, 20.70148879,  2.52841318]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 3.898614361705428}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9303945739318308
{'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'previousTarget': array([16., 23.]), 'currentState': array([15.92286546, 22.98130307,  2.91790007]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 0.0793682134950049}
episode index:3519
target Thresh 31.999999999999986
target distance 14.0
model initialize at round 3519
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 18.]), 'previousTarget': array([26., 18.]), 'currentState': array([13.98881939, 20.17350539,  6.26482183]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 12.206251897946823}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9303977232430436
{'scaleFactor': 20, 'currentTarget': array([26., 18.]), 'previousTarget': array([26., 18.]), 'currentState': array([25.67530238, 18.13033331,  6.22116817]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 0.34987899760497493}
episode index:3520
target Thresh 31.999999999999986
target distance 15.0
model initialize at round 3520
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 25.]), 'previousTarget': array([18., 25.]), 'currentState': array([ 3.79713718, 22.4822147 ,  0.06736684]), 'targetState': array([18, 25], dtype=int32), 'currentDistance': 14.424304324216834}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9303981968654986
{'scaleFactor': 20, 'currentTarget': array([18., 25.]), 'previousTarget': array([18., 25.]), 'currentState': array([17.45924823, 25.02371446,  0.28263494]), 'targetState': array([18, 25], dtype=int32), 'currentDistance': 0.5412715126711802}
episode index:3521
target Thresh 31.999999999999986
target distance 4.0
model initialize at round 3521
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  6.]), 'previousTarget': array([19.,  6.]), 'currentState': array([21.5373907 ,  7.03510556,  2.83691239]), 'targetState': array([19,  6], dtype=int32), 'currentDistance': 2.7404005354937637}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9304151195807553
{'scaleFactor': 20, 'currentTarget': array([19.,  6.]), 'previousTarget': array([19.,  6.]), 'currentState': array([19.78908127,  6.40366115,  4.15261948]), 'targetState': array([19,  6], dtype=int32), 'currentDistance': 0.8863360378191122}
episode index:3522
target Thresh 31.999999999999986
target distance 12.0
model initialize at round 3522
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'previousTarget': array([13., 22.]), 'currentState': array([22.00389242, 11.67910742,  2.48953205]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 13.696382818681128}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9304155879964029
{'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'previousTarget': array([13., 22.]), 'currentState': array([12.77878435, 22.15003906,  2.26015074]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 0.2672977435755064}
episode index:3523
target Thresh 31.999999999999986
target distance 16.0
model initialize at round 3523
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 13.]), 'previousTarget': array([ 2., 13.]), 'currentState': array([14.80210337, 27.52046687,  4.19917202]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 19.358145800412508}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9304082005069059
{'scaleFactor': 20, 'currentTarget': array([ 2., 13.]), 'previousTarget': array([ 2., 13.]), 'currentState': array([ 1.97601958, 12.44480204,  4.04800415]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 0.5557156028157474}
episode index:3524
target Thresh 31.999999999999986
target distance 11.0
model initialize at round 3524
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'previousTarget': array([27.,  8.]), 'currentState': array([17.85138006,  7.61043598,  0.37250644]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 9.156910332910671}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9304140393294288
{'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'previousTarget': array([27.,  8.]), 'currentState': array([27.66787467,  8.41910903,  6.2698472 ]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 0.7884852260717663}
episode index:3525
target Thresh 31.999999999999986
target distance 10.0
model initialize at round 3525
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 26.]), 'previousTarget': array([10., 26.]), 'currentState': array([18.43522779, 20.3804227 ,  2.50860596]), 'targetState': array([10, 26], dtype=int32), 'currentDistance': 10.135714922108594}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9304198748400841
{'scaleFactor': 20, 'currentTarget': array([10., 26.]), 'previousTarget': array([10., 26.]), 'currentState': array([10.10986414, 25.85897978,  2.87659907]), 'targetState': array([10, 26], dtype=int32), 'currentDistance': 0.17876473531076434}
episode index:3526
target Thresh 31.999999999999986
target distance 7.0
model initialize at round 3526
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 20.]), 'previousTarget': array([12., 20.]), 'currentState': array([ 9.841164  , 14.80952957,  1.25327295]), 'targetState': array([12, 20], dtype=int32), 'currentDistance': 5.621526136393358}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9304311816518676
{'scaleFactor': 20, 'currentTarget': array([12., 20.]), 'previousTarget': array([12., 20.]), 'currentState': array([11.91863728, 20.41821128,  1.44712564]), 'targetState': array([12, 20], dtype=int32), 'currentDistance': 0.4260523082229145}
episode index:3527
target Thresh 31.99999999999999
target distance 13.0
model initialize at round 3527
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 21.]), 'previousTarget': array([27., 21.]), 'currentState': array([22.57182206,  7.3985322 ,  0.64453381]), 'targetState': array([27, 21], dtype=int32), 'currentDistance': 14.30414926149265}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9304316448509194
{'scaleFactor': 20, 'currentTarget': array([27., 21.]), 'previousTarget': array([27., 21.]), 'currentState': array([26.46142573, 20.47618706,  1.26510182]), 'targetState': array([27, 21], dtype=int32), 'currentDistance': 0.751293711452335}
episode index:3528
target Thresh 31.99999999999999
target distance 10.0
model initialize at round 3528
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 23.]), 'previousTarget': array([18., 23.]), 'currentState': array([21.58336858, 14.32881368,  1.96615523]), 'targetState': array([18, 23], dtype=int32), 'currentDistance': 9.382430529089682}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9304374704119989
{'scaleFactor': 20, 'currentTarget': array([18., 23.]), 'previousTarget': array([18., 23.]), 'currentState': array([17.54215484, 23.45972699,  2.0279531 ]), 'targetState': array([18, 23], dtype=int32), 'currentDistance': 0.6488228538737494}
episode index:3529
target Thresh 31.99999999999999
target distance 1.0
model initialize at round 3529
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 11.]), 'previousTarget': array([27., 11.]), 'currentState': array([27.67974867,  9.89150661,  0.94503301]), 'targetState': array([27, 11], dtype=int32), 'currentDistance': 1.300313751427392}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9304543436498425
{'scaleFactor': 20, 'currentTarget': array([27., 11.]), 'previousTarget': array([27., 11.]), 'currentState': array([27.09708469, 11.47835351,  2.920111  ]), 'targetState': array([27, 11], dtype=int32), 'currentDistance': 0.4881060493464064}
episode index:3530
target Thresh 31.99999999999999
target distance 12.0
model initialize at round 3530
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 6.]), 'previousTarget': array([9., 6.]), 'currentState': array([12.18330987, 19.47153162,  3.08746505]), 'targetState': array([9, 6], dtype=int32), 'currentDistance': 13.842529610366372}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.930454799895738
{'scaleFactor': 20, 'currentTarget': array([9., 6.]), 'previousTarget': array([9., 6.]), 'currentState': array([9.02900992, 6.44458595, 4.61321642]), 'targetState': array([9, 6], dtype=int32), 'currentDistance': 0.4455314138091799}
episode index:3531
target Thresh 31.99999999999999
target distance 22.0
model initialize at round 3531
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  7.]), 'previousTarget': array([17.,  7.]), 'currentState': array([ 3.30278712, 27.60417388,  4.93292534]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 24.741576784816143}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9304423225661288
{'scaleFactor': 20, 'currentTarget': array([17.,  7.]), 'previousTarget': array([17.,  7.]), 'currentState': array([16.45929231,  7.87048917,  5.80529491]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 1.0247517769658183}
episode index:3532
target Thresh 31.99999999999999
target distance 14.0
model initialize at round 3532
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 19.]), 'previousTarget': array([ 4., 19.]), 'currentState': array([16.32268623, 26.13786443,  4.06958342]), 'targetState': array([ 4, 19], dtype=int32), 'currentDistance': 14.240705901420045}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9304427819562622
{'scaleFactor': 20, 'currentTarget': array([ 4., 19.]), 'previousTarget': array([ 4., 19.]), 'currentState': array([ 4.22663759, 19.30026459,  3.76329851]), 'targetState': array([ 4, 19], dtype=int32), 'currentDistance': 0.37619598929741294}
episode index:3533
target Thresh 31.99999999999999
target distance 17.0
model initialize at round 3533
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  9.]), 'previousTarget': array([20.,  9.]), 'currentState': array([ 4.66947405, 17.78729251,  5.14645821]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 17.670357536334038}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.930437992614306
{'scaleFactor': 20, 'currentTarget': array([20.,  9.]), 'previousTarget': array([20.,  9.]), 'currentState': array([20.04672652,  8.92833321,  5.69286646]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 0.08555405944377682}
episode index:3534
target Thresh 31.99999999999999
target distance 7.0
model initialize at round 3534
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([17.35479118, 10.35450814,  3.93935859]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 5.5234482781241265}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9304492687125765
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([11.62986154,  8.93287892,  3.36625597]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.3761751159168126}
episode index:3535
target Thresh 31.99999999999999
target distance 17.0
model initialize at round 3535
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([16.6112306 , 26.15221905,  3.91514301]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 19.089536876615636}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9304444802450342
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.75645963, 11.87300671,  4.43726749]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.1551501610506778}
episode index:3536
target Thresh 31.99999999999999
target distance 10.0
model initialize at round 3536
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([24.31753467,  8.34725688,  3.8922019 ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 10.653101318195034}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9304476003098224
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.05489058, 10.72503881,  2.85841623]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.9842944054746576}
episode index:3537
target Thresh 31.99999999999999
target distance 14.0
model initialize at round 3537
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 12.]), 'previousTarget': array([26., 12.]), 'currentState': array([23.2260575 , 24.06879095,  5.24318868]), 'targetState': array([26, 12], dtype=int32), 'currentDistance': 12.383475766148491}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9304507186108657
{'scaleFactor': 20, 'currentTarget': array([26., 12.]), 'previousTarget': array([26., 12.]), 'currentState': array([25.88303791, 12.44414241,  5.25509533]), 'targetState': array([26, 12], dtype=int32), 'currentDistance': 0.45928489001331185}
episode index:3538
target Thresh 31.99999999999999
target distance 15.0
model initialize at round 3538
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 19.]), 'previousTarget': array([16., 19.]), 'currentState': array([22.95154699,  5.31650681,  1.23336005]), 'targetState': array([16, 19], dtype=int32), 'currentDistance': 15.348028905621636}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.930448541152775
{'scaleFactor': 20, 'currentTarget': array([16., 19.]), 'previousTarget': array([16., 19.]), 'currentState': array([15.80264865, 19.26673029,  1.8982335 ]), 'targetState': array([16, 19], dtype=int32), 'currentDistance': 0.3318020481091938}
episode index:3539
target Thresh 31.99999999999999
target distance 21.0
model initialize at round 3539
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  2.]), 'previousTarget': array([23.,  2.]), 'currentState': array([ 3.68274107, 20.02775913,  5.28968019]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 26.42265300288467}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9304335898764604
{'scaleFactor': 20, 'currentTarget': array([23.,  2.]), 'previousTarget': array([23.,  2.]), 'currentState': array([22.55940087,  2.64436311,  5.93562193]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 0.7805968313655983}
episode index:3540
target Thresh 31.99999999999999
target distance 20.0
model initialize at round 3540
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 26.]), 'previousTarget': array([24., 26.]), 'currentState': array([ 4.72737526, 11.51766707,  0.11386824]), 'targetState': array([24, 26], dtype=int32), 'currentDistance': 24.10750986190889}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9304211502497559
{'scaleFactor': 20, 'currentTarget': array([24., 26.]), 'previousTarget': array([24., 26.]), 'currentState': array([23.82478952, 25.62689132,  0.83444981]), 'targetState': array([24, 26], dtype=int32), 'currentDistance': 0.4121999443976371}
episode index:3541
target Thresh 31.99999999999999
target distance 5.0
model initialize at round 3541
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 6.28363942, 13.28451767,  5.78885424]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 3.36173003091608}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9304351758990361
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([7.01649803, 9.73092406, 5.53446925]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.2695812438821678}
episode index:3542
target Thresh 31.99999999999999
target distance 14.0
model initialize at round 3542
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 27.]), 'previousTarget': array([20., 27.]), 'currentState': array([24.27306888, 14.10076506,  1.72294062]), 'targetState': array([20, 27], dtype=int32), 'currentDistance': 13.588575337042114}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9304356360096792
{'scaleFactor': 20, 'currentTarget': array([20., 27.]), 'previousTarget': array([20., 27.]), 'currentState': array([19.84109734, 27.20072091,  1.86304327]), 'targetState': array([20, 27], dtype=int32), 'currentDistance': 0.2560057419216471}
episode index:3543
target Thresh 31.99999999999999
target distance 23.0
model initialize at round 3543
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([ 1.55542369, 19.37681178,  5.45505428]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 25.63837711285277}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9304207052498004
{'scaleFactor': 20, 'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([25.12207171,  8.78390967,  5.79235981]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 0.2481864936191833}
episode index:3544
target Thresh 31.99999999999999
target distance 5.0
model initialize at round 3544
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([12.35242418, 19.65666051,  4.35704303]), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 5.356691595406994}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9304319544161614
{'scaleFactor': 20, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.93248656, 14.63807002,  5.26601979]), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.3681730237872918}
episode index:3545
target Thresh 31.99999999999999
target distance 7.0
model initialize at round 3545
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 14.]), 'previousTarget': array([17., 14.]), 'currentState': array([22.55805325, 12.13214191,  2.67338276]), 'targetState': array([17, 14], dtype=int32), 'currentDistance': 5.863518548151312}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9304431972378152
{'scaleFactor': 20, 'currentTarget': array([17., 14.]), 'previousTarget': array([17., 14.]), 'currentState': array([16.90448447, 14.09227014,  2.85612294]), 'targetState': array([17, 14], dtype=int32), 'currentDistance': 0.1328043522352134}
episode index:3546
target Thresh 31.99999999999999
target distance 15.0
model initialize at round 3546
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  4.]), 'previousTarget': array([21.,  4.]), 'currentState': array([7.89259178, 8.36853726, 6.08295034]), 'targetState': array([21,  4], dtype=int32), 'currentDistance': 13.816232053789856}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.930443654568142
{'scaleFactor': 20, 'currentTarget': array([21.,  4.]), 'previousTarget': array([21.,  4.]), 'currentState': array([21.05918142,  3.73476195,  5.97995305]), 'targetState': array([21,  4], dtype=int32), 'currentDistance': 0.2717603079844331}
episode index:3547
target Thresh 31.99999999999999
target distance 18.0
model initialize at round 3547
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 22.]), 'previousTarget': array([ 9., 22.]), 'currentState': array([26.47824525, 12.60005002,  2.89601064]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 19.84560698479259}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9304363091398559
{'scaleFactor': 20, 'currentTarget': array([ 9., 22.]), 'previousTarget': array([ 9., 22.]), 'currentState': array([ 8.9089813 , 22.05110585,  2.81902498]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 0.10438492218597159}
episode index:3548
target Thresh 31.99999999999999
target distance 12.0
model initialize at round 3548
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([3.56674806, 8.38543625, 0.63618868]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 11.408196384149564}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9304394209573428
{'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([14.46610399, 13.11114059,  0.40854232]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 0.4791713292193026}
episode index:3549
target Thresh 31.99999999999999
target distance 8.0
model initialize at round 3549
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  8.]), 'previousTarget': array([26.,  8.]), 'currentState': array([16.41710106, 14.42831912,  4.49817538]), 'targetState': array([26,  8], dtype=int32), 'currentDistance': 11.539291085122642}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.930442531021693
{'scaleFactor': 20, 'currentTarget': array([26.,  8.]), 'previousTarget': array([26.,  8.]), 'currentState': array([25.8523603 ,  8.00927315,  5.96634848]), 'targetState': array([26,  8], dtype=int32), 'currentDistance': 0.1479306288989168}
episode index:3550
target Thresh 31.99999999999999
target distance 11.0
model initialize at round 3550
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 28.]), 'previousTarget': array([ 2., 28.]), 'currentState': array([11.39478147, 22.50563381,  1.8264395 ]), 'targetState': array([ 2, 28], dtype=int32), 'currentDistance': 10.883472734078499}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9304456393343878
{'scaleFactor': 20, 'currentTarget': array([ 2., 28.]), 'previousTarget': array([ 2., 28.]), 'currentState': array([ 1.2591448 , 28.29674316,  2.4695442 ]), 'targetState': array([ 2, 28], dtype=int32), 'currentDistance': 0.7980745123995349}
episode index:3551
target Thresh 31.99999999999999
target distance 20.0
model initialize at round 3551
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  6.]), 'previousTarget': array([13.,  6.]), 'currentState': array([ 6.68296497, 25.99587696,  5.27073545]), 'targetState': array([13,  6], dtype=int32), 'currentDistance': 20.96997918638268}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9304357554984993
{'scaleFactor': 20, 'currentTarget': array([13.,  6.]), 'previousTarget': array([13.,  6.]), 'currentState': array([13.36641537,  5.15800982,  4.87941658]), 'targetState': array([13,  6], dtype=int32), 'currentDistance': 0.9182634064130342}
episode index:3552
target Thresh 31.99999999999999
target distance 15.0
model initialize at round 3552
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 18.]), 'previousTarget': array([12., 18.]), 'currentState': array([13.62175245,  3.4497856 ,  1.28054541]), 'targetState': array([12, 18], dtype=int32), 'currentDistance': 14.640314893134718}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9304362141510208
{'scaleFactor': 20, 'currentTarget': array([12., 18.]), 'previousTarget': array([12., 18.]), 'currentState': array([11.81920117, 17.11722019,  1.94693712]), 'targetState': array([12, 18], dtype=int32), 'currentDistance': 0.9011039925469786}
episode index:3553
target Thresh 31.99999999999999
target distance 7.0
model initialize at round 3553
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  6.]), 'previousTarget': array([20.,  6.]), 'currentState': array([22.62219897, 13.96646387,  3.5398916 ]), 'targetState': array([20,  6], dtype=int32), 'currentDistance': 8.386922799124005}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9304447003062962
{'scaleFactor': 20, 'currentTarget': array([20.,  6.]), 'previousTarget': array([20.,  6.]), 'currentState': array([20.33647827,  6.9300018 ,  3.75754005]), 'targetState': array([20,  6], dtype=int32), 'currentDistance': 0.9889999888258613}
episode index:3554
target Thresh 31.99999999999999
target distance 8.0
model initialize at round 3554
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 15.]), 'previousTarget': array([23., 15.]), 'currentState': array([16.83019945, 12.79124989,  0.27155775]), 'targetState': array([23, 15], dtype=int32), 'currentDistance': 6.553244686452715}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9304559110797684
{'scaleFactor': 20, 'currentTarget': array([23., 15.]), 'previousTarget': array([23., 15.]), 'currentState': array([22.51373961, 14.58894654,  0.08719826]), 'targetState': array([23, 15], dtype=int32), 'currentDistance': 0.6367213829678562}
episode index:3555
target Thresh 31.99999999999999
target distance 6.0
model initialize at round 3555
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 20.]), 'previousTarget': array([19., 20.]), 'currentState': array([11.42191771, 15.58484559,  1.77668047]), 'targetState': array([19, 20], dtype=int32), 'currentDistance': 8.770457215050666}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9304616855844986
{'scaleFactor': 20, 'currentTarget': array([19., 20.]), 'previousTarget': array([19., 20.]), 'currentState': array([19.25112976, 20.34222076,  6.08382219]), 'targetState': array([19, 20], dtype=int32), 'currentDistance': 0.4244775688374015}
episode index:3556
target Thresh 31.99999999999999
target distance 10.0
model initialize at round 3556
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 25.]), 'previousTarget': array([17., 25.]), 'currentState': array([19.41518131, 14.43366281,  2.47480154]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 10.83884598805986}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9304647832690126
{'scaleFactor': 20, 'currentTarget': array([17., 25.]), 'previousTarget': array([17., 25.]), 'currentState': array([16.37408414, 25.69154735,  1.46161372]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 0.9327424105059678}
episode index:3557
target Thresh 31.99999999999999
target distance 10.0
model initialize at round 3557
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 12.]), 'previousTarget': array([ 4., 12.]), 'currentState': array([12.18281305,  5.50835066,  2.27550912]), 'targetState': array([ 4, 12], dtype=int32), 'currentDistance': 10.445091691442205}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9304705520342265
{'scaleFactor': 20, 'currentTarget': array([ 4., 12.]), 'previousTarget': array([ 4., 12.]), 'currentState': array([ 4.41838942, 11.68864498,  2.92759719]), 'targetState': array([ 4, 12], dtype=int32), 'currentDistance': 0.5215281925331341}
episode index:3558
target Thresh 31.99999999999999
target distance 11.0
model initialize at round 3558
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 14.]), 'previousTarget': array([19., 14.]), 'currentState': array([18.79948063, 23.01083351,  4.65990992]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 9.013064328745903}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9304763175576505
{'scaleFactor': 20, 'currentTarget': array([19., 14.]), 'previousTarget': array([19., 14.]), 'currentState': array([19.36290051, 13.17660585,  4.68182854]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 0.8998192689057737}
episode index:3559
target Thresh 31.99999999999999
target distance 3.0
model initialize at round 3559
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 26.]), 'previousTarget': array([ 2., 26.]), 'currentState': array([ 6.66967636, 28.75868088,  5.13646734]), 'targetState': array([ 2, 26], dtype=int32), 'currentDistance': 5.423670109688697}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9304875037044039
{'scaleFactor': 20, 'currentTarget': array([ 2., 26.]), 'previousTarget': array([ 2., 26.]), 'currentState': array([ 2.13770641, 25.92784672,  3.08744258]), 'targetState': array([ 2, 26], dtype=int32), 'currentDistance': 0.1554643084569152}
episode index:3560
target Thresh 31.999999999999993
target distance 16.0
model initialize at round 3560
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([10.53649514, 19.4048314 ,  4.02683055]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 14.478985260450715}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9304879467946041
{'scaleFactor': 20, 'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([11.95669934,  5.7993437 ,  4.8980481 ]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 0.8005156452577425}
episode index:3561
target Thresh 31.999999999999993
target distance 7.0
model initialize at round 3561
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 20.]), 'previousTarget': array([24., 20.]), 'currentState': array([18.011948  , 14.69953388,  0.773785  ]), 'targetState': array([24, 20], dtype=int32), 'currentDistance': 7.996981171326617}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9304963993670929
{'scaleFactor': 20, 'currentTarget': array([24., 20.]), 'previousTarget': array([24., 20.]), 'currentState': array([24.08862283, 19.87451495,  0.8645799 ]), 'targetState': array([24, 20], dtype=int32), 'currentDistance': 0.15362455186889992}
episode index:3562
target Thresh 31.999999999999993
target distance 22.0
model initialize at round 3562
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  5.]), 'previousTarget': array([23.,  5.]), 'currentState': array([ 9.51198299, 25.94122081,  4.97719582]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 24.909061238103178}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.930484018921499
{'scaleFactor': 20, 'currentTarget': array([23.,  5.]), 'previousTarget': array([23.,  5.]), 'currentState': array([22.31302033,  5.93204948,  5.65012553]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 1.1578675649167833}
episode index:3563
target Thresh 31.999999999999993
target distance 12.0
model initialize at round 3563
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 16.]), 'previousTarget': array([15., 16.]), 'currentState': array([10.09315845,  5.5953989 ,  1.42182958]), 'targetState': array([15, 16], dtype=int32), 'currentDistance': 11.503600222364113}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.930487104255528
{'scaleFactor': 20, 'currentTarget': array([15., 16.]), 'previousTarget': array([15., 16.]), 'currentState': array([15.19777146, 16.41487042,  1.11313575]), 'targetState': array([15, 16], dtype=int32), 'currentDistance': 0.45959875335475087}
episode index:3564
target Thresh 31.999999999999993
target distance 15.0
model initialize at round 3564
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 24.]), 'previousTarget': array([15., 24.]), 'currentState': array([ 4.49133765, 10.81672537,  0.70024872]), 'targetState': array([15, 24], dtype=int32), 'currentDistance': 16.85914334347779}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9304849324715652
{'scaleFactor': 20, 'currentTarget': array([15., 24.]), 'previousTarget': array([15., 24.]), 'currentState': array([14.49633208, 23.22912331,  1.25523014]), 'targetState': array([15, 24], dtype=int32), 'currentDistance': 0.9208323686888145}
episode index:3565
target Thresh 31.999999999999993
target distance 5.0
model initialize at round 3565
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 17.]), 'previousTarget': array([11., 17.]), 'currentState': array([12.25294233, 13.8472665 ,  1.80460978]), 'targetState': array([11, 17], dtype=int32), 'currentDistance': 3.3925791055581214}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9304988458387914
{'scaleFactor': 20, 'currentTarget': array([11., 17.]), 'previousTarget': array([11., 17.]), 'currentState': array([10.78412442, 17.55979781,  1.90729544]), 'targetState': array([11, 17], dtype=int32), 'currentDistance': 0.5999798825463818}
episode index:3566
target Thresh 31.999999999999993
target distance 6.0
model initialize at round 3566
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 13.]), 'previousTarget': array([ 2., 13.]), 'currentState': array([6.57105549, 7.11089614, 2.68818331]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 7.4549374581667625}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9305072835074656
{'scaleFactor': 20, 'currentTarget': array([ 2., 13.]), 'previousTarget': array([ 2., 13.]), 'currentState': array([ 1.73156461, 13.3790615 ,  2.18328863]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 0.464483774294386}
episode index:3567
target Thresh 31.999999999999993
target distance 20.0
model initialize at round 3567
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 23.]), 'previousTarget': array([ 5., 23.]), 'currentState': array([12.01409272,  4.36395562,  1.18668032]), 'targetState': array([ 5, 23], dtype=int32), 'currentDistance': 19.912298886869376}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9304999614198819
{'scaleFactor': 20, 'currentTarget': array([ 5., 23.]), 'previousTarget': array([ 5., 23.]), 'currentState': array([ 5.19918975, 22.87408558,  2.0893056 ]), 'targetState': array([ 5, 23], dtype=int32), 'currentDistance': 0.23565016037560615}
episode index:3568
target Thresh 31.999999999999993
target distance 6.0
model initialize at round 3568
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 13.]), 'previousTarget': array([ 8., 13.]), 'currentState': array([12.13272135, 10.99774541,  2.49894941]), 'targetState': array([ 8, 13], dtype=int32), 'currentDistance': 4.5922118044297715}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.930513858880958
{'scaleFactor': 20, 'currentTarget': array([ 8., 13.]), 'previousTarget': array([ 8., 13.]), 'currentState': array([ 8.60687107, 12.86320574,  2.56227833]), 'targetState': array([ 8, 13], dtype=int32), 'currentDistance': 0.6220973930481853}
episode index:3569
target Thresh 31.999999999999993
target distance 1.0
model initialize at round 3569
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 10.]), 'previousTarget': array([25., 10.]), 'currentState': array([24.80879719,  7.81112421,  2.91601396]), 'targetState': array([25, 10], dtype=int32), 'currentDistance': 2.1972109028370106}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.930530521665585
{'scaleFactor': 20, 'currentTarget': array([25., 10.]), 'previousTarget': array([25., 10.]), 'currentState': array([24.22346962,  9.38902779,  0.91601396]), 'targetState': array([25, 10], dtype=int32), 'currentDistance': 0.9880721005816147}
episode index:3570
target Thresh 31.999999999999993
target distance 13.0
model initialize at round 3570
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 19.]), 'previousTarget': array([12., 19.]), 'currentState': array([23.57155774, 25.88991055,  3.59443736]), 'targetState': array([12, 19], dtype=int32), 'currentDistance': 13.467435387183356}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9305309514685091
{'scaleFactor': 20, 'currentTarget': array([12., 19.]), 'previousTarget': array([12., 19.]), 'currentState': array([11.48997867, 18.85830535,  3.62643318]), 'targetState': array([12, 19], dtype=int32), 'currentDistance': 0.5293383880016372}
episode index:3571
target Thresh 31.999999999999993
target distance 8.0
model initialize at round 3571
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 10.]), 'previousTarget': array([17., 10.]), 'currentState': array([24.07469498, 16.30926902,  3.73755783]), 'targetState': array([17, 10], dtype=int32), 'currentDistance': 9.479355706621662}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.930536679099649
{'scaleFactor': 20, 'currentTarget': array([17., 10.]), 'previousTarget': array([17., 10.]), 'currentState': array([16.6344242 ,  9.67274011,  3.83305498]), 'targetState': array([17, 10], dtype=int32), 'currentDistance': 0.49065741973715543}
episode index:3572
target Thresh 31.999999999999993
target distance 17.0
model initialize at round 3572
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 27.]), 'previousTarget': array([23., 27.]), 'currentState': array([22.57882807, 10.90147569,  1.56632042]), 'targetState': array([23, 27], dtype=int32), 'currentDistance': 16.104032747104643}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9305344983034911
{'scaleFactor': 20, 'currentTarget': array([23., 27.]), 'previousTarget': array([23., 27.]), 'currentState': array([23.0822908 , 26.86604701,  1.76844825]), 'targetState': array([23, 27], dtype=int32), 'currentDistance': 0.15721062368502528}
episode index:3573
target Thresh 31.999999999999993
target distance 10.0
model initialize at round 3573
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([15.21358007,  8.71751172,  2.37421644]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 8.319739680365142}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9305429094707257
{'scaleFactor': 20, 'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([15.91556068, 16.39606031,  1.78331186]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 0.6098140324746625}
episode index:3574
target Thresh 31.999999999999993
target distance 20.0
model initialize at round 3574
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 13.]), 'previousTarget': array([ 3., 13.]), 'currentState': array([21.13997872, 13.44401677,  3.00644606]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 18.14541205847435}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9305381470477921
{'scaleFactor': 20, 'currentTarget': array([ 3., 13.]), 'previousTarget': array([ 3., 13.]), 'currentState': array([ 3.16635531, 13.15562774,  3.33897598]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 0.22780272423486617}
episode index:3575
target Thresh 31.999999999999993
target distance 4.0
model initialize at round 3575
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 16.]), 'previousTarget': array([18., 16.]), 'currentState': array([20.39750934, 17.46473644,  4.46292627]), 'targetState': array([18, 16], dtype=int32), 'currentDistance': 2.809538019985556}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.930554775082734
{'scaleFactor': 20, 'currentTarget': array([18., 16.]), 'previousTarget': array([18., 16.]), 'currentState': array([18.90055445, 16.40240859,  3.04039228]), 'targetState': array([18, 16], dtype=int32), 'currentDistance': 0.9863726457143321}
episode index:3576
target Thresh 31.999999999999993
target distance 9.0
model initialize at round 3576
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  4.]), 'previousTarget': array([12.,  4.]), 'currentState': array([16.07074588, 11.52156221,  5.05020428]), 'targetState': array([12,  4], dtype=int32), 'currentDistance': 8.552477425342987}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9305631735269378
{'scaleFactor': 20, 'currentTarget': array([12.,  4.]), 'previousTarget': array([12.,  4.]), 'currentState': array([12.38173657,  4.73249204,  4.57689952]), 'targetState': array([12,  4], dtype=int32), 'currentDistance': 0.8259947957075874}
episode index:3577
target Thresh 31.999999999999993
target distance 8.0
model initialize at round 3577
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 20.]), 'previousTarget': array([25., 20.]), 'currentState': array([16.01584372, 17.36521958,  1.18539715]), 'targetState': array([25, 20], dtype=int32), 'currentDistance': 9.362538754807902}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9305688825477241
{'scaleFactor': 20, 'currentTarget': array([25., 20.]), 'previousTarget': array([25., 20.]), 'currentState': array([25.36965089, 20.09906769,  0.26941394]), 'targetState': array([25, 20], dtype=int32), 'currentDistance': 0.38269594815293817}
episode index:3578
target Thresh 31.999999999999993
target distance 19.0
model initialize at round 3578
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 27.]), 'previousTarget': array([ 7., 27.]), 'currentState': array([16.67400948,  9.42297423,  1.90901351]), 'targetState': array([ 7, 27], dtype=int32), 'currentDistance': 20.063357005833673}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9305615657532176
{'scaleFactor': 20, 'currentTarget': array([ 7., 27.]), 'previousTarget': array([ 7., 27.]), 'currentState': array([ 7.1228486 , 26.94593326,  2.25490301]), 'targetState': array([ 7, 27], dtype=int32), 'currentDistance': 0.1342199386561331}
episode index:3579
target Thresh 31.999999999999993
target distance 22.0
model initialize at round 3579
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  2.]), 'previousTarget': array([17.,  2.]), 'currentState': array([19.32734868, 24.18607972,  4.04079986]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 22.3078166858026}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9305517268393922
{'scaleFactor': 20, 'currentTarget': array([17.,  2.]), 'previousTarget': array([17.,  2.]), 'currentState': array([17.05267909,  2.5197607 ,  5.0848483 ]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 0.5224234589221477}
episode index:3580
target Thresh 31.999999999999993
target distance 10.0
model initialize at round 3580
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([14.77606439, 13.15514922,  3.3950994 ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 9.036812177669631}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9305574342739248
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.14454447, 10.56801435,  3.38863834]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.9583401084466867}
episode index:3581
target Thresh 31.999999999999993
target distance 13.0
model initialize at round 3581
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 26.]), 'previousTarget': array([26., 26.]), 'currentState': array([17.62876119, 12.576296  ,  0.75550192]), 'targetState': array([26, 26], dtype=int32), 'currentDistance': 15.820033767922801}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9305552531628566
{'scaleFactor': 20, 'currentTarget': array([26., 26.]), 'previousTarget': array([26., 26.]), 'currentState': array([26.06666211, 26.07586641,  1.28543593]), 'targetState': array([26, 26], dtype=int32), 'currentDistance': 0.100992816603514}
episode index:3582
target Thresh 31.999999999999993
target distance 12.0
model initialize at round 3582
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 23.]), 'previousTarget': array([19., 23.]), 'currentState': array([11.28493732,  9.91311262,  0.30790966]), 'targetState': array([19, 23], dtype=int32), 'currentDistance': 15.191735035765504}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9305530732692661
{'scaleFactor': 20, 'currentTarget': array([19., 23.]), 'previousTarget': array([19., 23.]), 'currentState': array([19.33750686, 23.47260007,  0.9923874 ]), 'targetState': array([19, 23], dtype=int32), 'currentDistance': 0.5807423725412507}
episode index:3583
target Thresh 31.999999999999993
target distance 21.0
model initialize at round 3583
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([23.20511493, 16.3698572 ,  4.03017771]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 21.36891815830838}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.930543247705926
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.47048777, 6.8390637 , 3.56058279]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.5534290347004122}
episode index:3584
target Thresh 31.999999999999993
target distance 9.0
model initialize at round 3584
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 17.]), 'previousTarget': array([12., 17.]), 'currentState': array([13.66273699, 27.02181981,  3.49912035]), 'targetState': array([12, 17], dtype=int32), 'currentDistance': 10.15881718514203}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9305489511375004
{'scaleFactor': 20, 'currentTarget': array([12., 17.]), 'previousTarget': array([12., 17.]), 'currentState': array([12.18863636, 17.75495746,  4.67030498]), 'targetState': array([12, 17], dtype=int32), 'currentDistance': 0.7781673599001526}
episode index:3585
target Thresh 31.999999999999993
target distance 20.0
model initialize at round 3585
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 26.]), 'previousTarget': array([12., 26.]), 'currentState': array([3.15212045, 7.67608098, 0.4702847 ]), 'targetState': array([12, 26], dtype=int32), 'currentDistance': 20.34824269439585}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9305416541837557
{'scaleFactor': 20, 'currentTarget': array([12., 26.]), 'previousTarget': array([12., 26.]), 'currentState': array([11.63629919, 25.48096355,  1.28207169]), 'targetState': array([12, 26], dtype=int32), 'currentDistance': 0.6337800226654189}
episode index:3586
target Thresh 31.999999999999993
target distance 7.0
model initialize at round 3586
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 11.]), 'previousTarget': array([19., 11.]), 'currentState': array([24.50165641, 17.24012099,  4.80472756]), 'targetState': array([19, 11], dtype=int32), 'currentDistance': 8.319094491717573}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9305500328723021
{'scaleFactor': 20, 'currentTarget': array([19., 11.]), 'previousTarget': array([19., 11.]), 'currentState': array([19.40446219, 11.39136275,  4.29548669]), 'targetState': array([19, 11], dtype=int32), 'currentDistance': 0.5628094442334001}
episode index:3587
target Thresh 31.999999999999993
target distance 11.0
model initialize at round 3587
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 15.]), 'previousTarget': array([17., 15.]), 'currentState': array([ 7.60120692, 10.1880608 ,  0.50084811]), 'targetState': array([17, 15], dtype=int32), 'currentDistance': 10.558980550075319}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.930555729644049
{'scaleFactor': 20, 'currentTarget': array([17., 15.]), 'previousTarget': array([17., 15.]), 'currentState': array([16.43564688, 14.72624161,  0.88039846]), 'targetState': array([17, 15], dtype=int32), 'currentDistance': 0.6272464377817158}
episode index:3588
target Thresh 31.999999999999993
target distance 8.0
model initialize at round 3588
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([8.78559978, 0.51163813, 6.20805073]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 8.496644207736606}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9305640997416683
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.49385808,  4.57085012,  0.65678925]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.6635881743498161}
episode index:3589
target Thresh 31.999999999999993
target distance 25.0
model initialize at round 3589
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 28.]), 'previousTarget': array([ 7., 28.]), 'currentState': array([22.11410144,  4.72403513,  2.48195702]), 'targetState': array([ 7, 28], dtype=int32), 'currentDistance': 27.752596331087783}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9305468801631244
{'scaleFactor': 20, 'currentTarget': array([ 7., 28.]), 'previousTarget': array([ 7., 28.]), 'currentState': array([ 6.95616915, 28.06142541,  2.38449782]), 'targetState': array([ 7, 28], dtype=int32), 'currentDistance': 0.07546008197390372}
episode index:3590
target Thresh 31.999999999999993
target distance 11.0
model initialize at round 3590
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([ 8.75771584, 11.13539339,  1.3911202 ]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 10.38377917989221}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9305525730536108
{'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([11.79396847, 20.60682192,  1.50900138]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 0.44388962065803067}
episode index:3591
target Thresh 31.999999999999993
target distance 17.0
model initialize at round 3591
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 29.]), 'previousTarget': array([11., 29.]), 'currentState': array([15.49326644, 13.60605419,  2.88453084]), 'targetState': array([11, 29], dtype=int32), 'currentDistance': 16.03630290546198}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9305503993680246
{'scaleFactor': 20, 'currentTarget': array([11., 29.]), 'previousTarget': array([11., 29.]), 'currentState': array([11.05561639, 28.59308484,  2.02364925]), 'targetState': array([11, 29], dtype=int32), 'currentDistance': 0.4106983495274432}
episode index:3592
target Thresh 31.999999999999993
target distance 15.0
model initialize at round 3592
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 19.]), 'previousTarget': array([25., 19.]), 'currentState': array([9.21687011, 6.48966294, 1.04479933]), 'targetState': array([25, 19], dtype=int32), 'currentDistance': 20.139903737146298}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9305431162273736
{'scaleFactor': 20, 'currentTarget': array([25., 19.]), 'previousTarget': array([25., 19.]), 'currentState': array([24.91567116, 18.78666507,  0.75527638]), 'targetState': array([25, 19], dtype=int32), 'currentDistance': 0.22939734848818055}
episode index:3593
target Thresh 31.999999999999993
target distance 8.0
model initialize at round 3593
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 17.]), 'previousTarget': array([17., 17.]), 'currentState': array([14.47888269, 23.29050932,  5.82941878]), 'targetState': array([17, 17], dtype=int32), 'currentDistance': 6.77691227380622}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9305541779646503
{'scaleFactor': 20, 'currentTarget': array([17., 17.]), 'previousTarget': array([17., 17.]), 'currentState': array([16.67102675, 17.90065431,  5.16119596]), 'targetState': array([17, 17], dtype=int32), 'currentDistance': 0.9588543113054119}
episode index:3594
target Thresh 31.999999999999993
target distance 19.0
model initialize at round 3594
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([19.6441747 , 22.35927378,  5.48831696]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 19.42896785424593}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9305468978247461
{'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([18.07745807,  2.67206238,  4.59577119]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 0.3369611776740635}
episode index:3595
target Thresh 31.999999999999993
target distance 6.0
model initialize at round 3595
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  4.]), 'previousTarget': array([24.,  4.]), 'currentState': array([28.66464697,  9.75233378,  5.1254887 ]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 7.4059621448242545}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.930555254085084
{'scaleFactor': 20, 'currentTarget': array([24.,  4.]), 'previousTarget': array([24.,  4.]), 'currentState': array([24.01557034,  4.00679363,  4.18613574]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 0.016987900874808247}
episode index:3596
target Thresh 31.999999999999993
target distance 21.0
model initialize at round 3596
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 13.]), 'previousTarget': array([24., 13.]), 'currentState': array([ 4.37092255, 16.97619642,  5.89196784]), 'targetState': array([24, 13], dtype=int32), 'currentDistance': 20.027751237334737}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9305479776939035
{'scaleFactor': 20, 'currentTarget': array([24., 13.]), 'previousTarget': array([24., 13.]), 'currentState': array([23.91954189, 13.02265303,  0.04796744]), 'targetState': array([24, 13], dtype=int32), 'currentDistance': 0.08358628485728449}
episode index:3597
target Thresh 31.999999999999993
target distance 8.0
model initialize at round 3597
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 15.]), 'previousTarget': array([ 9., 15.]), 'currentState': array([16.24147437, 15.49765951,  3.23483229]), 'targetState': array([ 9, 15], dtype=int32), 'currentDistance': 7.258554679803521}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9305563290091636
{'scaleFactor': 20, 'currentTarget': array([ 9., 15.]), 'previousTarget': array([ 9., 15.]), 'currentState': array([ 8.37712194, 14.73880909,  3.09507372]), 'targetState': array([ 9, 15], dtype=int32), 'currentDistance': 0.6754241400848113}
episode index:3598
target Thresh 31.999999999999993
target distance 11.0
model initialize at round 3598
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 20.]), 'previousTarget': array([ 6., 20.]), 'currentState': array([15.84588939, 20.77508499,  2.94674397]), 'targetState': array([ 6, 20], dtype=int32), 'currentDistance': 9.876350265824174}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9305620066198586
{'scaleFactor': 20, 'currentTarget': array([ 6., 20.]), 'previousTarget': array([ 6., 20.]), 'currentState': array([ 5.95097923, 20.18336174,  3.28979834]), 'targetState': array([ 6, 20], dtype=int32), 'currentDistance': 0.18980137964209315}
episode index:3599
target Thresh 31.999999999999993
target distance 8.0
model initialize at round 3599
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([10.33328189,  8.54527507,  0.96812248]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 8.589247234932708}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9305703493985752
{'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([15.64368976, 14.45523057,  1.21315387]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 0.6509460157379144}
episode index:3600
target Thresh 31.999999999999993
target distance 13.0
model initialize at round 3600
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 19.]), 'previousTarget': array([20., 19.]), 'currentState': array([13.10925077,  7.42791943,  1.11853886]), 'targetState': array([20, 19], dtype=int32), 'currentDistance': 13.468313694470181}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9305707645606159
{'scaleFactor': 20, 'currentTarget': array([20., 19.]), 'previousTarget': array([20., 19.]), 'currentState': array([20.28865959, 19.43296773,  1.0807074 ]), 'targetState': array([20, 19], dtype=int32), 'currentDistance': 0.5203704532394587}
episode index:3601
target Thresh 31.999999999999993
target distance 12.0
model initialize at round 3601
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([19.01425641, 20.12901362,  2.90102553]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 10.736380533287532}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9305764334349467
{'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([ 9.67993113, 23.64856073,  2.89353359]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 0.7653861177271934}
episode index:3602
target Thresh 31.999999999999993
target distance 23.0
model initialize at round 3602
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 21.]), 'previousTarget': array([ 2., 21.]), 'currentState': array([23.2484692 , 12.76996881,  3.24595833]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 22.786637678937296}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9305666532020362
{'scaleFactor': 20, 'currentTarget': array([ 2., 21.]), 'previousTarget': array([ 2., 21.]), 'currentState': array([ 2.82193114, 20.64717499,  3.04912041]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 0.894458657768234}
episode index:3603
target Thresh 31.999999999999993
target distance 4.0
model initialize at round 3603
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 28.]), 'previousTarget': array([12., 28.]), 'currentState': array([16.54590993, 24.66526155,  1.41637092]), 'targetState': array([12, 28], dtype=int32), 'currentDistance': 5.637887686509423}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9305776777155762
{'scaleFactor': 20, 'currentTarget': array([12., 28.]), 'previousTarget': array([12., 28.]), 'currentState': array([12.13922289, 27.88464591,  2.40987661]), 'targetState': array([12, 28], dtype=int32), 'currentDistance': 0.1808025977836245}
episode index:3604
target Thresh 31.999999999999993
target distance 3.0
model initialize at round 3604
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 27.]), 'previousTarget': array([16., 27.]), 'currentState': array([15.15837298, 25.60937394,  0.71789545]), 'targetState': array([16, 27], dtype=int32), 'currentDistance': 1.6254774293364231}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9305941610227285
{'scaleFactor': 20, 'currentTarget': array([16., 27.]), 'previousTarget': array([16., 27.]), 'currentState': array([16.22709891, 27.26825212,  1.28472608]), 'targetState': array([16, 27], dtype=int32), 'currentDistance': 0.3514727735937862}
episode index:3605
target Thresh 31.999999999999993
target distance 17.0
model initialize at round 3605
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([12.46952756, 24.69327722,  5.2352429 ]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 20.72082610407451}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.930586892002758
{'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([25.1232703 ,  9.52591492,  5.97328028]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 1.0223705177624018}
episode index:3606
target Thresh 31.999999999999993
target distance 20.0
model initialize at round 3606
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 10.]), 'previousTarget': array([25., 10.]), 'currentState': array([ 6.50650531, 22.70310322,  5.7655277 ]), 'targetState': array([25, 10], dtype=int32), 'currentDistance': 22.436090956750313}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9305771197161641
{'scaleFactor': 20, 'currentTarget': array([25., 10.]), 'previousTarget': array([25., 10.]), 'currentState': array([24.34941627, 10.27697591,  6.22601856]), 'targetState': array([25, 10], dtype=int32), 'currentDistance': 0.7070889979112117}
episode index:3607
target Thresh 31.999999999999993
target distance 18.0
model initialize at round 3607
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 17.]), 'previousTarget': array([27., 17.]), 'currentState': array([10.79715555, 22.1150043 ,  5.55420876]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 16.99103990862711}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9305723913702016
{'scaleFactor': 20, 'currentTarget': array([27., 17.]), 'previousTarget': array([27., 17.]), 'currentState': array([27.94104054, 16.94617183,  6.0314058 ]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 0.942578788104501}
episode index:3608
target Thresh 31.999999999999993
target distance 6.0
model initialize at round 3608
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 19.]), 'previousTarget': array([13., 19.]), 'currentState': array([15.32769702, 24.81081535,  4.26424193]), 'targetState': array([13, 19], dtype=int32), 'currentDistance': 6.259692363645684}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9305833990201406
{'scaleFactor': 20, 'currentTarget': array([13., 19.]), 'previousTarget': array([13., 19.]), 'currentState': array([13.17616185, 19.24853643,  4.67806999]), 'targetState': array([13, 19], dtype=int32), 'currentDistance': 0.3046364289125617}
episode index:3609
target Thresh 31.999999999999993
target distance 9.0
model initialize at round 3609
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 13.]), 'previousTarget': array([12., 13.]), 'currentState': array([17.28561198, 20.91391072,  4.57171593]), 'targetState': array([12, 13], dtype=int32), 'currentDistance': 9.516705147803703}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9305890518320188
{'scaleFactor': 20, 'currentTarget': array([12., 13.]), 'previousTarget': array([12., 13.]), 'currentState': array([11.81476157, 12.69589541,  4.27753783]), 'targetState': array([12, 13], dtype=int32), 'currentDistance': 0.3560798744416974}
episode index:3610
target Thresh 31.999999999999993
target distance 9.0
model initialize at round 3610
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([13.87818769, 23.74543823,  2.97279501]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 7.977453639885306}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9305973617068921
{'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([ 6.0121832 , 25.04981116,  3.24285597]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 0.051279448143472434}
episode index:3611
target Thresh 31.999999999999996
target distance 12.0
model initialize at round 3611
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 16.]), 'previousTarget': array([17., 16.]), 'currentState': array([6.96647830e+00, 1.53185109e+01, 5.04678488e-03]), 'targetState': array([17, 16], dtype=int32), 'currentDistance': 10.056638857535749}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.930603007523114
{'scaleFactor': 20, 'currentTarget': array([17., 16.]), 'previousTarget': array([17., 16.]), 'currentState': array([16.91683   , 15.97716927,  0.35090005]), 'targetState': array([17, 16], dtype=int32), 'currentDistance': 0.08624668857290993}
episode index:3612
target Thresh 31.999999999999996
target distance 14.0
model initialize at round 3612
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 14.]), 'previousTarget': array([19., 14.]), 'currentState': array([ 3.60965429, 20.05167153,  4.75019097]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 16.537396093572404}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.93060083251257
{'scaleFactor': 20, 'currentTarget': array([19., 14.]), 'previousTarget': array([19., 14.]), 'currentState': array([18.00618819, 14.47147402,  5.86535862]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 1.0999771174256874}
episode index:3613
target Thresh 31.999999999999996
target distance 17.0
model initialize at round 3613
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 10.]), 'previousTarget': array([18., 10.]), 'currentState': array([24.20314963, 25.26501164,  4.81257486]), 'targetState': array([18, 10], dtype=int32), 'currentDistance': 16.477246298912473}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9305986587056844
{'scaleFactor': 20, 'currentTarget': array([18., 10.]), 'previousTarget': array([18., 10.]), 'currentState': array([18.27549584, 10.53394884,  4.59003384]), 'targetState': array([18, 10], dtype=int32), 'currentDistance': 0.6008321879534837}
episode index:3614
target Thresh 31.999999999999996
target distance 11.0
model initialize at round 3614
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([4.83187073, 4.21155359, 1.32794857]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 14.850615962630574}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9305990644288383
{'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([15.20016296, 13.41487869,  0.72860344]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 0.9910127276816024}
episode index:3615
target Thresh 31.999999999999996
target distance 12.0
model initialize at round 3615
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 20.]), 'previousTarget': array([14., 20.]), 'currentState': array([4.7802057 , 9.5390105 , 0.16504252]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 13.94406354988329}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9305994699275878
{'scaleFactor': 20, 'currentTarget': array([14., 20.]), 'previousTarget': array([14., 20.]), 'currentState': array([13.99529813, 19.76222949,  1.04477854]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 0.23781699724357697}
episode index:3616
target Thresh 31.999999999999996
target distance 25.0
model initialize at round 3616
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 27.]), 'previousTarget': array([12., 27.]), 'currentState': array([23.72066261,  3.65962607,  2.74754685]), 'targetState': array([12, 27], dtype=int32), 'currentDistance': 26.1179437737665}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.930584795211821
{'scaleFactor': 20, 'currentTarget': array([12., 27.]), 'previousTarget': array([12., 27.]), 'currentState': array([12.2550276 , 26.68405592,  2.4665133 ]), 'targetState': array([12, 27], dtype=int32), 'currentDistance': 0.40602923324745116}
episode index:3617
target Thresh 31.999999999999996
target distance 17.0
model initialize at round 3617
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  6.]), 'previousTarget': array([22.,  6.]), 'currentState': array([ 8.16090473, 24.21847786,  6.08277542]), 'targetState': array([22,  6], dtype=int32), 'currentDistance': 22.87866896117571}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9305750532159799
{'scaleFactor': 20, 'currentTarget': array([22.,  6.]), 'previousTarget': array([22.,  6.]), 'currentState': array([21.3736853 ,  6.85142482,  5.42751815]), 'targetState': array([22,  6], dtype=int32), 'currentDistance': 1.056974136749502}
episode index:3618
target Thresh 31.999999999999996
target distance 6.0
model initialize at round 3618
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 17.]), 'previousTarget': array([ 4., 17.]), 'currentState': array([10.71890627, 15.52169703,  2.13944109]), 'targetState': array([ 4, 17], dtype=int32), 'currentDistance': 6.879613445634658}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9305833485895041
{'scaleFactor': 20, 'currentTarget': array([ 4., 17.]), 'previousTarget': array([ 4., 17.]), 'currentState': array([ 3.10840481, 17.18113311,  2.91040131]), 'targetState': array([ 4, 17], dtype=int32), 'currentDistance': 0.909808318780373}
episode index:3619
target Thresh 31.999999999999996
target distance 7.0
model initialize at round 3619
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 14.]), 'previousTarget': array([13., 14.]), 'currentState': array([17.39109638,  8.14767215,  2.25211555]), 'targetState': array([13, 14], dtype=int32), 'currentDistance': 7.316520260677204}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9305943197639268
{'scaleFactor': 20, 'currentTarget': array([13., 14.]), 'previousTarget': array([13., 14.]), 'currentState': array([13.92424656, 13.0183643 ,  2.40435628]), 'targetState': array([13, 14], dtype=int32), 'currentDistance': 1.3482730954305668}
episode index:3620
target Thresh 31.999999999999996
target distance 12.0
model initialize at round 3620
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 2.]), 'previousTarget': array([9., 2.]), 'currentState': array([17.31364688, 12.2232982 ,  3.7996397 ]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 13.176970463998575}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9305947260130689
{'scaleFactor': 20, 'currentTarget': array([9., 2.]), 'previousTarget': array([9., 2.]), 'currentState': array([8.77461555, 1.30653723, 4.08235582]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 0.7291699101701786}
episode index:3621
target Thresh 31.999999999999996
target distance 9.0
model initialize at round 3621
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([12.44798665, 12.65087836,  3.75449455]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 10.626572279513091}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9306003569694153
{'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([8.50290676, 3.65473342, 4.34291718]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 0.8255852871844634}
episode index:3622
target Thresh 31.999999999999996
target distance 15.0
model initialize at round 3622
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 29.]), 'previousTarget': array([24., 29.]), 'currentState': array([10.19312508, 20.52711398,  0.4697029 ]), 'targetState': array([24, 29], dtype=int32), 'currentDistance': 16.199370124594182}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9305981886938036
{'scaleFactor': 20, 'currentTarget': array([24., 29.]), 'previousTarget': array([24., 29.]), 'currentState': array([23.74447873, 28.91437212,  0.93898936]), 'targetState': array([24, 29], dtype=int32), 'currentDistance': 0.26948701816787196}
episode index:3623
target Thresh 31.999999999999996
target distance 9.0
model initialize at round 3623
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 28.]), 'previousTarget': array([19., 28.]), 'currentState': array([13.2750766 , 20.09843944,  1.72111433]), 'targetState': array([19, 28], dtype=int32), 'currentDistance': 9.757530796508915}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9306038155870724
{'scaleFactor': 20, 'currentTarget': array([19., 28.]), 'previousTarget': array([19., 28.]), 'currentState': array([18.96432984, 28.04310191,  1.23244545]), 'targetState': array([19, 28], dtype=int32), 'currentDistance': 0.05594760365107987}
episode index:3624
target Thresh 31.999999999999996
target distance 23.0
model initialize at round 3624
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([22.29540291, 24.02496619,  4.95665061]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 21.03676925039923}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9305940871563612
{'scaleFactor': 20, 'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([23.33107255,  2.09821487,  4.7548204 ]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 0.9606380428028162}
episode index:3625
target Thresh 31.999999999999996
target distance 7.0
model initialize at round 3625
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([20.33588062, 21.74881325,  4.30140448]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 6.757166274221565}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9306050372150605
{'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([20.23356184, 15.84128417,  4.80371979]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 0.873103770341722}
episode index:3626
target Thresh 31.999999999999996
target distance 10.0
model initialize at round 3626
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 24.]), 'previousTarget': array([ 5., 24.]), 'currentState': array([ 7.41353509, 13.43829086,  2.47188282]), 'targetState': array([ 5, 24], dtype=int32), 'currentDistance': 10.833967490011881}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9306080355917314
{'scaleFactor': 20, 'currentTarget': array([ 5., 24.]), 'previousTarget': array([ 5., 24.]), 'currentState': array([ 4.61193184, 24.93806735,  1.74746269]), 'targetState': array([ 5, 24], dtype=int32), 'currentDistance': 1.0151685781388258}
episode index:3627
target Thresh 31.999999999999996
target distance 23.0
model initialize at round 3627
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 28.]), 'previousTarget': array([26., 28.]), 'currentState': array([23.70112109,  6.91550984,  1.2903716 ]), 'targetState': array([26, 28], dtype=int32), 'currentDistance': 21.20944528722499}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9305983140423012
{'scaleFactor': 20, 'currentTarget': array([26., 28.]), 'previousTarget': array([26., 28.]), 'currentState': array([25.76659475, 28.72699139,  1.42945066]), 'targetState': array([26, 28], dtype=int32), 'currentDistance': 0.763540756723382}
episode index:3628
target Thresh 31.999999999999996
target distance 11.0
model initialize at round 3628
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([18.85155353, 11.76977287,  4.97135806]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 10.177417133104754}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9306039331483519
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.21174038,  2.28331121,  4.7986592 ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.35369369456411515}
episode index:3629
target Thresh 31.999999999999996
target distance 11.0
model initialize at round 3629
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 15.]), 'previousTarget': array([14., 15.]), 'currentState': array([ 3.76212755, 17.49948353,  6.19234467]), 'targetState': array([14, 15], dtype=int32), 'currentDistance': 10.538569650762303}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9306095491584763
{'scaleFactor': 20, 'currentTarget': array([14., 15.]), 'previousTarget': array([14., 15.]), 'currentState': array([13.43086429, 15.20639464,  0.17274718]), 'targetState': array([14, 15], dtype=int32), 'currentDistance': 0.6054041633185093}
episode index:3630
target Thresh 31.999999999999996
target distance 11.0
model initialize at round 3630
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 17.]), 'previousTarget': array([26., 17.]), 'currentState': array([15.18776904, 11.32753748,  5.8341918 ]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 12.209879989531935}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9306125429894436
{'scaleFactor': 20, 'currentTarget': array([26., 17.]), 'previousTarget': array([26., 17.]), 'currentState': array([25.45014563, 16.78004537,  0.79723481]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 0.5922160600651126}
episode index:3631
target Thresh 31.999999999999996
target distance 8.0
model initialize at round 3631
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 14.]), 'previousTarget': array([13., 14.]), 'currentState': array([7.32075028, 7.49910249, 0.92833766]), 'targetState': array([13, 14], dtype=int32), 'currentDistance': 8.632238751218926}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9306207983493033
{'scaleFactor': 20, 'currentTarget': array([13., 14.]), 'previousTarget': array([13., 14.]), 'currentState': array([12.49659247, 13.55414502,  1.10722797]), 'targetState': array([13, 14], dtype=int32), 'currentDistance': 0.6724624870951613}
episode index:3632
target Thresh 31.999999999999996
target distance 9.0
model initialize at round 3632
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 19.]), 'previousTarget': array([ 7., 19.]), 'currentState': array([ 5.55908014, 11.81283327,  1.16159248]), 'targetState': array([ 7, 19], dtype=int32), 'currentDistance': 7.330185239221535}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9306290491645113
{'scaleFactor': 20, 'currentTarget': array([ 7., 19.]), 'previousTarget': array([ 7., 19.]), 'currentState': array([ 7.01894824, 19.65464447,  1.53401807]), 'targetState': array([ 7, 19], dtype=int32), 'currentDistance': 0.6549186296168648}
episode index:3633
target Thresh 31.999999999999996
target distance 16.0
model initialize at round 3633
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 26.]), 'previousTarget': array([22., 26.]), 'currentState': array([ 7.63045028, 18.62857962,  1.26470714]), 'targetState': array([22, 26], dtype=int32), 'currentDistance': 16.149978258176986}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9306268795567136
{'scaleFactor': 20, 'currentTarget': array([22., 26.]), 'previousTarget': array([22., 26.]), 'currentState': array([21.54051193, 26.00029056,  0.6769896 ]), 'targetState': array([22, 26], dtype=int32), 'currentDistance': 0.45948815864455717}
episode index:3634
target Thresh 31.999999999999996
target distance 13.0
model initialize at round 3634
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([23.24762711, 11.1906552 ,  3.10976374]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 11.310471938901067}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.930629865325584
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([11.36370851,  9.76313007,  3.20173127]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.6789508239015057}
episode index:3635
target Thresh 31.999999999999996
target distance 16.0
model initialize at round 3635
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 24.]), 'previousTarget': array([ 3., 24.]), 'currentState': array([2.98363148, 9.36559774, 1.95657986]), 'targetState': array([ 3, 24], dtype=int32), 'currentDistance': 14.63441141034787}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9306302601227737
{'scaleFactor': 20, 'currentTarget': array([ 3., 24.]), 'previousTarget': array([ 3., 24.]), 'currentState': array([ 2.83440825, 23.25115656,  2.01687583]), 'targetState': array([ 3, 24], dtype=int32), 'currentDistance': 0.7669335818769587}
episode index:3636
target Thresh 31.999999999999996
target distance 11.0
model initialize at round 3636
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 17.]), 'previousTarget': array([13., 17.]), 'currentState': array([22.02428212, 29.37126318,  3.19922984]), 'targetState': array([13, 17], dtype=int32), 'currentDistance': 15.312929840689332}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.930628091971634
{'scaleFactor': 20, 'currentTarget': array([13., 17.]), 'previousTarget': array([13., 17.]), 'currentState': array([12.90272515, 16.62722411,  4.09097276]), 'targetState': array([13, 17], dtype=int32), 'currentDistance': 0.3852586940695452}
episode index:3637
target Thresh 31.999999999999996
target distance 11.0
model initialize at round 3637
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 16.]), 'previousTarget': array([10., 16.]), 'currentState': array([20.05654086,  6.68201998,  2.54719412]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 13.70980545805689}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9306284870392358
{'scaleFactor': 20, 'currentTarget': array([10., 16.]), 'previousTarget': array([10., 16.]), 'currentState': array([ 9.77360793, 16.14746475,  2.50317485]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 0.2701836834314799}
episode index:3638
target Thresh 31.999999999999996
target distance 5.0
model initialize at round 3638
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 13.]), 'previousTarget': array([15., 13.]), 'currentState': array([18.04824246, 12.05330089,  2.73157394]), 'targetState': array([15, 13], dtype=int32), 'currentDistance': 3.191867989866688}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.930642081849063
{'scaleFactor': 20, 'currentTarget': array([15., 13.]), 'previousTarget': array([15., 13.]), 'currentState': array([14.21420271, 13.17457827,  2.86317258]), 'targetState': array([15, 13], dtype=int32), 'currentDistance': 0.8049564923842014}
episode index:3639
target Thresh 31.999999999999996
target distance 6.0
model initialize at round 3639
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 19.]), 'previousTarget': array([ 8., 19.]), 'currentState': array([ 3.83281483, 20.21694586,  6.02498227]), 'targetState': array([ 8, 19], dtype=int32), 'currentDistance': 4.341242844778032}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9306556691892145
{'scaleFactor': 20, 'currentTarget': array([ 8., 19.]), 'previousTarget': array([ 8., 19.]), 'currentState': array([ 7.62184325, 19.0969593 ,  0.10001783]), 'targetState': array([ 8, 19], dtype=int32), 'currentDistance': 0.39038908223751734}
episode index:3640
target Thresh 31.999999999999996
target distance 7.0
model initialize at round 3640
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([25.46226183,  8.8331737 ,  1.52790564]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 8.23808539327439}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.930663892298473
{'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([20.34252425, 14.63402566,  2.55292595]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 0.5012584995040977}
episode index:3641
target Thresh 31.999999999999996
target distance 24.0
model initialize at round 3641
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([10.19311708, 25.56825851,  4.75449014]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 23.880404376281735}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9306517344125361
{'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([18.10508596,  3.19128206,  5.17220074]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 0.21824729875723434}
episode index:3642
target Thresh 31.999999999999996
target distance 12.0
model initialize at round 3642
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 14.]), 'previousTarget': array([23., 14.]), 'currentState': array([12.7782824 , 23.09463455,  5.69472034]), 'targetState': array([23, 14], dtype=int32), 'currentDistance': 13.681954844264066}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9306521224480823
{'scaleFactor': 20, 'currentTarget': array([23., 14.]), 'previousTarget': array([23., 14.]), 'currentState': array([23.16535242, 13.8568089 ,  5.77270032]), 'targetState': array([23, 14], dtype=int32), 'currentDistance': 0.2187352591705237}
episode index:3643
target Thresh 31.999999999999996
target distance 4.0
model initialize at round 3643
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 13.]), 'previousTarget': array([13., 13.]), 'currentState': array([10.4819879 ,  9.20244125,  0.51628607]), 'targetState': array([13, 13], dtype=int32), 'currentDistance': 4.556515924028484}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.930665692118102
{'scaleFactor': 20, 'currentTarget': array([13., 13.]), 'previousTarget': array([13., 13.]), 'currentState': array([12.49191678, 12.48390454,  0.56838721]), 'targetState': array([13, 13], dtype=int32), 'currentDistance': 0.7242258500987507}
episode index:3644
target Thresh 31.999999999999996
target distance 8.0
model initialize at round 3644
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([ 8.67486482, 18.39120761,  1.83818936]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 7.129588956707295}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9306739034535977
{'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([ 5.62098882, 25.76725486,  2.00064469]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 0.8557625207946071}
episode index:3645
target Thresh 31.999999999999996
target distance 9.0
model initialize at round 3645
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 19.]), 'previousTarget': array([13., 19.]), 'currentState': array([20.22775034, 15.87575562,  2.94945729]), 'targetState': array([13, 19], dtype=int32), 'currentDistance': 7.874089021792728}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9306821102847952
{'scaleFactor': 20, 'currentTarget': array([13., 19.]), 'previousTarget': array([13., 19.]), 'currentState': array([12.93847009, 18.96998082,  3.01963768]), 'targetState': array([13, 19], dtype=int32), 'currentDistance': 0.06846226336613873}
episode index:3646
target Thresh 31.999999999999996
target distance 25.0
model initialize at round 3646
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  4.]), 'previousTarget': array([18.,  4.]), 'currentState': array([16.67548842, 28.84148616,  5.17885852]), 'targetState': array([18,  4], dtype=int32), 'currentDistance': 24.876771604194758}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9306675336225287
{'scaleFactor': 20, 'currentTarget': array([18.,  4.]), 'previousTarget': array([18.,  4.]), 'currentState': array([18.01052485,  3.06538819,  4.65366889]), 'targetState': array([18,  4], dtype=int32), 'currentDistance': 0.9346710659863222}
episode index:3647
target Thresh 31.999999999999996
target distance 6.0
model initialize at round 3647
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 17.]), 'previousTarget': array([11., 17.]), 'currentState': array([15.26526046, 20.01215449,  3.7658834 ]), 'targetState': array([11, 17], dtype=int32), 'currentDistance': 5.221639730809294}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9306783975113384
{'scaleFactor': 20, 'currentTarget': array([11., 17.]), 'previousTarget': array([11., 17.]), 'currentState': array([10.50526856, 16.41226976,  3.72614454]), 'targetState': array([11, 17], dtype=int32), 'currentDistance': 0.7682356598161815}
episode index:3648
target Thresh 31.999999999999996
target distance 26.0
model initialize at round 3648
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 29.]), 'previousTarget': array([13., 29.]), 'currentState': array([6.1786864 , 1.79871467, 0.21510666]), 'targetState': array([13, 29], dtype=int32), 'currentDistance': 28.043541908887043}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9306614250299072
{'scaleFactor': 20, 'currentTarget': array([13., 29.]), 'previousTarget': array([13., 29.]), 'currentState': array([12.60510948, 28.28596212,  1.68595799]), 'targetState': array([13, 29], dtype=int32), 'currentDistance': 0.8159587057535116}
episode index:3649
target Thresh 31.999999999999996
target distance 13.0
model initialize at round 3649
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([ 8.32802279, 19.03380053,  5.93464774]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 14.506239140123716}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.930661809666312
{'scaleFactor': 20, 'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([11.88742732,  5.8080922 ,  5.16000763]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 0.8158955911656268}
episode index:3650
target Thresh 31.999999999999996
target distance 7.0
model initialize at round 3650
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([20.41654027, 16.42987425,  4.4971931 ]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 6.584061755590554}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9306726661961212
{'scaleFactor': 20, 'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([19.08067276, 10.63964745,  4.91010895]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 0.6447146342083194}
episode index:3651
target Thresh 31.999999999999996
target distance 11.0
model initialize at round 3651
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 20.]), 'previousTarget': array([23., 20.]), 'currentState': array([22.49656325,  8.24364953,  2.59770203]), 'targetState': array([23, 20], dtype=int32), 'currentDistance': 11.767124749691101}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9306756255288717
{'scaleFactor': 20, 'currentTarget': array([23., 20.]), 'previousTarget': array([23., 20.]), 'currentState': array([23.03543228, 19.87293481,  1.73266061]), 'targetState': array([23, 20], dtype=int32), 'currentDistance': 0.13191288306061344}
episode index:3652
target Thresh 31.999999999999996
target distance 9.0
model initialize at round 3652
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 19.]), 'previousTarget': array([10., 19.]), 'currentState': array([11.37536259, 26.21689334,  4.96231627]), 'targetState': array([10, 19], dtype=int32), 'currentDistance': 7.346779686844275}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9306838161624525
{'scaleFactor': 20, 'currentTarget': array([10., 19.]), 'previousTarget': array([10., 19.]), 'currentState': array([ 9.91529354, 18.4276181 ,  4.56706975]), 'targetState': array([10, 19], dtype=int32), 'currentDistance': 0.5786157820500253}
episode index:3653
target Thresh 31.999999999999996
target distance 13.0
model initialize at round 3653
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 12.]), 'previousTarget': array([ 7., 12.]), 'currentState': array([18.572799  ,  1.10810013,  2.6901412 ]), 'targetState': array([ 7, 12], dtype=int32), 'currentDistance': 15.892235821879089}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9306816434416714
{'scaleFactor': 20, 'currentTarget': array([ 7., 12.]), 'previousTarget': array([ 7., 12.]), 'currentState': array([ 7.05054619, 12.10918268,  2.54132778]), 'targetState': array([ 7, 12], dtype=int32), 'currentDistance': 0.12031531076133928}
episode index:3654
target Thresh 31.999999999999996
target distance 6.0
model initialize at round 3654
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 24.]), 'previousTarget': array([19., 24.]), 'currentState': array([14.69403651, 21.00827324,  0.83343063]), 'targetState': array([19, 24], dtype=int32), 'currentDistance': 5.243257628811763}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.930692482663712
{'scaleFactor': 20, 'currentTarget': array([19., 24.]), 'previousTarget': array([19., 24.]), 'currentState': array([19.60020533, 24.42837932,  0.65762161]), 'targetState': array([19, 24], dtype=int32), 'currentDistance': 0.7373976461095801}
episode index:3655
target Thresh 31.999999999999996
target distance 10.0
model initialize at round 3655
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  3.]), 'previousTarget': array([17.,  3.]), 'currentState': array([26.25767903, 11.51041308,  3.03759575]), 'targetState': array([17,  3], dtype=int32), 'currentDistance': 12.575044801875215}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9306954333384212
{'scaleFactor': 20, 'currentTarget': array([17.,  3.]), 'previousTarget': array([17.,  3.]), 'currentState': array([17.76518698,  3.52827537,  3.75596103]), 'targetState': array([17,  3], dtype=int32), 'currentDistance': 0.9298311569699156}
episode index:3656
target Thresh 31.999999999999996
target distance 13.0
model initialize at round 3656
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 22.]), 'previousTarget': array([23., 22.]), 'currentState': array([11.71991416, 24.29111152,  0.27003365]), 'targetState': array([23, 22], dtype=int32), 'currentDistance': 11.510409571020443}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9306983823994173
{'scaleFactor': 20, 'currentTarget': array([23., 22.]), 'previousTarget': array([23., 22.]), 'currentState': array([23.31327397, 21.87693603,  6.20010833]), 'targetState': array([23, 22], dtype=int32), 'currentDistance': 0.33657884789361014}
episode index:3657
target Thresh 31.999999999999996
target distance 12.0
model initialize at round 3657
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 28.]), 'previousTarget': array([19., 28.]), 'currentState': array([ 8.66161917, 23.1069947 ,  0.48566821]), 'targetState': array([19, 28], dtype=int32), 'currentDistance': 11.437815313780916}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9307013298480235
{'scaleFactor': 20, 'currentTarget': array([19., 28.]), 'previousTarget': array([19., 28.]), 'currentState': array([19.48288071, 28.22916972,  0.548798  ]), 'targetState': array([19, 28], dtype=int32), 'currentDistance': 0.5345021394902977}
episode index:3658
target Thresh 31.999999999999996
target distance 17.0
model initialize at round 3658
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'previousTarget': array([13., 22.]), 'currentState': array([13.69403691,  6.53319955,  2.15572236]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 15.482364274789271}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9306991553097834
{'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'previousTarget': array([13., 22.]), 'currentState': array([13.02899075, 22.39285269,  1.6260225 ]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 0.3939209346825284}
episode index:3659
target Thresh 31.999999999999996
target distance 20.0
model initialize at round 3659
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 27.]), 'previousTarget': array([23., 27.]), 'currentState': array([ 2.22925732, 18.49610956,  1.03650188]), 'targetState': array([23, 27], dtype=int32), 'currentDistance': 22.444150776324946}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9306894938614088
{'scaleFactor': 20, 'currentTarget': array([23., 27.]), 'previousTarget': array([23., 27.]), 'currentState': array([22.40693737, 26.64007896,  0.86473194]), 'targetState': array([23, 27], dtype=int32), 'currentDistance': 0.693733691076656}
episode index:3660
target Thresh 31.999999999999996
target distance 15.0
model initialize at round 3660
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 12.]), 'previousTarget': array([17., 12.]), 'currentState': array([19.5639635 , 27.87760313,  3.60303044]), 'targetState': array([17, 12], dtype=int32), 'currentDistance': 16.08328914767593}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9306873237441092
{'scaleFactor': 20, 'currentTarget': array([17., 12.]), 'previousTarget': array([17., 12.]), 'currentState': array([17.07557777, 12.47718052,  4.87824979]), 'targetState': array([17, 12], dtype=int32), 'currentDistance': 0.48312860077615305}
episode index:3661
target Thresh 31.999999999999996
target distance 21.0
model initialize at round 3661
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([24.20293973,  3.17699792,  1.78449839]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 24.981608993454863}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9306752258598853
{'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([ 9.63977666, 22.10046003,  2.33006254]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 1.103850686121278}
episode index:3662
target Thresh 31.999999999999996
target distance 21.0
model initialize at round 3662
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 29.]), 'previousTarget': array([12., 29.]), 'currentState': array([23.52384987,  8.71433162,  1.44835108]), 'targetState': array([12, 29], dtype=int32), 'currentDistance': 23.330397715377522}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9306631345811127
{'scaleFactor': 20, 'currentTarget': array([12., 29.]), 'previousTarget': array([12., 29.]), 'currentState': array([11.68341258, 29.37725113,  2.03134216]), 'targetState': array([12, 29], dtype=int32), 'currentDistance': 0.4924896056677906}
episode index:3663
target Thresh 31.999999999999996
target distance 23.0
model initialize at round 3663
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  3.]), 'previousTarget': array([26.,  3.]), 'currentState': array([4.76124933, 5.45009945, 5.7761702 ]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 21.37960517690676}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9306534935111557
{'scaleFactor': 20, 'currentTarget': array([26.,  3.]), 'previousTarget': array([26.,  3.]), 'currentState': array([26.3548761 ,  2.86363811,  6.27504459]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 0.3801731325549351}
episode index:3664
target Thresh 31.999999999999996
target distance 15.0
model initialize at round 3664
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([ 3.93521345, 19.90564306,  0.39067608]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 13.065127278211081}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9306538787374575
{'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([17.7746674 , 19.91827986,  6.23807717]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 0.7789658278737022}
episode index:3665
target Thresh 31.999999999999996
target distance 8.0
model initialize at round 3665
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 23.]), 'previousTarget': array([11., 23.]), 'currentState': array([ 4.63681829, 17.60857442,  0.77527016]), 'targetState': array([11, 23], dtype=int32), 'currentDistance': 8.340116980463293}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9306620462582601
{'scaleFactor': 20, 'currentTarget': array([11., 23.]), 'previousTarget': array([11., 23.]), 'currentState': array([10.64826433, 22.79304548,  1.0089705 ]), 'targetState': array([11, 23], dtype=int32), 'currentDistance': 0.40810312135845067}
episode index:3666
target Thresh 31.999999999999996
target distance 12.0
model initialize at round 3666
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  5.]), 'previousTarget': array([13.,  5.]), 'currentState': array([10.67748669, 16.86425575,  5.19244004]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 12.08944301999268}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9306649963818332
{'scaleFactor': 20, 'currentTarget': array([13.,  5.]), 'previousTarget': array([13.,  5.]), 'currentState': array([13.00945344,  5.20219995,  5.24210594]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 0.20242081325640635}
episode index:3667
target Thresh 31.999999999999996
target distance 6.0
model initialize at round 3667
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 23.]), 'previousTarget': array([11., 23.]), 'currentState': array([16.02130001, 25.36913637,  3.20140624]), 'targetState': array([11, 23], dtype=int32), 'currentDistance': 5.5521402113899}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.930675801726331
{'scaleFactor': 20, 'currentTarget': array([11., 23.]), 'previousTarget': array([11., 23.]), 'currentState': array([10.66129185, 22.94888877,  3.78069603]), 'targetState': array([11, 23], dtype=int32), 'currentDistance': 0.34254280098445383}
episode index:3668
target Thresh 31.999999999999996
target distance 13.0
model initialize at round 3668
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 19.]), 'previousTarget': array([19., 19.]), 'currentState': array([4.56544755, 8.88002691, 1.58134174]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 17.628674362900917}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9306711250966655
{'scaleFactor': 20, 'currentTarget': array([19., 19.]), 'previousTarget': array([19., 19.]), 'currentState': array([19.00591419, 19.01642383,  0.84212807]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 0.017456230410593996}
episode index:3669
target Thresh 31.999999999999996
target distance 12.0
model initialize at round 3669
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([14.18881583,  8.66977832,  2.61585617]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 10.451885047470464}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9306766615884376
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.53981571, 10.8505844 ,  3.33456733]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.5601125103829084}
episode index:3670
target Thresh 31.999999999999996
target distance 9.0
model initialize at round 3670
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 27.]), 'previousTarget': array([23., 27.]), 'currentState': array([22.71800291, 19.09035386,  1.42680144]), 'targetState': array([23, 27], dtype=int32), 'currentDistance': 7.914671463648877}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9306848117786886
{'scaleFactor': 20, 'currentTarget': array([23., 27.]), 'previousTarget': array([23., 27.]), 'currentState': array([23.00105361, 27.06363044,  1.73127624]), 'targetState': array([23, 27], dtype=int32), 'currentDistance': 0.06363916277314903}
episode index:3671
target Thresh 31.999999999999996
target distance 6.0
model initialize at round 3671
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([25.21752612,  5.45001169,  3.60636055]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 4.253235678463432}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9306982690739559
{'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([21.32997143,  5.72199026,  3.22186461]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 0.43147486397418106}
episode index:3672
target Thresh 31.999999999999996
target distance 5.0
model initialize at round 3672
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 17.]), 'previousTarget': array([12., 17.]), 'currentState': array([ 8.36520001, 18.00748092,  0.37314945]), 'targetState': array([12, 17], dtype=int32), 'currentDistance': 3.7718415641258}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9307117190415373
{'scaleFactor': 20, 'currentTarget': array([12., 17.]), 'previousTarget': array([12., 17.]), 'currentState': array([12.15043126, 17.16418048,  6.18876672]), 'targetState': array([12, 17], dtype=int32), 'currentDistance': 0.22267643529355982}
episode index:3673
target Thresh 31.999999999999996
target distance 16.0
model initialize at round 3673
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 20.]), 'previousTarget': array([ 7., 20.]), 'currentState': array([22.98734018, 27.6829224 ,  2.58831871]), 'targetState': array([ 7, 20], dtype=int32), 'currentDistance': 17.737596870065705}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9307070390002857
{'scaleFactor': 20, 'currentTarget': array([ 7., 20.]), 'previousTarget': array([ 7., 20.]), 'currentState': array([ 7.44415279, 19.99533594,  3.95771012]), 'targetState': array([ 7, 20], dtype=int32), 'currentDistance': 0.44417727804551344}
episode index:3674
target Thresh 31.999999999999996
target distance 1.0
model initialize at round 3674
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([16.75647917, 19.07403665,  5.44837714]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 2.0588282574141266}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9307204792617823
{'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([14.37586381, 17.52394301,  2.74726314]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 0.7849689442368551}
episode index:3675
target Thresh 31.999999999999996
target distance 20.0
model initialize at round 3675
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([22.22600377,  6.69535365,  3.3099755 ]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 23.799567507052593}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9307084184327438
{'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([ 4.45559316, 21.93830146,  2.43357067]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 0.4597519327156896}
episode index:3676
target Thresh 31.999999999999996
target distance 5.0
model initialize at round 3676
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 24.]), 'previousTarget': array([19., 24.]), 'currentState': array([15.6002556 , 28.13634294,  5.0323232 ]), 'targetState': array([19, 24], dtype=int32), 'currentDistance': 5.354212820244978}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9307191855204694
{'scaleFactor': 20, 'currentTarget': array([19., 24.]), 'previousTarget': array([19., 24.]), 'currentState': array([19.17673747, 23.49174504,  5.89527591]), 'targetState': array([19, 24], dtype=int32), 'currentDistance': 0.5381070898107002}
episode index:3677
target Thresh 31.999999999999996
target distance 13.0
model initialize at round 3677
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'previousTarget': array([ 2., 17.]), 'currentState': array([15.5360653 , 23.40468747,  4.02656031]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 14.974815039681562}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9307170173608466
{'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'previousTarget': array([ 2., 17.]), 'currentState': array([ 1.48293497, 16.17240606,  3.56124437]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 0.9758421903011254}
episode index:3678
target Thresh 31.999999999999996
target distance 14.0
model initialize at round 3678
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'previousTarget': array([16., 23.]), 'currentState': array([21.50038099,  8.23610805,  2.60272455]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 15.755211722097012}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9307148503798918
{'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'previousTarget': array([16., 23.]), 'currentState': array([15.94189562, 23.06819653,  2.03829579]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 0.08959288720508858}
episode index:3679
target Thresh 31.999999999999996
target distance 8.0
model initialize at round 3679
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 14.]), 'previousTarget': array([17., 14.]), 'currentState': array([13.32861904, 22.21021559,  4.02382565]), 'targetState': array([17, 14], dtype=int32), 'currentDistance': 8.993702139145789}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9307203599449787
{'scaleFactor': 20, 'currentTarget': array([17., 14.]), 'previousTarget': array([17., 14.]), 'currentState': array([17.25219974, 13.70727492,  5.6544219 ]), 'targetState': array([17, 14], dtype=int32), 'currentDistance': 0.3863841110113754}
episode index:3680
target Thresh 31.999999999999996
target distance 13.0
model initialize at round 3680
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 15.]), 'previousTarget': array([27., 15.]), 'currentState': array([18.65926457,  0.44955179,  6.12137997]), 'targetState': array([27, 15], dtype=int32), 'currentDistance': 16.77150591107728}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9307156864561275
{'scaleFactor': 20, 'currentTarget': array([27., 15.]), 'previousTarget': array([27., 15.]), 'currentState': array([27.01036231, 15.71468247,  0.91320515]), 'targetState': array([27, 15], dtype=int32), 'currentDistance': 0.7147575893660224}
episode index:3681
target Thresh 31.999999999999996
target distance 15.0
model initialize at round 3681
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([1.28814410e+01, 1.04336840e+01, 9.55939293e-03]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 15.955982167161713}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9307135216022361
{'scaleFactor': 20, 'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([26.98738898,  3.15854353,  5.99899614]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 0.15904430044505188}
episode index:3682
target Thresh 31.999999999999996
target distance 14.0
model initialize at round 3682
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([24.34859915, 21.32746352,  3.53603041]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 16.0953366234948}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9307113579239372
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.26027449,  9.14839469,  4.24670185]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.29960606609433477}
episode index:3683
target Thresh 31.999999999999996
target distance 15.0
model initialize at round 3683
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 18.]), 'previousTarget': array([23., 18.]), 'currentState': array([ 8.61367638, 20.43290416,  6.09563398]), 'targetState': array([23, 18], dtype=int32), 'currentDistance': 14.590590455302062}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9307117254565059
{'scaleFactor': 20, 'currentTarget': array([23., 18.]), 'previousTarget': array([23., 18.]), 'currentState': array([22.37783818, 18.0127429 ,  0.04403951]), 'targetState': array([23, 18], dtype=int32), 'currentDistance': 0.6222923080561764}
episode index:3684
target Thresh 31.999999999999996
target distance 17.0
model initialize at round 3684
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 10.]), 'previousTarget': array([22., 10.]), 'currentState': array([23.12158873, 25.01777904,  4.56603929]), 'targetState': array([22, 10], dtype=int32), 'currentDistance': 15.059603200466682}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9307095634399446
{'scaleFactor': 20, 'currentTarget': array([22., 10.]), 'previousTarget': array([22., 10.]), 'currentState': array([22.41980906,  9.18648567,  4.63935943]), 'targetState': array([22, 10], dtype=int32), 'currentDistance': 0.915448096102378}
episode index:3685
target Thresh 31.999999999999996
target distance 7.0
model initialize at round 3685
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([20.51016545, 14.21719664,  2.61537647]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 8.53775671211775}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9307176715372207
{'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([17.23182618, 21.29494718,  2.48298301]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 0.7421878813260062}
episode index:3686
target Thresh 31.999999999999996
target distance 4.0
model initialize at round 3686
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([14.31276405,  0.46373998,  3.28174639]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 4.845569167233201}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.930728406912448
{'scaleFactor': 20, 'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([10.24837617,  4.48813358,  2.04489884]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 0.8962213907157794}
episode index:3687
target Thresh 31.999999999999996
target distance 21.0
model initialize at round 3687
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  6.]), 'previousTarget': array([23.,  6.]), 'currentState': array([0.77015938, 9.85113964, 4.90296006]), 'targetState': array([23,  6], dtype=int32), 'currentDistance': 22.56096386262795}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.930716383177308
{'scaleFactor': 20, 'currentTarget': array([23.,  6.]), 'previousTarget': array([23.,  6.]), 'currentState': array([23.75315186,  6.22185858,  6.08907331]), 'targetState': array([23,  6], dtype=int32), 'currentDistance': 0.7851489968263136}
episode index:3688
target Thresh 31.999999999999996
target distance 15.0
model initialize at round 3688
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 19.]), 'previousTarget': array([21., 19.]), 'currentState': array([24.35027352,  5.88524221,  1.76781428]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 13.535922743659098}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9307167488495036
{'scaleFactor': 20, 'currentTarget': array([21., 19.]), 'previousTarget': array([21., 19.]), 'currentState': array([20.70431144, 19.35323257,  1.71413018]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 0.4606571085416145}
episode index:3689
target Thresh 31.999999999999996
target distance 6.0
model initialize at round 3689
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 18.]), 'previousTarget': array([11., 18.]), 'currentState': array([16.48707288, 11.3970983 ,  3.39268684]), 'targetState': array([11, 18], dtype=int32), 'currentDistance': 8.585236146304462}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9307222429690295
{'scaleFactor': 20, 'currentTarget': array([11., 18.]), 'previousTarget': array([11., 18.]), 'currentState': array([10.42132424, 18.53315541,  2.05101785]), 'targetState': array([11, 18], dtype=int32), 'currentDistance': 0.7868419950273245}
episode index:3690
target Thresh 31.999999999999996
target distance 24.0
model initialize at round 3690
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  5.]), 'previousTarget': array([19.,  5.]), 'currentState': array([14.20980386, 27.24370812,  5.68778968]), 'targetState': array([19,  5], dtype=int32), 'currentDistance': 22.753648723368244}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9307102306766283
{'scaleFactor': 20, 'currentTarget': array([19.,  5.]), 'previousTarget': array([19.,  5.]), 'currentState': array([19.54705305,  4.27461273,  4.82550892]), 'targetState': array([19,  5], dtype=int32), 'currentDistance': 0.9085448440751448}
episode index:3691
target Thresh 31.999999999999996
target distance 4.0
model initialize at round 3691
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 15.]), 'previousTarget': array([19., 15.]), 'currentState': array([16.44995813, 12.36007342,  0.56167459]), 'targetState': array([19, 15], dtype=int32), 'currentDistance': 3.6704122211490575}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9307236081872793
{'scaleFactor': 20, 'currentTarget': array([19., 15.]), 'previousTarget': array([19., 15.]), 'currentState': array([19.1716848 , 15.23305682,  0.55739754]), 'targetState': array([19, 15], dtype=int32), 'currentDistance': 0.28946701182742085}
episode index:3692
target Thresh 31.999999999999996
target distance 13.0
model initialize at round 3692
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([25.45475894, 13.66679693,  3.74421549]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 14.988611188064043}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9307239715069977
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.85815139,  4.67095435,  4.41869013]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.0893133351202573}
episode index:3693
target Thresh 31.999999999999996
target distance 20.0
model initialize at round 3693
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  9.]), 'previousTarget': array([24.,  9.]), 'currentState': array([24.7046624 , 27.92552869,  4.84405923]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 18.93864264990779}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9307168405117356
{'scaleFactor': 20, 'currentTarget': array([24.,  9.]), 'previousTarget': array([24.,  9.]), 'currentState': array([24.54118682,  8.14217885,  4.73985028]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 1.0142683599229743}
episode index:3694
target Thresh 31.999999999999996
target distance 21.0
model initialize at round 3694
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([23.0763979 ,  7.49021477,  2.67815369]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 20.15654386401885}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9307097133762815
{'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([ 4.24784906, 14.05483446,  2.94549045]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 0.253842416470655}
episode index:3695
target Thresh 31.999999999999996
target distance 12.0
model initialize at round 3695
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 29.]), 'previousTarget': array([13., 29.]), 'currentState': array([23.38142232, 18.46107958,  1.85407639]), 'targetState': array([13, 29], dtype=int32), 'currentDistance': 14.793335423153378}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.930710080160516
{'scaleFactor': 20, 'currentTarget': array([13., 29.]), 'previousTarget': array([13., 29.]), 'currentState': array([13.66236935, 28.30949525,  2.9145617 ]), 'targetState': array([13, 29], dtype=int32), 'currentDistance': 0.95683330576202}
episode index:3696
target Thresh 31.999999999999996
target distance 14.0
model initialize at round 3696
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 15.]), 'previousTarget': array([ 2., 15.]), 'currentState': array([14.81427996, 20.19434336,  3.36257136]), 'targetState': array([ 2, 15], dtype=int32), 'currentDistance': 13.82703778150532}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.930710446746328
{'scaleFactor': 20, 'currentTarget': array([ 2., 15.]), 'previousTarget': array([ 2., 15.]), 'currentState': array([ 1.84930478, 14.97571059,  3.6056216 ]), 'targetState': array([ 2, 15], dtype=int32), 'currentDistance': 0.15264018243665908}
episode index:3697
target Thresh 31.999999999999996
target distance 22.0
model initialize at round 3697
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 28.]), 'previousTarget': array([25., 28.]), 'currentState': array([ 4.46150925, 16.30249981,  1.08897561]), 'targetState': array([25, 28], dtype=int32), 'currentDistance': 23.63601305483582}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9306984603820688
{'scaleFactor': 20, 'currentTarget': array([25., 28.]), 'previousTarget': array([25., 28.]), 'currentState': array([25.12673634, 28.05525318,  0.61388035]), 'targetState': array([25, 28], dtype=int32), 'currentDistance': 0.1382570535722754}
episode index:3698
target Thresh 31.999999999999996
target distance 5.0
model initialize at round 3698
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 15.]), 'previousTarget': array([ 5., 15.]), 'currentState': array([ 8.17222016, 12.67137944,  2.3859117 ]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 3.9351562130768154}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9307118157590946
{'scaleFactor': 20, 'currentTarget': array([ 5., 15.]), 'previousTarget': array([ 5., 15.]), 'currentState': array([ 4.94350536, 15.02215775,  2.55972834]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 0.0606845122152173}
episode index:3699
target Thresh 31.999999999999996
target distance 3.0
model initialize at round 3699
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 19.]), 'previousTarget': array([16., 19.]), 'currentState': array([13.97360995, 17.62414773,  0.4555074 ]), 'targetState': array([16, 19], dtype=int32), 'currentDistance': 2.4493317666917647}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9307278395926731
{'scaleFactor': 20, 'currentTarget': array([16., 19.]), 'previousTarget': array([16., 19.]), 'currentState': array([15.5497701 , 18.83335907,  0.85733297]), 'targetState': array([16, 19], dtype=int32), 'currentDistance': 0.4800793299499712}
episode index:3700
target Thresh 31.999999999999996
target distance 11.0
model initialize at round 3700
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 29.]), 'previousTarget': array([16., 29.]), 'currentState': array([ 9.38997305, 18.49010337,  1.83609772]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 12.415731286256452}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9307307448371498
{'scaleFactor': 20, 'currentTarget': array([16., 29.]), 'previousTarget': array([16., 29.]), 'currentState': array([15.55469357, 28.49289913,  1.33688451]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 0.67486969481168}
episode index:3701
target Thresh 31.999999999999996
target distance 21.0
model initialize at round 3701
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 26.]), 'previousTarget': array([25., 26.]), 'currentState': array([6.34025789, 3.98211156, 0.36046427]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 28.861278337622217}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9307140012034198
{'scaleFactor': 20, 'currentTarget': array([25., 26.]), 'previousTarget': array([25., 26.]), 'currentState': array([24.15206907, 25.12408974,  1.13466702]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 1.2191003393677096}
episode index:3702
target Thresh 31.999999999999996
target distance 21.0
model initialize at round 3702
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'previousTarget': array([27.,  8.]), 'currentState': array([ 6.28062601, 20.65940867,  0.3932693 ]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 24.280714288911586}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9307020300639417
{'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'previousTarget': array([27.,  8.]), 'currentState': array([26.59558003,  8.46982284,  5.97582188]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 0.6199104906941779}
episode index:3703
target Thresh 31.999999999999996
target distance 19.0
model initialize at round 3703
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 27.]), 'previousTarget': array([ 6., 27.]), 'currentState': array([23.40831357, 19.07233084,  3.04905695]), 'targetState': array([ 6, 27], dtype=int32), 'currentDistance': 19.128442690881673}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9306949242445425
{'scaleFactor': 20, 'currentTarget': array([ 6., 27.]), 'previousTarget': array([ 6., 27.]), 'currentState': array([ 5.33297925, 27.39816781,  2.64680212]), 'targetState': array([ 6, 27], dtype=int32), 'currentDistance': 0.7768232042465154}
episode index:3704
target Thresh 31.999999999999996
target distance 13.0
model initialize at round 3704
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 16.]), 'previousTarget': array([ 3., 16.]), 'currentState': array([14.0018456 , 15.91694705,  3.16394448]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 11.002159082791643}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9306978352364874
{'scaleFactor': 20, 'currentTarget': array([ 3., 16.]), 'previousTarget': array([ 3., 16.]), 'currentState': array([ 2.02267617, 15.826826  ,  3.09043213]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 0.9925477850073491}
episode index:3705
target Thresh 31.999999999999996
target distance 21.0
model initialize at round 3705
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'previousTarget': array([ 2., 17.]), 'currentState': array([23.2558202 ,  8.33658661,  3.85498571]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 22.953531836953633}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9306858781497307
{'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'previousTarget': array([ 2., 17.]), 'currentState': array([ 1.34522892, 16.92403453,  2.60613498]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 0.6591630408735774}
episode index:3706
target Thresh 31.999999999999996
target distance 4.0
model initialize at round 3706
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 26.]), 'previousTarget': array([ 7., 26.]), 'currentState': array([ 4.76162136, 28.37936341,  5.30177448]), 'targetState': array([ 7, 26], dtype=int32), 'currentDistance': 3.26675820576247}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9306992080989756
{'scaleFactor': 20, 'currentTarget': array([ 7., 26.]), 'previousTarget': array([ 7., 26.]), 'currentState': array([ 7.70238691, 25.72422742,  5.31861705]), 'targetState': array([ 7, 26], dtype=int32), 'currentDistance': 0.7545845790735751}
episode index:3707
target Thresh 31.999999999999996
target distance 5.0
model initialize at round 3707
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 20.]), 'previousTarget': array([ 7., 20.]), 'currentState': array([12.24022167, 19.33426244,  3.84561491]), 'targetState': array([ 7, 20], dtype=int32), 'currentDistance': 5.282341305592303}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9307098876545044
{'scaleFactor': 20, 'currentTarget': array([ 7., 20.]), 'previousTarget': array([ 7., 20.]), 'currentState': array([ 6.48157194, 19.79203355,  2.67661738]), 'targetState': array([ 7, 20], dtype=int32), 'currentDistance': 0.5585854387019372}
episode index:3708
target Thresh 31.999999999999996
target distance 20.0
model initialize at round 3708
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 5.]), 'previousTarget': array([6., 5.]), 'currentState': array([ 4.59595323, 23.36625146,  5.47994113]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 18.419840934480685}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9307052522702577
{'scaleFactor': 20, 'currentTarget': array([6., 5.]), 'previousTarget': array([6., 5.]), 'currentState': array([6.16791595, 5.62282308, 5.14797313]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 0.6450615182237427}
episode index:3709
target Thresh 31.999999999999996
target distance 11.0
model initialize at round 3709
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  4.]), 'previousTarget': array([13.,  4.]), 'currentState': array([ 3.1779918 , 11.79756625,  0.2137745 ]), 'targetState': array([13,  4], dtype=int32), 'currentDistance': 12.540888508612703}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9307081565551986
{'scaleFactor': 20, 'currentTarget': array([13.,  4.]), 'previousTarget': array([13.,  4.]), 'currentState': array([12.41661907,  4.5709092 ,  5.90722396]), 'targetState': array([13,  4], dtype=int32), 'currentDistance': 0.8162540157532014}
episode index:3710
target Thresh 31.999999999999996
target distance 10.0
model initialize at round 3710
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([17.01856316, 11.69246313,  0.75370753]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 12.234434049486717}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.930711059274909
{'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([25.8035705 , 19.80812819,  1.08443771]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 0.27458940239181395}
episode index:3711
target Thresh 31.999999999999996
target distance 15.0
model initialize at round 3711
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'previousTarget': array([19.,  3.]), 'currentState': array([23.5413976 , 17.32436583,  4.86008203]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 15.027033923118738}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9307089131636894
{'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'previousTarget': array([19.,  3.]), 'currentState': array([18.77694097,  2.16591457,  4.2683486 ]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 0.8633966852116951}
episode index:3712
target Thresh 31.999999999999996
target distance 12.0
model initialize at round 3712
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 16.]), 'previousTarget': array([ 2., 16.]), 'currentState': array([12.43383215, 26.37390721,  3.96311045]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 14.713354617258439}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9307092784841159
{'scaleFactor': 20, 'currentTarget': array([ 2., 16.]), 'previousTarget': array([ 2., 16.]), 'currentState': array([ 2.56339759, 16.51130107,  4.30142728]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 0.7608190509781647}
episode index:3713
target Thresh 31.999999999999996
target distance 19.0
model initialize at round 3713
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 21.]), 'previousTarget': array([21., 21.]), 'currentState': array([ 3.77410753, 22.44902129,  5.80035592]), 'targetState': array([21, 21], dtype=int32), 'currentDistance': 17.28672999699558}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9307046495043096
{'scaleFactor': 20, 'currentTarget': array([21., 21.]), 'previousTarget': array([21., 21.]), 'currentState': array([21.6416009 , 21.01243992,  6.15429623]), 'targetState': array([21, 21], dtype=int32), 'currentDistance': 0.6417214894370094}
episode index:3714
target Thresh 31.999999999999996
target distance 11.0
model initialize at round 3714
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'previousTarget': array([ 2., 17.]), 'currentState': array([13.91839243, 18.58970095,  4.27961016]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 12.023943916784178}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9307075500426397
{'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'previousTarget': array([ 2., 17.]), 'currentState': array([ 2.38555837, 16.90998082,  3.54503171]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 0.3959276543919805}
episode index:3715
target Thresh 31.999999999999996
target distance 17.0
model initialize at round 3715
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 27.]), 'previousTarget': array([ 2., 27.]), 'currentState': array([ 7.61421862, 11.81587833,  2.45041215]), 'targetState': array([ 2, 27], dtype=int32), 'currentDistance': 16.188792471480518}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9307054071859081
{'scaleFactor': 20, 'currentTarget': array([ 2., 27.]), 'previousTarget': array([ 2., 27.]), 'currentState': array([ 1.92309269, 26.60063819,  2.17384799]), 'targetState': array([ 2, 27], dtype=int32), 'currentDistance': 0.40669963381516566}
episode index:3716
target Thresh 31.999999999999996
target distance 15.0
model initialize at round 3716
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([ 7.03119258, 23.66995115,  4.36188913]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 14.234429976079346}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9307057730564277
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([10.9609912 , 10.50035256,  5.2870107 ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.5018708698306092}
episode index:3717
target Thresh 31.999999999999996
target distance 20.0
model initialize at round 3717
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 24.]), 'previousTarget': array([ 5., 24.]), 'currentState': array([20.585744  ,  5.3921769 ,  2.14606637]), 'targetState': array([ 5, 24], dtype=int32), 'currentDistance': 24.272752143012838}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9306938524266966
{'scaleFactor': 20, 'currentTarget': array([ 5., 24.]), 'previousTarget': array([ 5., 24.]), 'currentState': array([ 5.07271838, 23.53851355,  2.55034   ]), 'targetState': array([ 5, 24], dtype=int32), 'currentDistance': 0.4671805881596806}
episode index:3718
target Thresh 31.999999999999996
target distance 9.0
model initialize at round 3718
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 21.]), 'previousTarget': array([ 5., 21.]), 'currentState': array([12.43679385, 15.37648206,  2.51112556]), 'targetState': array([ 5, 21], dtype=int32), 'currentDistance': 9.323618226146662}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9307018928024893
{'scaleFactor': 20, 'currentTarget': array([ 5., 21.]), 'previousTarget': array([ 5., 21.]), 'currentState': array([ 5.99402129, 20.08926185,  2.46346103]), 'targetState': array([ 5, 21], dtype=int32), 'currentDistance': 1.3481551484412688}
episode index:3719
target Thresh 31.999999999999996
target distance 11.0
model initialize at round 3719
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([23.49831589, 20.24017572,  4.62001395]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 13.307237574893152}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9307022593226787
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.70838508,  9.51715934,  3.98579932]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.5640694711074392}
episode index:3720
target Thresh 31.999999999999996
target distance 23.0
model initialize at round 3720
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 29.]), 'previousTarget': array([17., 29.]), 'currentState': array([20.03563892,  7.88321344,  2.15420401]), 'targetState': array([17, 29], dtype=int32), 'currentDistance': 21.33386458929632}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9306927554245158
{'scaleFactor': 20, 'currentTarget': array([17., 29.]), 'previousTarget': array([17., 29.]), 'currentState': array([16.76985006, 29.54331656,  1.63296628]), 'targetState': array([17, 29], dtype=int32), 'currentDistance': 0.5900524380740395}
episode index:3721
target Thresh 32.0
target distance 2.0
model initialize at round 3721
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 19.]), 'previousTarget': array([ 4., 19.]), 'currentState': array([ 2.98713807, 21.36306512,  6.21719498]), 'targetState': array([ 4, 19], dtype=int32), 'currentDistance': 2.5709854213374643}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9307086896654011
{'scaleFactor': 20, 'currentTarget': array([ 4., 19.]), 'previousTarget': array([ 4., 19.]), 'currentState': array([ 3.82880063, 19.90050389,  4.23210949]), 'targetState': array([ 4, 19], dtype=int32), 'currentDistance': 0.9166332277809047}
episode index:3722
target Thresh 32.0
target distance 22.0
model initialize at round 3722
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 28.]), 'previousTarget': array([12., 28.]), 'currentState': array([16.57182157,  6.60146906,  1.375467  ]), 'targetState': array([12, 28], dtype=int32), 'currentDistance': 21.88146884151332}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9306991891455497
{'scaleFactor': 20, 'currentTarget': array([12., 28.]), 'previousTarget': array([12., 28.]), 'currentState': array([11.87889797, 28.00435126,  2.06483035]), 'targetState': array([12, 28], dtype=int32), 'currentDistance': 0.12118017366001026}
episode index:3723
target Thresh 32.0
target distance 22.0
model initialize at round 3723
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 25.]), 'previousTarget': array([27., 25.]), 'currentState': array([ 6.03708638, 29.32545838,  6.18004257]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 21.404516759326643}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9306896937280184
{'scaleFactor': 20, 'currentTarget': array([27., 25.]), 'previousTarget': array([27., 25.]), 'currentState': array([27.47715885, 24.78700482,  5.95988574]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 0.5225394804788828}
episode index:3724
target Thresh 32.0
target distance 6.0
model initialize at round 3724
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 19.]), 'previousTarget': array([ 2., 19.]), 'currentState': array([ 6.25786074, 14.61493727,  2.11099482]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 6.112131649509045}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9307003270988297
{'scaleFactor': 20, 'currentTarget': array([ 2., 19.]), 'previousTarget': array([ 2., 19.]), 'currentState': array([ 2.16694047, 18.89011685,  2.81850216]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 0.19985852394015186}
episode index:3725
target Thresh 32.0
target distance 19.0
model initialize at round 3725
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 25.]), 'previousTarget': array([18., 25.]), 'currentState': array([27.11024029,  7.67935558,  2.51524597]), 'targetState': array([18, 25], dtype=int32), 'currentDistance': 19.57041648248053}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9306932636924716
{'scaleFactor': 20, 'currentTarget': array([18., 25.]), 'previousTarget': array([18., 25.]), 'currentState': array([17.80764686, 25.20808357,  1.9522485 ]), 'targetState': array([18, 25], dtype=int32), 'currentDistance': 0.28336990911803217}
episode index:3726
target Thresh 32.0
target distance 16.0
model initialize at round 3726
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([ 4.62536561, 22.43654865,  5.53557772]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 16.64489940182008}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9306886551557909
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.29476619, 5.04810764, 4.50934267]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.9964869109254307}
episode index:3727
target Thresh 32.0
target distance 5.0
model initialize at round 3727
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 23.]), 'previousTarget': array([11., 23.]), 'currentState': array([ 7.19175719, 19.81168065,  0.22604626]), 'targetState': array([11, 23], dtype=int32), 'currentDistance': 4.966698457503261}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9307019092718972
{'scaleFactor': 20, 'currentTarget': array([11., 23.]), 'previousTarget': array([11., 23.]), 'currentState': array([10.01724452, 22.40311975,  0.2548719 ]), 'targetState': array([11, 23], dtype=int32), 'currentDistance': 1.1498149269924562}
episode index:3728
target Thresh 32.0
target distance 18.0
model initialize at round 3728
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  9.]), 'previousTarget': array([17.,  9.]), 'currentState': array([11.07224024, 28.29718501,  6.15323621]), 'targetState': array([17,  9], dtype=int32), 'currentDistance': 20.187116811861443}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9306948511237978
{'scaleFactor': 20, 'currentTarget': array([17.,  9.]), 'previousTarget': array([17.,  9.]), 'currentState': array([16.97391691,  9.80112006,  5.44036342]), 'targetState': array([17,  9], dtype=int32), 'currentDistance': 0.8015445534132759}
episode index:3729
target Thresh 32.0
target distance 14.0
model initialize at round 3729
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 15.]), 'previousTarget': array([19., 15.]), 'currentState': array([ 7.67941746, 28.10205935,  5.24731571]), 'targetState': array([19, 15], dtype=int32), 'currentDistance': 17.315298101145157}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9306902458681301
{'scaleFactor': 20, 'currentTarget': array([19., 15.]), 'previousTarget': array([19., 15.]), 'currentState': array([19.48091131, 14.59076505,  5.221169  ]), 'targetState': array([19, 15], dtype=int32), 'currentDistance': 0.6314656990724007}
episode index:3730
target Thresh 32.0
target distance 22.0
model initialize at round 3730
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 23.]), 'previousTarget': array([ 5., 23.]), 'currentState': array([27.51125625, 11.60343542,  2.27213642]), 'targetState': array([ 5, 23], dtype=int32), 'currentDistance': 25.23169320657371}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9306759952053403
{'scaleFactor': 20, 'currentTarget': array([ 5., 23.]), 'previousTarget': array([ 5., 23.]), 'currentState': array([ 4.71250162, 23.34106541,  2.60959892]), 'targetState': array([ 5, 23], dtype=int32), 'currentDistance': 0.4460727864831238}
episode index:3731
target Thresh 32.0
target distance 14.0
model initialize at round 3731
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 11.]), 'previousTarget': array([24., 11.]), 'currentState': array([ 9.93528249, 21.31827478,  5.68392515]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 17.443711559778848}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9306713974701522
{'scaleFactor': 20, 'currentTarget': array([24., 11.]), 'previousTarget': array([24., 11.]), 'currentState': array([24.30949772, 10.5859799 ,  5.58298301]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 0.5169153499611033}
episode index:3732
target Thresh 32.0
target distance 17.0
model initialize at round 3732
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 20.]), 'previousTarget': array([24., 20.]), 'currentState': array([ 8.27042488, 17.89618448,  0.29466074]), 'targetState': array([24, 20], dtype=int32), 'currentDistance': 15.869643131405278}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9306692740565325
{'scaleFactor': 20, 'currentTarget': array([24., 20.]), 'previousTarget': array([24., 20.]), 'currentState': array([23.96125539, 20.0172989 ,  0.19006919]), 'targetState': array([24, 20], dtype=int32), 'currentDistance': 0.042431078824301}
episode index:3733
target Thresh 32.0
target distance 19.0
model initialize at round 3733
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 2.]), 'previousTarget': array([8., 2.]), 'currentState': array([17.2972613 , 19.1275836 ,  4.36617641]), 'targetState': array([8, 2], dtype=int32), 'currentDistance': 19.488283345601726}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.930662234099637
{'scaleFactor': 20, 'currentTarget': array([8., 2.]), 'previousTarget': array([8., 2.]), 'currentState': array([7.67569674, 1.65428668, 4.10521332]), 'targetState': array([8, 2], dtype=int32), 'currentDistance': 0.4740150881049705}
episode index:3734
target Thresh 32.0
target distance 13.0
model initialize at round 3734
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 7.32461299, 22.51298646,  5.27806833]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 13.836922452910875}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9306626097659845
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.10653579, 10.89035183,  5.46142703]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.1528809911109002}
episode index:3735
target Thresh 32.0
target distance 19.0
model initialize at round 3735
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([20.5097211 , 16.68451361,  3.67153579]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 21.050372715475}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9306555753616063
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.9824552 , 5.53842754, 4.16259261]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.120322472529741}
episode index:3736
target Thresh 32.0
target distance 5.0
model initialize at round 3736
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  7.]), 'previousTarget': array([19.,  7.]), 'currentState': array([16.19824089,  0.81821878,  0.23151749]), 'targetState': array([19,  7], dtype=int32), 'currentDistance': 6.787066609911797}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9306661837171424
{'scaleFactor': 20, 'currentTarget': array([19.,  7.]), 'previousTarget': array([19.,  7.]), 'currentState': array([18.28260729,  6.067819  ,  1.42116648]), 'targetState': array([19,  7], dtype=int32), 'currentDistance': 1.1762711069796346}
episode index:3737
target Thresh 32.0
target distance 17.0
model initialize at round 3737
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 17.]), 'previousTarget': array([ 9., 17.]), 'currentState': array([25.5990521 , 17.63451181,  2.82134783]), 'targetState': array([ 9, 17], dtype=int32), 'currentDistance': 16.61117503195165}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9306640645386273
{'scaleFactor': 20, 'currentTarget': array([ 9., 17.]), 'previousTarget': array([ 9., 17.]), 'currentState': array([ 9.7182539 , 16.85306142,  3.46939615]), 'targetState': array([ 9, 17], dtype=int32), 'currentDistance': 0.7331300060468907}
episode index:3738
target Thresh 32.0
target distance 3.0
model initialize at round 3738
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 16.]), 'previousTarget': array([17., 16.]), 'currentState': array([17.9913021 , 14.70237127,  2.55290216]), 'targetState': array([17, 16], dtype=int32), 'currentDistance': 1.6329483065252695}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9306799340051854
{'scaleFactor': 20, 'currentTarget': array([17., 16.]), 'previousTarget': array([17., 16.]), 'currentState': array([16.70005347, 16.1972717 ,  2.00810969]), 'targetState': array([17, 16], dtype=int32), 'currentDistance': 0.35900423839693885}
episode index:3739
target Thresh 32.0
target distance 10.0
model initialize at round 3739
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 15.]), 'previousTarget': array([23., 15.]), 'currentState': array([18.47923161,  5.72086863,  1.68894744]), 'targetState': array([23, 15], dtype=int32), 'currentDistance': 10.321803424993938}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9306853645174569
{'scaleFactor': 20, 'currentTarget': array([23., 15.]), 'previousTarget': array([23., 15.]), 'currentState': array([22.85610818, 14.5366787 ,  1.44729868]), 'targetState': array([23, 15], dtype=int32), 'currentDistance': 0.4851509903426921}
episode index:3740
target Thresh 32.0
target distance 12.0
model initialize at round 3740
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 6.]), 'previousTarget': array([8., 6.]), 'currentState': array([ 0.82261573, 16.79743841,  4.94756937]), 'targetState': array([8, 6], dtype=int32), 'currentDistance': 12.965319939673163}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9306882500520421
{'scaleFactor': 20, 'currentTarget': array([8., 6.]), 'previousTarget': array([8., 6.]), 'currentState': array([7.35564506, 6.83371529, 5.67546716]), 'targetState': array([8, 6], dtype=int32), 'currentDistance': 1.0536956222592655}
episode index:3741
target Thresh 32.0
target distance 23.0
model initialize at round 3741
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 12.]), 'previousTarget': array([25., 12.]), 'currentState': array([ 3.67228134, 18.20576968,  0.23895722]), 'targetState': array([25, 12], dtype=int32), 'currentDistance': 22.212229976144044}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9306788032332838
{'scaleFactor': 20, 'currentTarget': array([25., 12.]), 'previousTarget': array([25., 12.]), 'currentState': array([24.65157688, 12.27946572,  5.99380439]), 'targetState': array([25, 12], dtype=int32), 'currentDistance': 0.4466539611450127}
episode index:3742
target Thresh 32.0
target distance 16.0
model initialize at round 3742
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 19.]), 'previousTarget': array([14., 19.]), 'currentState': array([16.3762753 ,  2.5573871 ,  2.39771795]), 'targetState': array([14, 19], dtype=int32), 'currentDistance': 16.61343442331468}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9306766835141266
{'scaleFactor': 20, 'currentTarget': array([14., 19.]), 'previousTarget': array([14., 19.]), 'currentState': array([13.92028948, 18.16062555,  1.71872269]), 'targetState': array([14, 19], dtype=int32), 'currentDistance': 0.8431507762309122}
episode index:3743
target Thresh 32.0
target distance 8.0
model initialize at round 3743
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 13.]), 'previousTarget': array([11., 13.]), 'currentState': array([3.89255510e+00, 1.94267913e+01, 1.78861618e-03]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 9.58224502994848}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9306821090927554
{'scaleFactor': 20, 'currentTarget': array([11., 13.]), 'previousTarget': array([11., 13.]), 'currentState': array([11.16554034, 12.82848641,  5.64370916]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 0.23837054441180186}
episode index:3744
target Thresh 32.0
target distance 15.0
model initialize at round 3744
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 11.]), 'previousTarget': array([18., 11.]), 'currentState': array([25.8857991 , 27.26132646,  3.30433965]), 'targetState': array([18, 11], dtype=int32), 'currentDistance': 18.07253622383996}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9306775256851161
{'scaleFactor': 20, 'currentTarget': array([18., 11.]), 'previousTarget': array([18., 11.]), 'currentState': array([18.24790762, 11.45440309,  4.58351452]), 'targetState': array([18, 11], dtype=int32), 'currentDistance': 0.5176295494054602}
episode index:3745
target Thresh 32.0
target distance 12.0
model initialize at round 3745
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 18.]), 'previousTarget': array([ 5., 18.]), 'currentState': array([15.14029379, 22.51144458,  3.88740683]), 'targetState': array([ 5, 18], dtype=int32), 'currentDistance': 11.09858956119548}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.930680409460801
{'scaleFactor': 20, 'currentTarget': array([ 5., 18.]), 'previousTarget': array([ 5., 18.]), 'currentState': array([ 4.35370524, 17.47455259,  3.43480252]), 'targetState': array([ 5, 18], dtype=int32), 'currentDistance': 0.8329417142645011}
episode index:3746
target Thresh 32.0
target distance 3.0
model initialize at round 3746
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 2.]), 'previousTarget': array([9., 2.]), 'currentState': array([7.75706054, 2.30310407, 0.20693403]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 1.2793633533676665}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.930696240683256
{'scaleFactor': 20, 'currentTarget': array([9., 2.]), 'previousTarget': array([9., 2.]), 'currentState': array([9.73287193, 2.20906855, 5.97599506]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 0.7621095220737271}
episode index:3747
target Thresh 32.0
target distance 19.0
model initialize at round 3747
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([21.74189509, 27.48937769,  4.15890646]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 21.638720445397286}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9306868068555013
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([8.8353386 , 9.79576623, 4.10855449]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.26234483223018357}
episode index:3748
target Thresh 32.0
target distance 5.0
model initialize at round 3748
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  7.]), 'previousTarget': array([17.,  7.]), 'currentState': array([20.12470808,  5.6771876 ,  2.65741745]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 3.3931745058986342}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9306999872217708
{'scaleFactor': 20, 'currentTarget': array([17.,  7.]), 'previousTarget': array([17.,  7.]), 'currentState': array([16.40078125,  7.12247281,  2.71522142]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 0.6116066572753458}
episode index:3749
target Thresh 32.0
target distance 15.0
model initialize at round 3749
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  8.]), 'previousTarget': array([25.,  8.]), 'currentState': array([20.50705744, 21.39084006,  5.42513013]), 'targetState': array([25,  8], dtype=int32), 'currentDistance': 14.124486904035296}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9307003513179536
{'scaleFactor': 20, 'currentTarget': array([25.,  8.]), 'previousTarget': array([25.,  8.]), 'currentState': array([24.9453859 ,  8.21493053,  5.23371799]), 'targetState': array([25,  8], dtype=int32), 'currentDistance': 0.22176075251707406}
episode index:3750
target Thresh 32.0
target distance 23.0
model initialize at round 3750
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 15.]), 'previousTarget': array([ 4., 15.]), 'currentState': array([25.16501939, 16.44575377,  3.49286652]), 'targetState': array([ 4, 15], dtype=int32), 'currentDistance': 21.21434066337113}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9306909239393721
{'scaleFactor': 20, 'currentTarget': array([ 4., 15.]), 'previousTarget': array([ 4., 15.]), 'currentState': array([ 3.3928267 , 14.55549973,  3.13480395]), 'targetState': array([ 4, 15], dtype=int32), 'currentDistance': 0.7524891407422961}
episode index:3751
target Thresh 32.0
target distance 14.0
model initialize at round 3751
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([ 5.78655205, 15.51214115,  6.20869064]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 17.48490740632377}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9306863467334937
{'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([18.23963947,  2.77812611,  5.32194252]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 0.32658091902999115}
episode index:3752
target Thresh 32.0
target distance 14.0
model initialize at round 3752
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 14.]), 'previousTarget': array([13., 14.]), 'currentState': array([25.32779283, 16.79090405,  4.2721945 ]), 'targetState': array([13, 14], dtype=int32), 'currentDistance': 12.63976350654198}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9306892227800344
{'scaleFactor': 20, 'currentTarget': array([13., 14.]), 'previousTarget': array([13., 14.]), 'currentState': array([13.94051367, 14.02923495,  3.6936213 ]), 'targetState': array([13, 14], dtype=int32), 'currentDistance': 0.9409679257300868}
episode index:3753
target Thresh 32.0
target distance 15.0
model initialize at round 3753
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.35947697, 25.62440411,  4.37666249]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 14.638424275181317}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9306895893557209
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.98969865, 11.75484553,  5.22076688]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.7549158154054182}
episode index:3754
target Thresh 32.0
target distance 9.0
model initialize at round 3754
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 22.]), 'previousTarget': array([ 3., 22.]), 'currentState': array([10.42461973, 20.61102242,  3.77021885]), 'targetState': array([ 3, 22], dtype=int32), 'currentDistance': 7.553425504101285}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9306975537819909
{'scaleFactor': 20, 'currentTarget': array([ 3., 22.]), 'previousTarget': array([ 3., 22.]), 'currentState': array([ 2.85763798, 22.12615176,  3.36641872]), 'targetState': array([ 3, 22], dtype=int32), 'currentDistance': 0.19021359361007548}
episode index:3755
target Thresh 32.0
target distance 10.0
model initialize at round 3755
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 14.]), 'previousTarget': array([17., 14.]), 'currentState': array([8.76705629, 8.79470015, 0.86195063]), 'targetState': array([17, 14], dtype=int32), 'currentDistance': 9.740457314651362}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9307029564699884
{'scaleFactor': 20, 'currentTarget': array([17., 14.]), 'previousTarget': array([17., 14.]), 'currentState': array([17.16033802, 14.12944103,  0.49347925]), 'targetState': array([17, 14], dtype=int32), 'currentDistance': 0.20606615389090632}
episode index:3756
target Thresh 32.0
target distance 14.0
model initialize at round 3756
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 8.]), 'previousTarget': array([7., 8.]), 'currentState': array([ 7.34763884, 20.35332594,  3.91044962]), 'targetState': array([7, 8], dtype=int32), 'currentDistance': 12.358216483833935}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9307058250334515
{'scaleFactor': 20, 'currentTarget': array([7., 8.]), 'previousTarget': array([7., 8.]), 'currentState': array([7.08851338, 8.66371725, 5.00742035]), 'targetState': array([7, 8], dtype=int32), 'currentDistance': 0.6695933183410371}
episode index:3757
target Thresh 32.0
target distance 16.0
model initialize at round 3757
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 21.]), 'previousTarget': array([27., 21.]), 'currentState': array([16.58407722,  3.41801491,  6.07036567]), 'targetState': array([27, 21], dtype=int32), 'currentDistance': 20.435695409104348}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9306988203101878
{'scaleFactor': 20, 'currentTarget': array([27., 21.]), 'previousTarget': array([27., 21.]), 'currentState': array([26.46045583, 20.10284444,  1.51588203]), 'targetState': array([27, 21], dtype=int32), 'currentDistance': 1.0468982815643602}
episode index:3758
target Thresh 32.0
target distance 7.0
model initialize at round 3758
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 26.]), 'previousTarget': array([25., 26.]), 'currentState': array([22.66296358, 18.74127193,  0.85565489]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 7.625671936082392}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9307067738057158
{'scaleFactor': 20, 'currentTarget': array([25., 26.]), 'previousTarget': array([25., 26.]), 'currentState': array([25.13236103, 26.23936482,  1.00857678]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 0.2735232354528401}
episode index:3759
target Thresh 32.0
target distance 10.0
model initialize at round 3759
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  8.]), 'previousTarget': array([12.,  8.]), 'currentState': array([22.04532753,  2.3176405 ,  3.72932529]), 'targetState': array([12,  8], dtype=int32), 'currentDistance': 11.541135761285172}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9307096390651826
{'scaleFactor': 20, 'currentTarget': array([12.,  8.]), 'previousTarget': array([12.,  8.]), 'currentState': array([12.10271779,  8.08722345,  2.57796979]), 'targetState': array([12,  8], dtype=int32), 'currentDistance': 0.1347548730316836}
episode index:3760
target Thresh 32.0
target distance 11.0
model initialize at round 3760
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([24.46369147, 20.16934041,  4.75699816]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 13.230629525815203}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9307099995301764
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.59142787,  9.58365783,  3.82821402]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.5833283713512213}
episode index:3761
target Thresh 32.0
target distance 7.0
model initialize at round 3761
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 19.]), 'previousTarget': array([19., 19.]), 'currentState': array([24.27091926, 22.46795633,  4.21586871]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 6.309462018511106}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9307205229221143
{'scaleFactor': 20, 'currentTarget': array([19., 19.]), 'previousTarget': array([19., 19.]), 'currentState': array([19.2544504 , 19.33480219,  3.62563325]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 0.4205205255276103}
episode index:3762
target Thresh 32.0
target distance 14.0
model initialize at round 3762
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 25.]), 'previousTarget': array([25., 25.]), 'currentState': array([12.64210601, 18.63138645,  0.78918427]), 'targetState': array([25, 25], dtype=int32), 'currentDistance': 13.902402043234446}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9307208803031892
{'scaleFactor': 20, 'currentTarget': array([25., 25.]), 'previousTarget': array([25., 25.]), 'currentState': array([25.07799454, 24.88909986,  0.49120858]), 'targetState': array([25, 25], dtype=int32), 'currentDistance': 0.1355801991541735}
episode index:3763
target Thresh 32.0
target distance 16.0
model initialize at round 3763
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([17.50808491, 15.67123685,  3.95026314]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 18.619943537923888}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9307163097312392
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.65512611, 4.2782992 , 4.12773762]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.7117869484130622}
episode index:3764
target Thresh 32.0
target distance 23.0
model initialize at round 3764
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 25.]), 'previousTarget': array([21., 25.]), 'currentState': array([15.32987507,  2.20753508,  2.00796318]), 'targetState': array([21, 25], dtype=int32), 'currentDistance': 23.48716189106599}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9307045351129085
{'scaleFactor': 20, 'currentTarget': array([21., 25.]), 'previousTarget': array([21., 25.]), 'currentState': array([21.03118829, 25.28619333,  1.10920915]), 'targetState': array([21, 25], dtype=int32), 'currentDistance': 0.28788770336592073}
episode index:3765
target Thresh 32.0
target distance 16.0
model initialize at round 3765
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([5.54465809, 7.62020117, 0.83476949]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 18.38487075299361}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9306999713084397
{'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([16.74022816, 21.67299049,  1.21145911]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 0.41763216756030747}
episode index:3766
target Thresh 32.0
target distance 12.0
model initialize at round 3766
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 17.]), 'previousTarget': array([14., 17.]), 'currentState': array([24.13211963, 12.70563263,  2.68018998]), 'targetState': array([14, 17], dtype=int32), 'currentDistance': 11.004609908947426}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9307028330493721
{'scaleFactor': 20, 'currentTarget': array([14., 17.]), 'previousTarget': array([14., 17.]), 'currentState': array([13.10410424, 17.1686463 ,  2.77122383]), 'targetState': array([14, 17], dtype=int32), 'currentDistance': 0.9116308380855809}
episode index:3767
target Thresh 32.0
target distance 8.0
model initialize at round 3767
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 22.]), 'previousTarget': array([ 2., 22.]), 'currentState': array([ 9.01007624, 15.59674512,  2.74072385]), 'targetState': array([ 2, 22], dtype=int32), 'currentDistance': 9.49435842338386}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9307082171302773
{'scaleFactor': 20, 'currentTarget': array([ 2., 22.]), 'previousTarget': array([ 2., 22.]), 'currentState': array([ 1.61378722, 22.1738661 ,  2.28316907]), 'targetState': array([ 2, 22], dtype=int32), 'currentDistance': 0.42354424964179094}
episode index:3768
target Thresh 32.0
target distance 9.0
model initialize at round 3768
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 28.]), 'previousTarget': array([12., 28.]), 'currentState': array([19.14416939, 25.74543727,  2.77250122]), 'targetState': array([12, 28], dtype=int32), 'currentDistance': 7.491475779013354}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9307161470302161
{'scaleFactor': 20, 'currentTarget': array([12., 28.]), 'previousTarget': array([12., 28.]), 'currentState': array([11.53352364, 28.03408777,  2.76128271]), 'targetState': array([12, 28], dtype=int32), 'currentDistance': 0.4677201813284356}
episode index:3769
target Thresh 32.0
target distance 24.0
model initialize at round 3769
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([25.51800932, 26.79755357,  3.65788221]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 26.935983058477994}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9306997092757702
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.95571271,  1.13350272,  4.25845154]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.8676283156313079}
episode index:3770
target Thresh 32.0
target distance 16.0
model initialize at round 3770
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 19.]), 'previousTarget': array([10., 19.]), 'currentState': array([14.96927566,  4.70293712,  1.94511151]), 'targetState': array([10, 19], dtype=int32), 'currentDistance': 15.136040019949482}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9306975997518115
{'scaleFactor': 20, 'currentTarget': array([10., 19.]), 'previousTarget': array([10., 19.]), 'currentState': array([ 9.50715711, 19.65197969,  1.83761194]), 'targetState': array([10, 19], dtype=int32), 'currentDistance': 0.8172953134684114}
episode index:3771
target Thresh 32.0
target distance 22.0
model initialize at round 3771
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([5.40253363, 7.39246541, 0.51242155]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 24.149649050496873}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9306858519448031
{'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([25.88212614, 19.82291358,  0.86941636]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 0.2127295150160134}
episode index:3772
target Thresh 32.0
target distance 9.0
model initialize at round 3772
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 20.]), 'previousTarget': array([20., 20.]), 'currentState': array([14.38809659, 12.41591523,  1.02483174]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 9.434606599580633}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9306912333913855
{'scaleFactor': 20, 'currentTarget': array([20., 20.]), 'previousTarget': array([20., 20.]), 'currentState': array([20.33424369, 20.40349591,  0.80557194]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 0.5239539974412367}
episode index:3773
target Thresh 32.0
target distance 6.0
model initialize at round 3773
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 21.]), 'previousTarget': array([19., 21.]), 'currentState': array([18.46626853, 27.69530011,  3.72427785]), 'targetState': array([19, 21], dtype=int32), 'currentDistance': 6.71654024601887}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9306991572855583
{'scaleFactor': 20, 'currentTarget': array([19., 21.]), 'previousTarget': array([19., 21.]), 'currentState': array([19.17486883, 20.19423914,  4.60307854]), 'targetState': array([19, 21], dtype=int32), 'currentDistance': 0.8245178404639913}
episode index:3774
target Thresh 32.0
target distance 23.0
model initialize at round 3774
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 25.]), 'previousTarget': array([26., 25.]), 'currentState': array([ 2.27090125, 11.51683984,  1.00886035]), 'targetState': array([26, 25], dtype=int32), 'currentDistance': 27.292228484681104}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9306827458035671
{'scaleFactor': 20, 'currentTarget': array([26., 25.]), 'previousTarget': array([26., 25.]), 'currentState': array([26.50045161, 25.05970393,  0.46359996]), 'targetState': array([26, 25], dtype=int32), 'currentDistance': 0.5040003697594456}
episode index:3775
target Thresh 32.0
target distance 12.0
model initialize at round 3775
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 7.]), 'previousTarget': array([6., 7.]), 'currentState': array([ 6.63712105, 17.26103644,  4.3828702 ]), 'targetState': array([6, 7], dtype=int32), 'currentDistance': 10.280797251373512}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9306881237972368
{'scaleFactor': 20, 'currentTarget': array([6., 7.]), 'previousTarget': array([6., 7.]), 'currentState': array([5.98467995, 7.3352594 , 4.92091554]), 'targetState': array([6, 7], dtype=int32), 'currentDistance': 0.33560925302797356}
episode index:3776
target Thresh 32.0
target distance 23.0
model initialize at round 3776
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([26.36032035, 12.55666246,  2.97068965]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 22.819883845879957}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9306787645519261
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.88004248, 8.09507366, 3.55709829]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.8851631322619865}
episode index:3777
target Thresh 32.0
target distance 13.0
model initialize at round 3777
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 16.]), 'previousTarget': array([11., 16.]), 'currentState': array([23.15857747, 20.45753086,  3.10435224]), 'targetState': array([11, 16], dtype=int32), 'currentDistance': 12.949926155973516}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9306816235738554
{'scaleFactor': 20, 'currentTarget': array([11., 16.]), 'previousTarget': array([11., 16.]), 'currentState': array([11.98886901, 16.23054877,  3.60721061]), 'targetState': array([11, 16], dtype=int32), 'currentDistance': 1.0153889186745597}
episode index:3778
target Thresh 32.0
target distance 22.0
model initialize at round 3778
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  5.]), 'previousTarget': array([24.,  5.]), 'currentState': array([ 0.75367672, 21.86904183,  4.88850069]), 'targetState': array([24,  5], dtype=int32), 'currentDistance': 28.722049340280254}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9306652341028828
{'scaleFactor': 20, 'currentTarget': array([24.,  5.]), 'previousTarget': array([24.,  5.]), 'currentState': array([23.16200249,  5.53228794,  6.09769426]), 'targetState': array([24,  5], dtype=int32), 'currentDistance': 0.992758922397646}
episode index:3779
target Thresh 32.0
target distance 4.0
model initialize at round 3779
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 5.]), 'previousTarget': array([7., 5.]), 'currentState': array([10.36150751,  7.44285022,  3.31325817]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 4.155388065862727}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.930678312083279
{'scaleFactor': 20, 'currentTarget': array([7., 5.]), 'previousTarget': array([7., 5.]), 'currentState': array([7.34909128, 5.02402236, 3.42100978]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 0.34991683703578}
episode index:3780
target Thresh 32.0
target distance 4.0
model initialize at round 3780
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 4.]), 'previousTarget': array([5., 4.]), 'currentState': array([8.18979418, 4.52488827, 3.2001214 ]), 'targetState': array([5, 4], dtype=int32), 'currentDistance': 3.2326915437762374}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9306913831459389
{'scaleFactor': 20, 'currentTarget': array([5., 4.]), 'previousTarget': array([5., 4.]), 'currentState': array([4.24498212, 3.89644317, 3.2197514 ]), 'targetState': array([5, 4], dtype=int32), 'currentDistance': 0.7620866228512426}
episode index:3781
target Thresh 32.0
target distance 21.0
model initialize at round 3781
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  7.]), 'previousTarget': array([26.,  7.]), 'currentState': array([ 6.37132434, 17.82811196,  0.07953471]), 'targetState': array([26,  7], dtype=int32), 'currentDistance': 22.417245968755207}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9306820354122299
{'scaleFactor': 20, 'currentTarget': array([26.,  7.]), 'previousTarget': array([26.,  7.]), 'currentState': array([25.49295892,  7.28654918,  6.14822637]), 'targetState': array([26,  7], dtype=int32), 'currentDistance': 0.5824097287635235}
episode index:3782
target Thresh 32.0
target distance 10.0
model initialize at round 3782
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([6.56580554, 7.38303883, 0.63465803]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 8.456729701154327}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9306899428863477
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.41152026,  8.04184608,  0.25283705]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.5899656767795293}
episode index:3783
target Thresh 32.0
target distance 5.0
model initialize at round 3783
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 14.]), 'previousTarget': array([26., 14.]), 'currentState': array([22.68244218, 12.06838307,  1.04917758]), 'targetState': array([26, 14], dtype=int32), 'currentDistance': 3.838923526700424}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9307030005124348
{'scaleFactor': 20, 'currentTarget': array([26., 14.]), 'previousTarget': array([26., 14.]), 'currentState': array([26.09035863, 13.96974944,  0.50664121]), 'targetState': array([26, 14], dtype=int32), 'currentDistance': 0.09528786726787473}
episode index:3784
target Thresh 32.0
target distance 16.0
model initialize at round 3784
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 8.]), 'previousTarget': array([6., 8.]), 'currentState': array([20.98405845,  7.65826573,  3.05430508]), 'targetState': array([6, 8], dtype=int32), 'currentDistance': 14.987954827245689}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9307008979216596
{'scaleFactor': 20, 'currentTarget': array([6., 8.]), 'previousTarget': array([6., 8.]), 'currentState': array([5.00690363, 7.99882543, 3.03828028]), 'targetState': array([6, 8], dtype=int32), 'currentDistance': 0.9930970602207956}
episode index:3785
target Thresh 32.0
target distance 12.0
model initialize at round 3785
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 4.92880502, 20.59653667,  4.2870113 ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 10.797056913613043}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.930706256915843
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 6.8817574 , 10.94128958,  5.05580673]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.9486871929012977}
episode index:3786
target Thresh 32.0
target distance 17.0
model initialize at round 3786
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  5.]), 'previousTarget': array([21.,  5.]), 'currentState': array([ 5.29003767, 17.0808288 ,  5.97056991]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 19.817904549037994}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.930699305719142
{'scaleFactor': 20, 'currentTarget': array([21.,  5.]), 'previousTarget': array([21.,  5.]), 'currentState': array([21.12992081,  5.01613093,  5.88665568]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 0.1309183894902271}
episode index:3787
target Thresh 32.0
target distance 16.0
model initialize at round 3787
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 26.]), 'previousTarget': array([12., 26.]), 'currentState': array([ 9.31704738, 10.00765328,  2.12704515]), 'targetState': array([12, 26], dtype=int32), 'currentDistance': 16.215837579946232}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9306972057689594
{'scaleFactor': 20, 'currentTarget': array([12., 26.]), 'previousTarget': array([12., 26.]), 'currentState': array([11.93940365, 25.57958507,  1.73631886]), 'targetState': array([12, 26], dtype=int32), 'currentDistance': 0.42475950171008}
episode index:3788
target Thresh 32.0
target distance 11.0
model initialize at round 3788
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  6.]), 'previousTarget': array([22.,  6.]), 'currentState': array([1.19109481e+01, 5.84881037e-01, 1.11553033e-02]), 'targetState': array([22,  6], dtype=int32), 'currentDistance': 11.45043588687579}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9307000516237052
{'scaleFactor': 20, 'currentTarget': array([22.,  6.]), 'previousTarget': array([22.,  6.]), 'currentState': array([22.37973421,  6.26207885,  0.50680222]), 'targetState': array([22,  6], dtype=int32), 'currentDistance': 0.46139288666695316}
episode index:3789
target Thresh 32.0
target distance 18.0
model initialize at round 3789
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 16.]), 'previousTarget': array([ 5., 16.]), 'currentState': array([23.01644005, 22.68288972,  2.5710277 ]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 19.21596021858803}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9306931075665509
{'scaleFactor': 20, 'currentTarget': array([ 5., 16.]), 'previousTarget': array([ 5., 16.]), 'currentState': array([ 4.57319211, 15.88197244,  3.47046994]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 0.44282669805037433}
episode index:3790
target Thresh 32.0
target distance 13.0
model initialize at round 3790
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 29.]), 'previousTarget': array([ 7., 29.]), 'currentState': array([14.4695653 , 17.25867328,  2.21528009]), 'targetState': array([ 7, 29], dtype=int32), 'currentDistance': 13.915931835547257}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9306934695397349
{'scaleFactor': 20, 'currentTarget': array([ 7., 29.]), 'previousTarget': array([ 7., 29.]), 'currentState': array([ 6.98682048, 29.02288834,  2.36143743]), 'targetState': array([ 7, 29], dtype=int32), 'currentDistance': 0.026411660363228465}
episode index:3791
target Thresh 32.0
target distance 20.0
model initialize at round 3791
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 22.]), 'previousTarget': array([25., 22.]), 'currentState': array([ 6.78265315, 20.71518728,  6.16930372]), 'targetState': array([25, 22], dtype=int32), 'currentDistance': 18.262597565313584}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9306889399453108
{'scaleFactor': 20, 'currentTarget': array([25., 22.]), 'previousTarget': array([25., 22.]), 'currentState': array([24.64253713, 22.04645875,  0.19689562]), 'targetState': array([25, 22], dtype=int32), 'currentDistance': 0.36046930501728575}
episode index:3792
target Thresh 32.0
target distance 19.0
model initialize at round 3792
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  6.]), 'previousTarget': array([25.,  6.]), 'currentState': array([22.34637785, 25.31292473,  3.96456826]), 'targetState': array([25,  6], dtype=int32), 'currentDistance': 19.49437795828012}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9306820043099466
{'scaleFactor': 20, 'currentTarget': array([25.,  6.]), 'previousTarget': array([25.,  6.]), 'currentState': array([24.9951873 ,  5.91501727,  4.94246263]), 'targetState': array([25,  6], dtype=int32), 'currentDistance': 0.08511889397704345}
episode index:3793
target Thresh 32.0
target distance 10.0
model initialize at round 3793
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 26.]), 'previousTarget': array([20., 26.]), 'currentState': array([11.91606259, 24.01477717,  0.51763171]), 'targetState': array([20, 26], dtype=int32), 'currentDistance': 8.324130807951722}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9306898888660061
{'scaleFactor': 20, 'currentTarget': array([20., 26.]), 'previousTarget': array([20., 26.]), 'currentState': array([19.58265225, 25.96889211,  0.38615766]), 'targetState': array([20, 26], dtype=int32), 'currentDistance': 0.4185054867628801}
episode index:3794
target Thresh 32.0
target distance 10.0
model initialize at round 3794
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 21.]), 'previousTarget': array([15., 21.]), 'currentState': array([ 9.48463244, 11.73215383,  1.68151355]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 10.784815807612093}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9306952380520493
{'scaleFactor': 20, 'currentTarget': array([15., 21.]), 'previousTarget': array([15., 21.]), 'currentState': array([14.44229169, 20.22902285,  1.31036394]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 0.9515483804432169}
episode index:3795
target Thresh 32.0
target distance 18.0
model initialize at round 3795
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  2.]), 'previousTarget': array([17.,  2.]), 'currentState': array([18.38219812, 18.23684641,  4.13971746]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 16.295571580904912}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9306931435990399
{'scaleFactor': 20, 'currentTarget': array([17.,  2.]), 'previousTarget': array([17.,  2.]), 'currentState': array([17.20128208,  2.43453492,  5.02587453]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 0.4788894191987861}
episode index:3796
target Thresh 32.0
target distance 10.0
model initialize at round 3796
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  2.]), 'previousTarget': array([21.,  2.]), 'currentState': array([11.79970219, 10.51916763,  6.2175498 ]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 12.538807639975206}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9306959845276155
{'scaleFactor': 20, 'currentTarget': array([21.,  2.]), 'previousTarget': array([21.,  2.]), 'currentState': array([20.59149634,  2.61906806,  5.86325599]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 0.7417010893095841}
episode index:3797
target Thresh 32.0
target distance 22.0
model initialize at round 3797
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 15.]), 'previousTarget': array([27., 15.]), 'currentState': array([ 6.09319578, 10.27957457,  6.13697154]), 'targetState': array([27, 15], dtype=int32), 'currentDistance': 21.433079081303337}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9306866749619839
{'scaleFactor': 20, 'currentTarget': array([27., 15.]), 'previousTarget': array([27., 15.]), 'currentState': array([27.43270064, 15.15109985,  0.25918047]), 'targetState': array([27, 15], dtype=int32), 'currentDistance': 0.45832412707791537}
episode index:3798
target Thresh 32.0
target distance 8.0
model initialize at round 3798
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  8.]), 'previousTarget': array([17.,  8.]), 'currentState': array([10.70650218, 14.19341884,  5.24985579]), 'targetState': array([17,  8], dtype=int32), 'currentDistance': 8.82986702758111}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9306945479114542
{'scaleFactor': 20, 'currentTarget': array([17.,  8.]), 'previousTarget': array([17.,  8.]), 'currentState': array([16.40180436,  8.67551653,  5.92588739]), 'targetState': array([17,  8], dtype=int32), 'currentDistance': 0.9023084845478695}
episode index:3799
target Thresh 32.0
target distance 11.0
model initialize at round 3799
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 27.]), 'previousTarget': array([13., 27.]), 'currentState': array([13.48956696, 16.78331243,  1.49412554]), 'targetState': array([13, 27], dtype=int32), 'currentDistance': 10.22841047239525}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9306998888330301
{'scaleFactor': 20, 'currentTarget': array([13., 27.]), 'previousTarget': array([13., 27.]), 'currentState': array([13.00550695, 26.72798235,  1.9710022 ]), 'targetState': array([13, 27], dtype=int32), 'currentDistance': 0.272073389170376}
episode index:3800
target Thresh 32.0
target distance 19.0
model initialize at round 3800
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 21.]), 'previousTarget': array([ 5., 21.]), 'currentState': array([23.39140667, 17.43092312,  3.33238554]), 'targetState': array([ 5, 21], dtype=int32), 'currentDistance': 18.73451758397883}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9306953682749272
{'scaleFactor': 20, 'currentTarget': array([ 5., 21.]), 'previousTarget': array([ 5., 21.]), 'currentState': array([ 5.85665618, 20.81735954,  3.45135574]), 'targetState': array([ 5, 21], dtype=int32), 'currentDistance': 0.8759094413482335}
episode index:3801
target Thresh 32.0
target distance 15.0
model initialize at round 3801
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([13.36003561, 12.41435334,  0.46371114]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 15.607391323892443}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9306932770929579
{'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([27.33057464, 20.12303644,  0.63990512]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 0.35272872953515066}
episode index:3802
target Thresh 32.0
target distance 11.0
model initialize at round 3802
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([15.77623744, 24.24421615,  4.62993598]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 9.246923914720501}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9306986141355051
{'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([15.95944106, 14.26193196,  4.64841879]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 0.7391816192854217}
episode index:3803
target Thresh 32.0
target distance 21.0
model initialize at round 3803
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 28.]), 'previousTarget': array([19., 28.]), 'currentState': array([15.47804227,  8.59998382,  0.8761375 ]), 'targetState': array([19, 28], dtype=int32), 'currentDistance': 19.717119822768232}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9306916960127064
{'scaleFactor': 20, 'currentTarget': array([19., 28.]), 'previousTarget': array([19., 28.]), 'currentState': array([19.07152383, 28.1307726 ,  1.59131531]), 'targetState': array([19, 28], dtype=int32), 'currentDistance': 0.14905412037346297}
episode index:3804
target Thresh 32.0
target distance 15.0
model initialize at round 3804
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([22.32043068, 28.89306545,  4.21517467]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 14.565785045564239}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9306920570250308
{'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([ 9.70244956, 23.24171292,  3.86912989]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 0.7428731500860429}
episode index:3805
target Thresh 32.0
target distance 10.0
model initialize at round 3805
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 27.]), 'previousTarget': array([15., 27.]), 'currentState': array([24.31085117, 24.46459713,  3.28050089]), 'targetState': array([15, 27], dtype=int32), 'currentDistance': 9.649881725937968}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9306973901813301
{'scaleFactor': 20, 'currentTarget': array([15., 27.]), 'previousTarget': array([15., 27.]), 'currentState': array([14.72965639, 27.08416433,  2.89354847]), 'targetState': array([15, 27], dtype=int32), 'currentDistance': 0.2831418374634356}
episode index:3806
target Thresh 32.0
target distance 12.0
model initialize at round 3806
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([16.41006742, 16.09450447,  4.53924698]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 10.37821976526576}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9307027205358661
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.19071545,  6.39367293,  4.64991578]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.4374365740112218}
episode index:3807
target Thresh 32.0
target distance 1.0
model initialize at round 3807
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 15.]), 'previousTarget': array([23., 15.]), 'currentState': array([23.36362983, 14.53760258,  5.42464481]), 'targetState': array([23, 15], dtype=int32), 'currentDistance': 0.5882499662356709}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9307209183508515
{'scaleFactor': 20, 'currentTarget': array([23., 15.]), 'previousTarget': array([23., 15.]), 'currentState': array([23.36362983, 14.53760258,  5.42464481]), 'targetState': array([23, 15], dtype=int32), 'currentDistance': 0.5882499662356709}
episode index:3808
target Thresh 32.0
target distance 11.0
model initialize at round 3808
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  6.]), 'previousTarget': array([24.,  6.]), 'currentState': array([14.38353119, 15.55589158,  5.45904122]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 13.556973711424712}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9307212713121422
{'scaleFactor': 20, 'currentTarget': array([24.,  6.]), 'previousTarget': array([24.,  6.]), 'currentState': array([24.24934866,  5.63862484,  5.43424202]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 0.43905211093250024}
episode index:3809
target Thresh 32.0
target distance 20.0
model initialize at round 3809
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 29.]), 'previousTarget': array([18., 29.]), 'currentState': array([19.32849365, 10.86254629,  1.97783744]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 18.1860419723966}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9307167558203236
{'scaleFactor': 20, 'currentTarget': array([18., 29.]), 'previousTarget': array([18., 29.]), 'currentState': array([18.05378175, 28.73193575,  1.79475335]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 0.27340614723435736}
episode index:3810
target Thresh 32.0
target distance 10.0
model initialize at round 3810
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  6.]), 'previousTarget': array([10.,  6.]), 'currentState': array([20.56609031,  6.58490689,  2.23774321]), 'targetState': array([10,  6], dtype=int32), 'currentDistance': 10.582267265437231}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9307220754986443
{'scaleFactor': 20, 'currentTarget': array([10.,  6.]), 'previousTarget': array([10.,  6.]), 'currentState': array([10.93533291,  6.06293832,  3.51638727]), 'targetState': array([10,  6], dtype=int32), 'currentDistance': 0.9374480717651115}
episode index:3811
target Thresh 32.0
target distance 14.0
model initialize at round 3811
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 2.]), 'previousTarget': array([9., 2.]), 'currentState': array([20.55652927, 15.13467905,  4.69162416]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 17.4949467748464}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9307175621649572
{'scaleFactor': 20, 'currentTarget': array([9., 2.]), 'previousTarget': array([9., 2.]), 'currentState': array([8.77781436, 1.73853499, 4.01824703]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 0.34311864928832536}
episode index:3812
target Thresh 32.0
target distance 7.0
model initialize at round 3812
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 27.]), 'previousTarget': array([11., 27.]), 'currentState': array([16.32075561, 23.88807881,  2.19814396]), 'targetState': array([11, 27], dtype=int32), 'currentDistance': 6.163967369198062}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9307279428200411
{'scaleFactor': 20, 'currentTarget': array([11., 27.]), 'previousTarget': array([11., 27.]), 'currentState': array([11.20881836, 26.82689886,  2.71958557]), 'targetState': array([11, 27], dtype=int32), 'currentDistance': 0.2712362678384125}
episode index:3813
target Thresh 32.0
target distance 9.0
model initialize at round 3813
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 16.]), 'previousTarget': array([ 6., 16.]), 'currentState': array([ 8.1016854 , 23.57677508,  5.15928137]), 'targetState': array([ 6, 16], dtype=int32), 'currentDistance': 7.862862213645759}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9307357739860558
{'scaleFactor': 20, 'currentTarget': array([ 6., 16.]), 'previousTarget': array([ 6., 16.]), 'currentState': array([ 5.95757021, 16.05442913,  4.57669427]), 'targetState': array([ 6, 16], dtype=int32), 'currentDistance': 0.06901316440483599}
episode index:3814
target Thresh 32.0
target distance 16.0
model initialize at round 3814
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 11.]), 'previousTarget': array([19., 11.]), 'currentState': array([25.84950218, 25.32377254,  3.61284542]), 'targetState': array([19, 11], dtype=int32), 'currentDistance': 15.87722078746767}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9307336793387272
{'scaleFactor': 20, 'currentTarget': array([19., 11.]), 'previousTarget': array([19., 11.]), 'currentState': array([18.99028563, 11.07444205,  4.44871028]), 'targetState': array([19, 11], dtype=int32), 'currentDistance': 0.07507321099277993}
episode index:3815
target Thresh 32.0
target distance 11.0
model initialize at round 3815
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 17.]), 'previousTarget': array([27., 17.]), 'currentState': array([24.33691973, 26.07969553,  5.28017825]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 9.462180907261839}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9307389876119352
{'scaleFactor': 20, 'currentTarget': array([27., 17.]), 'previousTarget': array([27., 17.]), 'currentState': array([26.91833335, 16.5053258 ,  4.83817257]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 0.5013701304898917}
episode index:3816
target Thresh 32.0
target distance 9.0
model initialize at round 3816
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 22.]), 'previousTarget': array([20., 22.]), 'currentState': array([12.74237047, 17.74962253,  0.97278529]), 'targetState': array([20, 22], dtype=int32), 'currentDistance': 8.410641770199579}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9307468097294064
{'scaleFactor': 20, 'currentTarget': array([20., 22.]), 'previousTarget': array([20., 22.]), 'currentState': array([19.55594498, 21.73833482,  0.79889346]), 'targetState': array([20, 22], dtype=int32), 'currentDistance': 0.5154158755378473}
episode index:3817
target Thresh 32.0
target distance 4.0
model initialize at round 3817
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 17.]), 'previousTarget': array([11., 17.]), 'currentState': array([13.65404593, 17.77323085,  3.13054276]), 'targetState': array([11, 17], dtype=int32), 'currentDistance': 2.764388854179165}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9307623291611169
{'scaleFactor': 20, 'currentTarget': array([11., 17.]), 'previousTarget': array([11., 17.]), 'currentState': array([11.82506805, 17.08637741,  3.87861037]), 'targetState': array([11, 17], dtype=int32), 'currentDistance': 0.8295772085783761}
episode index:3818
target Thresh 32.0
target distance 15.0
model initialize at round 3818
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 6.]), 'previousTarget': array([7., 6.]), 'currentState': array([20.58818734,  9.91606392,  3.57602167]), 'targetState': array([7, 6], dtype=int32), 'currentDistance': 14.141230208791475}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9307626703548183
{'scaleFactor': 20, 'currentTarget': array([7., 6.]), 'previousTarget': array([7., 6.]), 'currentState': array([7.24341239, 6.06263516, 3.49488131]), 'targetState': array([7, 6], dtype=int32), 'currentDistance': 0.25134190810236867}
episode index:3819
target Thresh 32.0
target distance 14.0
model initialize at round 3819
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([19.53087715, 18.30085536,  4.84477741]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 17.610006954713985}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9307581558462132
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.26104899, 10.16296639,  3.80373865]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.3077411590863782}
episode index:3820
target Thresh 32.0
target distance 7.0
model initialize at round 3820
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  5.]), 'previousTarget': array([13.,  5.]), 'currentState': array([20.15419663,  8.32410875,  3.79413939]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 7.888740605844222}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9307659647585801
{'scaleFactor': 20, 'currentTarget': array([13.,  5.]), 'previousTarget': array([13.,  5.]), 'currentState': array([12.88660382,  5.07442151,  3.63520068]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 0.13563648245181117}
episode index:3821
target Thresh 32.0
target distance 7.0
model initialize at round 3821
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 22.]), 'previousTarget': array([21., 22.]), 'currentState': array([16.21816301, 27.3312301 ,  5.85238457]), 'targetState': array([21, 22], dtype=int32), 'currentDistance': 7.1615626331620295}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9307763083052157
{'scaleFactor': 20, 'currentTarget': array([21., 22.]), 'previousTarget': array([21., 22.]), 'currentState': array([20.22023567, 22.95216477,  5.59526595]), 'targetState': array([21, 22], dtype=int32), 'currentDistance': 1.2307112428006275}
episode index:3822
target Thresh 32.0
target distance 13.0
model initialize at round 3822
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 18.]), 'previousTarget': array([25., 18.]), 'currentState': array([23.58514219,  6.872303  ,  1.28386039]), 'targetState': array([25, 18], dtype=int32), 'currentDistance': 11.217284121641432}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9307791081590205
{'scaleFactor': 20, 'currentTarget': array([25., 18.]), 'previousTarget': array([25., 18.]), 'currentState': array([25.15077067, 18.73659128,  1.30058962]), 'targetState': array([25, 18], dtype=int32), 'currentDistance': 0.7518633577428382}
episode index:3823
target Thresh 32.0
target distance 13.0
model initialize at round 3823
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 29.]), 'previousTarget': array([ 3., 29.]), 'currentState': array([15.18893804, 17.47464117,  3.08364439]), 'targetState': array([ 3, 29], dtype=int32), 'currentDistance': 16.775103774825862}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9307770071094046
{'scaleFactor': 20, 'currentTarget': array([ 3., 29.]), 'previousTarget': array([ 3., 29.]), 'currentState': array([ 3.65297138, 28.27309134,  2.7731212 ]), 'targetState': array([ 3, 29], dtype=int32), 'currentDistance': 0.9771222141576066}
episode index:3824
target Thresh 32.0
target distance 1.0
model initialize at round 3824
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([23.66188356,  3.83151577,  1.62667471]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 1.062783294981233}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9307951046238858
{'scaleFactor': 20, 'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([23.66188356,  3.83151577,  1.62667471]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 1.062783294981233}
episode index:3825
target Thresh 32.0
target distance 19.0
model initialize at round 3825
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([ 1.37422107, 23.56499317,  4.41303658]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 23.028695644719786}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9307834971401148
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.46703208,  4.66157061,  5.39412329]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.5767611455184765}
episode index:3826
target Thresh 32.0
target distance 24.0
model initialize at round 3826
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 5.]), 'previousTarget': array([9., 5.]), 'currentState': array([ 1.72676754, 27.89942416,  4.86438465]), 'targetState': array([9, 5], dtype=int32), 'currentDistance': 24.02672547947162}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9307718957224446
{'scaleFactor': 20, 'currentTarget': array([9., 5.]), 'previousTarget': array([9., 5.]), 'currentState': array([8.88667032, 5.05944002, 5.25429542]), 'targetState': array([9, 5], dtype=int32), 'currentDistance': 0.1279716074098693}
episode index:3827
target Thresh 32.0
target distance 18.0
model initialize at round 3827
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 20.]), 'previousTarget': array([13., 20.]), 'currentState': array([13.57216462,  2.60057181,  1.37489622]), 'targetState': array([13, 20], dtype=int32), 'currentDistance': 17.40883320697648}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9307673882385785
{'scaleFactor': 20, 'currentTarget': array([13., 20.]), 'previousTarget': array([13., 20.]), 'currentState': array([12.95257619, 20.52507946,  1.53873982]), 'targetState': array([13, 20], dtype=int32), 'currentDistance': 0.5272167120568584}
episode index:3828
target Thresh 32.0
target distance 6.0
model initialize at round 3828
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'previousTarget': array([26., 11.]), 'currentState': array([27.67735306,  4.8626144 ,  0.92827624]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 6.362469272050278}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9307777125038597
{'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'previousTarget': array([26., 11.]), 'currentState': array([26.06779789, 10.29121848,  2.0358009 ]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 0.7120167116077415}
episode index:3829
target Thresh 32.0
target distance 15.0
model initialize at round 3829
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([ 3.31813418, 16.93904547,  4.187819  ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 18.964204929956917}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9307708206925033
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.14814556,  1.50933238,  5.25207883]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.5125444545353508}
episode index:3830
target Thresh 32.0
target distance 7.0
model initialize at round 3830
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 17.]), 'previousTarget': array([ 3., 17.]), 'currentState': array([ 8.20949607, 15.74686628,  2.50464582]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 5.3580960628263545}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9307811386719623
{'scaleFactor': 20, 'currentTarget': array([ 3., 17.]), 'previousTarget': array([ 3., 17.]), 'currentState': array([ 2.554558  , 17.44544007,  3.06421185]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 0.6299487527744827}
episode index:3831
target Thresh 32.0
target distance 6.0
model initialize at round 3831
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 21.]), 'previousTarget': array([21., 21.]), 'currentState': array([17.00453857, 16.35699839,  1.19370294]), 'targetState': array([21, 21], dtype=int32), 'currentDistance': 6.125453124183816}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9307914512662546
{'scaleFactor': 20, 'currentTarget': array([21., 21.]), 'previousTarget': array([21., 21.]), 'currentState': array([21.0230141 , 20.75447206,  0.77856622]), 'targetState': array([21, 21], dtype=int32), 'currentDistance': 0.24660417086302625}
episode index:3832
target Thresh 32.0
target distance 13.0
model initialize at round 3832
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 14.]), 'previousTarget': array([ 7., 14.]), 'currentState': array([18.62623831,  4.02780305,  2.74746084]), 'targetState': array([ 7, 14], dtype=int32), 'currentDistance': 15.317118826303085}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9307893519297458
{'scaleFactor': 20, 'currentTarget': array([ 7., 14.]), 'previousTarget': array([ 7., 14.]), 'currentState': array([ 6.53044517, 14.41163899,  2.38761117]), 'targetState': array([ 7, 14], dtype=int32), 'currentDistance': 0.6244424685465998}
episode index:3833
target Thresh 32.0
target distance 8.0
model initialize at round 3833
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 20.]), 'previousTarget': array([18., 20.]), 'currentState': array([24.58923623, 18.91768007,  3.57487535]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 6.677533271319372}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9307996570022733
{'scaleFactor': 20, 'currentTarget': array([18., 20.]), 'previousTarget': array([18., 20.]), 'currentState': array([18.76712294, 19.72835993,  3.08058668]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 0.8137972283676073}
episode index:3834
target Thresh 32.0
target distance 15.0
model initialize at round 3834
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 29.]), 'previousTarget': array([20., 29.]), 'currentState': array([25.81420517, 15.47290802,  2.07581551]), 'targetState': array([20, 29], dtype=int32), 'currentDistance': 14.723695161751062}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9307999870390151
{'scaleFactor': 20, 'currentTarget': array([20., 29.]), 'previousTarget': array([20., 29.]), 'currentState': array([20.3608453 , 28.29632632,  2.41844383]), 'targetState': array([20, 29], dtype=int32), 'currentDistance': 0.7908008460975805}
episode index:3835
target Thresh 32.0
target distance 21.0
model initialize at round 3835
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 27.]), 'previousTarget': array([23., 27.]), 'currentState': array([15.47970853,  6.7218739 ,  1.6882863 ]), 'targetState': array([23, 27], dtype=int32), 'currentDistance': 21.627694789611738}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9307907425831287
{'scaleFactor': 20, 'currentTarget': array([23., 27.]), 'previousTarget': array([23., 27.]), 'currentState': array([23.07838026, 27.12675673,  1.49407511]), 'targetState': array([23, 27], dtype=int32), 'currentDistance': 0.14903265860683806}
episode index:3836
target Thresh 32.0
target distance 22.0
model initialize at round 3836
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  6.]), 'previousTarget': array([19.,  6.]), 'currentState': array([19.87445266, 26.56204289,  4.24875879]), 'targetState': array([19,  6], dtype=int32), 'currentDistance': 20.580628644854382}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9307838599488899
{'scaleFactor': 20, 'currentTarget': array([19.,  6.]), 'previousTarget': array([19.,  6.]), 'currentState': array([19.01024731,  6.70262375,  4.92592005]), 'targetState': array([19,  6], dtype=int32), 'currentDistance': 0.7026984730482017}
episode index:3837
target Thresh 32.0
target distance 11.0
model initialize at round 3837
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([16.31716911, 21.0475942 ,  5.94507689]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 15.456446551943328}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9307817653252523
{'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([26.13950092,  8.58345317,  5.32729749]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 0.4392855163724452}
episode index:3838
target Thresh 32.0
target distance 3.0
model initialize at round 3838
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([16.84995619, 16.36578852,  4.1710657 ]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 1.6086652274452085}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9307971907575717
{'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([15.73432211, 14.70691483,  4.06859265]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 0.3955801552789667}
episode index:3839
target Thresh 32.0
target distance 9.0
model initialize at round 3839
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 18.]), 'previousTarget': array([12., 18.]), 'currentState': array([ 4.01496997, 10.36458929,  1.1860373 ]), 'targetState': array([12, 18], dtype=int32), 'currentDistance': 11.048085866075127}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9308024493146402
{'scaleFactor': 20, 'currentTarget': array([12., 18.]), 'previousTarget': array([12., 18.]), 'currentState': array([11.09428207, 17.30465207,  0.76792249]), 'targetState': array([12, 18], dtype=int32), 'currentDistance': 1.141855375459645}
episode index:3840
target Thresh 32.0
target distance 11.0
model initialize at round 3840
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 14.]), 'previousTarget': array([18., 14.]), 'currentState': array([21.02065246,  4.36867325,  1.18187928]), 'targetState': array([18, 14], dtype=int32), 'currentDistance': 10.093898956722324}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9308077051335898
{'scaleFactor': 20, 'currentTarget': array([18., 14.]), 'previousTarget': array([18., 14.]), 'currentState': array([18.0515663 , 13.68607359,  2.15740969]), 'targetState': array([18, 14], dtype=int32), 'currentDistance': 0.31813342360919006}
episode index:3841
target Thresh 32.0
target distance 8.0
model initialize at round 3841
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  8.]), 'previousTarget': array([21.,  8.]), 'currentState': array([11.65009713,  6.00506236,  1.49159241]), 'targetState': array([21,  8], dtype=int32), 'currentDistance': 9.560358769093709}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9308129582165587
{'scaleFactor': 20, 'currentTarget': array([21.,  8.]), 'previousTarget': array([21.,  8.]), 'currentState': array([20.89023463,  7.88338524,  0.10787656]), 'targetState': array([21,  8], dtype=int32), 'currentDistance': 0.16014817596129427}
episode index:3842
target Thresh 32.0
target distance 22.0
model initialize at round 3842
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 25.]), 'previousTarget': array([16., 25.]), 'currentState': array([7.26627805, 4.79424623, 0.65782976]), 'targetState': array([16, 25], dtype=int32), 'currentDistance': 22.012505188530188}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9308037272241159
{'scaleFactor': 20, 'currentTarget': array([16., 25.]), 'previousTarget': array([16., 25.]), 'currentState': array([15.86208106, 24.86604543,  1.5610955 ]), 'targetState': array([16, 25], dtype=int32), 'currentDistance': 0.19226404095971347}
episode index:3843
target Thresh 32.0
target distance 10.0
model initialize at round 3843
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 22.]), 'previousTarget': array([26., 22.]), 'currentState': array([22.00728106, 13.49305382,  1.76969866]), 'targetState': array([26, 22], dtype=int32), 'currentDistance': 9.397336742674147}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.930808978608787
{'scaleFactor': 20, 'currentTarget': array([26., 22.]), 'previousTarget': array([26., 22.]), 'currentState': array([26.22979682, 22.41732671,  1.08176039]), 'targetState': array([26, 22], dtype=int32), 'currentDistance': 0.47641176018809894}
episode index:3844
target Thresh 32.0
target distance 14.0
model initialize at round 3844
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 24.]), 'previousTarget': array([19., 24.]), 'currentState': array([ 7.07178213, 11.67576332,  0.81978953]), 'targetState': array([19, 24], dtype=int32), 'currentDistance': 17.15136120966383}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.930806881265697
{'scaleFactor': 20, 'currentTarget': array([19., 24.]), 'previousTarget': array([19., 24.]), 'currentState': array([18.10047443, 23.24267303,  0.83023769]), 'targetState': array([19, 24], dtype=int32), 'currentDistance': 1.1758785635065865}
episode index:3845
target Thresh 32.0
target distance 22.0
model initialize at round 3845
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 20.]), 'previousTarget': array([ 4., 20.]), 'currentState': array([24.14704979, 12.16477765,  2.39188242]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 21.616991563482983}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9307976590537868
{'scaleFactor': 20, 'currentTarget': array([ 4., 20.]), 'previousTarget': array([ 4., 20.]), 'currentState': array([ 3.74304129, 20.19092003,  2.76341736]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 0.3201222227979773}
episode index:3846
target Thresh 32.0
target distance 6.0
model initialize at round 3846
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([10.87259655, 10.75046026,  2.96832943]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 8.734873984832694}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9308054049209419
{'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([ 6.44217054, 17.17034479,  2.26144405]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 0.9401290056431714}
episode index:3847
target Thresh 32.0
target distance 7.0
model initialize at round 3847
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([22.86318215,  9.67739946,  2.66218162]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 7.366935397127576}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9308131467621786
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.67252598,  6.87276883,  3.37141369]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.35132179852110684}
episode index:3848
target Thresh 32.0
target distance 9.0
model initialize at round 3848
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 14.]), 'previousTarget': array([15., 14.]), 'currentState': array([4.47539047, 9.7127089 , 1.69430625]), 'targetState': array([15, 14], dtype=int32), 'currentDistance': 11.364342072150283}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.930815918132051
{'scaleFactor': 20, 'currentTarget': array([15., 14.]), 'previousTarget': array([15., 14.]), 'currentState': array([15.10845446, 13.88441324,  0.42348467]), 'targetState': array([15, 14], dtype=int32), 'currentDistance': 0.15850131845489843}
episode index:3849
target Thresh 32.0
target distance 8.0
model initialize at round 3849
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  8.]), 'previousTarget': array([12.,  8.]), 'currentState': array([14.00905381, 14.4665347 ,  4.8787601 ]), 'targetState': array([12,  8], dtype=int32), 'currentDistance': 6.771437673472212}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9308261734779907
{'scaleFactor': 20, 'currentTarget': array([12.,  8.]), 'previousTarget': array([12.,  8.]), 'currentState': array([12.21661356,  8.82156783,  4.44105646]), 'targetState': array([12,  8], dtype=int32), 'currentDistance': 0.8496441220332365}
episode index:3850
target Thresh 32.0
target distance 21.0
model initialize at round 3850
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 29.]), 'previousTarget': array([ 3., 29.]), 'currentState': array([24.42656621, 11.62801393,  2.32454056]), 'targetState': array([ 3, 29], dtype=int32), 'currentDistance': 27.584119336242704}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9308100528961394
{'scaleFactor': 20, 'currentTarget': array([ 3., 29.]), 'previousTarget': array([ 3., 29.]), 'currentState': array([ 2.78697333, 29.30807715,  2.37342796]), 'targetState': array([ 3, 29], dtype=int32), 'currentDistance': 0.3745555929361944}
episode index:3851
target Thresh 32.0
target distance 16.0
model initialize at round 3851
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([10.82684169,  4.79331543,  2.93107748]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 18.511503490608806}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9308055635904767
{'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([ 4.16989073, 21.15812876,  2.24460765]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 0.8588422677277876}
episode index:3852
target Thresh 32.0
target distance 16.0
model initialize at round 3852
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 5.]), 'previousTarget': array([6., 5.]), 'currentState': array([22.54105897, 19.40637417,  4.02969217]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 21.935137309205768}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9307963584751557
{'scaleFactor': 20, 'currentTarget': array([6., 5.]), 'previousTarget': array([6., 5.]), 'currentState': array([5.90747591, 5.03928952, 3.86540142]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 0.10052051436395593}
episode index:3853
target Thresh 32.0
target distance 9.0
model initialize at round 3853
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 26.]), 'previousTarget': array([16., 26.]), 'currentState': array([ 7.51033286, 21.60372955,  0.25271225]), 'targetState': array([16, 26], dtype=int32), 'currentDistance': 9.560420603575583}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9308015981459977
{'scaleFactor': 20, 'currentTarget': array([16., 26.]), 'previousTarget': array([16., 26.]), 'currentState': array([16.38572672, 26.15641577,  0.42510004]), 'targetState': array([16, 26], dtype=int32), 'currentDistance': 0.41623430066785555}
episode index:3854
target Thresh 32.0
target distance 14.0
model initialize at round 3854
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  6.]), 'previousTarget': array([25.,  6.]), 'currentState': array([18.03763837, 21.3250263 ,  6.17962605]), 'targetState': array([25,  6], dtype=int32), 'currentDistance': 16.8324362691981}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9307971145271489
{'scaleFactor': 20, 'currentTarget': array([25.,  6.]), 'previousTarget': array([25.,  6.]), 'currentState': array([25.27843714,  5.24017431,  4.90900017]), 'targetState': array([25,  6], dtype=int32), 'currentDistance': 0.8092356448122431}
episode index:3855
target Thresh 32.0
target distance 12.0
model initialize at round 3855
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([22.16628547, 27.46195355,  3.09907186]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 15.745234572619491}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9307950262439281
{'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([14.27175789, 14.26967214,  4.43956737]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 0.38285168667713343}
episode index:3856
target Thresh 32.0
target distance 4.0
model initialize at round 3856
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.42799658,  5.05689223,  4.75137877]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 2.134945798585185}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9308103762500872
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.01491346,  3.16938456,  5.28164262]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.1700398182841005}
episode index:3857
target Thresh 32.0
target distance 5.0
model initialize at round 3857
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 12.]), 'previousTarget': array([ 2., 12.]), 'currentState': array([ 7.81283737, 12.52633671,  4.2064414 ]), 'targetState': array([ 2, 12], dtype=int32), 'currentDistance': 5.836617910878762}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9308206117668705
{'scaleFactor': 20, 'currentTarget': array([ 2., 12.]), 'previousTarget': array([ 2., 12.]), 'currentState': array([ 2.14174315, 11.94765837,  2.9240191 ]), 'targetState': array([ 2, 12], dtype=int32), 'currentDistance': 0.15109853630604986}
episode index:3858
target Thresh 32.0
target distance 18.0
model initialize at round 3858
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 17.]), 'previousTarget': array([ 3., 17.]), 'currentState': array([19.07034194, 23.5785629 ,  3.63295484]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 17.364716518062774}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9308161278683778
{'scaleFactor': 20, 'currentTarget': array([ 3., 17.]), 'previousTarget': array([ 3., 17.]), 'currentState': array([ 2.413248  , 16.88298898,  3.35822307]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 0.5983055139651918}
episode index:3859
target Thresh 32.0
target distance 17.0
model initialize at round 3859
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([18.35494046,  8.35520023,  3.9389379 ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 15.414628256703837}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.930814036823445
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.60014263, 6.92560068, 3.22346541]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.4067200243075731}
episode index:3860
target Thresh 32.0
target distance 7.0
model initialize at round 3860
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 24.]), 'previousTarget': array([14., 24.]), 'currentState': array([ 8.93676932, 18.73585037,  0.78515202]), 'targetState': array([14, 24], dtype=int32), 'currentDistance': 7.303942512594275}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9308242634391344
{'scaleFactor': 20, 'currentTarget': array([14., 24.]), 'previousTarget': array([14., 24.]), 'currentState': array([13.08737021, 23.05754481,  0.91254334]), 'targetState': array([14, 24], dtype=int32), 'currentDistance': 1.3119126909946033}
episode index:3861
target Thresh 32.0
target distance 17.0
model initialize at round 3861
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 19.]), 'previousTarget': array([16., 19.]), 'currentState': array([6.29948177, 0.93054444, 0.32139939]), 'targetState': array([16, 19], dtype=int32), 'currentDistance': 20.508663491813156}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9308174166787951
{'scaleFactor': 20, 'currentTarget': array([16., 19.]), 'previousTarget': array([16., 19.]), 'currentState': array([15.62746484, 18.29296198,  1.57683393]), 'targetState': array([16, 19], dtype=int32), 'currentDistance': 0.7991778356892953}
episode index:3862
target Thresh 32.0
target distance 13.0
model initialize at round 3862
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'previousTarget': array([11., 21.]), 'currentState': array([23.11448288, 12.43116998,  3.13487887]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 14.838650321833052}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9308177397259678
{'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'previousTarget': array([11., 21.]), 'currentState': array([11.76337682, 20.37577911,  2.79368   ]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 0.9861013581803914}
episode index:3863
target Thresh 32.0
target distance 2.0
model initialize at round 3863
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 14.]), 'previousTarget': array([25., 14.]), 'currentState': array([22.95202997, 14.31732847,  5.69324243]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 2.072408888970328}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9308330560459145
{'scaleFactor': 20, 'currentTarget': array([25., 14.]), 'previousTarget': array([25., 14.]), 'currentState': array([24.86504184, 14.04359367,  0.3147372 ]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 0.14182423345572823}
episode index:3864
target Thresh 32.0
target distance 16.0
model initialize at round 3864
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 23.]), 'previousTarget': array([21., 23.]), 'currentState': array([23.93779718,  8.96692617,  1.91656399]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 14.337287516135365}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9308333748795138
{'scaleFactor': 20, 'currentTarget': array([21., 23.]), 'previousTarget': array([21., 23.]), 'currentState': array([21.11657766, 22.62968501,  2.00633162]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 0.38823129710326426}
episode index:3865
target Thresh 32.0
target distance 7.0
model initialize at round 3865
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 20.]), 'previousTarget': array([15., 20.]), 'currentState': array([17.48277579, 27.79609299,  5.76591045]), 'targetState': array([15, 20], dtype=int32), 'currentDistance': 8.181884959474925}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9308410734400726
{'scaleFactor': 20, 'currentTarget': array([15., 20.]), 'previousTarget': array([15., 20.]), 'currentState': array([15.30832407, 20.72650735,  4.55566317]), 'targetState': array([15, 20], dtype=int32), 'currentDistance': 0.7892253588825671}
episode index:3866
target Thresh 32.0
target distance 23.0
model initialize at round 3866
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  5.]), 'previousTarget': array([17.,  5.]), 'currentState': array([11.82865406, 26.53517219,  4.21720648]), 'targetState': array([17,  5], dtype=int32), 'currentDistance': 22.147380428195703}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9308318924679543
{'scaleFactor': 20, 'currentTarget': array([17.,  5.]), 'previousTarget': array([17.,  5.]), 'currentState': array([16.85256747,  5.33178373,  5.18560438]), 'targetState': array([17,  5], dtype=int32), 'currentDistance': 0.3630658299783578}
episode index:3867
target Thresh 32.0
target distance 18.0
model initialize at round 3867
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'previousTarget': array([25.,  3.]), 'currentState': array([24.32850943, 20.8037671 ,  4.26845765]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 17.816425637084418}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9308274160861072
{'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'previousTarget': array([25.,  3.]), 'currentState': array([24.94923621,  2.90646684,  4.88304352]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 0.10642093481274466}
episode index:3868
target Thresh 32.0
target distance 11.0
model initialize at round 3868
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 12.]), 'previousTarget': array([ 3., 12.]), 'currentState': array([12.75345351,  2.86928787,  2.8683033 ]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 13.360380207613003}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9308277360478081
{'scaleFactor': 20, 'currentTarget': array([ 3., 12.]), 'previousTarget': array([ 3., 12.]), 'currentState': array([ 2.84739846, 12.40484036,  2.22125943]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 0.4326464472102161}
episode index:3869
target Thresh 32.0
target distance 1.0
model initialize at round 3869
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 20.]), 'previousTarget': array([ 6., 20.]), 'currentState': array([ 6.65724823, 19.50821524,  1.21986526]), 'targetState': array([ 6, 20], dtype=int32), 'currentDistance': 0.8208699592288803}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9308456100178216
{'scaleFactor': 20, 'currentTarget': array([ 6., 20.]), 'previousTarget': array([ 6., 20.]), 'currentState': array([ 6.65724823, 19.50821524,  1.21986526]), 'targetState': array([ 6, 20], dtype=int32), 'currentDistance': 0.8208699592288803}
episode index:3870
target Thresh 32.0
target distance 16.0
model initialize at round 3870
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  7.]), 'previousTarget': array([26.,  7.]), 'currentState': array([11.21310022, 10.16652301,  6.03901261]), 'targetState': array([26,  7], dtype=int32), 'currentDistance': 15.122145120386076}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9308435172987335
{'scaleFactor': 20, 'currentTarget': array([26.,  7.]), 'previousTarget': array([26.,  7.]), 'currentState': array([26.79983146,  6.80531394,  5.85950974]), 'targetState': array([26,  7], dtype=int32), 'currentDistance': 0.8231846833795384}
episode index:3871
target Thresh 32.0
target distance 17.0
model initialize at round 3871
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([14.66204628,  5.74555954,  1.85309565]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 15.313003425465261}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9308414256605954
{'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([16.1281401 , 21.56775591,  1.3234625 ]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 0.5820366481613408}
episode index:3872
target Thresh 32.0
target distance 4.0
model initialize at round 3872
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 25.]), 'previousTarget': array([25., 25.]), 'currentState': array([22.76139874, 22.76785769,  0.68178701]), 'targetState': array([25, 25], dtype=int32), 'currentDistance': 3.1612963928061784}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9308567002731281
{'scaleFactor': 20, 'currentTarget': array([25., 25.]), 'previousTarget': array([25., 25.]), 'currentState': array([24.07958374, 24.2596203 ,  1.01573712]), 'targetState': array([25, 25], dtype=int32), 'currentDistance': 1.181240107142528}
episode index:3873
target Thresh 32.0
target distance 10.0
model initialize at round 3873
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 17.]), 'previousTarget': array([18., 17.]), 'currentState': array([7.19259343, 9.47664576, 1.06116724]), 'targetState': array([18, 17], dtype=int32), 'currentDistance': 13.168177388766603}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9308570122627083
{'scaleFactor': 20, 'currentTarget': array([18., 17.]), 'previousTarget': array([18., 17.]), 'currentState': array([18.51548282, 17.12347222,  0.39346443]), 'targetState': array([18, 17], dtype=int32), 'currentDistance': 0.5300640736263746}
episode index:3874
target Thresh 32.0
target distance 15.0
model initialize at round 3874
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([14.4439653 , 19.13550459,  4.73372561]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 15.188219506125026}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9308549187613316
{'scaleFactor': 20, 'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([19.86091217,  4.41500629,  4.83587222]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 0.6013011482527882}
episode index:3875
target Thresh 32.0
target distance 11.0
model initialize at round 3875
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([ 1.44812699, 20.34878709,  4.54890633]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 14.083169819215613}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9308552310495529
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([10.69055021, 10.40362642,  5.53000849]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.5085995023185467}
episode index:3876
target Thresh 32.0
target distance 3.0
model initialize at round 3876
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 24.]), 'previousTarget': array([11., 24.]), 'currentState': array([ 9.99342684, 25.3000253 ,  5.68965554]), 'targetState': array([11, 24], dtype=int32), 'currentDistance': 1.6441579316453072}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9308704863420343
{'scaleFactor': 20, 'currentTarget': array([11., 24.]), 'previousTarget': array([11., 24.]), 'currentState': array([11.26309388, 23.78888142,  5.12705362]), 'targetState': array([11, 24], dtype=int32), 'currentDistance': 0.3373269074622674}
episode index:3877
target Thresh 32.0
target distance 21.0
model initialize at round 3877
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  6.]), 'previousTarget': array([27.,  6.]), 'currentState': array([ 7.55964342, 13.3448981 ,  0.59772044]), 'targetState': array([27,  6], dtype=int32), 'currentDistance': 20.781602250192226}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9308613238273145
{'scaleFactor': 20, 'currentTarget': array([27.,  6.]), 'previousTarget': array([27.,  6.]), 'currentState': array([27.71791009,  5.5834369 ,  5.60047612]), 'targetState': array([27,  6], dtype=int32), 'currentDistance': 0.8300118769464702}
episode index:3878
target Thresh 32.0
target distance 21.0
model initialize at round 3878
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([ 3.12432302, 25.57086711,  6.13505626]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 27.20146394191996}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9308453105478459
{'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([23.5279909 ,  6.72373985,  5.39241051]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 0.5958976931524584}
episode index:3879
target Thresh 32.0
target distance 4.0
model initialize at round 3879
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 23.]), 'previousTarget': array([ 7., 23.]), 'currentState': array([ 4.74361257, 21.6081058 ,  1.0288636 ]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 2.651160817969135}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9308605566018284
{'scaleFactor': 20, 'currentTarget': array([ 7., 23.]), 'previousTarget': array([ 7., 23.]), 'currentState': array([ 6.35321018, 22.68131467,  0.13836026]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 0.7210391191380742}
episode index:3880
target Thresh 32.0
target distance 11.0
model initialize at round 3880
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 23.]), 'previousTarget': array([13., 23.]), 'currentState': array([14.80752804, 13.93839411,  2.07056141]), 'targetState': array([13, 23], dtype=int32), 'currentDistance': 9.240122234196283}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9308657432787927
{'scaleFactor': 20, 'currentTarget': array([13., 23.]), 'previousTarget': array([13., 23.]), 'currentState': array([12.67104568, 23.594832  ,  1.56086987]), 'targetState': array([13, 23], dtype=int32), 'currentDistance': 0.6797323443301456}
episode index:3881
target Thresh 32.0
target distance 14.0
model initialize at round 3881
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 18.]), 'previousTarget': array([ 3., 18.]), 'currentState': array([16.94700792, 23.68213552,  2.61228877]), 'targetState': array([ 3, 18], dtype=int32), 'currentDistance': 15.060069520863909}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.930863651303303
{'scaleFactor': 20, 'currentTarget': array([ 3., 18.]), 'previousTarget': array([ 3., 18.]), 'currentState': array([ 2.55496857, 17.58422035,  3.36732851]), 'targetState': array([ 3, 18], dtype=int32), 'currentDistance': 0.6090366884539393}
episode index:3882
target Thresh 32.0
target distance 14.0
model initialize at round 3882
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 21.]), 'previousTarget': array([23., 21.]), 'currentState': array([10.82114314, 19.68293625,  6.22990597]), 'targetState': array([23, 21], dtype=int32), 'currentDistance': 12.249865768062207}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9308663854001604
{'scaleFactor': 20, 'currentTarget': array([23., 21.]), 'previousTarget': array([23., 21.]), 'currentState': array([22.7377942 , 20.88458917,  0.29405494]), 'targetState': array([23, 21], dtype=int32), 'currentDistance': 0.28648130509254693}
episode index:3883
target Thresh 32.0
target distance 23.0
model initialize at round 3883
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 20.]), 'previousTarget': array([25., 20.]), 'currentState': array([ 3.92671641, 22.21377873,  5.95795507]), 'targetState': array([25, 20], dtype=int32), 'currentDistance': 21.189244858640578}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9308572380955411
{'scaleFactor': 20, 'currentTarget': array([25., 20.]), 'previousTarget': array([25., 20.]), 'currentState': array([25.71343345, 19.88625932,  5.89589129]), 'targetState': array([25, 20], dtype=int32), 'currentDistance': 0.7224432386415119}
episode index:3884
target Thresh 32.0
target distance 8.0
model initialize at round 3884
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 18.]), 'previousTarget': array([ 3., 18.]), 'currentState': array([ 9.37378187, 11.43336205,  1.87116027]), 'targetState': array([ 3, 18], dtype=int32), 'currentDistance': 9.151274731343497}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9308624202864818
{'scaleFactor': 20, 'currentTarget': array([ 3., 18.]), 'previousTarget': array([ 3., 18.]), 'currentState': array([ 2.6052898 , 18.66548859,  2.335368  ]), 'targetState': array([ 3, 18], dtype=int32), 'currentDistance': 0.7737384596711336}
episode index:3885
target Thresh 32.0
target distance 21.0
model initialize at round 3885
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 25.]), 'previousTarget': array([16., 25.]), 'currentState': array([26.70495009,  5.5282125 ,  2.14859289]), 'targetState': array([16, 25], dtype=int32), 'currentDistance': 22.220406495076325}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9308532787100464
{'scaleFactor': 20, 'currentTarget': array([16., 25.]), 'previousTarget': array([16., 25.]), 'currentState': array([16.22107448, 24.8602685 ,  2.14604696]), 'targetState': array([16, 25], dtype=int32), 'currentDistance': 0.26153167752292517}
episode index:3886
target Thresh 32.0
target distance 15.0
model initialize at round 3886
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 28.]), 'previousTarget': array([ 7., 28.]), 'currentState': array([ 8.38668611, 13.95367171,  1.61244648]), 'targetState': array([ 7, 28], dtype=int32), 'currentDistance': 14.114610757040673}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9308535905364413
{'scaleFactor': 20, 'currentTarget': array([ 7., 28.]), 'previousTarget': array([ 7., 28.]), 'currentState': array([ 7.09277224, 27.88507211,  1.70620028]), 'targetState': array([ 7, 28], dtype=int32), 'currentDistance': 0.14769938854539513}
episode index:3887
target Thresh 32.0
target distance 9.0
model initialize at round 3887
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([21.14195322, 10.76374318,  4.44815472]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 12.19046499721261}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9308563237048736
{'scaleFactor': 20, 'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([11.2890989 ,  4.22547887,  3.90877415]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 0.36663182084356094}
episode index:3888
target Thresh 32.0
target distance 22.0
model initialize at round 3888
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 24.]), 'previousTarget': array([25., 24.]), 'currentState': array([11.75921253,  3.13702882,  1.38980293]), 'targetState': array([25, 24], dtype=int32), 'currentDistance': 24.70995789429481}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9308448885153675
{'scaleFactor': 20, 'currentTarget': array([25., 24.]), 'previousTarget': array([25., 24.]), 'currentState': array([24.39402755, 23.36265303,  1.26661575]), 'targetState': array([25, 24], dtype=int32), 'currentDistance': 0.8794394583960452}
episode index:3889
target Thresh 32.0
target distance 10.0
model initialize at round 3889
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 13.]), 'previousTarget': array([27., 13.]), 'currentState': array([22.34625498, 21.55401173,  5.18872023]), 'targetState': array([27, 13], dtype=int32), 'currentDistance': 9.737990515280094}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9308500672200937
{'scaleFactor': 20, 'currentTarget': array([27., 13.]), 'previousTarget': array([27., 13.]), 'currentState': array([26.98803802, 12.71922625,  5.11845696]), 'targetState': array([27, 13], dtype=int32), 'currentDistance': 0.2810284432014985}
episode index:3890
target Thresh 32.0
target distance 14.0
model initialize at round 3890
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 14.]), 'previousTarget': array([24., 14.]), 'currentState': array([12.64713604, 26.89593197,  5.46383065]), 'targetState': array([24, 14], dtype=int32), 'currentDistance': 17.181169386963745}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9308479841122057
{'scaleFactor': 20, 'currentTarget': array([24., 14.]), 'previousTarget': array([24., 14.]), 'currentState': array([23.30068429, 14.99865271,  5.52317224]), 'targetState': array([24, 14], dtype=int32), 'currentDistance': 1.2191594173698912}
episode index:3891
target Thresh 32.0
target distance 17.0
model initialize at round 3891
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 13.]), 'previousTarget': array([ 3., 13.]), 'currentState': array([18.20022696,  7.07782829,  2.30677593]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 16.31315473858244}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9308459020747739
{'scaleFactor': 20, 'currentTarget': array([ 3., 13.]), 'previousTarget': array([ 3., 13.]), 'currentState': array([ 3.39518789, 12.75605943,  3.08192647]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 0.4644141152758429}
episode index:3892
target Thresh 32.0
target distance 15.0
model initialize at round 3892
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 27.]), 'previousTarget': array([ 9., 27.]), 'currentState': array([24.59394539, 12.54011683,  1.33671206]), 'targetState': array([ 9, 27], dtype=int32), 'currentDistance': 21.266390242709676}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.930836781178854
{'scaleFactor': 20, 'currentTarget': array([ 9., 27.]), 'previousTarget': array([ 9., 27.]), 'currentState': array([ 9.01160381, 27.1792013 ,  2.271176  ]), 'targetState': array([ 9, 27], dtype=int32), 'currentDistance': 0.1795766006519836}
episode index:3893
target Thresh 32.0
target distance 3.0
model initialize at round 3893
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 6.]), 'previousTarget': array([5., 6.]), 'currentState': array([2.39298833, 9.63644378, 0.32511163]), 'targetState': array([5, 6], dtype=int32), 'currentDistance': 4.474397530020276}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9308494322365893
{'scaleFactor': 20, 'currentTarget': array([5., 6.]), 'previousTarget': array([5., 6.]), 'currentState': array([4.56940984, 6.91015135, 5.76285762]), 'targetState': array([5, 6], dtype=int32), 'currentDistance': 1.0068680984555114}
episode index:3894
target Thresh 32.0
target distance 5.0
model initialize at round 3894
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([5.21490063, 8.3080034 , 2.99662828]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 3.2296210575296205}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9308620767982745
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.28365088, 7.92238101, 2.93614483]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.7205420001839207}
episode index:3895
target Thresh 32.0
target distance 7.0
model initialize at round 3895
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 12.]), 'previousTarget': array([26., 12.]), 'currentState': array([19.96861769, 17.62268224,  0.05070322]), 'targetState': array([26, 12], dtype=int32), 'currentDistance': 8.245733940256532}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9308697087113139
{'scaleFactor': 20, 'currentTarget': array([26., 12.]), 'previousTarget': array([26., 12.]), 'currentState': array([25.56551479, 12.30863166,  5.97855985]), 'targetState': array([26, 12], dtype=int32), 'currentDistance': 0.5329454939750565}
episode index:3896
target Thresh 32.0
target distance 1.0
model initialize at round 3896
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 24.]), 'previousTarget': array([10., 24.]), 'currentState': array([ 9.63135047, 25.22420162,  3.12872553]), 'targetState': array([10, 24], dtype=int32), 'currentDistance': 1.2785038422772055}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9308823415805182
{'scaleFactor': 20, 'currentTarget': array([10., 24.]), 'previousTarget': array([10., 24.]), 'currentState': array([10.22703986, 23.1025273 ,  0.39616841]), 'targetState': array([10, 24], dtype=int32), 'currentDistance': 0.9257452881624886}
episode index:3897
target Thresh 32.0
target distance 21.0
model initialize at round 3897
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 28.]), 'previousTarget': array([23., 28.]), 'currentState': array([ 3.30128245, 16.34626437,  0.1712501 ]), 'targetState': array([23, 28], dtype=int32), 'currentDistance': 22.887748410736418}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9308732230357973
{'scaleFactor': 20, 'currentTarget': array([23., 28.]), 'previousTarget': array([23., 28.]), 'currentState': array([22.0025652 , 27.63261263,  0.73269828]), 'targetState': array([23, 28], dtype=int32), 'currentDistance': 1.062943867511901}
episode index:3898
target Thresh 32.0
target distance 19.0
model initialize at round 3898
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 28.]), 'previousTarget': array([18., 28.]), 'currentState': array([14.18051365, 10.19948973,  1.80337113]), 'targetState': array([18, 28], dtype=int32), 'currentDistance': 18.20567608883772}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9308687716442733
{'scaleFactor': 20, 'currentTarget': array([18., 28.]), 'previousTarget': array([18., 28.]), 'currentState': array([17.84436903, 27.71949626,  1.60836571]), 'targetState': array([18, 28], dtype=int32), 'currentDistance': 0.3207855109975653}
episode index:3899
target Thresh 32.0
target distance 7.0
model initialize at round 3899
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([19.5992451 , 10.52421675,  5.5899387 ]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 8.257055866103594}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9308763940130823
{'scaleFactor': 20, 'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([22.85471641,  3.31558342,  5.38898629]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 0.3474193664686289}
episode index:3900
target Thresh 32.0
target distance 24.0
model initialize at round 3900
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 22.]), 'previousTarget': array([26., 22.]), 'currentState': array([3.96026773, 8.8804249 , 0.2723267 ]), 'targetState': array([26, 22], dtype=int32), 'currentDistance': 25.649036030786537}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9308627166557345
{'scaleFactor': 20, 'currentTarget': array([26., 22.]), 'previousTarget': array([26., 22.]), 'currentState': array([26.08040355, 22.11433286,  0.71684607]), 'targetState': array([26, 22], dtype=int32), 'currentDistance': 0.13977386497735964}
episode index:3901
target Thresh 32.0
target distance 17.0
model initialize at round 3901
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'previousTarget': array([10., 12.]), 'currentState': array([24.45639852, 27.74779761,  4.01780906]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 21.377104283031723}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9308536124880263
{'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'previousTarget': array([10., 12.]), 'currentState': array([ 9.5150823 , 11.67357995,  3.74732042]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 0.5845470278007686}
episode index:3902
target Thresh 32.0
target distance 5.0
model initialize at round 3902
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 24.]), 'previousTarget': array([17., 24.]), 'currentState': array([15.95424734, 20.31863166,  1.23129058]), 'targetState': array([17, 24], dtype=int32), 'currentDistance': 3.8270186162184756}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9308662300610503
{'scaleFactor': 20, 'currentTarget': array([17., 24.]), 'previousTarget': array([17., 24.]), 'currentState': array([17.15256162, 24.134475  ,  1.2647611 ]), 'targetState': array([17, 24], dtype=int32), 'currentDistance': 0.20336807008264227}
episode index:3903
target Thresh 32.0
target distance 18.0
model initialize at round 3903
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([20.24467647, 25.04885947,  5.15852749]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 16.738548087256145}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9308641497496688
{'scaleFactor': 20, 'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([24.8904342 ,  9.80027894,  5.32362576]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 0.8077444220124617}
episode index:3904
target Thresh 32.0
target distance 8.0
model initialize at round 3904
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 16.]), 'previousTarget': array([15., 16.]), 'currentState': array([9.38998157, 8.9488625 , 1.60898202]), 'targetState': array([15, 16], dtype=int32), 'currentDistance': 9.010596363603703}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9308717635423065
{'scaleFactor': 20, 'currentTarget': array([15., 16.]), 'previousTarget': array([15., 16.]), 'currentState': array([14.14704972, 15.18297077,  1.05970446]), 'targetState': array([15, 16], dtype=int32), 'currentDistance': 1.1811269830297906}
episode index:3905
target Thresh 32.0
target distance 6.0
model initialize at round 3905
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 20.]), 'previousTarget': array([20., 20.]), 'currentState': array([20.84605009, 15.85224325,  2.31867218]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 4.23316510336904}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9308843667774468
{'scaleFactor': 20, 'currentTarget': array([20., 20.]), 'previousTarget': array([20., 20.]), 'currentState': array([19.93997278, 19.6480516 ,  1.97320157]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 0.35703073185581147}
episode index:3906
target Thresh 32.0
target distance 11.0
model initialize at round 3906
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 16.]), 'previousTarget': array([ 5., 16.]), 'currentState': array([14.48438798, 22.73164765,  3.70184755]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 11.63050709799471}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9308870787770944
{'scaleFactor': 20, 'currentTarget': array([ 5., 16.]), 'previousTarget': array([ 5., 16.]), 'currentState': array([ 4.66673827, 15.86646395,  3.58870937]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 0.35901985875801024}
episode index:3907
target Thresh 32.0
target distance 13.0
model initialize at round 3907
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 23.]), 'previousTarget': array([21., 23.]), 'currentState': array([ 9.96104372, 20.39164766,  0.17059262]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 11.342929853598696}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.93088978938882
{'scaleFactor': 20, 'currentTarget': array([21., 23.]), 'previousTarget': array([21., 23.]), 'currentState': array([21.54901378, 23.25678174,  0.05983411]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 0.606096516572653}
episode index:3908
target Thresh 32.0
target distance 19.0
model initialize at round 3908
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  6.]), 'previousTarget': array([26.,  6.]), 'currentState': array([ 8.81463942, 15.60534041,  5.40031842]), 'targetState': array([26,  6], dtype=int32), 'currentDistance': 19.68753876627151}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9308830081879043
{'scaleFactor': 20, 'currentTarget': array([26.,  6.]), 'previousTarget': array([26.,  6.]), 'currentState': array([26.18338163,  5.83175544,  5.75427317]), 'targetState': array([26,  6], dtype=int32), 'currentDistance': 0.24886754283070298}
episode index:3909
target Thresh 32.0
target distance 13.0
model initialize at round 3909
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([21.87346109,  9.68786391,  1.64935213]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 12.041293378530158}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9308857184541991
{'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([25.87271168, 20.86488463,  1.41786006]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 0.1856299494601658}
episode index:3910
target Thresh 32.0
target distance 7.0
model initialize at round 3910
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 25.]), 'previousTarget': array([10., 25.]), 'currentState': array([16.93547048, 25.68173245,  2.61914837]), 'targetState': array([10, 25], dtype=int32), 'currentDistance': 6.968895885788508}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9308933150513726
{'scaleFactor': 20, 'currentTarget': array([10., 25.]), 'previousTarget': array([10., 25.]), 'currentState': array([ 9.21256893, 24.77023747,  2.91542161]), 'targetState': array([10, 25], dtype=int32), 'currentDistance': 0.8202673364530967}
episode index:3911
target Thresh 32.0
target distance 11.0
model initialize at round 3911
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 29.]), 'previousTarget': array([ 3., 29.]), 'currentState': array([ 6.66396143, 19.82297974,  2.42980981]), 'targetState': array([ 3, 29], dtype=int32), 'currentDistance': 9.881412562221426}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9308984522535323
{'scaleFactor': 20, 'currentTarget': array([ 3., 29.]), 'previousTarget': array([ 3., 29.]), 'currentState': array([ 3.08121426, 29.0735153 ,  1.87434014]), 'targetState': array([ 3, 29], dtype=int32), 'currentDistance': 0.1095456770526826}
episode index:3912
target Thresh 32.0
target distance 16.0
model initialize at round 3912
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([23.42085063, 18.09801798,  4.70755676]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 16.823173633662567}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9308963684922684
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.57958472,  3.77011113,  4.59874413]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.9638410689232112}
episode index:3913
target Thresh 32.0
target distance 21.0
model initialize at round 3913
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 4.]), 'previousTarget': array([5., 4.]), 'currentState': array([12.24898838, 23.4938892 ,  5.25982749]), 'targetState': array([5, 4], dtype=int32), 'currentDistance': 20.798065980799027}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9308895942731873
{'scaleFactor': 20, 'currentTarget': array([5., 4.]), 'previousTarget': array([5., 4.]), 'currentState': array([5.51767416, 4.97007169, 4.71371407]), 'targetState': array([5, 4], dtype=int32), 'currentDistance': 1.0995570074958514}
episode index:3914
target Thresh 32.0
target distance 17.0
model initialize at round 3914
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([9.31728418, 4.97075009, 2.14897346]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 18.68206291902432}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9308828235147545
{'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([16.99184458, 22.88759159,  1.11671723]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 0.8876290593593448}
episode index:3915
target Thresh 32.0
target distance 26.0
model initialize at round 3915
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6.0586151 , 25.72261111]), 'previousTarget': array([ 7.12278353, 24.46989687]), 'currentState': array([26.1759126 ,  3.4674018 ,  3.09249902]), 'targetState': array([ 4, 28], dtype=int32), 'currentDistance': 30.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.9308603670208498
{'scaleFactor': 20, 'currentTarget': array([ 4., 28.]), 'previousTarget': array([ 4., 28.]), 'currentState': array([ 3.51195136, 28.4120402 ,  2.0766579 ]), 'targetState': array([ 4, 28], dtype=int32), 'currentDistance': 0.6387241982806079}
episode index:3916
target Thresh 32.0
target distance 4.0
model initialize at round 3916
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'previousTarget': array([10., 17.]), 'currentState': array([11.38431951, 21.957105  ,  5.87811071]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 5.146768939109393}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9308704356021568
{'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'previousTarget': array([10., 17.]), 'currentState': array([ 9.9895699 , 16.76872488,  3.93943715]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 0.23151018655906203}
episode index:3917
target Thresh 32.0
target distance 2.0
model initialize at round 3917
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 16.]), 'previousTarget': array([ 9., 16.]), 'currentState': array([ 6.53172233, 14.82252586,  1.6209538 ]), 'targetState': array([ 9, 16], dtype=int32), 'currentDistance': 2.7347467880029024}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9308830005752038
{'scaleFactor': 20, 'currentTarget': array([ 9., 16.]), 'previousTarget': array([ 9., 16.]), 'currentState': array([ 9.85428847, 15.63855056,  0.20155108]), 'targetState': array([ 9, 16], dtype=int32), 'currentDistance': 0.9276068608539632}
episode index:3918
target Thresh 32.0
target distance 7.0
model initialize at round 3918
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([ 9.78937676, 14.33026172,  5.59691024]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 7.454011300947164}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9308905823586753
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.40302616,  8.6663885 ,  5.35360008]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.5231889941632857}
episode index:3919
target Thresh 32.0
target distance 11.0
model initialize at round 3919
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([4.74767287, 8.66394685, 0.71129346]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 9.419671530499816}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9308957097738644
{'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([ 6.07491642, 18.37873682,  1.39614097]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 0.38607518174331423}
episode index:3920
target Thresh 32.0
target distance 18.0
model initialize at round 3920
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  5.]), 'previousTarget': array([23.,  5.]), 'currentState': array([22.57263704, 21.32377259,  4.19001162]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 16.32936589488434}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9308936309635236
{'scaleFactor': 20, 'currentTarget': array([23.,  5.]), 'previousTarget': array([23.,  5.]), 'currentState': array([23.02252694,  5.45035509,  5.00275489]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 0.45091814333541813}
episode index:3921
target Thresh 32.0
target distance 23.0
model initialize at round 3921
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([22.85544232, 29.23384593,  3.32867086]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 25.47522841429196}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9308800224454298
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.92438583,  5.13553078,  4.32727744]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.15519695704947373}
episode index:3922
target Thresh 32.0
target distance 5.0
model initialize at round 3922
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 23.]), 'previousTarget': array([21., 23.]), 'currentState': array([19.06290057, 19.39793875,  1.15134025]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 4.089889904300593}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9308925689602283
{'scaleFactor': 20, 'currentTarget': array([21., 23.]), 'previousTarget': array([21., 23.]), 'currentState': array([20.88835263, 22.84506304,  1.63571399]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 0.19097276091536936}
episode index:3923
target Thresh 32.0
target distance 8.0
model initialize at round 3923
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 4.]), 'previousTarget': array([6., 4.]), 'currentState': array([14.10276755,  5.32017057,  3.7634902 ]), 'targetState': array([6, 4], dtype=int32), 'currentDistance': 8.209609751590257}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9309001386444892
{'scaleFactor': 20, 'currentTarget': array([6., 4.]), 'previousTarget': array([6., 4.]), 'currentState': array([6.29609992, 4.04179816, 3.52258793]), 'targetState': array([6, 4], dtype=int32), 'currentDistance': 0.2990355337243972}
episode index:3924
target Thresh 32.0
target distance 13.0
model initialize at round 3924
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 12.]), 'previousTarget': array([16., 12.]), 'currentState': array([4.17007179, 6.20967769, 6.07522482]), 'targetState': array([16, 12], dtype=int32), 'currentDistance': 13.170992144790109}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9309004355130911
{'scaleFactor': 20, 'currentTarget': array([16., 12.]), 'previousTarget': array([16., 12.]), 'currentState': array([16.45157595, 12.47654733,  0.23487307]), 'targetState': array([16, 12], dtype=int32), 'currentDistance': 0.6565197643404376}
episode index:3925
target Thresh 32.0
target distance 13.0
model initialize at round 3925
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 24.]), 'previousTarget': array([13., 24.]), 'currentState': array([16.55892222, 11.86930024,  1.5888021 ]), 'targetState': array([13, 24], dtype=int32), 'currentDistance': 12.641985758259286}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9309031302950289
{'scaleFactor': 20, 'currentTarget': array([13., 24.]), 'previousTarget': array([13., 24.]), 'currentState': array([13.23704663, 23.32246837,  2.26024099]), 'targetState': array([13, 24], dtype=int32), 'currentDistance': 0.7178023462585945}
episode index:3926
target Thresh 32.0
target distance 21.0
model initialize at round 3926
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 29.]), 'previousTarget': array([11., 29.]), 'currentState': array([15.66645222,  9.54538976,  2.17364223]), 'targetState': array([11, 29], dtype=int32), 'currentDistance': 20.006439856110923}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9308963767795498
{'scaleFactor': 20, 'currentTarget': array([11., 29.]), 'previousTarget': array([11., 29.]), 'currentState': array([11.01976564, 28.93796805,  1.85363461]), 'targetState': array([11, 29], dtype=int32), 'currentDistance': 0.06510486448421185}
episode index:3927
target Thresh 32.0
target distance 6.0
model initialize at round 3927
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([13.70299965,  3.81350387,  1.61299837]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 4.3828027244224765}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9309089031602069
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.74404916,  7.62567377,  1.52560586]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.4534654947830666}
episode index:3928
target Thresh 32.0
target distance 12.0
model initialize at round 3928
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 14.]), 'previousTarget': array([26., 14.]), 'currentState': array([15.67351228, 14.66897244,  0.77559465]), 'targetState': array([26, 14], dtype=int32), 'currentDistance': 10.348133778129998}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9309140141672672
{'scaleFactor': 20, 'currentTarget': array([26., 14.]), 'previousTarget': array([26., 14.]), 'currentState': array([25.40330366, 14.08611177,  0.236613  ]), 'targetState': array([26, 14], dtype=int32), 'currentDistance': 0.6028778938896676}
episode index:3929
target Thresh 32.0
target distance 21.0
model initialize at round 3929
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 24.]), 'previousTarget': array([27., 24.]), 'currentState': array([ 6.54908428, 10.59087854,  0.22845292]), 'targetState': array([27, 24], dtype=int32), 'currentDistance': 24.454948211703996}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9309026835966688
{'scaleFactor': 20, 'currentTarget': array([27., 24.]), 'previousTarget': array([27., 24.]), 'currentState': array([26.12356688, 23.59873855,  0.80537985]), 'targetState': array([27, 24], dtype=int32), 'currentDistance': 0.9639220762001343}
episode index:3930
target Thresh 32.0
target distance 6.0
model initialize at round 3930
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 28.]), 'previousTarget': array([21., 28.]), 'currentState': array([23.27435362, 23.73268667,  2.5828954 ]), 'targetState': array([21, 28], dtype=int32), 'currentDistance': 4.835560717525197}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9309151988132559
{'scaleFactor': 20, 'currentTarget': array([21., 28.]), 'previousTarget': array([21., 28.]), 'currentState': array([21.17572395, 27.05034476,  2.27931976]), 'targetState': array([21, 28], dtype=int32), 'currentDistance': 0.9657763628285672}
episode index:3931
target Thresh 32.0
target distance 13.0
model initialize at round 3931
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 21.]), 'previousTarget': array([ 5., 21.]), 'currentState': array([16.29928478, 15.90827384,  3.12289333]), 'targetState': array([ 5, 21], dtype=int32), 'currentDistance': 12.393527013004135}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9309178857284612
{'scaleFactor': 20, 'currentTarget': array([ 5., 21.]), 'previousTarget': array([ 5., 21.]), 'currentState': array([ 5.40434125, 20.56171511,  2.92383062]), 'targetState': array([ 5, 21], dtype=int32), 'currentDistance': 0.5963098969685502}
episode index:3932
target Thresh 32.0
target distance 22.0
model initialize at round 3932
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 19.]), 'previousTarget': array([ 4., 19.]), 'currentState': array([24.10178729, 14.56604921,  3.0944621 ]), 'targetState': array([ 4, 19], dtype=int32), 'currentDistance': 20.584988992769148}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9309111387641288
{'scaleFactor': 20, 'currentTarget': array([ 4., 19.]), 'previousTarget': array([ 4., 19.]), 'currentState': array([ 4.63592163, 18.54857113,  3.15174896]), 'targetState': array([ 4, 19], dtype=int32), 'currentDistance': 0.7798617491609176}
episode index:3933
target Thresh 32.0
target distance 7.0
model initialize at round 3933
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'previousTarget': array([10., 17.]), 'currentState': array([ 3.94048437, 11.60433628,  0.03217095]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 8.11362537093082}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9309186844863545
{'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'previousTarget': array([10., 17.]), 'currentState': array([ 9.6106091 , 16.83808585,  0.36573603]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 0.42171253554302607}
episode index:3934
target Thresh 32.0
target distance 10.0
model initialize at round 3934
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 13.]), 'previousTarget': array([17., 13.]), 'currentState': array([ 8.8022727 , 19.14418251,  5.71793443]), 'targetState': array([17, 13], dtype=int32), 'currentDistance': 10.244691872516247}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.930923785214541
{'scaleFactor': 20, 'currentTarget': array([17., 13.]), 'previousTarget': array([17., 13.]), 'currentState': array([16.95349256, 13.45558834,  6.01951942]), 'targetState': array([17, 13], dtype=int32), 'currentDistance': 0.45795597424444223}
episode index:3935
target Thresh 32.0
target distance 8.0
model initialize at round 3935
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([22.39379238, 22.50247901,  3.84840202]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 9.85737146903414}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.930928883350894
{'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([15.95016291, 14.89132372,  3.84434868]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 0.11955864203539628}
episode index:3936
target Thresh 32.0
target distance 25.0
model initialize at round 3936
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([25.73282233, 25.56215632,  4.4986167 ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 26.32174634326339}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9309153177272335
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.14222396,  2.34637961,  4.47182634]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.3744415674841927}
episode index:3937
target Thresh 32.0
target distance 11.0
model initialize at round 3937
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 19.]), 'previousTarget': array([10., 19.]), 'currentState': array([14.08990765,  9.43712357,  1.15602565]), 'targetState': array([10, 19], dtype=int32), 'currentDistance': 10.400766808559457}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9309204154245856
{'scaleFactor': 20, 'currentTarget': array([10., 19.]), 'previousTarget': array([10., 19.]), 'currentState': array([10.17319713, 18.35530802,  2.31227688]), 'targetState': array([10, 19], dtype=int32), 'currentDistance': 0.6675514917983179}
episode index:3938
target Thresh 32.0
target distance 5.0
model initialize at round 3938
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 24.]), 'previousTarget': array([15., 24.]), 'currentState': array([12.26108275, 20.33837764,  1.52604518]), 'targetState': array([15, 24], dtype=int32), 'currentDistance': 4.572651966434612}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9309329007215076
{'scaleFactor': 20, 'currentTarget': array([15., 24.]), 'previousTarget': array([15., 24.]), 'currentState': array([14.43849402, 23.56334207,  1.13386057]), 'targetState': array([15, 24], dtype=int32), 'currentDistance': 0.7113080329983488}
episode index:3939
target Thresh 32.0
target distance 17.0
model initialize at round 3939
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 21.]), 'previousTarget': array([19., 21.]), 'currentState': array([ 3.33610761, 17.97666987,  0.3563978 ]), 'targetState': array([19, 21], dtype=int32), 'currentDistance': 15.95299501634411}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9309308224965599
{'scaleFactor': 20, 'currentTarget': array([19., 21.]), 'previousTarget': array([19., 21.]), 'currentState': array([18.99422628, 20.94516235,  0.18200145]), 'targetState': array([19, 21], dtype=int32), 'currentDistance': 0.05514076216850578}
episode index:3940
target Thresh 32.0
target distance 6.0
model initialize at round 3940
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 29.]), 'previousTarget': array([ 4., 29.]), 'currentState': array([ 2.60056522, 24.86803783,  1.60082507]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 4.3625140812406045}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9309432988166573
{'scaleFactor': 20, 'currentTarget': array([ 4., 29.]), 'previousTarget': array([ 4., 29.]), 'currentState': array([ 3.68544024, 28.66013484,  1.56096768]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 0.46309412699000774}
episode index:3941
target Thresh 32.0
target distance 9.0
model initialize at round 3941
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  2.]), 'previousTarget': array([20.,  2.]), 'currentState': array([20.68296298, 11.00486719,  5.27607733]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 9.030729289652218}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9309483842431118
{'scaleFactor': 20, 'currentTarget': array([20.,  2.]), 'previousTarget': array([20.,  2.]), 'currentState': array([20.10198445,  1.16646842,  4.38649151]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 0.8397474136816971}
episode index:3942
target Thresh 32.0
target distance 17.0
model initialize at round 3942
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 11.]), 'previousTarget': array([19., 11.]), 'currentState': array([ 3.89490416, 19.4566385 ,  5.70675255]), 'targetState': array([19, 11], dtype=int32), 'currentDistance': 17.31122916373216}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9309439634628024
{'scaleFactor': 20, 'currentTarget': array([19., 11.]), 'previousTarget': array([19., 11.]), 'currentState': array([19.5868782 , 10.76406236,  5.5189621 ]), 'targetState': array([19, 11], dtype=int32), 'currentDistance': 0.632528725703598}
episode index:3943
target Thresh 32.0
target distance 6.0
model initialize at round 3943
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 25.]), 'previousTarget': array([20., 25.]), 'currentState': array([24.49748535, 28.75818059,  3.68426561]), 'targetState': array([20, 25], dtype=int32), 'currentDistance': 5.8609978529806845}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9309539419203423
{'scaleFactor': 20, 'currentTarget': array([20., 25.]), 'previousTarget': array([20., 25.]), 'currentState': array([19.77958718, 25.06802113,  3.95559011]), 'targetState': array([20, 25], dtype=int32), 'currentDistance': 0.23067007419188515}
episode index:3944
target Thresh 32.0
target distance 4.0
model initialize at round 3944
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 24.]), 'previousTarget': array([12., 24.]), 'currentState': array([ 9.86169609, 24.30300229,  5.73227865]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 2.15966524894437}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9309689092354448
{'scaleFactor': 20, 'currentTarget': array([12., 24.]), 'previousTarget': array([12., 24.]), 'currentState': array([11.78344424, 23.94900738,  0.19403082]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 0.2224784136033991}
episode index:3945
target Thresh 32.0
target distance 16.0
model initialize at round 3945
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 26.]), 'previousTarget': array([19., 26.]), 'currentState': array([16.62303238,  9.5548551 ,  0.74231499]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 16.616039418387878}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9309668250451743
{'scaleFactor': 20, 'currentTarget': array([19., 26.]), 'previousTarget': array([19., 26.]), 'currentState': array([18.84563117, 25.2194685 ,  1.75352749]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 0.7956501436512582}
episode index:3946
target Thresh 32.0
target distance 16.0
model initialize at round 3946
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 20.]), 'previousTarget': array([23., 20.]), 'currentState': array([11.49608013,  5.60575616,  0.86488414]), 'targetState': array([23, 20], dtype=int32), 'currentDistance': 18.426459996474513}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9309624040729012
{'scaleFactor': 20, 'currentTarget': array([23., 20.]), 'previousTarget': array([23., 20.]), 'currentState': array([22.76902758, 19.58707906,  1.09248489]), 'targetState': array([23, 20], dtype=int32), 'currentDistance': 0.4731299572853047}
episode index:3947
target Thresh 32.0
target distance 16.0
model initialize at round 3947
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 25.]), 'previousTarget': array([10., 25.]), 'currentState': array([24.10406534, 25.43637708,  3.1702686 ]), 'targetState': array([10, 25], dtype=int32), 'currentDistance': 14.110814434183453}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9309626834406404
{'scaleFactor': 20, 'currentTarget': array([10., 25.]), 'previousTarget': array([10., 25.]), 'currentState': array([10.12394467, 25.09154423,  3.25938918]), 'targetState': array([10, 25], dtype=int32), 'currentDistance': 0.15408642389896385}
episode index:3948
target Thresh 32.0
target distance 8.0
model initialize at round 3948
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 17.]), 'previousTarget': array([27., 17.]), 'currentState': array([22.59000308,  9.91885623,  1.55404258]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 8.342102247439703}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9309701874483789
{'scaleFactor': 20, 'currentTarget': array([27., 17.]), 'previousTarget': array([27., 17.]), 'currentState': array([26.702613  , 16.64362521,  1.25989984]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 0.464157327043292}
episode index:3949
target Thresh 32.0
target distance 8.0
model initialize at round 3949
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 13.]), 'previousTarget': array([11., 13.]), 'currentState': array([17.19880504,  7.83407438,  2.49298376]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 8.069198933041942}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9309776876566197
{'scaleFactor': 20, 'currentTarget': array([11., 13.]), 'previousTarget': array([11., 13.]), 'currentState': array([11.14801291, 13.02764843,  2.53538101]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 0.15057309482159179}
episode index:3950
target Thresh 32.0
target distance 14.0
model initialize at round 3950
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 24.]), 'previousTarget': array([24., 24.]), 'currentState': array([25.59637065, 10.53290604,  1.33219164]), 'targetState': array([24, 24], dtype=int32), 'currentDistance': 13.561379684423809}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9309779629439522
{'scaleFactor': 20, 'currentTarget': array([24., 24.]), 'previousTarget': array([24., 24.]), 'currentState': array([24.16089427, 24.37203111,  1.46673396]), 'targetState': array([24, 24], dtype=int32), 'currentDistance': 0.405332101783122}
episode index:3951
target Thresh 32.0
target distance 22.0
model initialize at round 3951
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 14.]), 'previousTarget': array([ 2., 14.]), 'currentState': array([23.19002324, 30.47523752,  3.08290863]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 26.841209660735117}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9309644363903224
{'scaleFactor': 20, 'currentTarget': array([ 2., 14.]), 'previousTarget': array([ 2., 14.]), 'currentState': array([ 2.96839983, 14.51523126,  4.00382481]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 1.0969327623888103}
episode index:3952
target Thresh 32.0
target distance 10.0
model initialize at round 3952
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 25.]), 'previousTarget': array([16., 25.]), 'currentState': array([25.83623347, 18.67498317,  2.6782586 ]), 'targetState': array([16, 25], dtype=int32), 'currentDistance': 11.69432883484779}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9309670965757538
{'scaleFactor': 20, 'currentTarget': array([16., 25.]), 'previousTarget': array([16., 25.]), 'currentState': array([15.85654585, 25.28845395,  2.38304684]), 'targetState': array([16, 25], dtype=int32), 'currentDistance': 0.32215644782170416}
episode index:3953
target Thresh 32.0
target distance 3.0
model initialize at round 3953
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([16.59552056,  8.79809039,  5.55125929]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 1.6153980249523594}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9309820264956892
{'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([18.20964365,  7.623243  ,  5.75883316]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 0.43115692700886166}
episode index:3954
target Thresh 32.0
target distance 17.0
model initialize at round 3954
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 20.]), 'previousTarget': array([22., 20.]), 'currentState': array([4.59527794, 6.63358139, 0.80365753]), 'targetState': array([22, 20], dtype=int32), 'currentDistance': 21.945056308754282}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9309730141638972
{'scaleFactor': 20, 'currentTarget': array([22., 20.]), 'previousTarget': array([22., 20.]), 'currentState': array([22.10852306, 19.91714967,  0.65742678]), 'targetState': array([22, 20], dtype=int32), 'currentDistance': 0.13653363169004845}
episode index:3955
target Thresh 32.0
target distance 16.0
model initialize at round 3955
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([14.8733277 , 25.02067226,  4.42263809]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 14.843469049269837}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9309732902846615
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.40202662, 11.79682802,  4.59665728]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.8925022671371557}
episode index:3956
target Thresh 32.0
target distance 10.0
model initialize at round 3956
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 26.]), 'previousTarget': array([14., 26.]), 'currentState': array([22.99999903, 20.35365658,  3.21705234]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 10.624555355475062}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9309783488541877
{'scaleFactor': 20, 'currentTarget': array([14., 26.]), 'previousTarget': array([14., 26.]), 'currentState': array([14.60173569, 25.48894897,  2.89220262]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 0.7894675422541294}
episode index:3957
target Thresh 32.0
target distance 16.0
model initialize at round 3957
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 18.]), 'previousTarget': array([ 3., 18.]), 'currentState': array([17.33891932, 18.27055459,  3.9901315 ]), 'targetState': array([ 3, 18], dtype=int32), 'currentDistance': 14.341471578788772}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9309786234876019
{'scaleFactor': 20, 'currentTarget': array([ 3., 18.]), 'previousTarget': array([ 3., 18.]), 'currentState': array([ 3.59947634, 17.8575808 ,  3.32275971]), 'targetState': array([ 3, 18], dtype=int32), 'currentDistance': 0.6161615915081973}
episode index:3958
target Thresh 32.0
target distance 19.0
model initialize at round 3958
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 12.]), 'previousTarget': array([ 8., 12.]), 'currentState': array([25.09627587, 11.48777991,  3.10808635]), 'targetState': array([ 8, 12], dtype=int32), 'currentDistance': 17.10394744195335}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9309742129354411
{'scaleFactor': 20, 'currentTarget': array([ 8., 12.]), 'previousTarget': array([ 8., 12.]), 'currentState': array([ 7.1580178 , 11.93790913,  2.78676595]), 'targetState': array([ 8, 12], dtype=int32), 'currentDistance': 0.8442685037204132}
episode index:3959
target Thresh 32.0
target distance 5.0
model initialize at round 3959
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.11476233,  5.43134285,  1.11468363]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 3.5705019612334983}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9309866184372252
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.56568406,  9.21865183,  1.31045765]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.4862498902180281}
episode index:3960
target Thresh 32.0
target distance 14.0
model initialize at round 3960
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 20.]), 'previousTarget': array([ 5., 20.]), 'currentState': array([18.89906448, 19.32005949,  3.64237833]), 'targetState': array([ 5, 20], dtype=int32), 'currentDistance': 13.915685847351252}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9309868907748848
{'scaleFactor': 20, 'currentTarget': array([ 5., 20.]), 'previousTarget': array([ 5., 20.]), 'currentState': array([ 5.04816082, 20.14678363,  3.00726942]), 'targetState': array([ 5, 20], dtype=int32), 'currentDistance': 0.154482677894977}
episode index:3961
target Thresh 32.0
target distance 14.0
model initialize at round 3961
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.39410112, 24.49653512,  4.45539713]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 13.51012855993253}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9309871629750697
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.95000938, 10.55733409,  4.60234466]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.44547970533771275}
episode index:3962
target Thresh 32.0
target distance 21.0
model initialize at round 3962
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 29.]), 'previousTarget': array([ 3., 29.]), 'currentState': array([24.76202853, 22.50056676,  2.11090663]), 'targetState': array([ 3, 29], dtype=int32), 'currentDistance': 22.711858532321195}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9309781675401172
{'scaleFactor': 20, 'currentTarget': array([ 3., 29.]), 'previousTarget': array([ 3., 29.]), 'currentState': array([ 3.91127893, 28.43462253,  2.83708403]), 'targetState': array([ 3, 29], dtype=int32), 'currentDistance': 1.0724182814771097}
episode index:3963
target Thresh 32.0
target distance 9.0
model initialize at round 3963
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  9.]), 'previousTarget': array([24.,  9.]), 'currentState': array([19.29529936, 16.47167245,  5.29034877]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 8.829501533765264}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9309856392460859
{'scaleFactor': 20, 'currentTarget': array([24.,  9.]), 'previousTarget': array([24.,  9.]), 'currentState': array([23.65190796,  9.80651546,  5.4505367 ]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 0.8784277217974422}
episode index:3964
target Thresh 32.0
target distance 13.0
model initialize at round 3964
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 11.]), 'previousTarget': array([23., 11.]), 'currentState': array([23.08985641, 22.58436349,  5.15100574]), 'targetState': array([23, 11], dtype=int32), 'currentDistance': 11.584711977328169}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9309882860330101
{'scaleFactor': 20, 'currentTarget': array([23., 11.]), 'previousTarget': array([23., 11.]), 'currentState': array([22.91572833, 10.70235892,  4.57573307]), 'targetState': array([23, 11], dtype=int32), 'currentDistance': 0.30934112258934104}
episode index:3965
target Thresh 32.0
target distance 14.0
model initialize at round 3965
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([ 7.56627877, 15.37387762,  5.46173573]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 16.185653711704965}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9309862074673003
{'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([17.83323372,  3.1571249 ,  5.55664657]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 0.22912710142131548}
episode index:3966
target Thresh 32.0
target distance 15.0
model initialize at round 3966
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 24.]), 'previousTarget': array([11., 24.]), 'currentState': array([ 4.36042584, 10.86493698,  0.82086349]), 'targetState': array([11, 24], dtype=int32), 'currentDistance': 14.717806410439355}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9309864794966524
{'scaleFactor': 20, 'currentTarget': array([11., 24.]), 'previousTarget': array([11., 24.]), 'currentState': array([10.56576721, 23.34755287,  1.42398199]), 'targetState': array([11, 24], dtype=int32), 'currentDistance': 0.7837380767338165}
episode index:3967
target Thresh 32.0
target distance 15.0
model initialize at round 3967
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 16.]), 'previousTarget': array([26., 16.]), 'currentState': array([12.65980785,  9.27825525,  1.17609852]), 'targetState': array([26, 16], dtype=int32), 'currentDistance': 14.937957660335407}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.930984402433883
{'scaleFactor': 20, 'currentTarget': array([26., 16.]), 'previousTarget': array([26., 16.]), 'currentState': array([26.76939884, 16.43928367,  0.25653087]), 'targetState': array([26, 16], dtype=int32), 'currentDistance': 0.8859710629578678}
episode index:3968
target Thresh 32.0
target distance 10.0
model initialize at round 3968
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 23.]), 'previousTarget': array([17., 23.]), 'currentState': array([25.64766096, 23.99821798,  2.76916504]), 'targetState': array([17, 23], dtype=int32), 'currentDistance': 8.705083531125224}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9309918631563738
{'scaleFactor': 20, 'currentTarget': array([17., 23.]), 'previousTarget': array([17., 23.]), 'currentState': array([17.8103888 , 23.03837716,  3.37438275]), 'targetState': array([17, 23], dtype=int32), 'currentDistance': 0.8112969945544771}
episode index:3969
target Thresh 32.0
target distance 4.0
model initialize at round 3969
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([24.16759091, 17.78792758,  4.46909696]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 5.014115487921532}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9310017641984
{'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([19.41214212, 14.44305314,  3.3828784 ]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 0.8097942241515715}
episode index:3970
target Thresh 32.0
target distance 6.0
model initialize at round 3970
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  3.]), 'previousTarget': array([21.,  3.]), 'currentState': array([17.60754912,  8.50182943,  4.97267592]), 'targetState': array([21,  3], dtype=int32), 'currentDistance': 6.463656089711799}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9310116602537517
{'scaleFactor': 20, 'currentTarget': array([21.,  3.]), 'previousTarget': array([21.,  3.]), 'currentState': array([20.73430392,  3.44061345,  5.50788284]), 'targetState': array([21,  3], dtype=int32), 'currentDistance': 0.5145236820321444}
episode index:3971
target Thresh 32.0
target distance 11.0
model initialize at round 3971
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 19.]), 'previousTarget': array([24., 19.]), 'currentState': array([21.05975626,  9.58692892,  1.51562765]), 'targetState': array([24, 19], dtype=int32), 'currentDistance': 9.861589144447922}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9310166900598057
{'scaleFactor': 20, 'currentTarget': array([24., 19.]), 'previousTarget': array([24., 19.]), 'currentState': array([24.18016069, 19.05076256,  1.17962442]), 'targetState': array([24, 19], dtype=int32), 'currentDistance': 0.1871756207359415}
episode index:3972
target Thresh 32.0
target distance 18.0
model initialize at round 3972
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 28.]), 'previousTarget': array([23., 28.]), 'currentState': array([12.51791999, 11.6012954 ,  0.24797773]), 'targetState': array([23, 28], dtype=int32), 'currentDistance': 19.462566988366046}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9310099861546833
{'scaleFactor': 20, 'currentTarget': array([23., 28.]), 'previousTarget': array([23., 28.]), 'currentState': array([23.11837352, 28.25585431,  1.09778899]), 'targetState': array([23, 28], dtype=int32), 'currentDistance': 0.2819108351592559}
episode index:3973
target Thresh 32.0
target distance 17.0
model initialize at round 3973
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([22.65324816, 25.68510539,  5.08496952]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 17.962696151531794}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9310055843583394
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.07161196,  9.15370676,  4.51734336]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.1695701654439162}
episode index:3974
target Thresh 32.0
target distance 6.0
model initialize at round 3974
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 12.]), 'previousTarget': array([ 8., 12.]), 'currentState': array([6.99210593, 7.94661653, 1.16906798]), 'targetState': array([ 8, 12], dtype=int32), 'currentDistance': 4.176813143891779}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9310179351547272
{'scaleFactor': 20, 'currentTarget': array([ 8., 12.]), 'previousTarget': array([ 8., 12.]), 'currentState': array([ 8.01036663, 11.79649912,  1.13051248]), 'targetState': array([ 8, 12], dtype=int32), 'currentDistance': 0.20376475268524136}
episode index:3975
target Thresh 32.0
target distance 11.0
model initialize at round 3975
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 3.]), 'previousTarget': array([9., 3.]), 'currentState': array([14.11513064, 12.55876495,  5.15788913]), 'targetState': array([9, 3], dtype=int32), 'currentDistance': 10.841335196688245}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9310229583224198
{'scaleFactor': 20, 'currentTarget': array([9., 3.]), 'previousTarget': array([9., 3.]), 'currentState': array([9.60445303, 3.94332231, 4.35635491]), 'targetState': array([9, 3], dtype=int32), 'currentDistance': 1.120366213548018}
episode index:3976
target Thresh 32.0
target distance 11.0
model initialize at round 3976
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  4.]), 'previousTarget': array([10.,  4.]), 'currentState': array([14.62563636, 14.02865419,  4.76684141]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 11.044021760727219}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9310279789640032
{'scaleFactor': 20, 'currentTarget': array([10.,  4.]), 'previousTarget': array([10.,  4.]), 'currentState': array([10.57673695,  4.97836514,  4.33687144]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 1.135704123298031}
episode index:3977
target Thresh 32.0
target distance 15.0
model initialize at round 3977
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 29.]), 'previousTarget': array([26., 29.]), 'currentState': array([19.07071386, 15.49917874,  1.65387729]), 'targetState': array([26, 29], dtype=int32), 'currentDistance': 15.175216020342091}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9310258966903642
{'scaleFactor': 20, 'currentTarget': array([26., 29.]), 'previousTarget': array([26., 29.]), 'currentState': array([26.45149802, 29.55842963,  0.8929719 ]), 'targetState': array([26, 29], dtype=int32), 'currentDistance': 0.7181184496813184}
episode index:3978
target Thresh 32.0
target distance 15.0
model initialize at round 3978
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'previousTarget': array([27.,  8.]), 'currentState': array([11.10711472, 21.57341528,  5.16314983]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 20.900272845679126}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9310192005803664
{'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'previousTarget': array([27.,  8.]), 'currentState': array([26.26984093,  8.71852261,  5.56764048]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 1.0244056847002245}
episode index:3979
target Thresh 32.0
target distance 24.0
model initialize at round 3979
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([25.62125836, 27.04884293,  4.32548153]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 24.923986845308708}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9310079859248728
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.85626667,  5.81325964,  3.86690113]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.1809250015387236}
episode index:3980
target Thresh 32.0
target distance 7.0
model initialize at round 3980
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'previousTarget': array([16., 23.]), 'currentState': array([21.30249827, 24.59033817,  4.27732861]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 5.535852541524593}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9310178555591544
{'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'previousTarget': array([16., 23.]), 'currentState': array([15.78822956, 22.91585046,  3.11892891]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 0.2278768641432334}
episode index:3981
target Thresh 32.0
target distance 6.0
model initialize at round 3981
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([14.54560592, 12.84683289,  3.62432075]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 5.363486804823078}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9310277202363119
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([9.48947084, 9.65969352, 3.94918512]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.6135540128971891}
episode index:3982
target Thresh 32.0
target distance 7.0
model initialize at round 3982
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([14.95468174, 11.36204004,  4.56351012]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 6.1222232514284505}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9310375799600789
{'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([12.03007338,  6.17961507,  4.20884013]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 0.1821153019106816}
episode index:3983
target Thresh 32.0
target distance 14.0
model initialize at round 3983
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 15.]), 'previousTarget': array([11., 15.]), 'currentState': array([13.34216093, 28.71024545,  4.32462335]), 'targetState': array([11, 15], dtype=int32), 'currentDistance': 13.908865814554328}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9310378379339611
{'scaleFactor': 20, 'currentTarget': array([11., 15.]), 'previousTarget': array([11., 15.]), 'currentState': array([11.12567616, 15.00980769,  4.56293773]), 'targetState': array([11, 15], dtype=int32), 'currentDistance': 0.1260582743316826}
episode index:3984
target Thresh 32.0
target distance 5.0
model initialize at round 3984
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  6.]), 'previousTarget': array([17.,  6.]), 'currentState': array([18.42806491,  9.35967993,  5.29241896]), 'targetState': array([17,  6], dtype=int32), 'currentDistance': 3.6505915429313722}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9310501496433881
{'scaleFactor': 20, 'currentTarget': array([17.,  6.]), 'previousTarget': array([17.,  6.]), 'currentState': array([17.37159758,  5.89683597,  5.05082977]), 'targetState': array([17,  6], dtype=int32), 'currentDistance': 0.385652151409004}
episode index:3985
target Thresh 32.0
target distance 8.0
model initialize at round 3985
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([13.54872635, 14.68591376,  2.7330547 ]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 6.501248989352858}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.931059996319343
{'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([11.97123671, 20.17790406,  1.35206231]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 0.8225989681079138}
episode index:3986
target Thresh 32.0
target distance 10.0
model initialize at round 3986
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 29.]), 'previousTarget': array([15., 29.]), 'currentState': array([ 4.79525014, 22.32945401,  5.60030389]), 'targetState': array([15, 29], dtype=int32), 'currentDistance': 12.191517689867243}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9310626098515933
{'scaleFactor': 20, 'currentTarget': array([15., 29.]), 'previousTarget': array([15., 29.]), 'currentState': array([14.24553269, 28.7736916 ,  0.63892163]), 'targetState': array([15, 29], dtype=int32), 'currentDistance': 0.7876778638397794}
episode index:3987
target Thresh 32.0
target distance 9.0
model initialize at round 3987
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 28.]), 'previousTarget': array([19., 28.]), 'currentState': array([11.66888735, 25.0983189 ,  0.50132765]), 'targetState': array([19, 28], dtype=int32), 'currentDistance': 7.884476263394968}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9310700154183305
{'scaleFactor': 20, 'currentTarget': array([19., 28.]), 'previousTarget': array([19., 28.]), 'currentState': array([19.03047246, 28.08781252,  0.58276784]), 'targetState': array([19, 28], dtype=int32), 'currentDistance': 0.09294950191527698}
episode index:3988
target Thresh 32.0
target distance 20.0
model initialize at round 3988
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 28.]), 'previousTarget': array([26., 28.]), 'currentState': array([14.32218651,  8.13171215,  2.05326009]), 'targetState': array([26, 28], dtype=int32), 'currentDistance': 23.046044995400774}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9310588133266527
{'scaleFactor': 20, 'currentTarget': array([26., 28.]), 'previousTarget': array([26., 28.]), 'currentState': array([26.35606766, 28.34491492,  0.92763104]), 'targetState': array([26, 28], dtype=int32), 'currentDistance': 0.4957322665160874}
episode index:3989
target Thresh 32.0
target distance 12.0
model initialize at round 3989
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 12.]), 'previousTarget': array([20., 12.]), 'currentState': array([9.55205174, 2.9214188 , 1.3201223 ]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 13.841252095005572}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9310590655909587
{'scaleFactor': 20, 'currentTarget': array([20., 12.]), 'previousTarget': array([20., 12.]), 'currentState': array([19.96362871, 12.04638351,  0.90912519]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 0.05894319791012917}
episode index:3990
target Thresh 32.0
target distance 12.0
model initialize at round 3990
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 2.]), 'previousTarget': array([6., 2.]), 'currentState': array([ 3.2320501 , 12.502455  ,  5.24854922]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 10.86108224645924}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9310640595734967
{'scaleFactor': 20, 'currentTarget': array([6., 2.]), 'previousTarget': array([6., 2.]), 'currentState': array([5.96122404, 2.93833458, 4.99575051]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 0.9391354323233536}
episode index:3991
target Thresh 32.0
target distance 16.0
model initialize at round 3991
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 20.]), 'previousTarget': array([18., 20.]), 'currentState': array([3.47423141, 7.81180653, 1.51335305]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 18.96180406207244}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9310573757096278
{'scaleFactor': 20, 'currentTarget': array([18., 20.]), 'previousTarget': array([18., 20.]), 'currentState': array([18.66618846, 20.45174564,  0.44454989]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 0.804910671870675}
episode index:3992
target Thresh 32.0
target distance 25.0
model initialize at round 3992
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 27.]), 'previousTarget': array([ 2., 27.]), 'currentState': array([8.68472903, 3.04997636, 1.45789123]), 'targetState': array([ 2, 27], dtype=int32), 'currentDistance': 24.865422468306843}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9310461880051466
{'scaleFactor': 20, 'currentTarget': array([ 2., 27.]), 'previousTarget': array([ 2., 27.]), 'currentState': array([ 2.05107048, 26.03954593,  1.90843288]), 'targetState': array([ 2, 27], dtype=int32), 'currentDistance': 0.961810902827087}
episode index:3993
target Thresh 32.0
target distance 19.0
model initialize at round 3993
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 27.]), 'previousTarget': array([ 7., 27.]), 'currentState': array([24.26081066, 22.72112836,  3.35292828]), 'targetState': array([ 7, 27], dtype=int32), 'currentDistance': 17.78325974573709}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9310417991867886
{'scaleFactor': 20, 'currentTarget': array([ 7., 27.]), 'previousTarget': array([ 7., 27.]), 'currentState': array([ 6.93493238, 27.07259754,  3.02582366]), 'targetState': array([ 7, 27], dtype=int32), 'currentDistance': 0.09748947698905429}
episode index:3994
target Thresh 32.0
target distance 9.0
model initialize at round 3994
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([11.01057812, 25.31706323,  5.72867441]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 8.856282792935355}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9310491969867418
{'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([15.56126341, 18.82163311,  5.46846788]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 0.9314347873406001}
episode index:3995
target Thresh 32.0
target distance 7.0
model initialize at round 3995
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  2.]), 'previousTarget': array([20.,  2.]), 'currentState': array([14.94150743,  2.548107  ,  6.19666019]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 5.088100663030047}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9310590192597682
{'scaleFactor': 20, 'currentTarget': array([20.,  2.]), 'previousTarget': array([20.,  2.]), 'currentState': array([20.88401851,  1.89379299,  5.9775477 ]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 0.8903755736870915}
episode index:3996
target Thresh 32.0
target distance 9.0
model initialize at round 3996
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 11.]), 'previousTarget': array([19., 11.]), 'currentState': array([10.49531109, 11.39156753,  6.02112007]), 'targetState': array([19, 11], dtype=int32), 'currentDistance': 8.51369828811998}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9310664090497957
{'scaleFactor': 20, 'currentTarget': array([19., 11.]), 'previousTarget': array([19., 11.]), 'currentState': array([18.43888605, 10.99313241,  0.27576232]), 'targetState': array([19, 11], dtype=int32), 'currentDistance': 0.5611559787546644}
episode index:3997
target Thresh 32.0
target distance 10.0
model initialize at round 3997
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 18.]), 'previousTarget': array([23., 18.]), 'currentState': array([23.46151673,  8.83448003,  1.52879923]), 'targetState': array([23, 18], dtype=int32), 'currentDistance': 9.177132119318733}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9310713924517092
{'scaleFactor': 20, 'currentTarget': array([23., 18.]), 'previousTarget': array([23., 18.]), 'currentState': array([22.97299514, 18.79711827,  1.42755116]), 'targetState': array([23, 18], dtype=int32), 'currentDistance': 0.7975755724438695}
episode index:3998
target Thresh 32.0
target distance 11.0
model initialize at round 3998
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([13.35888292,  0.44392899,  3.31157207]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 14.093436358372045}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9310716410027109
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.21125161, 9.66796127, 2.62094703]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.3935441042113301}
episode index:3999
target Thresh 32.0
target distance 18.0
model initialize at round 3999
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([27.29818295, 11.65634387,  2.40267938]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 20.545739435993692}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9310649686112125
{'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([ 9.56698983, 20.71022479,  3.02022361]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 0.6367473176579632}
episode index:4000
target Thresh 32.0
target distance 2.0
model initialize at round 4000
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  4.]), 'previousTarget': array([24.,  4.]), 'currentState': array([23.80689408,  3.92055334,  1.20647746]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 0.20881012169930396}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.931082198061697
{'scaleFactor': 20, 'currentTarget': array([24.,  4.]), 'previousTarget': array([24.,  4.]), 'currentState': array([23.80689408,  3.92055334,  1.20647746]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 0.20881012169930396}
episode index:4001
target Thresh 32.0
target distance 17.0
model initialize at round 4001
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 19.]), 'previousTarget': array([ 8., 19.]), 'currentState': array([23.52305701, 28.80686274,  3.65158999]), 'targetState': array([ 8, 19], dtype=int32), 'currentDistance': 18.361368569294324}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.931077809018574
{'scaleFactor': 20, 'currentTarget': array([ 8., 19.]), 'previousTarget': array([ 8., 19.]), 'currentState': array([ 8.3954697 , 19.13372577,  4.00141827]), 'targetState': array([ 8, 19], dtype=int32), 'currentDistance': 0.4174672019792994}
episode index:4002
target Thresh 32.0
target distance 26.0
model initialize at round 4002
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  3.]), 'previousTarget': array([21.,  3.]), 'currentState': array([24.38406052, 28.52975785,  4.4347744 ]), 'targetState': array([21,  3], dtype=int32), 'currentDistance': 25.75306586523608}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9310644298564408
{'scaleFactor': 20, 'currentTarget': array([21.,  3.]), 'previousTarget': array([21.,  3.]), 'currentState': array([20.90119283,  2.79292145,  4.68846579]), 'targetState': array([21,  3], dtype=int32), 'currentDistance': 0.22944363576956311}
episode index:4003
target Thresh 32.0
target distance 17.0
model initialize at round 4003
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  3.]), 'previousTarget': array([24.,  3.]), 'currentState': array([15.66925332, 18.11640757,  5.11034632]), 'targetState': array([24,  3], dtype=int32), 'currentDistance': 17.259986040491608}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9310600474432607
{'scaleFactor': 20, 'currentTarget': array([24.,  3.]), 'previousTarget': array([24.,  3.]), 'currentState': array([24.20156973,  2.31303236,  4.94530617]), 'targetState': array([24,  3], dtype=int32), 'currentDistance': 0.7159293940972827}
episode index:4004
target Thresh 32.0
target distance 9.0
model initialize at round 4004
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 23.]), 'previousTarget': array([ 4., 23.]), 'currentState': array([11.46611581, 20.00675403,  3.28101766]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 8.043780626836083}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9310674222154346
{'scaleFactor': 20, 'currentTarget': array([ 4., 23.]), 'previousTarget': array([ 4., 23.]), 'currentState': array([ 4.17155714, 22.97390201,  3.06888041]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 0.17353085187939613}
episode index:4005
target Thresh 32.0
target distance 18.0
model initialize at round 4005
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'previousTarget': array([13., 22.]), 'currentState': array([24.19863502,  5.82946737,  2.07487333]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 19.669660698384888}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9310607608706502
{'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'previousTarget': array([13., 22.]), 'currentState': array([12.88579206, 22.23642882,  2.20596853]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 0.2625681661772629}
episode index:4006
target Thresh 32.0
target distance 16.0
model initialize at round 4006
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([24.49993654, 22.76301881,  3.68104291]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 19.316904739079945}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9310541028507195
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([9.36523857, 9.82237056, 3.55463759]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.6591466382791881}
episode index:4007
target Thresh 32.0
target distance 4.0
model initialize at round 4007
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 24.]), 'previousTarget': array([13., 24.]), 'currentState': array([11.54236758, 27.32658309,  4.86152006]), 'targetState': array([13, 24], dtype=int32), 'currentDistance': 3.631920611224949}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9310663398510064
{'scaleFactor': 20, 'currentTarget': array([13., 24.]), 'previousTarget': array([13., 24.]), 'currentState': array([13.19468125, 23.77817747,  4.72518838]), 'targetState': array([13, 24], dtype=int32), 'currentDistance': 0.29513729292197566}
episode index:4008
target Thresh 32.0
target distance 3.0
model initialize at round 4008
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([11.06464866, 13.68172787,  0.52237344]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 3.9759215116413036}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9310785707465288
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.70522728, 11.40328434,  5.91600043]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.4995289901181591}
episode index:4009
target Thresh 32.0
target distance 3.0
model initialize at round 4009
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 24.]), 'previousTarget': array([19., 24.]), 'currentState': array([21.81920349, 22.32676938,  3.59475422]), 'targetState': array([19, 24], dtype=int32), 'currentDistance': 3.2783546227728273}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9310907955418539
{'scaleFactor': 20, 'currentTarget': array([19., 24.]), 'previousTarget': array([19., 24.]), 'currentState': array([18.86803019, 24.41219963,  2.79472613]), 'targetState': array([19, 24], dtype=int32), 'currentDistance': 0.4328100848765849}
episode index:4010
target Thresh 32.0
target distance 20.0
model initialize at round 4010
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([8.69992131, 3.33560241, 5.55454773]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 18.303155717418043}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9310864142035197
{'scaleFactor': 20, 'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([26.4287891 ,  3.2305876 ,  6.19238438]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 0.6159971821357516}
episode index:4011
target Thresh 32.0
target distance 16.0
model initialize at round 4011
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 13.]), 'previousTarget': array([10., 13.]), 'currentState': array([24.37507085, 14.43817044,  3.88820231]), 'targetState': array([10, 13], dtype=int32), 'currentDistance': 14.446833426903}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9310866582049413
{'scaleFactor': 20, 'currentTarget': array([10., 13.]), 'previousTarget': array([10., 13.]), 'currentState': array([10.62834729, 12.94814235,  3.30769144]), 'targetState': array([10, 13], dtype=int32), 'currentDistance': 0.6304835679387644}
episode index:4012
target Thresh 32.0
target distance 8.0
model initialize at round 4012
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([19.31897961,  5.91901527,  2.17973137]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 7.5222296985261465}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9310940116442125
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([12.74568941, 10.37649216,  2.37176345]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.454334922688748}
episode index:4013
target Thresh 32.0
target distance 19.0
model initialize at round 4013
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([ 3.68778307, 21.94620932,  4.82819748]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 20.698025192271906}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9310873569514782
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.41456757,  4.87005477,  5.73535508]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.0486784245703273}
episode index:4014
target Thresh 32.0
target distance 6.0
model initialize at round 4014
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([24.21186752,  3.81963824,  1.75104651]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 4.352475999026935}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9310995643345539
{'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([23.02023075,  7.62638372,  1.73915055]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 0.3741636135352179}
episode index:4015
target Thresh 32.0
target distance 13.0
model initialize at round 4015
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 21.]), 'previousTarget': array([21., 21.]), 'currentState': array([16.04258046,  9.70380793,  1.10946293]), 'targetState': array([21, 21], dtype=int32), 'currentDistance': 12.336124341478289}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9311021491415923
{'scaleFactor': 20, 'currentTarget': array([21., 21.]), 'previousTarget': array([21., 21.]), 'currentState': array([20.69513176, 20.73295379,  1.42871009]), 'targetState': array([21, 21], dtype=int32), 'currentDistance': 0.40528794998255324}
episode index:4016
target Thresh 32.0
target distance 14.0
model initialize at round 4016
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  7.]), 'previousTarget': array([19.,  7.]), 'currentState': array([13.11175236, 22.26348541,  6.12237829]), 'targetState': array([19,  7], dtype=int32), 'currentDistance': 16.359873079230574}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9311000686201301
{'scaleFactor': 20, 'currentTarget': array([19.,  7.]), 'previousTarget': array([19.,  7.]), 'currentState': array([18.80562979,  7.71497785,  5.29900064]), 'targetState': array([19,  7], dtype=int32), 'currentDistance': 0.7409271894117622}
episode index:4017
target Thresh 32.0
target distance 17.0
model initialize at round 4017
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([16.49077828, 28.74474015,  3.69319093]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 18.084837184238918}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9310956926069054
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.11986445, 11.3970476 ,  4.70845451]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.41474604097996504}
episode index:4018
target Thresh 32.0
target distance 17.0
model initialize at round 4018
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([ 5.9711636 , 27.3744924 ,  6.22886545]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 22.91391711814613}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9310867955085356
{'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([21.41145552, 11.98168502,  5.43345653]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 1.1445916704693957}
episode index:4019
target Thresh 32.0
target distance 1.0
model initialize at round 4019
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  5.]), 'previousTarget': array([25.,  5.]), 'currentState': array([27.45599801,  4.84456912,  1.5352121 ]), 'targetState': array([25,  5], dtype=int32), 'currentDistance': 2.460911412026164}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9310989878479613
{'scaleFactor': 20, 'currentTarget': array([25.,  5.]), 'previousTarget': array([25.,  5.]), 'currentState': array([24.42125352,  4.72608447,  3.91450423]), 'targetState': array([25,  5], dtype=int32), 'currentDistance': 0.6402946231606632}
episode index:4020
target Thresh 32.0
target distance 14.0
model initialize at round 4020
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 22.]), 'previousTarget': array([20., 22.]), 'currentState': array([ 7.47121844, 21.81725418,  5.78023738]), 'targetState': array([20, 22], dtype=int32), 'currentDistance': 12.530114266681673}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9311015695842341
{'scaleFactor': 20, 'currentTarget': array([20., 22.]), 'previousTarget': array([20., 22.]), 'currentState': array([19.33246259, 22.16681145,  0.12188856]), 'targetState': array([20, 22], dtype=int32), 'currentDistance': 0.6880641293625023}
episode index:4021
target Thresh 32.0
target distance 14.0
model initialize at round 4021
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 24.]), 'previousTarget': array([13., 24.]), 'currentState': array([25.98528983, 26.65733424,  3.05522251]), 'targetState': array([13, 24], dtype=int32), 'currentDistance': 13.25440218186345}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9311018092108684
{'scaleFactor': 20, 'currentTarget': array([13., 24.]), 'previousTarget': array([13., 24.]), 'currentState': array([12.32288926, 23.8896858 ,  3.169197  ]), 'targetState': array([13, 24], dtype=int32), 'currentDistance': 0.6860380315560948}
episode index:4022
target Thresh 32.0
target distance 14.0
model initialize at round 4022
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 20.]), 'previousTarget': array([20., 20.]), 'currentState': array([ 7.97127992, 23.92616034,  5.95589176]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 12.653254201855779}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9311043889623448
{'scaleFactor': 20, 'currentTarget': array([20., 20.]), 'previousTarget': array([20., 20.]), 'currentState': array([19.37914503, 20.29197265,  6.1851131 ]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 0.6860823044857389}
episode index:4023
target Thresh 32.0
target distance 4.0
model initialize at round 4023
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 12.]), 'previousTarget': array([12., 12.]), 'currentState': array([11.73658386,  9.67581977,  0.73942578]), 'targetState': array([12, 12], dtype=int32), 'currentDistance': 2.3390600310321563}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9311190250485868
{'scaleFactor': 20, 'currentTarget': array([12., 12.]), 'previousTarget': array([12., 12.]), 'currentState': array([12.0785763 , 11.50497217,  2.04556084]), 'targetState': array([12, 12], dtype=int32), 'currentDistance': 0.5012252891326906}
episode index:4024
target Thresh 32.0
target distance 14.0
model initialize at round 4024
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  2.]), 'previousTarget': array([23.,  2.]), 'currentState': array([16.13832989, 17.23959395,  6.10114283]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 16.71310085944941}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9311169444695505
{'scaleFactor': 20, 'currentTarget': array([23.,  2.]), 'previousTarget': array([23.,  2.]), 'currentState': array([22.68405154,  2.99645776,  5.25067486]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 1.0453475482179542}
episode index:4025
target Thresh 32.0
target distance 11.0
model initialize at round 4025
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 23.]), 'previousTarget': array([21., 23.]), 'currentState': array([11.89019428, 21.00895238,  0.58927255]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 9.324850180180675}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9311218806606659
{'scaleFactor': 20, 'currentTarget': array([21., 23.]), 'previousTarget': array([21., 23.]), 'currentState': array([21.60953551, 22.97641367,  6.27266062]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 0.6099916811425914}
episode index:4026
target Thresh 32.0
target distance 9.0
model initialize at round 4026
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([11.94229841, 23.6055604 ,  0.03347128]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 9.815745634586989}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9311268144002337
{'scaleFactor': 20, 'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([19.94329758, 17.87375075,  5.88750602]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 0.13839811575601677}
episode index:4027
target Thresh 32.0
target distance 11.0
model initialize at round 4027
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([13.36453843, 15.60294391,  4.38976383]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 10.912724703578036}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9311317456900798
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.99181715, 10.45575565,  3.79063645]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.0915193440920512}
episode index:4028
target Thresh 32.0
target distance 2.0
model initialize at round 4028
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 20.]), 'previousTarget': array([ 5., 20.]), 'currentState': array([ 5.79291743, 20.52033993,  3.49836516]), 'targetState': array([ 5, 20], dtype=int32), 'currentDistance': 0.9484048115558604}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9311488388284044
{'scaleFactor': 20, 'currentTarget': array([ 5., 20.]), 'previousTarget': array([ 5., 20.]), 'currentState': array([ 5.79291743, 20.52033993,  3.49836516]), 'targetState': array([ 5, 20], dtype=int32), 'currentDistance': 0.9484048115558604}
episode index:4029
target Thresh 32.0
target distance 18.0
model initialize at round 4029
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  4.]), 'previousTarget': array([25.,  4.]), 'currentState': array([ 8.47726627, 22.8062707 ,  5.77278716]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 25.033508491116123}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9311355316780746
{'scaleFactor': 20, 'currentTarget': array([25.,  4.]), 'previousTarget': array([25.,  4.]), 'currentState': array([25.39594129,  3.30275144,  5.03824962]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 0.8018260790204633}
episode index:4030
target Thresh 32.0
target distance 23.0
model initialize at round 4030
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([14.85924991, 28.23736709,  3.3255893 ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 26.983175724606177}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9311200541988116
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.67390326, 3.51142137, 4.12656817]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.5874080014851077}
episode index:4031
target Thresh 32.0
target distance 6.0
model initialize at round 4031
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 25.]), 'previousTarget': array([ 8., 25.]), 'currentState': array([13.11896047, 22.56606924,  3.15143204]), 'targetState': array([ 8, 25], dtype=int32), 'currentDistance': 5.668136842471178}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9311297711992583
{'scaleFactor': 20, 'currentTarget': array([ 8., 25.]), 'previousTarget': array([ 8., 25.]), 'currentState': array([ 7.78259282, 25.05017902,  2.30191123]), 'targetState': array([ 8, 25], dtype=int32), 'currentDistance': 0.22312287594402863}
episode index:4032
target Thresh 32.0
target distance 6.0
model initialize at round 4032
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([15.33888103, 10.3106662 ,  3.95499754]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 4.349988742675827}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9311419135818025
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([11.53227705,  9.93447387,  3.37810737]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.5362951865513199}
episode index:4033
target Thresh 32.0
target distance 20.0
model initialize at round 4033
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 23.]), 'previousTarget': array([25., 23.]), 'currentState': array([ 4.68127859, 24.34748533,  5.53185797]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 20.363353270781182}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9311352800075405
{'scaleFactor': 20, 'currentTarget': array([25., 23.]), 'previousTarget': array([25., 23.]), 'currentState': array([24.36809923, 23.07743436,  0.40483974]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 0.6366275716433575}
episode index:4034
target Thresh 32.0
target distance 2.0
model initialize at round 4034
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 23.]), 'previousTarget': array([21., 23.]), 'currentState': array([21.67050526, 25.20445114,  5.39496845]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 2.3041662504950886}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.931149868537898
{'scaleFactor': 20, 'currentTarget': array([21., 23.]), 'previousTarget': array([21., 23.]), 'currentState': array([21.47883675, 23.42008837,  3.79984755]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 0.6369920511409995}
episode index:4035
target Thresh 32.0
target distance 8.0
model initialize at round 4035
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([8.74279111, 7.51018193, 0.10368562]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 8.05269589037061}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9311571644104109
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.90881421, 10.90467899,  0.77593188]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.13191263528119868}
episode index:4036
target Thresh 32.0
target distance 21.0
model initialize at round 4036
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 28.]), 'previousTarget': array([ 4., 28.]), 'currentState': array([16.32675339,  8.66644324,  2.75485039]), 'targetState': array([ 4, 28], dtype=int32), 'currentDistance': 22.928917677940092}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9311482917549361
{'scaleFactor': 20, 'currentTarget': array([ 4., 28.]), 'previousTarget': array([ 4., 28.]), 'currentState': array([ 4.47734744, 27.03468448,  2.19355546]), 'targetState': array([ 4, 28], dtype=int32), 'currentDistance': 1.0768911854725565}
episode index:4037
target Thresh 32.0
target distance 5.0
model initialize at round 4037
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 16.]), 'previousTarget': array([10., 16.]), 'currentState': array([ 6.25810367, 15.88216119,  0.28356868]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 3.7437513458602885}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9311604145157695
{'scaleFactor': 20, 'currentTarget': array([10., 16.]), 'previousTarget': array([10., 16.]), 'currentState': array([10.22106896, 15.89312453,  0.18062526]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 0.24554805863481804}
episode index:4038
target Thresh 32.0
target distance 16.0
model initialize at round 4038
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([16.5682207 , 24.37336091,  3.44292974]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 19.093288756912482}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9311560463139789
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.84627979, 10.91103546,  3.93846493]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.243452895759256}
episode index:4039
target Thresh 32.0
target distance 22.0
model initialize at round 4039
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 24.]), 'previousTarget': array([18., 24.]), 'currentState': array([26.61122888,  2.48613742,  1.30303257]), 'targetState': array([18, 24], dtype=int32), 'currentDistance': 23.173250655309648}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9311449643400684
{'scaleFactor': 20, 'currentTarget': array([18., 24.]), 'previousTarget': array([18., 24.]), 'currentState': array([17.98357037, 24.66895775,  1.74912437]), 'targetState': array([18, 24], dtype=int32), 'currentDistance': 0.6691594787610863}
episode index:4040
target Thresh 32.0
target distance 1.0
model initialize at round 4040
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 24.]), 'previousTarget': array([16., 24.]), 'currentState': array([18.18553966, 24.19533088,  1.7985478 ]), 'targetState': array([16, 24], dtype=int32), 'currentDistance': 2.1942510742561137}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9311595288131345
{'scaleFactor': 20, 'currentTarget': array([16., 24.]), 'previousTarget': array([16., 24.]), 'currentState': array([16.6065003 , 24.78942543,  3.78478878]), 'targetState': array([16, 24], dtype=int32), 'currentDistance': 0.9955074702998232}
episode index:4041
target Thresh 32.0
target distance 13.0
model initialize at round 4041
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 22.]), 'previousTarget': array([24., 22.]), 'currentState': array([12.80769811, 27.69800626,  5.39291919]), 'targetState': array([24, 22], dtype=int32), 'currentDistance': 12.559255431058462}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9311620821581587
{'scaleFactor': 20, 'currentTarget': array([24., 22.]), 'previousTarget': array([24., 22.]), 'currentState': array([23.46472994, 22.38891511,  5.93595594]), 'targetState': array([24, 22], dtype=int32), 'currentDistance': 0.6616411369247265}
episode index:4042
target Thresh 32.0
target distance 13.0
model initialize at round 4042
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([16.07788668, 13.3246659 ,  3.68779588]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 16.55667249958344}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9311600001923583
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.48992151, 2.37979812, 4.00695632]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.6198949034104219}
episode index:4043
target Thresh 32.0
target distance 10.0
model initialize at round 4043
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 23.]), 'previousTarget': array([ 5., 23.]), 'currentState': array([13.3281433 , 17.80691129,  2.24657679]), 'targetState': array([ 5, 23], dtype=int32), 'currentDistance': 9.814588178039825}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.931164903765481
{'scaleFactor': 20, 'currentTarget': array([ 5., 23.]), 'previousTarget': array([ 5., 23.]), 'currentState': array([ 4.90575996, 23.01317984,  3.01083524]), 'targetState': array([ 5, 23], dtype=int32), 'currentDistance': 0.09515719982391581}
episode index:4044
target Thresh 32.0
target distance 14.0
model initialize at round 4044
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 22.]), 'previousTarget': array([18., 22.]), 'currentState': array([ 5.78084521, 15.9091024 ,  0.51231383]), 'targetState': array([18, 22], dtype=int32), 'currentDistance': 13.653086737640965}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.931165126372191
{'scaleFactor': 20, 'currentTarget': array([18., 22.]), 'previousTarget': array([18., 22.]), 'currentState': array([18.22970667, 22.07121289,  0.47976618]), 'targetState': array([18, 22], dtype=int32), 'currentDistance': 0.2404920576761797}
episode index:4045
target Thresh 32.0
target distance 24.0
model initialize at round 4045
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 26.]), 'previousTarget': array([15., 26.]), 'currentState': array([20.81968604,  3.8948528 ,  2.21338832]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 22.85839623272905}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9311562714853611
{'scaleFactor': 20, 'currentTarget': array([15., 26.]), 'previousTarget': array([15., 26.]), 'currentState': array([15.22759774, 25.08478073,  2.00212869]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 0.9430943993669201}
episode index:4046
target Thresh 32.0
target distance 5.0
model initialize at round 4046
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 18.]), 'previousTarget': array([12., 18.]), 'currentState': array([10.69626705, 14.85895956,  0.99844778]), 'targetState': array([12, 18], dtype=int32), 'currentDistance': 3.400860867649151}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9311683653149916
{'scaleFactor': 20, 'currentTarget': array([12., 18.]), 'previousTarget': array([12., 18.]), 'currentState': array([12.29832547, 18.49969668,  0.94970915]), 'targetState': array([12, 18], dtype=int32), 'currentDistance': 0.5819749688227205}
episode index:4047
target Thresh 32.0
target distance 12.0
model initialize at round 4047
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 29.]), 'previousTarget': array([ 9., 29.]), 'currentState': array([18.91338766, 18.67672375,  2.22344005]), 'targetState': array([ 9, 29], dtype=int32), 'currentDistance': 14.312417240415813}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9311685869016003
{'scaleFactor': 20, 'currentTarget': array([ 9., 29.]), 'previousTarget': array([ 9., 29.]), 'currentState': array([ 9.30328254, 28.756859  ,  2.75811628]), 'targetState': array([ 9, 29], dtype=int32), 'currentDistance': 0.3887130614223411}
episode index:4048
target Thresh 32.0
target distance 3.0
model initialize at round 4048
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 13.]), 'previousTarget': array([18., 13.]), 'currentState': array([17.66438195, 16.24944097,  5.42194814]), 'targetState': array([18, 13], dtype=int32), 'currentDistance': 3.266727119582652}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9311806717158998
{'scaleFactor': 20, 'currentTarget': array([18., 13.]), 'previousTarget': array([18., 13.]), 'currentState': array([17.84432747, 12.39964166,  4.72257811]), 'targetState': array([18, 13], dtype=int32), 'currentDistance': 0.6202129238898375}
episode index:4049
target Thresh 32.0
target distance 4.0
model initialize at round 4049
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 15.]), 'previousTarget': array([ 8., 15.]), 'currentState': array([ 3.60451372, 12.36414097,  5.48515201]), 'targetState': array([ 8, 15], dtype=int32), 'currentDistance': 5.125236821412416}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9311903305623898
{'scaleFactor': 20, 'currentTarget': array([ 8., 15.]), 'previousTarget': array([ 8., 15.]), 'currentState': array([ 8.28243544, 15.1553811 ,  0.79969608]), 'targetState': array([ 8, 15], dtype=int32), 'currentDistance': 0.3223554962186131}
episode index:4050
target Thresh 32.0
target distance 17.0
model initialize at round 4050
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 12.]), 'previousTarget': array([ 5., 12.]), 'currentState': array([20.40564398, 24.14222756,  4.4080776 ]), 'targetState': array([ 5, 12], dtype=int32), 'currentDistance': 19.615492770072677}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.931183712874028
{'scaleFactor': 20, 'currentTarget': array([ 5., 12.]), 'previousTarget': array([ 5., 12.]), 'currentState': array([ 4.71019717, 11.96991064,  3.94932394]), 'targetState': array([ 5, 12], dtype=int32), 'currentDistance': 0.29136068407088833}
episode index:4051
target Thresh 32.0
target distance 20.0
model initialize at round 4051
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 29.]), 'previousTarget': array([25., 29.]), 'currentState': array([11.37015824, 10.23330978,  1.43341064]), 'targetState': array([25, 29], dtype=int32), 'currentDistance': 23.193991636509715}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9311726568915112
{'scaleFactor': 20, 'currentTarget': array([25., 29.]), 'previousTarget': array([25., 29.]), 'currentState': array([25.44871035, 29.54678833,  0.82256415]), 'targetState': array([25, 29], dtype=int32), 'currentDistance': 0.7073319271506802}
episode index:4052
target Thresh 32.0
target distance 12.0
model initialize at round 4052
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 29.]), 'previousTarget': array([19., 29.]), 'currentState': array([14.81816399, 18.81979151,  1.26886304]), 'targetState': array([19, 29], dtype=int32), 'currentDistance': 11.005652969949962}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9311775464530727
{'scaleFactor': 20, 'currentTarget': array([19., 29.]), 'previousTarget': array([19., 29.]), 'currentState': array([18.46150503, 28.08491936,  1.14909407]), 'targetState': array([19, 29], dtype=int32), 'currentDistance': 1.0617671217672882}
episode index:4053
target Thresh 32.0
target distance 21.0
model initialize at round 4053
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  7.]), 'previousTarget': array([18.,  7.]), 'currentState': array([17.40875061, 28.54800862,  3.81992531]), 'targetState': array([18,  7], dtype=int32), 'currentDistance': 21.556118656311202}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9311687059764584
{'scaleFactor': 20, 'currentTarget': array([18.,  7.]), 'previousTarget': array([18.,  7.]), 'currentState': array([17.89685074,  6.9124326 ,  4.99999409]), 'targetState': array([18,  7], dtype=int32), 'currentDistance': 0.13530639582090992}
episode index:4054
target Thresh 32.0
target distance 17.0
model initialize at round 4054
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 12.]), 'previousTarget': array([ 2., 12.]), 'currentState': array([20.24221318, 22.86452896,  4.53265034]), 'targetState': array([ 2, 12], dtype=int32), 'currentDistance': 21.232435829196763}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9311598698601284
{'scaleFactor': 20, 'currentTarget': array([ 2., 12.]), 'previousTarget': array([ 2., 12.]), 'currentState': array([ 1.50106122, 11.89309749,  3.58452328]), 'targetState': array([ 2, 12], dtype=int32), 'currentDistance': 0.5102627316244998}
episode index:4055
target Thresh 32.0
target distance 22.0
model initialize at round 4055
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([16.03783649, 25.24665785,  4.20549584]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 23.554971061902158}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9311488306594026
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.67383335, 4.72171111, 4.18934878]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.42875329920149857}
episode index:4056
target Thresh 32.0
target distance 7.0
model initialize at round 4056
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 13.]), 'previousTarget': array([11., 13.]), 'currentState': array([13.16219562, 18.25011573,  4.69595838]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 5.677922601772483}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9311584806888186
{'scaleFactor': 20, 'currentTarget': array([11., 13.]), 'previousTarget': array([11., 13.]), 'currentState': array([10.90412198, 12.75234129,  4.09475538]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 0.2655700118361698}
episode index:4057
target Thresh 32.0
target distance 7.0
model initialize at round 4057
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  4.]), 'previousTarget': array([23.,  4.]), 'currentState': array([19.34780826, 12.00786952,  5.9152667 ]), 'targetState': array([23,  4], dtype=int32), 'currentDistance': 8.80139072485237}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9311657348852974
{'scaleFactor': 20, 'currentTarget': array([23.,  4.]), 'previousTarget': array([23.,  4.]), 'currentState': array([22.64958513,  4.9220917 ,  5.13634369]), 'targetState': array([23,  4], dtype=int32), 'currentDistance': 0.9864297704287307}
episode index:4058
target Thresh 32.0
target distance 13.0
model initialize at round 4058
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([13.03167627, 10.32967371,  1.92093247]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 11.710429823355877}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9311682760073756
{'scaleFactor': 20, 'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([14.20149122, 22.14947546,  1.58756065]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 0.2508816966576556}
episode index:4059
target Thresh 32.0
target distance 10.0
model initialize at round 4059
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 20.]), 'previousTarget': array([23., 20.]), 'currentState': array([14.29360329, 12.51739573,  0.72888309]), 'targetState': array([23, 20], dtype=int32), 'currentDistance': 11.480013512770848}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9311708158776696
{'scaleFactor': 20, 'currentTarget': array([23., 20.]), 'previousTarget': array([23., 20.]), 'currentState': array([23.4366757 , 20.25357557,  0.69727854]), 'targetState': array([23, 20], dtype=int32), 'currentDistance': 0.5049616207278805}
episode index:4060
target Thresh 32.0
target distance 22.0
model initialize at round 4060
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  4.]), 'previousTarget': array([13.,  4.]), 'currentState': array([16.69717537, 24.14514791,  5.16370058]), 'targetState': array([13,  4], dtype=int32), 'currentDistance': 20.4816037013086}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9311642192904082
{'scaleFactor': 20, 'currentTarget': array([13.,  4.]), 'previousTarget': array([13.,  4.]), 'currentState': array([13.18031501,  4.62566118,  4.79295896]), 'targetState': array([13,  4], dtype=int32), 'currentDistance': 0.651126267237925}
episode index:4061
target Thresh 32.0
target distance 22.0
model initialize at round 4061
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 26.]), 'previousTarget': array([15., 26.]), 'currentState': array([6.5892514 , 4.5537762 , 1.34529417]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 23.036518996751195}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9311553995058114
{'scaleFactor': 20, 'currentTarget': array([15., 26.]), 'previousTarget': array([15., 26.]), 'currentState': array([14.58644158, 25.00770305,  1.31346766]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 1.0750273471157379}
episode index:4062
target Thresh 32.0
target distance 2.0
model initialize at round 4062
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'previousTarget': array([10., 12.]), 'currentState': array([ 9.32276463, 13.04052004,  5.93971759]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 1.2415030024911171}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9311698825480201
{'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'previousTarget': array([10., 12.]), 'currentState': array([10.74494293, 11.71488821,  5.117881  ]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 0.7976394525921431}
episode index:4063
target Thresh 32.0
target distance 15.0
model initialize at round 4063
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 29.]), 'previousTarget': array([26., 29.]), 'currentState': array([12.6710412 , 19.79997655,  0.89086693]), 'targetState': array([26, 29], dtype=int32), 'currentDistance': 16.195727033557986}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9311678094210221
{'scaleFactor': 20, 'currentTarget': array([26., 29.]), 'previousTarget': array([26., 29.]), 'currentState': array([25.80783113, 28.78217231,  0.98632752]), 'targetState': array([26, 29], dtype=int32), 'currentDistance': 0.2904785326261857}
episode index:4064
target Thresh 32.0
target distance 11.0
model initialize at round 4064
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 14.]), 'previousTarget': array([ 5., 14.]), 'currentState': array([8.862166  , 4.58204579, 1.79662389]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 10.179105450780146}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9311726857409431
{'scaleFactor': 20, 'currentTarget': array([ 5., 14.]), 'previousTarget': array([ 5., 14.]), 'currentState': array([ 5.09194557, 13.79791779,  2.21789674]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 0.22201623207613042}
episode index:4065
target Thresh 32.0
target distance 21.0
model initialize at round 4065
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 28.]), 'previousTarget': array([ 3., 28.]), 'currentState': array([14.09951018,  8.68002554,  2.5216341 ]), 'targetState': array([ 3, 28], dtype=int32), 'currentDistance': 22.28139446626479}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9311638725507113
{'scaleFactor': 20, 'currentTarget': array([ 3., 28.]), 'previousTarget': array([ 3., 28.]), 'currentState': array([ 3.11660541, 27.61391997,  2.23050398]), 'targetState': array([ 3, 28], dtype=int32), 'currentDistance': 0.4033046153226104}
episode index:4066
target Thresh 32.0
target distance 8.0
model initialize at round 4066
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 16.]), 'previousTarget': array([12., 16.]), 'currentState': array([ 9.19650296, 22.40500789,  5.2192435 ]), 'targetState': array([12, 16], dtype=int32), 'currentDistance': 6.991689479329208}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9311734951539691
{'scaleFactor': 20, 'currentTarget': array([12., 16.]), 'previousTarget': array([12., 16.]), 'currentState': array([11.68008232, 16.96873995,  5.14787124]), 'targetState': array([12, 16], dtype=int32), 'currentDistance': 1.0201982247635255}
episode index:4067
target Thresh 32.0
target distance 11.0
model initialize at round 4067
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 22.]), 'previousTarget': array([ 8., 22.]), 'currentState': array([ 0.36808692, 11.41139734,  1.88464308]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 13.052379231352086}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9311737143901424
{'scaleFactor': 20, 'currentTarget': array([ 8., 22.]), 'previousTarget': array([ 8., 22.]), 'currentState': array([ 8.50762268, 22.43978133,  0.80090303]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 0.6716311516691954}
episode index:4068
target Thresh 32.0
target distance 13.0
model initialize at round 4068
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  4.]), 'previousTarget': array([19.,  4.]), 'currentState': array([20.35357824, 17.34883158,  3.94280767]), 'targetState': array([19,  4], dtype=int32), 'currentDistance': 13.417282835196422}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9311739335185566
{'scaleFactor': 20, 'currentTarget': array([19.,  4.]), 'previousTarget': array([19.,  4.]), 'currentState': array([18.85326702,  3.68257044,  4.62104181]), 'targetState': array([19,  4], dtype=int32), 'currentDistance': 0.34970286667370887}
episode index:4069
target Thresh 32.0
target distance 4.0
model initialize at round 4069
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  5.]), 'previousTarget': array([19.,  5.]), 'currentState': array([16.63856828,  8.05887629,  5.17565615]), 'targetState': array([19,  5], dtype=int32), 'currentDistance': 3.864334861344837}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9311859546651122
{'scaleFactor': 20, 'currentTarget': array([19.,  5.]), 'previousTarget': array([19.,  5.]), 'currentState': array([19.07039421,  4.93333294,  5.08794294]), 'targetState': array([19,  5], dtype=int32), 'currentDistance': 0.09695278067344199}
episode index:4070
target Thresh 32.0
target distance 8.0
model initialize at round 4070
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 12.]), 'previousTarget': array([22., 12.]), 'currentState': array([16.0392303 , 18.61822228,  5.11481476]), 'targetState': array([22, 12], dtype=int32), 'currentDistance': 8.906831179074835}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.931193178947926
{'scaleFactor': 20, 'currentTarget': array([22., 12.]), 'previousTarget': array([22., 12.]), 'currentState': array([21.3571924 , 12.71608941,  5.67237318]), 'targetState': array([22, 12], dtype=int32), 'currentDistance': 0.9622814825358984}
episode index:4071
target Thresh 32.0
target distance 6.0
model initialize at round 4071
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  8.]), 'previousTarget': array([19.,  8.]), 'currentState': array([21.83913771,  3.21869672,  1.32211125]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 5.560716139393969}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9312027825385576
{'scaleFactor': 20, 'currentTarget': array([19.,  8.]), 'previousTarget': array([19.,  8.]), 'currentState': array([19.00776843,  8.19797599,  2.13711271]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 0.1981283440845979}
episode index:4072
target Thresh 32.0
target distance 4.0
model initialize at round 4072
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 16.]), 'previousTarget': array([ 3., 16.]), 'currentState': array([ 7.73073928, 13.51605019,  2.13165048]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 5.3432107227587196}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9312123814134561
{'scaleFactor': 20, 'currentTarget': array([ 3., 16.]), 'previousTarget': array([ 3., 16.]), 'currentState': array([ 2.6609648 , 16.42330567,  2.64401552]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 0.5423398917967603}
episode index:4073
target Thresh 32.0
target distance 11.0
model initialize at round 4073
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([ 3.79642747, 18.13972955,  5.67793718]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 11.648239584940445}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9312149017296042
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.13973133, 10.66587406,  5.6955259 ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.36216706940440335}
episode index:4074
target Thresh 32.0
target distance 11.0
model initialize at round 4074
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 27.]), 'previousTarget': array([20., 27.]), 'currentState': array([19.57189479, 17.70931959,  0.94697213]), 'targetState': array([20, 27], dtype=int32), 'currentDistance': 9.300538512712158}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9312197545267012
{'scaleFactor': 20, 'currentTarget': array([20., 27.]), 'previousTarget': array([20., 27.]), 'currentState': array([20.14997527, 27.53426512,  1.67689249]), 'targetState': array([20, 27], dtype=int32), 'currentDistance': 0.5549160321364711}
episode index:4075
target Thresh 32.0
target distance 7.0
model initialize at round 4075
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 22.]), 'previousTarget': array([ 3., 22.]), 'currentState': array([ 8.47779535, 25.70357246,  3.88779937]), 'targetState': array([ 3, 22], dtype=int32), 'currentDistance': 6.612313583499286}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9312293421727939
{'scaleFactor': 20, 'currentTarget': array([ 3., 22.]), 'previousTarget': array([ 3., 22.]), 'currentState': array([ 3.55681662, 22.31720488,  3.82349057]), 'targetState': array([ 3, 22], dtype=int32), 'currentDistance': 0.6408304664709273}
episode index:4076
target Thresh 32.0
target distance 13.0
model initialize at round 4076
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 11.]), 'previousTarget': array([25., 11.]), 'currentState': array([18.38532942, 22.7065335 ,  4.96147418]), 'targetState': array([25, 11], dtype=int32), 'currentDistance': 13.446069816834255}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9312295472269353
{'scaleFactor': 20, 'currentTarget': array([25., 11.]), 'previousTarget': array([25., 11.]), 'currentState': array([25.02323901, 10.44553899,  5.11920055]), 'targetState': array([25, 11], dtype=int32), 'currentDistance': 0.5549478004009369}
episode index:4077
target Thresh 32.0
target distance 9.0
model initialize at round 4077
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([15.10307713,  3.6905331 ,  2.57580793]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 8.91457809611081}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9312367484193759
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.59844882, 10.212948  ,  2.41653502]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.9887324435816393}
episode index:4078
target Thresh 32.0
target distance 16.0
model initialize at round 4078
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 5.]), 'previousTarget': array([8., 5.]), 'currentState': array([22.42546664,  4.4056661 ,  2.49252415]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 14.437704823134155}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9312369515572744
{'scaleFactor': 20, 'currentTarget': array([8., 5.]), 'previousTarget': array([8., 5.]), 'currentState': array([8.61657812, 4.91750817, 3.25639781]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 0.622071930552933}
episode index:4079
target Thresh 32.0
target distance 5.0
model initialize at round 4079
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([ 3.24805306, 10.06872222,  4.43630528]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 3.0787312939217}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9312489277946377
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.00109091, 6.11503079, 4.38358906]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.8849698814843602}
episode index:4080
target Thresh 32.0
target distance 4.0
model initialize at round 4080
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 21.]), 'previousTarget': array([22., 21.]), 'currentState': array([20.96613111, 23.29267387,  4.88409179]), 'targetState': array([22, 21], dtype=int32), 'currentDistance': 2.5150026565217822}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9312633240387459
{'scaleFactor': 20, 'currentTarget': array([22., 21.]), 'previousTarget': array([22., 21.]), 'currentState': array([21.79385877, 21.49724594,  5.40990436]), 'targetState': array([22, 21], dtype=int32), 'currentDistance': 0.5382822015514421}
episode index:4081
target Thresh 32.0
target distance 14.0
model initialize at round 4081
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'previousTarget': array([ 5., 22.]), 'currentState': array([4.75657598, 9.66972807, 0.71297789]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 12.332674532600294}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9312658269356989
{'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'previousTarget': array([ 5., 22.]), 'currentState': array([ 4.95580599, 21.39907648,  1.92294143]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 0.602546422600845}
episode index:4082
target Thresh 32.0
target distance 19.0
model initialize at round 4082
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 26.]), 'previousTarget': array([11., 26.]), 'currentState': array([17.54451362,  7.6684802 ,  1.41846674]), 'targetState': array([11, 26], dtype=int32), 'currentDistance': 19.464718767618248}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9312592426222218
{'scaleFactor': 20, 'currentTarget': array([11., 26.]), 'previousTarget': array([11., 26.]), 'currentState': array([10.93226255, 26.44069849,  2.08772118]), 'targetState': array([11, 26], dtype=int32), 'currentDistance': 0.44587388098017866}
episode index:4083
target Thresh 32.0
target distance 17.0
model initialize at round 4083
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 29.]), 'previousTarget': array([21., 29.]), 'currentState': array([ 4.21761286, 14.66884173,  0.43113041]), 'targetState': array([21, 29], dtype=int32), 'currentDistance': 22.06877014016439}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9312504470814864
{'scaleFactor': 20, 'currentTarget': array([21., 29.]), 'previousTarget': array([21., 29.]), 'currentState': array([20.85055944, 28.86777973,  1.19682925]), 'targetState': array([21, 29], dtype=int32), 'currentDistance': 0.19953615884401626}
episode index:4084
target Thresh 32.0
target distance 9.0
model initialize at round 4084
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  3.]), 'previousTarget': array([21.,  3.]), 'currentState': array([13.85547317, 10.38625202,  5.93840647]), 'targetState': array([21,  3], dtype=int32), 'currentDistance': 10.276233867627209}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9312552792975987
{'scaleFactor': 20, 'currentTarget': array([21.,  3.]), 'previousTarget': array([21.,  3.]), 'currentState': array([20.77495063,  3.32831874,  5.84738837]), 'targetState': array([21,  3], dtype=int32), 'currentDistance': 0.3980457447852818}
episode index:4085
target Thresh 32.0
target distance 14.0
model initialize at round 4085
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 16.]), 'previousTarget': array([20., 16.]), 'currentState': array([ 7.69491028, 25.38159358,  5.14740202]), 'targetState': array([20, 16], dtype=int32), 'currentDistance': 15.473510627557173}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9312531964329707
{'scaleFactor': 20, 'currentTarget': array([20., 16.]), 'previousTarget': array([20., 16.]), 'currentState': array([20.32535588, 15.6884178 ,  5.75535936]), 'targetState': array([20, 16], dtype=int32), 'currentDistance': 0.4504885295754769}
episode index:4086
target Thresh 32.0
target distance 12.0
model initialize at round 4086
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 29.]), 'previousTarget': array([19., 29.]), 'currentState': array([ 9.45231795, 17.66111146,  1.72789192]), 'targetState': array([19, 29], dtype=int32), 'currentDistance': 14.823246124491877}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9312533951487706
{'scaleFactor': 20, 'currentTarget': array([19., 29.]), 'previousTarget': array([19., 29.]), 'currentState': array([18.25461154, 28.20591294,  1.12560597]), 'targetState': array([19, 29], dtype=int32), 'currentDistance': 1.0891180896776793}
episode index:4087
target Thresh 32.0
target distance 20.0
model initialize at round 4087
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'previousTarget': array([19.,  3.]), 'currentState': array([22.1460521 , 21.10644531,  4.22959089]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 18.377731238367964}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9312490565607898
{'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'previousTarget': array([19.,  3.]), 'currentState': array([19.12255786,  3.46390526,  4.86746579]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 0.47982134489339817}
episode index:4088
target Thresh 32.0
target distance 8.0
model initialize at round 4088
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([10.29507631,  2.34309986,  3.87863064]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 8.321046130200697}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9312562336098089
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.53696608, 2.88719038, 3.29702857]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.5486880537737109}
episode index:4089
target Thresh 32.0
target distance 12.0
model initialize at round 4089
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([12.97622758,  2.66423126,  3.04845572]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 11.057208061852325}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9312587333447212
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.14219432, 4.26648659, 2.75365222]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.8982458960327276}
episode index:4090
target Thresh 32.0
target distance 20.0
model initialize at round 4090
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 27.]), 'previousTarget': array([ 7., 27.]), 'currentState': array([25.49660978,  6.24355705,  2.59776354]), 'targetState': array([ 7, 27], dtype=int32), 'currentDistance': 27.80205922384869}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9312434527481492
{'scaleFactor': 20, 'currentTarget': array([ 7., 27.]), 'previousTarget': array([ 7., 27.]), 'currentState': array([ 6.99738567, 27.11874505,  2.53824854]), 'targetState': array([ 7, 27], dtype=int32), 'currentDistance': 0.11877382553571303}
episode index:4091
target Thresh 32.0
target distance 23.0
model initialize at round 4091
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 17.]), 'previousTarget': array([ 4., 17.]), 'currentState': array([25.34374913,  4.29869906,  1.9531641 ]), 'targetState': array([ 4, 17], dtype=int32), 'currentDistance': 24.837042341422958}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9312324902405655
{'scaleFactor': 20, 'currentTarget': array([ 4., 17.]), 'previousTarget': array([ 4., 17.]), 'currentState': array([ 4.83640995, 16.39132333,  2.77050851]), 'targetState': array([ 4, 17], dtype=int32), 'currentDistance': 1.0344413459175317}
episode index:4092
target Thresh 32.0
target distance 10.0
model initialize at round 4092
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 27.]), 'previousTarget': array([13., 27.]), 'currentState': array([12.42435806, 18.62859661,  2.32588825]), 'targetState': array([13, 27], dtype=int32), 'currentDistance': 8.391171449442965}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9312396643230868
{'scaleFactor': 20, 'currentTarget': array([13., 27.]), 'previousTarget': array([13., 27.]), 'currentState': array([12.97517838, 26.37523285,  1.71952304]), 'targetState': array([13, 27], dtype=int32), 'currentDistance': 0.6252600311337103}
episode index:4093
target Thresh 32.0
target distance 16.0
model initialize at round 4093
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 13.]), 'previousTarget': array([19., 13.]), 'currentState': array([ 1.40953927, 27.44970667,  4.48469615]), 'targetState': array([19, 13], dtype=int32), 'currentDistance': 22.76440931482892}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9312287080962653
{'scaleFactor': 20, 'currentTarget': array([19., 13.]), 'previousTarget': array([19., 13.]), 'currentState': array([19.30379043, 12.43232489,  5.43168294]), 'targetState': array([19, 13], dtype=int32), 'currentDistance': 0.6438506481835768}
episode index:4094
target Thresh 32.0
target distance 6.0
model initialize at round 4094
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 25.]), 'previousTarget': array([16., 25.]), 'currentState': array([23.13574204, 25.75803438,  4.44314228]), 'targetState': array([16, 25], dtype=int32), 'currentDistance': 7.175892316160784}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9312358795985617
{'scaleFactor': 20, 'currentTarget': array([16., 25.]), 'previousTarget': array([16., 25.]), 'currentState': array([15.81525841, 25.18815884,  3.20014017]), 'targetState': array([16, 25], dtype=int32), 'currentDistance': 0.2636914941464966}
episode index:4095
target Thresh 32.0
target distance 3.0
model initialize at round 4095
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 12.]), 'previousTarget': array([25., 12.]), 'currentState': array([23.51416821, 12.25339749,  0.54305404]), 'targetState': array([25, 12], dtype=int32), 'currentDistance': 1.5072844465900024}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.931250226307644
{'scaleFactor': 20, 'currentTarget': array([25., 12.]), 'previousTarget': array([25., 12.]), 'currentState': array([25.43476339, 12.39407472,  5.8768698 ]), 'targetState': array([25, 12], dtype=int32), 'currentDistance': 0.5867828294029552}
episode index:4096
target Thresh 32.0
target distance 18.0
model initialize at round 4096
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 24.]), 'previousTarget': array([ 4., 24.]), 'currentState': array([21.35888515, 25.44392808,  3.31157351]), 'targetState': array([ 4, 24], dtype=int32), 'currentDistance': 17.41883525876034}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9312458980238206
{'scaleFactor': 20, 'currentTarget': array([ 4., 24.]), 'previousTarget': array([ 4., 24.]), 'currentState': array([ 3.49979409, 24.18690324,  3.03922085]), 'targetState': array([ 4, 24], dtype=int32), 'currentDistance': 0.533983874317073}
episode index:4097
target Thresh 32.0
target distance 7.0
model initialize at round 4097
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  5.]), 'previousTarget': array([22.,  5.]), 'currentState': array([25.00113547, 10.2723725 ,  4.30423333]), 'targetState': array([22,  5], dtype=int32), 'currentDistance': 6.066689865314366}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9312554278193249
{'scaleFactor': 20, 'currentTarget': array([22.,  5.]), 'previousTarget': array([22.,  5.]), 'currentState': array([22.03177365,  5.10201242,  4.43506497]), 'targetState': array([22,  5], dtype=int32), 'currentDistance': 0.10684614812008195}
episode index:4098
target Thresh 32.0
target distance 15.0
model initialize at round 4098
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 18.]), 'previousTarget': array([25., 18.]), 'currentState': array([1.11851027e+01, 9.32661265e+00, 4.74786758e-03]), 'targetState': array([25, 18], dtype=int32), 'currentDistance': 16.311929272201112}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9312533515242793
{'scaleFactor': 20, 'currentTarget': array([25., 18.]), 'previousTarget': array([25., 18.]), 'currentState': array([24.51445031, 17.85021942,  0.66032077]), 'targetState': array([25, 18], dtype=int32), 'currentDistance': 0.508126676691654}
episode index:4099
target Thresh 32.0
target distance 1.0
model initialize at round 4099
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 28.]), 'previousTarget': array([ 5., 28.]), 'currentState': array([ 4.32851567, 30.65603913,  2.77688134]), 'targetState': array([ 5, 28], dtype=int32), 'currentDistance': 2.7396049082832796}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9312652653409809
{'scaleFactor': 20, 'currentTarget': array([ 5., 28.]), 'previousTarget': array([ 5., 28.]), 'currentState': array([ 4.33556677, 28.53272611,  0.13738185]), 'targetState': array([ 5, 28], dtype=int32), 'currentDistance': 0.8516270496817212}
episode index:4100
target Thresh 32.0
target distance 8.0
model initialize at round 4100
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 4.]), 'previousTarget': array([8., 4.]), 'currentState': array([ 6.36100753, 10.08284041,  5.2877956 ]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 6.299781246487699}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9312747834425802
{'scaleFactor': 20, 'currentTarget': array([8., 4.]), 'previousTarget': array([8., 4.]), 'currentState': array([7.79133844, 4.30423841, 4.73006088]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 0.36891822654114154}
episode index:4101
target Thresh 32.0
target distance 6.0
model initialize at round 4101
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([13.08197989, 25.50505023,  4.64282238]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 4.5976340145474985}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9312866862257488
{'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([13.82771485, 21.63872786,  4.55421644]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 0.6615553243756672}
episode index:4102
target Thresh 32.0
target distance 14.0
model initialize at round 4102
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 23.]), 'previousTarget': array([13., 23.]), 'currentState': array([25.0537564 , 22.64579166,  3.06296861]), 'targetState': array([13, 23], dtype=int32), 'currentDistance': 12.058959607864722}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.931289170618431
{'scaleFactor': 20, 'currentTarget': array([13., 23.]), 'previousTarget': array([13., 23.]), 'currentState': array([13.09545137, 22.85326418,  3.25969545]), 'targetState': array([13, 23], dtype=int32), 'currentDistance': 0.17504960313406118}
episode index:4103
target Thresh 32.0
target distance 15.0
model initialize at round 4103
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 24.]), 'previousTarget': array([ 3., 24.]), 'currentState': array([17.16917672, 22.53640143,  3.18608999]), 'targetState': array([ 3, 24], dtype=int32), 'currentDistance': 14.244567021930317}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9312893597454507
{'scaleFactor': 20, 'currentTarget': array([ 3., 24.]), 'previousTarget': array([ 3., 24.]), 'currentState': array([ 3.28419896, 24.00176202,  3.24595521]), 'targetState': array([ 3, 24], dtype=int32), 'currentDistance': 0.2842044244360171}
episode index:4104
target Thresh 32.0
target distance 11.0
model initialize at round 4104
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 29.]), 'previousTarget': array([21., 29.]), 'currentState': array([11.39138369, 22.39631105,  0.48963279]), 'targetState': array([21, 29], dtype=int32), 'currentDistance': 11.659082946624409}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9312918422764264
{'scaleFactor': 20, 'currentTarget': array([21., 29.]), 'previousTarget': array([21., 29.]), 'currentState': array([21.37512832, 29.00008203,  0.76162682]), 'targetState': array([21, 29], dtype=int32), 'currentDistance': 0.3751283309002533}
episode index:4105
target Thresh 32.0
target distance 13.0
model initialize at round 4105
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 27.]), 'previousTarget': array([17., 27.]), 'currentState': array([ 5.67993514, 20.10102475,  1.07006377]), 'targetState': array([17, 27], dtype=int32), 'currentDistance': 13.2566861580907}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9312920306606521
{'scaleFactor': 20, 'currentTarget': array([17., 27.]), 'previousTarget': array([17., 27.]), 'currentState': array([17.57614099, 27.26009566,  0.62100278]), 'targetState': array([17, 27], dtype=int32), 'currentDistance': 0.6321298869372126}
episode index:4106
target Thresh 32.0
target distance 13.0
model initialize at round 4106
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 24.]), 'previousTarget': array([ 4., 24.]), 'currentState': array([ 2.33917804, 11.27213695,  1.96917915]), 'targetState': array([ 4, 24], dtype=int32), 'currentDistance': 12.835763609468492}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9312945113323687
{'scaleFactor': 20, 'currentTarget': array([ 4., 24.]), 'previousTarget': array([ 4., 24.]), 'currentState': array([ 3.59485744, 23.06581574,  1.28485736]), 'targetState': array([ 4, 24], dtype=int32), 'currentDistance': 1.0182537621999197}
episode index:4107
target Thresh 32.0
target distance 14.0
model initialize at round 4107
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([19.54243831, 14.05709138,  4.37125877]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 13.26996891379687}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9312946989751572
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.58510963,  1.46536076,  4.1038188 ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.6767371202340311}
episode index:4108
target Thresh 32.0
target distance 19.0
model initialize at round 4108
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 24.]), 'previousTarget': array([24., 24.]), 'currentState': array([ 6.86930831, 21.67994307,  0.16631231]), 'targetState': array([24, 24], dtype=int32), 'currentDistance': 17.287083677803402}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9312903725085007
{'scaleFactor': 20, 'currentTarget': array([24., 24.]), 'previousTarget': array([24., 24.]), 'currentState': array([24.71228829, 23.93304529,  6.28157182]), 'targetState': array([24, 24], dtype=int32), 'currentDistance': 0.7154282210916899}
episode index:4109
target Thresh 32.0
target distance 22.0
model initialize at round 4109
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 29.]), 'previousTarget': array([23., 29.]), 'currentState': array([11.82905982,  5.5354018 ,  6.2374835 ]), 'targetState': array([23, 29], dtype=int32), 'currentDistance': 25.988021725920504}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.93127728994171
{'scaleFactor': 20, 'currentTarget': array([23., 29.]), 'previousTarget': array([23., 29.]), 'currentState': array([22.65754825, 28.50361519,  1.2824489 ]), 'targetState': array([23, 29], dtype=int32), 'currentDistance': 0.6030514740706917}
episode index:4110
target Thresh 32.0
target distance 23.0
model initialize at round 4110
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 26.]), 'previousTarget': array([26., 26.]), 'currentState': array([27.13873565,  4.95124307,  1.86729288]), 'targetState': array([26, 26], dtype=int32), 'currentDistance': 21.07953716551592}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9312685477778367
{'scaleFactor': 20, 'currentTarget': array([26., 26.]), 'previousTarget': array([26., 26.]), 'currentState': array([25.99778115, 26.88024408,  1.43334474]), 'targetState': array([26, 26], dtype=int32), 'currentDistance': 0.8802468788963184}
episode index:4111
target Thresh 32.0
target distance 4.0
model initialize at round 4111
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 15.]), 'previousTarget': array([26., 15.]), 'currentState': array([22.7454468 , 18.50887281,  0.10192633]), 'targetState': array([26, 15], dtype=int32), 'currentDistance': 4.785844218021279}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9312804231310037
{'scaleFactor': 20, 'currentTarget': array([26., 15.]), 'previousTarget': array([26., 15.]), 'currentState': array([25.53394523, 15.97194551,  5.53753654]), 'targetState': array([26, 15], dtype=int32), 'currentDistance': 1.0779077519287241}
episode index:4112
target Thresh 32.0
target distance 12.0
model initialize at round 4112
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 14.]), 'previousTarget': array([27., 14.]), 'currentState': array([16.75905674, 22.23341581,  5.37071894]), 'targetState': array([27, 14], dtype=int32), 'currentDistance': 13.140245610903305}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9312829030060996
{'scaleFactor': 20, 'currentTarget': array([27., 14.]), 'previousTarget': array([27., 14.]), 'currentState': array([26.15002394, 14.82881991,  5.64767753]), 'targetState': array([27, 14], dtype=int32), 'currentDistance': 1.1871822688473002}
episode index:4113
target Thresh 32.0
target distance 5.0
model initialize at round 4113
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 24.]), 'previousTarget': array([15., 24.]), 'currentState': array([20.58957687, 22.42367914,  4.06030107]), 'targetState': array([15, 24], dtype=int32), 'currentDistance': 5.807594773207994}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9312923867438231
{'scaleFactor': 20, 'currentTarget': array([15., 24.]), 'previousTarget': array([15., 24.]), 'currentState': array([15.33232895, 23.75382059,  2.48913103]), 'targetState': array([15, 24], dtype=int32), 'currentDistance': 0.4135780834000763}
episode index:4114
target Thresh 32.0
target distance 10.0
model initialize at round 4114
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 14.]), 'previousTarget': array([ 3., 14.]), 'currentState': array([12.12568789, 18.43804257,  3.1270684 ]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 10.147630332183265}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9312971735392438
{'scaleFactor': 20, 'currentTarget': array([ 3., 14.]), 'previousTarget': array([ 3., 14.]), 'currentState': array([ 3.23843172, 14.1317619 ,  3.8720169 ]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 0.27241674750079253}
episode index:4115
target Thresh 32.0
target distance 16.0
model initialize at round 4115
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([ 1.96666609, 17.67161409,  5.06127787]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 14.671651957992026}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9312973601705284
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.02500603, 3.74736056, 4.94346497]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.7477787855192568}
episode index:4116
target Thresh 32.0
target distance 12.0
model initialize at round 4116
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([4.15657722, 7.77741372, 0.1968643 ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 10.871255199542128}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9313021434325469
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.08627975,  7.12577907,  0.14841082]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.9223367445765276}
episode index:4117
target Thresh 32.0
target distance 11.0
model initialize at round 4117
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([21.72926597, 25.03562786,  4.8042202 ]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 9.03968294114897}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9313069243714657
{'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([22.04623398, 15.06979435,  4.5345977 ]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 0.9313539280164298}
episode index:4118
target Thresh 32.0
target distance 14.0
model initialize at round 4118
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 20.]), 'previousTarget': array([11., 20.]), 'currentState': array([23.64835531, 14.23795833,  3.12449193]), 'targetState': array([11, 20], dtype=int32), 'currentDistance': 13.898993356807026}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9313071084995393
{'scaleFactor': 20, 'currentTarget': array([11., 20.]), 'previousTarget': array([11., 20.]), 'currentState': array([11.06103095, 20.14091556,  2.95610481]), 'targetState': array([11, 20], dtype=int32), 'currentDistance': 0.1535642256288324}
episode index:4119
target Thresh 32.0
target distance 14.0
model initialize at round 4119
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 28.]), 'previousTarget': array([16., 28.]), 'currentState': array([ 3.65637573, 13.70199409,  0.83198994]), 'targetState': array([16, 28], dtype=int32), 'currentDistance': 18.889098259164953}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9313027905721083
{'scaleFactor': 20, 'currentTarget': array([16., 28.]), 'previousTarget': array([16., 28.]), 'currentState': array([15.3506903 , 27.33647103,  1.13061138]), 'targetState': array([16, 28], dtype=int32), 'currentDistance': 0.9283715773970671}
episode index:4120
target Thresh 32.0
target distance 18.0
model initialize at round 4120
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 24.]), 'previousTarget': array([21., 24.]), 'currentState': array([17.66181971,  6.2659762 ,  1.16870516]), 'targetState': array([21, 24], dtype=int32), 'currentDistance': 18.04547166952741}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9312984747402498
{'scaleFactor': 20, 'currentTarget': array([21., 24.]), 'previousTarget': array([21., 24.]), 'currentState': array([21.0718556 , 23.86920223,  1.77796366]), 'targetState': array([21, 24], dtype=int32), 'currentDistance': 0.14923566173762176}
episode index:4121
target Thresh 32.0
target distance 6.0
model initialize at round 4121
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 20.]), 'previousTarget': array([22., 20.]), 'currentState': array([17.58755477, 20.44071395,  0.6709128 ]), 'targetState': array([22, 20], dtype=int32), 'currentDistance': 4.434399811150313}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9313103140234279
{'scaleFactor': 20, 'currentTarget': array([22., 20.]), 'previousTarget': array([22., 20.]), 'currentState': array([21.40215832, 20.22105706,  0.05612301]), 'targetState': array([22, 20], dtype=int32), 'currentDistance': 0.6374016757009691}
episode index:4122
target Thresh 32.0
target distance 22.0
model initialize at round 4122
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 2.]), 'previousTarget': array([9., 2.]), 'currentState': array([18.86217089, 25.24005363,  3.32323122]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 25.246039440826685}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9312972678698931
{'scaleFactor': 20, 'currentTarget': array([9., 2.]), 'previousTarget': array([9., 2.]), 'currentState': array([8.66495587, 1.70496613, 4.28607169]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 0.446429788955138}
episode index:4123
target Thresh 32.0
target distance 14.0
model initialize at round 4123
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 25.]), 'previousTarget': array([22., 25.]), 'currentState': array([9.67291669, 9.45741411, 6.13372993]), 'targetState': array([22, 25], dtype=int32), 'currentDistance': 19.837564339776115}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9312907413924777
{'scaleFactor': 20, 'currentTarget': array([22., 25.]), 'previousTarget': array([22., 25.]), 'currentState': array([21.6552941 , 24.66964101,  1.16238041]), 'targetState': array([22, 25], dtype=int32), 'currentDistance': 0.47745075395403797}
episode index:4124
target Thresh 32.0
target distance 9.0
model initialize at round 4124
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 28.]), 'previousTarget': array([16., 28.]), 'currentState': array([23.29529176, 28.26046668,  2.88494539]), 'targetState': array([16, 28], dtype=int32), 'currentDistance': 7.299940051969649}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9312978457000188
{'scaleFactor': 20, 'currentTarget': array([16., 28.]), 'previousTarget': array([16., 28.]), 'currentState': array([15.33045189, 28.13221234,  3.00646701]), 'targetState': array([16, 28], dtype=int32), 'currentDistance': 0.6824769398973799}
episode index:4125
target Thresh 32.0
target distance 11.0
model initialize at round 4125
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 6.]), 'previousTarget': array([9., 6.]), 'currentState': array([11.18150014, 15.16217073,  4.11789989]), 'targetState': array([9, 6], dtype=int32), 'currentDistance': 9.418296841181187}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.931302618410683
{'scaleFactor': 20, 'currentTarget': array([9., 6.]), 'previousTarget': array([9., 6.]), 'currentState': array([8.66755949, 5.5412226 , 4.29829008]), 'targetState': array([9, 6], dtype=int32), 'currentDistance': 0.5665627873285255}
episode index:4126
target Thresh 32.0
target distance 19.0
model initialize at round 4126
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  2.]), 'previousTarget': array([12.,  2.]), 'currentState': array([11.58817761, 19.42315658,  4.05941343]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 17.42802291631576}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9312983088950718
{'scaleFactor': 20, 'currentTarget': array([12.,  2.]), 'previousTarget': array([12.,  2.]), 'currentState': array([12.04366164,  1.67753029,  4.85912152]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 0.3254121269981866}
episode index:4127
target Thresh 32.0
target distance 26.0
model initialize at round 4127
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  2.]), 'previousTarget': array([18.,  2.]), 'currentState': array([ 5.68241024, 27.95659656,  5.24739266]), 'targetState': array([18,  2], dtype=int32), 'currentDistance': 28.730957559816957}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.931283155674111
{'scaleFactor': 20, 'currentTarget': array([18.,  2.]), 'previousTarget': array([18.,  2.]), 'currentState': array([17.41359705,  2.72359333,  5.6544813 ]), 'targetState': array([18,  2], dtype=int32), 'currentDistance': 0.9313730300228047}
episode index:4128
target Thresh 32.0
target distance 3.0
model initialize at round 4128
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 24.]), 'previousTarget': array([19., 24.]), 'currentState': array([18.78277927, 25.3250259 ,  5.58326924]), 'targetState': array([19, 24], dtype=int32), 'currentDistance': 1.342713106829683}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9312973762709446
{'scaleFactor': 20, 'currentTarget': array([19., 24.]), 'previousTarget': array([19., 24.]), 'currentState': array([19.53946798, 23.55326708,  4.6393373 ]), 'targetState': array([19, 24], dtype=int32), 'currentDistance': 0.7004255907835023}
episode index:4129
target Thresh 32.0
target distance 12.0
model initialize at round 4129
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 26.]), 'previousTarget': array([18., 26.]), 'currentState': array([19.72217975, 15.97932814,  1.64790917]), 'targetState': array([18, 26], dtype=int32), 'currentDistance': 10.167584160069254}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9313021444727918
{'scaleFactor': 20, 'currentTarget': array([18., 26.]), 'previousTarget': array([18., 26.]), 'currentState': array([18.11331747, 25.77821481,  2.20161616]), 'targetState': array([18, 26], dtype=int32), 'currentDistance': 0.24905726158600588}
episode index:4130
target Thresh 32.0
target distance 10.0
model initialize at round 4130
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 12.]), 'previousTarget': array([16., 12.]), 'currentState': array([7.68445235, 4.05586834, 0.75154319]), 'targetState': array([16, 12], dtype=int32), 'currentDistance': 11.500328709182348}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9313046082842001
{'scaleFactor': 20, 'currentTarget': array([16., 12.]), 'previousTarget': array([16., 12.]), 'currentState': array([16.41563793, 12.27363708,  0.7238129 ]), 'targetState': array([16, 12], dtype=int32), 'currentDistance': 0.49762650833407984}
episode index:4131
target Thresh 32.0
target distance 11.0
model initialize at round 4131
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 16.]), 'previousTarget': array([26., 16.]), 'currentState': array([21.76175899,  6.13980142,  1.38756609]), 'targetState': array([26, 16], dtype=int32), 'currentDistance': 10.73248353633069}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9313093724278633
{'scaleFactor': 20, 'currentTarget': array([26., 16.]), 'previousTarget': array([26., 16.]), 'currentState': array([25.52506334, 15.34904672,  1.36759132]), 'targetState': array([26, 16], dtype=int32), 'currentDistance': 0.8057946352847577}
episode index:4132
target Thresh 32.0
target distance 5.0
model initialize at round 4132
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  5.]), 'previousTarget': array([11.,  5.]), 'currentState': array([6.3175572 , 8.65273879, 0.37096977]), 'targetState': array([11,  5], dtype=int32), 'currentDistance': 5.938667461472894}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9313188061630611
{'scaleFactor': 20, 'currentTarget': array([11.,  5.]), 'previousTarget': array([11.,  5.]), 'currentState': array([10.81063949,  5.30313509,  5.11206913]), 'targetState': array([11,  5], dtype=int32), 'currentDistance': 0.35741891624135524}
episode index:4133
target Thresh 32.0
target distance 3.0
model initialize at round 4133
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  7.]), 'previousTarget': array([27.,  7.]), 'currentState': array([27.46282076,  3.16780802,  0.4927656 ]), 'targetState': array([27,  7], dtype=int32), 'currentDistance': 3.860038660094872}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9313306061615703
{'scaleFactor': 20, 'currentTarget': array([27.,  7.]), 'previousTarget': array([27.,  7.]), 'currentState': array([26.85616038,  6.60032718,  1.45384794]), 'targetState': array([27,  7], dtype=int32), 'currentDistance': 0.4247684049079418}
episode index:4134
target Thresh 32.0
target distance 9.0
model initialize at round 4134
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 29.]), 'previousTarget': array([19., 29.]), 'currentState': array([ 9.1623162 , 28.45968282,  1.08178902]), 'targetState': array([19, 29], dtype=int32), 'currentDistance': 9.852510604027383}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.931335360561507
{'scaleFactor': 20, 'currentTarget': array([19., 29.]), 'previousTarget': array([19., 29.]), 'currentState': array([18.80614604, 28.98943238,  0.25119077]), 'targetState': array([19, 29], dtype=int32), 'currentDistance': 0.19414178342238494}
episode index:4135
target Thresh 32.0
target distance 17.0
model initialize at round 4135
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 7.]), 'previousTarget': array([5., 7.]), 'currentState': array([17.16072973, 22.78135543,  4.46345158]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 19.92321577386098}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.931328843809681
{'scaleFactor': 20, 'currentTarget': array([5., 7.]), 'previousTarget': array([5., 7.]), 'currentState': array([5.05509135, 7.04604191, 4.34084751]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 0.07179773722494513}
episode index:4136
target Thresh 32.0
target distance 22.0
model initialize at round 4136
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 23.]), 'previousTarget': array([27., 23.]), 'currentState': array([ 6.94057863, 26.52403021,  6.11898434]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 20.36661912453634}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9313223302083271
{'scaleFactor': 20, 'currentTarget': array([27., 23.]), 'previousTarget': array([27., 23.]), 'currentState': array([26.57342075, 23.08224205,  0.13294733]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 0.4344348182077696}
episode index:4137
target Thresh 32.0
target distance 6.0
model initialize at round 4137
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([6.93772174, 5.31000698, 1.24821424]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 7.069079045893089}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9313294045630375
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.30367495,  4.83988476,  0.06247008]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.3433006934845801}
episode index:4138
target Thresh 32.0
target distance 21.0
model initialize at round 4138
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 13.]), 'previousTarget': array([23., 13.]), 'currentState': array([2.69101357, 9.53456454, 0.13769341]), 'targetState': array([23, 13], dtype=int32), 'currentDistance': 20.60252830935471}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9313228939736309
{'scaleFactor': 20, 'currentTarget': array([23., 13.]), 'previousTarget': array([23., 13.]), 'currentState': array([22.31172804, 13.05910306,  0.32823748]), 'targetState': array([23, 13], dtype=int32), 'currentDistance': 0.6908049396311275}
episode index:4139
target Thresh 32.0
target distance 8.0
model initialize at round 4139
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([23.23184627, 14.50255954,  3.22841311]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 7.650935189982438}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9313299647746034
{'scaleFactor': 20, 'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([15.78480052, 17.09183297,  3.18672021]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 0.2339745887171977}
episode index:4140
target Thresh 32.0
target distance 20.0
model initialize at round 4140
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 29.]), 'previousTarget': array([21., 29.]), 'currentState': array([11.70080098, 10.65616063,  0.73952675]), 'targetState': array([21, 29], dtype=int32), 'currentDistance': 20.5662720317217}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9313234571943654
{'scaleFactor': 20, 'currentTarget': array([21., 29.]), 'previousTarget': array([21., 29.]), 'currentState': array([20.57810375, 28.43910194,  1.47766811]), 'targetState': array([21, 29], dtype=int32), 'currentDistance': 0.7018568790480937}
episode index:4141
target Thresh 32.0
target distance 13.0
model initialize at round 4141
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([16.67357204, 21.45770016,  4.11415482]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 17.214240185557028}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9313191582543096
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.37904626, 10.63208613,  3.48284854]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.7217646141952323}
episode index:4142
target Thresh 32.0
target distance 4.0
model initialize at round 4142
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 19.]), 'previousTarget': array([ 5., 19.]), 'currentState': array([ 7.36862164, 20.5858422 ,  4.39994419]), 'targetState': array([ 5, 19], dtype=int32), 'currentDistance': 2.8504848656499355}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9313333221070118
{'scaleFactor': 20, 'currentTarget': array([ 5., 19.]), 'previousTarget': array([ 5., 19.]), 'currentState': array([ 5.94007121, 19.3276597 ,  3.31655145]), 'targetState': array([ 5, 19], dtype=int32), 'currentDistance': 0.9955374210674546}
episode index:4143
target Thresh 32.0
target distance 11.0
model initialize at round 4143
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 26.]), 'previousTarget': array([14., 26.]), 'currentState': array([ 3.1194303 , 22.32127296,  5.79341269]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 11.485635798258675}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9313357706657217
{'scaleFactor': 20, 'currentTarget': array([14., 26.]), 'previousTarget': array([14., 26.]), 'currentState': array([14.23884353, 26.09238991,  0.56451037]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 0.2560900781003893}
episode index:4144
target Thresh 32.0
target distance 12.0
model initialize at round 4144
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'previousTarget': array([11., 19.]), 'currentState': array([23.7985729 , 20.51855834,  4.19678736]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 12.8883469752273}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9313359466795315
{'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'previousTarget': array([11., 19.]), 'currentState': array([10.18721798, 19.04455867,  2.96493024]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 0.8140025064138626}
episode index:4145
target Thresh 32.0
target distance 20.0
model initialize at round 4145
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 19.]), 'previousTarget': array([ 5., 19.]), 'currentState': array([24.63908807, 17.35618411,  3.48626089]), 'targetState': array([ 5, 19], dtype=int32), 'currentDistance': 19.707762710257487}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9313294455045025
{'scaleFactor': 20, 'currentTarget': array([ 5., 19.]), 'previousTarget': array([ 5., 19.]), 'currentState': array([ 4.83874914, 19.10060872,  3.24721993]), 'targetState': array([ 5, 19], dtype=int32), 'currentDistance': 0.1900630257374795}
episode index:4146
target Thresh 32.0
target distance 13.0
model initialize at round 4146
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([20.72114503, 23.80625985,  5.24624598]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 16.701593998796856}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9313273753933193
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.9589889 , 11.89642107,  3.91454492]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.3127187221268588}
episode index:4147
target Thresh 32.0
target distance 4.0
model initialize at round 4147
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  4.]), 'previousTarget': array([22.,  4.]), 'currentState': array([24.13376356,  3.65077351,  2.7652607 ]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 2.1621531125110542}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9313415201919226
{'scaleFactor': 20, 'currentTarget': array([22.,  4.]), 'previousTarget': array([22.,  4.]), 'currentState': array([22.17145276,  3.91585187,  3.2542707 ]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 0.1909894172912661}
episode index:4148
target Thresh 32.0
target distance 4.0
model initialize at round 4148
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 21.]), 'previousTarget': array([ 2., 21.]), 'currentState': array([ 2.46533875, 23.18742991,  4.32999015]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 2.2363787176894494}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9313556581721125
{'scaleFactor': 20, 'currentTarget': array([ 2., 21.]), 'previousTarget': array([ 2., 21.]), 'currentState': array([ 2.1081776 , 21.23359812,  4.73725227]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 0.25743052641132785}
episode index:4149
target Thresh 32.0
target distance 14.0
model initialize at round 4149
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  2.]), 'previousTarget': array([27.,  2.]), 'currentState': array([14.96259836,  4.20739796,  6.10325212]), 'targetState': array([27,  2], dtype=int32), 'currentDistance': 12.238122566991867}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9313580978085532
{'scaleFactor': 20, 'currentTarget': array([27.,  2.]), 'previousTarget': array([27.,  2.]), 'currentState': array([26.74232722,  2.18868429,  0.06878196]), 'targetState': array([27,  2], dtype=int32), 'currentDistance': 0.3193697299815127}
episode index:4150
target Thresh 32.0
target distance 17.0
model initialize at round 4150
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 20.]), 'previousTarget': array([24., 20.]), 'currentState': array([24.42368271,  4.70730819,  1.1057744 ]), 'targetState': array([24, 20], dtype=int32), 'currentDistance': 15.298559734210565}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9313560227896708
{'scaleFactor': 20, 'currentTarget': array([24., 20.]), 'previousTarget': array([24., 20.]), 'currentState': array([24.11784288, 20.56373923,  1.34856476]), 'targetState': array([24, 20], dtype=int32), 'currentDistance': 0.5759243552283646}
episode index:4151
target Thresh 32.0
target distance 11.0
model initialize at round 4151
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 14.]), 'previousTarget': array([24., 14.]), 'currentState': array([27.97785913, 23.63026294,  4.32238488]), 'targetState': array([24, 14], dtype=int32), 'currentDistance': 10.41946868394891}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9313607516015953
{'scaleFactor': 20, 'currentTarget': array([24., 14.]), 'previousTarget': array([24., 14.]), 'currentState': array([24.31077772, 14.38217873,  4.70648751]), 'targetState': array([24, 14], dtype=int32), 'currentDistance': 0.4925884408428405}
episode index:4152
target Thresh 32.0
target distance 3.0
model initialize at round 4152
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([12.88359895, 13.75089254,  2.52027464]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 5.557528484516198}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9313701275342701
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.21571724,  9.35979863,  4.8482703 ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.4195104077964109}
episode index:4153
target Thresh 32.0
target distance 5.0
model initialize at round 4153
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([ 9.9427561, 13.3180038,  5.6883688]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 5.241219067308675}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9313818583653886
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.09719093, 10.88251036,  5.82868081]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.262493067430457}
episode index:4154
target Thresh 32.0
target distance 3.0
model initialize at round 4154
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([7.09786153, 5.72442635, 0.1498658 ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 3.7878111793528015}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9313935835498975
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([8.52874061, 8.98644882, 0.56822544]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.47145418509376474}
episode index:4155
target Thresh 32.0
target distance 12.0
model initialize at round 4155
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 16.]), 'previousTarget': array([14., 16.]), 'currentState': array([25.47579513, 14.59924899,  2.8975423 ]), 'targetState': array([14, 16], dtype=int32), 'currentDistance': 11.560967831333445}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9313960105387933
{'scaleFactor': 20, 'currentTarget': array([14., 16.]), 'previousTarget': array([14., 16.]), 'currentState': array([13.5995119 , 16.22037534,  3.01693605]), 'targetState': array([14, 16], dtype=int32), 'currentDistance': 0.4571170624488574}
episode index:4156
target Thresh 32.0
target distance 5.0
model initialize at round 4156
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 10.]), 'previousTarget': array([20., 10.]), 'currentState': array([20.17548461,  6.6639041 ,  1.36197293]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 3.3407081163571206}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9314077266777063
{'scaleFactor': 20, 'currentTarget': array([20., 10.]), 'previousTarget': array([20., 10.]), 'currentState': array([19.83525274, 10.56664547,  1.25395298]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 0.5901090951220587}
episode index:4157
target Thresh 32.0
target distance 18.0
model initialize at round 4157
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([21.44480478, 20.07031346,  4.45026267]), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 22.305738093711206}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9313990519609148
{'scaleFactor': 20, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.35336211, 5.2898646 , 4.11717713]), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.45704076556928197}
episode index:4158
target Thresh 32.0
target distance 10.0
model initialize at round 4158
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([16.57568608, 10.10349683,  2.69336987]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 11.196782575443521}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9314014758843194
{'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([14.03113645, 21.57058968,  1.68233365]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 0.5714385873811163}
episode index:4159
target Thresh 32.0
target distance 6.0
model initialize at round 4159
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  8.]), 'previousTarget': array([21.,  8.]), 'currentState': array([23.14194432,  3.72374545,  2.51288009]), 'targetState': array([21,  8], dtype=int32), 'currentDistance': 4.7827061824300205}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9314131822603089
{'scaleFactor': 20, 'currentTarget': array([21.,  8.]), 'previousTarget': array([21.,  8.]), 'currentState': array([21.22939521,  7.13969598,  2.34321344]), 'targetState': array([21,  8], dtype=int32), 'currentDistance': 0.8903623819763461}
episode index:4160
target Thresh 32.0
target distance 25.0
model initialize at round 4160
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([13.90075723, 25.57824204,  4.26729918]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 25.650111156493118}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9314002305277299
{'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([23.9099117 ,  1.9454512 ,  5.50991179]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 0.10531606567606826}
episode index:4161
target Thresh 32.0
target distance 9.0
model initialize at round 4161
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([19.98590722,  7.21002482,  0.20149335]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 7.238888634225744}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9314072453714282
{'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([27.70655232,  9.12368055,  6.23901488]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 0.7172956577682875}
episode index:4162
target Thresh 32.0
target distance 2.0
model initialize at round 4162
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([10.69506917, 10.02499889,  5.39081085]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.30595384277368104}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9314237221320883
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([10.69506917, 10.02499889,  5.39081085]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.30595384277368104}
episode index:4163
target Thresh 32.0
target distance 18.0
model initialize at round 4163
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 8.]), 'previousTarget': array([8., 8.]), 'currentState': array([24.3970309 ,  6.51271645,  3.84202194]), 'targetState': array([8, 8], dtype=int32), 'currentDistance': 16.46434434727563}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9314216378314869
{'scaleFactor': 20, 'currentTarget': array([8., 8.]), 'previousTarget': array([8., 8.]), 'currentState': array([8.73195312, 7.96033316, 3.51747856]), 'targetState': array([8, 8], dtype=int32), 'currentDistance': 0.7330271698170134}
episode index:4164
target Thresh 32.0
target distance 11.0
model initialize at round 4164
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 26.]), 'previousTarget': array([ 9., 26.]), 'currentState': array([21.27778842, 21.09528299,  1.71864241]), 'targetState': array([ 9, 26], dtype=int32), 'currentDistance': 13.221207868970474}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.931421792383726
{'scaleFactor': 20, 'currentTarget': array([ 9., 26.]), 'previousTarget': array([ 9., 26.]), 'currentState': array([ 8.88316345, 26.15953778,  3.00429197]), 'targetState': array([ 9, 26], dtype=int32), 'currentDistance': 0.19774499401043774}
episode index:4165
target Thresh 32.0
target distance 12.0
model initialize at round 4165
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([15.93571065, 28.68174165,  2.61900556]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 13.67870096741796}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9314219468617682
{'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([ 4.2993744 , 22.08120131,  4.00422215]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 0.31019136461393243}
episode index:4166
target Thresh 32.0
target distance 25.0
model initialize at round 4166
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  3.]), 'previousTarget': array([20.,  3.]), 'currentState': array([18.78299831, 26.21376953,  4.73406768]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 23.245648815800635}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9314111388283758
{'scaleFactor': 20, 'currentTarget': array([20.,  3.]), 'previousTarget': array([20.,  3.]), 'currentState': array([19.95598446,  2.28458388,  4.42251113]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 0.7167688566961489}
episode index:4167
target Thresh 32.0
target distance 15.0
model initialize at round 4167
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 20.]), 'previousTarget': array([ 8., 20.]), 'currentState': array([21.47803267,  9.28166615,  2.57257175]), 'targetState': array([ 8, 20], dtype=int32), 'currentDistance': 17.22033812631909}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9314068456682643
{'scaleFactor': 20, 'currentTarget': array([ 8., 20.]), 'previousTarget': array([ 8., 20.]), 'currentState': array([ 7.47631431, 20.53246918,  2.26624049]), 'targetState': array([ 8, 20], dtype=int32), 'currentDistance': 0.7468400949059684}
episode index:4168
target Thresh 32.0
target distance 24.0
model initialize at round 4168
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([23.49243225, 28.2519174 ,  4.61221027]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 24.429248941101076}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9313960464420824
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.15159634,  5.46009371,  4.5655054 ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.48442509912986303}
episode index:4169
target Thresh 32.0
target distance 7.0
model initialize at round 4169
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 27.]), 'previousTarget': array([ 8., 27.]), 'currentState': array([ 5.75109627, 21.66446238,  0.70923638]), 'targetState': array([ 8, 27], dtype=int32), 'currentDistance': 5.790123460800571}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9314053756875399
{'scaleFactor': 20, 'currentTarget': array([ 8., 27.]), 'previousTarget': array([ 8., 27.]), 'currentState': array([ 8.11430763, 27.03075558,  1.36035931]), 'targetState': array([ 8, 27], dtype=int32), 'currentDistance': 0.11837288123879124}
episode index:4170
target Thresh 32.0
target distance 5.0
model initialize at round 4170
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 14.]), 'previousTarget': array([20., 14.]), 'currentState': array([19.90122973, 10.97972457,  1.38684019]), 'targetState': array([20, 14], dtype=int32), 'currentDistance': 3.021890013112758}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.931417050255824
{'scaleFactor': 20, 'currentTarget': array([20., 14.]), 'previousTarget': array([20., 14.]), 'currentState': array([19.88670861, 14.93284904,  1.24672669]), 'targetState': array([20, 14], dtype=int32), 'currentDistance': 0.9397032929777}
episode index:4171
target Thresh 32.0
target distance 13.0
model initialize at round 4171
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 13.]), 'previousTarget': array([11., 13.]), 'currentState': array([25.44517172, 22.13752218,  4.73512301]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 17.092609447843287}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9314127597949485
{'scaleFactor': 20, 'currentTarget': array([11., 13.]), 'previousTarget': array([11., 13.]), 'currentState': array([10.39132911, 12.90248724,  3.55892495]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 0.6164324746734939}
episode index:4172
target Thresh 32.0
target distance 17.0
model initialize at round 4172
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 28.]), 'previousTarget': array([22., 28.]), 'currentState': array([ 6.73551561, 15.99025018,  0.59373079]), 'targetState': array([22, 28], dtype=int32), 'currentDistance': 19.422630470833354}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9314062822764281
{'scaleFactor': 20, 'currentTarget': array([22., 28.]), 'previousTarget': array([22., 28.]), 'currentState': array([22.48133095, 28.25978246,  0.74677433]), 'targetState': array([22, 28], dtype=int32), 'currentDistance': 0.5469610706593916}
episode index:4173
target Thresh 32.0
target distance 7.0
model initialize at round 4173
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 21.]), 'previousTarget': array([ 8., 21.]), 'currentState': array([ 7.52452934, 28.71288034,  5.71058423]), 'targetState': array([ 8, 21], dtype=int32), 'currentDistance': 7.7275219530671295}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9314132755030029
{'scaleFactor': 20, 'currentTarget': array([ 8., 21.]), 'previousTarget': array([ 8., 21.]), 'currentState': array([ 8.01312736, 21.05248863,  5.02889204]), 'targetState': array([ 8, 21], dtype=int32), 'currentDistance': 0.05410530765876157}
episode index:4174
target Thresh 32.0
target distance 13.0
model initialize at round 4174
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 27.]), 'previousTarget': array([25., 27.]), 'currentState': array([13.63157919, 18.58728043,  0.76224011]), 'targetState': array([25, 27], dtype=int32), 'currentDistance': 14.142660371641734}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9314134316880098
{'scaleFactor': 20, 'currentTarget': array([25., 27.]), 'previousTarget': array([25., 27.]), 'currentState': array([24.77027382, 26.95528419,  1.06009772]), 'targetState': array([25, 27], dtype=int32), 'currentDistance': 0.23403765119148456}
episode index:4175
target Thresh 32.0
target distance 22.0
model initialize at round 4175
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 29.]), 'previousTarget': array([17., 29.]), 'currentState': array([7.64740232, 6.65582898, 0.8040449 ]), 'targetState': array([17, 29], dtype=int32), 'currentDistance': 24.222573397776266}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9314026489868671
{'scaleFactor': 20, 'currentTarget': array([17., 29.]), 'previousTarget': array([17., 29.]), 'currentState': array([16.66517592, 28.72861629,  1.63853114]), 'targetState': array([17, 29], dtype=int32), 'currentDistance': 0.4309945292616166}
episode index:4176
target Thresh 32.0
target distance 11.0
model initialize at round 4176
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 19.]), 'previousTarget': array([20., 19.]), 'currentState': array([10.65098086, 24.87131058,  5.6678119 ]), 'targetState': array([20, 19], dtype=int32), 'currentDistance': 11.03976661336317}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9314073383335066
{'scaleFactor': 20, 'currentTarget': array([20., 19.]), 'previousTarget': array([20., 19.]), 'currentState': array([19.2253648 , 19.76169888,  5.9565939 ]), 'targetState': array([20, 19], dtype=int32), 'currentDistance': 1.0863907583118964}
episode index:4177
target Thresh 32.0
target distance 7.0
model initialize at round 4177
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([21.14630306,  3.61736988,  3.1974417 ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 5.3287992350565645}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9314166470126992
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.37153386,  4.79153759,  2.53309035]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.6621376525380427}
episode index:4178
target Thresh 32.0
target distance 6.0
model initialize at round 4178
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 26.]), 'previousTarget': array([ 7., 26.]), 'currentState': array([12.27308264, 18.48211356,  3.25576258]), 'targetState': array([ 7, 26], dtype=int32), 'currentDistance': 9.1828109509848}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.931421330765484
{'scaleFactor': 20, 'currentTarget': array([ 7., 26.]), 'previousTarget': array([ 7., 26.]), 'currentState': array([ 6.55291943, 26.19085347,  1.77131084]), 'targetState': array([ 7, 26], dtype=int32), 'currentDistance': 0.48611323722424227}
episode index:4179
target Thresh 32.0
target distance 9.0
model initialize at round 4179
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 19.]), 'previousTarget': array([ 8., 19.]), 'currentState': array([4.32195229, 9.87137667, 2.20809364]), 'targetState': array([ 8, 19], dtype=int32), 'currentDistance': 9.84173759146868}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9314260122772386
{'scaleFactor': 20, 'currentTarget': array([ 8., 19.]), 'previousTarget': array([ 8., 19.]), 'currentState': array([ 7.79616113, 18.89604542,  0.91529283]), 'targetState': array([ 8, 19], dtype=int32), 'currentDistance': 0.22881616790057843}
episode index:4180
target Thresh 32.0
target distance 11.0
model initialize at round 4180
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([24.60752138,  9.36343389,  3.46701574]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 11.469920017655175}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9314284169979092
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.52170794,  4.80817035,  3.38544723]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.5153269934908585}
episode index:4181
target Thresh 32.0
target distance 5.0
model initialize at round 4181
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 10.]), 'previousTarget': array([27., 10.]), 'currentState': array([23.57165995,  7.6065623 ,  1.37551099]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 4.181155280628723}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9314400553486989
{'scaleFactor': 20, 'currentTarget': array([27., 10.]), 'previousTarget': array([27., 10.]), 'currentState': array([26.58848299,  9.95367671,  0.82982659]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 0.41411604217250514}
episode index:4182
target Thresh 32.0
target distance 8.0
model initialize at round 4182
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 14.]), 'previousTarget': array([13., 14.]), 'currentState': array([16.66912159, 23.03012171,  3.49289739]), 'targetState': array([13, 14], dtype=int32), 'currentDistance': 9.747079117663139}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9314447290265739
{'scaleFactor': 20, 'currentTarget': array([13., 14.]), 'previousTarget': array([13., 14.]), 'currentState': array([13.01452717, 14.04341323,  4.79439887]), 'targetState': array([13, 14], dtype=int32), 'currentDistance': 0.04577933074816493}
episode index:4183
target Thresh 32.0
target distance 27.0
model initialize at round 4183
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 2.]), 'previousTarget': array([8., 2.]), 'currentState': array([ 5.62123798, 29.96509242,  3.54088664]), 'targetState': array([8, 2], dtype=int32), 'currentDistance': 28.066081006180543}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9314297436259388
{'scaleFactor': 20, 'currentTarget': array([8., 2.]), 'previousTarget': array([8., 2.]), 'currentState': array([8.23525646, 2.73896686, 5.08204282]), 'targetState': array([8, 2], dtype=int32), 'currentDistance': 0.775511201361309}
episode index:4184
target Thresh 32.0
target distance 19.0
model initialize at round 4184
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([9.55715362, 6.63848312, 1.3991248 ]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 18.538725323420447}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9314254634595964
{'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([ 6.74581413, 24.29798864,  1.71651651]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 0.746612620407828}
episode index:4185
target Thresh 32.0
target distance 19.0
model initialize at round 4185
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 23.]), 'previousTarget': array([21., 23.]), 'currentState': array([2.97048996, 9.62503188, 0.05382984]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 22.448897620158075}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9314168425304993
{'scaleFactor': 20, 'currentTarget': array([21., 23.]), 'previousTarget': array([21., 23.]), 'currentState': array([20.2973809 , 22.8707034 ,  0.75756482]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 0.714416692087144}
episode index:4186
target Thresh 32.0
target distance 9.0
model initialize at round 4186
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  5.]), 'previousTarget': array([10.,  5.]), 'currentState': array([17.23432922, 12.35364323,  3.51525092]), 'targetState': array([10,  5], dtype=int32), 'currentDistance': 10.315599256110117}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.931421517287454
{'scaleFactor': 20, 'currentTarget': array([10.,  5.]), 'previousTarget': array([10.,  5.]), 'currentState': array([10.48405603,  5.14142229,  4.28547755]), 'targetState': array([10,  5], dtype=int32), 'currentDistance': 0.5042920827043363}
episode index:4187
target Thresh 32.0
target distance 14.0
model initialize at round 4187
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 20.]), 'previousTarget': array([22., 20.]), 'currentState': array([ 9.59575624, 22.43702071,  0.65534001]), 'targetState': array([22, 20], dtype=int32), 'currentDistance': 12.641373868222688}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9314239190620751
{'scaleFactor': 20, 'currentTarget': array([22., 20.]), 'previousTarget': array([22., 20.]), 'currentState': array([21.16956146, 20.53432095,  0.17359346]), 'targetState': array([22, 20], dtype=int32), 'currentDistance': 0.9874852157626358}
episode index:4188
target Thresh 32.0
target distance 8.0
model initialize at round 4188
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 12.]), 'previousTarget': array([17., 12.]), 'currentState': array([12.39210957,  5.18255217,  1.44252497]), 'targetState': array([17, 12], dtype=int32), 'currentDistance': 8.228623764646544}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9314308830369946
{'scaleFactor': 20, 'currentTarget': array([17., 12.]), 'previousTarget': array([17., 12.]), 'currentState': array([16.68357915, 11.7687887 ,  1.42347413]), 'targetState': array([17, 12], dtype=int32), 'currentDistance': 0.3918938943086628}
episode index:4189
target Thresh 32.0
target distance 18.0
model initialize at round 4189
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  4.]), 'previousTarget': array([20.,  4.]), 'currentState': array([3.60635985, 9.98002813, 6.26687002]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 17.450277185121294}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9314266077063136
{'scaleFactor': 20, 'currentTarget': array([20.,  4.]), 'previousTarget': array([20.,  4.]), 'currentState': array([20.43908995,  3.94418963,  5.86318802]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 0.44262261764643684}
episode index:4190
target Thresh 32.0
target distance 12.0
model initialize at round 4190
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 26.]), 'previousTarget': array([15., 26.]), 'currentState': array([ 4.7850123 , 18.90133775,  0.49939691]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 12.439331957087935}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9314290065470902
{'scaleFactor': 20, 'currentTarget': array([15., 26.]), 'previousTarget': array([15., 26.]), 'currentState': array([14.45671851, 25.77676108,  1.24567053]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 0.5873588323328129}
episode index:4191
target Thresh 32.0
target distance 25.0
model initialize at round 4191
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 23.]), 'previousTarget': array([ 2., 23.]), 'currentState': array([25.40892669, 13.14029691,  2.88159227]), 'targetState': array([ 2, 23], dtype=int32), 'currentDistance': 25.400621922521168}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9314161468181904
{'scaleFactor': 20, 'currentTarget': array([ 2., 23.]), 'previousTarget': array([ 2., 23.]), 'currentState': array([ 1.53868523, 23.0928299 ,  2.5560173 ]), 'targetState': array([ 2, 23], dtype=int32), 'currentDistance': 0.4705621164309099}
episode index:4192
target Thresh 32.0
target distance 3.0
model initialize at round 4192
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 11.]), 'previousTarget': array([25., 11.]), 'currentState': array([23.55144658,  9.59006235,  0.2269696 ]), 'targetState': array([25, 11], dtype=int32), 'currentDistance': 2.021442853872592}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9314301186410335
{'scaleFactor': 20, 'currentTarget': array([25., 11.]), 'previousTarget': array([25., 11.]), 'currentState': array([24.94812033, 10.89049205,  1.28304935]), 'targetState': array([25, 11], dtype=int32), 'currentDistance': 0.12117545860978159}
episode index:4193
target Thresh 32.0
target distance 13.0
model initialize at round 4193
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 20.]), 'previousTarget': array([14., 20.]), 'currentState': array([25.01050678, 22.80453381,  3.29281492]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 11.362071531396841}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9314325149287684
{'scaleFactor': 20, 'currentTarget': array([14., 20.]), 'previousTarget': array([14., 20.]), 'currentState': array([13.47530026, 19.74893199,  3.1626193 ]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 0.5816742729020441}
episode index:4194
target Thresh 32.0
target distance 7.0
model initialize at round 4194
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 22.]), 'previousTarget': array([ 7., 22.]), 'currentState': array([14.13021396, 23.67792503,  2.5033474 ]), 'targetState': array([ 7, 22], dtype=int32), 'currentDistance': 7.324983515195378}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9314394668942204
{'scaleFactor': 20, 'currentTarget': array([ 7., 22.]), 'previousTarget': array([ 7., 22.]), 'currentState': array([ 6.8045382 , 21.67292854,  2.94557625]), 'targetState': array([ 7, 22], dtype=int32), 'currentDistance': 0.38102631859844177}
episode index:4195
target Thresh 32.0
target distance 2.0
model initialize at round 4195
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 21.]), 'previousTarget': array([15., 21.]), 'currentState': array([15.36515074, 21.43408817,  3.32857049]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 0.5672456269998728}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9314558063921007
{'scaleFactor': 20, 'currentTarget': array([15., 21.]), 'previousTarget': array([15., 21.]), 'currentState': array([15.36515074, 21.43408817,  3.32857049]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 0.5672456269998728}
episode index:4196
target Thresh 32.0
target distance 22.0
model initialize at round 4196
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 18.]), 'previousTarget': array([26., 18.]), 'currentState': array([ 4.18605354, 27.32734577,  5.83316612]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 23.724410203932283}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9314450675465739
{'scaleFactor': 20, 'currentTarget': array([26., 18.]), 'previousTarget': array([26., 18.]), 'currentState': array([26.24643383, 17.90011962,  5.84898931]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 0.2659054740673432}
episode index:4197
target Thresh 32.0
target distance 6.0
model initialize at round 4197
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 25.]), 'previousTarget': array([ 2., 25.]), 'currentState': array([ 6.42023062, 27.86962507,  4.1838941 ]), 'targetState': array([ 2, 25], dtype=int32), 'currentDistance': 5.270027208972034}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9314543228901788
{'scaleFactor': 20, 'currentTarget': array([ 2., 25.]), 'previousTarget': array([ 2., 25.]), 'currentState': array([ 1.78755562, 24.29641155,  3.3672747 ]), 'targetState': array([ 2, 25], dtype=int32), 'currentDistance': 0.7349621191243}
episode index:4198
target Thresh 32.0
target distance 7.0
model initialize at round 4198
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 28.]), 'previousTarget': array([ 3., 28.]), 'currentState': array([10.04701857, 27.31768691,  3.73033047]), 'targetState': array([ 3, 28], dtype=int32), 'currentDistance': 7.079973292642826}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9314612630395261
{'scaleFactor': 20, 'currentTarget': array([ 3., 28.]), 'previousTarget': array([ 3., 28.]), 'currentState': array([ 2.40784113, 28.24598978,  3.06241959]), 'targetState': array([ 3, 28], dtype=int32), 'currentDistance': 0.6412200143859952}
episode index:4199
target Thresh 32.0
target distance 18.0
model initialize at round 4199
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 21.]), 'previousTarget': array([19., 21.]), 'currentState': array([8.29351665, 4.52750429, 0.9940033 ]), 'targetState': array([19, 21], dtype=int32), 'currentDistance': 19.646167576310887}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9314548156138046
{'scaleFactor': 20, 'currentTarget': array([19., 21.]), 'previousTarget': array([19., 21.]), 'currentState': array([19.22390719, 21.26831592,  0.9253452 ]), 'targetState': array([19, 21], dtype=int32), 'currentDistance': 0.34946797319266965}
episode index:4200
target Thresh 32.0
target distance 12.0
model initialize at round 4200
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([ 7.08314186, 18.32394572,  5.40241957]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 13.641635330251363}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9314549609440339
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.24291793,  7.74541135,  5.45600739]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.3518870530868862}
episode index:4201
target Thresh 32.0
target distance 12.0
model initialize at round 4201
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 15.]), 'previousTarget': array([ 2., 15.]), 'currentState': array([12.60593361, 25.05708432,  4.74622703]), 'targetState': array([ 2, 15], dtype=int32), 'currentDistance': 14.616113458897226}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9314551062050913
{'scaleFactor': 20, 'currentTarget': array([ 2., 15.]), 'previousTarget': array([ 2., 15.]), 'currentState': array([ 2.85846107, 15.36025704,  4.27153837]), 'targetState': array([ 2, 15], dtype=int32), 'currentDistance': 0.9309890137065754}
episode index:4202
target Thresh 32.0
target distance 12.0
model initialize at round 4202
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 21.]), 'previousTarget': array([15., 21.]), 'currentState': array([26.33872938, 15.45238595,  3.29858541]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 12.62310602580967}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9314574914164155
{'scaleFactor': 20, 'currentTarget': array([15., 21.]), 'previousTarget': array([15., 21.]), 'currentState': array([15.55509411, 20.34951157,  3.09590532]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 0.8551401514173294}
episode index:4203
target Thresh 32.0
target distance 6.0
model initialize at round 4203
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 27.]), 'previousTarget': array([10., 27.]), 'currentState': array([15.85180168, 27.67643233,  2.66896814]), 'targetState': array([10, 27], dtype=int32), 'currentDistance': 5.8907676539919365}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9314667305954316
{'scaleFactor': 20, 'currentTarget': array([10., 27.]), 'previousTarget': array([10., 27.]), 'currentState': array([10.16013649, 26.75802288,  3.60700029]), 'targetState': array([10, 27], dtype=int32), 'currentDistance': 0.2901665384652741}
episode index:4204
target Thresh 32.0
target distance 10.0
model initialize at round 4204
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 4.721067  , 19.04688261,  4.2821337 ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 8.494499713994433}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.931473657891366
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.48011982, 11.43289623,  4.83923188]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.6464628273556174}
episode index:4205
target Thresh 32.0
target distance 2.0
model initialize at round 4205
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([11.95945097,  6.28008246,  3.74323356]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 3.3522230690573505}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9314852190758902
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([9.85908743, 8.83859075, 1.95106661]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.21426455285676213}
episode index:4206
target Thresh 32.0
target distance 7.0
model initialize at round 4206
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([11.34518622, 12.3095489 ,  4.34553814]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 6.28682190246537}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9314944450756345
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.61411651, 8.997255  , 3.15521401]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.6141226457852541}
episode index:4207
target Thresh 32.0
target distance 21.0
model initialize at round 4207
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  8.]), 'previousTarget': array([25.,  8.]), 'currentState': array([5.88893328, 4.98799524, 0.58154052]), 'targetState': array([25,  8], dtype=int32), 'currentDistance': 19.346964724782456}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9314880020219115
{'scaleFactor': 20, 'currentTarget': array([25.,  8.]), 'previousTarget': array([25.,  8.]), 'currentState': array([25.50306268,  8.29240546,  6.19686288]), 'targetState': array([25,  8], dtype=int32), 'currentDistance': 0.5818702702600735}
episode index:4208
target Thresh 32.0
target distance 10.0
model initialize at round 4208
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'previousTarget': array([27.,  8.]), 'currentState': array([17.98259629, 13.63365721,  0.0626623 ]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 10.632575568911232}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9314926354378958
{'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'previousTarget': array([27.,  8.]), 'currentState': array([26.3995688 ,  8.4885472 ,  5.74187143]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 0.774077514453618}
episode index:4209
target Thresh 32.0
target distance 22.0
model initialize at round 4209
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 27.]), 'previousTarget': array([16., 27.]), 'currentState': array([10.22571897,  6.95681835,  1.15120578]), 'targetState': array([16, 27], dtype=int32), 'currentDistance': 20.858366477306777}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9314861958748486
{'scaleFactor': 20, 'currentTarget': array([16., 27.]), 'previousTarget': array([16., 27.]), 'currentState': array([15.3821745 , 26.19818776,  1.73296692]), 'targetState': array([16, 27], dtype=int32), 'currentDistance': 1.0122308142421355}
episode index:4210
target Thresh 32.0
target distance 24.0
model initialize at round 4210
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  3.]), 'previousTarget': array([12.,  3.]), 'currentState': array([11.42214647, 26.41453752,  4.50689578]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 23.42166693384075}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9314754855152763
{'scaleFactor': 20, 'currentTarget': array([12.,  3.]), 'previousTarget': array([12.,  3.]), 'currentState': array([12.15016938,  2.52548003,  4.33378146]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 0.49771482239110276}
episode index:4211
target Thresh 32.0
target distance 19.0
model initialize at round 4211
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 24.]), 'previousTarget': array([13., 24.]), 'currentState': array([22.04755175,  6.75858554,  2.08011808]), 'targetState': array([13, 24], dtype=int32), 'currentDistance': 19.471121310836136}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9314690530816329
{'scaleFactor': 20, 'currentTarget': array([13., 24.]), 'previousTarget': array([13., 24.]), 'currentState': array([12.71538397, 24.39405335,  1.71643932]), 'targetState': array([13, 24], dtype=int32), 'currentDistance': 0.4860908646053079}
episode index:4212
target Thresh 32.0
target distance 7.0
model initialize at round 4212
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([17.02071754,  1.66189545,  0.09085196]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 8.015433794842927}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9314759666721665
{'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([22.57331718,  6.96047953,  0.20366692]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 0.4285091503546769}
episode index:4213
target Thresh 32.0
target distance 7.0
model initialize at round 4213
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([22.81696377,  4.6729871 ,  2.68976974]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 7.203192634522188}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9314828769814516
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.24244371,  6.95785941,  2.48548242]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.7587274659400417}
episode index:4214
target Thresh 32.0
target distance 23.0
model initialize at round 4214
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 28.]), 'previousTarget': array([11., 28.]), 'currentState': array([3.68161794, 4.93255235, 0.9699127 ]), 'targetState': array([11, 28], dtype=int32), 'currentDistance': 24.200534230210522}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9314721775733222
{'scaleFactor': 20, 'currentTarget': array([11., 28.]), 'previousTarget': array([11., 28.]), 'currentState': array([10.8991542 , 27.70652796,  1.74698908]), 'targetState': array([11, 28], dtype=int32), 'currentDistance': 0.31031550775557853}
episode index:4215
target Thresh 32.0
target distance 8.0
model initialize at round 4215
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([21.43130981, 19.60958934,  3.78095436]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 8.62038611177849}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9314768070496806
{'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([20.79150938, 10.13340625,  4.86580867]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 0.8913209680085634}
episode index:4216
target Thresh 32.0
target distance 11.0
model initialize at round 4216
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  2.]), 'previousTarget': array([12.,  2.]), 'currentState': array([21.14429321, 11.38393462,  3.49158263]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 13.102531332325569}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9314791791963135
{'scaleFactor': 20, 'currentTarget': array([12.,  2.]), 'previousTarget': array([12.,  2.]), 'currentState': array([12.9672351 ,  2.77642127,  4.366983  ]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 1.2403119464172536}
episode index:4217
target Thresh 32.0
target distance 12.0
model initialize at round 4217
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 13.]), 'previousTarget': array([ 4., 13.]), 'currentState': array([ 5.324999  , 24.836416  ,  4.24894595]), 'targetState': array([ 4, 13], dtype=int32), 'currentDistance': 11.91034702050512}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9314815502181732
{'scaleFactor': 20, 'currentTarget': array([ 4., 13.]), 'previousTarget': array([ 4., 13.]), 'currentState': array([ 4.02135922, 12.9663393 ,  4.82636623]), 'targetState': array([ 4, 13], dtype=int32), 'currentDistance': 0.03986551721014876}
episode index:4218
target Thresh 32.0
target distance 15.0
model initialize at round 4218
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 14.]), 'previousTarget': array([15., 14.]), 'currentState': array([25.73356789, 27.46348191,  3.85850322]), 'targetState': array([15, 14], dtype=int32), 'currentDistance': 17.21844431914438}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9314772922654038
{'scaleFactor': 20, 'currentTarget': array([15., 14.]), 'previousTarget': array([15., 14.]), 'currentState': array([14.69322016, 13.34648334,  3.67214001]), 'targetState': array([15, 14], dtype=int32), 'currentDistance': 0.7219403705466026}
episode index:4219
target Thresh 32.0
target distance 12.0
model initialize at round 4219
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 7.]), 'previousTarget': array([7., 7.]), 'currentState': array([18.73865444, 14.33744577,  3.54646969]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 13.843197549254805}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9314774316150819
{'scaleFactor': 20, 'currentTarget': array([7., 7.]), 'previousTarget': array([7., 7.]), 'currentState': array([6.84534275, 7.0262442 , 3.72890895]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 0.15686816688236555}
episode index:4220
target Thresh 32.0
target distance 15.0
model initialize at round 4220
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 12.]), 'previousTarget': array([20., 12.]), 'currentState': array([14.78844378, 25.16201727,  5.106018  ]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 14.156235973081507}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9314775708987332
{'scaleFactor': 20, 'currentTarget': array([20., 12.]), 'previousTarget': array([20., 12.]), 'currentState': array([19.95716479, 12.20797599,  5.36959645]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 0.2123413925779843}
episode index:4221
target Thresh 32.0
target distance 8.0
model initialize at round 4221
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([17.67065642, 21.7967878 ,  5.15214378]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 8.61763416459924}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9314844677341432
{'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([14.70815568, 14.72869479,  4.66798246]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 1.0161105074718415}
episode index:4222
target Thresh 32.0
target distance 17.0
model initialize at round 4222
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([15.54008947, 26.16271311,  4.67231345]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 16.36109255238115}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9314823981690694
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13.23215266, 10.39078026,  4.90924442]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.4545372088828876}
episode index:4223
target Thresh 32.0
target distance 13.0
model initialize at round 4223
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 12.]), 'previousTarget': array([20., 12.]), 'currentState': array([17.54896702, 23.08455662,  4.84050474]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 11.352310693922327}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.931484765060933
{'scaleFactor': 20, 'currentTarget': array([20., 12.]), 'previousTarget': array([20., 12.]), 'currentState': array([20.2626664 , 11.45414032,  4.5801791 ]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 0.6057692836326084}
episode index:4224
target Thresh 32.0
target distance 19.0
model initialize at round 4224
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 29.]), 'previousTarget': array([23., 29.]), 'currentState': array([ 5.62042933, 22.14232379,  0.38295642]), 'targetState': array([23, 29], dtype=int32), 'currentDistance': 18.68360777814122}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9314805123940508
{'scaleFactor': 20, 'currentTarget': array([23., 29.]), 'previousTarget': array([23., 29.]), 'currentState': array([22.21157793, 28.93285619,  0.82690994]), 'targetState': array([23, 29], dtype=int32), 'currentDistance': 0.7912759660573546}
episode index:4225
target Thresh 32.0
target distance 17.0
model initialize at round 4225
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  2.]), 'previousTarget': array([25.,  2.]), 'currentState': array([ 8.19952871, 13.69417053,  0.48258483]), 'targetState': array([25,  2], dtype=int32), 'currentDistance': 20.469720567361588}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9314741000804244
{'scaleFactor': 20, 'currentTarget': array([25.,  2.]), 'previousTarget': array([25.,  2.]), 'currentState': array([24.32188334,  2.67016766,  6.05693807]), 'targetState': array([25,  2], dtype=int32), 'currentDistance': 0.9533975524794692}
episode index:4226
target Thresh 32.0
target distance 24.0
model initialize at round 4226
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([ 4.74585655, 12.03519744,  5.9056493 ]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 24.018361587461317}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9314634331231582
{'scaleFactor': 20, 'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([26.93532076,  3.06370972,  6.27235666]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 0.09078729657082411}
episode index:4227
target Thresh 32.0
target distance 16.0
model initialize at round 4227
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 13.]), 'previousTarget': array([25., 13.]), 'currentState': array([25.66355449, 27.35100166,  5.52111971]), 'targetState': array([25, 13], dtype=int32), 'currentDistance': 14.366334021748434}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.931463575487109
{'scaleFactor': 20, 'currentTarget': array([25., 13.]), 'previousTarget': array([25., 13.]), 'currentState': array([25.30241119, 13.60974624,  4.91520749]), 'targetState': array([25, 13], dtype=int32), 'currentDistance': 0.6806195713780154}
episode index:4228
target Thresh 32.0
target distance 17.0
model initialize at round 4228
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  8.]), 'previousTarget': array([26.,  8.]), 'currentState': array([ 7.31953204, 23.90826393,  4.20612812]), 'targetState': array([26,  8], dtype=int32), 'currentDistance': 24.536355570973626}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9314529160631858
{'scaleFactor': 20, 'currentTarget': array([26.,  8.]), 'previousTarget': array([26.,  8.]), 'currentState': array([25.28902498,  8.93531658,  5.8870512 ]), 'targetState': array([26,  8], dtype=int32), 'currentDistance': 1.1748627938957705}
episode index:4229
target Thresh 32.0
target distance 9.0
model initialize at round 4229
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([22.7312604 , 15.1818203 ,  5.27876902]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 7.186846578466572}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9314598056835017
{'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([23.1378908 ,  7.45698574,  5.22761106]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 0.5602484813489212}
episode index:4230
target Thresh 32.0
target distance 13.0
model initialize at round 4230
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 12.]), 'previousTarget': array([ 4., 12.]), 'currentState': array([15.93104121, 19.29989045,  3.26901472]), 'targetState': array([ 4, 12], dtype=int32), 'currentDistance': 13.987070632719227}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9314599488038572
{'scaleFactor': 20, 'currentTarget': array([ 4., 12.]), 'previousTarget': array([ 4., 12.]), 'currentState': array([ 4.11200684, 12.02892365,  4.14335333]), 'targetState': array([ 4, 12], dtype=int32), 'currentDistance': 0.1156810656376137}
episode index:4231
target Thresh 32.0
target distance 8.0
model initialize at round 4231
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 10.]), 'previousTarget': array([22., 10.]), 'currentState': array([25.20486716,  3.67045429,  2.45876414]), 'targetState': array([22, 10], dtype=int32), 'currentDistance': 7.094668590561944}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9314668335064081
{'scaleFactor': 20, 'currentTarget': array([22., 10.]), 'previousTarget': array([22., 10.]), 'currentState': array([21.48366106, 10.39136514,  2.84593862]), 'targetState': array([22, 10], dtype=int32), 'currentDistance': 0.6478985877236467}
episode index:4232
target Thresh 32.0
target distance 9.0
model initialize at round 4232
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 9.19617979, 22.4018263 ,  5.2477144 ]), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.405921622597504}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9314714456529695
{'scaleFactor': 20, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.33749418, 14.54123646,  5.2038504 ]), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.5695316600260975}
episode index:4233
target Thresh 32.0
target distance 5.0
model initialize at round 4233
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 12.]), 'previousTarget': array([15., 12.]), 'currentState': array([19.25185127, 15.8751638 ,  4.54117841]), 'targetState': array([15, 12], dtype=int32), 'currentDistance': 5.7528370100594755}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9314806160720406
{'scaleFactor': 20, 'currentTarget': array([15., 12.]), 'previousTarget': array([15., 12.]), 'currentState': array([15.26121184, 11.72038126,  3.35598767]), 'targetState': array([15, 12], dtype=int32), 'currentDistance': 0.3826463993037774}
episode index:4234
target Thresh 32.0
target distance 4.0
model initialize at round 4234
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 23.]), 'previousTarget': array([12., 23.]), 'currentState': array([12.55761351, 20.93536939,  1.58306909]), 'targetState': array([12, 23], dtype=int32), 'currentDistance': 2.138605241737732}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9314944341083872
{'scaleFactor': 20, 'currentTarget': array([12., 23.]), 'previousTarget': array([12., 23.]), 'currentState': array([11.94859459, 22.80791432,  2.19351864]), 'targetState': array([12, 23], dtype=int32), 'currentDistance': 0.1988452244819033}
episode index:4235
target Thresh 32.0
target distance 10.0
model initialize at round 4235
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 16.]), 'previousTarget': array([11., 16.]), 'currentState': array([19.42701859, 22.59842926,  3.78805888]), 'targetState': array([11, 16], dtype=int32), 'currentDistance': 10.702986084644344}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9314990364728327
{'scaleFactor': 20, 'currentTarget': array([11., 16.]), 'previousTarget': array([11., 16.]), 'currentState': array([11.71408003, 16.29881799,  4.12412849]), 'targetState': array([11, 16], dtype=int32), 'currentDistance': 0.7740816980877318}
episode index:4236
target Thresh 32.0
target distance 15.0
model initialize at round 4236
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 6.]), 'previousTarget': array([8., 6.]), 'currentState': array([ 3.36495145, 19.04546597,  5.08626893]), 'targetState': array([8, 6], dtype=int32), 'currentDistance': 13.844416110605865}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.93149917013142
{'scaleFactor': 20, 'currentTarget': array([8., 6.]), 'previousTarget': array([8., 6.]), 'currentState': array([7.94770364, 5.84604887, 5.05186975]), 'targetState': array([8, 6], dtype=int32), 'currentDistance': 0.16259107641856632}
episode index:4237
target Thresh 32.0
target distance 14.0
model initialize at round 4237
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 22.]), 'previousTarget': array([ 7., 22.]), 'currentState': array([19.2016455 , 21.53271995,  2.72951591]), 'targetState': array([ 7, 22], dtype=int32), 'currentDistance': 12.210589810543869}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9315015252468682
{'scaleFactor': 20, 'currentTarget': array([ 7., 22.]), 'previousTarget': array([ 7., 22.]), 'currentState': array([ 7.30126058, 21.94155957,  3.33717252]), 'targetState': array([ 7, 22], dtype=int32), 'currentDistance': 0.30687655473990444}
episode index:4238
target Thresh 32.0
target distance 10.0
model initialize at round 4238
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 13.]), 'previousTarget': array([18., 13.]), 'currentState': array([ 9.98549098, 12.24046539,  0.11930142]), 'targetState': array([18, 13], dtype=int32), 'currentDistance': 8.050419095554421}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9315083887724056
{'scaleFactor': 20, 'currentTarget': array([18., 13.]), 'previousTarget': array([18., 13.]), 'currentState': array([17.90642806, 12.93366292,  0.38026998]), 'targetState': array([18, 13], dtype=int32), 'currentDistance': 0.1147009870823219}
episode index:4239
target Thresh 32.0
target distance 17.0
model initialize at round 4239
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([18.68296982,  4.99917811,  1.00951165]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 18.26718452257849}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9315041455787054
{'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([12.05965857, 21.40014253,  2.28293197]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 0.6028168267669042}
episode index:4240
target Thresh 32.0
target distance 7.0
model initialize at round 4240
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  7.]), 'previousTarget': array([21.,  7.]), 'currentState': array([17.66646742, 14.23510513,  5.41334027]), 'targetState': array([21,  7], dtype=int32), 'currentDistance': 7.96612739596207}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.931511005249637
{'scaleFactor': 20, 'currentTarget': array([21.,  7.]), 'previousTarget': array([21.,  7.]), 'currentState': array([21.02246379,  7.05515275,  5.38174028]), 'targetState': array([21,  7], dtype=int32), 'currentDistance': 0.059552057316698985}
episode index:4241
target Thresh 32.0
target distance 22.0
model initialize at round 4241
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 28.]), 'previousTarget': array([12., 28.]), 'currentState': array([11.5967275 ,  7.57362777,  2.2183442 ]), 'targetState': array([12, 28], dtype=int32), 'currentDistance': 20.4303526894541}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9315046099336916
{'scaleFactor': 20, 'currentTarget': array([12., 28.]), 'previousTarget': array([12., 28.]), 'currentState': array([11.96618622, 27.37437153,  1.94339271]), 'targetState': array([12, 28], dtype=int32), 'currentDistance': 0.6265415840749843}
episode index:4242
target Thresh 32.0
target distance 16.0
model initialize at round 4242
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([15.44692774, 20.64834761,  3.75612462]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 16.720631318387134}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9315003706307337
{'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([17.12008225,  3.26877786,  4.44694368]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 0.741016578211769}
episode index:4243
target Thresh 32.0
target distance 21.0
model initialize at round 4243
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 7.]), 'previousTarget': array([7., 7.]), 'currentState': array([20.62159991, 27.54966479,  5.00230098]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 24.65434459198444}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9314897402115737
{'scaleFactor': 20, 'currentTarget': array([7., 7.]), 'previousTarget': array([7., 7.]), 'currentState': array([7.75458941, 7.64010296, 4.38437857]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 0.9895135099535232}
episode index:4244
target Thresh 32.0
target distance 13.0
model initialize at round 4244
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 28.]), 'previousTarget': array([19., 28.]), 'currentState': array([ 7.79758959, 22.7247148 ,  0.82024818]), 'targetState': array([19, 28], dtype=int32), 'currentDistance': 12.382351672517617}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9314920936648574
{'scaleFactor': 20, 'currentTarget': array([19., 28.]), 'previousTarget': array([19., 28.]), 'currentState': array([18.49566515, 27.94294452,  0.66260519]), 'targetState': array([19, 28], dtype=int32), 'currentDistance': 0.5075519418124422}
episode index:4245
target Thresh 32.0
target distance 23.0
model initialize at round 4245
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  5.]), 'previousTarget': array([26.63816566,  5.31463855]), 'currentState': array([ 5.55645404, 24.35981338,  4.88296628]), 'targetState': array([27,  5], dtype=int32), 'currentDistance': 28.889929690056448}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9314773159255979
{'scaleFactor': 20, 'currentTarget': array([27.,  5.]), 'previousTarget': array([27.,  5.]), 'currentState': array([26.3485314 ,  5.88880216,  5.88875305]), 'targetState': array([27,  5], dtype=int32), 'currentDistance': 1.1019893882898797}
episode index:4246
target Thresh 32.0
target distance 15.0
model initialize at round 4246
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 12.]), 'previousTarget': array([ 5., 12.]), 'currentState': array([ 6.70597074, 25.02176417,  4.55487148]), 'targetState': array([ 5, 12], dtype=int32), 'currentDistance': 13.133037663228846}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9314774543837994
{'scaleFactor': 20, 'currentTarget': array([ 5., 12.]), 'previousTarget': array([ 5., 12.]), 'currentState': array([ 5.10544259, 11.20913582,  4.12296548]), 'targetState': array([ 5, 12], dtype=int32), 'currentDistance': 0.7978623281290783}
episode index:4247
target Thresh 32.0
target distance 13.0
model initialize at round 4247
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([13.45274982, 16.73794226,  3.92648911]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 15.033063491344446}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9314753986493465
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.47194463, 6.31149761, 3.3382893 ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.8676854365162999}
episode index:4248
target Thresh 32.0
target distance 3.0
model initialize at round 4248
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 26.]), 'previousTarget': array([11., 26.]), 'currentState': array([13.65161989, 23.32332619,  1.20331829]), 'targetState': array([11, 26], dtype=int32), 'currentDistance': 3.767714258656495}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9314868424246703
{'scaleFactor': 20, 'currentTarget': array([11., 26.]), 'previousTarget': array([11., 26.]), 'currentState': array([11.17866378, 25.77435447,  1.81459993]), 'targetState': array([11, 26], dtype=int32), 'currentDistance': 0.2878135683238585}
episode index:4249
target Thresh 32.0
target distance 19.0
model initialize at round 4249
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([19.32214632, 23.95869448,  4.60605911]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 18.471482245380496}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.931482614284684
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.37356642,  6.4614193 ,  4.7736604 ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.5936831208551757}
episode index:4250
target Thresh 32.0
target distance 8.0
model initialize at round 4250
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([ 7.36121852, 16.61687686,  4.38125324]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 9.314476749709769}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9314872031897924
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.75411425, 8.54475201, 3.433584  ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.5174075130893371}
episode index:4251
target Thresh 32.0
target distance 21.0
model initialize at round 4251
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  2.]), 'previousTarget': array([13.,  2.]), 'currentState': array([25.83923069, 21.37606215,  3.98303007]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 23.243872941839495}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9314787015555188
{'scaleFactor': 20, 'currentTarget': array([13.,  2.]), 'previousTarget': array([13.,  2.]), 'currentState': array([13.94451139,  2.95416435,  4.44093057]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 1.3425838439603952}
episode index:4252
target Thresh 32.0
target distance 19.0
model initialize at round 4252
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 13.]), 'previousTarget': array([ 3., 13.]), 'currentState': array([20.48967627,  2.25749727,  2.58851242]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 20.52535361423131}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9314723303759875
{'scaleFactor': 20, 'currentTarget': array([ 3., 13.]), 'previousTarget': array([ 3., 13.]), 'currentState': array([ 3.37401402, 12.47772897,  2.91792963]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 0.6423811302007639}
episode index:4253
target Thresh 32.0
target distance 11.0
model initialize at round 4253
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 13.]), 'previousTarget': array([ 3., 13.]), 'currentState': array([11.68215961,  1.94777818,  0.97896546]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 14.05459009515358}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9314724697783221
{'scaleFactor': 20, 'currentTarget': array([ 3., 13.]), 'previousTarget': array([ 3., 13.]), 'currentState': array([ 3.29316088, 12.39831794,  2.65673639]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 0.6693015791396143}
episode index:4254
target Thresh 32.0
target distance 7.0
model initialize at round 4254
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 28.]), 'previousTarget': array([ 4., 28.]), 'currentState': array([ 9.28435684, 23.82961588,  2.1500864 ]), 'targetState': array([ 4, 28], dtype=int32), 'currentDistance': 6.731755416626226}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9314815946972931
{'scaleFactor': 20, 'currentTarget': array([ 4., 28.]), 'previousTarget': array([ 4., 28.]), 'currentState': array([ 4.49127889, 27.16391515,  3.03039747]), 'targetState': array([ 4, 28], dtype=int32), 'currentDistance': 0.9697385365314914}
episode index:4255
target Thresh 32.0
target distance 17.0
model initialize at round 4255
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  2.]), 'previousTarget': array([10.,  2.]), 'currentState': array([25.09671207,  6.38864752,  3.39886421]), 'targetState': array([10,  2], dtype=int32), 'currentDistance': 15.721671103066356}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9314795418541847
{'scaleFactor': 20, 'currentTarget': array([10.,  2.]), 'previousTarget': array([10.,  2.]), 'currentState': array([9.75230483, 1.9700389 , 3.20190992]), 'targetState': array([10,  2], dtype=int32), 'currentDistance': 0.2495006304654871}
episode index:4256
target Thresh 32.0
target distance 11.0
model initialize at round 4256
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([10.6826705 , 20.03175035,  5.29205209]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 11.15934119260983}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9314818910690184
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([8.88636749, 8.39161341, 4.13752518]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.6189075755534967}
episode index:4257
target Thresh 32.0
target distance 17.0
model initialize at round 4257
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 24.]), 'previousTarget': array([ 7., 24.]), 'currentState': array([4.67995813, 7.10064184, 1.06983584]), 'targetState': array([ 7, 24], dtype=int32), 'currentDistance': 17.057869166313466}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9314776720357667
{'scaleFactor': 20, 'currentTarget': array([ 7., 24.]), 'previousTarget': array([ 7., 24.]), 'currentState': array([ 7.03690222, 24.74951464,  0.88389955]), 'targetState': array([ 7, 24], dtype=int32), 'currentDistance': 0.7504225243695862}
episode index:4258
target Thresh 32.0
target distance 25.0
model initialize at round 4258
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 28.]), 'previousTarget': array([12., 28.]), 'currentState': array([14.6190723 ,  3.45933972,  1.2864415 ]), 'targetState': array([12, 28], dtype=int32), 'currentDistance': 24.680023226472812}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9314670843860087
{'scaleFactor': 20, 'currentTarget': array([12., 28.]), 'previousTarget': array([12., 28.]), 'currentState': array([11.96711258, 27.15044644,  2.16377112]), 'targetState': array([12, 28], dtype=int32), 'currentDistance': 0.8501898814920098}
episode index:4259
target Thresh 32.0
target distance 14.0
model initialize at round 4259
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([12.39656424, 24.51125517,  3.84293342]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 15.482229564359283}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9314650368766287
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([6.73715312, 9.6740959 , 4.03117935]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.41869077543654776}
episode index:4260
target Thresh 32.0
target distance 12.0
model initialize at round 4260
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'previousTarget': array([7., 3.]), 'currentState': array([17.14561083,  6.91438584,  3.85810554]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 10.874549902405743}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9314696191373711
{'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'previousTarget': array([7., 3.]), 'currentState': array([7.95935488, 3.141963  , 3.8466855 ]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 0.969801675526147}
episode index:4261
target Thresh 32.0
target distance 11.0
model initialize at round 4261
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 14.]), 'previousTarget': array([ 9., 14.]), 'currentState': array([18.47697181,  4.28391815,  2.57109261]), 'targetState': array([ 9, 14], dtype=int32), 'currentDistance': 13.572591545125638}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9314697589141824
{'scaleFactor': 20, 'currentTarget': array([ 9., 14.]), 'previousTarget': array([ 9., 14.]), 'currentState': array([ 8.80868892, 14.32902998,  2.01223476]), 'targetState': array([ 9, 14], dtype=int32), 'currentDistance': 0.38060563944937564}
episode index:4262
target Thresh 32.0
target distance 7.0
model initialize at round 4262
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 21.]), 'previousTarget': array([20., 21.]), 'currentState': array([14.66821091, 24.89983707,  5.77186774]), 'targetState': array([20, 21], dtype=int32), 'currentDistance': 6.605808353150416}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9314788673451198
{'scaleFactor': 20, 'currentTarget': array([20., 21.]), 'previousTarget': array([20., 21.]), 'currentState': array([19.59166923, 21.54591203,  5.95975761]), 'targetState': array([20, 21], dtype=int32), 'currentDistance': 0.6817286521227371}
episode index:4263
target Thresh 32.0
target distance 15.0
model initialize at round 4263
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 11.]), 'previousTarget': array([20., 11.]), 'currentState': array([ 3.42664354, 23.4025575 ,  4.51449919]), 'targetState': array([20, 11], dtype=int32), 'currentDistance': 20.70023131933325}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9314725125626769
{'scaleFactor': 20, 'currentTarget': array([20., 11.]), 'previousTarget': array([20., 11.]), 'currentState': array([19.08975365, 11.94824388,  5.91311117]), 'targetState': array([20, 11], dtype=int32), 'currentDistance': 1.3144256850684024}
episode index:4264
target Thresh 32.0
target distance 5.0
model initialize at round 4264
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 18.]), 'previousTarget': array([24., 18.]), 'currentState': array([22.22728254, 21.54656937,  4.86266708]), 'targetState': array([24, 18], dtype=int32), 'currentDistance': 3.964931465172985}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9314839140837643
{'scaleFactor': 20, 'currentTarget': array([24., 18.]), 'previousTarget': array([24., 18.]), 'currentState': array([23.97061871, 18.25825986,  4.3250913 ]), 'targetState': array([24, 18], dtype=int32), 'currentDistance': 0.259925783524574}
episode index:4265
target Thresh 32.0
target distance 3.0
model initialize at round 4265
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 13.]), 'previousTarget': array([ 2., 13.]), 'currentState': array([ 5.554133  , 14.58913429,  2.24526756]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 3.8932260622455614}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9314953102595535
{'scaleFactor': 20, 'currentTarget': array([ 2., 13.]), 'previousTarget': array([ 2., 13.]), 'currentState': array([ 2.37140456, 13.21050605,  3.39138441]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 0.42691234283862123}
episode index:4266
target Thresh 32.0
target distance 7.0
model initialize at round 4266
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 15.]), 'previousTarget': array([12., 15.]), 'currentState': array([10.6570013 ,  8.29450767,  1.18589848]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 6.838660163027696}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9315044041638751
{'scaleFactor': 20, 'currentTarget': array([12., 15.]), 'previousTarget': array([12., 15.]), 'currentState': array([11.50050525, 14.14790936,  1.85530349]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 0.9877010988243417}
episode index:4267
target Thresh 32.0
target distance 6.0
model initialize at round 4267
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 20.]), 'previousTarget': array([ 3., 20.]), 'currentState': array([ 6.32729655, 15.03473279,  1.67216462]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 5.977021067860543}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9315134938067609
{'scaleFactor': 20, 'currentTarget': array([ 3., 20.]), 'previousTarget': array([ 3., 20.]), 'currentState': array([ 3.00890068, 19.73937197,  2.56828791]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 0.2607799662602525}
episode index:4268
target Thresh 32.0
target distance 6.0
model initialize at round 4268
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 17.]), 'previousTarget': array([22., 17.]), 'currentState': array([17.81762956, 15.73772596,  0.72921234]), 'targetState': array([22, 17], dtype=int32), 'currentDistance': 4.368702118640861}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9315248750450352
{'scaleFactor': 20, 'currentTarget': array([22., 17.]), 'previousTarget': array([22., 17.]), 'currentState': array([21.48219286, 17.0323978 ,  0.77812594]), 'targetState': array([22, 17], dtype=int32), 'currentDistance': 0.5188196692611001}
episode index:4269
target Thresh 32.0
target distance 15.0
model initialize at round 4269
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([19.51938292, 16.6008215 ,  2.26706401]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 15.73580372225629}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9315228187966471
{'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([ 4.37051869, 14.08179542,  3.70351436]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 0.3794398323232287}
episode index:4270
target Thresh 32.0
target distance 16.0
model initialize at round 4270
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 14.]), 'previousTarget': array([ 9., 14.]), 'currentState': array([23.66425288, 21.70134924,  3.25850534]), 'targetState': array([ 9, 14], dtype=int32), 'currentDistance': 16.563547101840996}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9315207635111475
{'scaleFactor': 20, 'currentTarget': array([ 9., 14.]), 'previousTarget': array([ 9., 14.]), 'currentState': array([ 9.68618443, 14.1053165 ,  3.90842876]), 'targetState': array([ 9, 14], dtype=int32), 'currentDistance': 0.6942194435064806}
episode index:4271
target Thresh 32.0
target distance 7.0
model initialize at round 4271
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 18.]), 'previousTarget': array([17., 18.]), 'currentState': array([11.07582864, 21.2942105 ,  6.15046674]), 'targetState': array([17, 18], dtype=int32), 'currentDistance': 6.778468055352676}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9315298408136964
{'scaleFactor': 20, 'currentTarget': array([17., 18.]), 'previousTarget': array([17., 18.]), 'currentState': array([16.2741688 , 18.59013622,  5.20339707]), 'targetState': array([17, 18], dtype=int32), 'currentDistance': 0.9354633580683154}
episode index:4272
target Thresh 32.0
target distance 16.0
model initialize at round 4272
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'previousTarget': array([10., 12.]), 'currentState': array([11.45040592, 26.10389888,  4.54846179]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 14.17828060666795}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9315299661371444
{'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'previousTarget': array([10., 12.]), 'currentState': array([10.14390864, 12.24183585,  4.78799148]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 0.2814147720877845}
episode index:4273
target Thresh 32.0
target distance 21.0
model initialize at round 4273
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([11.87252647, 26.2494765 ,  3.31491196]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 24.341445801180708}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9315194034103262
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.5075147 , 4.56067101, 4.62475196]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.7562560048931538}
episode index:4274
target Thresh 32.0
target distance 4.0
model initialize at round 4274
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 24.]), 'previousTarget': array([19., 24.]), 'currentState': array([21.12902407, 26.55989804,  3.15849853]), 'targetState': array([19, 24], dtype=int32), 'currentDistance': 3.3295377257769156}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9315307672925695
{'scaleFactor': 20, 'currentTarget': array([19., 24.]), 'previousTarget': array([19., 24.]), 'currentState': array([18.94796009, 23.92726209,  2.9754467 ]), 'targetState': array([19, 24], dtype=int32), 'currentDistance': 0.08943687917056195}
episode index:4275
target Thresh 32.0
target distance 18.0
model initialize at round 4275
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 22.]), 'previousTarget': array([20., 22.]), 'currentState': array([4.57084844, 5.62733433, 0.81864047]), 'targetState': array([20, 22], dtype=int32), 'currentDistance': 22.497175357775202}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9315223031875568
{'scaleFactor': 20, 'currentTarget': array([20., 22.]), 'previousTarget': array([20., 22.]), 'currentState': array([19.4274194 , 21.71610142,  1.27925209]), 'targetState': array([20, 22], dtype=int32), 'currentDistance': 0.6390985389034131}
episode index:4276
target Thresh 32.0
target distance 10.0
model initialize at round 4276
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 25.]), 'previousTarget': array([17., 25.]), 'currentState': array([12.61165825, 15.48471204,  1.30214804]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 10.478465927313053}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.931526854916973
{'scaleFactor': 20, 'currentTarget': array([17., 25.]), 'previousTarget': array([17., 25.]), 'currentState': array([16.54892711, 24.61494706,  1.47954066]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 0.5930704132724324}
episode index:4277
target Thresh 32.0
target distance 20.0
model initialize at round 4277
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 2.]), 'previousTarget': array([8., 2.]), 'currentState': array([20.68270476, 20.63069348,  4.50360632]), 'targetState': array([8, 2], dtype=int32), 'currentDistance': 22.537829077321465}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9315183956835325
{'scaleFactor': 20, 'currentTarget': array([8., 2.]), 'previousTarget': array([8., 2.]), 'currentState': array([8.37021474, 2.55814426, 4.6137134 ]), 'targetState': array([8, 2], dtype=int32), 'currentDistance': 0.6697641152545452}
episode index:4278
target Thresh 32.0
target distance 13.0
model initialize at round 4278
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 25.]), 'previousTarget': array([14., 25.]), 'currentState': array([15.1702572 , 13.67433586,  2.47945824]), 'targetState': array([14, 25], dtype=int32), 'currentDistance': 11.38596373353262}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9315207237400216
{'scaleFactor': 20, 'currentTarget': array([14., 25.]), 'previousTarget': array([14., 25.]), 'currentState': array([13.82731157, 25.16037421,  2.37888291]), 'targetState': array([14, 25], dtype=int32), 'currentDistance': 0.23567176408652998}
episode index:4279
target Thresh 32.0
target distance 18.0
model initialize at round 4279
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 14.]), 'previousTarget': array([23., 14.]), 'currentState': array([ 6.03261546, 19.67105553,  0.09977406]), 'targetState': array([23, 14], dtype=int32), 'currentDistance': 17.890025405869185}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9315165173203356
{'scaleFactor': 20, 'currentTarget': array([23., 14.]), 'previousTarget': array([23., 14.]), 'currentState': array([23.01139931, 13.95291697,  6.12606312]), 'targetState': array([23, 14], dtype=int32), 'currentDistance': 0.048443326243719546}
episode index:4280
target Thresh 32.0
target distance 5.0
model initialize at round 4280
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 17.]), 'previousTarget': array([21., 17.]), 'currentState': array([22.37888034, 12.45206097,  1.85964394]), 'targetState': array([21, 17], dtype=int32), 'currentDistance': 4.752374184477036}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9315278659497865
{'scaleFactor': 20, 'currentTarget': array([21., 17.]), 'previousTarget': array([21., 17.]), 'currentState': array([21.15769379, 16.24700779,  2.08363146]), 'targetState': array([21, 17], dtype=int32), 'currentDistance': 0.7693273630216713}
episode index:4281
target Thresh 32.0
target distance 9.0
model initialize at round 4281
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 17.]), 'previousTarget': array([18., 17.]), 'currentState': array([25.46810423, 13.8836787 ,  3.47540808]), 'targetState': array([18, 17], dtype=int32), 'currentDistance': 8.09222091190315}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9315346544000551
{'scaleFactor': 20, 'currentTarget': array([18., 17.]), 'previousTarget': array([18., 17.]), 'currentState': array([18.3183675 , 16.67457675,  3.17531258]), 'targetState': array([18, 17], dtype=int32), 'currentDistance': 0.4552561479268157}
episode index:4282
target Thresh 32.0
target distance 3.0
model initialize at round 4282
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([9.32993199, 5.11561049, 4.36267736]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 1.1633752125292507}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9315483049593827
{'scaleFactor': 20, 'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([8.44475807, 3.32672828, 4.1408934 ]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 0.8726903332135725}
episode index:4283
target Thresh 32.0
target distance 6.0
model initialize at round 4283
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 23.]), 'previousTarget': array([10., 23.]), 'currentState': array([11.64989914, 29.33199535,  5.47175521]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 6.543419008061108}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9315573504064043
{'scaleFactor': 20, 'currentTarget': array([10., 23.]), 'previousTarget': array([10., 23.]), 'currentState': array([10.43726744, 23.94122772,  3.84474725]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 1.0378402740950534}
episode index:4284
target Thresh 32.0
target distance 8.0
model initialize at round 4284
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'previousTarget': array([22., 18.]), 'currentState': array([15.62783235, 25.4272586 ,  5.5298664 ]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 9.786147906854236}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9315618854587949
{'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'previousTarget': array([22., 18.]), 'currentState': array([21.97721794, 17.72102795,  5.2722628 ]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 0.27990074171846985}
episode index:4285
target Thresh 32.0
target distance 11.0
model initialize at round 4285
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([10.41120512, 20.44491539,  4.48771024]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 11.044261228342569}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9315641995661076
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.01065816,  9.68443366,  3.80027074]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.3157462735638993}
episode index:4286
target Thresh 32.0
target distance 11.0
model initialize at round 4286
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 25.]), 'previousTarget': array([12., 25.]), 'currentState': array([21.33427899, 15.95345886,  2.12167597]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 12.998794973625525}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9315643164656506
{'scaleFactor': 20, 'currentTarget': array([12., 25.]), 'previousTarget': array([12., 25.]), 'currentState': array([11.3610574 , 25.48142749,  1.64801332]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 0.8000125455657703}
episode index:4287
target Thresh 32.0
target distance 9.0
model initialize at round 4287
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 16.]), 'previousTarget': array([24., 16.]), 'currentState': array([14.81634061, 13.3270812 ,  5.61304307]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 9.564731815552008}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9315688467206493
{'scaleFactor': 20, 'currentTarget': array([24., 16.]), 'previousTarget': array([24., 16.]), 'currentState': array([23.95064876, 15.97294696,  0.58760959]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 0.05627975968182817}
episode index:4288
target Thresh 32.0
target distance 17.0
model initialize at round 4288
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([14.41645632,  4.56989253,  1.78613949]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 20.102908135819376}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9315625080002689
{'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([25.64381232, 20.79575507,  1.25650588]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 0.41059182826266843}
episode index:4289
target Thresh 32.0
target distance 13.0
model initialize at round 4289
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 16.]), 'previousTarget': array([23., 16.]), 'currentState': array([10.98631479, 15.63633904,  0.06538647]), 'targetState': array([23, 16], dtype=int32), 'currentDistance': 12.019188057766033}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9315648198047911
{'scaleFactor': 20, 'currentTarget': array([23., 16.]), 'previousTarget': array([23., 16.]), 'currentState': array([22.93202147, 15.9579733 ,  0.19238722]), 'targetState': array([23, 16], dtype=int32), 'currentDistance': 0.07992074046883665}
episode index:4290
target Thresh 32.0
target distance 20.0
model initialize at round 4290
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([22.47724914,  5.38611607,  3.98990965]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 20.755324963679136}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9315584849772927
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.99532908, 1.88891322, 3.73857346]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 1.001508986801915}
episode index:4291
target Thresh 32.0
target distance 8.0
model initialize at round 4291
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 14.]), 'previousTarget': array([ 2., 14.]), 'currentState': array([ 6.50594608, 20.23244768,  3.72079062]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 7.690705702469805}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.931565250477065
{'scaleFactor': 20, 'currentTarget': array([ 2., 14.]), 'previousTarget': array([ 2., 14.]), 'currentState': array([ 2.11382575, 13.97912461,  3.19032919]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 0.11572417383246371}
episode index:4292
target Thresh 32.0
target distance 11.0
model initialize at round 4292
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 15.]), 'previousTarget': array([ 6., 15.]), 'currentState': array([10.54402398, 24.08516282,  4.16677845]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 10.158166044473411}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9315697752381698
{'scaleFactor': 20, 'currentTarget': array([ 6., 15.]), 'previousTarget': array([ 6., 15.]), 'currentState': array([ 6.17420712, 15.14946451,  4.63053137]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 0.22953814006167522}
episode index:4293
target Thresh 32.0
target distance 3.0
model initialize at round 4293
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 19.]), 'previousTarget': array([ 4., 19.]), 'currentState': array([ 3.24770529, 20.57264624,  4.86153865]), 'targetState': array([ 4, 19], dtype=int32), 'currentDistance': 1.7433196858628657}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9315833826496187
{'scaleFactor': 20, 'currentTarget': array([ 4., 19.]), 'previousTarget': array([ 4., 19.]), 'currentState': array([ 4.04979077, 18.76731156,  5.40485704]), 'targetState': array([ 4, 19], dtype=int32), 'currentDistance': 0.23795594757458008}
episode index:4294
target Thresh 32.0
target distance 2.0
model initialize at round 4294
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 23.]), 'previousTarget': array([ 2., 23.]), 'currentState': array([ 2.2375251 , 22.93116409,  2.51264408]), 'targetState': array([ 2, 23], dtype=int32), 'currentDistance': 0.2472985145371124}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.931599312013379
{'scaleFactor': 20, 'currentTarget': array([ 2., 23.]), 'previousTarget': array([ 2., 23.]), 'currentState': array([ 2.2375251 , 22.93116409,  2.51264408]), 'targetState': array([ 2, 23], dtype=int32), 'currentDistance': 0.2472985145371124}
episode index:4295
target Thresh 32.0
target distance 3.0
model initialize at round 4295
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  2.]), 'previousTarget': array([22.,  2.]), 'currentState': array([20.1974154 ,  5.18261762,  6.05236608]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 3.657644891890797}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9316106017452195
{'scaleFactor': 20, 'currentTarget': array([22.,  2.]), 'previousTarget': array([22.,  2.]), 'currentState': array([22.02090993,  1.90165139,  5.46681279]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 0.10054687909446999}
episode index:4296
target Thresh 32.0
target distance 12.0
model initialize at round 4296
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'previousTarget': array([10., 17.]), 'currentState': array([20.62254124, 28.96695162,  3.53953755]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 16.00144722953416}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9316085384668119
{'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'previousTarget': array([10., 17.]), 'currentState': array([10.15472408, 17.03339173,  4.38887531]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 0.15828628382990884}
episode index:4297
target Thresh 32.0
target distance 13.0
model initialize at round 4297
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  6.]), 'previousTarget': array([19.,  6.]), 'currentState': array([21.38678655, 19.47951063,  3.86267042]), 'targetState': array([19,  6], dtype=int32), 'currentDistance': 13.689191242415506}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9316086447509999
{'scaleFactor': 20, 'currentTarget': array([19.,  6.]), 'previousTarget': array([19.,  6.]), 'currentState': array([19.10475007,  5.93119407,  4.78725628]), 'targetState': array([19,  6], dtype=int32), 'currentDistance': 0.12532690505623517}
episode index:4298
target Thresh 32.0
target distance 6.0
model initialize at round 4298
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.76533387, 10.85632713,  4.89875031]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 4.916263736404386}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9316199244335422
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.51673053,  6.94113774,  5.06401426]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.07366228229193}
episode index:4299
target Thresh 32.0
target distance 16.0
model initialize at round 4299
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([18.01763778,  6.75576076,  3.1741287 ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 14.12716761426636}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9316200280203966
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.17017083, 5.01678794, 3.59826781]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.17099691713690232}
episode index:4300
target Thresh 32.0
target distance 12.0
model initialize at round 4300
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 15.]), 'previousTarget': array([ 9., 15.]), 'currentState': array([ 5.3454351 , 25.44953785,  5.32291436]), 'targetState': array([ 9, 15], dtype=int32), 'currentDistance': 11.070170997716875}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9316223205387366
{'scaleFactor': 20, 'currentTarget': array([ 9., 15.]), 'previousTarget': array([ 9., 15.]), 'currentState': array([ 9.35936668, 14.21446482,  4.79516199]), 'targetState': array([ 9, 15], dtype=int32), 'currentDistance': 0.8638344369155435}
episode index:4301
target Thresh 32.0
target distance 8.0
model initialize at round 4301
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([ 5.40497675, 11.53692542,  3.82688212]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 8.922625251098772}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9316268225678769
{'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([8.01292301, 2.67292658, 5.07665449]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 0.32732862487232983}
episode index:4302
target Thresh 32.0
target distance 8.0
model initialize at round 4302
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([16.07683247,  8.44109891,  3.20362151]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 6.273600675996505}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9316358098273312
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.46947154,  9.70456239,  2.0639922 ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.5546953288961733}
episode index:4303
target Thresh 32.0
target distance 15.0
model initialize at round 4303
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 29.]), 'previousTarget': array([18., 29.]), 'currentState': array([ 3.14218636, 23.67695293,  0.47621012]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 15.782568114699071}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9316337440477309
{'scaleFactor': 20, 'currentTarget': array([18., 29.]), 'previousTarget': array([18., 29.]), 'currentState': array([18.16422604, 29.07929485,  0.21036827]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 0.18236739337299615}
episode index:4304
target Thresh 32.0
target distance 15.0
model initialize at round 4304
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([14.38173729, 25.53781618,  4.42979121]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 16.304543684633696}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9316316792278425
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.2861488 , 11.2773156 ,  4.64614839]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.3984784501427519}
episode index:4305
target Thresh 32.0
target distance 10.0
model initialize at round 4305
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([5.089965  , 4.31943629, 5.77587056]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 11.951515605515073}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9316339663783704
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.27074879, 10.77347928,  6.14865875]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.7636222640736917}
episode index:4306
target Thresh 32.0
target distance 13.0
model initialize at round 4306
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([ 5.31713948, 26.98080242,  4.16299987]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 16.811446704495026}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9316297600354646
{'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([16.46346753, 13.62694186,  5.00664181]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 0.594957582355849}
episode index:4307
target Thresh 32.0
target distance 19.0
model initialize at round 4307
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 24.]), 'previousTarget': array([25., 24.]), 'currentState': array([8.64664597, 5.34777167, 1.21814125]), 'targetState': array([25, 24], dtype=int32), 'currentDistance': 24.80600350303867}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9316192575079996
{'scaleFactor': 20, 'currentTarget': array([25., 24.]), 'previousTarget': array([25., 24.]), 'currentState': array([24.09882301, 23.49375668,  1.32773665]), 'targetState': array([25, 24], dtype=int32), 'currentDistance': 1.0336354647682588}
episode index:4308
target Thresh 32.0
target distance 5.0
model initialize at round 4308
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 22.]), 'previousTarget': array([21., 22.]), 'currentState': array([20.52635353, 17.70896614,  1.44483298]), 'targetState': array([21, 22], dtype=int32), 'currentDistance': 4.317095381236117}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.931630508550583
{'scaleFactor': 20, 'currentTarget': array([21., 22.]), 'previousTarget': array([21., 22.]), 'currentState': array([20.83896194, 21.65683497,  1.14499002]), 'targetState': array([21, 22], dtype=int32), 'currentDistance': 0.3790718845204715}
episode index:4309
target Thresh 32.0
target distance 16.0
model initialize at round 4309
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([15.10578769,  3.42575328,  1.12096596]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 14.574630642021843}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9316306094413851
{'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([14.56968846, 17.28316994,  1.97494387]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 0.8360701898117454}
episode index:4310
target Thresh 32.0
target distance 2.0
model initialize at round 4310
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([25.49127428,  7.25390599,  2.59110594]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 0.5530087401335656}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9316464687293828
{'scaleFactor': 20, 'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([25.49127428,  7.25390599,  2.59110594]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 0.5530087401335656}
episode index:4311
target Thresh 32.0
target distance 25.0
model initialize at round 4311
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 29.]), 'previousTarget': array([17., 29.]), 'currentState': array([3.99686532, 5.35596741, 1.94685477]), 'targetState': array([17, 29], dtype=int32), 'currentDistance': 26.983731925224532}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9316318813787426
{'scaleFactor': 20, 'currentTarget': array([17., 29.]), 'previousTarget': array([17., 29.]), 'currentState': array([16.9416303 , 29.55228864,  0.72411739]), 'targetState': array([17, 29], dtype=int32), 'currentDistance': 0.5553645351721356}
episode index:4312
target Thresh 32.0
target distance 6.0
model initialize at round 4312
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 6.]), 'previousTarget': array([7., 6.]), 'currentState': array([ 8.65144379, 10.35349765,  3.49381053]), 'targetState': array([7, 6], dtype=int32), 'currentDistance': 4.6562010637857645}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9316431190598513
{'scaleFactor': 20, 'currentTarget': array([7., 6.]), 'previousTarget': array([7., 6.]), 'currentState': array([7.47619242, 6.9868694 , 3.62466085]), 'targetState': array([7, 6], dtype=int32), 'currentDistance': 1.095751079725319}
episode index:4313
target Thresh 32.0
target distance 19.0
model initialize at round 4313
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([ 8.66271695, 10.73969167,  0.85470455]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 20.145850952491877}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9316367998563161
{'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([25.78995007, 20.81896602,  0.96324644]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 0.277298164258811}
episode index:4314
target Thresh 32.0
target distance 15.0
model initialize at round 4314
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 29.]), 'previousTarget': array([20., 29.]), 'currentState': array([ 9.8161766 , 15.47186696,  0.05456507]), 'targetState': array([20, 29], dtype=int32), 'currentDistance': 16.93282736618723}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9316326006553026
{'scaleFactor': 20, 'currentTarget': array([20., 29.]), 'previousTarget': array([20., 29.]), 'currentState': array([20.35521777, 29.61506845,  0.74161705]), 'targetState': array([20, 29], dtype=int32), 'currentDistance': 0.7102737936299207}
episode index:4315
target Thresh 32.0
target distance 18.0
model initialize at round 4315
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 26.]), 'previousTarget': array([ 4., 26.]), 'currentState': array([21.30259172, 17.46833105,  3.27511501]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 19.291683580249785}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9316262868171085
{'scaleFactor': 20, 'currentTarget': array([ 4., 26.]), 'previousTarget': array([ 4., 26.]), 'currentState': array([ 3.49568954, 26.0628583 ,  2.1669094 ]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 0.5082127553211181}
episode index:4316
target Thresh 32.0
target distance 8.0
model initialize at round 4316
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([14.31653532, 14.16348179,  4.2147752 ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 7.064434547678151}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9316329974316979
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.39291978, 10.95274993,  3.63095254]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.6089162183864787}
episode index:4317
target Thresh 32.0
target distance 11.0
model initialize at round 4317
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([24.6017452 , 26.04513808,  4.73349822]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 11.909134367974945}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.931635277920806
{'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([15.44185723, 19.05565289,  4.10065752]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 0.4453482396952287}
episode index:4318
target Thresh 32.0
target distance 17.0
model initialize at round 4318
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([19.90893962, 14.28139585,  3.28613877]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 17.496087717663965}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9316310829612234
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.70014082, 6.68163375, 3.15197644]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.4373472296746053}
episode index:4319
target Thresh 32.0
target distance 10.0
model initialize at round 4319
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'previousTarget': array([25.,  3.]), 'currentState': array([14.24804496,  9.49436004,  5.25920105]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 12.561100649441876}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9316333628377141
{'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'previousTarget': array([25.,  3.]), 'currentState': array([24.44088883,  3.46136122,  5.89158152]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 0.7248858387749085}
episode index:4320
target Thresh 32.0
target distance 12.0
model initialize at round 4320
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([26.13418236,  4.24338991,  1.84129828]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 14.3403384226309}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.931633462811116
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13.63167116,  9.76692679,  2.44631765]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.6732990201442975}
episode index:4321
target Thresh 32.0
target distance 16.0
model initialize at round 4321
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([23.50730928, 24.25139676,  4.81222231]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 16.998964548940126}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9316292711833216
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.43849357,  8.33146947,  4.13534449]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.8730535721804197}
episode index:4322
target Thresh 32.0
target distance 21.0
model initialize at round 4322
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([20.3207691 , 29.11212348,  4.08492088]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 21.28144816644509}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9316208763147292
{'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([23.13934046,  7.71920408,  4.34738632]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 0.3134678801571567}
episode index:4323
target Thresh 32.0
target distance 10.0
model initialize at round 4323
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 10.]), 'previousTarget': array([22., 10.]), 'currentState': array([25.97292345, 18.44853988,  4.78776598]), 'targetState': array([22, 10], dtype=int32), 'currentDistance': 9.336056283308055}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9316253557720802
{'scaleFactor': 20, 'currentTarget': array([22., 10.]), 'previousTarget': array([22., 10.]), 'currentState': array([21.70459853, 10.07874522,  2.87430155]), 'targetState': array([22, 10], dtype=int32), 'currentDistance': 0.30571692514171744}
episode index:4324
target Thresh 32.0
target distance 17.0
model initialize at round 4324
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 19.]), 'previousTarget': array([12., 19.]), 'currentState': array([13.88934926,  3.69682239,  2.61629426]), 'targetState': array([12, 19], dtype=int32), 'currentDistance': 15.419367227674075}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9316233024399775
{'scaleFactor': 20, 'currentTarget': array([12., 19.]), 'previousTarget': array([12., 19.]), 'currentState': array([12.0208598 , 19.24673157,  2.04281123]), 'targetState': array([12, 19], dtype=int32), 'currentDistance': 0.24761179261069516}
episode index:4325
target Thresh 32.0
target distance 10.0
model initialize at round 4325
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 13.]), 'previousTarget': array([13., 13.]), 'currentState': array([4.30051179, 1.93179723, 0.32236307]), 'targetState': array([13, 13], dtype=int32), 'currentDistance': 14.077862326853483}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9316234046233957
{'scaleFactor': 20, 'currentTarget': array([13., 13.]), 'previousTarget': array([13., 13.]), 'currentState': array([12.86616876, 12.78061252,  0.69225924]), 'targetState': array([13, 13], dtype=int32), 'currentDistance': 0.2569857300189098}
episode index:4326
target Thresh 32.0
target distance 22.0
model initialize at round 4326
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 24.]), 'previousTarget': array([ 4., 24.]), 'currentState': array([24.32878614, 12.53138912,  1.94859183]), 'targetState': array([ 4, 24], dtype=int32), 'currentDistance': 23.34070653242712}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9316129496816561
{'scaleFactor': 20, 'currentTarget': array([ 4., 24.]), 'previousTarget': array([ 4., 24.]), 'currentState': array([ 3.67550692, 23.98291901,  2.42527992]), 'targetState': array([ 4, 24], dtype=int32), 'currentDistance': 0.3249423348692711}
episode index:4327
target Thresh 32.0
target distance 1.0
model initialize at round 4327
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 19.]), 'previousTarget': array([10., 19.]), 'currentState': array([10.93991257, 19.39837527,  1.98576873]), 'targetState': array([10, 19], dtype=int32), 'currentDistance': 1.0208518461206761}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9316287507561288
{'scaleFactor': 20, 'currentTarget': array([10., 19.]), 'previousTarget': array([10., 19.]), 'currentState': array([10.93991257, 19.39837527,  1.98576873]), 'targetState': array([10, 19], dtype=int32), 'currentDistance': 1.0208518461206761}
episode index:4328
target Thresh 32.0
target distance 20.0
model initialize at round 4328
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 29.]), 'previousTarget': array([27., 29.]), 'currentState': array([6.42680403, 9.59786496, 1.7684176 ]), 'targetState': array([27, 29], dtype=int32), 'currentDistance': 28.278953949196133}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9316142247829278
{'scaleFactor': 20, 'currentTarget': array([27., 29.]), 'previousTarget': array([27., 29.]), 'currentState': array([26.46280759, 28.44764253,  0.91418651]), 'targetState': array([27, 29], dtype=int32), 'currentDistance': 0.7705027351047662}
episode index:4329
target Thresh 32.0
target distance 23.0
model initialize at round 4329
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.20637575,  4.76423719]), 'previousTarget': array([25.63816566,  4.31463855]), 'currentState': array([ 3.59685689, 25.57357871,  0.19826198]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 30.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.931595712899859
{'scaleFactor': 20, 'currentTarget': array([26.,  4.]), 'previousTarget': array([26.,  4.]), 'currentState': array([26.11332156,  3.77650732,  6.10640303]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 0.25058083059587305}
episode index:4330
target Thresh 32.0
target distance 9.0
model initialize at round 4330
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 25.]), 'previousTarget': array([14., 25.]), 'currentState': array([21.10228729, 25.0934234 ,  3.64982939]), 'targetState': array([14, 25], dtype=int32), 'currentDistance': 7.102901703525489}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9316024088816415
{'scaleFactor': 20, 'currentTarget': array([14., 25.]), 'previousTarget': array([14., 25.]), 'currentState': array([13.42895259, 25.11433116,  3.50947863]), 'targetState': array([14, 25], dtype=int32), 'currentDistance': 0.5823802534278346}
episode index:4331
target Thresh 32.0
target distance 5.0
model initialize at round 4331
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([3.13530626, 6.34465061, 4.87410426]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 3.532082676684622}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9316136040781139
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.6872064 , 2.78309724, 4.73756999]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.38063978302551976}
episode index:4332
target Thresh 32.0
target distance 25.0
model initialize at round 4332
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 27.]), 'previousTarget': array([18., 27.]), 'currentState': array([17.61079904,  3.63734868,  0.79416728]), 'targetState': array([18, 27], dtype=int32), 'currentDistance': 23.365892967518935}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9316031658753994
{'scaleFactor': 20, 'currentTarget': array([18., 27.]), 'previousTarget': array([18., 27.]), 'currentState': array([18.09143889, 27.37995355,  1.40104684]), 'targetState': array([18, 27], dtype=int32), 'currentDistance': 0.3908014448116867}
episode index:4333
target Thresh 32.0
target distance 10.0
model initialize at round 4333
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 16.]), 'previousTarget': array([ 7., 16.]), 'currentState': array([4.58821742, 7.91061472, 1.21999721]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 8.441258761621478}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9316098555025624
{'scaleFactor': 20, 'currentTarget': array([ 7., 16.]), 'previousTarget': array([ 7., 16.]), 'currentState': array([ 6.66321872, 15.58505382,  1.6962448 ]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 0.5344174095586341}
episode index:4334
target Thresh 32.0
target distance 22.0
model initialize at round 4334
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  8.]), 'previousTarget': array([24.,  8.]), 'currentState': array([ 3.27846952, 28.0944879 ,  5.98120565]), 'targetState': array([24,  8], dtype=int32), 'currentDistance': 28.864688974195612}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9315953539932812
{'scaleFactor': 20, 'currentTarget': array([24.,  8.]), 'previousTarget': array([24.,  8.]), 'currentState': array([23.39455527,  8.9098983 ,  5.8133218 ]), 'targetState': array([24,  8], dtype=int32), 'currentDistance': 1.0929218819726092}
episode index:4335
target Thresh 32.0
target distance 24.0
model initialize at round 4335
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 21.]), 'previousTarget': array([ 2., 21.]), 'currentState': array([25.80041855,  7.32890595,  3.58352041]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 27.447381213779344}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9315808591728881
{'scaleFactor': 20, 'currentTarget': array([ 2., 21.]), 'previousTarget': array([ 2., 21.]), 'currentState': array([ 1.99268162, 20.98134197,  3.12746579]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 0.02004197331847782}
episode index:4336
target Thresh 32.0
target distance 10.0
model initialize at round 4336
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 22.]), 'previousTarget': array([20., 22.]), 'currentState': array([10.98669905, 17.63661705,  0.06566828]), 'targetState': array([20, 22], dtype=int32), 'currentDistance': 10.013925546611203}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9315853344301459
{'scaleFactor': 20, 'currentTarget': array([20., 22.]), 'previousTarget': array([20., 22.]), 'currentState': array([19.84703064, 21.95560733,  0.85828232]), 'targetState': array([20, 22], dtype=int32), 'currentDistance': 0.15928067976135543}
episode index:4337
target Thresh 32.0
target distance 3.0
model initialize at round 4337
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 27.]), 'previousTarget': array([18., 27.]), 'currentState': array([19.67561183, 23.84279602,  0.91645497]), 'targetState': array([18, 27], dtype=int32), 'currentDistance': 3.574298806133981}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9315965180782718
{'scaleFactor': 20, 'currentTarget': array([18., 27.]), 'previousTarget': array([18., 27.]), 'currentState': array([18.02039596, 26.90299629,  1.49978369]), 'targetState': array([18, 27], dtype=int32), 'currentDistance': 0.09912474389313367}
episode index:4338
target Thresh 32.0
target distance 14.0
model initialize at round 4338
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 25.]), 'previousTarget': array([ 8., 25.]), 'currentState': array([20.97638851, 27.33589206,  3.23460913]), 'targetState': array([ 8, 25], dtype=int32), 'currentDistance': 13.18495545543978}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9315966261284744
{'scaleFactor': 20, 'currentTarget': array([ 8., 25.]), 'previousTarget': array([ 8., 25.]), 'currentState': array([ 7.39550361, 24.76699534,  2.61304697]), 'targetState': array([ 8, 25], dtype=int32), 'currentDistance': 0.6478480214233586}
episode index:4339
target Thresh 32.0
target distance 13.0
model initialize at round 4339
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  4.]), 'previousTarget': array([27.,  4.]), 'currentState': array([13.61386494,  5.63807442,  0.79229522]), 'targetState': array([27,  4], dtype=int32), 'currentDistance': 13.485989016811352}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9315967341288842
{'scaleFactor': 20, 'currentTarget': array([27.,  4.]), 'previousTarget': array([27.,  4.]), 'currentState': array([27.20100518,  3.97883483,  5.73159544]), 'targetState': array([27,  4], dtype=int32), 'currentDistance': 0.20211641587768336}
episode index:4340
target Thresh 32.0
target distance 18.0
model initialize at round 4340
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  6.]), 'previousTarget': array([27.,  6.]), 'currentState': array([8.62256264, 6.35989966, 5.49619651]), 'targetState': array([27,  6], dtype=int32), 'currentDistance': 18.38096111986152}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9315925693081872
{'scaleFactor': 20, 'currentTarget': array([27.,  6.]), 'previousTarget': array([27.,  6.]), 'currentState': array([26.3440757 ,  6.25239765,  0.54151245]), 'targetState': array([27,  6], dtype=int32), 'currentDistance': 0.7028095475265153}
episode index:4341
target Thresh 32.0
target distance 11.0
model initialize at round 4341
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  6.]), 'previousTarget': array([18.,  6.]), 'currentState': array([7.71371513, 7.47586129, 6.16033554]), 'targetState': array([18,  6], dtype=int32), 'currentDistance': 10.391622731229146}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9315970367150486
{'scaleFactor': 20, 'currentTarget': array([18.,  6.]), 'previousTarget': array([18.,  6.]), 'currentState': array([17.55866517,  6.27095155,  0.35881151]), 'targetState': array([18,  6], dtype=int32), 'currentDistance': 0.5178717742571026}
episode index:4342
target Thresh 32.0
target distance 10.0
model initialize at round 4342
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([19.67369099, 27.82351609,  5.1681276 ]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 9.828934123794001}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9316015020646192
{'scaleFactor': 20, 'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([20.08222501, 18.09759877,  4.00381962]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 0.1276184644005338}
episode index:4343
target Thresh 32.0
target distance 19.0
model initialize at round 4343
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([11.64130218, 24.15851988,  4.65465027]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 17.31988047903653}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9315973390225886
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.92312887,  6.38205201,  4.87881835]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.6227109233027721}
episode index:4344
target Thresh 32.0
target distance 14.0
model initialize at round 4344
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 23.]), 'previousTarget': array([21., 23.]), 'currentState': array([12.7429213 , 10.11899116,  1.40424514]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 15.300318213490081}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9315953015900006
{'scaleFactor': 20, 'currentTarget': array([21., 23.]), 'previousTarget': array([21., 23.]), 'currentState': array([21.27111866, 23.53262184,  0.76213334]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 0.5976548812706408}
episode index:4345
target Thresh 32.0
target distance 11.0
model initialize at round 4345
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 13.]), 'previousTarget': array([25., 13.]), 'currentState': array([24.7455509 ,  3.66362401,  0.71256936]), 'targetState': array([25, 13], dtype=int32), 'currentDistance': 9.3398426679562}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9315997642564318
{'scaleFactor': 20, 'currentTarget': array([25., 13.]), 'previousTarget': array([25., 13.]), 'currentState': array([25.08256616, 13.24787341,  1.90371788]), 'targetState': array([25, 13], dtype=int32), 'currentDistance': 0.2612630803562356}
episode index:4346
target Thresh 32.0
target distance 26.0
model initialize at round 4346
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 28.]), 'previousTarget': array([ 9., 28.]), 'currentState': array([21.41084906,  3.6320512 ,  2.33418271]), 'targetState': array([ 9, 28], dtype=int32), 'currentDistance': 27.34640932499937}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.93158530510035
{'scaleFactor': 20, 'currentTarget': array([ 9., 28.]), 'previousTarget': array([ 9., 28.]), 'currentState': array([ 8.78413582, 28.53352812,  2.01304242]), 'targetState': array([ 9, 28], dtype=int32), 'currentDistance': 0.5755428672461812}
episode index:4347
target Thresh 32.0
target distance 17.0
model initialize at round 4347
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 19.]), 'previousTarget': array([ 9., 19.]), 'currentState': array([24.2275893 , 17.54651937,  3.51669812]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 15.296799727331733}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9315832718412257
{'scaleFactor': 20, 'currentTarget': array([ 9., 19.]), 'previousTarget': array([ 9., 19.]), 'currentState': array([ 8.40755833, 19.09713882,  2.91194772]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 0.6003524647817086}
episode index:4348
target Thresh 32.0
target distance 12.0
model initialize at round 4348
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 18.]), 'previousTarget': array([25., 18.]), 'currentState': array([14.03770588, 16.67502657,  0.10361021]), 'targetState': array([25, 18], dtype=int32), 'currentDistance': 11.042076201840013}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9315855475086342
{'scaleFactor': 20, 'currentTarget': array([25., 18.]), 'previousTarget': array([25., 18.]), 'currentState': array([25.88911654, 18.21340568,  6.18432846]), 'targetState': array([25, 18], dtype=int32), 'currentDistance': 0.914368742658633}
episode index:4349
target Thresh 32.0
target distance 16.0
model initialize at round 4349
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([10.11601959, 23.56597601,  4.61493909]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 15.709515669353681}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9315835151286156
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.02561327,  8.84242694,  5.42659266]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.15964119035895477}
episode index:4350
target Thresh 32.0
target distance 12.0
model initialize at round 4350
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  4.]), 'previousTarget': array([18.,  4.]), 'currentState': array([19.56940963, 14.37292206,  5.4634788 ]), 'targetState': array([18,  4], dtype=int32), 'currentDistance': 10.490975104522361}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9315879753756328
{'scaleFactor': 20, 'currentTarget': array([18.,  4.]), 'previousTarget': array([18.,  4.]), 'currentState': array([18.34793697,  4.86853638,  3.96135038]), 'targetState': array([18,  4], dtype=int32), 'currentDistance': 0.9356364551419425}
episode index:4351
target Thresh 32.0
target distance 4.0
model initialize at round 4351
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 28.]), 'previousTarget': array([22., 28.]), 'currentState': array([20.25192605, 25.55845919,  0.83950013]), 'targetState': array([22, 28], dtype=int32), 'currentDistance': 3.002812692297568}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9316013972562909
{'scaleFactor': 20, 'currentTarget': array([22., 28.]), 'previousTarget': array([22., 28.]), 'currentState': array([21.15888878, 27.31623554,  1.35455686]), 'targetState': array([22, 28], dtype=int32), 'currentDistance': 1.0839750579299317}
episode index:4352
target Thresh 32.0
target distance 11.0
model initialize at round 4352
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 6.]), 'previousTarget': array([9., 6.]), 'currentState': array([ 1.58838618, 15.3681415 ,  5.47530675]), 'targetState': array([9, 6], dtype=int32), 'currentDistance': 11.945463350146117}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9316036666686833
{'scaleFactor': 20, 'currentTarget': array([9., 6.]), 'previousTarget': array([9., 6.]), 'currentState': array([9.07401993, 6.06907025, 5.76366838]), 'targetState': array([9, 6], dtype=int32), 'currentDistance': 0.10124055165631816}
episode index:4353
target Thresh 32.0
target distance 19.0
model initialize at round 4353
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([ 6.17216614, 22.32585935,  5.82486725]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 24.276929227999737}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9315932810933613
{'scaleFactor': 20, 'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([24.86791446,  7.58979159,  5.80084765]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 0.6044011172454493}
episode index:4354
target Thresh 32.0
target distance 21.0
model initialize at round 4354
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 29.]), 'previousTarget': array([25., 29.]), 'currentState': array([25.35809138,  8.36949177,  1.91024208]), 'targetState': array([25, 29], dtype=int32), 'currentDistance': 20.63361576186768}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9315870328256037
{'scaleFactor': 20, 'currentTarget': array([25., 29.]), 'previousTarget': array([25., 29.]), 'currentState': array([24.54052101, 28.22836342,  2.02276495]), 'targetState': array([25, 29], dtype=int32), 'currentDistance': 0.8980779212079467}
episode index:4355
target Thresh 32.0
target distance 14.0
model initialize at round 4355
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 29.]), 'previousTarget': array([24., 29.]), 'currentState': array([22.69840682, 16.06688484,  1.4449687 ]), 'targetState': array([24, 29], dtype=int32), 'currentDistance': 12.998446541633184}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9315871426316371
{'scaleFactor': 20, 'currentTarget': array([24., 29.]), 'previousTarget': array([24., 29.]), 'currentState': array([24.21540078, 29.39032255,  6.27809331]), 'targetState': array([24, 29], dtype=int32), 'currentDistance': 0.4458129583306524}
episode index:4356
target Thresh 32.0
target distance 10.0
model initialize at round 4356
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 24.]), 'previousTarget': array([ 2., 24.]), 'currentState': array([10.87030564, 26.24746893,  3.3166908 ]), 'targetState': array([ 2, 24], dtype=int32), 'currentDistance': 9.150597725693006}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9315915959039044
{'scaleFactor': 20, 'currentTarget': array([ 2., 24.]), 'previousTarget': array([ 2., 24.]), 'currentState': array([ 1.33229607, 23.62993958,  2.84922694]), 'targetState': array([ 2, 24], dtype=int32), 'currentDistance': 0.7633958695442595}
episode index:4357
target Thresh 32.0
target distance 8.0
model initialize at round 4357
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 4.]), 'previousTarget': array([6., 4.]), 'currentState': array([ 4.58905477, 10.09932217,  5.18917912]), 'targetState': array([6, 4], dtype=int32), 'currentDistance': 6.260391148811891}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9316004778231555
{'scaleFactor': 20, 'currentTarget': array([6., 4.]), 'previousTarget': array([6., 4.]), 'currentState': array([6.06367088, 4.51729167, 4.14173737]), 'targetState': array([6, 4], dtype=int32), 'currentDistance': 0.521195410539163}
episode index:4358
target Thresh 32.0
target distance 4.0
model initialize at round 4358
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 15.]), 'previousTarget': array([ 6., 15.]), 'currentState': array([ 3.48371556, 13.20565988,  0.51845663]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 3.090524852763333}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9316116041186767
{'scaleFactor': 20, 'currentTarget': array([ 6., 15.]), 'previousTarget': array([ 6., 15.]), 'currentState': array([ 6.5140177 , 15.46155359,  6.09157014]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 0.6908298719127248}
episode index:4359
target Thresh 32.0
target distance 14.0
model initialize at round 4359
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 13.]), 'previousTarget': array([17., 13.]), 'currentState': array([19.43575278, 25.09407345,  4.61880532]), 'targetState': array([17, 13], dtype=int32), 'currentDistance': 12.336916316699963}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9316138675464937
{'scaleFactor': 20, 'currentTarget': array([17., 13.]), 'previousTarget': array([17., 13.]), 'currentState': array([17.12751383, 13.4171875 ,  5.03130792]), 'targetState': array([17, 13], dtype=int32), 'currentDistance': 0.4362398237709158}
episode index:4360
target Thresh 32.0
target distance 20.0
model initialize at round 4360
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([ 5.93332762, 15.49425951,  5.95694034]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 18.401480930103833}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9316097178973162
{'scaleFactor': 20, 'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([23.52541124, 12.31363798,  0.3301156 ]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 0.5688613812795983}
episode index:4361
target Thresh 32.0
target distance 22.0
model initialize at round 4361
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 7.]), 'previousTarget': array([5., 7.]), 'currentState': array([10.65497582, 27.61589032,  4.4066813 ]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 21.3774106290598}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9316014025686508
{'scaleFactor': 20, 'currentTarget': array([5., 7.]), 'previousTarget': array([5., 7.]), 'currentState': array([5.00022389, 6.48053433, 4.2105192 ]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 0.5194657180370482}
episode index:4362
target Thresh 32.0
target distance 12.0
model initialize at round 4362
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  6.]), 'previousTarget': array([20.,  6.]), 'currentState': array([20.69120194, 16.02869996,  4.43753247]), 'targetState': array([20,  6], dtype=int32), 'currentDistance': 10.05249137938457}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9316058464483967
{'scaleFactor': 20, 'currentTarget': array([20.,  6.]), 'previousTarget': array([20.,  6.]), 'currentState': array([20.04049223,  6.11968225,  5.07530916]), 'targetState': array([20,  6], dtype=int32), 'currentDistance': 0.12634658775476984}
episode index:4363
target Thresh 32.0
target distance 18.0
model initialize at round 4363
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 7.]), 'previousTarget': array([7., 7.]), 'currentState': array([20.29728508, 23.34349474,  3.87996387]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 21.069589720783227}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9315996081872969
{'scaleFactor': 20, 'currentTarget': array([7., 7.]), 'previousTarget': array([7., 7.]), 'currentState': array([7.82507942, 7.9483637 , 4.4582463 ]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 1.2570400799407184}
episode index:4364
target Thresh 32.0
target distance 6.0
model initialize at round 4364
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 28.]), 'previousTarget': array([ 7., 28.]), 'currentState': array([11.42669538, 23.58231157,  2.92840505]), 'targetState': array([ 7, 28], dtype=int32), 'currentDistance': 6.253927011235814}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9316084740273456
{'scaleFactor': 20, 'currentTarget': array([ 7., 28.]), 'previousTarget': array([ 7., 28.]), 'currentState': array([ 7.38208617, 27.52063449,  1.46126187]), 'targetState': array([ 7, 28], dtype=int32), 'currentDistance': 0.6130098939978782}
episode index:4365
target Thresh 32.0
target distance 9.0
model initialize at round 4365
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([17.59747576, 28.52958406,  5.59329671]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 10.497389770166963}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9316129132339129
{'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([21.70351825, 19.48911448,  5.11395442]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 0.5719566416277406}
episode index:4366
target Thresh 32.0
target distance 11.0
model initialize at round 4366
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([16.44443943, 19.58862851,  2.9172135 ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 14.87297443610735}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9316130168369982
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.97326152, 9.81414164, 4.32342822]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 1.2688832109637547}
episode index:4367
target Thresh 32.0
target distance 4.0
model initialize at round 4367
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 8.]), 'previousTarget': array([7., 8.]), 'currentState': array([ 2.58492759, 11.08897961,  4.72359538]), 'targetState': array([7, 8], dtype=int32), 'currentDistance': 5.388381896355461}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9316218735181252
{'scaleFactor': 20, 'currentTarget': array([7., 8.]), 'previousTarget': array([7., 8.]), 'currentState': array([6.96346188, 8.04997164, 0.07562583]), 'targetState': array([7, 8], dtype=int32), 'currentDistance': 0.061904756553989644}
episode index:4368
target Thresh 32.0
target distance 4.0
model initialize at round 4368
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 19.]), 'previousTarget': array([ 9., 19.]), 'currentState': array([10.95038137, 16.68223841,  2.61028337]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 3.0291923475057727}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9316352354147792
{'scaleFactor': 20, 'currentTarget': array([ 9., 19.]), 'previousTarget': array([ 9., 19.]), 'currentState': array([ 9.78030252, 18.2389043 ,  1.8124525 ]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 1.090017743951033}
episode index:4369
target Thresh 32.0
target distance 1.0
model initialize at round 4369
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([ 8.64431303, 14.64495436,  0.77374697]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 1.5012838834454272}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9316485911961487
{'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([10.28808589, 14.28384036,  5.05693669]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 0.4044240756565119}
episode index:4370
target Thresh 32.0
target distance 10.0
model initialize at round 4370
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  9.]), 'previousTarget': array([17.,  9.]), 'currentState': array([21.67679772, 18.85599478,  5.18751443]), 'targetState': array([17,  9], dtype=int32), 'currentDistance': 10.909311164232584}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9316508404659278
{'scaleFactor': 20, 'currentTarget': array([17.,  9.]), 'previousTarget': array([17.,  9.]), 'currentState': array([16.3205382 ,  8.73623719,  4.14883435]), 'targetState': array([17,  9], dtype=int32), 'currentDistance': 0.7288615442712001}
episode index:4371
target Thresh 32.0
target distance 15.0
model initialize at round 4371
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 24.]), 'previousTarget': array([ 2., 24.]), 'currentState': array([17.71814312, 21.52205734,  2.13994254]), 'targetState': array([ 2, 24], dtype=int32), 'currentDistance': 15.912266432084667}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9316488033785449
{'scaleFactor': 20, 'currentTarget': array([ 2., 24.]), 'previousTarget': array([ 2., 24.]), 'currentState': array([ 2.22904907, 23.9182444 ,  3.50632206]), 'targetState': array([ 2, 24], dtype=int32), 'currentDistance': 0.243202496120507}
episode index:4372
target Thresh 32.0
target distance 6.0
model initialize at round 4372
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  4.]), 'previousTarget': array([27.,  4.]), 'currentState': array([22.64515303,  2.8848997 ,  6.13471324]), 'targetState': array([27,  4], dtype=int32), 'currentDistance': 4.495346574279705}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9316598830027438
{'scaleFactor': 20, 'currentTarget': array([27.,  4.]), 'previousTarget': array([27.,  4.]), 'currentState': array([26.28351071,  3.89350893,  5.83256835]), 'targetState': array([27,  4], dtype=int32), 'currentDistance': 0.7243598937818373}
episode index:4373
target Thresh 32.0
target distance 6.0
model initialize at round 4373
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 18.]), 'previousTarget': array([21., 18.]), 'currentState': array([26.81440883, 11.32548701,  3.59502256]), 'targetState': array([21, 18], dtype=int32), 'currentDistance': 8.851919214300082}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9316643023367396
{'scaleFactor': 20, 'currentTarget': array([21., 18.]), 'previousTarget': array([21., 18.]), 'currentState': array([20.90864522, 18.25743569,  1.3450017 ]), 'targetState': array([21, 18], dtype=int32), 'currentDistance': 0.27316447865684623}
episode index:4374
target Thresh 32.0
target distance 7.0
model initialize at round 4374
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  3.]), 'previousTarget': array([26.,  3.]), 'currentState': array([19.82022086,  1.53043346,  6.23145866]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 6.35210957031898}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9316731331247768
{'scaleFactor': 20, 'currentTarget': array([26.,  3.]), 'previousTarget': array([26.,  3.]), 'currentState': array([25.5052358 ,  2.81099296,  0.70106697]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 0.5296369323338825}
episode index:4375
target Thresh 32.0
target distance 15.0
model initialize at round 4375
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 3.]), 'previousTarget': array([6., 3.]), 'currentState': array([20.3435862 ,  3.54968029,  2.98146373]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 14.35411485978617}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9316732227533835
{'scaleFactor': 20, 'currentTarget': array([6., 3.]), 'previousTarget': array([6., 3.]), 'currentState': array([6.77959395, 3.10669889, 2.40097121]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 0.7868617236783471}
episode index:4376
target Thresh 32.0
target distance 5.0
model initialize at round 4376
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([16.61400367, 15.52315597,  0.72273033]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 5.511154242679231}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9316820474683131
{'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([16.12431372, 20.61038225,  2.66778737]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 0.40896930193548064}
episode index:4377
target Thresh 32.0
target distance 8.0
model initialize at round 4377
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([16.38457535,  9.37172196,  1.17727485]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 8.687178128817557}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9316886518453189
{'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([21.21241225, 15.48160221,  1.46909752]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 0.9428842617417716}
episode index:4378
target Thresh 32.0
target distance 6.0
model initialize at round 4378
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 14.]), 'previousTarget': array([12., 14.]), 'currentState': array([18.12507303, 18.32168393,  3.77677441]), 'targetState': array([12, 14], dtype=int32), 'currentDistance': 7.4962304910444635}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9316952532059388
{'scaleFactor': 20, 'currentTarget': array([12., 14.]), 'previousTarget': array([12., 14.]), 'currentState': array([11.58083027, 13.78341479,  3.97434231]), 'targetState': array([12, 14], dtype=int32), 'currentDistance': 0.471818201839112}
episode index:4379
target Thresh 32.0
target distance 17.0
model initialize at round 4379
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 14.]), 'previousTarget': array([27., 14.]), 'currentState': array([ 9.44864858, 19.40990576,  5.38862014]), 'targetState': array([27, 14], dtype=int32), 'currentDistance': 18.366192226174235}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9316911029763218
{'scaleFactor': 20, 'currentTarget': array([27., 14.]), 'previousTarget': array([27., 14.]), 'currentState': array([26.34203104, 14.30745466,  0.43798536]), 'targetState': array([27, 14], dtype=int32), 'currentDistance': 0.7262585776933148}
episode index:4380
target Thresh 32.0
target distance 17.0
model initialize at round 4380
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'previousTarget': array([13., 22.]), 'currentState': array([27.36768231,  5.98073094,  1.63209408]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 21.518533318604867}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9316828051336563
{'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'previousTarget': array([13., 22.]), 'currentState': array([12.9409669 , 22.27268254,  1.97021192]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 0.2789994186456737}
episode index:4381
target Thresh 32.0
target distance 5.0
model initialize at round 4381
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 20.]), 'previousTarget': array([21., 20.]), 'currentState': array([17.68246397, 18.04126845,  1.03452366]), 'targetState': array([21, 20], dtype=int32), 'currentDistance': 3.85261918861579}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9316938542424802
{'scaleFactor': 20, 'currentTarget': array([21., 20.]), 'previousTarget': array([21., 20.]), 'currentState': array([21.05475362, 19.87740784,  0.94338065]), 'targetState': array([21, 20], dtype=int32), 'currentDistance': 0.13426390813048122}
episode index:4382
target Thresh 32.0
target distance 6.0
model initialize at round 4382
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 28.]), 'previousTarget': array([20., 28.]), 'currentState': array([16.68581017, 23.05132924,  1.45686221]), 'targetState': array([20, 28], dtype=int32), 'currentDistance': 5.955937925556503}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9317026621698719
{'scaleFactor': 20, 'currentTarget': array([20., 28.]), 'previousTarget': array([20., 28.]), 'currentState': array([19.84407626, 27.65152867,  6.24014968]), 'targetState': array([20, 28], dtype=int32), 'currentDistance': 0.3817649506249917}
episode index:4383
target Thresh 32.0
target distance 12.0
model initialize at round 4383
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  8.]), 'previousTarget': array([24.,  8.]), 'currentState': array([13.71463565, 12.23413774,  0.19695943]), 'targetState': array([24,  8], dtype=int32), 'currentDistance': 11.122798307444413}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9317048924361199
{'scaleFactor': 20, 'currentTarget': array([24.,  8.]), 'previousTarget': array([24.,  8.]), 'currentState': array([24.19992168,  7.44037985,  5.81178713]), 'targetState': array([24,  8], dtype=int32), 'currentDistance': 0.5942586863249342}
episode index:4384
target Thresh 32.0
target distance 20.0
model initialize at round 4384
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 28.]), 'previousTarget': array([10., 28.]), 'currentState': array([11.41628892,  9.86438422,  1.49614519]), 'targetState': array([10, 28], dtype=int32), 'currentDistance': 18.1908337945616}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9317007447405776
{'scaleFactor': 20, 'currentTarget': array([10., 28.]), 'previousTarget': array([10., 28.]), 'currentState': array([ 9.96894218, 27.642427  ,  2.18630105]), 'targetState': array([10, 28], dtype=int32), 'currentDistance': 0.35891926202013946}
episode index:4385
target Thresh 32.0
target distance 15.0
model initialize at round 4385
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 26.]), 'previousTarget': array([12., 26.]), 'currentState': array([25.13227328, 23.71471455,  2.80054962]), 'targetState': array([12, 26], dtype=int32), 'currentDistance': 13.329633569497732}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9317008278694345
{'scaleFactor': 20, 'currentTarget': array([12., 26.]), 'previousTarget': array([12., 26.]), 'currentState': array([11.5502303 , 25.94427671,  2.41430305]), 'targetState': array([12, 26], dtype=int32), 'currentDistance': 0.4532084194478841}
episode index:4386
target Thresh 32.0
target distance 3.0
model initialize at round 4386
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 28.]), 'previousTarget': array([ 5., 28.]), 'currentState': array([ 6.33820609, 24.73386266,  2.29039478]), 'targetState': array([ 5, 28], dtype=int32), 'currentDistance': 3.52965277043161}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9317118602770321
{'scaleFactor': 20, 'currentTarget': array([ 5., 28.]), 'previousTarget': array([ 5., 28.]), 'currentState': array([ 4.99555004, 28.38163077,  2.38041472]), 'targetState': array([ 5, 28], dtype=int32), 'currentDistance': 0.3816567155361558}
episode index:4387
target Thresh 32.0
target distance 24.0
model initialize at round 4387
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 29.]), 'previousTarget': array([ 5., 29.]), 'currentState': array([20.71343077,  6.65839264,  2.75190598]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 27.31408658036953}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9316975106764149
{'scaleFactor': 20, 'currentTarget': array([ 5., 29.]), 'previousTarget': array([ 5., 29.]), 'currentState': array([ 4.66268419, 29.15868543,  1.61339822]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 0.37277743858064216}
episode index:4388
target Thresh 32.0
target distance 5.0
model initialize at round 4388
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([12.75262647, 17.8702003 ,  4.88757157]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 4.066250251528787}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9317085388125106
{'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([13.96727712, 14.35586838,  4.10745335]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 0.3573696828383529}
episode index:4389
target Thresh 32.0
target distance 10.0
model initialize at round 4389
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  3.]), 'previousTarget': array([17.,  3.]), 'currentState': array([ 6.53990246, 11.38114289,  5.44547939]), 'targetState': array([17,  3], dtype=int32), 'currentDistance': 13.403626252018421}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9317086200902087
{'scaleFactor': 20, 'currentTarget': array([17.,  3.]), 'previousTarget': array([17.,  3.]), 'currentState': array([17.25880279,  2.83748971,  4.73340709]), 'targetState': array([17,  3], dtype=int32), 'currentDistance': 0.30559528662395447}
episode index:4390
target Thresh 32.0
target distance 10.0
model initialize at round 4390
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([21.11764958,  5.62394392,  2.75818658]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 10.991781071008846}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9317108454441851
{'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([13.4014648 , 14.29186893,  2.9388347 ]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 0.6659067945067072}
episode index:4391
target Thresh 32.0
target distance 20.0
model initialize at round 4391
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 14.]), 'previousTarget': array([25., 14.]), 'currentState': array([6.74963497, 9.8505273 , 0.04203546]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 18.71614136017713}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9317067030038481
{'scaleFactor': 20, 'currentTarget': array([25., 14.]), 'previousTarget': array([25., 14.]), 'currentState': array([24.0673748 , 14.11730714,  0.84149338]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 0.9399737943402307}
episode index:4392
target Thresh 32.0
target distance 25.0
model initialize at round 4392
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 28.]), 'previousTarget': array([ 6.13069936, 27.80779506]), 'currentState': array([22.06747313,  4.40099316,  3.16807282]), 'targetState': array([ 6, 28], dtype=int32), 'currentDistance': 28.549550197518705}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9316923709095537
{'scaleFactor': 20, 'currentTarget': array([ 6., 28.]), 'previousTarget': array([ 6., 28.]), 'currentState': array([ 6.49778038, 27.12678813,  2.44489381]), 'targetState': array([ 6, 28], dtype=int32), 'currentDistance': 1.0051289888993693}
episode index:4393
target Thresh 32.0
target distance 17.0
model initialize at round 4393
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 10.]), 'previousTarget': array([23., 10.]), 'currentState': array([ 7.67720951, 20.13912703,  5.35594732]), 'targetState': array([23, 10], dtype=int32), 'currentDistance': 18.3736170978846}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9316882345592064
{'scaleFactor': 20, 'currentTarget': array([23., 10.]), 'previousTarget': array([23., 10.]), 'currentState': array([22.67615545, 10.45906802,  6.16190989]), 'targetState': array([23, 10], dtype=int32), 'currentDistance': 0.561799550831715}
episode index:4394
target Thresh 32.0
target distance 16.0
model initialize at round 4394
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 25.]), 'previousTarget': array([10., 25.]), 'currentState': array([25.87856313, 29.67858308,  2.65301538]), 'targetState': array([10, 25], dtype=int32), 'currentDistance': 16.55348624352465}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9316861996240229
{'scaleFactor': 20, 'currentTarget': array([10., 25.]), 'previousTarget': array([10., 25.]), 'currentState': array([10.97828298, 24.80019649,  3.89237023]), 'targetState': array([10, 25], dtype=int32), 'currentDistance': 0.9984783624049988}
episode index:4395
target Thresh 32.0
target distance 7.0
model initialize at round 4395
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  4.]), 'previousTarget': array([19.,  4.]), 'currentState': array([24.47995424,  5.92483705,  3.10006642]), 'targetState': array([19,  4], dtype=int32), 'currentDistance': 5.808174941741884}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9316949832455825
{'scaleFactor': 20, 'currentTarget': array([19.,  4.]), 'previousTarget': array([19.,  4.]), 'currentState': array([19.14269756,  3.8722085 ,  3.75055939]), 'targetState': array([19,  4], dtype=int32), 'currentDistance': 0.1915548492089087}
episode index:4396
target Thresh 32.0
target distance 16.0
model initialize at round 4396
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 27.]), 'previousTarget': array([ 6., 27.]), 'currentState': array([21.86418318, 19.67748081,  2.66158485]), 'targetState': array([ 6, 27], dtype=int32), 'currentDistance': 17.472595552906267}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9316908491232805
{'scaleFactor': 20, 'currentTarget': array([ 6., 27.]), 'previousTarget': array([ 6., 27.]), 'currentState': array([ 5.75668633, 27.15256198,  1.8178157 ]), 'targetState': array([ 6, 27], dtype=int32), 'currentDistance': 0.287187568304721}
episode index:4397
target Thresh 32.0
target distance 7.0
model initialize at round 4397
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([17.28770305, 11.524802  ,  3.01781261]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 9.806011156618338}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9316952372999009
{'scaleFactor': 20, 'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([11.53354783,  4.28019615,  4.46640372]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 0.6026467979411804}
episode index:4398
target Thresh 32.0
target distance 11.0
model initialize at round 4398
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 28.]), 'previousTarget': array([17., 28.]), 'currentState': array([ 7.32071799, 17.95688348,  0.34150332]), 'targetState': array([17, 28], dtype=int32), 'currentDistance': 13.94821456676114}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9316953214350696
{'scaleFactor': 20, 'currentTarget': array([17., 28.]), 'previousTarget': array([17., 28.]), 'currentState': array([16.89162783, 27.88991924,  1.27741487]), 'targetState': array([17, 28], dtype=int32), 'currentDistance': 0.15447426764073754}
episode index:4399
target Thresh 32.0
target distance 7.0
model initialize at round 4399
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 20.]), 'previousTarget': array([14., 20.]), 'currentState': array([13.40629235, 27.54081792,  3.82444072]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 7.564153863244783}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9317018897733798
{'scaleFactor': 20, 'currentTarget': array([14., 20.]), 'previousTarget': array([14., 20.]), 'currentState': array([13.82400273, 20.47585522,  3.59109569]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 0.5073590712141295}
episode index:4400
target Thresh 32.0
target distance 8.0
model initialize at round 4400
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([6.30077554, 9.06788158, 5.96057529]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 6.972280109685791}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9317084551267599
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.38291134, 10.76765148,  5.23316044]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.4478916500257048}
episode index:4401
target Thresh 32.0
target distance 10.0
model initialize at round 4401
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([ 9.63902574, 25.0678633 ,  4.20221734]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 8.232668070807096}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9317150174972445
{'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([ 8.05393141, 17.60032483,  3.62117153]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 0.6027424805252601}
episode index:4402
target Thresh 32.0
target distance 11.0
model initialize at round 4402
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  5.]), 'previousTarget': array([27.,  5.]), 'currentState': array([24.37932663, 15.0357147 ,  4.66306457]), 'targetState': array([27,  5], dtype=int32), 'currentDistance': 10.372246549580817}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9317193952016285
{'scaleFactor': 20, 'currentTarget': array([27.,  5.]), 'previousTarget': array([27.,  5.]), 'currentState': array([26.82832895,  5.53693146,  5.51269478]), 'targetState': array([27,  5], dtype=int32), 'currentDistance': 0.5637076725271636}
episode index:4403
target Thresh 32.0
target distance 10.0
model initialize at round 4403
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([6.56104794, 3.81622644, 5.89468187]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 8.71692485229724}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9317237709179541
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.85951634,  6.01970523,  0.70473325]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.8597421898413563}
episode index:4404
target Thresh 32.0
target distance 15.0
model initialize at round 4404
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([13.60320132, 19.42884239,  4.06895828]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 17.74734032984144}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9317196377684798
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.98149425, 6.4525624 , 2.94139664]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.4529405985054549}
episode index:4405
target Thresh 32.0
target distance 9.0
model initialize at round 4405
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 22.]), 'previousTarget': array([ 8., 22.]), 'currentState': array([16.26925613, 17.48395202,  3.25324011]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 9.422063806552876}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9317240114434981
{'scaleFactor': 20, 'currentTarget': array([ 8., 22.]), 'previousTarget': array([ 8., 22.]), 'currentState': array([ 7.96355546, 22.08993836,  1.82544512]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 0.0970418068297732}
episode index:4406
target Thresh 32.0
target distance 14.0
model initialize at round 4406
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 29.]), 'previousTarget': array([ 6., 29.]), 'currentState': array([18.103843  , 24.38564756,  2.49263722]), 'targetState': array([ 6, 29], dtype=int32), 'currentDistance': 12.953581122093379}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9317240888967462
{'scaleFactor': 20, 'currentTarget': array([ 6., 29.]), 'previousTarget': array([ 6., 29.]), 'currentState': array([ 5.47111907, 29.4468099 ,  3.08100398]), 'targetState': array([ 6, 29], dtype=int32), 'currentDistance': 0.6923540412779072}
episode index:4407
target Thresh 32.0
target distance 15.0
model initialize at round 4407
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 29.]), 'previousTarget': array([ 8., 29.]), 'currentState': array([ 4.55355126, 14.64719902,  1.40472859]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 14.760789444241622}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9317220518290353
{'scaleFactor': 20, 'currentTarget': array([ 8., 29.]), 'previousTarget': array([ 8., 29.]), 'currentState': array([ 8.59397653, 29.71361669,  1.33809575]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 0.9284701956527766}
episode index:4408
target Thresh 32.0
target distance 20.0
model initialize at round 4408
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 29.]), 'previousTarget': array([26., 29.]), 'currentState': array([ 6.80184723, 16.52032804,  6.21899891]), 'targetState': array([26, 29], dtype=int32), 'currentDistance': 22.89784448136205}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9317137996635624
{'scaleFactor': 20, 'currentTarget': array([26., 29.]), 'previousTarget': array([26., 29.]), 'currentState': array([25.1900447 , 28.17768776,  0.24644667]), 'targetState': array([26, 29], dtype=int32), 'currentDistance': 1.154220522370335}
episode index:4409
target Thresh 32.0
target distance 3.0
model initialize at round 4409
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 12.]), 'previousTarget': array([15., 12.]), 'currentState': array([16.17399448, 12.20099464,  3.69877368]), 'targetState': array([15, 12], dtype=int32), 'currentDistance': 1.1910759404173181}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9317270164890354
{'scaleFactor': 20, 'currentTarget': array([15., 12.]), 'previousTarget': array([15., 12.]), 'currentState': array([14.25975001, 11.77256489,  3.01796925]), 'targetState': array([15, 12], dtype=int32), 'currentDistance': 0.7744009129678161}
episode index:4410
target Thresh 32.0
target distance 1.0
model initialize at round 4410
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  2.]), 'previousTarget': array([12.,  2.]), 'currentState': array([11.45493661,  3.66789873,  3.74311352]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 1.7547023345571922}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9317402273218423
{'scaleFactor': 20, 'currentTarget': array([12.,  2.]), 'previousTarget': array([12.,  2.]), 'currentState': array([11.48078904,  1.98185331,  5.73221946]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 0.5195279771307707}
episode index:4411
target Thresh 32.0
target distance 11.0
model initialize at round 4411
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 13.]), 'previousTarget': array([21., 13.]), 'currentState': array([21.5531065 ,  2.6482656 ,  1.40541523]), 'targetState': array([21, 13], dtype=int32), 'currentDistance': 10.366500463055868}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9317445903822633
{'scaleFactor': 20, 'currentTarget': array([21., 13.]), 'previousTarget': array([21., 13.]), 'currentState': array([20.89704981, 12.52999148,  2.12260435]), 'targetState': array([21, 13], dtype=int32), 'currentDistance': 0.4811514820133451}
episode index:4412
target Thresh 32.0
target distance 10.0
model initialize at round 4412
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([15.25901379,  5.76445182,  3.28125572]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 8.870179297373287}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9317511282067857
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([7.88597062, 8.12903665, 3.33842134]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 1.2423852467133023}
episode index:4413
target Thresh 32.0
target distance 16.0
model initialize at round 4413
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 12.]), 'previousTarget': array([ 2., 12.]), 'currentState': array([16.3031924 , 24.30492527,  4.25310218]), 'targetState': array([ 2, 12], dtype=int32), 'currentDistance': 18.867763481101235}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9317469972868213
{'scaleFactor': 20, 'currentTarget': array([ 2., 12.]), 'previousTarget': array([ 2., 12.]), 'currentState': array([ 2.9679214 , 12.44581319,  4.3378194 ]), 'targetState': array([ 2, 12], dtype=int32), 'currentDistance': 1.0656553122531478}
episode index:4414
target Thresh 32.0
target distance 10.0
model initialize at round 4414
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 26.]), 'previousTarget': array([ 9., 26.]), 'currentState': array([ 5.90884973, 17.41646795,  2.01032633]), 'targetState': array([ 9, 26], dtype=int32), 'currentDistance': 9.123170084746356}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9317513558491345
{'scaleFactor': 20, 'currentTarget': array([ 9., 26.]), 'previousTarget': array([ 9., 26.]), 'currentState': array([ 9.10228735, 26.34345905,  0.48202873]), 'targetState': array([ 9, 26], dtype=int32), 'currentDistance': 0.3583668809132091}
episode index:4415
target Thresh 32.0
target distance 18.0
model initialize at round 4415
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([16.37214893, 21.43852887,  5.34008217]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 17.01002403660171}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9317472267485082
{'scaleFactor': 20, 'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([11.72688772,  4.92253305,  3.05164166]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 0.2838863232140775}
episode index:4416
target Thresh 32.0
target distance 14.0
model initialize at round 4416
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([23.90546518, 22.35900077,  4.41470373]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 14.67110356221491}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9317472987705048
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.7246256 , 10.44881409,  4.59232988]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.8523592807635948}
episode index:4417
target Thresh 32.0
target distance 17.0
model initialize at round 4417
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([22.86843513, 10.55840071,  4.24457932]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 17.87389111986011}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9317431724574022
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.46572758, 10.84198623,  3.54494223]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.49180334383235536}
episode index:4418
target Thresh 32.0
target distance 16.0
model initialize at round 4418
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 24.]), 'previousTarget': array([20., 24.]), 'currentState': array([ 5.6515621 , 19.32383142,  1.20355171]), 'targetState': array([20, 24], dtype=int32), 'currentDistance': 15.091196868097137}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9317411361419395
{'scaleFactor': 20, 'currentTarget': array([20., 24.]), 'previousTarget': array([20., 24.]), 'currentState': array([20.66651401, 23.76608051,  0.16471746]), 'targetState': array([20, 24], dtype=int32), 'currentDistance': 0.7063704775660465}
episode index:4419
target Thresh 32.0
target distance 22.0
model initialize at round 4419
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 5.]), 'previousTarget': array([8., 5.]), 'currentState': array([ 8.37334304, 26.56828802,  4.4110105 ]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 21.571519030774255}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9317329001958121
{'scaleFactor': 20, 'currentTarget': array([8., 5.]), 'previousTarget': array([8., 5.]), 'currentState': array([8.08628094, 4.75531488, 4.11064935]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 0.2594517446237419}
episode index:4420
target Thresh 32.0
target distance 16.0
model initialize at round 4420
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 3.]), 'previousTarget': array([6., 3.]), 'currentState': array([20.88490975, 12.73945971,  2.97814035]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 17.788131261612737}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9317287799396002
{'scaleFactor': 20, 'currentTarget': array([6., 3.]), 'previousTarget': array([6., 3.]), 'currentState': array([6.08162816, 3.02763529, 4.08510985]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 0.08617926452519495}
episode index:4421
target Thresh 32.0
target distance 26.0
model initialize at round 4421
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  3.]), 'previousTarget': array([13.,  3.]), 'currentState': array([ 8.74804631, 27.03996963,  4.85516191]), 'targetState': array([13,  3], dtype=int32), 'currentDistance': 24.413095871750066}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9317185257767275
{'scaleFactor': 20, 'currentTarget': array([13.,  3.]), 'previousTarget': array([13.,  3.]), 'currentState': array([13.09080231,  3.61259625,  5.46022522]), 'targetState': array([13,  3], dtype=int32), 'currentDistance': 0.6192892892884565}
episode index:4422
target Thresh 32.0
target distance 12.0
model initialize at round 4422
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([19.44883092, 17.34711213,  4.54998589]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 13.373569026654094}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9317186041900511
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([8.92798312, 9.18888198, 2.51257369]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.2021455729404419}
episode index:4423
target Thresh 32.0
target distance 3.0
model initialize at round 4423
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 17.]), 'previousTarget': array([14., 17.]), 'currentState': array([10.71424131, 18.3414675 ,  5.55176806]), 'targetState': array([14, 17], dtype=int32), 'currentDistance': 3.5490484905618938}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9317295403102613
{'scaleFactor': 20, 'currentTarget': array([14., 17.]), 'previousTarget': array([14., 17.]), 'currentState': array([13.93301894, 16.81465511,  4.67735761]), 'targetState': array([14, 17], dtype=int32), 'currentDistance': 0.19707661544984756}
episode index:4424
target Thresh 32.0
target distance 8.0
model initialize at round 4424
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  4.]), 'previousTarget': array([25.,  4.]), 'currentState': array([18.5495518 ,  2.6567214 ,  5.67405361]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 6.588829898654989}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.931738256572338
{'scaleFactor': 20, 'currentTarget': array([25.,  4.]), 'previousTarget': array([25.,  4.]), 'currentState': array([24.00729899,  4.00091989,  0.74235684]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 0.9927014316713777}
episode index:4425
target Thresh 32.0
target distance 11.0
model initialize at round 4425
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 29.]), 'previousTarget': array([16., 29.]), 'currentState': array([13.36084643, 19.90869827,  0.96264476]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 9.466620233765276}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9317426062771115
{'scaleFactor': 20, 'currentTarget': array([16., 29.]), 'previousTarget': array([16., 29.]), 'currentState': array([16.31500771, 29.31092434,  1.53217572]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 0.44261021390683003}
episode index:4426
target Thresh 32.0
target distance 24.0
model initialize at round 4426
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 26.]), 'previousTarget': array([ 7.37891167, 25.52137473]), 'currentState': array([24.80398351,  3.57193245,  2.49721289]), 'targetState': array([ 7, 26], dtype=int32), 'currentDistance': 28.63564287212001}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9317283761453048
{'scaleFactor': 20, 'currentTarget': array([ 7., 26.]), 'previousTarget': array([ 7., 26.]), 'currentState': array([ 7.69478917, 25.43906275,  2.26095913]), 'targetState': array([ 7, 26], dtype=int32), 'currentDistance': 0.8929628172102898}
episode index:4427
target Thresh 32.0
target distance 14.0
model initialize at round 4427
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 15.]), 'previousTarget': array([18., 15.]), 'currentState': array([23.44536734, 28.35540294,  4.54464698]), 'targetState': array([18, 15], dtype=int32), 'currentDistance': 14.422857317693591}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.931728452245522
{'scaleFactor': 20, 'currentTarget': array([18., 15.]), 'previousTarget': array([18., 15.]), 'currentState': array([18.41074145, 15.40459829,  4.694987  ]), 'targetState': array([18, 15], dtype=int32), 'currentDistance': 0.5765486252723279}
episode index:4428
target Thresh 32.0
target distance 5.0
model initialize at round 4428
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 22.]), 'previousTarget': array([22., 22.]), 'currentState': array([22.64224457, 25.03628594,  4.42201051]), 'targetState': array([22, 22], dtype=int32), 'currentDistance': 3.1034674823988024}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9317393737961551
{'scaleFactor': 20, 'currentTarget': array([22., 22.]), 'previousTarget': array([22., 22.]), 'currentState': array([21.85745581, 21.35568742,  3.6368055 ]), 'targetState': array([22, 22], dtype=int32), 'currentDistance': 0.6598920688110933}
episode index:4429
target Thresh 32.0
target distance 15.0
model initialize at round 4429
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 26.]), 'previousTarget': array([15., 26.]), 'currentState': array([14.65824041, 12.64790426,  0.76528788]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 13.356468850226184}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9317394473794759
{'scaleFactor': 20, 'currentTarget': array([15., 26.]), 'previousTarget': array([15., 26.]), 'currentState': array([15.00900764, 26.40880683,  1.43108691]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 0.40890605282680076}
episode index:4430
target Thresh 32.0
target distance 5.0
model initialize at round 4430
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 29.]), 'previousTarget': array([23., 29.]), 'currentState': array([25.79422468, 22.82590717,  2.90367889]), 'targetState': array([23, 29], dtype=int32), 'currentDistance': 6.776954610626317}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9317459598061563
{'scaleFactor': 20, 'currentTarget': array([23., 29.]), 'previousTarget': array([23., 29.]), 'currentState': array([22.87726198, 29.75436721,  1.85886797]), 'targetState': array([23, 29], dtype=int32), 'currentDistance': 0.7642869315820786}
episode index:4431
target Thresh 32.0
target distance 14.0
model initialize at round 4431
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 15.]), 'previousTarget': array([24., 15.]), 'currentState': array([20.08223372, 27.71114071,  4.40086314]), 'targetState': array([24, 15], dtype=int32), 'currentDistance': 13.301202608214622}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9317460318702584
{'scaleFactor': 20, 'currentTarget': array([24., 15.]), 'previousTarget': array([24., 15.]), 'currentState': array([23.95894552, 14.51726855,  5.13433289]), 'targetState': array([24, 15], dtype=int32), 'currentDistance': 0.48447406575185914}
episode index:4432
target Thresh 32.0
target distance 20.0
model initialize at round 4432
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 13.]), 'previousTarget': array([ 6., 13.]), 'currentState': array([25.06003923, 17.39601642,  3.17338848]), 'targetState': array([ 6, 13], dtype=int32), 'currentDistance': 19.56042064979292}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9317398590850426
{'scaleFactor': 20, 'currentTarget': array([ 6., 13.]), 'previousTarget': array([ 6., 13.]), 'currentState': array([ 5.89386907, 12.58477386,  3.1694305 ]), 'targetState': array([ 6, 13], dtype=int32), 'currentDistance': 0.4285749877993501}
episode index:4433
target Thresh 32.0
target distance 13.0
model initialize at round 4433
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 23.]), 'previousTarget': array([20., 23.]), 'currentState': array([ 8.85581729, 22.60857681,  0.359003  ]), 'targetState': array([20, 23], dtype=int32), 'currentDistance': 11.15105467954285}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9317420558126737
{'scaleFactor': 20, 'currentTarget': array([20., 23.]), 'previousTarget': array([20., 23.]), 'currentState': array([20.73588333, 22.96821797,  0.08214259]), 'targetState': array([20, 23], dtype=int32), 'currentDistance': 0.7365693289560178}
episode index:4434
target Thresh 32.0
target distance 20.0
model initialize at round 4434
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 12.]), 'previousTarget': array([22., 12.]), 'currentState': array([ 2.53201813, 27.40333322,  6.04402447]), 'targetState': array([22, 12], dtype=int32), 'currentDistance': 24.82468514465231}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9317318287136667
{'scaleFactor': 20, 'currentTarget': array([22., 12.]), 'previousTarget': array([22., 12.]), 'currentState': array([21.40840931, 12.92219148,  6.00987698]), 'targetState': array([22, 12], dtype=int32), 'currentDistance': 1.0956352864153407}
episode index:4435
target Thresh 32.0
target distance 14.0
model initialize at round 4435
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([11.33819618, 16.26607544,  3.99282777]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 17.050595777889235}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9317298027591387
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.58355175, 2.92413713, 4.21931967]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 1.092960240234215}
episode index:4436
target Thresh 32.0
target distance 16.0
model initialize at round 4436
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 24.]), 'previousTarget': array([12., 24.]), 'currentState': array([3.24828791, 9.38543944, 1.48726813]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 17.03460726731247}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.93172777771782
{'scaleFactor': 20, 'currentTarget': array([12., 24.]), 'previousTarget': array([12., 24.]), 'currentState': array([11.14176573, 23.04772828,  1.59995876]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 1.2819467593966414}
episode index:4437
target Thresh 32.0
target distance 15.0
model initialize at round 4437
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 18.]), 'previousTarget': array([ 7., 18.]), 'currentState': array([7.142684  , 4.4482394 , 1.09529138]), 'targetState': array([ 7, 18], dtype=int32), 'currentDistance': 13.552511727719866}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9317278537814048
{'scaleFactor': 20, 'currentTarget': array([ 7., 18.]), 'previousTarget': array([ 7., 18.]), 'currentState': array([ 7.15529366, 18.30670258,  1.62484375]), 'targetState': array([ 7, 18], dtype=int32), 'currentDistance': 0.3437769543476179}
episode index:4438
target Thresh 32.0
target distance 14.0
model initialize at round 4438
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 25.]), 'previousTarget': array([19., 25.]), 'currentState': array([12.90965749, 12.41661644,  2.00902854]), 'targetState': array([19, 25], dtype=int32), 'currentDistance': 13.979764435724245}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.931727929810719
{'scaleFactor': 20, 'currentTarget': array([19., 25.]), 'previousTarget': array([19., 25.]), 'currentState': array([18.80188504, 24.73845864,  0.80178728]), 'targetState': array([19, 25], dtype=int32), 'currentDistance': 0.3281058045035912}
episode index:4439
target Thresh 32.0
target distance 22.0
model initialize at round 4439
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 7.]), 'previousTarget': array([9., 7.]), 'currentState': array([28.66701172, 28.76878556,  5.13536472]), 'targetState': array([9, 7], dtype=int32), 'currentDistance': 29.337201213986518}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.9317117880145096
{'scaleFactor': 20, 'currentTarget': array([9., 7.]), 'previousTarget': array([9., 7.]), 'currentState': array([8.97259553, 6.93767702, 3.83529771]), 'targetState': array([9, 7], dtype=int32), 'currentDistance': 0.06808199729609356}
episode index:4440
target Thresh 32.0
target distance 20.0
model initialize at round 4440
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 28.]), 'previousTarget': array([18., 28.]), 'currentState': array([24.6316755 ,  9.95225896,  1.95922729]), 'targetState': array([18, 28], dtype=int32), 'currentDistance': 19.2275863407096}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9317056340597684
{'scaleFactor': 20, 'currentTarget': array([18., 28.]), 'previousTarget': array([18., 28.]), 'currentState': array([17.58589524, 28.44087274,  1.42638082]), 'targetState': array([18, 28], dtype=int32), 'currentDistance': 0.6048566136568104}
episode index:4441
target Thresh 32.0
target distance 8.0
model initialize at round 4441
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([11.6326372 ,  8.40851419,  5.51836855]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 6.380453966790072}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9317143223456621
{'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([17.35425399,  8.30849693,  0.51067287]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 0.7156523389880012}
episode index:4442
target Thresh 32.0
target distance 14.0
model initialize at round 4442
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.76506354, 23.02086537,  4.44810376]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 12.045186855141722}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9317165203711079
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.03439755, 11.11422719,  4.26936434]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.11929393156767414}
episode index:4443
target Thresh 32.0
target distance 10.0
model initialize at round 4443
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 16.]), 'previousTarget': array([15., 16.]), 'currentState': array([ 5.69648658, 17.53208829,  0.13412404]), 'targetState': array([15, 16], dtype=int32), 'currentDistance': 9.428820528133482}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9317208573489496
{'scaleFactor': 20, 'currentTarget': array([15., 16.]), 'previousTarget': array([15., 16.]), 'currentState': array([14.9322687 , 15.7043025 ,  4.73046515]), 'targetState': array([15, 16], dtype=int32), 'currentDistance': 0.3033554697840816}
episode index:4444
target Thresh 32.0
target distance 9.0
model initialize at round 4444
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 16.]), 'previousTarget': array([21., 16.]), 'currentState': array([17.84550626, 23.77544608,  4.96643114]), 'targetState': array([21, 16], dtype=int32), 'currentDistance': 8.39097089557433}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9317273534462839
{'scaleFactor': 20, 'currentTarget': array([21., 16.]), 'previousTarget': array([21., 16.]), 'currentState': array([20.8650684 , 16.3816035 ,  5.05801797]), 'targetState': array([21, 16], dtype=int32), 'currentDistance': 0.40475643442367804}
episode index:4445
target Thresh 32.0
target distance 11.0
model initialize at round 4445
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([14.68290915, 18.98568685,  5.26468051]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 11.475556949036914}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9317295470576098
{'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([18.04243392,  7.56819661,  5.08043752]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 0.4338833953829915}
episode index:4446
target Thresh 32.0
target distance 13.0
model initialize at round 4446
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 5.]), 'previousTarget': array([7., 5.]), 'currentState': array([ 8.05933908, 16.69225813,  4.38322928]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 11.740149038512444}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9317317396823779
{'scaleFactor': 20, 'currentTarget': array([7., 5.]), 'previousTarget': array([7., 5.]), 'currentState': array([6.85358528, 5.26322874, 3.31456211]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 0.3012086345901544}
episode index:4447
target Thresh 32.0
target distance 18.0
model initialize at round 4447
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 7.]), 'previousTarget': array([7., 7.]), 'currentState': array([17.64908933, 23.35402011,  3.49234104]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 19.51555988072102}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9317255909268306
{'scaleFactor': 20, 'currentTarget': array([7., 7.]), 'previousTarget': array([7., 7.]), 'currentState': array([6.98999435, 6.71754154, 3.67308477]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 0.2826356199969139}
episode index:4448
target Thresh 32.0
target distance 25.0
model initialize at round 4448
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 27.]), 'previousTarget': array([ 9., 27.]), 'currentState': array([9.31468678, 3.89597658, 1.89809108]), 'targetState': array([ 9, 27], dtype=int32), 'currentDistance': 23.106166401314447}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9317153997110046
{'scaleFactor': 20, 'currentTarget': array([ 9., 27.]), 'previousTarget': array([ 9., 27.]), 'currentState': array([ 9.21947655, 27.28260593,  0.22106765]), 'targetState': array([ 9, 27], dtype=int32), 'currentDistance': 0.35782127427660626}
episode index:4449
target Thresh 32.0
target distance 11.0
model initialize at round 4449
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  7.]), 'previousTarget': array([12.,  7.]), 'currentState': array([13.42924071, 16.25560265,  5.11499536]), 'targetState': array([12,  7], dtype=int32), 'currentDistance': 9.365303489854375}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9317197310930694
{'scaleFactor': 20, 'currentTarget': array([12.,  7.]), 'previousTarget': array([12.,  7.]), 'currentState': array([11.72968842,  7.17005883,  3.1956628 ]), 'targetState': array([12,  7], dtype=int32), 'currentDistance': 0.3193561599545585}
episode index:4450
target Thresh 32.0
target distance 21.0
model initialize at round 4450
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  5.]), 'previousTarget': array([22.,  5.]), 'currentState': array([15.66474303, 26.24701971,  5.42049355]), 'targetState': array([22,  5], dtype=int32), 'currentDistance': 22.171407880034458}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9317115573171012
{'scaleFactor': 20, 'currentTarget': array([22.,  5.]), 'previousTarget': array([22.,  5.]), 'currentState': array([21.94408017,  5.29479719,  5.37040364]), 'targetState': array([22,  5], dtype=int32), 'currentDistance': 0.3000540135841368}
episode index:4451
target Thresh 32.0
target distance 7.0
model initialize at round 4451
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 15.]), 'previousTarget': array([17., 15.]), 'currentState': array([14.71468337,  9.82151972,  1.56209385]), 'targetState': array([17, 15], dtype=int32), 'currentDistance': 5.660329504200258}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9317202247570568
{'scaleFactor': 20, 'currentTarget': array([17., 15.]), 'previousTarget': array([17., 15.]), 'currentState': array([1.71608396e+01, 1.47983718e+01, 1.46526694e-02]), 'targetState': array([17, 15], dtype=int32), 'currentDistance': 0.2579211285807512}
episode index:4452
target Thresh 32.0
target distance 9.0
model initialize at round 4452
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 27.]), 'previousTarget': array([17., 27.]), 'currentState': array([ 8.73633973, 22.51333799,  0.10795307]), 'targetState': array([17, 27], dtype=int32), 'currentDistance': 9.403096139459414}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9317245521375066
{'scaleFactor': 20, 'currentTarget': array([17., 27.]), 'previousTarget': array([17., 27.]), 'currentState': array([17.32146352, 26.97220635,  0.62632746]), 'targetState': array([17, 27], dtype=int32), 'currentDistance': 0.3226628016969798}
episode index:4453
target Thresh 32.0
target distance 4.0
model initialize at round 4453
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 15.]), 'previousTarget': array([11., 15.]), 'currentState': array([ 9.27433253, 20.09930191,  5.98497719]), 'targetState': array([11, 15], dtype=int32), 'currentDistance': 5.383382599438578}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9317332127679202
{'scaleFactor': 20, 'currentTarget': array([11., 15.]), 'previousTarget': array([11., 15.]), 'currentState': array([10.67870455, 15.10295358,  3.83545226]), 'targetState': array([11, 15], dtype=int32), 'currentDistance': 0.3373873261297566}
episode index:4454
target Thresh 32.0
target distance 4.0
model initialize at round 4454
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([15.22225774,  8.49248285,  3.06118608]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 4.073747133814518}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9317440695102842
{'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([12.35396074,  6.19522542,  3.16350877]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 0.4042291026999083}
episode index:4455
target Thresh 32.0
target distance 17.0
model initialize at round 4455
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([9.3426307 , 3.35227663, 5.92740917]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 23.55025786160067}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9317338901570988
{'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([25.5147621 , 19.72206592,  0.41435614]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 0.5591986813566112}
episode index:4456
target Thresh 32.0
target distance 8.0
model initialize at round 4456
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([12.52370929, 17.28536857,  4.83463747]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 8.072248276872124}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9317403658402585
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.11702895, 10.40348189,  4.68082971]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.4201111909914004}
episode index:4457
target Thresh 32.0
target distance 9.0
model initialize at round 4457
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 4.3170389 , 12.00547774,  4.14833784]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.101275779747922}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9317446838492446
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.41990696, 2.52393427, 4.35648662]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.750430880106535}
episode index:4458
target Thresh 32.0
target distance 13.0
model initialize at round 4458
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 18.]), 'previousTarget': array([14., 18.]), 'currentState': array([25.09962045, 15.58762119,  2.65937701]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 11.358747537252068}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9317468671785901
{'scaleFactor': 20, 'currentTarget': array([14., 18.]), 'previousTarget': array([14., 18.]), 'currentState': array([13.64207494, 17.77822451,  2.30861181]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 0.421063789634612}
episode index:4459
target Thresh 32.0
target distance 5.0
model initialize at round 4459
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 21.]), 'previousTarget': array([13., 21.]), 'currentState': array([ 7.71451812, 24.34141983,  5.55193496]), 'targetState': array([13, 21], dtype=int32), 'currentDistance': 6.253111643756744}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.931755511154559
{'scaleFactor': 20, 'currentTarget': array([13., 21.]), 'previousTarget': array([13., 21.]), 'currentState': array([12.80161723, 21.36674676,  6.16014594]), 'targetState': array([13, 21], dtype=int32), 'currentDistance': 0.4169639205799528}
episode index:4460
target Thresh 32.0
target distance 9.0
model initialize at round 4460
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 12.]), 'previousTarget': array([14., 12.]), 'currentState': array([21.00833901, 12.88528915,  3.32335028]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 7.06403231731022}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9317619761845624
{'scaleFactor': 20, 'currentTarget': array([14., 12.]), 'previousTarget': array([14., 12.]), 'currentState': array([13.15949687, 11.88097332,  3.65775685]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 0.8488891899551343}
episode index:4461
target Thresh 32.0
target distance 15.0
model initialize at round 4461
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 24.]), 'previousTarget': array([ 6., 24.]), 'currentState': array([19.42966204, 26.39466805,  2.49951839]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 13.641490296409605}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9317620441746391
{'scaleFactor': 20, 'currentTarget': array([ 6., 24.]), 'previousTarget': array([ 6., 24.]), 'currentState': array([ 6.20534815, 24.00809149,  3.69354492]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 0.2055075095188831}
episode index:4462
target Thresh 32.0
target distance 5.0
model initialize at round 4462
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([4.90654276e+00, 1.65820549e+01, 8.04537138e-03]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 7.612254035646134}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9317685048436567
{'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([ 8.40443337, 22.43362668,  6.27878523]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 0.821874897428389}
episode index:4463
target Thresh 32.0
target distance 17.0
model initialize at round 4463
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([17.20522683, 20.14891416,  4.16704261]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 15.308580133037227}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.93176648338075
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.70705559,  5.13710151,  3.21669471]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.32343972411663685}
episode index:4464
target Thresh 32.0
target distance 1.0
model initialize at round 4464
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 15.]), 'previousTarget': array([ 9., 15.]), 'currentState': array([ 8.60966158, 14.26104614,  3.66125238]), 'targetState': array([ 9, 15], dtype=int32), 'currentDistance': 0.8357133992788756}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9317817652433746
{'scaleFactor': 20, 'currentTarget': array([ 9., 15.]), 'previousTarget': array([ 9., 15.]), 'currentState': array([ 8.60966158, 14.26104614,  3.66125238]), 'targetState': array([ 9, 15], dtype=int32), 'currentDistance': 0.8357133992788756}
episode index:4465
target Thresh 32.0
target distance 4.0
model initialize at round 4465
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'previousTarget': array([13., 22.]), 'currentState': array([10.7243705 , 21.56109604,  1.0813542 ]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 2.3175690496915196}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9317948011221826
{'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'previousTarget': array([13., 22.]), 'currentState': array([12.40033689, 21.71468291,  5.3646028 ]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 0.6640795830286902}
episode index:4466
target Thresh 32.0
target distance 14.0
model initialize at round 4466
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([10.02124088, 20.31716403,  5.73501039]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 17.89303545841026}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9317907094379115
{'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([22.43577758,  8.14183917,  4.25077898]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 0.5817776981483802}
episode index:4467
target Thresh 32.0
target distance 18.0
model initialize at round 4467
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([17.67285639,  8.03492895,  1.46928024]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 17.59685263696968}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9317866195851913
{'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([13.05840264, 25.10889785,  2.00984377]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 0.12357026647039172}
episode index:4468
target Thresh 32.0
target distance 8.0
model initialize at round 4468
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 23.]), 'previousTarget': array([ 7., 23.]), 'currentState': array([13.10981358, 21.35504102,  2.46839809]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 6.327377974853273}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9317952372581414
{'scaleFactor': 20, 'currentTarget': array([ 7., 23.]), 'previousTarget': array([ 7., 23.]), 'currentState': array([ 7.46394692, 22.73590302,  3.34323859]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 0.5338482551890966}
episode index:4469
target Thresh 32.0
target distance 15.0
model initialize at round 4469
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'previousTarget': array([19.,  3.]), 'currentState': array([21.64722762, 17.65499383,  5.06672323]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 14.892167684149177}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9317932125282017
{'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'previousTarget': array([19.,  3.]), 'currentState': array([19.09492556,  2.21311253,  5.19775644]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 0.792592427166}
episode index:4470
target Thresh 32.0
target distance 12.0
model initialize at round 4470
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 14.]), 'previousTarget': array([17., 14.]), 'currentState': array([10.4934646 ,  2.77585537,  1.4891259 ]), 'targetState': array([17, 14], dtype=int32), 'currentDistance': 12.973682032256136}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.931795379143472
{'scaleFactor': 20, 'currentTarget': array([17., 14.]), 'previousTarget': array([17., 14.]), 'currentState': array([16.11102658, 13.11647535,  1.66004693]), 'targetState': array([17, 14], dtype=int32), 'currentDistance': 1.253351327084659}
episode index:4471
target Thresh 32.0
target distance 14.0
model initialize at round 4471
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 15.]), 'previousTarget': array([ 6., 15.]), 'currentState': array([15.31251642, 27.94658236,  4.59686473]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 15.947945224872901}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9317933552873191
{'scaleFactor': 20, 'currentTarget': array([ 6., 15.]), 'previousTarget': array([ 6., 15.]), 'currentState': array([ 6.19026476, 15.22303495,  4.63913772]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 0.2931642299421772}
episode index:4472
target Thresh 32.0
target distance 25.0
model initialize at round 4472
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([22.31848686, 28.07001166,  4.10998058]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 25.5033733031732}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9317812219691235
{'scaleFactor': 20, 'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([27.03809408,  3.09989683,  5.35663697]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 0.10691368215794116}
episode index:4473
target Thresh 32.0
target distance 12.0
model initialize at round 4473
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([22.70127328,  6.86928172,  1.10831037]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 10.388243231672956}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9317855154040655
{'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([24.59277295, 16.51675457,  1.85238537]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 0.6319493821456328}
episode index:4474
target Thresh 32.0
target distance 15.0
model initialize at round 4474
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 27.]), 'previousTarget': array([23., 27.]), 'currentState': array([ 6.47801009, 16.71828602,  1.69064498]), 'targetState': array([23, 27], dtype=int32), 'currentDistance': 19.45995356575765}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9317793917302342
{'scaleFactor': 20, 'currentTarget': array([23., 27.]), 'previousTarget': array([23., 27.]), 'currentState': array([22.8078527 , 26.97837256,  0.97108856]), 'targetState': array([23, 27], dtype=int32), 'currentDistance': 0.1933606247551341}
episode index:4475
target Thresh 32.0
target distance 9.0
model initialize at round 4475
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 18.]), 'previousTarget': array([13., 18.]), 'currentState': array([20.3481355 , 24.88053072,  3.61930072]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 10.066618015139522}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9317836836556518
{'scaleFactor': 20, 'currentTarget': array([13., 18.]), 'previousTarget': array([13., 18.]), 'currentState': array([13.08763024, 18.15189847,  4.27411363]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 0.1753630618091827}
episode index:4476
target Thresh 32.0
target distance 17.0
model initialize at round 4476
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 28.]), 'previousTarget': array([19., 28.]), 'currentState': array([18.61224355, 10.0478865 ,  2.73291588]), 'targetState': array([19, 28], dtype=int32), 'currentDistance': 17.956300682775517}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.931779603593965
{'scaleFactor': 20, 'currentTarget': array([19., 28.]), 'previousTarget': array([19., 28.]), 'currentState': array([19.34066368, 27.26907992,  1.36864416]), 'targetState': array([19, 28], dtype=int32), 'currentDistance': 0.8064092680173244}
episode index:4477
target Thresh 32.0
target distance 15.0
model initialize at round 4477
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([10.66298459,  5.25859297,  5.42744917]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 13.729326568602994}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9317796674046646
{'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([23.95889216,  1.92148288,  0.22544679]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 0.08862726732371307}
episode index:4478
target Thresh 32.0
target distance 7.0
model initialize at round 4478
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([13.64781448, 26.19127699,  5.22239339]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 5.2032097199718095}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9317882673896155
{'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([14.5482143 , 20.40106393,  4.54968452]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 0.8119503245130262}
episode index:4479
target Thresh 32.0
target distance 22.0
model initialize at round 4479
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 19.]), 'previousTarget': array([25., 19.]), 'currentState': array([4.4256872 , 9.8956009 , 5.83521909]), 'targetState': array([25, 19], dtype=int32), 'currentDistance': 22.498720637549326}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9317801312259701
{'scaleFactor': 20, 'currentTarget': array([25., 19.]), 'previousTarget': array([25., 19.]), 'currentState': array([24.13028861, 18.89089837,  0.93608185]), 'targetState': array([25, 19], dtype=int32), 'currentDistance': 0.8765278497470745}
episode index:4480
target Thresh 32.0
target distance 13.0
model initialize at round 4480
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([20.36578138, 26.40214125,  3.91031098]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 14.837134586044721}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9317801948762003
{'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([14.45653997, 13.92640526,  4.83708984]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 1.0327901280881078}
episode index:4481
target Thresh 32.0
target distance 15.0
model initialize at round 4481
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([23.51102567, 28.78443835,  3.66671109]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 16.170213205283357}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9317781789234005
{'scaleFactor': 20, 'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([20.34826162, 13.53927086,  4.1449939 ]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 0.6419495434165609}
episode index:4482
target Thresh 32.0
target distance 19.0
model initialize at round 4482
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 29.]), 'previousTarget': array([24., 29.]), 'currentState': array([ 9.23312677, 11.14533247,  1.75850266]), 'targetState': array([24, 29], dtype=int32), 'currentDistance': 23.17001721533246}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9317680532693281
{'scaleFactor': 20, 'currentTarget': array([24., 29.]), 'previousTarget': array([24., 29.]), 'currentState': array([24.29534278, 29.26253962,  1.04542406]), 'targetState': array([24, 29], dtype=int32), 'currentDistance': 0.3951637735349656}
episode index:4483
target Thresh 32.0
target distance 11.0
model initialize at round 4483
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 24.]), 'previousTarget': array([20., 24.]), 'currentState': array([19.53387311, 13.82635346,  1.61834502]), 'targetState': array([20, 24], dtype=int32), 'currentDistance': 10.184319231183869}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9317723400660788
{'scaleFactor': 20, 'currentTarget': array([20., 24.]), 'previousTarget': array([20., 24.]), 'currentState': array([19.59429856, 23.66066719,  1.10439748]), 'targetState': array([20, 24], dtype=int32), 'currentDistance': 0.5289049158813423}
episode index:4484
target Thresh 32.0
target distance 15.0
model initialize at round 4484
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 14.]), 'previousTarget': array([11., 14.]), 'currentState': array([21.33195568, 29.22364309,  4.01831245]), 'targetState': array([11, 14], dtype=int32), 'currentDistance': 18.39860366983087}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9317682698113224
{'scaleFactor': 20, 'currentTarget': array([11., 14.]), 'previousTarget': array([11., 14.]), 'currentState': array([11.51987305, 14.48241223,  4.69008576]), 'targetState': array([11, 14], dtype=int32), 'currentDistance': 0.7092175553543149}
episode index:4485
target Thresh 32.0
target distance 4.0
model initialize at round 4485
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 16.]), 'previousTarget': array([12., 16.]), 'currentState': array([ 7.3145978 , 16.53707902,  0.98024559]), 'targetState': array([12, 16], dtype=int32), 'currentDistance': 4.716083927385542}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9317768589174723
{'scaleFactor': 20, 'currentTarget': array([12., 16.]), 'previousTarget': array([12., 16.]), 'currentState': array([1.26946202e+01, 1.62758166e+01, 8.51181149e-03]), 'targetState': array([12, 16], dtype=int32), 'currentDistance': 0.747376735718807}
episode index:4486
target Thresh 32.0
target distance 4.0
model initialize at round 4486
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([20.14839868, 20.30507571,  4.81313133]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 2.309847660951166}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9317898348793805
{'scaleFactor': 20, 'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([20.0613386 , 18.31406558,  4.52133113]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 0.3199993981347259}
episode index:4487
target Thresh 32.0
target distance 22.0
model initialize at round 4487
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 24.]), 'previousTarget': array([17., 24.]), 'currentState': array([11.31703079,  1.9983539 ,  2.13257074]), 'targetState': array([17, 24], dtype=int32), 'currentDistance': 22.72374463008201}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9317797179089788
{'scaleFactor': 20, 'currentTarget': array([17., 24.]), 'previousTarget': array([17., 24.]), 'currentState': array([17.51828474, 24.62950963,  1.40653309]), 'targetState': array([17, 24], dtype=int32), 'currentDistance': 0.8154148934609128}
episode index:4488
target Thresh 32.0
target distance 15.0
model initialize at round 4488
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 4.19606778, 24.39782398,  5.39700021]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 17.21123583428763}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9317756496375541
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.71631981, 10.86221712,  5.20510421]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.7294506085244417}
episode index:4489
target Thresh 32.0
target distance 16.0
model initialize at round 4489
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 21.]), 'previousTarget': array([ 6., 21.]), 'currentState': array([22.97443963, 13.37217182,  1.96329468]), 'targetState': array([ 6, 21], dtype=int32), 'currentDistance': 18.60955033045671}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.931771583178277
{'scaleFactor': 20, 'currentTarget': array([ 6., 21.]), 'previousTarget': array([ 6., 21.]), 'currentState': array([ 6.80218955, 20.39005675,  3.30077871]), 'targetState': array([ 6, 21], dtype=int32), 'currentDistance': 1.0077394717649888}
episode index:4490
target Thresh 32.0
target distance 9.0
model initialize at round 4490
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([15.65277296, 10.99135369,  2.77425051]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 10.014667626003181}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9317758625073176
{'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([15.89470543, 20.31290248,  1.09627694]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 0.6951186606692593}
episode index:4491
target Thresh 32.0
target distance 12.0
model initialize at round 4491
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([23.00770537,  6.64068419,  3.07181406]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 11.026334559919526}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9317780228561364
{'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([11.242334  ,  5.72781913,  2.61383799]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 0.8050715487634204}
episode index:4492
target Thresh 32.0
target distance 22.0
model initialize at round 4492
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 29.]), 'previousTarget': array([ 6., 29.]), 'currentState': array([16.67156482,  7.1955994 ,  1.126486  ]), 'targetState': array([ 6, 29], dtype=int32), 'currentDistance': 24.275794147766476}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9317679197733099
{'scaleFactor': 20, 'currentTarget': array([ 6., 29.]), 'previousTarget': array([ 6., 29.]), 'currentState': array([ 6.01664463, 28.32206784,  2.48337719]), 'targetState': array([ 6, 29], dtype=int32), 'currentDistance': 0.6781364591952985}
episode index:4493
target Thresh 32.0
target distance 12.0
model initialize at round 4493
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 14.]), 'previousTarget': array([24., 14.]), 'currentState': array([17.33851029,  3.54752042,  0.96474147]), 'targetState': array([24, 14], dtype=int32), 'currentDistance': 12.394747864769164}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9317700809281002
{'scaleFactor': 20, 'currentTarget': array([24., 14.]), 'previousTarget': array([24., 14.]), 'currentState': array([23.63137591, 13.6639251 ,  1.4870243 ]), 'targetState': array([24, 14], dtype=int32), 'currentDistance': 0.4988286879824534}
episode index:4494
target Thresh 32.0
target distance 7.0
model initialize at round 4494
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  5.]), 'previousTarget': array([13.,  5.]), 'currentState': array([4.63884208, 2.98976624, 1.50287652]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 8.599418675410126}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9317743567832664
{'scaleFactor': 20, 'currentTarget': array([13.,  5.]), 'previousTarget': array([13.,  5.]), 'currentState': array([13.61885203,  5.56386419,  0.01507509]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 0.8372100449056635}
episode index:4495
target Thresh 32.0
target distance 7.0
model initialize at round 4495
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 28.]), 'previousTarget': array([27., 28.]), 'currentState': array([19.01382218, 22.36376002,  1.18687868]), 'targetState': array([27, 28], dtype=int32), 'currentDistance': 9.774775558846274}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9317786307363616
{'scaleFactor': 20, 'currentTarget': array([27., 28.]), 'previousTarget': array([27., 28.]), 'currentState': array([26.77993734, 28.17040923,  0.18459907]), 'targetState': array([27, 28], dtype=int32), 'currentDistance': 0.2783287284028576}
episode index:4496
target Thresh 32.0
target distance 10.0
model initialize at round 4496
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 29.]), 'previousTarget': array([14., 29.]), 'currentState': array([15.52757345, 18.12236316,  0.34845942]), 'targetState': array([14, 29], dtype=int32), 'currentDistance': 10.984373624384764}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9317807880676191
{'scaleFactor': 20, 'currentTarget': array([14., 29.]), 'previousTarget': array([14., 29.]), 'currentState': array([13.96892674, 29.06302356,  0.84622871]), 'targetState': array([14, 29], dtype=int32), 'currentDistance': 0.07026746603457254}
episode index:4497
target Thresh 32.0
target distance 4.0
model initialize at round 4497
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  6.]), 'previousTarget': array([24.,  6.]), 'currentState': array([21.07891878,  5.70836442,  0.13510853]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 2.9356033136871806}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.931793731422873
{'scaleFactor': 20, 'currentTarget': array([24.,  6.]), 'previousTarget': array([24.,  6.]), 'currentState': array([23.05492792,  6.01637339,  0.17454572]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 0.9452139052308471}
episode index:4498
target Thresh 32.0
target distance 8.0
model initialize at round 4498
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([22.76922091, 11.66707201,  2.71835583]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 7.79780600996135}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9318001333518744
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.14727936, 10.75607662,  2.70206332]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.28493828419092926}
episode index:4499
target Thresh 32.0
target distance 17.0
model initialize at round 4499
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 28.]), 'previousTarget': array([10., 28.]), 'currentState': array([ 9.77616228, 12.96638366,  1.43072069]), 'targetState': array([10, 28], dtype=int32), 'currentDistance': 15.035282622524555}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9317981210321135
{'scaleFactor': 20, 'currentTarget': array([10., 28.]), 'previousTarget': array([10., 28.]), 'currentState': array([10.32419729, 28.40989788,  0.18977291]), 'targetState': array([10, 28], dtype=int32), 'currentDistance': 0.5226089909933194}
episode index:4500
target Thresh 32.0
target distance 16.0
model initialize at round 4500
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 12.]), 'previousTarget': array([14., 12.]), 'currentState': array([14.27013304, 26.3388509 ,  3.86359608]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 14.341395221010101}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.93179818040267
{'scaleFactor': 20, 'currentTarget': array([14., 12.]), 'previousTarget': array([14., 12.]), 'currentState': array([14.3794285 , 12.7119156 ,  5.25290495]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 0.8067154448940662}
episode index:4501
target Thresh 32.0
target distance 13.0
model initialize at round 4501
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  5.]), 'previousTarget': array([21.,  5.]), 'currentState': array([13.40434613, 16.86947772,  4.81666652]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 14.091786941419588}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9317982397468514
{'scaleFactor': 20, 'currentTarget': array([21.,  5.]), 'previousTarget': array([21.,  5.]), 'currentState': array([20.97524634,  5.30704169,  5.74703496]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 0.3080378909522746}
episode index:4502
target Thresh 32.0
target distance 26.0
model initialize at round 4502
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([ 4.55045731, 28.14488944,  4.68456554]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 26.861856264401855}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9317842374312888
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.30043692,  2.34671928,  4.23813526]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.7190535719199839}
episode index:4503
target Thresh 32.0
target distance 9.0
model initialize at round 4503
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([ 8.3006013 , 15.50796569,  5.17740628]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 10.754082443907535}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9317885015992436
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.49023524,  8.79221251,  6.14117879]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.942051365327521}
episode index:4504
target Thresh 32.0
target distance 18.0
model initialize at round 4504
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 5.]), 'previousTarget': array([8., 5.]), 'currentState': array([ 2.0668207 , 21.59944137,  5.13464665]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 17.627934373988158}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9317844458269648
{'scaleFactor': 20, 'currentTarget': array([8., 5.]), 'previousTarget': array([8., 5.]), 'currentState': array([8.3917086 , 4.95261017, 4.54410182]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 0.3945648526942292}
episode index:4505
target Thresh 32.0
target distance 22.0
model initialize at round 4505
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  5.]), 'previousTarget': array([25.,  5.]), 'currentState': array([ 5.04062316, 25.31752033,  5.7465291 ]), 'targetState': array([25,  5], dtype=int32), 'currentDistance': 28.48119302844152}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9317704558950833
{'scaleFactor': 20, 'currentTarget': array([25.,  5.]), 'previousTarget': array([25.,  5.]), 'currentState': array([24.78060904,  5.6651522 ,  5.62099938]), 'targetState': array([25,  5], dtype=int32), 'currentDistance': 0.7003997741176461}
episode index:4506
target Thresh 32.0
target distance 3.0
model initialize at round 4506
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 2.28082027, 12.47843112,  5.28085268]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 2.2674517656772197}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9317833756963048
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.91859971, 11.44893863,  6.17207611]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.456258594339736}
episode index:4507
target Thresh 32.0
target distance 21.0
model initialize at round 4507
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 29.]), 'previousTarget': array([ 7., 29.]), 'currentState': array([4.74297164, 9.90317664, 1.20863295]), 'targetState': array([ 7, 29], dtype=int32), 'currentDistance': 19.22973841597565}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9317772973243686
{'scaleFactor': 20, 'currentTarget': array([ 7., 29.]), 'previousTarget': array([ 7., 29.]), 'currentState': array([ 6.99598085, 29.49414215,  0.69507498]), 'targetState': array([ 7, 29], dtype=int32), 'currentDistance': 0.4941584928259594}
episode index:4508
target Thresh 32.0
target distance 4.0
model initialize at round 4508
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  6.]), 'previousTarget': array([18.,  6.]), 'currentState': array([20.95414748,  3.57533385,  2.73956954]), 'targetState': array([18,  6], dtype=int32), 'currentDistance': 3.821778805822365}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.931788014268852
{'scaleFactor': 20, 'currentTarget': array([18.,  6.]), 'previousTarget': array([18.,  6.]), 'currentState': array([17.93856064,  6.0826161 ,  2.83849043]), 'targetState': array([18,  6], dtype=int32), 'currentDistance': 0.10295734717680882}
episode index:4509
target Thresh 32.0
target distance 19.0
model initialize at round 4509
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  2.]), 'previousTarget': array([20.,  2.]), 'currentState': array([ 5.11226134, 19.32077831,  5.78914285]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 22.839748768101604}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9317799322821535
{'scaleFactor': 20, 'currentTarget': array([20.,  2.]), 'previousTarget': array([20.,  2.]), 'currentState': array([19.56230139,  2.95513842,  5.92789469]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 1.0506519302955823}
episode index:4510
target Thresh 32.0
target distance 21.0
model initialize at round 4510
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 5.]), 'previousTarget': array([9., 5.]), 'currentState': array([ 8.54687933, 26.66298749,  5.6780985 ]), 'targetState': array([9, 5], dtype=int32), 'currentDistance': 21.667725890647002}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9317718538786901
{'scaleFactor': 20, 'currentTarget': array([9., 5.]), 'previousTarget': array([9., 5.]), 'currentState': array([8.75705339, 5.32584785, 4.77630028]), 'targetState': array([9, 5], dtype=int32), 'currentDistance': 0.40644787963466517}
episode index:4511
target Thresh 32.0
target distance 19.0
model initialize at round 4511
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 21.]), 'previousTarget': array([24., 21.]), 'currentState': array([18.04297397,  3.32083057,  1.91240757]), 'targetState': array([24, 21], dtype=int32), 'currentDistance': 18.65580850604169}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9317678080882655
{'scaleFactor': 20, 'currentTarget': array([24., 21.]), 'previousTarget': array([24., 21.]), 'currentState': array([23.25790464, 20.12946788,  1.85857955]), 'targetState': array([24, 21], dtype=int32), 'currentDistance': 1.143910698845242}
episode index:4512
target Thresh 32.0
target distance 14.0
model initialize at round 4512
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 4.]), 'previousTarget': array([5., 4.]), 'currentState': array([18.47410116, 17.59869274,  2.89860171]), 'targetState': array([5, 4], dtype=int32), 'currentDistance': 19.143558873642718}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9317617399001247
{'scaleFactor': 20, 'currentTarget': array([5., 4.]), 'previousTarget': array([5., 4.]), 'currentState': array([4.82650041, 3.85622797, 3.08581513]), 'targetState': array([5, 4], dtype=int32), 'currentDistance': 0.22532754977227945}
episode index:4513
target Thresh 32.0
target distance 21.0
model initialize at round 4513
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([10.0837288 , 25.27901902,  4.61700785]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 19.374020483976906}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9317556744005918
{'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([12.25348152,  5.63791442,  4.66379096]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 0.44199417425139553}
episode index:4514
target Thresh 32.0
target distance 13.0
model initialize at round 4514
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([ 3.34244788, 17.70860843,  4.32561088]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 15.377318914733141}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9317536786132224
{'scaleFactor': 20, 'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([12.47457671,  5.15254282,  4.97517634]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 0.4984900873531228}
episode index:4515
target Thresh 32.0
target distance 20.0
model initialize at round 4515
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 25.]), 'previousTarget': array([22., 25.]), 'currentState': array([11.96009812,  6.75440924,  1.05596761]), 'targetState': array([22, 25], dtype=int32), 'currentDistance': 20.825494282646265}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9317476175849663
{'scaleFactor': 20, 'currentTarget': array([22., 25.]), 'previousTarget': array([22., 25.]), 'currentState': array([21.17367713, 24.38724624,  1.56344764]), 'targetState': array([22, 25], dtype=int32), 'currentDistance': 1.0287257470781939}
episode index:4516
target Thresh 32.0
target distance 25.0
model initialize at round 4516
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 28.]), 'previousTarget': array([24., 28.]), 'currentState': array([11.89011908,  1.57168767,  6.27969027]), 'targetState': array([24, 28], dtype=int32), 'currentDistance': 29.070688127391783}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.9317317465947197
{'scaleFactor': 20, 'currentTarget': array([24., 28.]), 'previousTarget': array([24., 28.]), 'currentState': array([24.25052694, 28.1445584 ,  1.07174967]), 'targetState': array([24, 28], dtype=int32), 'currentDistance': 0.28924190464423866}
episode index:4517
target Thresh 32.0
target distance 2.0
model initialize at round 4517
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 26.]), 'previousTarget': array([ 6., 26.]), 'currentState': array([ 6.17771923, 26.53132404,  4.00081229]), 'targetState': array([ 6, 26], dtype=int32), 'currentDistance': 0.5602582974168413}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9317468568765712
{'scaleFactor': 20, 'currentTarget': array([ 6., 26.]), 'previousTarget': array([ 6., 26.]), 'currentState': array([ 6.17771923, 26.53132404,  4.00081229]), 'targetState': array([ 6, 26], dtype=int32), 'currentDistance': 0.5602582974168413}
episode index:4518
target Thresh 32.0
target distance 3.0
model initialize at round 4518
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 26.]), 'previousTarget': array([ 4., 26.]), 'currentState': array([ 5.76929679, 24.1479888 ,  1.38101363]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 2.5613193116699615}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9317597475920223
{'scaleFactor': 20, 'currentTarget': array([ 4., 26.]), 'previousTarget': array([ 4., 26.]), 'currentState': array([ 4.67054668, 25.51543768,  3.13183451]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 0.8273049544141848}
episode index:4519
target Thresh 32.0
target distance 9.0
model initialize at round 4519
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([24.07495391, 20.07956216,  2.62396312]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 7.134576278516391}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9317661272960948
{'scaleFactor': 20, 'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([16.81451205, 21.30816865,  1.60034823]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 0.3596855493259504}
episode index:4520
target Thresh 32.0
target distance 14.0
model initialize at round 4520
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 14.]), 'previousTarget': array([ 2., 14.]), 'currentState': array([16.11472095,  9.67905545,  2.51257765]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 14.761297688138592}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9317661934807024
{'scaleFactor': 20, 'currentTarget': array([ 2., 14.]), 'previousTarget': array([ 2., 14.]), 'currentState': array([ 2.79936574, 13.28501149,  3.33949876]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 1.0724710485353113}
episode index:4521
target Thresh 32.0
target distance 12.0
model initialize at round 4521
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([7.63780917, 9.38725857, 1.24218481]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 14.310206113961614}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9317662596360378
{'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([15.53191415, 20.71704067,  0.35529112]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 0.5469646653439612}
episode index:4522
target Thresh 32.0
target distance 17.0
model initialize at round 4522
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([18.35468756, 15.6459731 ,  4.36353421]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 17.153056130254203}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9317622249218762
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.72914218, 7.81212598, 2.69917212]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.3296370797482387}
episode index:4523
target Thresh 32.0
target distance 25.0
model initialize at round 4523
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 29.]), 'previousTarget': array([23., 29.]), 'currentState': array([10.84376746,  5.67570268,  0.65376163]), 'targetState': array([23, 29], dtype=int32), 'currentDistance': 26.302031005274156}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9317502352662788
{'scaleFactor': 20, 'currentTarget': array([23., 29.]), 'previousTarget': array([23., 29.]), 'currentState': array([22.65117343, 28.69746381,  1.38292885]), 'targetState': array([23, 29], dtype=int32), 'currentDistance': 0.46174465296111505}
episode index:4524
target Thresh 32.0
target distance 13.0
model initialize at round 4524
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  9.]), 'previousTarget': array([17.,  9.]), 'currentState': array([23.0426521 , 20.31757054,  3.72773504]), 'targetState': array([17,  9], dtype=int32), 'currentDistance': 12.829693967751746}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9317523855235461
{'scaleFactor': 20, 'currentTarget': array([17.,  9.]), 'previousTarget': array([17.,  9.]), 'currentState': array([17.60308696,  9.9180963 ,  3.71940327]), 'targetState': array([17,  9], dtype=int32), 'currentDistance': 1.0984601494447381}
episode index:4525
target Thresh 32.0
target distance 8.0
model initialize at round 4525
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 14.]), 'previousTarget': array([25., 14.]), 'currentState': array([18.6160749 , 10.06962389,  0.14696324]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 7.496823061306048}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9317587583968285
{'scaleFactor': 20, 'currentTarget': array([25., 14.]), 'previousTarget': array([25., 14.]), 'currentState': array([25.11644939, 14.21349583,  6.14597959]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 0.24318908482983426}
episode index:4526
target Thresh 32.0
target distance 15.0
model initialize at round 4526
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([26.34855156, 16.44822585,  3.30492353]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 15.34809749990705}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9317567672185717
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.75565326, 10.96572024,  2.44543144]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.24673960221893818}
episode index:4527
target Thresh 32.0
target distance 9.0
model initialize at round 4527
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  5.]), 'previousTarget': array([13.,  5.]), 'currentState': array([22.81904954,  4.47022002,  2.07252313]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 9.833331111137559}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9317610148516726
{'scaleFactor': 20, 'currentTarget': array([13.,  5.]), 'previousTarget': array([13.,  5.]), 'currentState': array([13.30996294,  4.7580559 ,  3.53034158]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 0.3932098354867602}
episode index:4528
target Thresh 32.0
target distance 15.0
model initialize at round 4528
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([ 3.0361806 , 30.3261666 ,  6.18072576]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 16.257715168417032}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9317590240544936
{'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([16.70986627, 22.35449469,  6.21653444]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 0.45808739728241665}
episode index:4529
target Thresh 32.0
target distance 5.0
model initialize at round 4529
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([25.67551246,  8.15825951,  5.3673603 ]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 5.810852615956992}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9317675317754528
{'scaleFactor': 20, 'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([23.0749234 ,  3.37187369,  4.00332803]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 0.37934622601604256}
episode index:4530
target Thresh 32.0
target distance 18.0
model initialize at round 4530
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 28.]), 'previousTarget': array([ 6., 28.]), 'currentState': array([22.30770124, 15.79607779,  2.0652889 ]), 'targetState': array([ 6, 28], dtype=int32), 'currentDistance': 20.368525649142843}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9317614877549789
{'scaleFactor': 20, 'currentTarget': array([ 6., 28.]), 'previousTarget': array([ 6., 28.]), 'currentState': array([ 6.4791089 , 27.50339478,  2.02038091]), 'targetState': array([ 6, 28], dtype=int32), 'currentDistance': 0.6900449823833384}
episode index:4531
target Thresh 32.0
target distance 4.0
model initialize at round 4531
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([27.66402648, 17.25180146,  1.16018122]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 4.100972620549507}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9317721537991636
{'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([26.14657283, 20.68712007,  1.73169052]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 0.34551041125232135}
episode index:4532
target Thresh 32.0
target distance 14.0
model initialize at round 4532
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([14.60722616, 26.50078848,  4.97202832]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 14.537995018257975}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9317722184790903
{'scaleFactor': 20, 'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([20.03778712, 13.67329328,  5.51111293]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 0.674352808819285}
episode index:4533
target Thresh 32.0
target distance 9.0
model initialize at round 4533
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([ 6.88019716, 11.07762996,  0.64819043]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 7.696222391569955}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.931778575733506
{'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([14.19345321, 14.06322643,  0.59623918]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 0.20352327729646463}
episode index:4534
target Thresh 32.0
target distance 8.0
model initialize at round 4534
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 14.]), 'previousTarget': array([17., 14.]), 'currentState': array([23.37198814,  9.12802335,  2.29176342]), 'targetState': array([17, 14], dtype=int32), 'currentDistance': 8.021121445467664}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9317849301842813
{'scaleFactor': 20, 'currentTarget': array([17., 14.]), 'previousTarget': array([17., 14.]), 'currentState': array([16.98630997, 13.68482926,  1.96737769]), 'targetState': array([17, 14], dtype=int32), 'currentDistance': 0.3154679227967107}
episode index:4535
target Thresh 32.0
target distance 11.0
model initialize at round 4535
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([14.77058614,  7.2007649 ,  5.43826103]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 10.593867833369975}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9317891641171993
{'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([23.55282   ,  2.66807579,  6.27503233]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 0.8039248829052185}
episode index:4536
target Thresh 32.0
target distance 6.0
model initialize at round 4536
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([25.74719583, 15.1237748 ,  1.4004333 ]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 4.882774024176912}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9317998123067259
{'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([25.86645756, 19.04866948,  1.08620721]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 0.9606577628611467}
episode index:4537
target Thresh 32.0
target distance 24.0
model initialize at round 4537
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 28.]), 'previousTarget': array([ 8., 28.]), 'currentState': array([3.61797888, 3.53682355, 0.731188  ]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 24.85254737364864}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9317898046071688
{'scaleFactor': 20, 'currentTarget': array([ 8., 28.]), 'previousTarget': array([ 8., 28.]), 'currentState': array([ 7.64101602, 27.01216844,  1.49968861]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 1.0510379092741027}
episode index:4538
target Thresh 32.0
target distance 2.0
model initialize at round 4538
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 26.]), 'previousTarget': array([16., 26.]), 'currentState': array([14.93593991, 24.60112901,  0.02864998]), 'targetState': array([16, 26], dtype=int32), 'currentDistance': 1.7575733005923717}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9318026290608794
{'scaleFactor': 20, 'currentTarget': array([16., 26.]), 'previousTarget': array([16., 26.]), 'currentState': array([16.17668436, 25.90367516,  1.60651522]), 'targetState': array([16, 26], dtype=int32), 'currentDistance': 0.20123577970618717}
episode index:4539
target Thresh 32.0
target distance 15.0
model initialize at round 4539
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([17.51602366, 20.73079434,  5.72236747]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 16.669865248929007}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9317986014437919
{'scaleFactor': 20, 'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([11.63559354,  4.99596217,  2.8648237 ]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 0.36442882926119297}
episode index:4540
target Thresh 32.0
target distance 18.0
model initialize at round 4540
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 27.]), 'previousTarget': array([18., 27.]), 'currentState': array([25.65487958, 10.64720368,  2.78732777]), 'targetState': array([18, 27], dtype=int32), 'currentDistance': 18.055778262572293}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9317945756005943
{'scaleFactor': 20, 'currentTarget': array([18., 27.]), 'previousTarget': array([18., 27.]), 'currentState': array([18.16229613, 26.48384573,  1.26289123]), 'targetState': array([18, 27], dtype=int32), 'currentDistance': 0.5410686325410271}
episode index:4541
target Thresh 32.0
target distance 20.0
model initialize at round 4541
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 27.]), 'previousTarget': array([ 6., 27.]), 'currentState': array([15.42855898,  8.62749049,  2.32331631]), 'targetState': array([ 6, 27], dtype=int32), 'currentDistance': 20.65058910462965}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9317885402636079
{'scaleFactor': 20, 'currentTarget': array([ 6., 27.]), 'previousTarget': array([ 6., 27.]), 'currentState': array([ 6.04293938, 26.16067465,  2.52644538]), 'targetState': array([ 6, 27], dtype=int32), 'currentDistance': 0.8404230112388169}
episode index:4542
target Thresh 32.0
target distance 19.0
model initialize at round 4542
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 23.]), 'previousTarget': array([ 8., 23.]), 'currentState': array([10.09838208,  5.68013689,  2.52223194]), 'targetState': array([ 8, 23], dtype=int32), 'currentDistance': 17.446514421204093}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.931784518407394
{'scaleFactor': 20, 'currentTarget': array([ 8., 23.]), 'previousTarget': array([ 8., 23.]), 'currentState': array([ 8.03222091, 23.01595049,  0.68502465]), 'targetState': array([ 8, 23], dtype=int32), 'currentDistance': 0.035952822594239285}
episode index:4543
target Thresh 32.0
target distance 17.0
model initialize at round 4543
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 19.]), 'previousTarget': array([26., 19.]), 'currentState': array([10.69844747, 14.24908815,  1.0978679 ]), 'targetState': array([26, 19], dtype=int32), 'currentDistance': 16.0221307356855}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9317825290095111
{'scaleFactor': 20, 'currentTarget': array([26., 19.]), 'previousTarget': array([26., 19.]), 'currentState': array([25.6226963 , 19.13190637,  0.72749764]), 'targetState': array([26, 19], dtype=int32), 'currentDistance': 0.3996966027170877}
episode index:4544
target Thresh 32.0
target distance 9.0
model initialize at round 4544
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 2.]), 'previousTarget': array([8., 2.]), 'currentState': array([4.04681094, 9.61298208, 5.12029052]), 'targetState': array([8, 2], dtype=int32), 'currentDistance': 8.578181618104201}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9317888686092889
{'scaleFactor': 20, 'currentTarget': array([8., 2.]), 'previousTarget': array([8., 2.]), 'currentState': array([7.90976025, 2.79418692, 4.59009472]), 'targetState': array([8, 2], dtype=int32), 'currentDistance': 0.79929724232582}
episode index:4545
target Thresh 32.0
target distance 6.0
model initialize at round 4545
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([14.99289236, 26.32825695,  4.83394903]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 4.770984102471948}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.931799495782934
{'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([16.75113732, 22.88160238,  4.65743911]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 0.9160542543099723}
episode index:4546
target Thresh 32.0
target distance 10.0
model initialize at round 4546
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 21.]), 'previousTarget': array([ 2., 21.]), 'currentState': array([ 4.88629415, 12.26176359,  1.28394723]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 9.2025794985122}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9318037162698741
{'scaleFactor': 20, 'currentTarget': array([ 2., 21.]), 'previousTarget': array([ 2., 21.]), 'currentState': array([ 2.36012669, 21.55843494,  1.71599212]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 0.6644853777160646}
episode index:4547
target Thresh 32.0
target distance 16.0
model initialize at round 4547
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 24.]), 'previousTarget': array([22., 24.]), 'currentState': array([ 6.08905504, 24.31938782,  5.77532911]), 'targetState': array([22, 24], dtype=int32), 'currentDistance': 15.914150243654147}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9318017244005157
{'scaleFactor': 20, 'currentTarget': array([22., 24.]), 'previousTarget': array([22., 24.]), 'currentState': array([21.68500713, 24.208931  ,  5.56284004]), 'targetState': array([22, 24], dtype=int32), 'currentDistance': 0.3779850116330035}
episode index:4548
target Thresh 32.0
target distance 3.0
model initialize at round 4548
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 23.]), 'previousTarget': array([23., 23.]), 'currentState': array([27.50434746, 23.24546244,  4.80828149]), 'targetState': array([23, 23], dtype=int32), 'currentDistance': 4.5110306870167625}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9318101872001638
{'scaleFactor': 20, 'currentTarget': array([23., 23.]), 'previousTarget': array([23., 23.]), 'currentState': array([22.39485734, 23.1225034 ,  2.38040272]), 'targetState': array([23, 23], dtype=int32), 'currentDistance': 0.617417783879955}
episode index:4549
target Thresh 32.0
target distance 5.0
model initialize at round 4549
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 12.]), 'previousTarget': array([11., 12.]), 'currentState': array([7.0875339 , 5.71560494, 0.14178913]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 7.402770576065542}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9318165137546253
{'scaleFactor': 20, 'currentTarget': array([11., 12.]), 'previousTarget': array([11., 12.]), 'currentState': array([11.0014845 , 12.11133989,  0.25192868]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 0.1113497871141495}
episode index:4550
target Thresh 32.0
target distance 18.0
model initialize at round 4550
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 26.]), 'previousTarget': array([22., 26.]), 'currentState': array([5.32311845, 8.14302568, 2.04650593]), 'targetState': array([22, 26], dtype=int32), 'currentDistance': 24.4333769671337}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9318065309723712
{'scaleFactor': 20, 'currentTarget': array([22., 26.]), 'previousTarget': array([22., 26.]), 'currentState': array([21.09486935, 25.45347167,  0.76387022]), 'targetState': array([22, 26], dtype=int32), 'currentDistance': 1.0573337753753322}
episode index:4551
target Thresh 32.0
target distance 5.0
model initialize at round 4551
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([27.24238188, 15.6654246 ,  2.43627343]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 5.253047543723352}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9318149871386777
{'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([21.46410177, 16.20237106,  3.0915696 ]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 0.5728358901879098}
episode index:4552
target Thresh 32.0
target distance 5.0
model initialize at round 4552
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 8.31748895, 15.96069807,  4.1749475 ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 5.132670107458937}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9318234395904372
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.43500059, 10.48347273,  4.17732761]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.765522538141307}
episode index:4553
target Thresh 32.0
target distance 18.0
model initialize at round 4553
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 21.]), 'previousTarget': array([25., 21.]), 'currentState': array([ 7.19915807, 29.67114456,  0.44218111]), 'targetState': array([25, 21], dtype=int32), 'currentDistance': 19.800472754181}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9318174138186801
{'scaleFactor': 20, 'currentTarget': array([25., 21.]), 'previousTarget': array([25., 21.]), 'currentState': array([24.89675031, 21.13967066,  6.18407235]), 'targetState': array([25, 21], dtype=int32), 'currentDistance': 0.17369051040111524}
episode index:4554
target Thresh 32.0
target distance 11.0
model initialize at round 4554
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 22.]), 'previousTarget': array([24., 22.]), 'currentState': array([18.34649588, 11.31354779,  1.94419146]), 'targetState': array([24, 22], dtype=int32), 'currentDistance': 12.089763008988745}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9318195351656795
{'scaleFactor': 20, 'currentTarget': array([24., 22.]), 'previousTarget': array([24., 22.]), 'currentState': array([23.63441417, 21.71049471,  1.53305697]), 'targetState': array([24, 22], dtype=int32), 'currentDistance': 0.4663328363790572}
episode index:4555
target Thresh 32.0
target distance 21.0
model initialize at round 4555
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([ 7.63855138, 25.61589393,  5.04292488]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 21.072192975951797}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9318115278608272
{'scaleFactor': 20, 'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([12.05310562,  4.46161293,  3.82198027]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 0.5409998553393204}
episode index:4556
target Thresh 32.0
target distance 2.0
model initialize at round 4556
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([10.45512207,  4.33236217,  2.53951406]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 2.7061832300592568}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9318242968913603
{'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([9.74935872, 6.08090241, 1.35752225]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 0.9526601817666154}
episode index:4557
target Thresh 32.0
target distance 22.0
model initialize at round 4557
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  5.]), 'previousTarget': array([10.,  5.]), 'currentState': array([22.66772381, 25.54166201,  4.24773675]), 'targetState': array([10,  5], dtype=int32), 'currentDistance': 24.13360943898317}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9318143277326997
{'scaleFactor': 20, 'currentTarget': array([10.,  5.]), 'previousTarget': array([10.,  5.]), 'currentState': array([10.21630932,  5.18913143,  3.64792753]), 'targetState': array([10,  5], dtype=int32), 'currentDistance': 0.28733328931292157}
episode index:4558
target Thresh 32.0
target distance 7.0
model initialize at round 4558
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([17.32259956,  9.13680586,  4.07021451]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 7.872335469070018}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9318206408895909
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.21966153,  1.94444778,  3.71463719]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.2265772195827793}
episode index:4559
target Thresh 32.0
target distance 15.0
model initialize at round 4559
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  5.]), 'previousTarget': array([22.,  5.]), 'currentState': array([24.33828143, 20.26660733,  3.9925077 ]), 'targetState': array([22,  5], dtype=int32), 'currentDistance': 15.44463853636419}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9318186505504545
{'scaleFactor': 20, 'currentTarget': array([22.,  5.]), 'previousTarget': array([22.,  5.]), 'currentState': array([21.76145845,  5.15651394,  3.16701959]), 'targetState': array([22,  5], dtype=int32), 'currentDistance': 0.28530454634057173}
episode index:4560
target Thresh 32.0
target distance 12.0
model initialize at round 4560
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 25.]), 'previousTarget': array([ 5., 25.]), 'currentState': array([10.30978962, 14.82163798,  2.33343601]), 'targetState': array([ 5, 25], dtype=int32), 'currentDistance': 11.480109723277126}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9318207688356663
{'scaleFactor': 20, 'currentTarget': array([ 5., 25.]), 'previousTarget': array([ 5., 25.]), 'currentState': array([ 4.8596017 , 25.27783522,  2.10501471]), 'targetState': array([ 5, 25], dtype=int32), 'currentDistance': 0.3112942206589029}
episode index:4561
target Thresh 32.0
target distance 7.0
model initialize at round 4561
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 10.]), 'previousTarget': array([27., 10.]), 'currentState': array([22.40731236, 15.60575915,  5.74382395]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 7.246883152060912}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9318270764290824
{'scaleFactor': 20, 'currentTarget': array([27., 10.]), 'previousTarget': array([27., 10.]), 'currentState': array([27.1541698 ,  9.4837321 ,  5.26428756]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 0.5387957632898799}
episode index:4562
target Thresh 32.0
target distance 15.0
model initialize at round 4562
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([12.47392   , 17.18762821,  4.76944879]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 14.619235081212038}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9318271286472455
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.15413446,  3.84568856,  5.50226856]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.8596200190367091}
episode index:4563
target Thresh 32.0
target distance 22.0
model initialize at round 4563
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 25.]), 'previousTarget': array([ 2., 25.]), 'currentState': array([22.77962067, 26.84109432,  2.89115953]), 'targetState': array([ 2, 25], dtype=int32), 'currentDistance': 20.861022586494492}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.931821115270024
{'scaleFactor': 20, 'currentTarget': array([ 2., 25.]), 'previousTarget': array([ 2., 25.]), 'currentState': array([ 2.945032  , 25.20477518,  2.99157426]), 'targetState': array([ 2, 25], dtype=int32), 'currentDistance': 0.9669634669736854}
episode index:4564
target Thresh 32.0
target distance 13.0
model initialize at round 4564
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([24.90608819,  0.72103752,  2.99481916]), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 17.37870399996483}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9318171056604322
{'scaleFactor': 20, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.85125066, 14.94555976,  1.30154282]), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.15839856493525098}
episode index:4565
target Thresh 32.0
target distance 6.0
model initialize at round 4565
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'previousTarget': array([25.,  3.]), 'currentState': array([23.43287275,  8.38640383,  4.52478647]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 5.609744559592492}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9318255335829769
{'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'previousTarget': array([25.,  3.]), 'currentState': array([24.98033258,  2.77254277,  5.21599519]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 0.2283059305170827}
episode index:4566
target Thresh 32.0
target distance 2.0
model initialize at round 4566
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 25.]), 'previousTarget': array([17., 25.]), 'currentState': array([16.47524831, 24.81004594,  5.77540225]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 0.5580742620061341}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.931840461208643
{'scaleFactor': 20, 'currentTarget': array([17., 25.]), 'previousTarget': array([17., 25.]), 'currentState': array([16.47524831, 24.81004594,  5.77540225]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 0.5580742620061341}
episode index:4567
target Thresh 32.0
target distance 6.0
model initialize at round 4567
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 21.]), 'previousTarget': array([ 5., 21.]), 'currentState': array([ 9.63985433, 20.99115707,  3.52185416]), 'targetState': array([ 5, 21], dtype=int32), 'currentDistance': 4.639862758295681}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9318510259062768
{'scaleFactor': 20, 'currentTarget': array([ 5., 21.]), 'previousTarget': array([ 5., 21.]), 'currentState': array([ 5.76845923, 20.70955777,  3.73189187]), 'targetState': array([ 5, 21], dtype=int32), 'currentDistance': 0.8215146221157682}
episode index:4568
target Thresh 32.0
target distance 12.0
model initialize at round 4568
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 21.]), 'previousTarget': array([25., 21.]), 'currentState': array([1.38841674e+01, 2.84320042e+01, 7.65657425e-03]), 'targetState': array([25, 21], dtype=int32), 'currentDistance': 13.371477867702225}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9318510728141343
{'scaleFactor': 20, 'currentTarget': array([25., 21.]), 'previousTarget': array([25., 21.]), 'currentState': array([25.28686245, 20.95009681,  5.00969208]), 'targetState': array([25, 21], dtype=int32), 'currentDistance': 0.2911707261038909}
episode index:4569
target Thresh 32.0
target distance 16.0
model initialize at round 4569
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 24.]), 'previousTarget': array([ 5., 24.]), 'currentState': array([5.64955506, 9.00433388, 1.49213195]), 'targetState': array([ 5, 24], dtype=int32), 'currentDistance': 15.009727649663539}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9318490801711613
{'scaleFactor': 20, 'currentTarget': array([ 5., 24.]), 'previousTarget': array([ 5., 24.]), 'currentState': array([ 5.02778054, 24.76298317,  0.82315001]), 'targetState': array([ 5, 24], dtype=int32), 'currentDistance': 0.7634887509076218}
episode index:4570
target Thresh 32.0
target distance 14.0
model initialize at round 4570
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 21.]), 'previousTarget': array([21., 21.]), 'currentState': array([ 8.83769386, 14.77543469,  0.5279384 ]), 'targetState': array([21, 21], dtype=int32), 'currentDistance': 13.662609703416022}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9318491274841642
{'scaleFactor': 20, 'currentTarget': array([21., 21.]), 'previousTarget': array([21., 21.]), 'currentState': array([21.01966886, 21.29416513,  6.13623109]), 'targetState': array([21, 21], dtype=int32), 'currentDistance': 0.2948219562812822}
episode index:4571
target Thresh 32.0
target distance 19.0
model initialize at round 4571
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'previousTarget': array([ 2., 17.]), 'currentState': array([19.3997965 , 14.52130889,  3.83664548]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 17.575460961688492}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9318451178866137
{'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'previousTarget': array([ 2., 17.]), 'currentState': array([ 1.95281642, 16.78259961,  2.36177885]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 0.2224617281260609}
episode index:4572
target Thresh 32.0
target distance 19.0
model initialize at round 4572
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 14.]), 'previousTarget': array([25., 14.]), 'currentState': array([6.78189562, 4.49031115, 0.0776217 ]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 20.550754465817953}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9318391124103665
{'scaleFactor': 20, 'currentTarget': array([25., 14.]), 'previousTarget': array([25., 14.]), 'currentState': array([24.22580157, 13.9491111 ,  0.86038206]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 0.7758691192337374}
episode index:4573
target Thresh 32.0
target distance 24.0
model initialize at round 4573
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 24.]), 'previousTarget': array([ 2., 24.]), 'currentState': array([24.32000214, 10.90002357,  2.19103241]), 'targetState': array([ 2, 24], dtype=int32), 'currentDistance': 25.880337675793264}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9318272370082215
{'scaleFactor': 20, 'currentTarget': array([ 2., 24.]), 'previousTarget': array([ 2., 24.]), 'currentState': array([ 1.95692504, 23.78914943,  2.8907302 ]), 'targetState': array([ 2, 24], dtype=int32), 'currentDistance': 0.2152055164534107}
episode index:4574
target Thresh 32.0
target distance 14.0
model initialize at round 4574
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  2.]), 'previousTarget': array([12.,  2.]), 'currentState': array([26.95997792, 11.61767208,  4.3093903 ]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 17.784840608433573}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9318232348247188
{'scaleFactor': 20, 'currentTarget': array([12.,  2.]), 'previousTarget': array([12.,  2.]), 'currentState': array([12.20727429,  1.8803184 ,  2.9802619 ]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 0.23934560355069334}
episode index:4575
target Thresh 32.0
target distance 13.0
model initialize at round 4575
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([ 0.80144387, 14.81853849,  4.92980838]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 15.750919445633945}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9318212508779538
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.92476849,  8.3431218 ,  5.26416552]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.3512724709962055}
episode index:4576
target Thresh 32.0
target distance 11.0
model initialize at round 4576
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([13.48890011, 15.25907813,  4.60746598]), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 13.31578421406326}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9318213042091815
{'scaleFactor': 20, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.92841201, 4.67470578, 3.1682994 ]), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.3330783280278961}
episode index:4577
target Thresh 32.0
target distance 17.0
model initialize at round 4577
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([18.9753922 , 25.33512806,  3.23535514]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 21.464134095227067}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9318133349977462
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.59567216, 11.08120271,  3.91002335]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.41240135669739786}
episode index:4578
target Thresh 32.0
target distance 17.0
model initialize at round 4578
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 15.]), 'previousTarget': array([ 9., 15.]), 'currentState': array([27.12688664, 22.74999417,  4.43603501]), 'targetState': array([ 9, 15], dtype=int32), 'currentDistance': 19.714117503374837}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9318073443316642
{'scaleFactor': 20, 'currentTarget': array([ 9., 15.]), 'previousTarget': array([ 9., 15.]), 'currentState': array([ 9.01116724, 15.15803054,  3.27881467]), 'targetState': array([ 9, 15], dtype=int32), 'currentDistance': 0.15842461118136889}
episode index:4579
target Thresh 32.0
target distance 15.0
model initialize at round 4579
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  2.]), 'previousTarget': array([10.,  2.]), 'currentState': array([ 3.5163187 , 15.13635168,  5.43487793]), 'targetState': array([10,  2], dtype=int32), 'currentDistance': 14.649295497784125}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9318074006643227
{'scaleFactor': 20, 'currentTarget': array([10.,  2.]), 'previousTarget': array([10.,  2.]), 'currentState': array([9.73758351, 2.7893043 , 4.57697695]), 'targetState': array([10,  2], dtype=int32), 'currentDistance': 0.8317834450409409}
episode index:4580
target Thresh 32.0
target distance 6.0
model initialize at round 4580
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 18.]), 'previousTarget': array([17., 18.]), 'currentState': array([17.65539787, 13.95326622,  1.51995251]), 'targetState': array([17, 18], dtype=int32), 'currentDistance': 4.099463462049665}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9318179425982531
{'scaleFactor': 20, 'currentTarget': array([17., 18.]), 'previousTarget': array([17., 18.]), 'currentState': array([16.93649006, 17.70040881,  1.03954098]), 'targetState': array([17, 18], dtype=int32), 'currentDistance': 0.3062489060265852}
episode index:4581
target Thresh 32.0
target distance 8.0
model initialize at round 4581
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 11.]), 'previousTarget': array([17., 11.]), 'currentState': array([9.32307597, 4.65166885, 0.36762953]), 'targetState': array([17, 11], dtype=int32), 'currentDistance': 9.961750392227673}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9318221268207109
{'scaleFactor': 20, 'currentTarget': array([17., 11.]), 'previousTarget': array([17., 11.]), 'currentState': array([17.12271614, 10.83156901,  0.55749121]), 'targetState': array([17, 11], dtype=int32), 'currentDistance': 0.20839445466304304}
episode index:4582
target Thresh 32.0
target distance 3.0
model initialize at round 4582
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 26.]), 'previousTarget': array([27., 26.]), 'currentState': array([25.83641889, 24.21592232,  1.32420135]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 2.129989246438213}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9318348210980792
{'scaleFactor': 20, 'currentTarget': array([27., 26.]), 'previousTarget': array([27., 26.]), 'currentState': array([26.93776079, 25.83536067,  0.61584991]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 0.17601087286028602}
episode index:4583
target Thresh 32.0
target distance 20.0
model initialize at round 4583
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 10.]), 'previousTarget': array([27., 10.]), 'currentState': array([8.2605049 , 8.11513026, 5.99746567]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 18.834049236760624}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9318288322791243
{'scaleFactor': 20, 'currentTarget': array([27., 10.]), 'previousTarget': array([27., 10.]), 'currentState': array([2.79631409e+01, 9.76680131e+00, 1.24329031e-02]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 0.9909702792983288}
episode index:4584
target Thresh 32.0
target distance 9.0
model initialize at round 4584
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([6.59532343, 2.92696916, 1.54827785]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 11.653858381351117}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9318309372555958
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.97293468, 11.10837295,  0.34439003]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.1117015165125909}
episode index:4585
target Thresh 32.0
target distance 3.0
model initialize at round 4585
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 12.]), 'previousTarget': array([11., 12.]), 'currentState': array([ 9.88374671, 12.3572837 ,  5.78260663]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 1.172037988364615}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9318436213076552
{'scaleFactor': 20, 'currentTarget': array([11., 12.]), 'previousTarget': array([11., 12.]), 'currentState': array([11.61988309, 11.36463625,  5.74459714]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 0.8876610530858059}
episode index:4586
target Thresh 32.0
target distance 16.0
model initialize at round 4586
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  9.]), 'previousTarget': array([20.,  9.]), 'currentState': array([19.52509863, 23.19905004,  4.37402081]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 14.20698959446331}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9318436696456973
{'scaleFactor': 20, 'currentTarget': array([20.,  9.]), 'previousTarget': array([20.,  9.]), 'currentState': array([20.02123881,  9.30761981,  4.79580133]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 0.30835213065559647}
episode index:4587
target Thresh 32.0
target distance 13.0
model initialize at round 4587
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 15.]), 'previousTarget': array([12., 15.]), 'currentState': array([ 3.12106772, 26.3442108 ,  5.34220657]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 14.405782070979296}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9318437179626681
{'scaleFactor': 20, 'currentTarget': array([12., 15.]), 'previousTarget': array([12., 15.]), 'currentState': array([11.73063483, 15.4091255 ,  4.9643669 ]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 0.48983799927342075}
episode index:4588
target Thresh 32.0
target distance 20.0
model initialize at round 4588
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  3.]), 'previousTarget': array([10.,  3.]), 'currentState': array([12.06405499, 21.30171778,  3.76810169]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 18.417741356699224}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9318397243975167
{'scaleFactor': 20, 'currentTarget': array([10.,  3.]), 'previousTarget': array([10.,  3.]), 'currentState': array([10.52631253,  3.78611449,  4.69866925]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 0.9460342812909446}
episode index:4589
target Thresh 32.0
target distance 4.0
model initialize at round 4589
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 17.]), 'previousTarget': array([20., 17.]), 'currentState': array([16.06207165, 19.68182496,  0.52390575]), 'targetState': array([20, 17], dtype=int32), 'currentDistance': 4.764395536873523}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9318481033246632
{'scaleFactor': 20, 'currentTarget': array([20., 17.]), 'previousTarget': array([20., 17.]), 'currentState': array([20.64367602, 16.77559747,  5.61036283]), 'targetState': array([20, 17], dtype=int32), 'currentDistance': 0.6816709776484445}
episode index:4590
target Thresh 32.0
target distance 11.0
model initialize at round 4590
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 15.]), 'previousTarget': array([22., 15.]), 'currentState': array([12.13220717, 24.32223084,  5.80102587]), 'targetState': array([22, 15], dtype=int32), 'currentDistance': 13.574878387020608}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9318481506443282
{'scaleFactor': 20, 'currentTarget': array([22., 15.]), 'previousTarget': array([22., 15.]), 'currentState': array([22.26819376, 14.86825607,  5.0490045 ]), 'targetState': array([22, 15], dtype=int32), 'currentDistance': 0.2988048793829742}
episode index:4591
target Thresh 32.0
target distance 7.0
model initialize at round 4591
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 27.]), 'previousTarget': array([10., 27.]), 'currentState': array([12.04971823, 21.85598657,  2.21370459]), 'targetState': array([10, 27], dtype=int32), 'currentDistance': 5.537347647635021}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9318565240871322
{'scaleFactor': 20, 'currentTarget': array([10., 27.]), 'previousTarget': array([10., 27.]), 'currentState': array([ 9.72646756, 27.28149196,  1.52588737]), 'targetState': array([10, 27], dtype=int32), 'currentDistance': 0.39250187649338375}
episode index:4592
target Thresh 32.0
target distance 12.0
model initialize at round 4592
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 25.]), 'previousTarget': array([ 9., 25.]), 'currentState': array([19.07896826, 22.55309742,  2.80789454]), 'targetState': array([ 9, 25], dtype=int32), 'currentDistance': 10.371737239163052}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9318606898885283
{'scaleFactor': 20, 'currentTarget': array([ 9., 25.]), 'previousTarget': array([ 9., 25.]), 'currentState': array([ 9.40135469, 25.01381576,  2.76373869]), 'targetState': array([ 9, 25], dtype=int32), 'currentDistance': 0.4015924080880213}
episode index:4593
target Thresh 32.0
target distance 15.0
model initialize at round 4593
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([15.76287891, 18.33381824,  3.56102419]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 18.457567196749974}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9318566969755102
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.36096794, 5.39858565, 4.03010374]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.5377437815566523}
episode index:4594
target Thresh 32.0
target distance 8.0
model initialize at round 4594
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([15.49644758, 16.22991382,  4.7979185 ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 11.935416643095332}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9318587913068325
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.20719962, 9.14849626, 3.83116078]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.254917278493381}
episode index:4595
target Thresh 32.0
target distance 13.0
model initialize at round 4595
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([25.15831974, 11.24858065,  3.71199575]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 13.306014462301095}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9318588362495218
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.49646507,  3.58799972,  3.49396119]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.6506086789843579}
episode index:4596
target Thresh 32.0
target distance 8.0
model initialize at round 4596
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'previousTarget': array([22., 18.]), 'currentState': array([21.94011837, 24.02928488,  4.97733897]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 6.029582238567479}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9318671982603441
{'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'previousTarget': array([22., 18.]), 'currentState': array([22.22385005, 18.05298126,  4.66208281]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 0.2300344693649261}
episode index:4597
target Thresh 32.0
target distance 15.0
model initialize at round 4597
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 21.]), 'previousTarget': array([ 5., 21.]), 'currentState': array([19.66091068, 27.6484558 ,  2.78366774]), 'targetState': array([ 5, 21], dtype=int32), 'currentDistance': 16.097958454576627}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9318652142447216
{'scaleFactor': 20, 'currentTarget': array([ 5., 21.]), 'previousTarget': array([ 5., 21.]), 'currentState': array([ 5.40348386, 21.02149733,  3.26961468]), 'targetState': array([ 5, 21], dtype=int32), 'currentDistance': 0.4040561342114376}
episode index:4598
target Thresh 32.0
target distance 11.0
model initialize at round 4598
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([20.30277167, 21.53175088,  3.00795281]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 12.724851635167107}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9318673049025074
{'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([10.51053789, 28.27242597,  2.67946724]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 0.8888267035784864}
episode index:4599
target Thresh 32.0
target distance 7.0
model initialize at round 4599
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  5.]), 'previousTarget': array([10.,  5.]), 'currentState': array([4.05427678, 8.31182642, 6.16700632]), 'targetState': array([10,  5], dtype=int32), 'currentDistance': 6.805866506995619}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9318756596188328
{'scaleFactor': 20, 'currentTarget': array([10.,  5.]), 'previousTarget': array([10.,  5.]), 'currentState': array([9.39195251, 5.76444291, 5.4443076 ]), 'targetState': array([10,  5], dtype=int32), 'currentDistance': 0.9767777181317302}
episode index:4600
target Thresh 32.0
target distance 9.0
model initialize at round 4600
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([11.24226314,  5.64341468,  2.1581161 ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 9.007962121199725}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.931881901816264
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.69528885, 10.1401382 ,  2.60480903]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.1057978547620533}
episode index:4601
target Thresh 32.0
target distance 15.0
model initialize at round 4601
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([18.44740153,  6.37758823,  3.97146463]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 16.124171441209725}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9318799163300866
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.58876248, 10.34542392,  2.95334256]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.8804039410705679}
episode index:4602
target Thresh 32.0
target distance 22.0
model initialize at round 4602
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([25.48812564,  7.26065975,  2.58641958]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 20.826568553073884}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9318739424345137
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.84077092, 10.47607462,  3.02297205]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.9906530890808971}
episode index:4603
target Thresh 32.0
target distance 6.0
model initialize at round 4603
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 17.]), 'previousTarget': array([ 7., 17.]), 'currentState': array([11.81459351, 23.19465457,  3.36230886]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 7.845639295236341}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9318801809374602
{'scaleFactor': 20, 'currentTarget': array([ 7., 17.]), 'previousTarget': array([ 7., 17.]), 'currentState': array([ 7.09390997, 16.92770458,  4.08002085]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 0.11851459713548476}
episode index:4604
target Thresh 32.0
target distance 19.0
model initialize at round 4604
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([23.15700135, 26.32436917,  3.79581308]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 21.16427293880333}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9318742095789523
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([11.93281317,  9.84676851,  4.16067014]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 1.259824323388912}
episode index:4605
target Thresh 32.0
target distance 23.0
model initialize at round 4605
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([23.00966575,  2.91118787,  3.03272888]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 21.11324681572165}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9318662773263859
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.28315231, 5.37898339, 2.73414467]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.8108631365564102}
episode index:4606
target Thresh 32.0
target distance 21.0
model initialize at round 4606
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 25.]), 'previousTarget': array([ 4., 25.]), 'currentState': array([4.01888469, 5.91199167, 2.08312106]), 'targetState': array([ 4, 25], dtype=int32), 'currentDistance': 19.08801766857936}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9318603115781077
{'scaleFactor': 20, 'currentTarget': array([ 4., 25.]), 'previousTarget': array([ 4., 25.]), 'currentState': array([ 4.29293044, 25.36570676,  1.05272558]), 'targetState': array([ 4, 25], dtype=int32), 'currentDistance': 0.4685612830664084}
episode index:4607
target Thresh 32.0
target distance 16.0
model initialize at round 4607
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  4.]), 'previousTarget': array([10.,  4.]), 'currentState': array([ 9.54077419, 19.32294537,  4.85916031]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 15.329825280591903}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9318583333625803
{'scaleFactor': 20, 'currentTarget': array([10.,  4.]), 'previousTarget': array([10.,  4.]), 'currentState': array([9.66945784, 3.63015514, 4.44164212]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 0.496027557813816}
episode index:4608
target Thresh 32.0
target distance 4.0
model initialize at round 4608
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 23.]), 'previousTarget': array([25., 23.]), 'currentState': array([25.91182591, 25.4524978 ,  4.6797781 ]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 2.6165190564786727}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9318709481741744
{'scaleFactor': 20, 'currentTarget': array([25., 23.]), 'previousTarget': array([25., 23.]), 'currentState': array([25.28884597, 23.58175389,  4.09624207]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 0.64951488401136}
episode index:4609
target Thresh 32.0
target distance 20.0
model initialize at round 4609
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([20.06800405, 10.4848951 ,  3.36245747]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 18.616311517246213}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9318669668941981
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.62540158, 6.17161963, 3.32772043]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.648521726444056}
episode index:4610
target Thresh 32.0
target distance 14.0
model initialize at round 4610
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 13.]), 'previousTarget': array([ 7., 13.]), 'currentState': array([19.22890742, 12.63100092,  2.57131267]), 'targetState': array([ 7, 13], dtype=int32), 'currentDistance': 12.234473304703688}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9318690517310029
{'scaleFactor': 20, 'currentTarget': array([ 7., 13.]), 'previousTarget': array([ 7., 13.]), 'currentState': array([ 7.47983668, 12.71862765,  2.68799955]), 'targetState': array([ 7, 13], dtype=int32), 'currentDistance': 0.5562496203455902}
episode index:4611
target Thresh 32.0
target distance 11.0
model initialize at round 4611
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([18.76110949, 17.58347762,  4.60056007]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 9.973302805231205}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9318731976542832
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.32485393,  8.04521314,  3.97669607]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.3279852149191028}
episode index:4612
target Thresh 32.0
target distance 9.0
model initialize at round 4612
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'previousTarget': array([11., 19.]), 'currentState': array([18.03833187, 19.99086099,  3.48966557]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 7.107736701063172}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9318794241473128
{'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'previousTarget': array([11., 19.]), 'currentState': array([10.26916449, 19.4976352 ,  2.92608164]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 0.8841726836223637}
episode index:4613
target Thresh 32.0
target distance 19.0
model initialize at round 4613
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([ 2.92927071, 26.68148311,  0.60283518]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 23.47074126979655}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9318695640362528
{'scaleFactor': 20, 'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([21.88041968, 13.15896423,  6.08285538]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 0.1989197821149298}
episode index:4614
target Thresh 32.0
target distance 5.0
model initialize at round 4614
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  8.]), 'previousTarget': array([11.,  8.]), 'currentState': array([12.46810769, 12.17717079,  4.76233983]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 4.427651294932266}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9318800148349448
{'scaleFactor': 20, 'currentTarget': array([11.,  8.]), 'previousTarget': array([11.,  8.]), 'currentState': array([11.40531444,  8.40250165,  4.80175877]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 0.5712156949602268}
episode index:4615
target Thresh 32.0
target distance 11.0
model initialize at round 4615
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 11.]), 'previousTarget': array([25., 11.]), 'currentState': array([15.14467504, 19.46778116,  5.87471104]), 'targetState': array([25, 11], dtype=int32), 'currentDistance': 12.993488667612358}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9318800549850904
{'scaleFactor': 20, 'currentTarget': array([25., 11.]), 'previousTarget': array([25., 11.]), 'currentState': array([24.99219961, 10.47273915,  4.08987662]), 'targetState': array([25, 11], dtype=int32), 'currentDistance': 0.5273185518569679}
episode index:4616
target Thresh 32.0
target distance 7.0
model initialize at round 4616
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 22.]), 'previousTarget': array([ 7., 22.]), 'currentState': array([14.26869231, 22.66142446,  2.42039138]), 'targetState': array([ 7, 22], dtype=int32), 'currentDistance': 7.298723877453923}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.931886274598479
{'scaleFactor': 20, 'currentTarget': array([ 7., 22.]), 'previousTarget': array([ 7., 22.]), 'currentState': array([ 6.71660656, 21.91648834,  2.50910446]), 'targetState': array([ 7, 22], dtype=int32), 'currentDistance': 0.29544210502842994}
episode index:4617
target Thresh 32.0
target distance 11.0
model initialize at round 4617
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 26.]), 'previousTarget': array([ 5., 26.]), 'currentState': array([11.15252699, 16.77478354,  2.33614308]), 'targetState': array([ 5, 26], dtype=int32), 'currentDistance': 11.088652174677183}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9318883520941054
{'scaleFactor': 20, 'currentTarget': array([ 5., 26.]), 'previousTarget': array([ 5., 26.]), 'currentState': array([ 5.00467233, 26.8310238 ,  1.83832724]), 'targetState': array([ 5, 26], dtype=int32), 'currentDistance': 0.8310369344168816}
episode index:4618
target Thresh 32.0
target distance 15.0
model initialize at round 4618
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 27.]), 'previousTarget': array([ 5., 27.]), 'currentState': array([18.66886336, 13.03057328,  1.47358537]), 'targetState': array([ 5, 27], dtype=int32), 'currentDistance': 19.54437792842353}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9318823970655091
{'scaleFactor': 20, 'currentTarget': array([ 5., 27.]), 'previousTarget': array([ 5., 27.]), 'currentState': array([ 4.78557673, 27.03238576,  2.26739888]), 'targetState': array([ 5, 27], dtype=int32), 'currentDistance': 0.21685520030163566}
episode index:4619
target Thresh 32.0
target distance 8.0
model initialize at round 4619
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  4.]), 'previousTarget': array([12.,  4.]), 'currentState': array([12.56888003, 10.06830747,  4.74540639]), 'targetState': array([12,  4], dtype=int32), 'currentDistance': 6.094914280289389}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9318907123475295
{'scaleFactor': 20, 'currentTarget': array([12.,  4.]), 'previousTarget': array([12.,  4.]), 'currentState': array([12.15794858,  4.08673188,  4.58982447]), 'targetState': array([12,  4], dtype=int32), 'currentDistance': 0.1801948197178705}
episode index:4620
target Thresh 32.0
target distance 15.0
model initialize at round 4620
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 10.]), 'previousTarget': array([26., 10.]), 'currentState': array([11.15379354, 21.67592829,  0.46928644]), 'targetState': array([26, 10], dtype=int32), 'currentDistance': 18.887486539321053}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9318847593855432
{'scaleFactor': 20, 'currentTarget': array([26., 10.]), 'previousTarget': array([26., 10.]), 'currentState': array([26.22364629,  9.64002403,  5.5121567 ]), 'targetState': array([26, 10], dtype=int32), 'currentDistance': 0.42379283240033355}
episode index:4621
target Thresh 32.0
target distance 17.0
model initialize at round 4621
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 13.]), 'previousTarget': array([ 6., 13.]), 'currentState': array([21.26185751, 16.94063123,  4.08764434]), 'targetState': array([ 6, 13], dtype=int32), 'currentDistance': 15.762387798396045}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9318827818725709
{'scaleFactor': 20, 'currentTarget': array([ 6., 13.]), 'previousTarget': array([ 6., 13.]), 'currentState': array([ 6.08123957, 13.01233734,  3.59373355]), 'targetState': array([ 6, 13], dtype=int32), 'currentDistance': 0.08217103073287911}
episode index:4622
target Thresh 32.0
target distance 18.0
model initialize at round 4622
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 12.]), 'previousTarget': array([26., 12.]), 'currentState': array([ 9.59361847, 20.45890841,  4.94586826]), 'targetState': array([26, 12], dtype=int32), 'currentDistance': 18.458669684245738}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9318788092283163
{'scaleFactor': 20, 'currentTarget': array([26., 12.]), 'previousTarget': array([26., 12.]), 'currentState': array([25.31739945, 12.53597213,  0.0372481 ]), 'targetState': array([26, 12], dtype=int32), 'currentDistance': 0.8678765106212594}
episode index:4623
target Thresh 32.0
target distance 3.0
model initialize at round 4623
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 21.]), 'previousTarget': array([ 5., 21.]), 'currentState': array([ 3.537713  , 22.68397866,  5.69170863]), 'targetState': array([ 5, 21], dtype=int32), 'currentDistance': 2.230261727013393}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9318913786899883
{'scaleFactor': 20, 'currentTarget': array([ 5., 21.]), 'previousTarget': array([ 5., 21.]), 'currentState': array([ 4.84745206, 21.20191182,  5.17540652]), 'targetState': array([ 5, 21], dtype=int32), 'currentDistance': 0.25305978312234545}
episode index:4624
target Thresh 32.0
target distance 5.0
model initialize at round 4624
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 26.]), 'previousTarget': array([27., 26.]), 'currentState': array([28.68260845, 20.96406338,  0.98860842]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 5.3095978079701505}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9318996830405417
{'scaleFactor': 20, 'currentTarget': array([27., 26.]), 'previousTarget': array([27., 26.]), 'currentState': array([27.056047  , 26.47204764,  1.95895359]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 0.4753632743777537}
episode index:4625
target Thresh 32.0
target distance 3.0
model initialize at round 4625
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 29.]), 'previousTarget': array([25., 29.]), 'currentState': array([25.49165291, 27.61131719,  2.28181416]), 'targetState': array([25, 29], dtype=int32), 'currentDistance': 1.4731471553315818}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9319122425556648
{'scaleFactor': 20, 'currentTarget': array([25., 29.]), 'previousTarget': array([25., 29.]), 'currentState': array([24.63680921, 29.39331582,  1.7490201 ]), 'targetState': array([25, 29], dtype=int32), 'currentDistance': 0.5353549186778466}
episode index:4626
target Thresh 32.0
target distance 17.0
model initialize at round 4626
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 13.]), 'previousTarget': array([ 4., 13.]), 'currentState': array([19.19786964,  7.76053486,  3.11027688]), 'targetState': array([ 4, 13], dtype=int32), 'currentDistance': 16.075672198494292}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9319102612398819
{'scaleFactor': 20, 'currentTarget': array([ 4., 13.]), 'previousTarget': array([ 4., 13.]), 'currentState': array([ 4.20618656, 12.73693318,  2.8653498 ]), 'targetState': array([ 4, 13], dtype=int32), 'currentDistance': 0.33424101011129254}
episode index:4627
target Thresh 32.0
target distance 8.0
model initialize at round 4627
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 16.]), 'previousTarget': array([ 8., 16.]), 'currentState': array([10.28193372, 22.90957164,  4.56833598]), 'targetState': array([ 8, 16], dtype=int32), 'currentDistance': 7.276633958089415}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9319164595434168
{'scaleFactor': 20, 'currentTarget': array([ 8., 16.]), 'previousTarget': array([ 8., 16.]), 'currentState': array([ 7.80856338, 15.34173172,  4.12604338]), 'targetState': array([ 8, 16], dtype=int32), 'currentDistance': 0.6855400143283702}
episode index:4628
target Thresh 32.0
target distance 3.0
model initialize at round 4628
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.06907288,  5.59794337,  3.11625385]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 4.402598507119827}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9319247512998342
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.42188029, 10.42888729,  0.10936379]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.6016039226339075}
episode index:4629
target Thresh 32.0
target distance 9.0
model initialize at round 4629
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 16.]), 'previousTarget': array([10., 16.]), 'currentState': array([19.37131213, 13.64149791,  2.35833693]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 9.663540925841097}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9319288690749098
{'scaleFactor': 20, 'currentTarget': array([10., 16.]), 'previousTarget': array([10., 16.]), 'currentState': array([ 9.96181862, 16.13734025,  2.76819745]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 0.14254881038439046}
episode index:4630
target Thresh 32.0
target distance 8.0
model initialize at round 4630
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 17.]), 'previousTarget': array([14., 17.]), 'currentState': array([ 6.65342297, 23.22652399,  5.65046096]), 'targetState': array([14, 17], dtype=int32), 'currentDistance': 9.630254156309281}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9319329850716329
{'scaleFactor': 20, 'currentTarget': array([14., 17.]), 'previousTarget': array([14., 17.]), 'currentState': array([13.9818491 , 16.57094577,  5.63425401]), 'targetState': array([14, 17], dtype=int32), 'currentDistance': 0.42943799280733014}
episode index:4631
target Thresh 32.0
target distance 2.0
model initialize at round 4631
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 19.]), 'previousTarget': array([ 3., 19.]), 'currentState': array([ 2.93734983, 18.5810522 ,  1.61937368]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 0.4236063043502348}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9319476800230423
{'scaleFactor': 20, 'currentTarget': array([ 3., 19.]), 'previousTarget': array([ 3., 19.]), 'currentState': array([ 2.93734983, 18.5810522 ,  1.61937368]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 0.4236063043502348}
episode index:4632
target Thresh 32.0
target distance 13.0
model initialize at round 4632
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([9.68234503, 3.95413852, 0.9827463 ]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 13.331671988353827}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9319477054208157
{'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([21.33199271, 11.41314431,  0.23178091]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 0.5300069664721272}
episode index:4633
target Thresh 32.0
target distance 16.0
model initialize at round 4633
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([ 3.60550814, 21.42972997,  6.09042692]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 19.17365658465518}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.931941756860088
{'scaleFactor': 20, 'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([19.09650659,  9.40481888,  5.29067023]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 0.6029544677766062}
episode index:4634
target Thresh 32.0
target distance 10.0
model initialize at round 4634
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 17.]), 'previousTarget': array([20., 17.]), 'currentState': array([11.88250108, 20.37026853,  5.74612826]), 'targetState': array([20, 17], dtype=int32), 'currentDistance': 8.789340061037917}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9319479390074752
{'scaleFactor': 20, 'currentTarget': array([20., 17.]), 'previousTarget': array([20., 17.]), 'currentState': array([19.22126354, 17.57406412,  5.45193595]), 'targetState': array([20, 17], dtype=int32), 'currentDistance': 0.967460634389818}
episode index:4635
target Thresh 32.0
target distance 20.0
model initialize at round 4635
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 22.]), 'previousTarget': array([27., 22.]), 'currentState': array([ 8.13913825, 14.23885113,  6.10049051]), 'targetState': array([27, 22], dtype=int32), 'currentDistance': 20.39528224064345}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9319419929626092
{'scaleFactor': 20, 'currentTarget': array([27., 22.]), 'previousTarget': array([27., 22.]), 'currentState': array([26.29595957, 22.0150446 ,  6.13790609]), 'targetState': array([27, 22], dtype=int32), 'currentDistance': 0.7042011534368138}
episode index:4636
target Thresh 32.0
target distance 17.0
model initialize at round 4636
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 29.]), 'previousTarget': array([26., 29.]), 'currentState': array([25.07245376, 13.98750564,  1.34927094]), 'targetState': array([26, 29], dtype=int32), 'currentDistance': 15.041121264052101}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9319400095037921
{'scaleFactor': 20, 'currentTarget': array([26., 29.]), 'previousTarget': array([26., 29.]), 'currentState': array([26.16989409, 29.60675013,  0.50149391]), 'targetState': array([26, 29], dtype=int32), 'currentDistance': 0.6300870745164927}
episode index:4637
target Thresh 32.0
target distance 17.0
model initialize at round 4637
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'previousTarget': array([7., 3.]), 'currentState': array([17.56484093, 18.74326171,  4.39624381]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 18.959592641527802}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9319360373688157
{'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'previousTarget': array([7., 3.]), 'currentState': array([7.98066004, 3.71319641, 3.59285541]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 1.2125771028058103}
episode index:4638
target Thresh 32.0
target distance 23.0
model initialize at round 4638
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  3.]), 'previousTarget': array([20.,  3.]), 'currentState': array([ 2.58343346, 24.42139508,  6.07640648]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 27.60820452860401}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9319224158502557
{'scaleFactor': 20, 'currentTarget': array([20.,  3.]), 'previousTarget': array([20.,  3.]), 'currentState': array([20.06857542,  3.12896976,  5.26951662]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 0.14606775192509272}
episode index:4639
target Thresh 32.0
target distance 19.0
model initialize at round 4639
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 29.]), 'previousTarget': array([27., 29.]), 'currentState': array([ 7.5224832 , 29.61380476,  0.84848285]), 'targetState': array([27, 29], dtype=int32), 'currentDistance': 19.487185967130515}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9319164804319708
{'scaleFactor': 20, 'currentTarget': array([27., 29.]), 'previousTarget': array([27., 29.]), 'currentState': array([27.05100157, 29.09042526,  6.00364693]), 'targetState': array([27, 29], dtype=int32), 'currentDistance': 0.10381660794886477}
episode index:4640
target Thresh 32.0
target distance 18.0
model initialize at round 4640
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 27.]), 'previousTarget': array([23., 27.]), 'currentState': array([ 6.10359036, 27.72937952,  0.15436571]), 'targetState': array([23, 27], dtype=int32), 'currentDistance': 16.91214513573607}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9319145041798691
{'scaleFactor': 20, 'currentTarget': array([23., 27.]), 'previousTarget': array([23., 27.]), 'currentState': array([22.045207  , 27.51483269,  0.08163595]), 'targetState': array([23, 27], dtype=int32), 'currentDistance': 1.0847499114467651}
episode index:4641
target Thresh 32.0
target distance 21.0
model initialize at round 4641
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([ 3.28678566, 10.65835523,  0.38955617]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 20.756619639968875}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9319085730232186
{'scaleFactor': 20, 'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([23.11702633, 12.305835  ,  0.27077983]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 0.9344396985949123}
episode index:4642
target Thresh 32.0
target distance 24.0
model initialize at round 4642
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 29.]), 'previousTarget': array([13., 29.]), 'currentState': array([19.17111697,  6.46469827,  1.07577014]), 'targetState': array([13, 29], dtype=int32), 'currentDistance': 23.36498467632528}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9318987682200081
{'scaleFactor': 20, 'currentTarget': array([13., 29.]), 'previousTarget': array([13., 29.]), 'currentState': array([13.02643439, 29.39907318,  1.72009605]), 'targetState': array([13, 29], dtype=int32), 'currentDistance': 0.39994772616631546}
episode index:4643
target Thresh 32.0
target distance 13.0
model initialize at round 4643
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 13.]), 'previousTarget': array([12., 13.]), 'currentState': array([ 8.6158856 , 24.1206415 ,  4.76739103]), 'targetState': array([12, 13], dtype=int32), 'currentDistance': 11.624151478482633}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9319008313942503
{'scaleFactor': 20, 'currentTarget': array([12., 13.]), 'previousTarget': array([12., 13.]), 'currentState': array([12.34147121, 12.80304864,  4.68177314]), 'targetState': array([12, 13], dtype=int32), 'currentDistance': 0.39419845937816705}
episode index:4644
target Thresh 32.0
target distance 16.0
model initialize at round 4644
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 21.]), 'previousTarget': array([19., 21.]), 'currentState': array([3.99466129, 6.6931233 , 0.70529175]), 'targetState': array([19, 21], dtype=int32), 'currentDistance': 20.732749713934428}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9318949070118207
{'scaleFactor': 20, 'currentTarget': array([19., 21.]), 'previousTarget': array([19., 21.]), 'currentState': array([18.20552194, 20.69006972,  0.61940705]), 'targetState': array([19, 21], dtype=int32), 'currentDistance': 0.8527908059750653}
episode index:4645
target Thresh 32.0
target distance 9.0
model initialize at round 4645
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 18.]), 'previousTarget': array([ 2., 18.]), 'currentState': array([4.39822176, 8.48357523, 2.44347763]), 'targetState': array([ 2, 18], dtype=int32), 'currentDistance': 9.813959853009948}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9318990170296614
{'scaleFactor': 20, 'currentTarget': array([ 2., 18.]), 'previousTarget': array([ 2., 18.]), 'currentState': array([ 1.71307207, 17.96023579,  1.6988164 ]), 'targetState': array([ 2, 18], dtype=int32), 'currentDistance': 0.2896702077838244}
episode index:4646
target Thresh 32.0
target distance 13.0
model initialize at round 4646
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  6.]), 'previousTarget': array([10.,  6.]), 'currentState': array([11.21432995, 20.16524283,  6.03795785]), 'targetState': array([10,  6], dtype=int32), 'currentDistance': 14.217197390452576}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9318990528228349
{'scaleFactor': 20, 'currentTarget': array([10.,  6.]), 'previousTarget': array([10.,  6.]), 'currentState': array([10.48647043,  6.85740546,  4.18290982]), 'targetState': array([10,  6], dtype=int32), 'currentDistance': 0.9857979494391592}
episode index:4647
target Thresh 32.0
target distance 16.0
model initialize at round 4647
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 29.]), 'previousTarget': array([ 4., 29.]), 'currentState': array([21.20764316, 21.17217204,  1.78049367]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 18.90444058609429}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9318931326468852
{'scaleFactor': 20, 'currentTarget': array([ 4., 29.]), 'previousTarget': array([ 4., 29.]), 'currentState': array([ 3.21200167, 29.03837946,  2.37801868]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 0.7889324079609439}
episode index:4648
target Thresh 32.0
target distance 11.0
model initialize at round 4648
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 14.]), 'previousTarget': array([15., 14.]), 'currentState': array([24.43714182,  5.00303361,  1.90224695]), 'targetState': array([15, 14], dtype=int32), 'currentDistance': 13.038598468044599}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9318951948143953
{'scaleFactor': 20, 'currentTarget': array([15., 14.]), 'previousTarget': array([15., 14.]), 'currentState': array([15.7532832 , 13.00611628,  2.03008932]), 'targetState': array([15, 14], dtype=int32), 'currentDistance': 1.2470927931558917}
episode index:4649
target Thresh 32.0
target distance 16.0
model initialize at round 4649
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 25.]), 'previousTarget': array([ 4., 25.]), 'currentState': array([12.36398853,  9.98586174,  1.6358506 ]), 'targetState': array([ 4, 25], dtype=int32), 'currentDistance': 17.186641669074262}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9318912425676574
{'scaleFactor': 20, 'currentTarget': array([ 4., 25.]), 'previousTarget': array([ 4., 25.]), 'currentState': array([ 3.96895605, 25.74858427,  1.93798413]), 'targetState': array([ 4, 25], dtype=int32), 'currentDistance': 0.7492276912670192}
episode index:4650
target Thresh 32.0
target distance 24.0
model initialize at round 4650
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 29.]), 'previousTarget': array([19., 29.]), 'currentState': array([26.78386124,  6.66903329,  2.70957899]), 'targetState': array([19, 29], dtype=int32), 'currentDistance': 23.648690661695188}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.931881458355477
{'scaleFactor': 20, 'currentTarget': array([19., 29.]), 'previousTarget': array([19., 29.]), 'currentState': array([18.85282111, 28.95852761,  1.53019575]), 'targetState': array([19, 29], dtype=int32), 'currentDistance': 0.15291038120033254}
episode index:4651
target Thresh 32.0
target distance 7.0
model initialize at round 4651
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 24.]), 'previousTarget': array([17., 24.]), 'currentState': array([11.89634087, 23.42400916,  6.08429283]), 'targetState': array([17, 24], dtype=int32), 'currentDistance': 5.136058994409938}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9318897166404392
{'scaleFactor': 20, 'currentTarget': array([17., 24.]), 'previousTarget': array([17., 24.]), 'currentState': array([17.76680117, 23.7033122 ,  6.17887211]), 'targetState': array([17, 24], dtype=int32), 'currentDistance': 0.8221968690635252}
episode index:4652
target Thresh 32.0
target distance 16.0
model initialize at round 4652
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 27.]), 'previousTarget': array([27., 27.]), 'currentState': array([16.36114833, 11.38282278,  1.90211535]), 'targetState': array([27, 27], dtype=int32), 'currentDistance': 18.89659729635251}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9318838048326524
{'scaleFactor': 20, 'currentTarget': array([27., 27.]), 'previousTarget': array([27., 27.]), 'currentState': array([27.58895001, 27.19453445,  0.55615129]), 'targetState': array([27, 27], dtype=int32), 'currentDistance': 0.6202465354874441}
episode index:4653
target Thresh 32.0
target distance 6.0
model initialize at round 4653
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 27.]), 'previousTarget': array([27., 27.]), 'currentState': array([20.05667863, 27.39374778,  1.15579772]), 'targetState': array([27, 27], dtype=int32), 'currentDistance': 6.954476901570234}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9318899741934532
{'scaleFactor': 20, 'currentTarget': array([27., 27.]), 'previousTarget': array([27., 27.]), 'currentState': array([27.45511296, 26.72919093,  6.13519764]), 'targetState': array([27, 27], dtype=int32), 'currentDistance': 0.5295897981451355}
episode index:4654
target Thresh 32.0
target distance 6.0
model initialize at round 4654
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 17.]), 'previousTarget': array([22., 17.]), 'currentState': array([28.11804588, 21.74208049,  4.42898478]), 'targetState': array([22, 17], dtype=int32), 'currentDistance': 7.740659709292032}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9318961409036157
{'scaleFactor': 20, 'currentTarget': array([22., 17.]), 'previousTarget': array([22., 17.]), 'currentState': array([21.92510153, 16.998984  ,  3.85394808]), 'targetState': array([22, 17], dtype=int32), 'currentDistance': 0.0749053590659552}
episode index:4655
target Thresh 32.0
target distance 3.0
model initialize at round 4655
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 12.]), 'previousTarget': array([25., 12.]), 'currentState': array([23.17817054, 13.53132959,  5.21222103]), 'targetState': array([25, 12], dtype=int32), 'currentDistance': 2.379922877398953}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9319086202547961
{'scaleFactor': 20, 'currentTarget': array([25., 12.]), 'previousTarget': array([25., 12.]), 'currentState': array([24.72728431, 12.36526077,  6.07225424]), 'targetState': array([25, 12], dtype=int32), 'currentDistance': 0.45583909239467985}
episode index:4656
target Thresh 32.0
target distance 22.0
model initialize at round 4656
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 25.]), 'previousTarget': array([21., 25.]), 'currentState': array([2.58171434, 4.57923922, 0.20786762]), 'targetState': array([21, 25], dtype=int32), 'currentDistance': 27.49983122611762}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9318950572727291
{'scaleFactor': 20, 'currentTarget': array([21., 25.]), 'previousTarget': array([21., 25.]), 'currentState': array([21.02581563, 25.24454973,  0.56390823]), 'targetState': array([21, 25], dtype=int32), 'currentDistance': 0.24590855764150255}
episode index:4657
target Thresh 32.0
target distance 22.0
model initialize at round 4657
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 24.]), 'previousTarget': array([ 5., 24.]), 'currentState': array([26.63402859, 22.35730313,  3.48318195]), 'targetState': array([ 5, 24], dtype=int32), 'currentDistance': 21.696304889992252}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9318872090968996
{'scaleFactor': 20, 'currentTarget': array([ 5., 24.]), 'previousTarget': array([ 5., 24.]), 'currentState': array([ 4.8485871 , 23.88876563,  2.76221042]), 'targetState': array([ 5, 24], dtype=int32), 'currentDistance': 0.18788015777228897}
episode index:4658
target Thresh 32.0
target distance 11.0
model initialize at round 4658
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([12.99996564,  7.73145736,  2.055566  ]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 10.531169876826725}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.931891309298832
{'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([ 8.00747827, 16.29523072,  2.3757098 ]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 0.704808954683139}
episode index:4659
target Thresh 32.0
target distance 11.0
model initialize at round 4659
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 23.]), 'previousTarget': array([20., 23.]), 'currentState': array([10.68327306, 20.96778063,  0.98983579]), 'targetState': array([20, 23], dtype=int32), 'currentDistance': 9.53579133335095}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.931895407741021
{'scaleFactor': 20, 'currentTarget': array([20., 23.]), 'previousTarget': array([20., 23.]), 'currentState': array([20.06440776, 23.38815328,  6.09088606]), 'targetState': array([20, 23], dtype=int32), 'currentDistance': 0.39346070961622953}
episode index:4660
target Thresh 32.0
target distance 10.0
model initialize at round 4660
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([12.51910361, 19.70087076,  5.63494269]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 11.455523294220157}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.931897464111255
{'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([21.16420859, 11.52237802,  5.47333764]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 0.5050615963310686}
episode index:4661
target Thresh 32.0
target distance 9.0
model initialize at round 4661
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([13.08292698,  3.76577381,  1.87228465]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 7.8637438469713}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9319036199555039
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.71892971, 10.87125111,  1.54217091]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.3091549470944755}
episode index:4662
target Thresh 32.0
target distance 23.0
model initialize at round 4662
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([25.52174507,  9.80445662,  3.65321839]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 22.893122420018294}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9318938582681269
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.03907269, 2.24507273, 3.08958856]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.9916864083513278}
episode index:4663
target Thresh 32.0
target distance 8.0
model initialize at round 4663
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 15.]), 'previousTarget': array([ 5., 15.]), 'currentState': array([7.31723168, 6.97394522, 2.1470747 ]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 8.353868441564348}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9319000122457708
{'scaleFactor': 20, 'currentTarget': array([ 5., 15.]), 'previousTarget': array([ 5., 15.]), 'currentState': array([ 4.95969377, 14.50863625,  2.22012103]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 0.4930141240600682}
episode index:4664
target Thresh 32.0
target distance 14.0
model initialize at round 4664
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 15.]), 'previousTarget': array([19., 15.]), 'currentState': array([22.3212757 , 30.04241   ,  5.94114715]), 'targetState': array([19, 15], dtype=int32), 'currentDistance': 15.404706128880429}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9318980496910403
{'scaleFactor': 20, 'currentTarget': array([19., 15.]), 'previousTarget': array([19., 15.]), 'currentState': array([19.1587335 , 15.05083384,  4.64205364]), 'targetState': array([19, 15], dtype=int32), 'currentDistance': 0.16667454311312685}
episode index:4665
target Thresh 32.0
target distance 13.0
model initialize at round 4665
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 23.]), 'previousTarget': array([10., 23.]), 'currentState': array([19.46046182, 10.95747249,  1.81765425]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 15.314137476605813}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9318960879775249
{'scaleFactor': 20, 'currentTarget': array([10., 23.]), 'previousTarget': array([10., 23.]), 'currentState': array([ 9.44718734, 23.23379368,  1.85845098]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 0.6002177288302724}
episode index:4666
target Thresh 32.0
target distance 18.0
model initialize at round 4666
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  4.]), 'previousTarget': array([22.,  4.]), 'currentState': array([ 4.11092763, 20.5710359 ,  5.16582036]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 24.38479323321119}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9318863362705908
{'scaleFactor': 20, 'currentTarget': array([22.,  4.]), 'previousTarget': array([22.,  4.]), 'currentState': array([21.73443439,  4.54596427,  5.33476501]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 0.6071260837312277}
episode index:4667
target Thresh 32.0
target distance 9.0
model initialize at round 4667
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 26.]), 'previousTarget': array([23., 26.]), 'currentState': array([15.86539706, 28.61547994,  5.53565437]), 'targetState': array([23, 26], dtype=int32), 'currentDistance': 7.598900875263905}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9318924865862998
{'scaleFactor': 20, 'currentTarget': array([23., 26.]), 'previousTarget': array([23., 26.]), 'currentState': array([23.25125525, 26.17000554,  5.36696184]), 'targetState': array([23, 26], dtype=int32), 'currentDistance': 0.303366251034638}
episode index:4668
target Thresh 32.0
target distance 20.0
model initialize at round 4668
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  5.]), 'previousTarget': array([27.,  5.]), 'currentState': array([14.0231451 , 23.62954658,  5.10312605]), 'targetState': array([27,  5], dtype=int32), 'currentDistance': 22.70371707059022}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9318846574510828
{'scaleFactor': 20, 'currentTarget': array([27.,  5.]), 'previousTarget': array([27.,  5.]), 'currentState': array([26.65653575,  5.74020287,  5.72229002]), 'targetState': array([27,  5], dtype=int32), 'currentDistance': 0.8160073394036147}
episode index:4669
target Thresh 32.0
target distance 11.0
model initialize at round 4669
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([22.03000907, 12.62467979,  3.08812571]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 10.66486688119445}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9318887485415429
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([12.68007016,  9.26003329,  3.53824145]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.7280884083761381}
episode index:4670
target Thresh 32.0
target distance 14.0
model initialize at round 4670
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([14.83352114,  9.85443947,  1.00327516]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 12.201447188396585}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9318908019350046
{'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([15.96038976, 21.68166913,  1.284201  ]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 0.32078577573338585}
episode index:4671
target Thresh 32.0
target distance 16.0
model initialize at round 4671
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 17.]), 'previousTarget': array([ 9., 17.]), 'currentState': array([24.87863346, 10.32141183,  3.63021183]), 'targetState': array([ 9, 17], dtype=int32), 'currentDistance': 17.225984458465145}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9318868692392744
{'scaleFactor': 20, 'currentTarget': array([ 9., 17.]), 'previousTarget': array([ 9., 17.]), 'currentState': array([ 8.55072324, 16.88644605,  2.63165989]), 'targetState': array([ 9, 17], dtype=int32), 'currentDistance': 0.46340490684383157}
episode index:4672
target Thresh 32.0
target distance 12.0
model initialize at round 4672
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 22.]), 'previousTarget': array([15., 22.]), 'currentState': array([ 6.5494769 , 11.26456362,  0.68488913]), 'targetState': array([15, 22], dtype=int32), 'currentDistance': 13.662391262845173}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9318869074328691
{'scaleFactor': 20, 'currentTarget': array([15., 22.]), 'previousTarget': array([15., 22.]), 'currentState': array([14.96811543, 22.34931376,  0.79503801]), 'targetState': array([15, 22], dtype=int32), 'currentDistance': 0.35076591255741035}
episode index:4673
target Thresh 32.0
target distance 18.0
model initialize at round 4673
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 2.]), 'previousTarget': array([8., 2.]), 'currentState': array([ 4.46676349, 18.40373971,  5.39999032]), 'targetState': array([8, 2], dtype=int32), 'currentDistance': 16.779941498568313}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9318849514608954
{'scaleFactor': 20, 'currentTarget': array([8., 2.]), 'previousTarget': array([8., 2.]), 'currentState': array([7.73833449, 2.94131616, 5.18997127]), 'targetState': array([8, 2], dtype=int32), 'currentDistance': 0.9770081623747198}
episode index:4674
target Thresh 32.0
target distance 8.0
model initialize at round 4674
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 16.]), 'previousTarget': array([16., 16.]), 'currentState': array([23.25697077, 14.51006479,  3.03806472]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 7.408342039110956}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9318910928637915
{'scaleFactor': 20, 'currentTarget': array([16., 16.]), 'previousTarget': array([16., 16.]), 'currentState': array([15.53010541, 16.35922658,  3.09198563]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 0.5914766817542091}
episode index:4675
target Thresh 32.0
target distance 4.0
model initialize at round 4675
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 19.]), 'previousTarget': array([ 4., 19.]), 'currentState': array([ 4.14139193, 16.44747375,  1.09618378]), 'targetState': array([ 4, 19], dtype=int32), 'currentDistance': 2.5564393055191648}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9319035199183543
{'scaleFactor': 20, 'currentTarget': array([ 4., 19.]), 'previousTarget': array([ 4., 19.]), 'currentState': array([ 4.12146008, 18.36833587,  2.07595891]), 'targetState': array([ 4, 19], dtype=int32), 'currentDistance': 0.6432356674946043}
episode index:4676
target Thresh 32.0
target distance 17.0
model initialize at round 4676
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 27.]), 'previousTarget': array([21., 27.]), 'currentState': array([ 5.90716009, 15.60003604,  0.25938331]), 'targetState': array([21, 27], dtype=int32), 'currentDistance': 18.914359487721637}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9318995887076562
{'scaleFactor': 20, 'currentTarget': array([21., 27.]), 'previousTarget': array([21., 27.]), 'currentState': array([20.06547983, 26.09046891,  1.33545008]), 'targetState': array([21, 27], dtype=int32), 'currentDistance': 1.3040608672884315}
episode index:4677
target Thresh 32.0
target distance 6.0
model initialize at round 4677
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([12.1267463 , 11.39655343,  3.447047  ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 4.1457556187436655}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9319098923441017
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.22699049, 10.97182595,  3.69509959]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.22873228515634997}
episode index:4678
target Thresh 32.0
target distance 9.0
model initialize at round 4678
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([25.49509517,  8.22729129,  4.79616535]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 10.56661093299652}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9319119377078668
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.31385511,  7.32933831,  3.06229189]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.7610903597190296}
episode index:4679
target Thresh 32.0
target distance 5.0
model initialize at round 4679
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 24.]), 'previousTarget': array([ 2., 24.]), 'currentState': array([ 6.95710821, 24.68242337,  2.60628486]), 'targetState': array([ 2, 24], dtype=int32), 'currentDistance': 5.003860857588181}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9319201400716044
{'scaleFactor': 20, 'currentTarget': array([ 2., 24.]), 'previousTarget': array([ 2., 24.]), 'currentState': array([ 1.13430141, 24.20908989,  3.24395409]), 'targetState': array([ 2, 24], dtype=int32), 'currentDistance': 0.8905911685328204}
episode index:4680
target Thresh 32.0
target distance 21.0
model initialize at round 4680
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'previousTarget': array([25.,  3.]), 'currentState': array([ 5.2530524 , 14.69991776,  6.26230967]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 22.952777938292318}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9319104123919729
{'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'previousTarget': array([25.,  3.]), 'currentState': array([25.50062362,  2.99232929,  5.3426936 ]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 0.500682384267029}
episode index:4681
target Thresh 32.0
target distance 18.0
model initialize at round 4681
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 25.]), 'previousTarget': array([ 5., 25.]), 'currentState': array([12.55437091,  7.64522799,  1.4034602 ]), 'targetState': array([ 5, 25], dtype=int32), 'currentDistance': 18.927668406578846}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9319045327812545
{'scaleFactor': 20, 'currentTarget': array([ 5., 25.]), 'previousTarget': array([ 5., 25.]), 'currentState': array([ 4.90939089, 25.91705205,  1.83333006]), 'targetState': array([ 5, 25], dtype=int32), 'currentDistance': 0.9215174841095655}
episode index:4682
target Thresh 32.0
target distance 20.0
model initialize at round 4682
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 23.]), 'previousTarget': array([15., 23.]), 'currentState': array([13.19589001,  4.47844351,  1.05893612]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 18.609214590614855}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9319006063910564
{'scaleFactor': 20, 'currentTarget': array([15., 23.]), 'previousTarget': array([15., 23.]), 'currentState': array([14.70884921, 22.15707109,  1.82987499]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 0.8917947748709062}
episode index:4683
target Thresh 32.0
target distance 11.0
model initialize at round 4683
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 15.]), 'previousTarget': array([13., 15.]), 'currentState': array([0.42562926, 5.59476453, 1.77038765]), 'targetState': array([13, 15], dtype=int32), 'currentDistance': 15.702651174750393}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9318986516703128
{'scaleFactor': 20, 'currentTarget': array([13., 15.]), 'previousTarget': array([13., 15.]), 'currentState': array([12.68338254, 14.76341836,  1.00663358]), 'targetState': array([13, 15], dtype=int32), 'currentDistance': 0.3952435796667733}
episode index:4684
target Thresh 32.0
target distance 11.0
model initialize at round 4684
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  3.]), 'previousTarget': array([10.,  3.]), 'currentState': array([11.15279914, 12.77385065,  4.45696387]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 9.841600601361485}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9319027266752711
{'scaleFactor': 20, 'currentTarget': array([10.,  3.]), 'previousTarget': array([10.,  3.]), 'currentState': array([9.80513921, 2.99455259, 4.79218609]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 0.19493691733364407}
episode index:4685
target Thresh 32.0
target distance 11.0
model initialize at round 4685
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([25.44672008, 19.00448969,  3.01434422]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 10.260431667415125}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9319067999410039
{'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([16.3781388 , 15.21515936,  4.05320151]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 0.43506608618327386}
episode index:4686
target Thresh 32.0
target distance 24.0
model initialize at round 4686
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 27.]), 'previousTarget': array([15., 27.]), 'currentState': array([21.37059621,  2.57877397,  2.38456964]), 'targetState': array([15, 27], dtype=int32), 'currentDistance': 25.238478097301943}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9318951964042124
{'scaleFactor': 20, 'currentTarget': array([15., 27.]), 'previousTarget': array([15., 27.]), 'currentState': array([15.32447256, 27.56127186,  1.60282178]), 'targetState': array([15, 27], dtype=int32), 'currentDistance': 0.6483120745081732}
episode index:4687
target Thresh 32.0
target distance 12.0
model initialize at round 4687
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  7.]), 'previousTarget': array([24.,  7.]), 'currentState': array([21.41402349, 18.0873524 ,  4.70003224]), 'targetState': array([24,  7], dtype=int32), 'currentDistance': 11.384931167378785}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9318972409760974
{'scaleFactor': 20, 'currentTarget': array([24.,  7.]), 'previousTarget': array([24.,  7.]), 'currentState': array([24.21177397,  6.60694967,  4.51865015]), 'targetState': array([24,  7], dtype=int32), 'currentDistance': 0.4464714712430088}
episode index:4688
target Thresh 32.0
target distance 19.0
model initialize at round 4688
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 29.]), 'previousTarget': array([13., 29.]), 'currentState': array([16.67245252,  9.81214192,  0.89814394]), 'targetState': array([13, 29], dtype=int32), 'currentDistance': 19.536140998556604}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.931891372951792
{'scaleFactor': 20, 'currentTarget': array([13., 29.]), 'previousTarget': array([13., 29.]), 'currentState': array([13.13576472, 29.16873945,  1.92036127]), 'targetState': array([13, 29], dtype=int32), 'currentDistance': 0.21657576380508414}
episode index:4689
target Thresh 32.0
target distance 13.0
model initialize at round 4689
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([20.33907771, 26.2060408 ,  2.8811779 ]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 11.55167950482028}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9318934174670265
{'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([ 8.72971949, 23.88818789,  2.91989125]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 0.29249530642423516}
episode index:4690
target Thresh 32.0
target distance 10.0
model initialize at round 4690
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'previousTarget': array([ 5., 22.]), 'currentState': array([13.39684715, 13.51214159,  1.82238054]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 11.939463235297776}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9318954611105853
{'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'previousTarget': array([ 5., 22.]), 'currentState': array([ 5.05388781, 21.86722141,  1.90573224]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 0.14329707146804282}
episode index:4691
target Thresh 32.0
target distance 13.0
model initialize at round 4691
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 13.]), 'previousTarget': array([12., 13.]), 'currentState': array([23.89490604, 20.73068702,  2.98604298]), 'targetState': array([12, 13], dtype=int32), 'currentDistance': 14.18634243327166}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9318954973183424
{'scaleFactor': 20, 'currentTarget': array([12., 13.]), 'previousTarget': array([12., 13.]), 'currentState': array([12.49462538, 13.20746822,  4.08214167]), 'targetState': array([12, 13], dtype=int32), 'currentDistance': 0.5363742444330237}
episode index:4692
target Thresh 32.0
target distance 15.0
model initialize at round 4692
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([ 2.63070403, 10.97847669,  1.51114583]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 14.420654332681572}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.931895533510669
{'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([ 5.63363276, 24.42252489,  0.86003335]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 0.6838877479784581}
episode index:4693
target Thresh 32.0
target distance 10.0
model initialize at round 4693
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'previousTarget': array([10., 12.]), 'currentState': array([20.18719247,  8.32747285,  3.81384706]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 10.828958682327134}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9318975753973096
{'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'previousTarget': array([10., 12.]), 'currentState': array([ 9.64505319, 12.78996969,  2.41184075]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 0.8660481244460421}
episode index:4694
target Thresh 32.0
target distance 4.0
model initialize at round 4694
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 14.]), 'previousTarget': array([18., 14.]), 'currentState': array([14.07629837, 12.68123962,  0.51544523]), 'targetState': array([18, 14], dtype=int32), 'currentDistance': 4.13939167266053}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9319078421544134
{'scaleFactor': 20, 'currentTarget': array([18., 14.]), 'previousTarget': array([18., 14.]), 'currentState': array([17.86558158, 13.80684197,  0.59063137]), 'targetState': array([18, 14], dtype=int32), 'currentDistance': 0.2353260240387383}
episode index:4695
target Thresh 32.0
target distance 6.0
model initialize at round 4695
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 2.]), 'previousTarget': array([8., 2.]), 'currentState': array([13.73801073,  5.33754708,  3.5460825 ]), 'targetState': array([8, 2], dtype=int32), 'currentDistance': 6.638071080066148}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9319160174435627
{'scaleFactor': 20, 'currentTarget': array([8., 2.]), 'previousTarget': array([8., 2.]), 'currentState': array([8.5939118 , 2.40174534, 3.99142328]), 'targetState': array([8, 2], dtype=int32), 'currentDistance': 0.7170289728666246}
episode index:4696
target Thresh 32.0
target distance 22.0
model initialize at round 4696
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  7.]), 'previousTarget': array([17.,  7.]), 'currentState': array([16.08978675, 27.33819014,  4.78341436]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 20.358547750061618}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9319101554162187
{'scaleFactor': 20, 'currentTarget': array([17.,  7.]), 'previousTarget': array([17.,  7.]), 'currentState': array([16.84816267,  7.4189398 ,  4.40473079]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 0.4456064745177793}
episode index:4697
target Thresh 32.0
target distance 25.0
model initialize at round 4697
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 29.]), 'previousTarget': array([14., 29.]), 'currentState': array([16.33251973,  5.88987433,  1.89790678]), 'targetState': array([14, 29], dtype=int32), 'currentDistance': 23.227538758905343}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9319004650620892
{'scaleFactor': 20, 'currentTarget': array([14., 29.]), 'previousTarget': array([14., 29.]), 'currentState': array([14.04515429, 29.63291767,  1.32409937]), 'targetState': array([14, 29], dtype=int32), 'currentDistance': 0.6345263460976738}
episode index:4698
target Thresh 32.0
target distance 15.0
model initialize at round 4698
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([24.13560892,  6.95952905,  2.51340652]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 13.482903446732116}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9319005001510114
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([10.80976848, 10.43275387,  2.85863748]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.47271972744284085}
episode index:4699
target Thresh 32.0
target distance 16.0
model initialize at round 4699
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  7.]), 'previousTarget': array([20.,  7.]), 'currentState': array([22.52640907, 21.40147519,  4.02051353]), 'targetState': array([20,  7], dtype=int32), 'currentDistance': 14.621396321406603}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.931900535225002
{'scaleFactor': 20, 'currentTarget': array([20.,  7.]), 'previousTarget': array([20.,  7.]), 'currentState': array([20.117611  ,  7.81146771,  4.92946485]), 'targetState': array([20,  7], dtype=int32), 'currentDistance': 0.8199464607729163}
episode index:4700
target Thresh 32.0
target distance 9.0
model initialize at round 4700
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 14.]), 'previousTarget': array([17., 14.]), 'currentState': array([27.03405994, 10.32828069,  1.91869777]), 'targetState': array([17, 14], dtype=int32), 'currentDistance': 10.684749953584879}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9319045959598828
{'scaleFactor': 20, 'currentTarget': array([17., 14.]), 'previousTarget': array([17., 14.]), 'currentState': array([17.98238879, 13.72996824,  2.49523015]), 'targetState': array([17, 14], dtype=int32), 'currentDistance': 1.0188252451727455}
episode index:4701
target Thresh 32.0
target distance 15.0
model initialize at round 4701
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  2.]), 'previousTarget': array([11.,  2.]), 'currentState': array([24.65501726, 17.01163705,  3.50671303]), 'targetState': array([11,  2], dtype=int32), 'currentDistance': 20.293071310405832}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9318987425951548
{'scaleFactor': 20, 'currentTarget': array([11.,  2.]), 'previousTarget': array([11.,  2.]), 'currentState': array([11.25000727,  2.35379236,  3.76898709]), 'targetState': array([11,  2], dtype=int32), 'currentDistance': 0.43321203614037707}
episode index:4702
target Thresh 32.0
target distance 11.0
model initialize at round 4702
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([23.22986174, 21.49642082,  3.05609787]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 14.656906940659496}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9318987780204816
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.790863  , 11.60481936,  4.23175045]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.9956258103727973}
episode index:4703
target Thresh 32.0
target distance 3.0
model initialize at round 4703
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 13.]), 'previousTarget': array([19., 13.]), 'currentState': array([17.2835673 , 12.38844889,  0.2418294 ]), 'targetState': array([19, 13], dtype=int32), 'currentDistance': 1.8221240270541421}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9319111294707323
{'scaleFactor': 20, 'currentTarget': array([19., 13.]), 'previousTarget': array([19., 13.]), 'currentState': array([19.21229828, 12.91675792,  0.29340141]), 'targetState': array([19, 13], dtype=int32), 'currentDistance': 0.22803465151647934}
episode index:4704
target Thresh 32.0
target distance 5.0
model initialize at round 4704
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([23.99086973, 20.06231863,  0.18937522]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 3.0097755080755193}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9319213715261051
{'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([27.87565804, 19.35068145,  5.98797388]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 1.0901337493810497}
episode index:4705
target Thresh 32.0
target distance 8.0
model initialize at round 4705
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 21.]), 'previousTarget': array([27., 21.]), 'currentState': array([20.67696032, 13.85790079,  0.92546576]), 'targetState': array([27, 21], dtype=int32), 'currentDistance': 9.538889453297433}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9319254235189597
{'scaleFactor': 20, 'currentTarget': array([27., 21.]), 'previousTarget': array([27., 21.]), 'currentState': array([27.52189303, 21.09804139,  0.71422522]), 'targetState': array([27, 21], dtype=int32), 'currentDistance': 0.5310220781391075}
episode index:4706
target Thresh 32.0
target distance 20.0
model initialize at round 4706
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([25.2191983 , 21.49088457,  3.06323707]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 19.533659627443683}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9319195719471495
{'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([ 5.56740641, 18.11004098,  3.17905796]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 0.4463700631650819}
episode index:4707
target Thresh 32.0
target distance 12.0
model initialize at round 4707
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 17.]), 'previousTarget': array([14., 17.]), 'currentState': array([16.77405106,  6.75759314,  1.65223622]), 'targetState': array([14, 17], dtype=int32), 'currentDistance': 10.61142108872253}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9319236226009202
{'scaleFactor': 20, 'currentTarget': array([14., 17.]), 'previousTarget': array([14., 17.]), 'currentState': array([14.09004952, 16.33507572,  2.11192484]), 'targetState': array([14, 17], dtype=int32), 'currentDistance': 0.6709941992295568}
episode index:4708
target Thresh 32.0
target distance 5.0
model initialize at round 4708
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 22.]), 'previousTarget': array([20., 22.]), 'currentState': array([16.68731563, 19.03843437,  0.79153863]), 'targetState': array([20, 22], dtype=int32), 'currentDistance': 4.443506355075361}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9319338533032772
{'scaleFactor': 20, 'currentTarget': array([20., 22.]), 'previousTarget': array([20., 22.]), 'currentState': array([19.5982793 , 21.7244111 ,  1.08938365]), 'targetState': array([20, 22], dtype=int32), 'currentDistance': 0.48716399853700976}
episode index:4709
target Thresh 32.0
target distance 14.0
model initialize at round 4709
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([13.30729551, 24.70992201,  4.78842545]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 14.85664228356294}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9319338812214522
{'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([20.60948116, 12.90725612,  5.49904056]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 0.9877340919059043}
episode index:4710
target Thresh 32.0
target distance 15.0
model initialize at round 4710
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 26.]), 'previousTarget': array([23., 26.]), 'currentState': array([17.4788273 , 12.82996534,  0.734025  ]), 'targetState': array([23, 26], dtype=int32), 'currentDistance': 14.280516835939743}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9319339091277747
{'scaleFactor': 20, 'currentTarget': array([23., 26.]), 'previousTarget': array([23., 26.]), 'currentState': array([22.74158549, 25.66549861,  1.44100625]), 'targetState': array([23, 26], dtype=int32), 'currentDistance': 0.4226928444440271}
episode index:4711
target Thresh 32.0
target distance 10.0
model initialize at round 4711
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([11.38075277,  3.53331121,  2.41587698]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 8.519862686030248}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9319399919165845
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.87418805, 2.15120396, 3.73179793]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.8871681866518237}
episode index:4712
target Thresh 32.0
target distance 8.0
model initialize at round 4712
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([21.36909098, 24.15955292,  3.97235233]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 6.599445755096598}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9319481308955965
{'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([19.24626317, 18.63521892,  4.72334251]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 0.6812845421538154}
episode index:4713
target Thresh 32.0
target distance 12.0
model initialize at round 4713
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 17.]), 'previousTarget': array([13., 17.]), 'currentState': array([23.95917091, 26.67747859,  3.03562307]), 'targetState': array([13, 17], dtype=int32), 'currentDistance': 14.620431553446624}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9319481557613181
{'scaleFactor': 20, 'currentTarget': array([13., 17.]), 'previousTarget': array([13., 17.]), 'currentState': array([13.73678606, 17.5899551 ,  4.22367045]), 'targetState': array([13, 17], dtype=int32), 'currentDistance': 0.943875375083681}
episode index:4714
target Thresh 32.0
target distance 14.0
model initialize at round 4714
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  4.]), 'previousTarget': array([22.,  4.]), 'currentState': array([9.64917344, 6.33558168, 5.47392935]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 12.569719907771297}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.931950177393055
{'scaleFactor': 20, 'currentTarget': array([22.,  4.]), 'previousTarget': array([22.,  4.]), 'currentState': array([21.21372949,  4.14200122,  5.74195564]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 0.7989904048300704}
episode index:4715
target Thresh 32.0
target distance 6.0
model initialize at round 4715
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 25.]), 'previousTarget': array([11., 25.]), 'currentState': array([ 6.41921388, 20.09544483,  0.44255942]), 'targetState': array([11, 25], dtype=int32), 'currentDistance': 6.711055271055169}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9319583090348291
{'scaleFactor': 20, 'currentTarget': array([11., 25.]), 'previousTarget': array([11., 25.]), 'currentState': array([10.48395137, 24.40603494,  1.14343097]), 'targetState': array([11, 25], dtype=int32), 'currentDistance': 0.7868295160410756}
episode index:4716
target Thresh 32.0
target distance 8.0
model initialize at round 4716
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 18.]), 'previousTarget': array([17., 18.]), 'currentState': array([23.32189412, 20.1510636 ,  2.99994016]), 'targetState': array([17, 18], dtype=int32), 'currentDistance': 6.677830478714964}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9319664372288009
{'scaleFactor': 20, 'currentTarget': array([17., 18.]), 'previousTarget': array([17., 18.]), 'currentState': array([17.70653869, 18.33841879,  3.84129402]), 'targetState': array([17, 18], dtype=int32), 'currentDistance': 0.7834055165318271}
episode index:4717
target Thresh 32.0
target distance 11.0
model initialize at round 4717
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  6.]), 'previousTarget': array([18.,  6.]), 'currentState': array([ 8.49707952, 14.85809775,  5.02744424]), 'targetState': array([18,  6], dtype=int32), 'currentDistance': 12.991204458882947}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9319684537002236
{'scaleFactor': 20, 'currentTarget': array([18.,  6.]), 'previousTarget': array([18.,  6.]), 'currentState': array([17.08221982,  6.71614696,  5.18640481]), 'targetState': array([18,  6], dtype=int32), 'currentDistance': 1.1641249635702167}
episode index:4718
target Thresh 32.0
target distance 16.0
model initialize at round 4718
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 18.]), 'previousTarget': array([ 7., 18.]), 'currentState': array([23.36838443, 23.35784258,  3.92306519]), 'targetState': array([ 7, 18], dtype=int32), 'currentDistance': 17.222963915330514}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9319645437179781
{'scaleFactor': 20, 'currentTarget': array([ 7., 18.]), 'previousTarget': array([ 7., 18.]), 'currentState': array([ 6.36978191, 17.8612238 ,  3.13476328]), 'targetState': array([ 7, 18], dtype=int32), 'currentDistance': 0.6453167236508783}
episode index:4719
target Thresh 32.0
target distance 9.0
model initialize at round 4719
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 24.]), 'previousTarget': array([23., 24.]), 'currentState': array([14.47058915, 21.38416154,  6.00578547]), 'targetState': array([23, 24], dtype=int32), 'currentDistance': 8.92151670891915}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9319685745455589
{'scaleFactor': 20, 'currentTarget': array([23., 24.]), 'previousTarget': array([23., 24.]), 'currentState': array([23.87817062, 24.30153217,  0.09501215]), 'targetState': array([23, 24], dtype=int32), 'currentDistance': 0.9284962478619927}
episode index:4720
target Thresh 32.0
target distance 18.0
model initialize at round 4720
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([11.35827214, 20.62970589,  4.37343192]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 19.95955378111685}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9319627311861992
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.87567402, 3.12398671, 4.24008205]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.17558375540789348}
episode index:4721
target Thresh 32.0
target distance 6.0
model initialize at round 4721
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 12.]), 'previousTarget': array([18., 12.]), 'currentState': array([22.49981979, 16.1642218 ,  4.60095739]), 'targetState': array([18, 12], dtype=int32), 'currentDistance': 6.130996764631947}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9319708498369433
{'scaleFactor': 20, 'currentTarget': array([18., 12.]), 'previousTarget': array([18., 12.]), 'currentState': array([18.22893004, 12.23746148,  3.49744284]), 'targetState': array([18, 12], dtype=int32), 'currentDistance': 0.3298437822303382}
episode index:4722
target Thresh 32.0
target distance 21.0
model initialize at round 4722
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 26.]), 'previousTarget': array([11., 26.]), 'currentState': array([9.09907087, 6.42151848, 1.12568402]), 'targetState': array([11, 26], dtype=int32), 'currentDistance': 19.670548294264744}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9319650084702635
{'scaleFactor': 20, 'currentTarget': array([11., 26.]), 'previousTarget': array([11., 26.]), 'currentState': array([11.39069866, 26.18515184,  1.43691229]), 'targetState': array([11, 26], dtype=int32), 'currentDistance': 0.432350143176762}
episode index:4723
target Thresh 32.0
target distance 8.0
model initialize at round 4723
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([12.62311468, 12.4448447 ,  5.54068536]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 6.829489876372569}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9319731232017473
{'scaleFactor': 20, 'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([18.12765556, 10.21451907,  6.1959408 ]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 0.8983336001720651}
episode index:4724
target Thresh 32.0
target distance 12.0
model initialize at round 4724
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([14.22687868, 12.50511823,  3.22509289]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 12.975336917283453}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9319751352707842
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.90883711, 6.58299494, 3.69106476]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 1.0797536688846856}
episode index:4725
target Thresh 32.0
target distance 15.0
model initialize at round 4725
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 13.]), 'previousTarget': array([ 8., 13.]), 'currentState': array([ 3.45621576, 27.32983711,  4.56114912]), 'targetState': array([ 8, 13], dtype=int32), 'currentDistance': 15.0329706541142}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9319731821516892
{'scaleFactor': 20, 'currentTarget': array([ 8., 13.]), 'previousTarget': array([ 8., 13.]), 'currentState': array([ 8.15603044, 12.18835106,  4.7162809 ]), 'targetState': array([ 8, 13], dtype=int32), 'currentDistance': 0.8265104312353757}
episode index:4726
target Thresh 32.0
target distance 23.0
model initialize at round 4726
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([ 3.90889589, 24.44029216,  5.81652173]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 27.367836289683822}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9319598063595625
{'scaleFactor': 20, 'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([25.10117522,  6.49779417,  5.39953527]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 0.5122959336149407}
episode index:4727
target Thresh 32.0
target distance 20.0
model initialize at round 4727
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 28.]), 'previousTarget': array([ 4., 28.]), 'currentState': array([23.90892089, 14.6805037 ,  2.63494086]), 'targetState': array([ 4, 28], dtype=int32), 'currentDistance': 23.953582458440493}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9319501669909832
{'scaleFactor': 20, 'currentTarget': array([ 4., 28.]), 'previousTarget': array([ 4., 28.]), 'currentState': array([ 4.06363417, 28.0176439 ,  2.76020185]), 'targetState': array([ 4, 28], dtype=int32), 'currentDistance': 0.06603494870963321}
episode index:4728
target Thresh 32.0
target distance 15.0
model initialize at round 4728
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([25.44910831,  7.59025357,  2.91427606]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 18.99209871357051}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9319462691437623
{'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([12.83201059, 20.21354981,  2.21418944]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 1.1448779557222715}
episode index:4729
target Thresh 32.0
target distance 23.0
model initialize at round 4729
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 27.]), 'previousTarget': array([20., 27.]), 'currentState': array([8.63716369, 3.61002162, 0.77615422]), 'targetState': array([20, 27], dtype=int32), 'currentDistance': 26.00394465827582}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9319347627492285
{'scaleFactor': 20, 'currentTarget': array([20., 27.]), 'previousTarget': array([20., 27.]), 'currentState': array([20.00619732, 26.82015003,  1.27278977]), 'targetState': array([20, 27], dtype=int32), 'currentDistance': 0.1799567119654366}
episode index:4730
target Thresh 32.0
target distance 18.0
model initialize at round 4730
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([9.81462206, 7.75519818, 1.58439618]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 16.265214272116747}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9319328202279177
{'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([ 8.98135443, 23.62950745,  1.97727805]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 0.3709614360251179}
episode index:4731
target Thresh 32.0
target distance 20.0
model initialize at round 4731
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 23.]), 'previousTarget': array([ 4., 23.]), 'currentState': array([16.31982488,  3.09695185,  2.07395315]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 23.40746485333111}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9319231947104808
{'scaleFactor': 20, 'currentTarget': array([ 4., 23.]), 'previousTarget': array([ 4., 23.]), 'currentState': array([ 3.92629941, 23.5518084 ,  1.96906657]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 0.5567084407152786}
episode index:4732
target Thresh 32.0
target distance 16.0
model initialize at round 4732
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 20.]), 'previousTarget': array([21., 20.]), 'currentState': array([6.31703186, 4.00251167, 2.13010025]), 'targetState': array([21, 20], dtype=int32), 'currentDistance': 21.71426228163608}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9319154649533602
{'scaleFactor': 20, 'currentTarget': array([21., 20.]), 'previousTarget': array([21., 20.]), 'currentState': array([20.74033583, 19.67638981,  1.16606211]), 'targetState': array([21, 20], dtype=int32), 'currentDistance': 0.41490846702826284}
episode index:4733
target Thresh 32.0
target distance 18.0
model initialize at round 4733
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  9.]), 'previousTarget': array([24.,  9.]), 'currentState': array([ 5.69714402, 13.3445042 ,  5.54145002]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 18.811412856441134}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9319115785533877
{'scaleFactor': 20, 'currentTarget': array([24.,  9.]), 'previousTarget': array([24.,  9.]), 'currentState': array([23.11301916,  9.45969935,  6.16144758]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 0.9990287763359029}
episode index:4734
target Thresh 32.0
target distance 10.0
model initialize at round 4734
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 22.]), 'previousTarget': array([15., 22.]), 'currentState': array([ 6.82241159, 14.4722982 ,  0.8583638 ]), 'targetState': array([15, 22], dtype=int32), 'currentDistance': 11.114821034692158}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9319135993708846
{'scaleFactor': 20, 'currentTarget': array([15., 22.]), 'previousTarget': array([15., 22.]), 'currentState': array([15.56484495, 22.58523675,  0.70689161]), 'targetState': array([15, 22], dtype=int32), 'currentDistance': 0.8133583881128276}
episode index:4735
target Thresh 32.0
target distance 19.0
model initialize at round 4735
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([ 7.49754487, 14.60774289,  0.26067615]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 19.575540778903}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9319077861267202
{'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([26.30722562, 21.26466676,  0.28872431]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 0.4055071807005743}
episode index:4736
target Thresh 32.0
target distance 22.0
model initialize at round 4736
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  6.]), 'previousTarget': array([23.,  6.]), 'currentState': array([ 9.63649009, 26.85746046,  5.56122606]), 'targetState': array([23,  6], dtype=int32), 'currentDistance': 24.77129495915167}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9318981760540138
{'scaleFactor': 20, 'currentTarget': array([23.,  6.]), 'previousTarget': array([23.,  6.]), 'currentState': array([22.83557573,  6.99403405,  5.50636689]), 'targetState': array([23,  6], dtype=int32), 'currentDistance': 1.007541080573439}
episode index:4737
target Thresh 32.0
target distance 6.0
model initialize at round 4737
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 19.]), 'previousTarget': array([21., 19.]), 'currentState': array([16.08049552, 23.68392499,  0.09291631]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 6.79269295824402}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9319062809134367
{'scaleFactor': 20, 'currentTarget': array([21., 19.]), 'previousTarget': array([21., 19.]), 'currentState': array([20.36945786, 19.81559466,  5.32481542]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 1.0309112681477532}
episode index:4738
target Thresh 32.0
target distance 15.0
model initialize at round 4738
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 12.]), 'previousTarget': array([18., 12.]), 'currentState': array([24.58442675, 25.72477797,  4.41723824]), 'targetState': array([18, 12], dtype=int32), 'currentDistance': 15.222490135573086}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9319043476814287
{'scaleFactor': 20, 'currentTarget': array([18., 12.]), 'previousTarget': array([18., 12.]), 'currentState': array([17.69868752, 11.36472436,  4.36034863]), 'targetState': array([18, 12], dtype=int32), 'currentDistance': 0.7031104774950051}
episode index:4739
target Thresh 32.0
target distance 12.0
model initialize at round 4739
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([24.4742293 , 15.84030671,  4.30034137]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 14.396988238285232}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9319043816477212
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.52311687,  5.20332715,  3.68443567]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.5612425422982581}
episode index:4740
target Thresh 32.0
target distance 10.0
model initialize at round 4740
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([ 4.29102599, 19.65761635,  0.38699865]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 14.41696786942832}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9319044155996847
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.4321211 ,  9.87421476,  5.12178813]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.0424672103549792}
episode index:4741
target Thresh 32.0
target distance 15.0
model initialize at round 4741
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 22.]), 'previousTarget': array([ 2., 22.]), 'currentState': array([8.54943033, 8.89407087, 2.20900047]), 'targetState': array([ 2, 22], dtype=int32), 'currentDistance': 14.651294002709742}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9319044495373286
{'scaleFactor': 20, 'currentTarget': array([ 2., 22.]), 'previousTarget': array([ 2., 22.]), 'currentState': array([ 2.3320344 , 21.36315885,  1.88583089]), 'targetState': array([ 2, 22], dtype=int32), 'currentDistance': 0.7182015682958869}
episode index:4742
target Thresh 32.0
target distance 11.0
model initialize at round 4742
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 15.]), 'previousTarget': array([25., 15.]), 'currentState': array([15.08260698, 20.52599299,  6.07921052]), 'targetState': array([25, 15], dtype=int32), 'currentDistance': 11.353029677299832}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9319064684493809
{'scaleFactor': 20, 'currentTarget': array([25., 15.]), 'previousTarget': array([25., 15.]), 'currentState': array([25.54538549, 14.82819022,  6.04714839]), 'targetState': array([25, 15], dtype=int32), 'currentDistance': 0.5718076006706049}
episode index:4743
target Thresh 32.0
target distance 17.0
model initialize at round 4743
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 29.]), 'previousTarget': array([ 5., 29.]), 'currentState': array([20.08635559, 27.73156452,  2.82369411]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 15.139585649383957}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9319045372153966
{'scaleFactor': 20, 'currentTarget': array([ 5., 29.]), 'previousTarget': array([ 5., 29.]), 'currentState': array([ 4.38705134, 28.42011758,  2.76268798]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 0.8437829560166797}
episode index:4744
target Thresh 32.0
target distance 14.0
model initialize at round 4744
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 13.]), 'previousTarget': array([19., 13.]), 'currentState': array([6.68157918, 4.06840722, 1.05065792]), 'targetState': array([19, 13], dtype=int32), 'currentDistance': 15.21567747935298}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9319026067954203
{'scaleFactor': 20, 'currentTarget': array([19., 13.]), 'previousTarget': array([19., 13.]), 'currentState': array([19.47152187, 13.25088919,  0.55965793]), 'targetState': array([19, 13], dtype=int32), 'currentDistance': 0.5341144640403124}
episode index:4745
target Thresh 32.0
target distance 1.0
model initialize at round 4745
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  6.]), 'previousTarget': array([11.,  6.]), 'currentState': array([12.36945834,  6.02174255,  4.65290839]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 1.3696309316589663}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9319148481340642
{'scaleFactor': 20, 'currentTarget': array([11.,  6.]), 'previousTarget': array([11.,  6.]), 'currentState': array([10.91004665,  5.18358475,  2.65291125]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 0.8213558684763407}
episode index:4746
target Thresh 32.0
target distance 19.0
model initialize at round 4746
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 26.]), 'previousTarget': array([27., 26.]), 'currentState': array([ 6.72191208, 16.09493349,  1.42322373]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 22.567923974048625}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9319071429320682
{'scaleFactor': 20, 'currentTarget': array([27., 26.]), 'previousTarget': array([27., 26.]), 'currentState': array([26.21007153, 25.59084459,  0.60215493]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 0.8896039193555738}
episode index:4747
target Thresh 32.0
target distance 10.0
model initialize at round 4747
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 12.]), 'previousTarget': array([13., 12.]), 'currentState': array([21.37631181,  6.16104737,  2.41146642]), 'targetState': array([13, 12], dtype=int32), 'currentDistance': 10.210581150396932}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9319111620784388
{'scaleFactor': 20, 'currentTarget': array([13., 12.]), 'previousTarget': array([13., 12.]), 'currentState': array([13.08935377, 11.67083913,  2.89337168]), 'targetState': array([13, 12], dtype=int32), 'currentDistance': 0.3410732683010817}
episode index:4748
target Thresh 32.0
target distance 5.0
model initialize at round 4748
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 24.]), 'previousTarget': array([18., 24.]), 'currentState': array([21.68584922, 17.94862194,  2.80636024]), 'targetState': array([18, 24], dtype=int32), 'currentDistance': 7.085524742886063}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.931917202265409
{'scaleFactor': 20, 'currentTarget': array([18., 24.]), 'previousTarget': array([18., 24.]), 'currentState': array([17.61303074, 24.62566431,  2.28405458]), 'targetState': array([18, 24], dtype=int32), 'currentDistance': 0.735663675321791}
episode index:4749
target Thresh 32.0
target distance 26.0
model initialize at round 4749
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 29.]), 'previousTarget': array([ 8., 29.]), 'currentState': array([4.85373792, 4.23226269, 1.31005311]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 24.96677344914448}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9319057504381949
{'scaleFactor': 20, 'currentTarget': array([ 8., 29.]), 'previousTarget': array([ 8., 29.]), 'currentState': array([ 8.15602378, 29.6324768 ,  1.47700236]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 0.6514371219754848}
episode index:4750
target Thresh 32.0
target distance 16.0
model initialize at round 4750
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 21.]), 'previousTarget': array([ 8., 21.]), 'currentState': array([22.33220727,  7.7744884 ,  2.26599312]), 'targetState': array([ 8, 21], dtype=int32), 'currentDistance': 19.501956879977726}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9318999571998389
{'scaleFactor': 20, 'currentTarget': array([ 8., 21.]), 'previousTarget': array([ 8., 21.]), 'currentState': array([ 7.49994439, 21.12079753,  2.39469959]), 'targetState': array([ 8, 21], dtype=int32), 'currentDistance': 0.5144391678767024}
episode index:4751
target Thresh 32.0
target distance 4.0
model initialize at round 4751
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 18.]), 'previousTarget': array([10., 18.]), 'currentState': array([ 7.58894641, 17.58117646,  5.63889259]), 'targetState': array([10, 18], dtype=int32), 'currentDistance': 2.4471601072404647}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.931912183639822
{'scaleFactor': 20, 'currentTarget': array([10., 18.]), 'previousTarget': array([10., 18.]), 'currentState': array([ 9.47544846, 17.43756298,  0.50381315]), 'targetState': array([10, 18], dtype=int32), 'currentDistance': 0.7690836928537802}
episode index:4752
target Thresh 32.0
target distance 9.0
model initialize at round 4752
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 14.]), 'previousTarget': array([15., 14.]), 'currentState': array([16.36629025,  5.40420359,  1.88904881]), 'targetState': array([15, 14], dtype=int32), 'currentDistance': 8.70370409374441}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9319182185285996
{'scaleFactor': 20, 'currentTarget': array([15., 14.]), 'previousTarget': array([15., 14.]), 'currentState': array([15.05319267, 13.23404657,  2.12719342]), 'targetState': array([15, 14], dtype=int32), 'currentDistance': 0.7677982276435671}
episode index:4753
target Thresh 32.0
target distance 12.0
model initialize at round 4753
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 17.]), 'previousTarget': array([15., 17.]), 'currentState': array([ 5.54873411, 27.7610314 ,  5.38233051]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 14.32222831328095}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9319182494771436
{'scaleFactor': 20, 'currentTarget': array([15., 17.]), 'previousTarget': array([15., 17.]), 'currentState': array([14.86216315, 17.34730585,  5.30735211]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 0.37365806389718875}
episode index:4754
target Thresh 32.0
target distance 1.0
model initialize at round 4754
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 12.]), 'previousTarget': array([12., 12.]), 'currentState': array([12.10651064, 12.93543684,  1.9521777 ]), 'targetState': array([12, 12], dtype=int32), 'currentDistance': 0.9414810693880469}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9319325674057499
{'scaleFactor': 20, 'currentTarget': array([12., 12.]), 'previousTarget': array([12., 12.]), 'currentState': array([12.10651064, 12.93543684,  1.9521777 ]), 'targetState': array([12, 12], dtype=int32), 'currentDistance': 0.9414810693880469}
episode index:4755
target Thresh 32.0
target distance 14.0
model initialize at round 4755
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  3.]), 'previousTarget': array([12.,  3.]), 'currentState': array([25.12485772,  5.5624625 ,  3.15553951]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 13.372662571816361}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9319325953242742
{'scaleFactor': 20, 'currentTarget': array([12.,  3.]), 'previousTarget': array([12.,  3.]), 'currentState': array([11.47961612,  3.18877451,  3.54972452]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 0.5535658970331842}
episode index:4756
target Thresh 32.0
target distance 9.0
model initialize at round 4756
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([21.27147875, 14.48288271,  5.27470565]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 7.679929648155266}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9319386208476451
{'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([22.73050352,  6.76562295,  5.31373143]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 0.3571567618771705}
episode index:4757
target Thresh 32.0
target distance 9.0
model initialize at round 4757
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 17.]), 'previousTarget': array([ 9., 17.]), 'currentState': array([17.52663074,  9.5984518 ,  2.2625331 ]), 'targetState': array([ 9, 17], dtype=int32), 'currentDistance': 11.290985235142553}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9319406262130411
{'scaleFactor': 20, 'currentTarget': array([ 9., 17.]), 'previousTarget': array([ 9., 17.]), 'currentState': array([ 8.42061941, 17.18773333,  2.29298115]), 'targetState': array([ 9, 17], dtype=int32), 'currentDistance': 0.6090366702618215}
episode index:4758
target Thresh 32.0
target distance 12.0
model initialize at round 4758
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 16.]), 'previousTarget': array([24., 16.]), 'currentState': array([19.3694421 , 26.13825639,  5.47124684]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 11.145685673236805}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9319426307356694
{'scaleFactor': 20, 'currentTarget': array([24., 16.]), 'previousTarget': array([24., 16.]), 'currentState': array([24.05765414, 15.25636018,  5.18254131]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 0.7458714245937502}
episode index:4759
target Thresh 32.0
target distance 22.0
model initialize at round 4759
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  7.]), 'previousTarget': array([26.,  7.]), 'currentState': array([18.56449794, 29.62026952,  5.65064103]), 'targetState': array([26,  7], dtype=int32), 'currentDistance': 23.810990826224135}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9319330597778922
{'scaleFactor': 20, 'currentTarget': array([26.,  7.]), 'previousTarget': array([26.,  7.]), 'currentState': array([26.12707375,  7.13699584,  4.72427362]), 'targetState': array([26,  7], dtype=int32), 'currentDistance': 0.1868571551163785}
episode index:4760
target Thresh 32.0
target distance 22.0
model initialize at round 4760
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([ 9.84453165, 26.32422625,  5.62987971]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 23.18444837939439}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9319234928406813
{'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([21.11386342,  5.45897624,  5.41042944]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 0.5528757460018343}
episode index:4761
target Thresh 32.0
target distance 3.0
model initialize at round 4761
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 20.]), 'previousTarget': array([ 4., 20.]), 'currentState': array([ 3.49053215, 22.21852575,  4.7902934 ]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 2.2762719505355724}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9319356886632683
{'scaleFactor': 20, 'currentTarget': array([ 4., 20.]), 'previousTarget': array([ 4., 20.]), 'currentState': array([ 4.03350211, 20.30759989,  5.1921902 ]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 0.30941894910206214}
episode index:4762
target Thresh 32.0
target distance 20.0
model initialize at round 4762
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 17.]), 'previousTarget': array([ 3., 17.]), 'currentState': array([24.05760723,  5.30914287,  1.90127962]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 24.085243669477418}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9319261251913079
{'scaleFactor': 20, 'currentTarget': array([ 3., 17.]), 'previousTarget': array([ 3., 17.]), 'currentState': array([ 3.33399944, 16.85140678,  2.31307452]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 0.3655619954159003}
episode index:4763
target Thresh 32.0
target distance 7.0
model initialize at round 4763
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 13.]), 'previousTarget': array([12., 13.]), 'currentState': array([10.47819858, 18.3999652 ,  5.4071455 ]), 'targetState': array([12, 13], dtype=int32), 'currentDistance': 5.610303356196864}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9319341799509234
{'scaleFactor': 20, 'currentTarget': array([12., 13.]), 'previousTarget': array([12., 13.]), 'currentState': array([12.19333442, 12.78803097,  4.71508443]), 'targetState': array([12, 13], dtype=int32), 'currentDistance': 0.28689556943900746}
episode index:4764
target Thresh 32.0
target distance 8.0
model initialize at round 4764
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 29.]), 'previousTarget': array([16., 29.]), 'currentState': array([10.61747503, 20.53506709,  0.73010224]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 10.03128427329602}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9319381790841761
{'scaleFactor': 20, 'currentTarget': array([16., 29.]), 'previousTarget': array([16., 29.]), 'currentState': array([15.99675114, 28.8558054 ,  1.44583898]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 0.1442311981106038}
episode index:4765
target Thresh 32.0
target distance 6.0
model initialize at round 4765
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 11.]), 'previousTarget': array([25., 11.]), 'currentState': array([20.44669637,  7.37935331,  0.8199645 ]), 'targetState': array([25, 11], dtype=int32), 'currentDistance': 5.817358193019473}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9319462279345571
{'scaleFactor': 20, 'currentTarget': array([25., 11.]), 'previousTarget': array([25., 11.]), 'currentState': array([24.99383662, 11.19369214,  0.41288313]), 'targetState': array([25, 11], dtype=int32), 'currentDistance': 0.19379017667220938}
episode index:4766
target Thresh 32.0
target distance 19.0
model initialize at round 4766
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 6.]), 'previousTarget': array([6., 6.]), 'currentState': array([ 5.61528193, 23.43353384,  4.07665873]), 'targetState': array([6, 6], dtype=int32), 'currentDistance': 17.437778245395656}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9319423619852281
{'scaleFactor': 20, 'currentTarget': array([6., 6.]), 'previousTarget': array([6., 6.]), 'currentState': array([6.07660112, 5.6129605 , 4.75289263]), 'targetState': array([6, 6], dtype=int32), 'currentDistance': 0.3945469654708022}
episode index:4767
target Thresh 32.0
target distance 16.0
model initialize at round 4767
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 26.]), 'previousTarget': array([ 2., 26.]), 'currentState': array([15.90522732,  8.72177433,  2.99414587]), 'targetState': array([ 2, 26], dtype=int32), 'currentDistance': 22.17864804162456}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9319346849492116
{'scaleFactor': 20, 'currentTarget': array([ 2., 26.]), 'previousTarget': array([ 2., 26.]), 'currentState': array([ 2.11793457, 25.54813945,  2.07939252]), 'targetState': array([ 2, 26], dtype=int32), 'currentDistance': 0.4669973432933379}
episode index:4768
target Thresh 32.0
target distance 19.0
model initialize at round 4768
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 22.]), 'previousTarget': array([ 7., 22.]), 'currentState': array([24.03134397, 23.6473124 ,  3.32079648]), 'targetState': array([ 7, 22], dtype=int32), 'currentDistance': 17.110824512299573}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9319308230415861
{'scaleFactor': 20, 'currentTarget': array([ 7., 22.]), 'previousTarget': array([ 7., 22.]), 'currentState': array([ 6.21359036, 22.16611261,  3.16685663]), 'targetState': array([ 7, 22], dtype=int32), 'currentDistance': 0.8037621027167067}
episode index:4769
target Thresh 32.0
target distance 7.0
model initialize at round 4769
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 9.65618816, 16.7009534 ,  5.09454692]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 9.44497841655558}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9319348186866298
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.1495685 , 9.95539745, 4.40994847]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.15607729713370444}
episode index:4770
target Thresh 32.0
target distance 25.0
model initialize at round 4770
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 27.]), 'previousTarget': array([ 9., 27.]), 'currentState': array([23.65129162,  0.99333532,  2.77278042]), 'targetState': array([ 9, 27], dtype=int32), 'currentDistance': 29.84973959477609}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.9319197534038703
{'scaleFactor': 20, 'currentTarget': array([ 9., 27.]), 'previousTarget': array([ 9., 27.]), 'currentState': array([ 9.02600196, 26.80191707,  2.39533161]), 'targetState': array([ 9, 27], dtype=int32), 'currentDistance': 0.19978225313333023}
episode index:4771
target Thresh 32.0
target distance 10.0
model initialize at round 4771
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 14.]), 'previousTarget': array([ 9., 14.]), 'currentState': array([15.78211527, 25.16152696,  3.38987565]), 'targetState': array([ 9, 14], dtype=int32), 'currentDistance': 13.060504266290986}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9319197839140344
{'scaleFactor': 20, 'currentTarget': array([ 9., 14.]), 'previousTarget': array([ 9., 14.]), 'currentState': array([ 9.03366414, 13.35011745,  4.07914922]), 'targetState': array([ 9, 14], dtype=int32), 'currentDistance': 0.6507538718709727}
episode index:4772
target Thresh 32.0
target distance 19.0
model initialize at round 4772
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([ 7.17634199, 20.20358116,  6.07002837]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 22.181463888115832}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9319121196505408
{'scaleFactor': 20, 'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([24.75841211,  7.25105024,  6.02706779]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 0.3484120159361021}
episode index:4773
target Thresh 32.0
target distance 4.0
model initialize at round 4773
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([13.63435747, 26.59842343,  5.03224916]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 3.616952531039446}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.93192221346712
{'scaleFactor': 20, 'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([13.85992422, 22.6610762 ,  5.08736092]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 0.36672955103930427}
episode index:4774
target Thresh 32.0
target distance 6.0
model initialize at round 4774
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'previousTarget': array([22., 18.]), 'currentState': array([19.75147423, 22.87147375,  4.88655114]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 5.365363418328776}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9319302504904776
{'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'previousTarget': array([22., 18.]), 'currentState': array([22.05086286, 17.3939426 ,  5.43753409]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 0.6081879637682114}
episode index:4775
target Thresh 32.0
target distance 14.0
model initialize at round 4775
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([ 6.31987363, 19.90220704,  4.20973277]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 15.017829278019079}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9319283272165951
{'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([12.55247053,  5.65303295,  5.00999218]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 0.6523877833608364}
episode index:4776
target Thresh 32.0
target distance 14.0
model initialize at round 4776
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([25.58521956, 25.19628008,  3.8028146 ]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 13.414317924945257}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9319283559000137
{'scaleFactor': 20, 'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([19.71587085, 12.6843252 ,  4.50081417]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 0.4247116099376114}
episode index:4777
target Thresh 32.0
target distance 13.0
model initialize at round 4777
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  8.]), 'previousTarget': array([24.,  8.]), 'currentState': array([12.62613075, 21.43368984,  5.53381926]), 'targetState': array([24,  8], dtype=int32), 'currentDistance': 17.601957975263684}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9319245025914292
{'scaleFactor': 20, 'currentTarget': array([24.,  8.]), 'previousTarget': array([24.,  8.]), 'currentState': array([24.29818386,  7.89858954,  5.13654829]), 'targetState': array([24,  8], dtype=int32), 'currentDistance': 0.31495665186915955}
episode index:4778
target Thresh 32.0
target distance 18.0
model initialize at round 4778
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  3.]), 'previousTarget': array([24.,  3.]), 'currentState': array([ 7.63829797, 17.61481452,  5.04226607]), 'targetState': array([24,  3], dtype=int32), 'currentDistance': 21.938507171922904}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9319168469629855
{'scaleFactor': 20, 'currentTarget': array([24.,  3.]), 'previousTarget': array([24.,  3.]), 'currentState': array([23.94926131,  3.20592081,  5.15716147]), 'targetState': array([24,  3], dtype=int32), 'currentDistance': 0.21207968628851107}
episode index:4779
target Thresh 32.0
target distance 11.0
model initialize at round 4779
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 19.]), 'previousTarget': array([27., 19.]), 'currentState': array([17.05569846, 25.68931552,  0.11726003]), 'targetState': array([27, 19], dtype=int32), 'currentDistance': 11.98482687622667}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9319188476538721
{'scaleFactor': 20, 'currentTarget': array([27., 19.]), 'previousTarget': array([27., 19.]), 'currentState': array([26.72603911, 19.17947827,  6.16574046]), 'targetState': array([27, 19], dtype=int32), 'currentDistance': 0.3275164388087109}
episode index:4780
target Thresh 32.0
target distance 21.0
model initialize at round 4780
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 23.]), 'previousTarget': array([ 7., 23.]), 'currentState': array([10.4821601 ,  3.92668966,  1.71044023]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 19.388568962125042}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9319130880277174
{'scaleFactor': 20, 'currentTarget': array([ 7., 23.]), 'previousTarget': array([ 7., 23.]), 'currentState': array([ 7.21757503, 23.53617626,  2.0503838 ]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 0.5786396715786897}
episode index:4781
target Thresh 32.0
target distance 10.0
model initialize at round 4781
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 15.]), 'previousTarget': array([14., 15.]), 'currentState': array([23.65090151, 24.67302557,  5.07765764]), 'targetState': array([14, 15], dtype=int32), 'currentDistance': 13.664088831460214}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9319131198679265
{'scaleFactor': 20, 'currentTarget': array([14., 15.]), 'previousTarget': array([14., 15.]), 'currentState': array([14.29199195, 15.17527573,  3.58588538]), 'targetState': array([14, 15], dtype=int32), 'currentDistance': 0.3405596584909557}
episode index:4782
target Thresh 32.0
target distance 15.0
model initialize at round 4782
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 24.]), 'previousTarget': array([21., 24.]), 'currentState': array([ 5.8791426 , 15.67862491,  0.63267016]), 'targetState': array([21, 24], dtype=int32), 'currentDistance': 17.259363019231998}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9319092737729265
{'scaleFactor': 20, 'currentTarget': array([21., 24.]), 'previousTarget': array([21., 24.]), 'currentState': array([21.23724942, 24.63488443,  0.32872708]), 'targetState': array([21, 24], dtype=int32), 'currentDistance': 0.6777650949348474}
episode index:4783
target Thresh 32.0
target distance 17.0
model initialize at round 4783
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 27.]), 'previousTarget': array([ 9., 27.]), 'currentState': array([ 7.23361161, 11.49834472,  1.03359365]), 'targetState': array([ 9, 27], dtype=int32), 'currentDistance': 15.601969242513963}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9319073580999866
{'scaleFactor': 20, 'currentTarget': array([ 9., 27.]), 'previousTarget': array([ 9., 27.]), 'currentState': array([ 8.782855  , 27.14464454,  1.17672299]), 'targetState': array([ 9, 27], dtype=int32), 'currentDistance': 0.26090993115894306}
episode index:4784
target Thresh 32.0
target distance 18.0
model initialize at round 4784
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 25.]), 'previousTarget': array([25., 25.]), 'currentState': array([11.14192649,  8.6122005 ,  0.68101561]), 'targetState': array([25, 25], dtype=int32), 'currentDistance': 21.46173743839264}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9318997156540427
{'scaleFactor': 20, 'currentTarget': array([25., 25.]), 'previousTarget': array([25., 25.]), 'currentState': array([24.80467807, 25.14032238,  0.72038114]), 'targetState': array([25, 25], dtype=int32), 'currentDistance': 0.24050161788192323}
episode index:4785
target Thresh 32.0
target distance 20.0
model initialize at round 4785
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 21.]), 'previousTarget': array([ 4., 21.]), 'currentState': array([22.00061616, 25.97624243,  3.19160502]), 'targetState': array([ 4, 21], dtype=int32), 'currentDistance': 18.675791034010825}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9318958747705971
{'scaleFactor': 20, 'currentTarget': array([ 4., 21.]), 'previousTarget': array([ 4., 21.]), 'currentState': array([ 4.93735469, 21.27173461,  3.23364434]), 'targetState': array([ 4, 21], dtype=int32), 'currentDistance': 0.9759474953828887}
episode index:4786
target Thresh 32.0
target distance 21.0
model initialize at round 4786
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 29.]), 'previousTarget': array([18., 29.]), 'currentState': array([20.17476809,  9.98808308,  1.36902726]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 19.13589823464998}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9318901271625415
{'scaleFactor': 20, 'currentTarget': array([18., 29.]), 'previousTarget': array([18., 29.]), 'currentState': array([17.47180318, 29.39354224,  1.48372175]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 0.6586861002378119}
episode index:4787
target Thresh 32.0
target distance 11.0
model initialize at round 4787
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  2.]), 'previousTarget': array([25.,  2.]), 'currentState': array([14.5524147 ,  3.41027484,  6.05682659]), 'targetState': array([25,  2], dtype=int32), 'currentDistance': 10.542339101522705}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9318941162859202
{'scaleFactor': 20, 'currentTarget': array([25.,  2.]), 'previousTarget': array([25.,  2.]), 'currentState': array([24.43837645,  2.08465675,  5.98734279]), 'targetState': array([25,  2], dtype=int32), 'currentDistance': 0.567968112543666}
episode index:4788
target Thresh 32.0
target distance 10.0
model initialize at round 4788
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 17.]), 'previousTarget': array([ 9., 17.]), 'currentState': array([14.52206744, 26.19495128,  4.6503675 ]), 'targetState': array([ 9, 17], dtype=int32), 'currentDistance': 10.725686828323006}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9318981037433464
{'scaleFactor': 20, 'currentTarget': array([ 9., 17.]), 'previousTarget': array([ 9., 17.]), 'currentState': array([ 9.35473582, 17.81321489,  4.04304953]), 'targetState': array([ 9, 17], dtype=int32), 'currentDistance': 0.8872181011844958}
episode index:4789
target Thresh 32.0
target distance 16.0
model initialize at round 4789
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 8.]), 'previousTarget': array([8., 8.]), 'currentState': array([20.31734497, 24.03255978,  4.13224483]), 'targetState': array([8, 8], dtype=int32), 'currentDistance': 20.217812946785177}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9318923592697065
{'scaleFactor': 20, 'currentTarget': array([8., 8.]), 'previousTarget': array([8., 8.]), 'currentState': array([8.3951019 , 8.43212771, 4.65650892]), 'targetState': array([8, 8], dtype=int32), 'currentDistance': 0.5855252928286567}
episode index:4790
target Thresh 32.0
target distance 19.0
model initialize at round 4790
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 26.]), 'previousTarget': array([ 6., 26.]), 'currentState': array([24.9513211 , 27.68226587,  2.60972476]), 'targetState': array([ 6, 26], dtype=int32), 'currentDistance': 19.025840061575675}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9318866171940936
{'scaleFactor': 20, 'currentTarget': array([ 6., 26.]), 'previousTarget': array([ 6., 26.]), 'currentState': array([ 5.48888005, 25.52885115,  3.01721382]), 'targetState': array([ 6, 26], dtype=int32), 'currentDistance': 0.6951437610254056}
episode index:4791
target Thresh 32.0
target distance 3.0
model initialize at round 4791
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'previousTarget': array([ 2., 17.]), 'currentState': array([ 2.43043215, 18.40687782,  5.36400902]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 1.4712501599602632}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9318987443607893
{'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'previousTarget': array([ 2., 17.]), 'currentState': array([ 1.95017295, 16.76000843,  3.47437131]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 0.24510954378414704}
episode index:4792
target Thresh 32.0
target distance 7.0
model initialize at round 4792
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([21.10934676, 16.73440218,  4.42209037]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 5.803157032420083}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9319067560978306
{'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([22.08446538, 10.92692282,  5.27507337]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 0.11169008515920581}
episode index:4793
target Thresh 32.0
target distance 22.0
model initialize at round 4793
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 27.]), 'previousTarget': array([26., 27.]), 'currentState': array([ 5.01844267, 14.66016321,  0.08915299]), 'targetState': array([26, 27], dtype=int32), 'currentDistance': 24.341267839238547}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9318972605024235
{'scaleFactor': 20, 'currentTarget': array([26., 27.]), 'previousTarget': array([26., 27.]), 'currentState': array([25.60996565, 26.71443959,  0.57813386]), 'targetState': array([26, 27], dtype=int32), 'currentDistance': 0.4833958382827292}
episode index:4794
target Thresh 32.0
target distance 22.0
model initialize at round 4794
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 22.]), 'previousTarget': array([26., 22.]), 'currentState': array([ 3.24622444, 21.49527063,  5.25799155]), 'targetState': array([26, 22], dtype=int32), 'currentDistance': 22.759372881692897}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9318877688676402
{'scaleFactor': 20, 'currentTarget': array([26., 22.]), 'previousTarget': array([26., 22.]), 'currentState': array([26.70696271, 22.20110663,  6.04949494]), 'targetState': array([26, 22], dtype=int32), 'currentDistance': 0.7350103028497326}
episode index:4795
target Thresh 32.0
target distance 13.0
model initialize at round 4795
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 10.]), 'previousTarget': array([18., 10.]), 'currentState': array([15.63095906, 21.35798999,  5.50131297]), 'targetState': array([18, 10], dtype=int32), 'currentDistance': 11.60242611069633}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9318897689469842
{'scaleFactor': 20, 'currentTarget': array([18., 10.]), 'previousTarget': array([18., 10.]), 'currentState': array([17.78516399,  9.82284192,  5.34305796]), 'targetState': array([18, 10], dtype=int32), 'currentDistance': 0.2784591464267407}
episode index:4796
target Thresh 32.0
target distance 5.0
model initialize at round 4796
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([11.96258288,  5.38051523,  6.23509461]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 5.265797208658304}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9318977758744498
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.40693813,  1.66525463,  5.19185687]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.5269279892809083}
episode index:4797
target Thresh 32.0
target distance 1.0
model initialize at round 4797
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  8.]), 'previousTarget': array([19.,  8.]), 'currentState': array([20.00859484,  8.68296419,  2.57566237]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 1.2180737421181125}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9319098855501742
{'scaleFactor': 20, 'currentTarget': array([19.,  8.]), 'previousTarget': array([19.,  8.]), 'currentState': array([18.46669812,  7.99877618,  4.56265032]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 0.5333032867275936}
episode index:4798
target Thresh 32.0
target distance 3.0
model initialize at round 4798
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 13.]), 'previousTarget': array([12., 13.]), 'currentState': array([13.25227563, 11.77753447,  2.43965584]), 'targetState': array([12, 13], dtype=int32), 'currentDistance': 1.7500332072469573}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9319219901791489
{'scaleFactor': 20, 'currentTarget': array([12., 13.]), 'previousTarget': array([12., 13.]), 'currentState': array([11.97365486, 13.30248227,  2.09400278]), 'targetState': array([12, 13], dtype=int32), 'currentDistance': 0.3036273863975081}
episode index:4799
target Thresh 32.0
target distance 12.0
model initialize at round 4799
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([ 3.15640298, 21.22275109,  6.08646351]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 10.988276025927632}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9319239814623201
{'scaleFactor': 20, 'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([14.90740706, 23.19914056,  0.47493099]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 0.929001906018927}
episode index:4800
target Thresh 32.0
target distance 14.0
model initialize at round 4800
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 24.]), 'previousTarget': array([ 6., 24.]), 'currentState': array([18.17152531, 28.21724978,  3.36222324]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 12.881429432453352}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9319259719159629
{'scaleFactor': 20, 'currentTarget': array([ 6., 24.]), 'previousTarget': array([ 6., 24.]), 'currentState': array([ 6.93362698, 24.2388917 ,  3.14203301]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 0.9637056471184514}
episode index:4801
target Thresh 32.0
target distance 6.0
model initialize at round 4801
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([17.58295887, 24.0278664 ,  5.06510233]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 6.692510209222327}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.931933962967209
{'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([21.56911164, 19.62359318,  5.64650844]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 0.7579796983087204}
episode index:4802
target Thresh 32.0
target distance 16.0
model initialize at round 4802
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 11.]), 'previousTarget': array([19., 11.]), 'currentState': array([12.96304251, 25.31743582,  5.70042753]), 'targetState': array([19, 11], dtype=int32), 'currentDistance': 15.538140947157126}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9319320497320353
{'scaleFactor': 20, 'currentTarget': array([19., 11.]), 'previousTarget': array([19., 11.]), 'currentState': array([18.86543812, 10.82413861,  4.7948545 ]), 'targetState': array([19, 11], dtype=int32), 'currentDistance': 0.2214365085174152}
episode index:4803
target Thresh 32.0
target distance 19.0
model initialize at round 4803
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([24.18654694, 20.80647813,  4.48485659]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 18.027373120436856}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9319282165092525
{'scaleFactor': 20, 'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([26.87325739,  3.11529358,  4.79634956]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 0.17133679784106068}
episode index:4804
target Thresh 32.0
target distance 16.0
model initialize at round 4804
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'previousTarget': array([16., 23.]), 'currentState': array([23.60274812,  8.19501003,  1.72238159]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 16.643001746939028}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9319263052663636
{'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'previousTarget': array([16., 23.]), 'currentState': array([16.3554654 , 22.31216765,  1.7359024 ]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 0.774253829296872}
episode index:4805
target Thresh 32.0
target distance 14.0
model initialize at round 4805
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 17.]), 'previousTarget': array([23., 17.]), 'currentState': array([10.91466918, 19.49228095,  5.7814844 ]), 'targetState': array([23, 17], dtype=int32), 'currentDistance': 12.339638783869988}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9319282931656842
{'scaleFactor': 20, 'currentTarget': array([23., 17.]), 'previousTarget': array([23., 17.]), 'currentState': array([22.592357  , 17.18321256,  5.78042847]), 'targetState': array([23, 17], dtype=int32), 'currentDistance': 0.4469224328456299}
episode index:4806
target Thresh 32.0
target distance 10.0
model initialize at round 4806
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([ 8.18793172, 21.19214358,  6.0603531 ]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 8.89234242207225}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9319342569095648
{'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([16.07273388, 20.20221807,  5.89352656]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 0.9490598578604459}
episode index:4807
target Thresh 32.0
target distance 6.0
model initialize at round 4807
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([18.13485838,  3.67755814,  0.48057914]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 6.100335029519709}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9319422362654487
{'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([23.77737745,  2.35827059,  5.60251725]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 0.4218039988382183}
episode index:4808
target Thresh 32.0
target distance 7.0
model initialize at round 4808
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 17.]), 'previousTarget': array([19., 17.]), 'currentState': array([12.40091667, 17.36548053,  5.96292138]), 'targetState': array([19, 17], dtype=int32), 'currentDistance': 6.609196382026338}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9319502123028233
{'scaleFactor': 20, 'currentTarget': array([19., 17.]), 'previousTarget': array([19., 17.]), 'currentState': array([18.34149645, 17.29391792,  0.2989659 ]), 'targetState': array([19, 17], dtype=int32), 'currentDistance': 0.7211204310428603}
episode index:4809
target Thresh 32.0
target distance 21.0
model initialize at round 4809
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  8.]), 'previousTarget': array([25.,  8.]), 'currentState': array([ 5.16759572, 14.78793221,  0.20591563]), 'targetState': array([25,  8], dtype=int32), 'currentDistance': 20.96187690317523}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9319426006691343
{'scaleFactor': 20, 'currentTarget': array([25.,  8.]), 'previousTarget': array([25.,  8.]), 'currentState': array([25.63480029,  7.3978635 ,  5.86496281]), 'targetState': array([25,  8], dtype=int32), 'currentDistance': 0.8749512952260741}
episode index:4810
target Thresh 32.0
target distance 5.0
model initialize at round 4810
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 23.]), 'previousTarget': array([27., 23.]), 'currentState': array([22.45385048, 29.62061958,  0.28774357]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 8.031194127834697}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9319485564806766
{'scaleFactor': 20, 'currentTarget': array([27., 23.]), 'previousTarget': array([27., 23.]), 'currentState': array([26.693976  , 23.54574155,  5.76841134]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 0.625687244429925}
episode index:4811
target Thresh 32.0
target distance 10.0
model initialize at round 4811
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  5.]), 'previousTarget': array([10.,  5.]), 'currentState': array([18.35981745,  5.62291957,  2.35756767]), 'targetState': array([10,  5], dtype=int32), 'currentDistance': 8.382993294965518}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9319545098168195
{'scaleFactor': 20, 'currentTarget': array([10.,  5.]), 'previousTarget': array([10.,  5.]), 'currentState': array([10.71094104,  4.98719236,  3.7300968 ]), 'targetState': array([10,  5], dtype=int32), 'currentDistance': 0.7110563975440318}
episode index:4812
target Thresh 32.0
target distance 17.0
model initialize at round 4812
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 24.]), 'previousTarget': array([24., 24.]), 'currentState': array([ 8.34868291, 27.00669881,  5.91439838]), 'targetState': array([24, 24], dtype=int32), 'currentDistance': 15.93750182327286}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9319525962877546
{'scaleFactor': 20, 'currentTarget': array([24., 24.]), 'previousTarget': array([24., 24.]), 'currentState': array([23.95410641, 24.26253506,  5.65990149]), 'targetState': array([24, 24], dtype=int32), 'currentDistance': 0.26651619024960926}
episode index:4813
target Thresh 32.0
target distance 22.0
model initialize at round 4813
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 23.]), 'previousTarget': array([25., 23.]), 'currentState': array([ 3.84157252, 18.45744426,  0.03713751]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 21.64056066863441}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9319449904834278
{'scaleFactor': 20, 'currentTarget': array([25., 23.]), 'previousTarget': array([25., 23.]), 'currentState': array([25.18024519, 23.10937215,  0.20271014]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 0.21083310179880174}
episode index:4814
target Thresh 32.0
target distance 13.0
model initialize at round 4814
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 25.]), 'previousTarget': array([27., 25.]), 'currentState': array([20.3419612 , 11.71139056,  2.30393267]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 14.863264162969514}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9319430797261993
{'scaleFactor': 20, 'currentTarget': array([27., 25.]), 'previousTarget': array([27., 25.]), 'currentState': array([27.37478286, 25.31806527,  1.02892959]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 0.49155641643416864}
episode index:4815
target Thresh 32.0
target distance 14.0
model initialize at round 4815
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 16.]), 'previousTarget': array([ 3., 16.]), 'currentState': array([16.59455612, 20.63340238,  2.82409942]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 14.362464057008813}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9319431051141106
{'scaleFactor': 20, 'currentTarget': array([ 3., 16.]), 'previousTarget': array([ 3., 16.]), 'currentState': array([ 3.57557851, 16.07913885,  3.87122721]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 0.5809936181941245}
episode index:4816
target Thresh 32.0
target distance 18.0
model initialize at round 4816
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 7.]), 'previousTarget': array([5., 7.]), 'currentState': array([16.23103029, 23.16690538,  4.12218395]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 19.685143405817126}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9319373834968995
{'scaleFactor': 20, 'currentTarget': array([5., 7.]), 'previousTarget': array([5., 7.]), 'currentState': array([4.82159033, 6.88900028, 4.48430195]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 0.21012126623069288}
episode index:4817
target Thresh 32.0
target distance 11.0
model initialize at round 4817
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([13.01018464,  8.6829392 ,  2.5747447 ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 12.390317341558404}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9319393641456967
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.69669398, 3.40907227, 3.37901209]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.8079125085518746}
episode index:4818
target Thresh 32.0
target distance 14.0
model initialize at round 4818
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 24.]), 'previousTarget': array([ 6., 24.]), 'currentState': array([18.34585919, 18.12371153,  2.57376125]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 13.673002793403914}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9319393902888303
{'scaleFactor': 20, 'currentTarget': array([ 6., 24.]), 'previousTarget': array([ 6., 24.]), 'currentState': array([ 5.7607    , 24.10968647,  2.98613053]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 0.26324059979314424}
episode index:4819
target Thresh 32.0
target distance 18.0
model initialize at round 4819
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 23.]), 'previousTarget': array([23., 23.]), 'currentState': array([6.81912303, 7.81128985, 0.57770008]), 'targetState': array([23, 23], dtype=int32), 'currentDistance': 22.192739705881436}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9319317966921435
{'scaleFactor': 20, 'currentTarget': array([23., 23.]), 'previousTarget': array([23., 23.]), 'currentState': array([22.69435755, 22.84646173,  1.1568908 ]), 'targetState': array([23, 23], dtype=int32), 'currentDistance': 0.3420399224853933}
episode index:4820
target Thresh 32.0
target distance 15.0
model initialize at round 4820
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 25.]), 'previousTarget': array([ 4., 25.]), 'currentState': array([19.18598932, 20.67266137,  2.47005758]), 'targetState': array([ 4, 25], dtype=int32), 'currentDistance': 15.79050762924251}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9319298910496909
{'scaleFactor': 20, 'currentTarget': array([ 4., 25.]), 'previousTarget': array([ 4., 25.]), 'currentState': array([ 3.89148567, 24.99578105,  2.84872108]), 'targetState': array([ 4, 25], dtype=int32), 'currentDistance': 0.108596317008823}
episode index:4821
target Thresh 32.0
target distance 4.0
model initialize at round 4821
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([16.64839228, 18.33939796,  1.21305865]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 3.71758253050699}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9319398807031438
{'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([15.76273992, 22.11065322,  1.61987906]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 0.2617947321086378}
episode index:4822
target Thresh 32.0
target distance 10.0
model initialize at round 4822
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 25.]), 'previousTarget': array([18., 25.]), 'currentState': array([ 9.15097773, 17.47451554,  0.27229464]), 'targetState': array([18, 25], dtype=int32), 'currentDistance': 11.616286476259338}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9319418587808337
{'scaleFactor': 20, 'currentTarget': array([18., 25.]), 'previousTarget': array([18., 25.]), 'currentState': array([18.25523266, 25.12886986,  0.82702225]), 'targetState': array([18, 25], dtype=int32), 'currentDistance': 0.2859215847994955}
episode index:4823
target Thresh 32.0
target distance 5.0
model initialize at round 4823
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([17.01251506,  2.6371377 ,  3.07543051]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 6.4429188525985355}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9319498100953484
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.0908929 ,  8.00656276,  1.48127612]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.9975866191837363}
episode index:4824
target Thresh 32.0
target distance 17.0
model initialize at round 4824
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([23.02892119,  9.82322996,  2.97704989]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 15.074921552641024}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9319479022993551
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.21472744, 11.40719039,  2.95789672]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.884566000850677}
episode index:4825
target Thresh 32.0
target distance 10.0
model initialize at round 4825
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 23.]), 'previousTarget': array([ 3., 23.]), 'currentState': array([10.53206528, 14.85882282,  2.3251791 ]), 'targetState': array([ 3, 23], dtype=int32), 'currentDistance': 11.091022187305812}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9319498774852444
{'scaleFactor': 20, 'currentTarget': array([ 3., 23.]), 'previousTarget': array([ 3., 23.]), 'currentState': array([ 2.42916943, 23.56974724,  1.96424995]), 'targetState': array([ 3, 23], dtype=int32), 'currentDistance': 0.8065106694200961}
episode index:4826
target Thresh 32.0
target distance 18.0
model initialize at round 4826
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 23.]), 'previousTarget': array([12., 23.]), 'currentState': array([19.30564077,  6.8835623 ,  1.94370866]), 'targetState': array([12, 23], dtype=int32), 'currentDistance': 17.69496965573331}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9319460588339078
{'scaleFactor': 20, 'currentTarget': array([12., 23.]), 'previousTarget': array([12., 23.]), 'currentState': array([12.11993109, 23.28284844,  2.0424902 ]), 'targetState': array([12, 23], dtype=int32), 'currentDistance': 0.307224197088134}
episode index:4827
target Thresh 32.0
target distance 10.0
model initialize at round 4827
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 21.]), 'previousTarget': array([10., 21.]), 'currentState': array([16.73292577, 12.54354427,  2.35399896]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 10.809437216269275}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9319500033225295
{'scaleFactor': 20, 'currentTarget': array([10., 21.]), 'previousTarget': array([10., 21.]), 'currentState': array([10.59643974, 20.32558933,  1.88641317]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 0.9003166760851172}
episode index:4828
target Thresh 32.0
target distance 16.0
model initialize at round 4828
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 27.]), 'previousTarget': array([18., 27.]), 'currentState': array([2.65952939, 9.45164311, 6.12506771]), 'targetState': array([18, 27], dtype=int32), 'currentDistance': 23.308257513566428}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.931940567594303
{'scaleFactor': 20, 'currentTarget': array([18., 27.]), 'previousTarget': array([18., 27.]), 'currentState': array([18.19493472, 27.11849738,  1.05445798]), 'targetState': array([18, 27], dtype=int32), 'currentDistance': 0.22812534469120396}
episode index:4829
target Thresh 32.0
target distance 10.0
model initialize at round 4829
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 18.]), 'previousTarget': array([14., 18.]), 'currentState': array([ 5.87124459, 14.69800303,  0.44980086]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 8.773816076579878}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9319465003981137
{'scaleFactor': 20, 'currentTarget': array([14., 18.]), 'previousTarget': array([14., 18.]), 'currentState': array([13.27476467, 17.61786059,  0.19148295]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 0.8197541165135757}
episode index:4830
target Thresh 32.0
target distance 24.0
model initialize at round 4830
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 29.]), 'previousTarget': array([22., 29.]), 'currentState': array([11.74529125,  6.66358394,  0.71272492]), 'targetState': array([22, 29], dtype=int32), 'currentDistance': 24.577927777614637}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9319352345158121
{'scaleFactor': 20, 'currentTarget': array([22., 29.]), 'previousTarget': array([22., 29.]), 'currentState': array([22.14764272, 29.32842887,  1.44229105]), 'targetState': array([22, 29], dtype=int32), 'currentDistance': 0.36008873652294315}
episode index:4831
target Thresh 32.0
target distance 12.0
model initialize at round 4831
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 25.]), 'previousTarget': array([14., 25.]), 'currentState': array([ 7.34661458, 12.68572479,  2.31946361]), 'targetState': array([14, 25], dtype=int32), 'currentDistance': 13.99674646253402}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9319352614432522
{'scaleFactor': 20, 'currentTarget': array([14., 25.]), 'previousTarget': array([14., 25.]), 'currentState': array([13.53230004, 24.5136681 ,  0.70102879]), 'targetState': array([14, 25], dtype=int32), 'currentDistance': 0.6747310365184891}
episode index:4832
target Thresh 32.0
target distance 8.0
model initialize at round 4832
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 22.]), 'previousTarget': array([21., 22.]), 'currentState': array([22.46020433, 15.59407586,  0.88733721]), 'targetState': array([21, 22], dtype=int32), 'currentDistance': 6.570240544925129}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9319431993159103
{'scaleFactor': 20, 'currentTarget': array([21., 22.]), 'previousTarget': array([21., 22.]), 'currentState': array([21.15341438, 21.17204642,  2.09306416]), 'targetState': array([21, 22], dtype=int32), 'currentDistance': 0.8420469720955304}
episode index:4833
target Thresh 32.0
target distance 14.0
model initialize at round 4833
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 29.]), 'previousTarget': array([24., 29.]), 'currentState': array([16.96863625, 13.62372536,  0.0524823 ]), 'targetState': array([24, 29], dtype=int32), 'currentDistance': 16.907687542737158}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9319393875757712
{'scaleFactor': 20, 'currentTarget': array([24., 29.]), 'previousTarget': array([24., 29.]), 'currentState': array([24.61027551, 29.2537858 ,  0.82590679]), 'targetState': array([24, 29], dtype=int32), 'currentDistance': 0.6609413239676991}
episode index:4834
target Thresh 32.0
target distance 21.0
model initialize at round 4834
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 26.]), 'previousTarget': array([19., 26.]), 'currentState': array([21.59623431,  4.46668574,  0.68755263]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 21.68926130194432}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9319318175378566
{'scaleFactor': 20, 'currentTarget': array([19., 26.]), 'previousTarget': array([19., 26.]), 'currentState': array([19.13618457, 25.95573446,  1.93987181]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 0.1431980325265613}
episode index:4835
target Thresh 32.0
target distance 15.0
model initialize at round 4835
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 22.]), 'previousTarget': array([ 7., 22.]), 'currentState': array([10.03601435,  8.96891785,  1.86001205]), 'targetState': array([ 7, 22], dtype=int32), 'currentDistance': 13.380077919443393}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9319318451495955
{'scaleFactor': 20, 'currentTarget': array([ 7., 22.]), 'previousTarget': array([ 7., 22.]), 'currentState': array([ 6.95932995, 22.55858701,  1.48677011]), 'targetState': array([ 7, 22], dtype=int32), 'currentDistance': 0.560065623803709}
episode index:4836
target Thresh 32.0
target distance 5.0
model initialize at round 4836
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([ 3.35153276, 11.99713048,  0.37162321]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 6.561160046209469}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.931939777164243
{'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([ 5.67432454, 17.2363924 ,  1.72974738]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 0.8301572571769948}
episode index:4837
target Thresh 32.0
target distance 18.0
model initialize at round 4837
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  4.]), 'previousTarget': array([12.,  4.]), 'currentState': array([18.75323903, 20.50975258,  4.43519944]), 'targetState': array([12,  4], dtype=int32), 'currentDistance': 17.83754937615878}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9319359692829531
{'scaleFactor': 20, 'currentTarget': array([12.,  4.]), 'previousTarget': array([12.,  4.]), 'currentState': array([12.05256833,  3.90427216,  4.62675977]), 'targetState': array([12,  4], dtype=int32), 'currentDistance': 0.1092119387983037}
episode index:4838
target Thresh 32.0
target distance 11.0
model initialize at round 4838
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([22.63825668, 20.64363313,  2.79743022]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 14.358696493057387}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9319359960195979
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.81691129, 11.31003355,  4.29869286]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.8737647571160553}
episode index:4839
target Thresh 32.0
target distance 14.0
model initialize at round 4839
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([26.77216871,  9.49537406,  2.10413738]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 16.504705046186835}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9319340969903434
{'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([16.33693183, 21.48964815,  1.92679621]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 0.6115407331285483}
episode index:4840
target Thresh 32.0
target distance 2.0
model initialize at round 4840
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 20.]), 'previousTarget': array([21., 20.]), 'currentState': array([21.4494394 , 19.16387852,  2.0628072 ]), 'targetState': array([21, 20], dtype=int32), 'currentDistance': 0.9492601880484726}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9319481572884243
{'scaleFactor': 20, 'currentTarget': array([21., 20.]), 'previousTarget': array([21., 20.]), 'currentState': array([21.4494394 , 19.16387852,  2.0628072 ]), 'targetState': array([21, 20], dtype=int32), 'currentDistance': 0.9492601880484726}
episode index:4841
target Thresh 32.0
target distance 2.0
model initialize at round 4841
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 27.]), 'previousTarget': array([21., 27.]), 'currentState': array([21.60494147, 27.56713995,  3.96389132]), 'targetState': array([21, 27], dtype=int32), 'currentDistance': 0.8292176507971616}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.9319622117788645
{'scaleFactor': 20, 'currentTarget': array([21., 27.]), 'previousTarget': array([21., 27.]), 'currentState': array([21.60494147, 27.56713995,  3.96389132]), 'targetState': array([21, 27], dtype=int32), 'currentDistance': 0.8292176507971616}
episode index:4842
target Thresh 32.0
target distance 14.0
model initialize at round 4842
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 27.]), 'previousTarget': array([ 5., 27.]), 'currentState': array([12.35205589, 13.34156742,  1.92721772]), 'targetState': array([ 5, 27], dtype=int32), 'currentDistance': 15.511463711732695}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9319603085128412
{'scaleFactor': 20, 'currentTarget': array([ 5., 27.]), 'previousTarget': array([ 5., 27.]), 'currentState': array([ 4.88661294, 27.37821424,  2.36069968]), 'targetState': array([ 5, 27], dtype=int32), 'currentDistance': 0.394845080773189}
episode index:4843
target Thresh 32.0
target distance 4.0
model initialize at round 4843
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 19.]), 'previousTarget': array([ 6., 19.]), 'currentState': array([ 8.23828343, 17.91309987,  2.44385809]), 'targetState': array([ 6, 19], dtype=int32), 'currentDistance': 2.488225192518194}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9319722902823471
{'scaleFactor': 20, 'currentTarget': array([ 6., 19.]), 'previousTarget': array([ 6., 19.]), 'currentState': array([ 6.39673113, 18.57975882,  3.15173972]), 'targetState': array([ 6, 19], dtype=int32), 'currentDistance': 0.577925807742476}
episode index:4844
target Thresh 32.0
target distance 11.0
model initialize at round 4844
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 17.]), 'previousTarget': array([12., 17.]), 'currentState': array([21.71835558, 11.44211562,  2.76492268]), 'targetState': array([12, 17], dtype=int32), 'currentDistance': 11.195379136918236}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.93197425268877
{'scaleFactor': 20, 'currentTarget': array([12., 17.]), 'previousTarget': array([12., 17.]), 'currentState': array([11.58523316, 17.64568561,  2.59637225]), 'targetState': array([12, 17], dtype=int32), 'currentDistance': 0.7674252003126251}
episode index:4845
target Thresh 32.0
target distance 7.0
model initialize at round 4845
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([13.11116649, 19.57088731,  3.14598751]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 8.174449248867154}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9319801589531759
{'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([ 7.32215037, 24.7088468 ,  2.89410195]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 0.4342246518242341}
episode index:4846
target Thresh 32.0
target distance 11.0
model initialize at round 4846
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 13.]), 'previousTarget': array([13., 13.]), 'currentState': array([ 4.43526845, 22.63509624,  5.27917239]), 'targetState': array([13, 13], dtype=int32), 'currentDistance': 12.89145865156847}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9319821189264477
{'scaleFactor': 20, 'currentTarget': array([13., 13.]), 'previousTarget': array([13., 13.]), 'currentState': array([12.3610629 , 13.67580187,  5.51939379]), 'targetState': array([13, 13], dtype=int32), 'currentDistance': 0.9300262340369114}
episode index:4847
target Thresh 32.0
target distance 19.0
model initialize at round 4847
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 26.]), 'previousTarget': array([ 8., 26.]), 'currentState': array([25.32648452, 15.17814048,  2.02554512]), 'targetState': array([ 8, 26], dtype=int32), 'currentDistance': 20.428404472860542}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9319764258480817
{'scaleFactor': 20, 'currentTarget': array([ 8., 26.]), 'previousTarget': array([ 8., 26.]), 'currentState': array([ 8.46563335, 25.55857497,  2.99301297]), 'targetState': array([ 8, 26], dtype=int32), 'currentDistance': 0.6416155142880801}
episode index:4848
target Thresh 32.0
target distance 13.0
model initialize at round 4848
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 12.]), 'previousTarget': array([19., 12.]), 'currentState': array([ 7.15702742, 25.22216023,  6.0859527 ]), 'targetState': array([19, 12], dtype=int32), 'currentDistance': 17.750535781644484}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9319726190470167
{'scaleFactor': 20, 'currentTarget': array([19., 12.]), 'previousTarget': array([19., 12.]), 'currentState': array([19.02730141, 11.96229501,  5.1702756 ]), 'targetState': array([19, 12], dtype=int32), 'currentDistance': 0.04655140793363176}
episode index:4849
target Thresh 32.0
target distance 9.0
model initialize at round 4849
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 2.]), 'previousTarget': array([9., 2.]), 'currentState': array([16.56152487,  6.75858871,  3.29636407]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 8.934250107277954}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.93197852077711
{'scaleFactor': 20, 'currentTarget': array([9., 2.]), 'previousTarget': array([9., 2.]), 'currentState': array([9.90778485, 2.4933297 , 3.98092747]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 1.033173525882669}
episode index:4850
target Thresh 32.0
target distance 7.0
model initialize at round 4850
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 19.]), 'previousTarget': array([26., 19.]), 'currentState': array([24.61491686, 12.47374213,  1.29534834]), 'targetState': array([26, 19], dtype=int32), 'currentDistance': 6.67161877157532}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9319864202780835
{'scaleFactor': 20, 'currentTarget': array([26., 19.]), 'previousTarget': array([26., 19.]), 'currentState': array([25.74183372, 18.3201387 ,  1.68739144]), 'targetState': array([26, 19], dtype=int32), 'currentDistance': 0.7272284470251962}
episode index:4851
target Thresh 32.0
target distance 24.0
model initialize at round 4851
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 26.]), 'previousTarget': array([15., 26.]), 'currentState': array([11.31745557,  2.03784588,  2.1091032 ]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 24.24347259650018}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9319770217726092
{'scaleFactor': 20, 'currentTarget': array([15., 26.]), 'previousTarget': array([15., 26.]), 'currentState': array([14.92766343, 25.48523358,  1.69242809]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 0.5198240540231037}
episode index:4852
target Thresh 32.0
target distance 12.0
model initialize at round 4852
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([10.50401194,  1.24479395,  0.54465168]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 11.522922805335538}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.931978979969112
{'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([21.42674354,  6.01993725,  0.455648  ]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 0.42720901205920825}
episode index:4853
target Thresh 32.0
target distance 13.0
model initialize at round 4853
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([17.93800209,  3.39733323,  1.98960656]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 11.784469080579434}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9319809373587767
{'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([19.8962531 , 15.06041744,  1.45006597]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 0.1200570119064549}
episode index:4854
target Thresh 32.0
target distance 15.0
model initialize at round 4854
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 25.]), 'previousTarget': array([26., 25.]), 'currentState': array([10.95083376, 19.6822517 ,  0.59001446]), 'targetState': array([26, 25], dtype=int32), 'currentDistance': 15.961073009590706}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9319790349400474
{'scaleFactor': 20, 'currentTarget': array([26., 25.]), 'previousTarget': array([26., 25.]), 'currentState': array([25.91584085, 25.09570346,  0.5270723 ]), 'targetState': array([26, 25], dtype=int32), 'currentDistance': 0.12744377356720904}
episode index:4855
target Thresh 32.0
target distance 19.0
model initialize at round 4855
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 15.]), 'previousTarget': array([ 6., 15.]), 'currentState': array([23.82850158,  4.20829613,  3.35073316]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 20.840257702048778}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9319714894745035
{'scaleFactor': 20, 'currentTarget': array([ 6., 15.]), 'previousTarget': array([ 6., 15.]), 'currentState': array([ 5.38920383, 15.63298933,  2.67537776]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 0.8796291592815352}
episode index:4856
target Thresh 32.0
target distance 14.0
model initialize at round 4856
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 20.]), 'previousTarget': array([22., 20.]), 'currentState': array([12.68200242,  5.94293923,  0.9760887 ]), 'targetState': array([22, 20], dtype=int32), 'currentDistance': 16.864935107904376}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9319695897843558
{'scaleFactor': 20, 'currentTarget': array([22., 20.]), 'previousTarget': array([22., 20.]), 'currentState': array([21.42531425, 19.28676319,  1.18107101]), 'targetState': array([22, 20], dtype=int32), 'currentDistance': 0.915953311496966}
episode index:4857
target Thresh 32.0
target distance 18.0
model initialize at round 4857
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 29.]), 'previousTarget': array([ 5., 29.]), 'currentState': array([20.32604655, 11.98919789,  2.19355363]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 22.89661746608586}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9319620493694679
{'scaleFactor': 20, 'currentTarget': array([ 5., 29.]), 'previousTarget': array([ 5., 29.]), 'currentState': array([ 5.60259036, 28.19584674,  2.25831443]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 1.004876910224208}
episode index:4858
target Thresh 32.0
target distance 19.0
model initialize at round 4858
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 28.]), 'previousTarget': array([ 4., 28.]), 'currentState': array([21.14203393, 24.82916776,  2.585783  ]), 'targetState': array([ 4, 28], dtype=int32), 'currentDistance': 17.43282835197724}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9319582533616708
{'scaleFactor': 20, 'currentTarget': array([ 4., 28.]), 'previousTarget': array([ 4., 28.]), 'currentState': array([ 3.58422254, 28.09876953,  3.1697308 ]), 'targetState': array([ 4, 28], dtype=int32), 'currentDistance': 0.4273480048732202}
episode index:4859
target Thresh 32.0
target distance 12.0
model initialize at round 4859
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 23.]), 'previousTarget': array([17., 23.]), 'currentState': array([18.47487672, 10.28839121,  2.56815767]), 'targetState': array([17, 23], dtype=int32), 'currentDistance': 12.79688475167025}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9319582753975856
{'scaleFactor': 20, 'currentTarget': array([17., 23.]), 'previousTarget': array([17., 23.]), 'currentState': array([16.77702362, 23.88487625,  1.63282036]), 'targetState': array([17, 23], dtype=int32), 'currentDistance': 0.9125373623186072}
episode index:4860
target Thresh 32.0
target distance 18.0
model initialize at round 4860
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 29.]), 'previousTarget': array([ 9., 29.]), 'currentState': array([21.26074816, 12.51191759,  3.03556478]), 'targetState': array([ 9, 29], dtype=int32), 'currentDistance': 20.547087554639184}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9319526024495524
{'scaleFactor': 20, 'currentTarget': array([ 9., 29.]), 'previousTarget': array([ 9., 29.]), 'currentState': array([ 9.3934561 , 28.1704872 ,  2.47115494]), 'targetState': array([ 9, 29], dtype=int32), 'currentDistance': 0.9180954082630703}
episode index:4861
target Thresh 32.0
target distance 8.0
model initialize at round 4861
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 20.]), 'previousTarget': array([22., 20.]), 'currentState': array([15.37942466, 24.58900447,  5.77317488]), 'targetState': array([22, 20], dtype=int32), 'currentDistance': 8.055493769445777}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9319584937304142
{'scaleFactor': 20, 'currentTarget': array([22., 20.]), 'previousTarget': array([22., 20.]), 'currentState': array([22.0282909 , 20.19185444,  5.7382985 ]), 'targetState': array([22, 20], dtype=int32), 'currentDistance': 0.19392910964546234}
episode index:4862
target Thresh 32.0
target distance 23.0
model initialize at round 4862
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 28.]), 'previousTarget': array([15., 28.]), 'currentState': array([26.50137595,  5.760433  ,  1.4788267 ]), 'targetState': array([15, 28], dtype=int32), 'currentDistance': 25.037571552706332}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.931947299514759
{'scaleFactor': 20, 'currentTarget': array([15., 28.]), 'previousTarget': array([15., 28.]), 'currentState': array([14.41571552, 28.47840524,  2.02413363]), 'targetState': array([15, 28], dtype=int32), 'currentDistance': 0.7551555626418324}
episode index:4863
target Thresh 32.0
target distance 4.0
model initialize at round 4863
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 20.]), 'previousTarget': array([12., 20.]), 'currentState': array([14.27106591, 17.1030773 ,  1.72475832]), 'targetState': array([12, 20], dtype=int32), 'currentDistance': 3.6810190844395514}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9319571993298258
{'scaleFactor': 20, 'currentTarget': array([12., 20.]), 'previousTarget': array([12., 20.]), 'currentState': array([11.76326819, 20.08913304,  2.33630284]), 'targetState': array([12, 20], dtype=int32), 'currentDistance': 0.2529558252526577}
episode index:4864
target Thresh 32.0
target distance 13.0
model initialize at round 4864
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 17.]), 'previousTarget': array([26., 17.]), 'currentState': array([20.50773251,  3.25224954,  0.54960268]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 14.804244149973643}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9319572215597491
{'scaleFactor': 20, 'currentTarget': array([26., 17.]), 'previousTarget': array([26., 17.]), 'currentState': array([25.52246828, 16.06018103,  1.64365889]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 1.0541803674040797}
episode index:4865
target Thresh 32.0
target distance 13.0
model initialize at round 4865
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([11.29184988, 21.92133788,  4.57747907]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 11.99112892928971}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9319591785938308
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.1034468, 10.0325955,  4.6531179]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.10846062415417719}
episode index:4866
target Thresh 32.0
target distance 16.0
model initialize at round 4866
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 22.]), 'previousTarget': array([ 3., 22.]), 'currentState': array([17.49458564, 27.82433814,  3.27644598]), 'targetState': array([ 3, 22], dtype=int32), 'currentDistance': 15.621009174771556}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9319572853363486
{'scaleFactor': 20, 'currentTarget': array([ 3., 22.]), 'previousTarget': array([ 3., 22.]), 'currentState': array([ 2.82820542, 21.80369385,  3.38872052]), 'targetState': array([ 3, 22], dtype=int32), 'currentDistance': 0.2608629520018684}
episode index:4867
target Thresh 32.0
target distance 23.0
model initialize at round 4867
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 29.]), 'previousTarget': array([20., 29.]), 'currentState': array([6.38870598, 7.93835245, 1.10677713]), 'targetState': array([20, 29], dtype=int32), 'currentDistance': 25.077087595163974}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9319461028666819
{'scaleFactor': 20, 'currentTarget': array([20., 29.]), 'previousTarget': array([20., 29.]), 'currentState': array([20.16045712, 29.70714948,  0.96547071]), 'targetState': array([20, 29], dtype=int32), 'currentDistance': 0.7251254158644135}
episode index:4868
target Thresh 32.0
target distance 10.0
model initialize at round 4868
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 27.]), 'previousTarget': array([22., 27.]), 'currentState': array([25.58463438, 18.93299953,  2.0475226 ]), 'targetState': array([22, 27], dtype=int32), 'currentDistance': 8.827576124038359}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9319519870127351
{'scaleFactor': 20, 'currentTarget': array([22., 27.]), 'previousTarget': array([22., 27.]), 'currentState': array([22.27721946, 26.18375358,  2.2177506 ]), 'targetState': array([22, 27], dtype=int32), 'currentDistance': 0.8620376160617582}
episode index:4869
target Thresh 32.0
target distance 14.0
model initialize at round 4869
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 28.]), 'previousTarget': array([11., 28.]), 'currentState': array([11.74394243, 15.9444442 ,  1.35625696]), 'targetState': array([11, 28], dtype=int32), 'currentDistance': 12.078488149382665}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9319539435142523
{'scaleFactor': 20, 'currentTarget': array([11., 28.]), 'previousTarget': array([11., 28.]), 'currentState': array([11.00111207, 27.82915928,  1.32849489]), 'targetState': array([11, 28], dtype=int32), 'currentDistance': 0.17084433568713966}
episode index:4870
target Thresh 32.0
target distance 12.0
model initialize at round 4870
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 6.]), 'previousTarget': array([5., 6.]), 'currentState': array([16.65404822, 14.64702928,  2.7878325 ]), 'targetState': array([5, 6], dtype=int32), 'currentDistance': 14.511648954911548}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9319539663852013
{'scaleFactor': 20, 'currentTarget': array([5., 6.]), 'previousTarget': array([5., 6.]), 'currentState': array([5.81140973, 6.41811018, 4.19708656]), 'targetState': array([5, 6], dtype=int32), 'currentDistance': 0.9127989218083611}
episode index:4871
target Thresh 32.0
target distance 23.0
model initialize at round 4871
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  3.]), 'previousTarget': array([17.,  3.]), 'currentState': array([19.31898895, 26.08117841,  4.10333872]), 'targetState': array([17,  3], dtype=int32), 'currentDistance': 23.19738146320673}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9319446131227488
{'scaleFactor': 20, 'currentTarget': array([17.,  3.]), 'previousTarget': array([17.,  3.]), 'currentState': array([17.15979374,  2.36258852,  4.64267884]), 'targetState': array([17,  3], dtype=int32), 'currentDistance': 0.6571357855013172}
episode index:4872
target Thresh 32.0
target distance 19.0
model initialize at round 4872
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([11.6079703 ,  3.94585485,  1.5347724 ]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 18.058400939269276}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9319408315989156
{'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([11.92542867, 21.81958793,  2.00285899]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 0.19521628620445242}
episode index:4873
target Thresh 32.0
target distance 11.0
model initialize at round 4873
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 27.]), 'previousTarget': array([16., 27.]), 'currentState': array([27.61449307, 26.43322422,  4.07615519]), 'targetState': array([16, 27], dtype=int32), 'currentDistance': 11.628313899562208}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9319427887835283
{'scaleFactor': 20, 'currentTarget': array([16., 27.]), 'previousTarget': array([16., 27.]), 'currentState': array([15.96637809, 26.9527781 ,  3.43762795]), 'targetState': array([16, 27], dtype=int32), 'currentDistance': 0.05796844112366858}
episode index:4874
target Thresh 32.0
target distance 24.0
model initialize at round 4874
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 26.]), 'previousTarget': array([13., 26.]), 'currentState': array([6.32918241, 2.20188281, 2.01134682]), 'targetState': array([13, 26], dtype=int32), 'currentDistance': 24.71538365265663}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9319334435697709
{'scaleFactor': 20, 'currentTarget': array([13., 26.]), 'previousTarget': array([13., 26.]), 'currentState': array([12.56242369, 25.09681963,  1.58130843]), 'targetState': array([13, 26], dtype=int32), 'currentDistance': 1.0035974322908836}
episode index:4875
target Thresh 32.0
target distance 12.0
model initialize at round 4875
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 14.]), 'previousTarget': array([ 3., 14.]), 'currentState': array([ 3.05771968, 24.05280715,  4.34170043]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 10.052972847734198}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9319373518155318
{'scaleFactor': 20, 'currentTarget': array([ 3., 14.]), 'previousTarget': array([ 3., 14.]), 'currentState': array([ 2.9760256 , 14.16219957,  5.16752072]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 0.16396179780437758}
episode index:4876
target Thresh 32.0
target distance 13.0
model initialize at round 4876
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 17.]), 'previousTarget': array([17., 17.]), 'currentState': array([ 4.57709611, 19.58093269,  0.2107904 ]), 'targetState': array([17, 17], dtype=int32), 'currentDistance': 12.688173812826403}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9319393085097262
{'scaleFactor': 20, 'currentTarget': array([17., 17.]), 'previousTarget': array([17., 17.]), 'currentState': array([16.1781415 , 17.21199869,  0.23709402]), 'targetState': array([17, 17], dtype=int32), 'currentDistance': 0.8487607668396188}
episode index:4877
target Thresh 32.0
target distance 13.0
model initialize at round 4877
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 13.]), 'previousTarget': array([11., 13.]), 'currentState': array([22.44601174, 23.35385097,  4.54564548]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 15.434163878316165}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9319374235949902
{'scaleFactor': 20, 'currentTarget': array([11., 13.]), 'previousTarget': array([11., 13.]), 'currentState': array([10.82496949, 12.75276933,  4.08892187]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 0.30291696359561343}
episode index:4878
target Thresh 32.0
target distance 4.0
model initialize at round 4878
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 13.]), 'previousTarget': array([15., 13.]), 'currentState': array([16.67262623, 18.03463373,  3.48950267]), 'targetState': array([15, 13], dtype=int32), 'currentDistance': 5.305206431887344}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9319452861849481
{'scaleFactor': 20, 'currentTarget': array([15., 13.]), 'previousTarget': array([15., 13.]), 'currentState': array([15.07134158, 12.59856622,  4.8830191 ]), 'targetState': array([15, 13], dtype=int32), 'currentDistance': 0.407723805468513}
episode index:4879
target Thresh 32.0
target distance 9.0
model initialize at round 4879
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 23.]), 'previousTarget': array([20., 23.]), 'currentState': array([10.05674476, 19.39379254,  1.15575027]), 'targetState': array([20, 23], dtype=int32), 'currentDistance': 10.577006055968903}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9319491888004634
{'scaleFactor': 20, 'currentTarget': array([20., 23.]), 'previousTarget': array([20., 23.]), 'currentState': array([19.1669965 , 22.84665537,  0.07445896]), 'targetState': array([20, 23], dtype=int32), 'currentDistance': 0.8470002406568771}
episode index:4880
target Thresh 32.0
target distance 5.0
model initialize at round 4880
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 24.]), 'previousTarget': array([22., 24.]), 'currentState': array([15.97871477, 20.33767132,  1.21286893]), 'targetState': array([22, 24], dtype=int32), 'currentDistance': 7.047590166121149}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9319550578480355
{'scaleFactor': 20, 'currentTarget': array([22., 24.]), 'previousTarget': array([22., 24.]), 'currentState': array([22.72821429, 24.22633951,  0.45171295]), 'targetState': array([22, 24], dtype=int32), 'currentDistance': 0.7625782740731054}
episode index:4881
target Thresh 32.0
target distance 26.0
model initialize at round 4881
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  3.]), 'previousTarget': array([26.9916736 ,  3.01443242]), 'currentState': array([13.41709091, 27.73736159,  6.1184535 ]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 28.22113530283914}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.9319403309526634
{'scaleFactor': 20, 'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([26.81258176,  2.09908476,  4.98988522]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 0.9202031640341864}
episode index:4882
target Thresh 32.0
target distance 15.0
model initialize at round 4882
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([13.68181924, 24.06222653,  5.31016797]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 15.239574471750634}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9319384477586178
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.71421161,  8.43832606,  4.82634947]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.6302004637813784}
episode index:4883
target Thresh 32.0
target distance 7.0
model initialize at round 4883
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 17.]), 'previousTarget': array([22., 17.]), 'currentState': array([17.62079473, 24.4532246 ,  5.54585189]), 'targetState': array([22, 17], dtype=int32), 'currentDistance': 8.644535602369087}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9319443154003543
{'scaleFactor': 20, 'currentTarget': array([22., 17.]), 'previousTarget': array([22., 17.]), 'currentState': array([21.62188977, 17.63737048,  5.56385729]), 'targetState': array([22, 17], dtype=int32), 'currentDistance': 0.7410860123122994}
episode index:4884
target Thresh 32.0
target distance 16.0
model initialize at round 4884
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 23.]), 'previousTarget': array([22., 23.]), 'currentState': array([ 7.57222732, 17.11555247,  0.14438736]), 'targetState': array([22, 23], dtype=int32), 'currentDistance': 15.581634931651948}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.93194243216167
{'scaleFactor': 20, 'currentTarget': array([22., 23.]), 'previousTarget': array([22., 23.]), 'currentState': array([22.25051854, 23.10774063,  0.12543967]), 'targetState': array([22, 23], dtype=int32), 'currentDistance': 0.2727042014314695}
episode index:4885
target Thresh 32.0
target distance 7.0
model initialize at round 4885
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 18.]), 'previousTarget': array([23., 18.]), 'currentState': array([17.86525212, 16.69113224,  0.53698808]), 'targetState': array([23, 18], dtype=int32), 'currentDistance': 5.298940515094854}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9319502824620871
{'scaleFactor': 20, 'currentTarget': array([23., 18.]), 'previousTarget': array([23., 18.]), 'currentState': array([23.58288207, 18.26751833,  0.03175879]), 'targetState': array([23, 18], dtype=int32), 'currentDistance': 0.6413404413354399}
episode index:4886
target Thresh 32.0
target distance 13.0
model initialize at round 4886
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 15.]), 'previousTarget': array([ 7., 15.]), 'currentState': array([13.09902522,  3.77071051,  2.2430625 ]), 'targetState': array([ 7, 15], dtype=int32), 'currentDistance': 12.778695199555026}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9319522325064782
{'scaleFactor': 20, 'currentTarget': array([ 7., 15.]), 'previousTarget': array([ 7., 15.]), 'currentState': array([ 7.20940717, 14.07390016,  2.17035878]), 'targetState': array([ 7, 15], dtype=int32), 'currentDistance': 0.9494800014753275}
episode index:4887
target Thresh 32.0
target distance 21.0
model initialize at round 4887
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 22.]), 'previousTarget': array([ 6., 22.]), 'currentState': array([26.35725682,  3.44459996,  3.31052685]), 'targetState': array([ 6, 22], dtype=int32), 'currentDistance': 27.544886568170327}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9319393015695433
{'scaleFactor': 20, 'currentTarget': array([ 6., 22.]), 'previousTarget': array([ 6., 22.]), 'currentState': array([ 6.57110303, 21.21929235,  2.12743824]), 'targetState': array([ 6, 22], dtype=int32), 'currentDistance': 0.9672968003475575}
episode index:4888
target Thresh 32.0
target distance 11.0
model initialize at round 4888
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 17.]), 'previousTarget': array([12., 17.]), 'currentState': array([23.74868885, 15.50726676,  2.11977653]), 'targetState': array([12, 17], dtype=int32), 'currentDistance': 11.843139031075335}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9319412530622477
{'scaleFactor': 20, 'currentTarget': array([12., 17.]), 'previousTarget': array([12., 17.]), 'currentState': array([12.19815739, 16.92591245,  3.33557664]), 'targetState': array([12, 17], dtype=int32), 'currentDistance': 0.21155452040180514}
episode index:4889
target Thresh 32.0
target distance 13.0
model initialize at round 4889
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 15.]), 'previousTarget': array([ 8., 15.]), 'currentState': array([18.44063734,  3.58729377,  2.91960782]), 'targetState': array([ 8, 15], dtype=int32), 'currentDistance': 15.467927192710487}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9319393723754104
{'scaleFactor': 20, 'currentTarget': array([ 8., 15.]), 'previousTarget': array([ 8., 15.]), 'currentState': array([ 7.72983957, 15.27843281,  2.25590467]), 'targetState': array([ 8, 15], dtype=int32), 'currentDistance': 0.3879580994695662}
episode index:4890
target Thresh 32.0
target distance 6.0
model initialize at round 4890
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 15.]), 'previousTarget': array([ 8., 15.]), 'currentState': array([ 3.52222504, 19.7047372 ,  5.64080681]), 'targetState': array([ 8, 15], dtype=int32), 'currentDistance': 6.494999670075653}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9319472152761719
{'scaleFactor': 20, 'currentTarget': array([ 8., 15.]), 'previousTarget': array([ 8., 15.]), 'currentState': array([ 7.56219372, 15.29230966,  5.36304714]), 'targetState': array([ 8, 15], dtype=int32), 'currentDistance': 0.5264211934861767}
episode index:4891
target Thresh 32.0
target distance 13.0
model initialize at round 4891
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 21.]), 'previousTarget': array([ 2., 21.]), 'currentState': array([3.35474576, 8.35429727, 1.91948676]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 12.718063277001507}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9319491639544476
{'scaleFactor': 20, 'currentTarget': array([ 2., 21.]), 'previousTarget': array([ 2., 21.]), 'currentState': array([ 2.36188184, 20.21852293,  2.02424157]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 0.8611996714679746}
episode index:4892
target Thresh 32.0
target distance 26.0
model initialize at round 4892
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  2.]), 'previousTarget': array([20.,  2.]), 'currentState': array([20.57217327, 28.89089781,  3.59374607]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 26.896984356543683}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.9319344713713057
{'scaleFactor': 20, 'currentTarget': array([20.,  2.]), 'previousTarget': array([20.,  2.]), 'currentState': array([20.318172  ,  1.08248914,  5.26914698]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 0.9711125575496876}
episode index:4893
target Thresh 32.0
target distance 11.0
model initialize at round 4893
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 25.]), 'previousTarget': array([11., 25.]), 'currentState': array([21.93248315, 26.31838484,  3.66226053]), 'targetState': array([11, 25], dtype=int32), 'currentDistance': 11.011690441303097}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9319364218572129
{'scaleFactor': 20, 'currentTarget': array([11., 25.]), 'previousTarget': array([11., 25.]), 'currentState': array([10.23694003, 24.4770774 ,  3.79143347]), 'targetState': array([11, 25], dtype=int32), 'currentDistance': 0.9250451709338066}
episode index:4894
target Thresh 32.0
target distance 9.0
model initialize at round 4894
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  4.]), 'previousTarget': array([10.,  4.]), 'currentState': array([18.84209987,  3.32445363,  3.60842848]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 8.867868572174412}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9319422767271093
{'scaleFactor': 20, 'currentTarget': array([10.,  4.]), 'previousTarget': array([10.,  4.]), 'currentState': array([10.96234954,  3.77798862,  3.19918963]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 0.9876262894199164}
episode index:4895
target Thresh 32.0
target distance 22.0
model initialize at round 4895
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 26.]), 'previousTarget': array([ 3., 26.]), 'currentState': array([20.18890856,  4.6303532 ,  3.31182873]), 'targetState': array([ 3, 26], dtype=int32), 'currentDistance': 27.424813254688367}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.9319223771120608
{'scaleFactor': 20, 'currentTarget': array([ 3., 26.]), 'previousTarget': array([ 3., 26.]), 'currentState': array([ 2.69462299, 25.75535216,  1.61639331]), 'targetState': array([ 3, 26], dtype=int32), 'currentDistance': 0.391289767121512}
episode index:4896
target Thresh 32.0
target distance 3.0
model initialize at round 4896
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 17.]), 'previousTarget': array([23., 17.]), 'currentState': array([22.84424115, 18.19924368,  4.96578059]), 'targetState': array([23, 17], dtype=int32), 'currentDistance': 1.209316424448549}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9319342369492852
{'scaleFactor': 20, 'currentTarget': array([23., 17.]), 'previousTarget': array([23., 17.]), 'currentState': array([23.42260874, 16.28526669,  5.04671989]), 'targetState': array([23, 17], dtype=int32), 'currentDistance': 0.8303263518191336}
episode index:4897
target Thresh 32.0
target distance 25.0
model initialize at round 4897
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 28.]), 'previousTarget': array([ 3., 28.]), 'currentState': array([2.18763203, 4.65790052, 2.72058046]), 'targetState': array([ 3, 28], dtype=int32), 'currentDistance': 23.35623149843782}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.9319178064744272
{'scaleFactor': 20, 'currentTarget': array([ 3., 28.]), 'previousTarget': array([ 3., 28.]), 'currentState': array([ 2.98642233, 28.26612847,  0.84243107]), 'targetState': array([ 3, 28], dtype=int32), 'currentDistance': 0.26647460360689584}
episode index:4898
target Thresh 32.0
target distance 21.0
model initialize at round 4898
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 26.]), 'previousTarget': array([ 6., 26.]), 'currentState': array([23.07775638,  6.29260561,  1.885791  ]), 'targetState': array([ 6, 26], dtype=int32), 'currentDistance': 26.07740701419482}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.9318996446836351
{'scaleFactor': 20, 'currentTarget': array([ 6., 26.]), 'previousTarget': array([ 6., 26.]), 'currentState': array([ 5.87442298, 26.15815597,  1.40052059]), 'targetState': array([ 6, 26], dtype=int32), 'currentDistance': 0.20194776702977013}
episode index:4899
target Thresh 32.0
target distance 16.0
model initialize at round 4899
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  4.]), 'previousTarget': array([10.,  4.]), 'currentState': array([ 5.05816942, 18.60524438,  5.12845683]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 15.418652753006812}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.93189777632644
{'scaleFactor': 20, 'currentTarget': array([10.,  4.]), 'previousTarget': array([10.,  4.]), 'currentState': array([9.68738774, 3.53675267, 5.18194899]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 0.5588600138330134}
episode index:4900
target Thresh 32.0
target distance 14.0
model initialize at round 4900
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 28.]), 'previousTarget': array([22., 28.]), 'currentState': array([ 7.50435272, 15.6083289 ,  0.85973644]), 'targetState': array([22, 28], dtype=int32), 'currentDistance': 19.070325188749642}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9318921620229677
{'scaleFactor': 20, 'currentTarget': array([22., 28.]), 'previousTarget': array([22., 28.]), 'currentState': array([22.2453492 , 28.52033691,  0.63907392]), 'targetState': array([22, 28], dtype=int32), 'currentDistance': 0.5752796967526874}
episode index:4901
target Thresh 32.0
target distance 17.0
model initialize at round 4901
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  7.]), 'previousTarget': array([22.,  7.]), 'currentState': array([22.3702692 , 23.58004097,  4.40379214]), 'targetState': array([22,  7], dtype=int32), 'currentDistance': 16.58417491765301}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9318884135703892
{'scaleFactor': 20, 'currentTarget': array([22.,  7.]), 'previousTarget': array([22.,  7.]), 'currentState': array([22.52313114,  6.33444998,  5.78244931]), 'targetState': array([22,  7], dtype=int32), 'currentDistance': 0.846535889582856}
episode index:4902
target Thresh 32.0
target distance 11.0
model initialize at round 4902
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 12.]), 'previousTarget': array([16., 12.]), 'currentState': array([25.2061774 , 17.2039219 ,  3.89885491]), 'targetState': array([16, 12], dtype=int32), 'currentDistance': 10.575183466764559}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.931892309478268
{'scaleFactor': 20, 'currentTarget': array([16., 12.]), 'previousTarget': array([16., 12.]), 'currentState': array([16.73297442, 12.57961768,  3.33879405]), 'targetState': array([16, 12], dtype=int32), 'currentDistance': 0.9344560742393494}
episode index:4903
target Thresh 32.0
target distance 10.0
model initialize at round 4903
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 19.]), 'previousTarget': array([27., 19.]), 'currentState': array([18.06905234,  7.70018649,  0.12749403]), 'targetState': array([27, 19], dtype=int32), 'currentDistance': 14.403041744185343}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.931890444140778
{'scaleFactor': 20, 'currentTarget': array([27., 19.]), 'previousTarget': array([27., 19.]), 'currentState': array([26.99901672, 19.78328555,  0.93686157]), 'targetState': array([27, 19], dtype=int32), 'currentDistance': 0.783286168754041}
episode index:4904
target Thresh 32.0
target distance 16.0
model initialize at round 4904
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 20.]), 'previousTarget': array([13., 20.]), 'currentState': array([12.72937757,  5.10358812,  1.41643238]), 'targetState': array([13, 20], dtype=int32), 'currentDistance': 14.898869873036354}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9318885795638743
{'scaleFactor': 20, 'currentTarget': array([13., 20.]), 'previousTarget': array([13., 20.]), 'currentState': array([13.09075709, 20.17459481,  2.05061108]), 'targetState': array([13, 20], dtype=int32), 'currentDistance': 0.19677448499289457}
episode index:4905
target Thresh 32.0
target distance 12.0
model initialize at round 4905
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 24.]), 'previousTarget': array([21., 24.]), 'currentState': array([10.12859521, 18.75153657,  0.1742174 ]), 'targetState': array([21, 24], dtype=int32), 'currentDistance': 12.072025942389706}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.931890534633144
{'scaleFactor': 20, 'currentTarget': array([21., 24.]), 'previousTarget': array([21., 24.]), 'currentState': array([20.62717319, 23.73052149,  0.27612498]), 'targetState': array([21, 24], dtype=int32), 'currentDistance': 0.46002010410550326}
episode index:4906
target Thresh 32.0
target distance 8.0
model initialize at round 4906
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 16.]), 'previousTarget': array([14., 16.]), 'currentState': array([20.06194166, 19.55501   ,  3.17942154]), 'targetState': array([14, 16], dtype=int32), 'currentDistance': 7.027462760291079}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9318983619136344
{'scaleFactor': 20, 'currentTarget': array([14., 16.]), 'previousTarget': array([14., 16.]), 'currentState': array([14.90834594, 16.95333828,  4.24759901]), 'targetState': array([14, 16], dtype=int32), 'currentDistance': 1.3167939128888448}
episode index:4907
target Thresh 32.0
target distance 10.0
model initialize at round 4907
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 12.]), 'previousTarget': array([ 8., 12.]), 'currentState': array([17.79572725, 11.67052708,  2.70247269]), 'targetState': array([ 8, 12], dtype=int32), 'currentDistance': 9.801266491356124}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.931902251825612
{'scaleFactor': 20, 'currentTarget': array([ 8., 12.]), 'previousTarget': array([ 8., 12.]), 'currentState': array([ 8.2281633 , 12.3860689 ,  3.78964114]), 'targetState': array([ 8, 12], dtype=int32), 'currentDistance': 0.4484503170041791}
episode index:4908
target Thresh 32.0
target distance 7.0
model initialize at round 4908
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([13.31704029, 24.99411072,  4.155092  ]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 7.4910518305743174}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9319080969586685
{'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([15.93368831, 17.97418883,  5.21118885]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 0.07115797445112088}
episode index:4909
target Thresh 32.0
target distance 6.0
model initialize at round 4909
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 23.]), 'previousTarget': array([23., 23.]), 'currentState': array([24.86208619, 18.44540496,  2.04300427]), 'targetState': array([23, 23], dtype=int32), 'currentDistance': 4.920538685261341}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9319159158798581
{'scaleFactor': 20, 'currentTarget': array([23., 23.]), 'previousTarget': array([23., 23.]), 'currentState': array([22.55045295, 23.50995951,  2.38422843]), 'targetState': array([23, 23], dtype=int32), 'currentDistance': 0.6798170757215541}
episode index:4910
target Thresh 32.0
target distance 7.0
model initialize at round 4910
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 25.]), 'previousTarget': array([17., 25.]), 'currentState': array([11.06319   , 18.69538699,  0.12299221]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 8.659899427230654}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9319197998411735
{'scaleFactor': 20, 'currentTarget': array([17., 25.]), 'previousTarget': array([17., 25.]), 'currentState': array([17.21846936, 25.43053112,  2.18257534]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 0.4827897073600595}
episode index:4911
target Thresh 32.0
target distance 4.0
model initialize at round 4911
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 24.]), 'previousTarget': array([ 7., 24.]), 'currentState': array([12.33563185, 22.023951  ,  1.66406697]), 'targetState': array([ 7, 24], dtype=int32), 'currentDistance': 5.689792340192128}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9319276131962546
{'scaleFactor': 20, 'currentTarget': array([ 7., 24.]), 'previousTarget': array([ 7., 24.]), 'currentState': array([ 7.24840441, 23.93829818,  3.0326243 ]), 'targetState': array([ 7, 24], dtype=int32), 'currentDistance': 0.25595286313298354}
episode index:4912
target Thresh 32.0
target distance 20.0
model initialize at round 4912
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 26.]), 'previousTarget': array([14., 26.]), 'currentState': array([15.47444828,  7.59880689,  0.87838459]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 18.460170789636337}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9319220065326707
{'scaleFactor': 20, 'currentTarget': array([14., 26.]), 'previousTarget': array([14., 26.]), 'currentState': array([13.98309382, 25.92478938,  2.21407619]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 0.07708732579171035}
episode index:4913
target Thresh 32.0
target distance 14.0
model initialize at round 4913
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([3.97123816, 3.68452162, 6.23060028]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 12.249593933476552}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9319239516166895
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.12132229,  5.05511993,  0.93212531]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.290299521210531}
episode index:4914
target Thresh 32.0
target distance 5.0
model initialize at round 4914
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([12.51466272,  6.87687117,  2.2593348 ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 3.471042617617777}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9319337534576627
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([10.96802279, 10.28326165,  2.86426091]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.2850608829601019}
episode index:4915
target Thresh 32.0
target distance 18.0
model initialize at round 4915
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 20.]), 'previousTarget': array([15., 20.]), 'currentState': array([14.48029843,  2.8006901 ,  1.50582808]), 'targetState': array([15, 20], dtype=int32), 'currentDistance': 17.207159864061296}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9319300072196696
{'scaleFactor': 20, 'currentTarget': array([15., 20.]), 'previousTarget': array([15., 20.]), 'currentState': array([15.16364391, 19.54705283,  2.10717757]), 'targetState': array([15, 20], dtype=int32), 'currentDistance': 0.48160198704974516}
episode index:4916
target Thresh 32.0
target distance 15.0
model initialize at round 4916
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  6.]), 'previousTarget': array([20.,  6.]), 'currentState': array([23.73649276, 22.11172728,  3.43000877]), 'targetState': array([20,  6], dtype=int32), 'currentDistance': 16.539320844507753}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9319262625054665
{'scaleFactor': 20, 'currentTarget': array([20.,  6.]), 'previousTarget': array([20.,  6.]), 'currentState': array([19.706285  ,  6.00240626,  5.43663216]), 'targetState': array([20,  6], dtype=int32), 'currentDistance': 0.29372485511035556}
episode index:4917
target Thresh 32.0
target distance 11.0
model initialize at round 4917
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 26.]), 'previousTarget': array([ 6., 26.]), 'currentState': array([16.89188215, 20.67949356,  2.64508289]), 'targetState': array([ 6, 26], dtype=int32), 'currentDistance': 12.12191756637565}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9319262907863534
{'scaleFactor': 20, 'currentTarget': array([ 6., 26.]), 'previousTarget': array([ 6., 26.]), 'currentState': array([ 5.39207028, 25.61032939,  3.69791991]), 'targetState': array([ 6, 26], dtype=int32), 'currentDistance': 0.7220953779579561}
episode index:4918
target Thresh 32.0
target distance 10.0
model initialize at round 4918
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 17.]), 'previousTarget': array([14., 17.]), 'currentState': array([ 5.76767624, 19.38536238,  5.31519351]), 'targetState': array([14, 17], dtype=int32), 'currentDistance': 8.570945583473549}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.931932119149682
{'scaleFactor': 20, 'currentTarget': array([14., 17.]), 'previousTarget': array([14., 17.]), 'currentState': array([13.00040957, 16.81239845,  6.23592864]), 'targetState': array([14, 17], dtype=int32), 'currentDistance': 1.0170424643104519}
episode index:4919
target Thresh 32.0
target distance 8.0
model initialize at round 4919
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([10.62022185, 25.54473169,  4.99925757]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 8.723842779469036}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9319359927128426
{'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([15.69026571, 17.59447623,  5.94792684]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 0.8005724702177008}
episode index:4920
target Thresh 32.0
target distance 2.0
model initialize at round 4920
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.00482182, 8.35008192, 6.20415956]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 2.5521098402075406}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9319477919421226
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.81429817, 6.87456829, 4.20416385]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.8940664749301335}
episode index:4921
target Thresh 32.0
target distance 12.0
model initialize at round 4921
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 15.]), 'previousTarget': array([ 8., 15.]), 'currentState': array([7.44290282, 2.36137925, 2.52080584]), 'targetState': array([ 8, 15], dtype=int32), 'currentDistance': 12.650892922312876}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9319478158259026
{'scaleFactor': 20, 'currentTarget': array([ 8., 15.]), 'previousTarget': array([ 8., 15.]), 'currentState': array([ 7.64294718, 15.38806417,  2.14475836]), 'targetState': array([ 8, 15], dtype=int32), 'currentDistance': 0.5273333985758047}
episode index:4922
target Thresh 32.0
target distance 18.0
model initialize at round 4922
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 25.]), 'previousTarget': array([15., 25.]), 'currentState': array([19.03509937,  8.99856209,  1.61204235]), 'targetState': array([15, 25], dtype=int32), 'currentDistance': 16.502364739006808}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.931945946412659
{'scaleFactor': 20, 'currentTarget': array([15., 25.]), 'previousTarget': array([15., 25.]), 'currentState': array([15.78556726, 24.08229492,  2.50228526]), 'targetState': array([15, 25], dtype=int32), 'currentDistance': 1.208014297711184}
episode index:4923
target Thresh 32.0
target distance 9.0
model initialize at round 4923
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([23.95191206,  4.57509917,  2.73778665]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 10.223123596505124}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9319498140210033
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.50313383, 10.74163297,  2.57267436]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.5655945290948317}
episode index:4924
target Thresh 32.0
target distance 9.0
model initialize at round 4924
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 12.]), 'previousTarget': array([13., 12.]), 'currentState': array([14.33424987,  3.2401345 ,  1.98841906]), 'targetState': array([13, 12], dtype=int32), 'currentDistance': 8.860895343854088}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9319536800587451
{'scaleFactor': 20, 'currentTarget': array([13., 12.]), 'previousTarget': array([13., 12.]), 'currentState': array([12.48703324, 12.59046132,  2.16369119]), 'targetState': array([13, 12], dtype=int32), 'currentDistance': 0.7821633254134746}
episode index:4925
target Thresh 32.0
target distance 4.0
model initialize at round 4925
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([17.75358982, 12.5048224 ,  0.09652233]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 3.28542610711008}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9319634539767193
{'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([21.57605592, 11.60044121,  0.09786153]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 0.7010618023476716}
episode index:4926
target Thresh 32.0
target distance 15.0
model initialize at round 4926
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 19.]), 'previousTarget': array([18., 19.]), 'currentState': array([21.77593678,  5.95269454,  2.01079988]), 'targetState': array([18, 19], dtype=int32), 'currentDistance': 13.582705119499373}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.931963474657444
{'scaleFactor': 20, 'currentTarget': array([18., 19.]), 'previousTarget': array([18., 19.]), 'currentState': array([17.75639592, 18.96444802,  2.11134706]), 'targetState': array([18, 19], dtype=int32), 'currentDistance': 0.24618467015790915}
episode index:4927
target Thresh 32.0
target distance 18.0
model initialize at round 4927
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  8.]), 'previousTarget': array([17.,  8.]), 'currentState': array([ 2.49102203, 24.39025299,  6.01845455]), 'targetState': array([17,  8], dtype=int32), 'currentDistance': 21.88951426290421}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9319560425916164
{'scaleFactor': 20, 'currentTarget': array([17.,  8.]), 'previousTarget': array([17.,  8.]), 'currentState': array([16.26807145,  8.05885358,  6.13928114]), 'targetState': array([17,  8], dtype=int32), 'currentDistance': 0.7342909142394249}
episode index:4928
target Thresh 32.0
target distance 11.0
model initialize at round 4928
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 29.]), 'previousTarget': array([27., 29.]), 'currentState': array([19.48932661, 17.21623068,  0.52556771]), 'targetState': array([27, 29], dtype=int32), 'currentDistance': 13.973812441513303}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9319560647675781
{'scaleFactor': 20, 'currentTarget': array([27., 29.]), 'previousTarget': array([27., 29.]), 'currentState': array([27.05573167, 28.45064557,  1.70358836]), 'targetState': array([27, 29], dtype=int32), 'currentDistance': 0.5521741633278737}
episode index:4929
target Thresh 32.0
target distance 6.0
model initialize at round 4929
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 23.]), 'previousTarget': array([ 6., 23.]), 'currentState': array([10.83540691, 21.78504691,  2.93815136]), 'targetState': array([ 6, 23], dtype=int32), 'currentDistance': 4.985706671499724}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9319638422392276
{'scaleFactor': 20, 'currentTarget': array([ 6., 23.]), 'previousTarget': array([ 6., 23.]), 'currentState': array([ 5.1433419 , 22.89471737,  3.22109641]), 'targetState': array([ 6, 23], dtype=int32), 'currentDistance': 0.8631034343542968}
episode index:4930
target Thresh 32.0
target distance 13.0
model initialize at round 4930
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 2.]), 'previousTarget': array([8., 2.]), 'currentState': array([ 2.00218442, 13.04397053,  5.08156717]), 'targetState': array([8, 2], dtype=int32), 'currentDistance': 12.567540606579723}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9319657721331968
{'scaleFactor': 20, 'currentTarget': array([8., 2.]), 'previousTarget': array([8., 2.]), 'currentState': array([7.08873651, 2.62286803, 5.85861671]), 'targetState': array([8, 2], dtype=int32), 'currentDistance': 1.1037960573365204}
episode index:4931
target Thresh 32.0
target distance 5.0
model initialize at round 4931
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 20.]), 'previousTarget': array([19., 20.]), 'currentState': array([22.22474803, 16.15428781,  1.76578968]), 'targetState': array([19, 20], dtype=int32), 'currentDistance': 5.018814814773488}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.931975531709001
{'scaleFactor': 20, 'currentTarget': array([19., 20.]), 'previousTarget': array([19., 20.]), 'currentState': array([19.86898137, 19.29245478,  2.37380134]), 'targetState': array([19, 20], dtype=int32), 'currentDistance': 1.1206020063613793}
episode index:4932
target Thresh 32.0
target distance 10.0
model initialize at round 4932
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([23.30810991,  9.05888458,  1.6904928 ]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 12.205254410507145}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9319774584508806
{'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([15.49046888, 17.65500238,  2.51180076]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 0.599652461136188}
episode index:4933
target Thresh 32.0
target distance 22.0
model initialize at round 4933
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  2.]), 'previousTarget': array([26.,  2.]), 'currentState': array([ 5.68525945, 11.10355951,  0.04299301]), 'targetState': array([26,  2], dtype=int32), 'currentDistance': 22.261255111136183}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.931970032588661
{'scaleFactor': 20, 'currentTarget': array([26.,  2.]), 'previousTarget': array([26.,  2.]), 'currentState': array([25.26161936,  2.28845405,  5.76413631]), 'targetState': array([26,  2], dtype=int32), 'currentDistance': 0.7927242275563916}
episode index:4934
target Thresh 32.0
target distance 12.0
model initialize at round 4934
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([19.80089644, 12.41439008,  3.87281567]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 11.467699302772356}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9319719596640029
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.1705888 ,  1.69754213,  4.3376807 ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.34724817922939905}
episode index:4935
target Thresh 32.0
target distance 17.0
model initialize at round 4935
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 8.]), 'previousTarget': array([7., 8.]), 'currentState': array([ 2.42051459, 23.41994154,  5.37087131]), 'targetState': array([7, 8], dtype=int32), 'currentDistance': 16.08559242513741}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9319700902828773
{'scaleFactor': 20, 'currentTarget': array([7., 8.]), 'previousTarget': array([7., 8.]), 'currentState': array([6.78620744, 8.39940638, 5.00878272]), 'targetState': array([7, 8], dtype=int32), 'currentDistance': 0.45302617990037336}
episode index:4936
target Thresh 32.0
target distance 13.0
model initialize at round 4936
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([ 4.917998  , 13.16769548,  5.88957787]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 11.113212818468943}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9319720165658666
{'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([16.43555198, 14.53581018,  0.16553921]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 0.6905056673886187}
episode index:4937
target Thresh 32.0
target distance 16.0
model initialize at round 4937
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([22.73819583, 18.73095313,  3.11416662]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 16.64277779183256}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9319701479303587
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.94204989, 11.45306109,  3.41084608]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.0453336001143996}
episode index:4938
target Thresh 32.0
target distance 17.0
model initialize at round 4938
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'previousTarget': array([26., 11.]), 'currentState': array([10.68275105, 28.0271473 ,  5.2893166 ]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 22.902878870118307}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9319627310658777
{'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'previousTarget': array([26., 11.]), 'currentState': array([25.17470902, 11.88944791,  5.25564707]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 1.2133518826859941}
episode index:4939
target Thresh 32.0
target distance 11.0
model initialize at round 4939
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 14.]), 'previousTarget': array([19., 14.]), 'currentState': array([24.29468232, 23.28362077,  3.65040874]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 10.687341836724006}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9319665827498521
{'scaleFactor': 20, 'currentTarget': array([19., 14.]), 'previousTarget': array([19., 14.]), 'currentState': array([19.42528179, 14.78493752,  4.65480456]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 0.8927438125665398}
episode index:4940
target Thresh 32.0
target distance 7.0
model initialize at round 4940
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 20.]), 'previousTarget': array([14., 20.]), 'currentState': array([12.16537537, 14.67482509,  0.4623735 ]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 5.632347235433094}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9319743407780346
{'scaleFactor': 20, 'currentTarget': array([14., 20.]), 'previousTarget': array([14., 20.]), 'currentState': array([13.80648621, 20.16172021,  1.44360124]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 0.2521924114747856}
episode index:4941
target Thresh 32.0
target distance 25.0
model initialize at round 4941
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 17.]), 'previousTarget': array([27., 17.]), 'currentState': array([ 1.86948466, 13.67790162,  0.63842487]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 25.349144736855234}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9319633223001351
{'scaleFactor': 20, 'currentTarget': array([27., 17.]), 'previousTarget': array([27., 17.]), 'currentState': array([26.92700079, 17.24800572,  6.15057098]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 0.25852606105427206}
episode index:4942
target Thresh 32.0
target distance 16.0
model initialize at round 4942
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 20.]), 'previousTarget': array([10., 20.]), 'currentState': array([3.3181548 , 3.93847925, 2.16815567]), 'targetState': array([10, 20], dtype=int32), 'currentDistance': 17.39596229410913}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.931959590543142
{'scaleFactor': 20, 'currentTarget': array([10., 20.]), 'previousTarget': array([10., 20.]), 'currentState': array([ 9.78114898, 20.2422938 ,  0.99441221]), 'targetState': array([10, 20], dtype=int32), 'currentDistance': 0.326499700806379}
episode index:4943
target Thresh 32.0
target distance 13.0
model initialize at round 4943
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 26.]), 'previousTarget': array([26., 26.]), 'currentState': array([14.09752871, 25.27586003,  6.13358039]), 'targetState': array([26, 26], dtype=int32), 'currentDistance': 11.924479084849624}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9319615162225228
{'scaleFactor': 20, 'currentTarget': array([26., 26.]), 'previousTarget': array([26., 26.]), 'currentState': array([25.91121804, 26.18322208,  6.09468538]), 'targetState': array([26, 26], dtype=int32), 'currentDistance': 0.20359903799855908}
episode index:4944
target Thresh 32.0
target distance 18.0
model initialize at round 4944
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 22.]), 'previousTarget': array([11., 22.]), 'currentState': array([16.24445099,  5.87072665,  2.03503001]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 16.960475374086904}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.931957786340068
{'scaleFactor': 20, 'currentTarget': array([11., 22.]), 'previousTarget': array([11., 22.]), 'currentState': array([10.38715934, 22.66748503,  1.8740558 ]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 0.9061511644135006}
episode index:4945
target Thresh 32.0
target distance 7.0
model initialize at round 4945
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 13.]), 'previousTarget': array([ 7., 13.]), 'currentState': array([12.11782403, 13.57833925,  2.89274037]), 'targetState': array([ 7, 13], dtype=int32), 'currentDistance': 5.150397953582284}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9319655383040104
{'scaleFactor': 20, 'currentTarget': array([ 7., 13.]), 'previousTarget': array([ 7., 13.]), 'currentState': array([ 6.20679933, 13.03812352,  3.40364832]), 'targetState': array([ 7, 13], dtype=int32), 'currentDistance': 0.7941163062075397}
episode index:4946
target Thresh 32.0
target distance 19.0
model initialize at round 4946
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 26.]), 'previousTarget': array([26., 26.]), 'currentState': array([ 8.09958853, 24.72591478,  0.1512205 ]), 'targetState': array([26, 26], dtype=int32), 'currentDistance': 17.94569652642643}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9319618091164583
{'scaleFactor': 20, 'currentTarget': array([26., 26.]), 'previousTarget': array([26., 26.]), 'currentState': array([25.82376511, 26.18220159,  6.22739513]), 'targetState': array([26, 26], dtype=int32), 'currentDistance': 0.253487980461255}
episode index:4947
target Thresh 32.0
target distance 22.0
model initialize at round 4947
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([25.56996964,  9.62710233,  2.83918053]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 21.729363936564273}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.931954407427926
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.93305721, 6.97275398, 3.62041616]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.07227504636808926}
episode index:4948
target Thresh 32.0
target distance 7.0
model initialize at round 4948
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  6.]), 'previousTarget': array([24.,  6.]), 'currentState': array([18.48871853,  8.71610155,  5.89551836]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 6.144219323134492}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9319621553755056
{'scaleFactor': 20, 'currentTarget': array([24.,  6.]), 'previousTarget': array([24.,  6.]), 'currentState': array([23.87369858,  6.08150269,  5.82923571]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 0.1503154611429111}
episode index:4949
target Thresh 32.0
target distance 24.0
model initialize at round 4949
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  4.]), 'previousTarget': array([27.,  4.]), 'currentState': array([ 1.70047361, 12.93059865,  4.84015155]), 'targetState': array([27,  4], dtype=int32), 'currentDistance': 26.829491753622758}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9319493843972012
{'scaleFactor': 20, 'currentTarget': array([27.,  4.]), 'previousTarget': array([27.,  4.]), 'currentState': array([27.3394041 ,  3.78060066,  6.06656211]), 'targetState': array([27,  4], dtype=int32), 'currentDistance': 0.4041425719219389}
episode index:4950
target Thresh 32.0
target distance 4.0
model initialize at round 4950
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([20.55046695, 12.94170613,  1.65252337]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 2.130630776150221}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9319611094256001
{'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([19.93481362, 14.82502833,  2.12569535]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 0.1867199789617707}
episode index:4951
target Thresh 32.0
target distance 14.0
model initialize at round 4951
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 16.]), 'previousTarget': array([19., 16.]), 'currentState': array([6.6804964 , 1.9087864 , 0.95577544]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 18.717170448806176}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9319573848977443
{'scaleFactor': 20, 'currentTarget': array([19., 16.]), 'previousTarget': array([19., 16.]), 'currentState': array([18.10457627, 15.58947629,  0.55934657]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 0.9850448559892139}
episode index:4952
target Thresh 32.0
target distance 13.0
model initialize at round 4952
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 11.]), 'previousTarget': array([20., 11.]), 'currentState': array([ 7.94669457, 21.39145876,  6.24655801]), 'targetState': array([20, 11], dtype=int32), 'currentDistance': 15.914288762211093}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9319555248754406
{'scaleFactor': 20, 'currentTarget': array([20., 11.]), 'previousTarget': array([20., 11.]), 'currentState': array([19.99650779, 11.28790179,  5.7523906 ]), 'targetState': array([20, 11], dtype=int32), 'currentDistance': 0.287922966632135}
episode index:4953
target Thresh 32.0
target distance 12.0
model initialize at round 4953
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([13.55343445, 19.1398627 ,  4.68803692]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 11.233391864582448}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9319574474883848
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.32048307,  7.39225699,  4.67155018]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.6870669316187697}
episode index:4954
target Thresh 32.0
target distance 7.0
model initialize at round 4954
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 29.]), 'previousTarget': array([ 8., 29.]), 'currentState': array([ 8.12810065, 23.97580321,  1.75413644]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 5.0258295955025005}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9319651854404557
{'scaleFactor': 20, 'currentTarget': array([ 8., 29.]), 'previousTarget': array([ 8., 29.]), 'currentState': array([ 7.68393301, 29.91546264,  1.50890142]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 0.9684886128564023}
episode index:4955
target Thresh 32.0
target distance 14.0
model initialize at round 4955
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([25.36673659,  5.64252621,  2.36112347]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 13.472528344490687}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9319652056508001
{'scaleFactor': 20, 'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([19.61249038, 18.24439352,  1.8404029 ]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 0.4581396043213251}
episode index:4956
target Thresh 32.0
target distance 6.0
model initialize at round 4956
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 20.]), 'previousTarget': array([ 4., 20.]), 'currentState': array([ 3.55394158, 13.35373872,  0.61587494]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 6.6612128895310105}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.931970981483834
{'scaleFactor': 20, 'currentTarget': array([ 4., 20.]), 'previousTarget': array([ 4., 20.]), 'currentState': array([ 3.99532629, 20.9914059 ,  1.69101847]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 0.9914169200026333}
episode index:4957
target Thresh 32.0
target distance 6.0
model initialize at round 4957
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 19.]), 'previousTarget': array([ 2., 19.]), 'currentState': array([ 4.03494018, 23.67286515,  4.36470986]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 5.096729370602651}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9319806888292386
{'scaleFactor': 20, 'currentTarget': array([ 2., 19.]), 'previousTarget': array([ 2., 19.]), 'currentState': array([ 2.52373928, 19.98716232,  4.54441692]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 1.1174937506471476}
episode index:4958
target Thresh 32.0
target distance 20.0
model initialize at round 4958
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 16.]), 'previousTarget': array([25., 16.]), 'currentState': array([6.68045118, 4.90795704, 0.95528191]), 'targetState': array([25, 16], dtype=int32), 'currentDistance': 21.415865284324305}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9319732997518901
{'scaleFactor': 20, 'currentTarget': array([25., 16.]), 'previousTarget': array([25., 16.]), 'currentState': array([25.30364131, 16.17246091,  0.91821796]), 'targetState': array([25, 16], dtype=int32), 'currentDistance': 0.3492002434237983}
episode index:4959
target Thresh 32.0
target distance 8.0
model initialize at round 4959
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([ 9.58651954, 15.15911632,  5.47918218]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 8.200029539007607}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9319790704596015
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.90977089,  9.24598331,  5.81938653]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.2620096988448765}
episode index:4960
target Thresh 32.0
target distance 13.0
model initialize at round 4960
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 15.]), 'previousTarget': array([18., 15.]), 'currentState': array([18.68140961, 28.07245564,  5.31625086]), 'targetState': array([18, 15], dtype=int32), 'currentDistance': 13.090203036231248}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9319790878507419
{'scaleFactor': 20, 'currentTarget': array([18., 15.]), 'previousTarget': array([18., 15.]), 'currentState': array([18.31506061, 14.44145321,  4.67608855]), 'targetState': array([18, 15], dtype=int32), 'currentDistance': 0.6412781828863624}
episode index:4961
target Thresh 32.0
target distance 13.0
model initialize at round 4961
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([ 5.63105079, 11.58519713,  5.02414835]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 13.667048758381242}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9319791052348726
{'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([17.02914699,  3.91789572,  6.03674344]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 0.08712439012294863}
episode index:4962
target Thresh 32.0
target distance 23.0
model initialize at round 4962
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  6.]), 'previousTarget': array([22.,  6.]), 'currentState': array([ 8.4239367 , 28.40973517,  4.5099411 ]), 'targetState': array([22,  6], dtype=int32), 'currentDistance': 26.201254266514507}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.9319681324195922
{'scaleFactor': 20, 'currentTarget': array([22.,  6.]), 'previousTarget': array([22.,  6.]), 'currentState': array([21.820201  ,  6.49176494,  5.45446794]), 'targetState': array([22,  6], dtype=int32), 'currentDistance': 0.5236033211471341}
episode index:4963
target Thresh 32.0
target distance 5.0
model initialize at round 4963
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 13.]), 'previousTarget': array([ 6., 13.]), 'currentState': array([8.83345285, 9.59356573, 2.48031557]), 'targetState': array([ 6, 13], dtype=int32), 'currentDistance': 4.43082943368433}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9319778286056479
{'scaleFactor': 20, 'currentTarget': array([ 6., 13.]), 'previousTarget': array([ 6., 13.]), 'currentState': array([ 6.27466034, 12.61328888,  2.56435442]), 'targetState': array([ 6, 13], dtype=int32), 'currentDistance': 0.47432456413459767}
episode index:4964
target Thresh 32.0
target distance 6.0
model initialize at round 4964
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 18.]), 'previousTarget': array([24., 18.]), 'currentState': array([19.96343303, 17.36093698,  0.28777116]), 'targetState': array([24, 18], dtype=int32), 'currentDistance': 4.086841618230503}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9319875208858884
{'scaleFactor': 20, 'currentTarget': array([24., 18.]), 'previousTarget': array([24., 18.]), 'currentState': array([23.88266948, 17.98323382,  0.43960842]), 'targetState': array([24, 18], dtype=int32), 'currentDistance': 0.11852238638528143}
episode index:4965
target Thresh 32.0
target distance 9.0
model initialize at round 4965
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 24.]), 'previousTarget': array([ 7., 24.]), 'currentState': array([13.39937439, 15.93495423,  1.59901017]), 'targetState': array([ 7, 24], dtype=int32), 'currentDistance': 10.295482300151301}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9319913474120691
{'scaleFactor': 20, 'currentTarget': array([ 7., 24.]), 'previousTarget': array([ 7., 24.]), 'currentState': array([ 7.21197272, 23.49735784,  2.58346623]), 'targetState': array([ 7, 24], dtype=int32), 'currentDistance': 0.5455103777970962}
episode index:4966
target Thresh 32.0
target distance 8.0
model initialize at round 4966
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 17.]), 'previousTarget': array([ 5., 17.]), 'currentState': array([11.60393182, 19.93988389,  3.55905557]), 'targetState': array([ 5, 17], dtype=int32), 'currentDistance': 7.228750429564201}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9319971063536008
{'scaleFactor': 20, 'currentTarget': array([ 5., 17.]), 'previousTarget': array([ 5., 17.]), 'currentState': array([ 4.49803925, 16.38481683,  3.43150038]), 'targetState': array([ 5, 17], dtype=int32), 'currentDistance': 0.7939867352325622}
episode index:4967
target Thresh 32.0
target distance 10.0
model initialize at round 4967
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 2.]), 'previousTarget': array([6., 2.]), 'currentState': array([14.47491944, 12.71170033,  3.71496761]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 13.658871972070239}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9319971200898234
{'scaleFactor': 20, 'currentTarget': array([6., 2.]), 'previousTarget': array([6., 2.]), 'currentState': array([6.06654317, 1.84203625, 4.19359159]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 0.17140752649153124}
episode index:4968
target Thresh 32.0
target distance 15.0
model initialize at round 4968
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 24.]), 'previousTarget': array([10., 24.]), 'currentState': array([23.29808259, 15.96217598,  2.99912274]), 'targetState': array([10, 24], dtype=int32), 'currentDistance': 15.538520380001339}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9319952580601067
{'scaleFactor': 20, 'currentTarget': array([10., 24.]), 'previousTarget': array([10., 24.]), 'currentState': array([ 9.81950375, 23.94028009,  2.5858874 ]), 'targetState': array([10, 24], dtype=int32), 'currentDistance': 0.19011934231518035}
episode index:4969
target Thresh 32.0
target distance 13.0
model initialize at round 4969
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 22.]), 'previousTarget': array([ 8., 22.]), 'currentState': array([20.60762517, 23.36340901,  3.46707916]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 12.681131519228265}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.931997166488948
{'scaleFactor': 20, 'currentTarget': array([ 8., 22.]), 'previousTarget': array([ 8., 22.]), 'currentState': array([ 8.95288047, 21.87848341,  3.39191917]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 0.9605974532756106}
episode index:4970
target Thresh 32.0
target distance 17.0
model initialize at round 4970
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 24.]), 'previousTarget': array([23., 24.]), 'currentState': array([21.96195228,  8.91696992,  1.08451581]), 'targetState': array([23, 24], dtype=int32), 'currentDistance': 15.118708259688713}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9319953051990544
{'scaleFactor': 20, 'currentTarget': array([23., 24.]), 'previousTarget': array([23., 24.]), 'currentState': array([22.96769013, 24.6882703 ,  1.49026494]), 'targetState': array([23, 24], dtype=int32), 'currentDistance': 0.6890282513209178}
episode index:4971
target Thresh 32.0
target distance 16.0
model initialize at round 4971
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  7.]), 'previousTarget': array([20.,  7.]), 'currentState': array([27.12187887, 21.32144895,  3.77487135]), 'targetState': array([20,  7], dtype=int32), 'currentDistance': 15.994532149139701}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9319934446578695
{'scaleFactor': 20, 'currentTarget': array([20.,  7.]), 'previousTarget': array([20.,  7.]), 'currentState': array([20.17092291,  7.09808623,  4.33990107]), 'targetState': array([20,  7], dtype=int32), 'currentDistance': 0.19706737294893636}
episode index:4972
target Thresh 32.0
target distance 14.0
model initialize at round 4972
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([ 7.74171645, 16.48928997,  4.1587882 ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 13.35912567369681}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9319934591165965
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.85653129, 3.45512779, 4.45043096]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.563443874560164}
episode index:4973
target Thresh 32.0
target distance 18.0
model initialize at round 4973
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 8.]), 'previousTarget': array([7., 8.]), 'currentState': array([10.41585755, 24.3692178 ,  3.95207262]), 'targetState': array([7, 8], dtype=int32), 'currentDistance': 16.72182329276421}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9319915996946647
{'scaleFactor': 20, 'currentTarget': array([7., 8.]), 'previousTarget': array([7., 8.]), 'currentState': array([7.32249422, 8.87392084, 4.39502868]), 'targetState': array([7, 8], dtype=int32), 'currentDistance': 0.9315257176236575}
episode index:4974
target Thresh 32.0
target distance 13.0
model initialize at round 4974
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 27.]), 'previousTarget': array([10., 27.]), 'currentState': array([19.35227904, 13.65735775,  2.33661985]), 'targetState': array([10, 27], dtype=int32), 'currentDistance': 16.29390147445463}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.9319897410202392
{'scaleFactor': 20, 'currentTarget': array([10., 27.]), 'previousTarget': array([10., 27.]), 'currentState': array([10.06364062, 26.54054358,  2.40487424]), 'targetState': array([10, 27], dtype=int32), 'currentDistance': 0.46384300468431255}
episode index:4975
target Thresh 32.0
target distance 21.0
model initialize at round 4975
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 25.]), 'previousTarget': array([ 8., 25.]), 'currentState': array([27.36382122,  5.64317443,  2.36289805]), 'targetState': array([ 8, 25], dtype=int32), 'currentDistance': 27.379632363118215}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.9319770312275841
{'scaleFactor': 20, 'currentTarget': array([ 8., 25.]), 'previousTarget': array([ 8., 25.]), 'currentState': array([ 7.6948557 , 25.08575034,  2.39691259]), 'targetState': array([ 8, 25], dtype=int32), 'currentDistance': 0.3169639808451235}
episode index:4976
target Thresh 32.0
target distance 18.0
model initialize at round 4976
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  2.]), 'previousTarget': array([10.,  2.]), 'currentState': array([22.57006192, 19.39395243,  4.90480382]), 'targetState': array([10,  2], dtype=int32), 'currentDistance': 21.460569376664537}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9319696696087437
{'scaleFactor': 20, 'currentTarget': array([10.,  2.]), 'previousTarget': array([10.,  2.]), 'currentState': array([10.14758179,  1.67304591,  3.87489707]), 'targetState': array([10,  2], dtype=int32), 'currentDistance': 0.35871905949335386}
episode index:4977
target Thresh 32.0
target distance 17.0
model initialize at round 4977
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'previousTarget': array([26., 11.]), 'currentState': array([ 8.70038273, 10.34391499,  5.54340601]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 17.31205375839613}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9319659628144237
{'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'previousTarget': array([26., 11.]), 'currentState': array([26.31335637, 11.07942961,  0.24861168]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 0.32326656811595655}
episode index:4978
target Thresh 32.0
target distance 2.0
model initialize at round 4978
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 12.]), 'previousTarget': array([ 6., 12.]), 'currentState': array([ 6.51846364, 14.79839721,  3.65731287]), 'targetState': array([ 6, 12], dtype=int32), 'currentDistance': 2.846020287625312}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.9319756302249851
{'scaleFactor': 20, 'currentTarget': array([ 6., 12.]), 'previousTarget': array([ 6., 12.]), 'currentState': array([ 6.53031982, 11.15527056,  4.75614542]), 'targetState': array([ 6, 12], dtype=int32), 'currentDistance': 0.9974000913724329}
episode index:4979
target Thresh 32.0
target distance 4.0
model initialize at round 4979
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 16.]), 'previousTarget': array([12., 16.]), 'currentState': array([14.20474765, 17.11848994,  3.60065993]), 'targetState': array([12, 16], dtype=int32), 'currentDistance': 2.472232180260572}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9319872817048596
{'scaleFactor': 20, 'currentTarget': array([12., 16.]), 'previousTarget': array([12., 16.]), 'currentState': array([12.52174855, 16.04540406,  3.81994041]), 'targetState': array([12, 16], dtype=int32), 'currentDistance': 0.5237204166866544}
episode index:4980
target Thresh 32.0
target distance 20.0
model initialize at round 4980
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([23.87411395, 10.58475291,  2.60432762]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 23.056720567347014}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9319781264328282
{'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([ 9.44915182, 29.50957402,  1.92436741]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 0.750399495897421}
episode index:4981
target Thresh 32.0
target distance 16.0
model initialize at round 4981
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 23.]), 'previousTarget': array([ 5., 23.]), 'currentState': array([17.33581357,  8.08496481,  2.3612932 ]), 'targetState': array([ 5, 23], dtype=int32), 'currentDistance': 19.355375772960247}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.9319725872815988
{'scaleFactor': 20, 'currentTarget': array([ 5., 23.]), 'previousTarget': array([ 5., 23.]), 'currentState': array([ 4.79307168, 23.44155041,  2.39402936]), 'targetState': array([ 5, 23], dtype=int32), 'currentDistance': 0.48763315529616075}
episode index:4982
target Thresh 32.0
target distance 1.0
model initialize at round 4982
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 28.]), 'previousTarget': array([ 8., 28.]), 'currentState': array([ 5.6457631 , 27.99925043,  1.49593556]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 2.354237024210686}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9319842323574002
{'scaleFactor': 20, 'currentTarget': array([ 8., 28.]), 'previousTarget': array([ 8., 28.]), 'currentState': array([ 7.11789309, 28.81486147,  5.77912087]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 1.2008796001889561}
episode index:4983
target Thresh 32.0
target distance 7.0
model initialize at round 4983
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 15.]), 'previousTarget': array([12., 15.]), 'currentState': array([13.39210176, 20.36334356,  3.93753195]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 5.5410650115075}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9319919199111004
{'scaleFactor': 20, 'currentTarget': array([12., 15.]), 'previousTarget': array([12., 15.]), 'currentState': array([11.91261195, 14.66733733,  4.7374813 ]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 0.34394930017313924}
episode index:4984
target Thresh 32.0
target distance 13.0
model initialize at round 4984
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 24.]), 'previousTarget': array([21., 24.]), 'currentState': array([16.31772575, 10.95161158,  2.16034842]), 'targetState': array([21, 24], dtype=int32), 'currentDistance': 13.863049173257643}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.931991934640889
{'scaleFactor': 20, 'currentTarget': array([21., 24.]), 'previousTarget': array([21., 24.]), 'currentState': array([20.84986463, 23.75715473,  0.89931694]), 'targetState': array([21, 24], dtype=int32), 'currentDistance': 0.2855073642297881}
episode index:4985
target Thresh 32.0
target distance 17.0
model initialize at round 4985
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([ 5.22377376, 17.84467932,  0.25336665]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 16.82407771023255}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9319882293285832
{'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([21.84369532, 11.89651816,  5.84093127]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 0.8500178160709204}
episode index:4986
target Thresh 32.0
target distance 18.0
model initialize at round 4986
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 11.]), 'previousTarget': array([27., 11.]), 'currentState': array([10.69437059, 24.9427371 ,  5.63226393]), 'targetState': array([27, 11], dtype=int32), 'currentDistance': 21.45398490176548}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.9319808802259022
{'scaleFactor': 20, 'currentTarget': array([27., 11.]), 'previousTarget': array([27., 11.]), 'currentState': array([27.18338569, 10.68392616,  5.68376071]), 'targetState': array([27, 11], dtype=int32), 'currentDistance': 0.36542164985242337}
episode index:4987
target Thresh 32.0
target distance 11.0
model initialize at round 4987
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 7.]), 'previousTarget': array([7., 7.]), 'currentState': array([17.42479332, 17.10425892,  4.71194306]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 14.518001384734076}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9319808971600804
{'scaleFactor': 20, 'currentTarget': array([7., 7.]), 'previousTarget': array([7., 7.]), 'currentState': array([7.80226481, 7.6139964 , 3.60727969]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 1.0102575901327078}
episode index:4988
target Thresh 32.0
target distance 6.0
model initialize at round 4988
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 14.]), 'previousTarget': array([17., 14.]), 'currentState': array([12.68263315, 11.03367144,  1.03000849]), 'targetState': array([17, 14], dtype=int32), 'currentDistance': 5.238202136180004}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9319885776777873
{'scaleFactor': 20, 'currentTarget': array([17., 14.]), 'previousTarget': array([17., 14.]), 'currentState': array([17.35384864, 14.61666867,  0.40568149]), 'targetState': array([17, 14], dtype=int32), 'currentDistance': 0.7109775733358588}
episode index:4989
target Thresh 32.0
target distance 9.0
model initialize at round 4989
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([12.1198841 ,  5.8605826 ,  0.85560632]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 7.382825785071015}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.9319943106301565
{'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([13.88718233, 13.51685485,  1.15130183]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 0.5290243548341046}
episode index:4990
target Thresh 32.0
target distance 7.0
model initialize at round 4990
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([ 8.55564716, 10.64214483,  5.66466278]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 6.550282192780586}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.9320019853825847
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.52784229,  7.35635068,  6.00270823]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.5915392699887557}
episode index:4991
target Thresh 32.0
target distance 13.0
model initialize at round 4991
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  8.]), 'previousTarget': array([25.,  8.]), 'currentState': array([21.75270211, 19.33685757,  4.39677489]), 'targetState': array([25,  8], dtype=int32), 'currentDistance': 11.792764021240867}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.9320038840532615
{'scaleFactor': 20, 'currentTarget': array([25.,  8.]), 'previousTarget': array([25.,  8.]), 'currentState': array([25.08056271,  8.03163505,  4.77181771]), 'targetState': array([25,  8], dtype=int32), 'currentDistance': 0.08655129079074722}
episode index:4992
target Thresh 32.0
target distance 1.0
model initialize at round 4992
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  6.]), 'previousTarget': array([27.,  6.]), 'currentState': array([26.71445189,  4.47607763,  6.16059077]), 'targetState': array([27,  6], dtype=int32), 'currentDistance': 1.550444170015777}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.9320154995381297
{'scaleFactor': 20, 'currentTarget': array([27.,  6.]), 'previousTarget': array([27.,  6.]), 'currentState': array([27.80335441,  5.7594718 ,  1.8769899 ]), 'targetState': array([27,  6], dtype=int32), 'currentDistance': 0.8385893616618804}
episode index:4993
target Thresh 32.0
target distance 15.0
model initialize at round 4993
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 25.]), 'previousTarget': array([ 5., 25.]), 'currentState': array([ 9.00368061, 11.78797295,  2.3876493 ]), 'targetState': array([ 5, 25], dtype=int32), 'currentDistance': 13.805329302120498}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9320155095197814
{'scaleFactor': 20, 'currentTarget': array([ 5., 25.]), 'previousTarget': array([ 5., 25.]), 'currentState': array([ 4.95832252, 24.99885674,  1.6737581 ]), 'targetState': array([ 5, 25], dtype=int32), 'currentDistance': 0.0416931602840168}
episode index:4994
target Thresh 32.0
target distance 14.0
model initialize at round 4994
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([19.82834002,  9.32580738,  3.60021305]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 13.844765202678449}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9320155194974366
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([6.05932792, 9.90919055, 2.89574581]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.10847192482780761}
episode index:4995
target Thresh 32.0
target distance 7.0
model initialize at round 4995
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([22.62454112, 16.56039309,  5.00891084]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 9.323266734875059}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.9320193174418725
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.10593625,  9.94103389,  4.35030821]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.1212414584046328}
episode index:4996
target Thresh 32.0
target distance 19.0
model initialize at round 4996
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 24.]), 'previousTarget': array([23., 24.]), 'currentState': array([ 5.60722449, 26.49921692,  5.57434565]), 'targetState': array([23, 24], dtype=int32), 'currentDistance': 17.571417849324305}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9320156148062996
{'scaleFactor': 20, 'currentTarget': array([23., 24.]), 'previousTarget': array([23., 24.]), 'currentState': array([23.18598832, 24.27246921,  0.16472706]), 'targetState': array([23, 24], dtype=int32), 'currentDistance': 0.3298956283253673}
episode index:4997
target Thresh 32.0
target distance 20.0
model initialize at round 4997
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  3.]), 'previousTarget': array([11.,  3.]), 'currentState': array([ 8.44560424, 21.21183638,  5.04347897]), 'targetState': array([11,  3], dtype=int32), 'currentDistance': 18.390103917097314}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.9320119136523733
{'scaleFactor': 20, 'currentTarget': array([11.,  3.]), 'previousTarget': array([11.,  3.]), 'currentState': array([10.97024599,  3.59900177,  4.61294433]), 'targetState': array([11,  3], dtype=int32), 'currentDistance': 0.5997402905865727}
episode index:4998
target Thresh 32.0
target distance 19.0
model initialize at round 4998
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 29.]), 'previousTarget': array([12., 29.]), 'currentState': array([25.93147646,  8.69975174,  3.01450539]), 'targetState': array([12, 29], dtype=int32), 'currentDistance': 24.62084718106461}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.9320027864185394
{'scaleFactor': 20, 'currentTarget': array([12., 29.]), 'previousTarget': array([12., 29.]), 'currentState': array([12.56902922, 28.15603465,  2.50855654]), 'targetState': array([12, 29], dtype=int32), 'currentDistance': 1.0178761012463087}
episode index:4999
target Thresh 32.0
target distance 10.0
model initialize at round 4999
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([19.44840486, 26.14296357,  4.73888329]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 14.651271505014066}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.9320027989308372
{'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([ 8.89142321, 17.50146962,  3.64725832]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 1.0227937805922693}

Process finished with exit code 0
