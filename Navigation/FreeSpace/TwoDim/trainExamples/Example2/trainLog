/home/yangyutu/anaconda3/bin/python /home/yangyutu/Dropbox/UniversalNavigationProject/activeParticleModel/Navigation/FreeSpace/TwoDim/DDPGHER_MLP.py
episode index:0
target Thresh 6.399999999999999
target distance 3.0
model initialize at round 0
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([15.02964886,  7.01294537,  0.        ]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 3.130057940703636}
done in step count: 99
reward sum = -0.6142025452801495
running average episode reward sum: -0.6142025452801495
{'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([88.76781463, 94.06683894,  0.        ]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 111.4252410874575}
episode index:1
target Thresh 6.425587204265597
target distance 6.0
model initialize at round 1
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 24.]), 'previousTarget': array([10., 24.]), 'currentState': array([17.3849349 , 25.83259463,  0.        ]), 'targetState': array([10, 24], dtype=int32), 'currentDistance': 7.608920199271198}
done in step count: 99
reward sum = -2.1851298809088346
running average episode reward sum: -1.3996662130944921
{'scaleFactor': 20, 'currentTarget': array([10., 24.]), 'previousTarget': array([10., 24.]), 'currentState': array([ 13.30204439, 223.32792497,   0.        ]), 'targetState': array([10, 24], dtype=int32), 'currentDistance': 199.35527374187043}
episode index:2
target Thresh 6.451148834116271
target distance 4.0
model initialize at round 2
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([16.99240589, 12.93001103,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 5.7671051502877075}
done in step count: 99
reward sum = -1.3668067847949408
running average episode reward sum: -1.3887130703279749
{'scaleFactor': 20, 'currentTarget': array([16.05356   , 16.47598452]), 'previousTarget': array([15.48388077, 14.25438488]), 'currentState': array([ 63.14705048, 210.85243154,   0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 200.00000000000003}
episode index:3
target Thresh 6.476684915113651
target distance 5.0
model initialize at round 3
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 4.92983544, 11.99697316,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 3.2279817082699456}
done in step count: 99
reward sum = -1.8397247407903252
running average episode reward sum: -1.5014659879435626
{'scaleFactor': 20, 'currentTarget': array([29.73105462, 43.55097721]), 'previousTarget': array([28.12339887, 41.29444577]), 'currentState': array([140.77862948, 209.88929546,   0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 199.99999999999997}
episode index:4
target Thresh 6.502195472793815
target distance 5.0
model initialize at round 4
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'previousTarget': array([26., 11.]), 'currentState': array([26.90037513, 16.89152223,  0.        ]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 5.959925296698261}
done in step count: 99
reward sum = -1.8193548180308041
running average episode reward sum: -1.565043753961011
{'scaleFactor': 20, 'currentTarget': array([53.81742339, 45.89722427]), 'previousTarget': array([52.28905756, 43.95081268]), 'currentState': array([178.48200345, 202.29013485,   0.        ]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 200.0}
episode index:5
target Thresh 6.527680532667333
target distance 5.0
model initialize at round 5
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 17.]), 'previousTarget': array([21., 17.]), 'currentState': array([25.98086023, 20.41830039,  0.        ]), 'targetState': array([21, 17], dtype=int32), 'currentDistance': 6.041005398249297}
done in step count: 99
reward sum = -1.3746704601449826
running average episode reward sum: -1.5333148716583398
{'scaleFactor': 20, 'currentTarget': array([21., 17.]), 'previousTarget': array([21., 17.]), 'currentState': array([ 84.20181963, 206.72238395,   0.        ]), 'targetState': array([21, 17], dtype=int32), 'currentDistance': 199.97263057099812}
episode index:6
target Thresh 6.553140120219254
target distance 6.0
model initialize at round 6
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 8.]), 'previousTarget': array([6., 8.]), 'currentState': array([10.96913362,  2.57709438,  0.        ]), 'targetState': array([6, 8], dtype=int32), 'currentDistance': 7.355283427203591}
done in step count: 99
reward sum = -1.2032778114225693
running average episode reward sum: -1.4861667201960869
{'scaleFactor': 20, 'currentTarget': array([6., 8.]), 'previousTarget': array([6., 8.]), 'currentState': array([ 56.85601108, 189.66384387,   0.        ]), 'targetState': array([6, 8], dtype=int32), 'currentDistance': 188.6480480492743}
episode index:7
target Thresh 6.578574260909178
target distance 4.0
model initialize at round 7
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  9.]), 'previousTarget': array([19.,  9.]), 'currentState': array([22.97395086,  5.49456641,  0.        ]), 'targetState': array([19,  9], dtype=int32), 'currentDistance': 5.299089554730284}
done in step count: 99
reward sum = -1.215087802662004
running average episode reward sum: -1.4522818555043266
{'scaleFactor': 20, 'currentTarget': array([19.,  9.]), 'previousTarget': array([19.,  9.]), 'currentState': array([ 63.31260758, 194.9859325 ,   0.        ]), 'targetState': array([19,  9], dtype=int32), 'currentDistance': 191.1919827756939}
episode index:8
target Thresh 6.603982980171246
target distance 3.0
model initialize at round 8
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([14.99941301,  4.62019226,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.7044188799242306}
done in step count: 99
reward sum = -1.2053972659991397
running average episode reward sum: -1.4248502344481946
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([ 58.0459694 , 189.05104919,   0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 187.8178643036321}
episode index:9
target Thresh 6.6293663034141765
target distance 3.0
model initialize at round 9
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  7.]), 'previousTarget': array([24.,  7.]), 'currentState': array([28.86895573,  5.66941267,  0.        ]), 'targetState': array([24,  7], dtype=int32), 'currentDistance': 5.047493689971924}
done in step count: 99
reward sum = -1.1805895238820743
running average episode reward sum: -1.4004241633915826
{'scaleFactor': 20, 'currentTarget': array([24.,  7.]), 'previousTarget': array([24.,  7.]), 'currentState': array([ 68.23482307, 194.46013147,   0.        ]), 'targetState': array([24,  7], dtype=int32), 'currentDistance': 192.60846414719921}
episode index:10
target Thresh 6.6547242560212965
target distance 5.0
model initialize at round 10
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([ 7.99975288, 10.792943  ,  0.        ]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 4.288053772023543}
done in step count: 99
reward sum = -1.249346614414504
running average episode reward sum: -1.3866898407573027
{'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([ 55.36322928, 194.06269914,   0.        ]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 192.4844824389142}
episode index:11
target Thresh 6.680056863350558
target distance 4.0
model initialize at round 11
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([13.78777796, 16.20812249,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 7.074415713478545}
done in step count: 99
reward sum = -1.2542346736395908
running average episode reward sum: -1.3756519101641602
{'scaleFactor': 20, 'currentTarget': array([ 9.80042727, 14.62197834]), 'previousTarget': array([ 9.36165958, 12.6343141 ]), 'currentState': array([ 52.95749725, 209.91014314,   0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 200.0}
episode index:12
target Thresh 6.705364150734578
target distance 6.0
model initialize at round 12
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  9.]), 'previousTarget': array([20.,  9.]), 'currentState': array([25.29389608, 12.96256888,  0.        ]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 6.612661175181348}
done in step count: 99
reward sum = -1.2586762276823336
running average episode reward sum: -1.3666537807424812
{'scaleFactor': 20, 'currentTarget': array([20.79614482, 12.31888868]), 'previousTarget': array([20.33847125, 10.40025322]), 'currentState': array([ 67.44921143, 206.80151355,   0.        ]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 200.0}
episode index:13
target Thresh 6.730646143480637
target distance 3.0
model initialize at round 13
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 14.]), 'previousTarget': array([25., 14.]), 'currentState': array([25.9923594 , 16.12530065,  0.        ]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 2.3455660326498156}
done in step count: 99
reward sum = -1.1958441880856907
running average episode reward sum: -1.3544530955527103
{'scaleFactor': 20, 'currentTarget': array([25., 14.]), 'previousTarget': array([25., 14.]), 'currentState': array([ 65.35046506, 203.29385996,   0.        ]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 193.5467009497686}
episode index:14
target Thresh 6.755902866870734
target distance 2.0
model initialize at round 14
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 23.]), 'previousTarget': array([ 7., 23.]), 'currentState': array([10.74944305, 23.66510707,  0.        ]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 3.807977236421982}
done in step count: 99
reward sum = -1.1107308560047198
running average episode reward sum: -1.338204946249511
{'scaleFactor': 20, 'currentTarget': array([ 7., 23.]), 'previousTarget': array([ 7., 23.]), 'currentState': array([ 47.99678409, 208.72011381,   0.        ]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 190.19121162961605}
episode index:15
target Thresh 6.781134346161593
target distance 5.0
model initialize at round 15
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([16.16646111, 10.97925079,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 4.27999185621277}
done in step count: 99
reward sum = -0.8132150024618949
running average episode reward sum: -1.3053930747627849
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([ 85.23742813, 114.24435966,   0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 127.39940109451645}
episode index:16
target Thresh 6.806340606584698
target distance 5.0
model initialize at round 16
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([21.99051154, 21.04597274,  0.        ]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 5.052078698491318}
done in step count: 99
reward sum = -0.4212308507581686
running average episode reward sum: -1.253383532174278
{'scaleFactor': 20, 'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([ 85.9600587 , 102.62282708,   0.        ]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 100.45330684595305}
episode index:17
target Thresh 6.831521673346312
target distance 3.0
model initialize at round 17
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([26.99683213, 16.68937388,  0.        ]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 3.3106276350074793}
done in step count: 99
reward sum = -0.4085620035548774
running average episode reward sum: -1.2064490028065336
{'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([74.21590302, 77.10950721,  0.        ]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 74.1001842848516}
episode index:18
target Thresh 6.8566775716275
target distance 2.0
model initialize at round 18
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  8.]), 'previousTarget': array([21.,  8.]), 'currentState': array([20.99783695,  7.36485696,  0.        ]), 'targetState': array([21,  8], dtype=int32), 'currentDistance': 0.6351467248619833}
done in step count: 0
reward sum = 0.9780262044828234
running average episode reward sum: -1.0914766234755147
{'scaleFactor': 20, 'currentTarget': array([21.,  8.]), 'previousTarget': array([21.,  8.]), 'currentState': array([20.99783695,  7.36485696,  0.        ]), 'targetState': array([21,  8], dtype=int32), 'currentDistance': 0.6351467248619833}
episode index:19
target Thresh 6.881808326584164
target distance 6.0
model initialize at round 19
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  8.]), 'previousTarget': array([17.,  8.]), 'currentState': array([12.99988484,  5.5141412 ,  0.        ]), 'targetState': array([17,  8], dtype=int32), 'currentDistance': 4.709608818827706}
done in step count: 99
reward sum = -0.931504710519829
running average episode reward sum: -1.0834780278277303
{'scaleFactor': 20, 'currentTarget': array([17.,  8.]), 'previousTarget': array([17.,  8.]), 'currentState': array([ -50.92693195, -119.60373626,    0.        ]), 'targetState': array([17,  8], dtype=int32), 'currentDistance': 144.557191424331}
episode index:20
target Thresh 6.9069139633470655
target distance 4.0
model initialize at round 20
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 19.]), 'previousTarget': array([20., 19.]), 'currentState': array([22.07458365, 19.00167775,  0.        ]), 'targetState': array([20, 19], dtype=int32), 'currentDistance': 2.0745843280485357}
done in step count: 99
reward sum = -2.304337409432054
running average episode reward sum: -1.1416141888565077
{'scaleFactor': 20, 'currentTarget': array([ 9.07549398, -1.83480754]), 'previousTarget': array([ 9.2162802 , -0.97373433]), 'currentState': array([ -83.79951555, -178.96253717,    0.        ]), 'targetState': array([20, 19], dtype=int32), 'currentDistance': 200.0}
episode index:21
target Thresh 6.931994507021834
target distance 4.0
model initialize at round 21
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([ 3.978553  , 10.00062668,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 2.2271221551207643}
done in step count: 99
reward sum = -1.3611936044053259
running average episode reward sum: -1.151595071381454
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([ -29.55087979, -187.32149816,    0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 198.01527066503698}
episode index:22
target Thresh 6.957049982689021
target distance 3.0
model initialize at round 22
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  8.]), 'previousTarget': array([21.,  8.]), 'currentState': array([23.20719969,  3.03152978,  0.        ]), 'targetState': array([21,  8], dtype=int32), 'currentDistance': 5.4366742378808}
done in step count: 99
reward sum = -0.4637092737417331
running average episode reward sum: -1.1216869932232054
{'scaleFactor': 20, 'currentTarget': array([21.,  8.]), 'previousTarget': array([21.,  8.]), 'currentState': array([23.17770215, -1.71025648,  0.        ]), 'targetState': array([21,  8], dtype=int32), 'currentDistance': 9.951455554067559}
episode index:23
target Thresh 6.982080415404106
target distance 4.0
model initialize at round 23
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 18.]), 'previousTarget': array([ 9., 18.]), 'currentState': array([12.57002234, 19.01372004,  0.        ]), 'targetState': array([ 9, 18], dtype=int32), 'currentDistance': 3.711157211887967}
done in step count: 99
reward sum = -0.10631423241065355
running average episode reward sum: -1.0793797948560158
{'scaleFactor': 20, 'currentTarget': array([ 9., 18.]), 'previousTarget': array([ 9., 18.]), 'currentState': array([11.42749133,  6.8216573 ,  0.        ]), 'targetState': array([ 9, 18], dtype=int32), 'currentDistance': 11.438883670747602}
episode index:24
target Thresh 7.007085830197521
target distance 4.0
model initialize at round 24
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  3.]), 'previousTarget': array([11.,  3.]), 'currentState': array([8.39682388, 2.01002657, 0.        ]), 'targetState': array([11,  3], dtype=int32), 'currentDistance': 2.785062527079715}
done in step count: 99
reward sum = -0.11784176009321314
running average episode reward sum: -1.0409182734655036
{'scaleFactor': 20, 'currentTarget': array([11.,  3.]), 'previousTarget': array([11.,  3.]), 'currentState': array([ 11.20165623, -15.85143855,   0.        ]), 'targetState': array([11,  3], dtype=int32), 'currentDistance': 18.85251709136545}
episode index:25
target Thresh 7.032066252074685
target distance 4.0
model initialize at round 25
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 18.]), 'previousTarget': array([ 9., 18.]), 'currentState': array([12.67003945, 16.15838873,  0.        ]), 'targetState': array([ 9, 18], dtype=int32), 'currentDistance': 4.106180900131368}
done in step count: 99
reward sum = -0.135392223316715
running average episode reward sum: -1.006090348459781
{'scaleFactor': 20, 'currentTarget': array([ 9., 18.]), 'previousTarget': array([ 9., 18.]), 'currentState': array([ 7.88621713, -4.20341414,  0.        ]), 'targetState': array([ 9, 18], dtype=int32), 'currentDistance': 22.231331761948525}
episode index:26
target Thresh 7.057021706016016
target distance 1.0
model initialize at round 26
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 26.]), 'previousTarget': array([20., 26.]), 'currentState': array([20.9605349 , 24.77461886,  0.        ]), 'targetState': array([20, 26], dtype=int32), 'currentDistance': 1.5569798380519129}
done in step count: 99
reward sum = -0.09668609911768412
running average episode reward sum: -0.9724087095952588
{'scaleFactor': 20, 'currentTarget': array([20., 26.]), 'previousTarget': array([20., 26.]), 'currentState': array([20.71594619,  8.99232517,  0.        ]), 'targetState': array([20, 26], dtype=int32), 'currentDistance': 17.02273721240084}
episode index:27
target Thresh 7.081952216976976
target distance 7.0
model initialize at round 27
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 20.]), 'previousTarget': array([18., 20.]), 'currentState': array([24.1930204 , 22.62926519,  0.        ]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 6.728041108059112}
done in step count: 99
reward sum = -0.10390174398616417
running average episode reward sum: -0.9413906036806482
{'scaleFactor': 20, 'currentTarget': array([18., 20.]), 'previousTarget': array([18., 20.]), 'currentState': array([16.57051051,  1.24835929,  0.        ]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 18.806048745115408}
episode index:28
target Thresh 7.10685780988808
target distance 5.0
model initialize at round 28
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 29.]), 'previousTarget': array([26., 29.]), 'currentState': array([22.22229624, 23.70464563,  0.        ]), 'targetState': array([26, 29], dtype=int32), 'currentDistance': 6.5047539214454355}
done in step count: 99
reward sum = -0.0694388908061817
running average episode reward sum: -0.9113233032367012
{'scaleFactor': 20, 'currentTarget': array([26., 29.]), 'previousTarget': array([26., 29.]), 'currentState': array([24.09910752,  3.28058437,  0.        ]), 'targetState': array([26, 29], dtype=int32), 'currentDistance': 25.789566352100692}
episode index:29
target Thresh 7.131738509654916
target distance 2.0
model initialize at round 29
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 12.]), 'previousTarget': array([18., 12.]), 'currentState': array([17.50807649, 12.70941448,  0.        ]), 'targetState': array([18, 12], dtype=int32), 'currentDistance': 0.8632830637394215}
done in step count: 0
reward sum = 0.9903812365074529
running average episode reward sum: -0.847933151911896
{'scaleFactor': 20, 'currentTarget': array([18., 12.]), 'previousTarget': array([18., 12.]), 'currentState': array([17.50807649, 12.70941448,  0.        ]), 'targetState': array([18, 12], dtype=int32), 'currentDistance': 0.8632830637394215}
episode index:30
target Thresh 7.15659434115819
target distance 7.0
model initialize at round 30
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 12.]), 'previousTarget': array([ 7., 12.]), 'currentState': array([12.51319242, 15.81708521,  0.        ]), 'targetState': array([ 7, 12], dtype=int32), 'currentDistance': 6.705626747827394}
done in step count: 99
reward sum = -0.1414506991229547
running average episode reward sum: -0.8251433953703173
{'scaleFactor': 20, 'currentTarget': array([ 7., 12.]), 'previousTarget': array([ 7., 12.]), 'currentState': array([  7.5267334 , -33.63271774,   0.        ]), 'targetState': array([ 7, 12], dtype=int32), 'currentDistance': 45.63575765489576}
episode index:31
target Thresh 7.1814253292537344
target distance 7.0
model initialize at round 31
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 18.]), 'previousTarget': array([26., 18.]), 'currentState': array([26.59877843, 23.48187971,  0.        ]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 5.514484633376823}
done in step count: 99
reward sum = -0.28601675472002797
running average episode reward sum: -0.8082956878499957
{'scaleFactor': 20, 'currentTarget': array([26., 18.]), 'previousTarget': array([26., 18.]), 'currentState': array([ 32.58226475, -67.11336726,   0.        ]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 85.36750842961528}
episode index:32
target Thresh 7.206231498772539
target distance 6.0
model initialize at round 32
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 11.]), 'previousTarget': array([27., 11.]), 'currentState': array([23.04813707, 15.2312876 ,  0.        ]), 'targetState': array([27, 11], dtype=int32), 'currentDistance': 5.789733618185879}
done in step count: 99
reward sum = -0.37025075814605657
running average episode reward sum: -0.7950215990710885
{'scaleFactor': 20, 'currentTarget': array([27., 11.]), 'previousTarget': array([27., 11.]), 'currentState': array([ 31.50120845, -83.2394097 ,   0.        ]), 'targetState': array([27, 11], dtype=int32), 'currentDistance': 94.3468453001245}
episode index:33
target Thresh 7.231012874520779
target distance 7.0
model initialize at round 33
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 12.]), 'previousTarget': array([22., 12.]), 'currentState': array([24.32794848, 17.83144021,  0.        ]), 'targetState': array([22, 12], dtype=int32), 'currentDistance': 6.278936140482955}
done in step count: 99
reward sum = -0.22478060972031536
running average episode reward sum: -0.778249805266654
{'scaleFactor': 20, 'currentTarget': array([22., 12.]), 'previousTarget': array([22., 12.]), 'currentState': array([ 24.37117531, -59.9822927 ,   0.        ]), 'targetState': array([22, 12], dtype=int32), 'currentDistance': 72.02133665963692}
episode index:34
target Thresh 7.2557694812798275
target distance 4.0
model initialize at round 34
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 5.6954954 , 12.21031654,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 4.306063797704151}
done in step count: 99
reward sum = -0.0386484479390865
running average episode reward sum: -0.7571183379144377
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 3.40900531, -5.80744028,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 15.870112287590732}
episode index:35
target Thresh 7.280501343806296
target distance 5.0
model initialize at round 35
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([5.20601797, 5.27386114, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 3.6411683007768003}
done in step count: 99
reward sum = -0.05358147709994706
running average episode reward sum: -0.7375756473362575
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([  6.00221237, -14.95595257,   0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 22.317740860059597}
episode index:36
target Thresh 7.305208486832047
target distance 6.0
model initialize at round 36
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([9.97553396, 6.32492626, 0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 5.536293079815544}
done in step count: 99
reward sum = -0.0924851003380054
running average episode reward sum: -0.720140767687656
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([ 19.68848573, -11.15068131,   0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 15.859541059163307}
episode index:37
target Thresh 7.329890935064228
target distance 7.0
model initialize at round 37
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 15.]), 'previousTarget': array([ 3., 15.]), 'currentState': array([ 9.07085443, 20.83603954,  0.        ]), 'targetState': array([ 3, 15], dtype=int32), 'currentDistance': 8.421082536970074}
done in step count: 99
reward sum = -0.04356944937669573
running average episode reward sum: -0.7023362593110519
{'scaleFactor': 20, 'currentTarget': array([ 3., 15.]), 'previousTarget': array([ 3., 15.]), 'currentState': array([8.78122436, 2.20183258, 0.        ]), 'targetState': array([ 3, 15], dtype=int32), 'currentDistance': 14.0433487631769}
episode index:38
target Thresh 7.354548713185292
target distance 4.0
model initialize at round 38
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 24.]), 'previousTarget': array([17., 24.]), 'currentState': array([18.8641075, 19.5761967,  0.       ]), 'targetState': array([17, 24], dtype=int32), 'currentDistance': 4.8005137665064295}
done in step count: 99
reward sum = -0.010973533004238912
running average episode reward sum: -0.6846090099185697
{'scaleFactor': 20, 'currentTarget': array([17., 24.]), 'previousTarget': array([17., 24.]), 'currentState': array([22.19616342, 12.26371175,  0.        ]), 'targetState': array([17, 24], dtype=int32), 'currentDistance': 12.835130549873586}
episode index:39
target Thresh 7.379181845853012
target distance 6.0
model initialize at round 39
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 20.]), 'previousTarget': array([ 6., 20.]), 'currentState': array([ 8.85797159, 13.7552247 ,  0.        ]), 'targetState': array([ 6, 20], dtype=int32), 'currentDistance': 6.867693944758412}
done in step count: 99
reward sum = -0.008993532980369376
running average episode reward sum: -0.6677186229951146
{'scaleFactor': 20, 'currentTarget': array([ 6., 20.]), 'previousTarget': array([ 6., 20.]), 'currentState': array([13.2331281 , 11.36921529,  0.        ]), 'targetState': array([ 6, 20], dtype=int32), 'currentDistance': 11.260931879741053}
episode index:40
target Thresh 7.403790357700526
target distance 5.0
model initialize at round 40
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 15.]), 'previousTarget': array([27., 15.]), 'currentState': array([22.34453824, 18.27730811,  0.        ]), 'targetState': array([27, 15], dtype=int32), 'currentDistance': 5.693335806723213}
done in step count: 99
reward sum = -0.10386610460527577
running average episode reward sum: -0.653966122546582
{'scaleFactor': 20, 'currentTarget': array([27., 15.]), 'previousTarget': array([27., 15.]), 'currentState': array([34.87235563, 10.13650573,  0.        ]), 'targetState': array([27, 15], dtype=int32), 'currentDistance': 9.253516071684496}
episode index:41
target Thresh 7.428374273336345
target distance 4.0
model initialize at round 41
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 16.]), 'previousTarget': array([20., 16.]), 'currentState': array([24.12253515, 14.393466  ,  0.        ]), 'targetState': array([20, 16], dtype=int32), 'currentDistance': 4.424505351391992}
done in step count: 99
reward sum = -0.015035640738302027
running average episode reward sum: -0.6387534920273372
{'scaleFactor': 20, 'currentTarget': array([20., 16.]), 'previousTarget': array([20., 16.]), 'currentState': array([28.66992732, 12.87062702,  0.        ]), 'targetState': array([20, 16], dtype=int32), 'currentDistance': 9.217408253427562}
episode index:42
target Thresh 7.452933617344396
target distance 3.0
model initialize at round 42
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([20.29831699, 21.15796059,  0.        ]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 3.495678428078702}
done in step count: 99
reward sum = -0.034403091733843726
running average episode reward sum: -0.6246988315553955
{'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([31.62259492, 27.24721964,  0.        ]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 16.320002289699612}
episode index:43
target Thresh 7.477468414284015
target distance 7.0
model initialize at round 43
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 7.]), 'previousTarget': array([5., 7.]), 'currentState': array([12.84230804,  6.79071458,  0.        ]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 7.845100117406748}
done in step count: 99
reward sum = -0.05524187366781836
running average episode reward sum: -0.6117566279670416
{'scaleFactor': 20, 'currentTarget': array([5., 7.]), 'previousTarget': array([5., 7.]), 'currentState': array([27.10169171, 28.02895201,  0.        ]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 30.50740236436515}
episode index:44
target Thresh 7.501978688690002
target distance 6.0
model initialize at round 44
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 28.]), 'previousTarget': array([23., 28.]), 'currentState': array([19.38832   , 20.58919585,  0.        ]), 'targetState': array([23, 28], dtype=int32), 'currentDistance': 8.24404334148695}
done in step count: 99
reward sum = -0.11518153388927185
running average episode reward sum: -0.6007216258764244
{'scaleFactor': 20, 'currentTarget': array([23., 28.]), 'previousTarget': array([23., 28.]), 'currentState': array([45.8532831 , 44.35495077,  0.        ]), 'targetState': array([23, 28], dtype=int32), 'currentDistance': 28.102614882694226}
episode index:45
target Thresh 7.526464465072639
target distance 7.0
model initialize at round 45
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([5.30396155, 1.27366936, 0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 10.99935349874166}
done in step count: 99
reward sum = -0.38566503751800774
running average episode reward sum: -0.5960464826512414
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([43.5893113 , 44.21389559,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 46.56688994796638}
episode index:46
target Thresh 7.550925767917704
target distance 4.0
model initialize at round 46
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 13.]), 'previousTarget': array([ 2., 13.]), 'currentState': array([3.428491 , 8.5048877, 0.       ]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 4.7166323913961845}
done in step count: 99
reward sum = -0.5344173196432989
running average episode reward sum: -0.5947352238638384
{'scaleFactor': 20, 'currentTarget': array([ 2., 13.]), 'previousTarget': array([ 2., 13.]), 'currentState': array([30.63366872, 46.09060086,  0.        ]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 43.759283010383314}
episode index:47
target Thresh 7.575362621686498
target distance 5.0
model initialize at round 47
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([17.39708918,  2.66666394,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 5.513286829534239}
done in step count: 99
reward sum = -0.5073688600603257
running average episode reward sum: -0.5929150912845985
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([35.95521137, 29.38289869,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 29.24788568581501}
episode index:48
target Thresh 7.599775050815879
target distance 1.0
model initialize at round 48
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 19.]), 'previousTarget': array([17., 19.]), 'currentState': array([17.22263825, 16.69632435,  0.        ]), 'targetState': array([17, 19], dtype=int32), 'currentDistance': 2.314409060136171}
done in step count: 99
reward sum = -0.20356803657260214
running average episode reward sum: -0.58496923302517
{'scaleFactor': 20, 'currentTarget': array([17., 19.]), 'previousTarget': array([17., 19.]), 'currentState': array([36.55649071, 36.51460413,  0.        ]), 'targetState': array([17, 19], dtype=int32), 'currentDistance': 26.25295576916456}
episode index:49
target Thresh 7.624163079718272
target distance 5.0
model initialize at round 49
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.95344245, 6.17079568, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 3.311041792591157}
done in step count: 99
reward sum = -0.20029951442860977
running average episode reward sum: -0.5772758386532388
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([16.92611499, 19.57070351,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 21.01600970156959}
episode index:50
target Thresh 7.648526732781718
target distance 7.0
model initialize at round 50
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  5.]), 'previousTarget': array([19.,  5.]), 'currentState': array([15.23533249, 10.00647545,  0.        ]), 'targetState': array([19,  5], dtype=int32), 'currentDistance': 6.263985782694103}
done in step count: 3
reward sum = 0.8726905719322529
running average episode reward sum: -0.54884512472019
{'scaleFactor': 20, 'currentTarget': array([19.,  5.]), 'previousTarget': array([19.,  5.]), 'currentState': array([18.33559388,  4.25900459,  0.        ]), 'targetState': array([19,  5], dtype=int32), 'currentDistance': 0.9952435313409312}
episode index:51
target Thresh 7.672866034369868
target distance 6.0
model initialize at round 51
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 14.]), 'previousTarget': array([ 2., 14.]), 'currentState': array([ 8.82211989, 16.90402321,  0.        ]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 7.4144905831053}
done in step count: 99
reward sum = -0.047188243367500424
running average episode reward sum: -0.539197877001869
{'scaleFactor': 20, 'currentTarget': array([ 2., 14.]), 'previousTarget': array([ 2., 14.]), 'currentState': array([16.3323516 , 30.40883518,  0.        ]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 21.78683488930198}
episode index:52
target Thresh 7.697181008822028
target distance 7.0
model initialize at round 52
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 13.]), 'previousTarget': array([ 7., 13.]), 'currentState': array([15.53557312, 11.07668369,  0.        ]), 'targetState': array([ 7, 13], dtype=int32), 'currentDistance': 8.749580229270965}
done in step count: 99
reward sum = -0.08773149084881977
running average episode reward sum: -0.5306796433008681
{'scaleFactor': 20, 'currentTarget': array([ 7., 13.]), 'previousTarget': array([ 7., 13.]), 'currentState': array([22.77519164, 28.77351714,  0.        ]), 'targetState': array([ 7, 13], dtype=int32), 'currentDistance': 22.30830594947749}
episode index:53
target Thresh 7.721471680453163
target distance 7.0
model initialize at round 53
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 19.]), 'previousTarget': array([18., 19.]), 'currentState': array([15.57175431, 10.17929685,  0.        ]), 'targetState': array([18, 19], dtype=int32), 'currentDistance': 9.148834963410629}
done in step count: 99
reward sum = -1.352806204589097
running average episode reward sum: -0.5459042092506502
{'scaleFactor': 20, 'currentTarget': array([18.44186599, 12.07732609]), 'previousTarget': array([18.34156472, 14.01556036]), 'currentState': array([  31.18170117, -187.51650298,    0.        ]), 'targetState': array([18, 19], dtype=int32), 'currentDistance': 200.0}
episode index:54
target Thresh 7.745738073553962
target distance 7.0
model initialize at round 54
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 22.]), 'previousTarget': array([23., 22.]), 'currentState': array([17.40733236, 13.00958776,  0.        ]), 'targetState': array([23, 22], dtype=int32), 'currentDistance': 10.587985808598766}
done in step count: 99
reward sum = -1.5347370440010093
running average episode reward sum: -0.5638829880642932
{'scaleFactor': 20, 'currentTarget': array([25.63507628, 10.21753744]), 'previousTarget': array([25.41042388, 11.73905473]), 'currentState': array([  69.28555739, -184.96093345,    0.        ]), 'targetState': array([23, 22], dtype=int32), 'currentDistance': 199.99999999999997}
episode index:55
target Thresh 7.769980212390813
target distance 5.0
model initialize at round 55
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([ 4.9214515 , 17.03733897,  0.        ]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 4.324683674987803}
done in step count: 99
reward sum = -1.7042963016733679
running average episode reward sum: -0.584247511521598
{'scaleFactor': 20, 'currentTarget': array([ 8.98020387, 10.7317028 ]), 'previousTarget': array([ 8.40760118, 12.64807151]), 'currentState': array([  66.4344392, -180.8381561,    0.       ]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 200.0}
episode index:56
target Thresh 7.794198121205852
target distance 4.0
model initialize at round 56
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([14.06131212, 20.05935943,  0.        ]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 2.2632048536167098}
done in step count: 99
reward sum = -1.3842437860366554
running average episode reward sum: -0.5982825338815113
{'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([  47.14916028, -177.72903347,    0.        ]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 198.35176593656806}
episode index:57
target Thresh 7.818391824217002
target distance 6.0
model initialize at round 57
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([9.90543865, 5.19519311, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 6.025171555429479}
done in step count: 99
reward sum = -1.3336153460007383
running average episode reward sum: -0.6109606858146015
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([  52.39292011, -189.347565  ,    0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 199.3117046397605}
episode index:58
target Thresh 7.8425613456179555
target distance 4.0
model initialize at round 58
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 25.]), 'previousTarget': array([26., 25.]), 'currentState': array([24.81115922, 27.03791702,  0.        ]), 'targetState': array([26, 25], dtype=int32), 'currentDistance': 2.3593321437004504}
done in step count: 99
reward sum = -1.4431009312954328
running average episode reward sum: -0.6250647577719037
{'scaleFactor': 20, 'currentTarget': array([27.10924389, 21.18459932]), 'previousTarget': array([26.49612304, 23.28227678]), 'currentState': array([  82.94309057, -170.86378407,    0.        ]), 'targetState': array([26, 25], dtype=int32), 'currentDistance': 200.0}
episode index:59
target Thresh 7.866706709578246
target distance 1.0
model initialize at round 59
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.58561429, 2.02697968, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.7167572512146707}
done in step count: 99
reward sum = -1.6980969996503228
running average episode reward sum: -0.6429486284698774
{'scaleFactor': 20, 'currentTarget': array([ 7.79080138, -9.38425652]), 'previousTarget': array([ 6.80911983, -7.00402562]), 'currentState': array([  79.94895597, -195.91361272,    0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 200.0}
episode index:60
target Thresh 7.890827940243231
target distance 7.0
model initialize at round 60
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([13.56502053,  1.02069342,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 9.33848365419805}
done in step count: 99
reward sum = -1.555591414644753
running average episode reward sum: -0.6579099856202852
{'scaleFactor': 20, 'currentTarget': array([12.01819229,  1.5893552 ]), 'previousTarget': array([11.80012777,  3.52791504]), 'currentState': array([  36.05469046, -196.96100807,    0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 199.99999999999997}
episode index:61
target Thresh 7.914925061734152
target distance 5.0
model initialize at round 61
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'previousTarget': array([ 5., 22.]), 'currentState': array([ 8.93705445, 25.30803001,  0.        ]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 5.142320513873536}
done in step count: 99
reward sum = -1.3410201753638327
running average episode reward sum: -0.6689278919064714
{'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'previousTarget': array([ 5., 22.]), 'currentState': array([  30.35818899, -171.98448443,    0.        ]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 195.6349098445291}
episode index:62
target Thresh 7.938998098148122
target distance 6.0
model initialize at round 62
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'previousTarget': array([10., 12.]), 'currentState': array([ 3.87787881, 12.00045872,  0.        ]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 6.122121209700244}
done in step count: 99
reward sum = -1.5402072294806026
running average episode reward sum: -0.6827577226616163
{'scaleFactor': 20, 'currentTarget': array([10.34027502, 10.26466947]), 'previousTarget': array([10., 12.]), 'currentState': array([  48.82470636, -185.99777865,    0.        ]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 200.0}
episode index:63
target Thresh 7.963047073558187
target distance 5.0
model initialize at round 63
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([11.84317797,  7.00118721,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 5.12562655014432}
done in step count: 99
reward sum = -1.5815182041052576
running average episode reward sum: -0.6968008551841732
{'scaleFactor': 20, 'currentTarget': array([19.56152854, -0.89329968]), 'previousTarget': array([18.97864218,  0.96103775]), 'currentState': array([  81.71335941, -190.99103749,    0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 200.0}
episode index:64
target Thresh 7.987072012013325
target distance 6.0
model initialize at round 64
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 13.]), 'previousTarget': array([ 3., 13.]), 'currentState': array([3.57594848, 5.00853896, 0.        ]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 8.012188601075959}
done in step count: 99
reward sum = -1.5107289253227265
running average episode reward sum: -0.7093228254939971
{'scaleFactor': 20, 'currentTarget': array([4.73798397, 3.64836452]), 'previousTarget': array([4.40712337, 5.54509353]), 'currentState': array([  41.28186319, -192.98465633,    0.        ]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 200.0}
episode index:65
target Thresh 8.011072937538472
target distance 5.0
model initialize at round 65
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 13.]), 'previousTarget': array([ 4., 13.]), 'currentState': array([5.60616055, 6.01765597, 0.        ]), 'targetState': array([ 4, 13], dtype=int32), 'currentDistance': 7.164696775753141}
done in step count: 99
reward sum = -1.4003370147741871
running average episode reward sum: -0.7197927374527878
{'scaleFactor': 20, 'currentTarget': array([5.1545025 , 5.62626107]), 'previousTarget': array([4.8461308 , 7.61201979]), 'currentState': array([  36.09150036, -191.96650743,    0.        ]), 'targetState': array([ 4, 13], dtype=int32), 'currentDistance': 200.0}
episode index:66
target Thresh 8.035049874134558
target distance 5.0
model initialize at round 66
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 28.]), 'previousTarget': array([ 9., 28.]), 'currentState': array([ 6.78606845, 21.0022831 ,  0.        ]), 'targetState': array([ 9, 28], dtype=int32), 'currentDistance': 7.3395868243935904}
done in step count: 99
reward sum = -1.5623991167646072
running average episode reward sum: -0.7323689520693822
{'scaleFactor': 20, 'currentTarget': array([16.40755587,  9.14916087]), 'previousTarget': array([15.66407668, 11.08483953]), 'currentState': array([  89.55401871, -176.99485588,    0.        ]), 'targetState': array([ 9, 28], dtype=int32), 'currentDistance': 200.0}
episode index:67
target Thresh 8.059002845778519
target distance 3.0
model initialize at round 67
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 17.]), 'previousTarget': array([15., 17.]), 'currentState': array([11.76927683, 16.00146472,  0.        ]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 3.3815151800914616}
done in step count: 99
reward sum = -1.6195930944349342
running average episode reward sum: -0.7454163659276991
{'scaleFactor': 20, 'currentTarget': array([18.61868794,  6.69929032]), 'previousTarget': array([17.72852834,  9.08496143]), 'currentState': array([  84.90805487, -181.99548924,    0.        ]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 200.00000000000003}
episode index:68
target Thresh 8.08293187642333
target distance 5.0
model initialize at round 68
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 26.]), 'previousTarget': array([13., 26.]), 'currentState': array([17.62120542, 23.21706867,  0.        ]), 'targetState': array([13, 26], dtype=int32), 'currentDistance': 5.3944644129652115}
done in step count: 99
reward sum = -1.5389978771594657
running average episode reward sum: -0.7569175472498987
{'scaleFactor': 20, 'currentTarget': array([21.07600801,  7.94805238]), 'previousTarget': array([20.0698866, 10.1115513]), 'currentState': array([ 102.75037204, -174.61508262,    0.        ]), 'targetState': array([13, 26], dtype=int32), 'currentDistance': 200.0}
episode index:69
target Thresh 8.10683698999803
target distance 8.0
model initialize at round 69
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 18.]), 'previousTarget': array([ 5., 18.]), 'currentState': array([13.42375931, 15.30635571,  0.        ]), 'targetState': array([ 5, 18], dtype=int32), 'currentDistance': 8.843949370741157}
done in step count: 99
reward sum = -1.1543053020953784
running average episode reward sum: -0.7625945151762626
{'scaleFactor': 20, 'currentTarget': array([ 5., 18.]), 'previousTarget': array([ 5., 18.]), 'currentState': array([  50.82912627, -170.30987322,    0.        ]), 'targetState': array([ 5, 18], dtype=int32), 'currentDistance': 193.80639093754039}
episode index:70
target Thresh 8.130718210407721
target distance 6.0
model initialize at round 70
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 20.]), 'previousTarget': array([14., 20.]), 'currentState': array([ 9.83654705, 24.00041854,  0.        ]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 5.773879892964745}
done in step count: 99
reward sum = -1.437751670450587
running average episode reward sum: -0.7721037708843518
{'scaleFactor': 20, 'currentTarget': array([14.367697  , 18.67885533]), 'previousTarget': array([14., 20.]), 'currentState': array([  67.99291366, -173.99791062,    0.        ]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 199.99999999999997}
episode index:71
target Thresh 8.15457556153364
target distance 4.0
model initialize at round 71
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 17.]), 'previousTarget': array([ 7., 17.]), 'currentState': array([ 9.53109226, 11.0399828 ,  0.        ]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 6.475201393416953}
done in step count: 99
reward sum = -1.3951905067411183
running average episode reward sum: -0.7807577533268069
{'scaleFactor': 20, 'currentTarget': array([8.95445861, 8.25503708]), 'previousTarget': array([ 8.60212778, 10.01222351]), 'currentState': array([  52.57732072, -186.92960858,    0.        ]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 200.0}
episode index:72
target Thresh 8.178409067233133
target distance 6.0
model initialize at round 72
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 19.]), 'previousTarget': array([17., 19.]), 'currentState': array([17.59722793, 11.00285816,  0.        ]), 'targetState': array([17, 19], dtype=int32), 'currentDistance': 8.01941137365223}
done in step count: 99
reward sum = -1.3522770369824124
running average episode reward sum: -0.7885867846097604
{'scaleFactor': 20, 'currentTarget': array([18.03442284, 11.24869503]), 'previousTarget': array([17.77533431, 13.22667652]), 'currentState': array([  44.49017214, -186.99381638,    0.        ]), 'targetState': array([17, 19], dtype=int32), 'currentDistance': 199.99999999999997}
episode index:73
target Thresh 8.202218751339704
target distance 5.0
model initialize at round 73
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([21.4992781 ,  9.52991915,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 6.534706480842289}
done in step count: 99
reward sum = -1.1989225894108704
running average episode reward sum: -0.7941318630530186
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([  29.50848065, -182.24191022,    0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 188.72598075624813}
episode index:74
target Thresh 8.226004637663046
target distance 5.0
model initialize at round 74
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 16.]), 'previousTarget': array([10., 16.]), 'currentState': array([11.63462877,  9.00403666,  0.        ]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 7.1843937955711406}
done in step count: 99
reward sum = -1.3036988269145908
running average episode reward sum: -0.8009260892378396
{'scaleFactor': 20, 'currentTarget': array([10.80103629,  9.50556987]), 'previousTarget': array([10.55473588, 11.5043311 ]), 'currentState': array([  35.28391587, -188.99024522,    0.        ]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 200.00000000000003}
episode index:75
target Thresh 8.249766749989046
target distance 6.0
model initialize at round 75
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 19.]), 'previousTarget': array([12., 19.]), 'currentState': array([18.22881995, 20.72660244,  0.        ]), 'targetState': array([12, 19], dtype=int32), 'currentDistance': 6.463695069856015}
done in step count: 99
reward sum = -1.332615663342993
running average episode reward sum: -0.8079220046865916
{'scaleFactor': 20, 'currentTarget': array([12., 19.]), 'previousTarget': array([12., 19.]), 'currentState': array([  43.23498662, -175.10643291,    0.        ]), 'targetState': array([12, 19], dtype=int32), 'currentDistance': 196.60348849345755}
episode index:76
target Thresh 8.273505112079814
target distance 7.0
model initialize at round 76
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 21.]), 'previousTarget': array([15., 21.]), 'currentState': array([22.03158313, 14.50147665,  0.        ]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 9.5746523258511}
done in step count: 99
reward sum = -1.3807384902379503
running average episode reward sum: -0.8153611798236222
{'scaleFactor': 20, 'currentTarget': array([19.889276  ,  6.65793702]), 'previousTarget': array([19.23254614,  8.61274824]), 'currentState': array([  84.42330888, -182.6443572 ,    0.        ]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 200.00000000000003}
episode index:77
target Thresh 8.297219747673719
target distance 3.0
model initialize at round 77
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 26.]), 'previousTarget': array([12., 26.]), 'currentState': array([12.67372048, 21.00110412,  0.        ]), 'targetState': array([12, 26], dtype=int32), 'currentDistance': 5.044091527571035}
done in step count: 99
reward sum = -1.437985777099648
running average episode reward sum: -0.8233435464553662
{'scaleFactor': 20, 'currentTarget': array([15.85291834, 13.76613656]), 'previousTarget': array([15.20398175, 15.80413061]), 'currentState': array([  75.93161314, -176.99693751,    0.        ]), 'targetState': array([12, 26], dtype=int32), 'currentDistance': 199.99999999999997}
episode index:78
target Thresh 8.320910680485394
target distance 5.0
model initialize at round 78
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([7.10703436, 4.37789094, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 5.289648699312538}
done in step count: 99
reward sum = -1.3938072898785687
running average episode reward sum: -0.8305646064986979
{'scaleFactor': 20, 'currentTarget': array([ 3.61090704, -2.14784871]), 'previousTarget': array([ 2.99484322, -0.17422657]), 'currentState': array([  63.34036173, -193.02056086,    0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 200.0}
episode index:79
target Thresh 8.344577934205777
target distance 5.0
model initialize at round 79
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 20.]), 'previousTarget': array([10., 20.]), 'currentState': array([14.63327929, 13.02598631,  0.        ]), 'targetState': array([10, 20], dtype=int32), 'currentDistance': 8.372821741154965}
done in step count: 99
reward sum = -1.3840094589536562
running average episode reward sum: -0.8374826671543849
{'scaleFactor': 20, 'currentTarget': array([13.07220457,  8.38698059]), 'previousTarget': array([12.53727143, 10.39951996]), 'currentState': array([  64.2222432 , -184.96159983,    0.        ]), 'targetState': array([10, 20], dtype=int32), 'currentDistance': 200.00000000000003}
episode index:80
target Thresh 8.368221532502123
target distance 5.0
model initialize at round 80
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([ 9.29672074, 24.00000775,  0.        ]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 5.578609348155019}
done in step count: 99
reward sum = -1.4204395215197907
running average episode reward sum: -0.844679665356427
{'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([  56.12046482, -173.99969518,    0.        ]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 199.4969039308825}
episode index:81
target Thresh 8.391841499018032
target distance 7.0
model initialize at round 81
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 21.]), 'previousTarget': array([20., 21.]), 'currentState': array([12.81752925, 21.        ,  0.        ]), 'targetState': array([20, 21], dtype=int32), 'currentDistance': 7.18247075378893}
done in step count: 99
reward sum = -1.421630520654312
running average episode reward sum: -0.8517156513966452
{'scaleFactor': 20, 'currentTarget': array([20.61483307, 18.23423889]), 'previousTarget': array([20.22154809, 20.02532612]), 'currentState': array([  64.01568933, -176.99989152,    0.        ]), 'targetState': array([20, 21], dtype=int32), 'currentDistance': 199.99999999999997}
episode index:82
target Thresh 8.415437857373472
target distance 4.0
model initialize at round 82
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 20.]), 'previousTarget': array([11., 20.]), 'currentState': array([ 8.25886464, 14.00000596,  0.        ]), 'targetState': array([11, 20], dtype=int32), 'currentDistance': 6.596495397541232}
done in step count: 99
reward sum = -1.4253454870307192
running average episode reward sum: -0.85862685423561
{'scaleFactor': 20, 'currentTarget': array([15.00989707,  7.05038628]), 'previousTarget': array([14.36610888,  9.0982172 ]), 'currentState': array([  74.16930531, -183.99977577,    0.        ]), 'targetState': array([11, 20], dtype=int32), 'currentDistance': 200.0}
episode index:83
target Thresh 8.439010631164805
target distance 7.0
model initialize at round 83
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 17.]), 'previousTarget': array([27., 17.]), 'currentState': array([25.90124705,  8.00002778,  0.        ]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 9.066794255557504}
done in step count: 99
reward sum = -1.4206297570708688
running average episode reward sum: -0.8653173649836488
{'scaleFactor': 20, 'currentTarget': array([29.43790523,  5.58807396]), 'previousTarget': array([29.02997724,  7.54509739]), 'currentState': array([  71.22069403, -189.99873173,    0.        ]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 199.99999999999997}
episode index:84
target Thresh 8.462559843964808
target distance 6.0
model initialize at round 84
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 19.]), 'previousTarget': array([ 9., 19.]), 'currentState': array([ 3.74115175, 11.00000024,  0.        ]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 9.573686912621868}
done in step count: 99
reward sum = -1.419995672109655
running average episode reward sum: -0.8718429921263077
{'scaleFactor': 20, 'currentTarget': array([10.71145368,  9.70091253]), 'previousTarget': array([10.30656872, 11.79497002]), 'currentState': array([  46.91251323, -186.99551165,    0.        ]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 200.00000000000003}
episode index:85
target Thresh 8.486085519322689
target distance 2.0
model initialize at round 85
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 29.]), 'previousTarget': array([24., 29.]), 'currentState': array([25.40802139, 25.01025796,  0.        ]), 'targetState': array([24, 29], dtype=int32), 'currentDistance': 4.230906024332813}
done in step count: 99
reward sum = -1.3706402012385799
running average episode reward sum: -0.8776429596741249
{'scaleFactor': 20, 'currentTarget': array([24.73436195, 24.47352786]), 'previousTarget': array([24.44155936, 26.34495997]), 'currentState': array([  56.76301852, -172.94522798,    0.        ]), 'targetState': array([24, 29], dtype=int32), 'currentDistance': 200.0}
episode index:86
target Thresh 8.509587680764135
target distance 7.0
model initialize at round 86
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 29.]), 'previousTarget': array([ 5., 29.]), 'currentState': array([ 3.25710237, 20.0000788 ,  0.        ]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 9.167130074446996}
done in step count: 99
reward sum = -1.5119943092127759
running average episode reward sum: -0.8849343544964082
{'scaleFactor': 20, 'currentTarget': array([ 8.13211131, 16.24906643]), 'previousTarget': array([ 7.72378307, 18.07868593]), 'currentState': array([  55.84141205, -177.97714865,    0.        ]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 200.0}
episode index:87
target Thresh 8.533066351791302
target distance 8.0
model initialize at round 87
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([1.99523816, 4.00007284, 0.        ]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 10.19890255453229}
done in step count: 99
reward sum = -1.4981135626693594
running average episode reward sum: -0.8919023000438281
{'scaleFactor': 20, 'currentTarget': array([ 8.50581072, -1.71055676]), 'previousTarget': array([ 8.10933092, -0.0269185 ]), 'currentState': array([  63.64324419, -193.96003902,    0.        ]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 200.0}
episode index:88
target Thresh 8.556521555882863
target distance 8.0
model initialize at round 88
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 18.]), 'previousTarget': array([11., 18.]), 'currentState': array([ 1.03334022, 23.        ,  0.        ]), 'targetState': array([11, 18], dtype=int32), 'currentDistance': 11.150529460806892}
done in step count: 99
reward sum = -1.7310757721962577
running average episode reward sum: -0.9013312154612713
{'scaleFactor': 20, 'currentTarget': array([12.69693183, 13.26726858]), 'previousTarget': array([12.06439706, 15.05976292]), 'currentState': array([  80.19949291, -174.9969188 ,    0.        ]), 'targetState': array([11, 18], dtype=int32), 'currentDistance': 200.0}
episode index:89
target Thresh 8.579953316494027
target distance 8.0
model initialize at round 89
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 11.]), 'previousTarget': array([17., 11.]), 'currentState': array([25.14481433, 12.19102198,  0.        ]), 'targetState': array([17, 11], dtype=int32), 'currentDistance': 8.231435705954961}
done in step count: 99
reward sum = -0.9622226088298341
running average episode reward sum: -0.9020077864986997
{'scaleFactor': 20, 'currentTarget': array([17., 11.]), 'previousTarget': array([17., 11.]), 'currentState': array([  59.64085352, -159.26724494,    0.        ]), 'targetState': array([17, 11], dtype=int32), 'currentDistance': 175.52543145304296}
episode index:90
target Thresh 8.603361657056556
target distance 5.0
model initialize at round 90
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([15.4290905 , 17.00000072,  0.        ]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 6.868540514386852}
done in step count: 99
reward sum = -1.508595120266627
running average episode reward sum: -0.9086735813752704
{'scaleFactor': 20, 'currentTarget': array([23.60293684, 12.91163713]), 'previousTarget': array([23.02593065, 15.05724562]), 'currentState': array([  74.52346523, -180.49751413,    0.        ]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 199.99999999999997}
episode index:91
target Thresh 8.626746600978795
target distance 4.0
model initialize at round 91
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 24.]), 'previousTarget': array([ 8., 24.]), 'currentState': array([ 2.66659153, 22.00000453,  0.        ]), 'targetState': array([ 8, 24], dtype=int32), 'currentDistance': 5.696071263650116}
done in step count: 99
reward sum = -1.4693233984322323
running average episode reward sum: -0.9147676011258896
{'scaleFactor': 20, 'currentTarget': array([10.70373276, 15.30202962]), 'previousTarget': array([10.04966675, 17.37672699]), 'currentState': array([  70.07094656, -175.68366075,    0.        ]), 'targetState': array([ 8, 24], dtype=int32), 'currentDistance': 200.0}
episode index:92
target Thresh 8.650108171645684
target distance 3.0
model initialize at round 92
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 20.]), 'previousTarget': array([23., 20.]), 'currentState': array([20.9046041 , 15.00025189,  0.        ]), 'targetState': array([23, 20], dtype=int32), 'currentDistance': 5.4210852373111}
done in step count: 99
reward sum = -1.4695482103781539
running average episode reward sum: -0.9207329840210752
{'scaleFactor': 20, 'currentTarget': array([25.91769756, 10.00699637]), 'previousTarget': array([25.27323364, 12.14310384]), 'currentState': array([  81.97207829, -181.97713423,    0.        ]), 'targetState': array([23, 20], dtype=int32), 'currentDistance': 200.0}
episode index:93
target Thresh 8.6734463924188
target distance 4.0
model initialize at round 93
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'previousTarget': array([10., 17.]), 'currentState': array([14.71960586, 16.07844341,  0.        ]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 4.8087364302059346}
done in step count: 99
reward sum = -1.1601659989095805
running average episode reward sum: -0.9232801437539316
{'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'previousTarget': array([10., 17.]), 'currentState': array([  56.46124584, -162.7878812 ,    0.        ]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 185.69418297296156}
episode index:94
target Thresh 8.696761286636363
target distance 7.0
model initialize at round 94
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 13.]), 'previousTarget': array([10., 13.]), 'currentState': array([16.89300251,  5.2056886 ,  0.        ]), 'targetState': array([10, 13], dtype=int32), 'currentDistance': 10.405035985998518}
done in step count: 99
reward sum = -1.3606373209279494
running average episode reward sum: -0.9278839035136581
{'scaleFactor': 20, 'currentTarget': array([11.75705869,  7.90219376]), 'previousTarget': array([11.14597428,  9.70705174]), 'currentState': array([  76.92849167, -181.18160862,    0.        ]), 'targetState': array([10, 13], dtype=int32), 'currentDistance': 200.0}
episode index:95
target Thresh 8.720052877613274
target distance 8.0
model initialize at round 95
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 10.]), 'previousTarget': array([25., 10.]), 'currentState': array([24.01913214, 16.00043178,  0.        ]), 'targetState': array([25, 10], dtype=int32), 'currentDistance': 6.080072636331893}
done in step count: 99
reward sum = -1.6814562549533991
running average episode reward sum: -0.9357336155078221
{'scaleFactor': 20, 'currentTarget': array([25.94912891,  7.2384571 ]), 'previousTarget': array([25.2225756 ,  9.34880118]), 'currentState': array([  90.95580501, -181.90205169,    0.        ]), 'targetState': array([25, 10], dtype=int32), 'currentDistance': 200.0}
episode index:96
target Thresh 8.743321188641119
target distance 4.0
model initialize at round 96
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 23.]), 'previousTarget': array([17., 23.]), 'currentState': array([11.08566284, 21.00000024,  0.        ]), 'targetState': array([17, 23], dtype=int32), 'currentDistance': 6.2433471044968}
done in step count: 99
reward sum = -1.5798737304763335
running average episode reward sum: -0.9423742352497656
{'scaleFactor': 20, 'currentTarget': array([18.61042117, 16.73168723]), 'previousTarget': array([18.19992865, 18.43949116]), 'currentState': array([  68.37715778, -176.97755861,    0.        ]), 'targetState': array([17, 23], dtype=int32), 'currentDistance': 200.0}
episode index:97
target Thresh 8.766566242988219
target distance 8.0
model initialize at round 97
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 13.]), 'previousTarget': array([ 9., 13.]), 'currentState': array([11.5911988 ,  3.24193072,  0.        ]), 'targetState': array([ 9, 13], dtype=int32), 'currentDistance': 10.096248176549214}
done in step count: 99
reward sum = -1.375605840615808
running average episode reward sum: -0.9467949659167659
{'scaleFactor': 20, 'currentTarget': array([11.30663104,  3.75439261]), 'previousTarget': array([10.76697929,  5.8543917 ]), 'currentState': array([  59.71950808, -190.29764509,    0.        ]), 'targetState': array([ 9, 13], dtype=int32), 'currentDistance': 200.0}
episode index:98
target Thresh 8.789788063899621
target distance 7.0
model initialize at round 98
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 13.]), 'previousTarget': array([16., 13.]), 'currentState': array([23.86209893,  9.15843457,  0.        ]), 'targetState': array([16, 13], dtype=int32), 'currentDistance': 8.750441393151165}
done in step count: 99
reward sum = -1.3315517005206785
running average episode reward sum: -0.9506813975794317
{'scaleFactor': 20, 'currentTarget': array([16.21024706, 12.24761078]), 'previousTarget': array([16., 13.]), 'currentState': array([  70.0360606 , -180.37321311,    0.        ]), 'targetState': array([16, 13], dtype=int32), 'currentDistance': 200.0}
episode index:99
target Thresh 8.812986674597155
target distance 6.0
model initialize at round 99
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 22.]), 'previousTarget': array([ 8., 22.]), 'currentState': array([ 4.43683839, 14.00000846,  0.        ]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 8.757624406675186}
done in step count: 99
reward sum = -1.448199582845154
running average episode reward sum: -0.9556565794320889
{'scaleFactor': 20, 'currentTarget': array([12.26529492,  8.02010861]), 'previousTarget': array([11.68504865,  9.96360938]), 'currentState': array([  70.62966943, -183.27442813,    0.        ]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 200.00000000000003}
episode index:100
target Thresh 8.836162098279434
target distance 5.0
model initialize at round 100
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 20.]), 'previousTarget': array([ 7., 20.]), 'currentState': array([13.26739407, 21.38202423,  0.        ]), 'targetState': array([ 7, 20], dtype=int32), 'currentDistance': 6.417960684290654}
done in step count: 99
reward sum = -1.2994816318474167
running average episode reward sum: -0.9590607878718446
{'scaleFactor': 20, 'currentTarget': array([ 7., 20.]), 'previousTarget': array([ 7., 20.]), 'currentState': array([  54.38564684, -167.76017594,    0.        ]), 'targetState': array([ 7, 20], dtype=int32), 'currentDistance': 193.6473165229971}
episode index:101
target Thresh 8.859314358121878
target distance 3.0
model initialize at round 101
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 5.]), 'previousTarget': array([6., 5.]), 'currentState': array([10.31258535,  4.35309809,  0.        ]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 4.360834152961953}
done in step count: 99
reward sum = -1.4478060317175703
running average episode reward sum: -0.9638524079095478
{'scaleFactor': 20, 'currentTarget': array([6.31851199, 4.14938461]), 'previousTarget': array([6., 5.]), 'currentState': array([  76.45269583, -183.15036514,    0.        ]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 200.00000000000003}
episode index:102
target Thresh 8.882443477276752
target distance 7.0
model initialize at round 102
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 18.]), 'previousTarget': array([10., 18.]), 'currentState': array([17.63166118, 10.7636447 ,  0.        ]), 'targetState': array([10, 18], dtype=int32), 'currentDistance': 10.516990558700224}
done in step count: 99
reward sum = -1.3365203945268498
running average episode reward sum: -0.9674705437019487
{'scaleFactor': 20, 'currentTarget': array([12.32459842,  9.22930242]), 'previousTarget': array([11.66877937, 11.5426319 ]), 'currentState': array([  63.56373393, -184.0956856 ,    0.        ]), 'targetState': array([10, 18], dtype=int32), 'currentDistance': 200.0}
episode index:103
target Thresh 8.905549478873176
target distance 6.0
model initialize at round 103
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 29.]), 'previousTarget': array([25., 29.]), 'currentState': array([21.23443556, 21.00482702,  0.        ]), 'targetState': array([25, 29], dtype=int32), 'currentDistance': 8.837548670517123}
done in step count: 99
reward sum = -1.5220181259415393
running average episode reward sum: -0.972802731992714
{'scaleFactor': 20, 'currentTarget': array([27.43014885, 19.30841522]), 'previousTarget': array([27.0792218 , 20.95362896]), 'currentState': array([  76.07389697, -174.68587792,    0.        ]), 'targetState': array([25, 29], dtype=int32), 'currentDistance': 199.99999999999997}
episode index:104
target Thresh 8.928632386017156
target distance 8.0
model initialize at round 104
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 22.]), 'previousTarget': array([24., 22.]), 'currentState': array([17.03229463, 15.00002539,  0.        ]), 'targetState': array([24, 22], dtype=int32), 'currentDistance': 9.876667587181284}
done in step count: 99
reward sum = -1.3723162182166062
running average episode reward sum: -0.9766076223377036
{'scaleFactor': 20, 'currentTarget': array([24.86732072, 16.18581172]), 'previousTarget': array([24.62514345, 17.98892688]), 'currentState': array([  54.37543858, -181.62538862,    0.        ]), 'targetState': array([24, 22], dtype=int32), 'currentDistance': 199.99999999999997}
episode index:105
target Thresh 8.9516922217916
target distance 2.0
model initialize at round 105
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 23.]), 'previousTarget': array([ 6., 23.]), 'currentState': array([ 4.00831461, 23.02135777,  0.        ]), 'targetState': array([ 6, 23], dtype=int32), 'currentDistance': 1.9917999018884414}
done in step count: 99
reward sum = -1.3111788573147258
running average episode reward sum: -0.9797639547431473
{'scaleFactor': 20, 'currentTarget': array([ 6., 23.]), 'previousTarget': array([ 6., 23.]), 'currentState': array([  21.69498668, -171.27435017,    0.        ]), 'targetState': array([ 6, 23], dtype=int32), 'currentDistance': 194.90730037506947}
episode index:106
target Thresh 8.974729009256343
target distance 7.0
model initialize at round 106
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 17.]), 'previousTarget': array([20., 17.]), 'currentState': array([12.60819218, 12.00008345,  0.        ]), 'targetState': array([20, 17], dtype=int32), 'currentDistance': 8.924011903533216}
done in step count: 99
reward sum = -1.632126052607413
running average episode reward sum: -0.9858607967792619
{'scaleFactor': 20, 'currentTarget': array([28.77988665, -2.44341131]), 'previousTarget': array([27.94853154, -0.54478062]), 'currentState': array([ 111.08937602, -184.72107852,    0.        ]), 'targetState': array([20, 17], dtype=int32), 'currentDistance': 200.0}
episode index:107
target Thresh 8.997742771448177
target distance 3.0
model initialize at round 107
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 22.]), 'previousTarget': array([27., 22.]), 'currentState': array([22.00009966, 22.00014484,  0.        ]), 'targetState': array([27, 22], dtype=int32), 'currentDistance': 4.999900343131684}
done in step count: 99
reward sum = -1.3517637070465833
running average episode reward sum: -0.9892487866891445
{'scaleFactor': 20, 'currentTarget': array([27., 22.]), 'previousTarget': array([27., 22.]), 'currentState': array([  38.31399644, -174.72342932,    0.        ]), 'targetState': array([27, 22], dtype=int32), 'currentDistance': 197.04850712403137}
episode index:108
target Thresh 9.020733531380866
target distance 6.0
model initialize at round 108
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 21.]), 'previousTarget': array([25., 21.]), 'currentState': array([23.00000942, 25.00010681,  0.        ]), 'targetState': array([25, 21], dtype=int32), 'currentDistance': 4.472227278834297}
done in step count: 99
reward sum = -1.3913744258637994
running average episode reward sum: -0.9929380127366184
{'scaleFactor': 20, 'currentTarget': array([25., 21.]), 'previousTarget': array([25., 21.]), 'currentState': array([  34.33822221, -171.44591582,    0.        ]), 'targetState': array([25, 21], dtype=int32), 'currentDistance': 192.67234598973457}
episode index:109
target Thresh 9.043701312045172
target distance 9.0
model initialize at round 109
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 15.]), 'previousTarget': array([13., 15.]), 'currentState': array([20.79282308, 22.38258815,  0.        ]), 'targetState': array([13, 15], dtype=int32), 'currentDistance': 10.734556314185742}
done in step count: 99
reward sum = -1.1023874368428646
running average episode reward sum: -0.9939330075012207
{'scaleFactor': 20, 'currentTarget': array([13., 15.]), 'previousTarget': array([13., 15.]), 'currentState': array([  19.79080003, -159.51686209,    0.        ]), 'targetState': array([13, 15], dtype=int32), 'currentDistance': 174.64893392263005}
episode index:110
target Thresh 9.066646136408878
target distance 7.0
model initialize at round 110
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 18.]), 'previousTarget': array([ 8., 18.]), 'currentState': array([15.4625656 ,  9.44774354,  0.        ]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 11.35037338547663}
done in step count: 99
reward sum = -1.2524568125841973
running average episode reward sum: -0.9962620507902565
{'scaleFactor': 20, 'currentTarget': array([ 8.01071742, 12.67877163]), 'previousTarget': array([ 8.0042895 , 14.67901045]), 'currentState': array([   8.41353416, -187.32082272,    0.        ]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 200.00000000000003}
episode index:111
target Thresh 9.089568027416803
target distance 4.0
model initialize at round 111
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 12.]), 'previousTarget': array([16., 12.]), 'currentState': array([11.14829278,  8.00117981,  0.        ]), 'targetState': array([16, 12], dtype=int32), 'currentDistance': 6.28725900743755}
done in step count: 99
reward sum = -1.2927487819952104
running average episode reward sum: -0.9989092537474437
{'scaleFactor': 20, 'currentTarget': array([15.99824033, 11.95262133]), 'previousTarget': array([16., 12.]), 'currentState': array([   8.57522877, -187.90957844,    0.        ]), 'targetState': array([16, 12], dtype=int32), 'currentDistance': 200.0}
episode index:112
target Thresh 9.112467007990855
target distance 9.0
model initialize at round 112
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 4.]), 'previousTarget': array([7., 4.]), 'currentState': array([17.90032649, 12.8449297 ,  0.        ]), 'targetState': array([7, 4], dtype=int32), 'currentDistance': 14.037446310107175}
done in step count: 99
reward sum = -2.167895086208189
running average episode reward sum: -1.009254261114353
{'scaleFactor': 20, 'currentTarget': array([7., 4.]), 'previousTarget': array([7., 4.]), 'currentState': array([184.9623592 ,  82.76446132,   0.        ]), 'targetState': array([7, 4], dtype=int32), 'currentDistance': 194.61357008215688}
episode index:113
target Thresh 9.135343101030003
target distance 9.0
model initialize at round 113
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([ 3.00774205, 11.        ,  0.        ]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 11.394285185937704}
done in step count: 99
reward sum = -1.8083428160185364
running average episode reward sum: -1.0162638098415826
{'scaleFactor': 20, 'currentTarget': array([21.61357884, -3.53925884]), 'previousTarget': array([20.49432693, -1.15947218]), 'currentState': array([ 101.25158787, -186.99984729,    0.        ]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 199.99999999999997}
episode index:114
target Thresh 9.158196329410345
target distance 5.0
model initialize at round 114
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 28.]), 'previousTarget': array([26., 28.]), 'currentState': array([21.39998424, 22.00002623,  0.        ]), 'targetState': array([26, 28], dtype=int32), 'currentDistance': 7.560412044030052}
done in step count: 99
reward sum = -1.3965336933365367
running average episode reward sum: -1.019570504480669
{'scaleFactor': 20, 'currentTarget': array([25.49983299, 23.03168866]), 'previousTarget': array([25.71556879, 25.08936345]), 'currentState': array([   5.46680554, -175.96247661,    0.        ]), 'targetState': array([26, 28], dtype=int32), 'currentDistance': 199.99999999999997}
episode index:115
target Thresh 9.181026715985116
target distance 1.0
model initialize at round 115
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([18.95966184,  0.05443525,  0.        ]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 3.097951343670531}
done in step count: 99
reward sum = -1.4658925846668698
running average episode reward sum: -1.0234181086202054
{'scaleFactor': 20, 'currentTarget': array([17.9526222,  2.5047129]), 'previousTarget': array([18.,  3.]), 'currentState': array([  -1.09189351, -196.58648837,    0.        ]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 200.0}
episode index:116
target Thresh 9.203834283584698
target distance 8.0
model initialize at round 116
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 23.]), 'previousTarget': array([27., 23.]), 'currentState': array([19.69164014, 16.00000072,  0.        ]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 10.119887047282699}
done in step count: 99
reward sum = -1.2941057449987279
running average episode reward sum: -1.0257316781619021
{'scaleFactor': 20, 'currentTarget': array([27., 23.]), 'previousTarget': array([27., 23.]), 'currentState': array([75.12352194, 61.48208332,  0.        ]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 61.61772553537781}
episode index:117
target Thresh 9.22661905501667
target distance 9.0
model initialize at round 117
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 13.]), 'previousTarget': array([13., 13.]), 'currentState': array([14.55351019,  3.12681109,  0.        ]), 'targetState': array([13, 13], dtype=int32), 'currentDistance': 9.994661235714387}
done in step count: 99
reward sum = -1.2247170726542094
running average episode reward sum: -1.0274179950643794
{'scaleFactor': 20, 'currentTarget': array([13., 13.]), 'previousTarget': array([13., 13.]), 'currentState': array([66.67271945, 45.36468982,  0.        ]), 'targetState': array([13, 13], dtype=int32), 'currentDistance': 62.675624933341346}
episode index:118
target Thresh 9.249381053065793
target distance 9.0
model initialize at round 118
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([25.94096947,  8.19317234,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 12.572199348568462}
done in step count: 99
reward sum = -1.404664820673725
running average episode reward sum: -1.0305881364560545
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([154.96239431,  64.00542821,   0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 153.0821509754879}
episode index:119
target Thresh 9.272120300494077
target distance 7.0
model initialize at round 119
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  4.]), 'previousTarget': array([25.,  4.]), 'currentState': array([16.,  7.,  0.]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 9.486832980505124}
done in step count: 99
reward sum = -1.7813451016828412
running average episode reward sum: -1.0368444444996112
{'scaleFactor': 20, 'currentTarget': array([25.,  4.]), 'previousTarget': array([25.,  4.]), 'currentState': array([158.90555988,  55.05815802,   0.        ]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 143.30957562628407}
episode index:120
target Thresh 9.294836820040768
target distance 8.0
model initialize at round 120
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 22.]), 'previousTarget': array([23., 22.]), 'currentState': array([13.62350965, 18.        ,  0.        ]), 'targetState': array([23, 22], dtype=int32), 'currentDistance': 10.194045878292995}
done in step count: 99
reward sum = -1.4064325633555759
running average episode reward sum: -1.0398988917628837
{'scaleFactor': 20, 'currentTarget': array([23., 22.]), 'previousTarget': array([23., 22.]), 'currentState': array([   8.19674853, -171.75722522,    0.        ]), 'targetState': array([23, 22], dtype=int32), 'currentDistance': 194.32189423052094}
episode index:121
target Thresh 9.317530634422383
target distance 8.0
model initialize at round 121
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([10.99427557,  2.48734403,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 10.411371251191575}
done in step count: 99
reward sum = -1.6831150629930005
running average episode reward sum: -1.045171155461491
{'scaleFactor': 20, 'currentTarget': array([13.90942595,  2.19092454]), 'previousTarget': array([12.04452333,  4.04873012]), 'currentState': array([ 156.12945511, -138.4272226 ,    0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 200.0}
episode index:122
target Thresh 9.340201766332743
target distance 2.0
model initialize at round 122
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 18.]), 'previousTarget': array([17., 18.]), 'currentState': array([13.00056386, 17.00001442,  0.        ]), 'targetState': array([17, 18], dtype=int32), 'currentDistance': 4.122555104541625}
done in step count: 99
reward sum = -1.235766953963161
running average episode reward sum: -1.046720714798903
{'scaleFactor': 20, 'currentTarget': array([17., 18.]), 'previousTarget': array([17., 18.]), 'currentState': array([   9.10236924, -164.26938367,    0.        ]), 'targetState': array([17, 18], dtype=int32), 'currentDistance': 182.44040340505043}
episode index:123
target Thresh 9.362850238442984
target distance 3.0
model initialize at round 123
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 24.]), 'previousTarget': array([11., 24.]), 'currentState': array([15.99889076, 23.32520807,  0.        ]), 'targetState': array([11, 24], dtype=int32), 'currentDistance': 5.044229669400546}
done in step count: 99
reward sum = -1.7791098813985715
running average episode reward sum: -1.0526270790456747
{'scaleFactor': 20, 'currentTarget': array([21.50226309, 13.43441106]), 'previousTarget': array([21.32025668, 13.84728233]), 'currentState': array([ 162.49789743, -128.41138937,    0.        ]), 'targetState': array([11, 24], dtype=int32), 'currentDistance': 199.99999999999997}
episode index:124
target Thresh 9.385476073401577
target distance 4.0
model initialize at round 124
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 13.]), 'previousTarget': array([26., 13.]), 'currentState': array([25.26319659,  7.2410872 ,  0.        ]), 'targetState': array([26, 13], dtype=int32), 'currentDistance': 5.805855313961351}
done in step count: 99
reward sum = -1.5684755502703396
running average episode reward sum: -1.056753866815472
{'scaleFactor': 20, 'currentTarget': array([26., 13.]), 'previousTarget': array([26., 13.]), 'currentState': array([ 141.56340629, -126.55042319,    0.        ]), 'targetState': array([26, 13], dtype=int32), 'currentDistance': 181.18835913600495}
episode index:125
target Thresh 9.408079293834355
target distance 8.0
model initialize at round 125
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 15.]), 'previousTarget': array([23., 15.]), 'currentState': array([16.08676505, 10.        ,  0.        ]), 'targetState': array([23, 15], dtype=int32), 'currentDistance': 8.531870689457504}
done in step count: 99
reward sum = -1.7166931251834903
running average episode reward sum: -1.0619914799771228
{'scaleFactor': 20, 'currentTarget': array([23., 15.]), 'previousTarget': array([23., 15.]), 'currentState': array([212.50379479, -30.54782443,   0.        ]), 'targetState': array([23, 15], dtype=int32), 'currentDistance': 194.90072485658612}
episode index:126
target Thresh 9.430659922344546
target distance 5.0
model initialize at round 126
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([28.99967718, 10.72178841,  0.        ]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 7.444014936076346}
done in step count: 99
reward sum = -1.6160841023646249
running average episode reward sum: -1.06635441401167
{'scaleFactor': 20, 'currentTarget': array([29.14566412, 16.38712642]), 'previousTarget': array([26.908556  , 16.73409839]), 'currentState': array([226.99533355, -12.86194714,   0.        ]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 200.00000000000003}
episode index:127
target Thresh 9.453217981512779
target distance 9.0
model initialize at round 127
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 15.]), 'previousTarget': array([14., 15.]), 'currentState': array([ 3.34273231, 10.        ,  0.        ]), 'targetState': array([14, 15], dtype=int32), 'currentDistance': 11.771888319637918}
done in step count: 99
reward sum = -1.755548762092875
running average episode reward sum: -1.0717387448560545
{'scaleFactor': 20, 'currentTarget': array([14., 15.]), 'previousTarget': array([14., 15.]), 'currentState': array([197.6696735 ,  -6.74605014,   0.        ]), 'targetState': array([14, 15], dtype=int32), 'currentDistance': 184.95253353500945}
episode index:128
target Thresh 9.475753493897116
target distance 1.0
model initialize at round 128
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 11.]), 'previousTarget': array([25., 11.]), 'currentState': array([22.00069988, 10.00060928,  0.        ]), 'targetState': array([25, 11], dtype=int32), 'currentDistance': 3.161421047133447}
done in step count: 99
reward sum = -1.7699309561552736
running average episode reward sum: -1.0771510875793042
{'scaleFactor': 20, 'currentTarget': array([25., 11.]), 'previousTarget': array([25., 11.]), 'currentState': array([214.39512843,   1.21770126,   0.        ]), 'targetState': array([25, 11], dtype=int32), 'currentDistance': 189.64758907308612}
episode index:129
target Thresh 9.49826648203307
target distance 7.0
model initialize at round 129
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 26.]), 'previousTarget': array([22., 26.]), 'currentState': array([28.99883294, 18.26739639,  0.        ]), 'targetState': array([22, 26], dtype=int32), 'currentDistance': 10.42961270227285}
done in step count: 99
reward sum = -1.6413267297510845
running average episode reward sum: -1.0814909002113948
{'scaleFactor': 20, 'currentTarget': array([27.03890513, 25.86701312]), 'previousTarget': array([25.02382896, 25.9293668 ]), 'currentState': array([226.96928763,  20.59044686,   0.        ]), 'targetState': array([22, 26], dtype=int32), 'currentDistance': 200.0}
episode index:130
target Thresh 9.52075696843363
target distance 6.0
model initialize at round 130
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 8.]), 'previousTarget': array([6., 8.]), 'currentState': array([10.9998163 ,  0.88689268,  0.        ]), 'targetState': array([6, 8], dtype=int32), 'currentDistance': 8.694507393539205}
done in step count: 99
reward sum = -1.637352246737378
running average episode reward sum: -1.0857341165970893
{'scaleFactor': 20, 'currentTarget': array([10.31194371,  7.50315636]), 'previousTarget': array([8.12810343, 7.77274237]), 'currentState': array([208.99733996, -15.39036487,   0.        ]), 'targetState': array([6, 8], dtype=int32), 'currentDistance': 200.0}
episode index:131
target Thresh 9.543224975589283
target distance 4.0
model initialize at round 131
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 15.]), 'previousTarget': array([24., 15.]), 'currentState': array([26.99195766, 17.022264  ,  0.        ]), 'targetState': array([24, 15], dtype=int32), 'currentDistance': 3.611282648447322}
done in step count: 99
reward sum = -1.5357479313827123
running average episode reward sum: -1.089143312163647
{'scaleFactor': 20, 'currentTarget': array([28.16488056, 14.24904819]), 'previousTarget': array([25.93281365, 14.66478251]), 'currentState': array([224.99104524, -21.23983597,   0.        ]), 'targetState': array([24, 15], dtype=int32), 'currentDistance': 200.0}
episode index:132
target Thresh 9.565670525968045
target distance 9.0
model initialize at round 132
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 27.]), 'previousTarget': array([14., 27.]), 'currentState': array([ 8.99998629, 16.00000012,  0.        ]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 12.08305153792966}
done in step count: 99
reward sum = -1.6449732696622177
running average episode reward sum: -1.0933224847764182
{'scaleFactor': 20, 'currentTarget': array([14., 27.]), 'previousTarget': array([14., 27.]), 'currentState': array([206.71618581,  -7.08002404,   0.        ]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 195.70635224929666}
episode index:133
target Thresh 9.588093642015458
target distance 3.0
model initialize at round 133
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([23.99998975, 10.57035005,  0.        ]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 5.968393138479}
done in step count: 99
reward sum = -1.634376752085222
running average episode reward sum: -1.0973602031891705
{'scaleFactor': 20, 'currentTarget': array([25.0211196, 14.1167192]), 'previousTarget': array([23.36627498, 14.37412685]), 'currentState': array([221.99659526, -20.53385058,   0.        ]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 200.0}
episode index:134
target Thresh 9.610494346154649
target distance 5.0
model initialize at round 134
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 29.]), 'previousTarget': array([ 8., 29.]), 'currentState': array([10.99999309, 22.51407695,  0.        ]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 7.146128766648799}
done in step count: 99
reward sum = -1.7356903756691502
running average episode reward sum: -1.1020885748371703
{'scaleFactor': 20, 'currentTarget': array([16.62985676, 26.53814465]), 'previousTarget': array([15.29127269, 26.82673143]), 'currentState': array([208.95710659, -28.32740894,   0.        ]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 200.0}
episode index:135
target Thresh 9.632872660786319
target distance 9.0
model initialize at round 135
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([ 5., 23.,  0.]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 12.529964086141659}
done in step count: 99
reward sum = -1.6743784363143666
running average episode reward sum: -1.1062965885245024
{'scaleFactor': 20, 'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([1.94942419e+02, 1.38743844e-01, 0.00000000e+00]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 179.73505825909444}
episode index:136
target Thresh 9.655228608288784
target distance 4.0
model initialize at round 136
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([ 4., 16.,  0.]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 4.472135954999572}
done in step count: 99
reward sum = -1.860726827852964
running average episode reward sum: -1.1118033785925934
{'scaleFactor': 20, 'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([197.80147886,  -5.80886322,   0.        ]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 190.83236738134428}
episode index:137
target Thresh 9.677562211018
target distance 5.0
model initialize at round 137
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 25.]), 'previousTarget': array([24., 25.]), 'currentState': array([17.0205245, 24.       ,  0.       ]), 'targetState': array([24, 25], dtype=int32), 'currentDistance': 7.050750189161811}
done in step count: 99
reward sum = -1.8895716632502453
running average episode reward sum: -1.11743938065533
{'scaleFactor': 20, 'currentTarget': array([24., 25.]), 'previousTarget': array([24., 25.]), 'currentState': array([214.9013617 , -13.19440962,   0.        ]), 'targetState': array([24, 25], dtype=int32), 'currentDistance': 194.6847267427473}
episode index:138
target Thresh 9.69987349130756
target distance 7.0
model initialize at round 138
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([15.99999905,  8.38981557,  0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 9.020660057545177}
done in step count: 99
reward sum = -1.709033503761654
running average episode reward sum: -1.121695453483433
{'scaleFactor': 20, 'currentTarget': array([14.39982146,  8.53025104]), 'previousTarget': array([12.36623457,  8.6739362 ]), 'currentState': array([213.99805009,  -4.14046829,   0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 200.0}
episode index:139
target Thresh 9.72216247146876
target distance 7.0
model initialize at round 139
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 23.]), 'previousTarget': array([26., 23.]), 'currentState': array([25.99947202, 14.34920466,  0.        ]), 'targetState': array([26, 23], dtype=int32), 'currentDistance': 8.650795356649859}
done in step count: 99
reward sum = -1.589598287974436
running average episode reward sum: -1.1250376165869402
{'scaleFactor': 20, 'currentTarget': array([26., 23.]), 'previousTarget': array([26., 23.]), 'currentState': array([223.92202306,  -1.42061318,   0.        ]), 'targetState': array([26, 23], dtype=int32), 'currentDistance': 199.42290129093618}
episode index:140
target Thresh 9.744429173790568
target distance 8.0
model initialize at round 140
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([18.99990904, 12.8295995 ,  0.        ]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 11.536679416127516}
done in step count: 99
reward sum = -1.3515155326401416
running average episode reward sum: -1.1266438429419274
{'scaleFactor': 20, 'currentTarget': array([17.23286774, 21.73224823]), 'previousTarget': array([15.19517448, 21.84883037]), 'currentState': array([216.97157133,  11.51215574,   0.        ]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 200.00000000000003}
episode index:141
target Thresh 9.766673620539702
target distance 7.0
model initialize at round 141
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 14.]), 'previousTarget': array([15., 14.]), 'currentState': array([ 6.01520383, 14.        ,  0.        ]), 'targetState': array([15, 14], dtype=int32), 'currentDistance': 8.98479616641999}
done in step count: 99
reward sum = -1.450648385590136
running average episode reward sum: -1.1289255650732528
{'scaleFactor': 20, 'currentTarget': array([15., 14.]), 'previousTarget': array([15., 14.]), 'currentState': array([ 2.03403809e+02, -6.38452312e-03,  0.00000000e+00]), 'targetState': array([15, 14], dtype=int32), 'currentDistance': 188.92372532289355}
episode index:142
target Thresh 9.788895833960598
target distance 6.0
model initialize at round 142
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([8.99998224, 6.00639355, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 6.407106341445448}
done in step count: 99
reward sum = -1.4141282741302588
running average episode reward sum: -1.1309199896121132
{'scaleFactor': 20, 'currentTarget': array([7.42592591, 1.7056668 ]), 'previousTarget': array([5.49393954, 1.86606249]), 'currentState': array([206.6918745 , -15.41396324,   0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 200.0}
episode index:143
target Thresh 9.81109583627548
target distance 5.0
model initialize at round 143
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 13.]), 'previousTarget': array([ 2., 13.]), 'currentState': array([4.99997759, 6.92290831, 0.        ]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 6.777234612157557}
done in step count: 99
reward sum = -1.2072403294339353
running average episode reward sum: -1.131449991971987
{'scaleFactor': 20, 'currentTarget': array([ 2., 13.]), 'previousTarget': array([ 2., 13.]), 'currentState': array([1.96315786e+02, 1.63206108e-01, 0.00000000e+00]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 194.7393337433224}
episode index:144
target Thresh 9.833273649684351
target distance 1.0
model initialize at round 144
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 25.]), 'previousTarget': array([ 3., 25.]), 'currentState': array([ 5.99999785, 23.61693704,  0.        ]), 'targetState': array([ 3, 25], dtype=int32), 'currentDistance': 3.3034603483585685}
done in step count: 99
reward sum = -1.1670957904438561
running average episode reward sum: -1.1316958250648963
{'scaleFactor': 20, 'currentTarget': array([ 3., 25.]), 'previousTarget': array([ 3., 25.]), 'currentState': array([188.49075794,   1.5862191 ,   0.        ]), 'targetState': array([ 3, 25], dtype=int32), 'currentDistance': 186.96263374781043}
episode index:145
target Thresh 9.85542929636502
target distance 2.0
model initialize at round 145
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 12.]), 'previousTarget': array([ 6., 12.]), 'currentState': array([ 9.99999177, 11.70624781,  0.        ]), 'targetState': array([ 6, 12], dtype=int32), 'currentDistance': 4.010763586612603}
done in step count: 99
reward sum = -0.710662509371584
running average episode reward sum: -1.1288120352313806
{'scaleFactor': 20, 'currentTarget': array([ 6., 12.]), 'previousTarget': array([ 6., 12.]), 'currentState': array([152.75942442,  10.47926529,   0.        ]), 'targetState': array([ 6, 12], dtype=int32), 'currentDistance': 146.76730320351098}
episode index:146
target Thresh 9.877562798473143
target distance 6.0
model initialize at round 146
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 22.]), 'previousTarget': array([23., 22.]), 'currentState': array([26.9993372 , 15.29631448,  0.        ]), 'targetState': array([23, 22], dtype=int32), 'currentDistance': 7.806029566242227}
done in step count: 99
reward sum = -0.13086497686621154
running average episode reward sum: -1.1220232797322978
{'scaleFactor': 20, 'currentTarget': array([23., 22.]), 'previousTarget': array([23., 22.]), 'currentState': array([51.45005451, 10.4669262 ,  0.        ]), 'targetState': array([23, 22], dtype=int32), 'currentDistance': 30.698817451352813}
episode index:147
target Thresh 9.89967417814222
target distance 4.0
model initialize at round 147
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([20.99997962,  7.13884509,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 6.771418157038748}
done in step count: 99
reward sum = -0.2734057649210001
running average episode reward sum: -1.1162893776051943
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([ 38.48932493, -12.21346573,   0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 28.541633741117494}
episode index:148
target Thresh 9.921763457483635
target distance 9.0
model initialize at round 148
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 17.]), 'previousTarget': array([12., 17.]), 'currentState': array([22.9998529 , 24.38303527,  0.        ]), 'targetState': array([12, 17], dtype=int32), 'currentDistance': 13.24786675509504}
done in step count: 99
reward sum = -0.5868954077854348
running average episode reward sum: -1.1127363979419747
{'scaleFactor': 20, 'currentTarget': array([12., 17.]), 'previousTarget': array([12., 17.]), 'currentState': array([53.46977674, 54.96521434,  0.        ]), 'targetState': array([12, 17], dtype=int32), 'currentDistance': 56.22365946004328}
episode index:149
target Thresh 9.943830658586666
target distance 3.0
model initialize at round 149
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 19.]), 'previousTarget': array([19., 19.]), 'currentState': array([23.99983966, 15.00384915,  0.        ]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 6.40059515097667}
done in step count: 99
reward sum = -0.1973895125219256
running average episode reward sum: -1.1066340853725076
{'scaleFactor': 20, 'currentTarget': array([19., 19.]), 'previousTarget': array([19., 19.]), 'currentState': array([ 34.1440396 , -23.57365781,   0.        ]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 45.186925926069705}
episode index:150
target Thresh 9.965875803518518
target distance 5.0
model initialize at round 150
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 29.]), 'previousTarget': array([18., 29.]), 'currentState': array([23.99463487, 23.07112741,  0.        ]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 8.43132120832261}
done in step count: 99
reward sum = -0.1671504953800887
running average episode reward sum: -1.100412339743419
{'scaleFactor': 20, 'currentTarget': array([18., 29.]), 'previousTarget': array([18., 29.]), 'currentState': array([ 36.43330846, -21.30018416,   0.        ]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 53.5714045669401}
episode index:151
target Thresh 9.987898914324337
target distance 7.0
model initialize at round 151
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 25.]), 'previousTarget': array([12., 25.]), 'currentState': array([20.97923648, 20.12747186,  0.        ]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 10.216076461238911}
done in step count: 99
reward sum = -0.20940413989967993
running average episode reward sum: -1.0945504436918152
{'scaleFactor': 20, 'currentTarget': array([12., 25.]), 'previousTarget': array([12., 25.]), 'currentState': array([ 38.52236905, -38.34651801,   0.        ]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 68.67472172370104}
episode index:152
target Thresh 10.009900013027238
target distance 9.0
model initialize at round 152
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([ 7., 11.,  0.]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 12.206555615733699}
done in step count: 99
reward sum = -0.5810353618574292
running average episode reward sum: -1.0911941359674076
{'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([ 40.14308816, -55.59249458,   0.        ]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 63.92861596746246}
episode index:153
target Thresh 10.031879121628318
target distance 9.0
model initialize at round 153
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 29.]), 'previousTarget': array([ 8., 29.]), 'currentState': array([18.99523401, 27.40618843,  0.        ]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 11.110148798179015}
done in step count: 99
reward sum = -0.20770957895630773
running average episode reward sum: -1.0854572232595432
{'scaleFactor': 20, 'currentTarget': array([ 8., 29.]), 'previousTarget': array([ 8., 29.]), 'currentState': array([ 28.42583121, -27.94705829,   0.        ]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 60.49943824997182}
episode index:154
target Thresh 10.053836262106685
target distance 7.0
model initialize at round 154
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 18.]), 'previousTarget': array([13., 18.]), 'currentState': array([21.99998677, 22.72032112,  0.        ]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 10.162735521743336}
done in step count: 99
reward sum = -0.25811489187626546
running average episode reward sum: -1.080119530799006
{'scaleFactor': 20, 'currentTarget': array([13., 18.]), 'previousTarget': array([13., 18.]), 'currentState': array([ 29.42324972, -27.87276597,   0.        ]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 48.724057603576206}
episode index:155
target Thresh 10.075771456419492
target distance 10.0
model initialize at round 155
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 19.]), 'previousTarget': array([27., 19.]), 'currentState': array([24.99944758,  7.20786178,  0.        ]), 'targetState': array([27, 19], dtype=int32), 'currentDistance': 11.960632664776487}
done in step count: 99
reward sum = -0.2593585457111622
running average episode reward sum: -1.0748582424330582
{'scaleFactor': 20, 'currentTarget': array([27., 19.]), 'previousTarget': array([27., 19.]), 'currentState': array([ 51.46517793, -48.42900619,   0.        ]), 'targetState': array([27, 19], dtype=int32), 'currentDistance': 71.73015967343049}
episode index:156
target Thresh 10.097684726501925
target distance 6.0
model initialize at round 156
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 8.]), 'previousTarget': array([8., 8.]), 'currentState': array([4.        , 1.00000012, 0.        ]), 'targetState': array([8, 8], dtype=int32), 'currentDistance': 8.062257644795913}
done in step count: 99
reward sum = -0.3336814262650414
running average episode reward sum: -1.0701373709924975
{'scaleFactor': 20, 'currentTarget': array([8., 8.]), 'previousTarget': array([8., 8.]), 'currentState': array([ 35.09069641, -62.55559492,   0.        ]), 'targetState': array([8, 8], dtype=int32), 'currentDistance': 75.57776000131905}
episode index:157
target Thresh 10.11957609426726
target distance 8.0
model initialize at round 157
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 21.]), 'previousTarget': array([25., 21.]), 'currentState': array([23.00000012, 27.        ,  0.        ]), 'targetState': array([25, 21], dtype=int32), 'currentDistance': 6.324555282639457}
done in step count: 99
reward sum = -0.5141078799668493
running average episode reward sum: -1.0666181969986643
{'scaleFactor': 20, 'currentTarget': array([25., 21.]), 'previousTarget': array([25., 21.]), 'currentState': array([ 48.34632441, -42.85502359,   0.        ]), 'targetState': array([25, 21], dtype=int32), 'currentDistance': 67.98907928005269}
episode index:158
target Thresh 10.141445581606867
target distance 8.0
model initialize at round 158
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([10.99998081, 15.23970544,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 9.377294669722488}
done in step count: 99
reward sum = -0.24115585482245272
running average episode reward sum: -1.0614266099409524
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([ 20.60937047, -19.28600085,   0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 32.8019668838583}
episode index:159
target Thresh 10.163293210390233
target distance 6.0
model initialize at round 159
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 13.]), 'previousTarget': array([17., 13.]), 'currentState': array([12.19143987, 13.        ,  0.        ]), 'targetState': array([17, 13], dtype=int32), 'currentDistance': 4.808560132980405}
done in step count: 99
reward sum = -0.307068471594316
running average episode reward sum: -1.0567118715762858
{'scaleFactor': 20, 'currentTarget': array([17., 13.]), 'previousTarget': array([17., 13.]), 'currentState': array([ 31.35216841, -17.39220352,   0.        ]), 'targetState': array([17, 13], dtype=int32), 'currentDistance': 33.61057531331362}
episode index:160
target Thresh 10.185119002464987
target distance 10.0
model initialize at round 160
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  3.]), 'previousTarget': array([10.,  3.]), 'currentState': array([21.99848199,  4.26285416,  0.        ]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 12.064757381096255}
done in step count: 99
reward sum = -0.1544121237233684
running average episode reward sum: -1.0511075253163298
{'scaleFactor': 20, 'currentTarget': array([10.,  3.]), 'previousTarget': array([10.,  3.]), 'currentState': array([ 29.13587726, -25.32130975,   0.        ]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 34.180087541688316}
episode index:161
target Thresh 10.20692297965693
target distance 9.0
model initialize at round 161
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 17.]), 'previousTarget': array([12., 17.]), 'currentState': array([ 5., 12.,  0.]), 'targetState': array([12, 17], dtype=int32), 'currentDistance': 8.60232526704263}
done in step count: 99
reward sum = -0.325230011137501
running average episode reward sum: -1.0466267999201642
{'scaleFactor': 20, 'currentTarget': array([12., 17.]), 'previousTarget': array([12., 17.]), 'currentState': array([ 27.76498058, -14.96603054,   0.        ]), 'targetState': array([12, 17], dtype=int32), 'currentDistance': 35.64213407509844}
episode index:162
target Thresh 10.228705163770034
target distance 8.0
model initialize at round 162
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 25.]), 'previousTarget': array([19., 25.]), 'currentState': array([28.99293578, 21.99710369,  0.        ]), 'targetState': array([19, 25], dtype=int32), 'currentDistance': 10.434373564304646}
done in step count: 99
reward sum = -0.1580382563273526
running average episode reward sum: -1.0411753364625396
{'scaleFactor': 20, 'currentTarget': array([19., 25.]), 'previousTarget': array([19., 25.]), 'currentState': array([40.4045209 , -1.86732596,  0.        ]), 'targetState': array([19, 25], dtype=int32), 'currentDistance': 34.35122587113678}
episode index:163
target Thresh 10.250465576586489
target distance 6.0
model initialize at round 163
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([6.37714291, 4.        , 0.        ]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 4.6228570938110485}
done in step count: 99
reward sum = -0.3030845786683178
running average episode reward sum: -1.0366747830613554
{'scaleFactor': 20, 'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([ 35.51422358, -20.25037512,   0.        ]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 34.48228314952487}
episode index:164
target Thresh 10.272204239866706
target distance 8.0
model initialize at round 164
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([13.9907856 ,  6.21251589,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 10.372360605978598}
done in step count: 99
reward sum = -0.15885027334731577
running average episode reward sum: -1.031354634517634
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([ 31.79165092, -13.99972919,   0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 36.07441481021103}
episode index:165
target Thresh 10.293921175349354
target distance 7.0
model initialize at round 165
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  7.]), 'previousTarget': array([21.,  7.]), 'currentState': array([16.00000012, 12.        ,  0.        ]), 'targetState': array([21,  7], dtype=int32), 'currentDistance': 7.071067727571712}
done in step count: 99
reward sum = -0.39776693613584574
running average episode reward sum: -1.0275378411538885
{'scaleFactor': 20, 'currentTarget': array([21.,  7.]), 'previousTarget': array([21.,  7.]), 'currentState': array([ 49.49528421, -18.32245432,   0.        ]), 'targetState': array([21,  7], dtype=int32), 'currentDistance': 38.120964242081186}
episode index:166
target Thresh 10.315616404751367
target distance 5.0
model initialize at round 166
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([5.99999952, 9.42438436, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 5.265586804887441}
done in step count: 99
reward sum = -0.22963859226057884
running average episode reward sum: -1.022760001340156
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([ 30.46208647, -17.44548538,   0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 36.87521052308811}
episode index:167
target Thresh 10.337289949767978
target distance 4.0
model initialize at round 167
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 17.]), 'previousTarget': array([19., 17.]), 'currentState': array([16.99999976, 12.00318873,  0.        ]), 'targetState': array([19, 17], dtype=int32), 'currentDistance': 5.382204365202645}
done in step count: 99
reward sum = -0.27038808843725587
running average episode reward sum: -1.0182815970966863
{'scaleFactor': 20, 'currentTarget': array([19., 17.]), 'previousTarget': array([19., 17.]), 'currentState': array([ 48.93860517, -10.61028519,   0.        ]), 'targetState': array([19, 17], dtype=int32), 'currentDistance': 40.72650154261615}
episode index:168
target Thresh 10.358941832072734
target distance 7.0
model initialize at round 168
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 14.]), 'previousTarget': array([ 2., 14.]), 'currentState': array([10.98080182,  9.08877915,  0.        ]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 10.235960705620718}
done in step count: 99
reward sum = -0.13707840412778366
running average episode reward sum: -1.0130673770199472
{'scaleFactor': 20, 'currentTarget': array([ 2., 14.]), 'previousTarget': array([ 2., 14.]), 'currentState': array([ 34.48454043, -12.55902154,   0.        ]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 41.95982593325156}
episode index:169
target Thresh 10.38057207331752
target distance 5.0
model initialize at round 169
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 28.]), 'previousTarget': array([ 8., 28.]), 'currentState': array([ 5.        , 24.00000608,  0.        ]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 4.999995136262249}
done in step count: 99
reward sum = -0.27468527924632097
running average episode reward sum: -1.0087239529153964
{'scaleFactor': 20, 'currentTarget': array([ 8., 28.]), 'previousTarget': array([ 8., 28.]), 'currentState': array([41.35677027, -1.69642904,  0.        ]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 44.66040775328256}
episode index:170
target Thresh 10.402180695132575
target distance 4.0
model initialize at round 170
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([17.99998915, 23.64883518,  0.        ]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 6.2224213359109095}
done in step count: 99
reward sum = -0.16632346017553826
running average episode reward sum: -1.003797634244403
{'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([42.56654203, -2.73140076,  0.        ]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 39.31864284662822}
episode index:171
target Thresh 10.42376771912653
target distance 8.0
model initialize at round 171
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.99998951, 8.        , 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 6.000000000009188}
done in step count: 99
reward sum = -0.2714166699095486
running average episode reward sum: -0.999539605381991
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([ 31.77367542, -19.51353641,   0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 35.13131502329811}
episode index:172
target Thresh 10.4453331668864
target distance 7.0
model initialize at round 172
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 25.]), 'previousTarget': array([16., 25.]), 'currentState': array([11.        , 20.00000036,  0.        ]), 'targetState': array([16, 25], dtype=int32), 'currentDistance': 7.071067558984426}
done in step count: 99
reward sum = -0.27383050583270907
running average episode reward sum: -0.9953447550955791
{'scaleFactor': 20, 'currentTarget': array([16., 25.]), 'previousTarget': array([16., 25.]), 'currentState': array([46.4586678,  1.0451007,  0.       ]), 'targetState': array([16, 25], dtype=int32), 'currentDistance': 38.750066377017774}
episode index:173
target Thresh 10.466877059977644
target distance 7.0
model initialize at round 173
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 19.]), 'previousTarget': array([25., 19.]), 'currentState': array([22., 24.,  0.]), 'targetState': array([25, 19], dtype=int32), 'currentDistance': 5.8309518948453265}
done in step count: 99
reward sum = -0.3518262943022741
running average episode reward sum: -0.9916463731369968
{'scaleFactor': 20, 'currentTarget': array([25., 19.]), 'previousTarget': array([25., 19.]), 'currentState': array([ 59.40347317, -12.02394849,   0.        ]), 'targetState': array([25, 19], dtype=int32), 'currentDistance': 46.32584964652957}
episode index:174
target Thresh 10.488399419944155
target distance 9.0
model initialize at round 174
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([13., 21.,  0.]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 7.615773105863935}
done in step count: 99
reward sum = -0.3180200243032223
running average episode reward sum: -0.9877970797150896
{'scaleFactor': 20, 'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([45.60969366, -2.40624035,  0.        ]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 32.74555015347219}
episode index:175
target Thresh 10.50990026830829
target distance 9.0
model initialize at round 175
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 27.]), 'previousTarget': array([ 7., 27.]), 'currentState': array([11.8494637 , 17.28960317,  0.        ]), 'targetState': array([ 7, 27], dtype=int32), 'currentDistance': 10.853990266869694}
done in step count: 99
reward sum = -0.1174079573382798
running average episode reward sum: -0.9828516869743122
{'scaleFactor': 20, 'currentTarget': array([ 7., 27.]), 'previousTarget': array([ 7., 27.]), 'currentState': array([33.57653222,  5.34560852,  0.        ]), 'targetState': array([ 7, 27], dtype=int32), 'currentDistance': 34.28155094634317}
episode index:176
target Thresh 10.531379626570903
target distance 4.0
model initialize at round 176
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 5.]), 'previousTarget': array([9., 5.]), 'currentState': array([6.99999964, 5.        , 0.        ]), 'targetState': array([9, 5], dtype=int32), 'currentDistance': 2.000000357627833}
done in step count: 99
reward sum = -0.23748455701601062
running average episode reward sum: -0.9786405732457343
{'scaleFactor': 20, 'currentTarget': array([9., 5.]), 'previousTarget': array([9., 5.]), 'currentState': array([ 37.16863118, -18.58276283,   0.        ]), 'targetState': array([9, 5], dtype=int32), 'currentDistance': 36.737154015746164}
episode index:177
target Thresh 10.552837516211355
target distance 9.0
model initialize at round 177
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 25.]), 'previousTarget': array([27., 25.]), 'currentState': array([20., 21.,  0.]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 8.062257748298554}
done in step count: 99
reward sum = -0.29901025787558827
running average episode reward sum: -0.9748224254065762
{'scaleFactor': 20, 'currentTarget': array([27., 25.]), 'previousTarget': array([27., 25.]), 'currentState': array([56.83591577, -1.71751582,  0.        ]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 40.05006268944517}
episode index:178
target Thresh 10.574273958687535
target distance 7.0
model initialize at round 178
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 24.]), 'previousTarget': array([ 8., 24.]), 'currentState': array([16.99969411, 27.62735909,  0.        ]), 'targetState': array([ 8, 24], dtype=int32), 'currentDistance': 9.703207100692287}
done in step count: 99
reward sum = -1.2727366766682484
running average episode reward sum: -0.9764867508326189
{'scaleFactor': 20, 'currentTarget': array([11.66134758, 27.65430218]), 'previousTarget': array([ 9.77386553, 25.76903859]), 'currentState': array([153.21883523, 168.93939583,   0.        ]), 'targetState': array([ 8, 24], dtype=int32), 'currentDistance': 200.0}
episode index:179
target Thresh 10.595688975435891
target distance 3.0
model initialize at round 179
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  9.]), 'previousTarget': array([24.,  9.]), 'currentState': array([22.99572814,  7.00000703,  0.        ]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 2.23797538978177}
done in step count: 99
reward sum = -0.34563910305274775
running average episode reward sum: -0.9729820416782864
{'scaleFactor': 20, 'currentTarget': array([24.,  9.]), 'previousTarget': array([24.,  9.]), 'currentState': array([72.12086173, 60.87350222,  0.        ]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 70.75646660516084}
episode index:180
target Thresh 10.617082587871437
target distance 10.0
model initialize at round 180
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 20.]), 'previousTarget': array([ 2., 20.]), 'currentState': array([13.34121633, 10.0832882 ,  0.        ]), 'targetState': array([ 2, 20], dtype=int32), 'currentDistance': 15.065336394119651}
done in step count: 99
reward sum = -0.051270668372001324
running average episode reward sum: -0.9678897136489699
{'scaleFactor': 20, 'currentTarget': array([ 2., 20.]), 'previousTarget': array([ 2., 20.]), 'currentState': array([29.56854856, -3.73946157,  0.        ]), 'targetState': array([ 2, 20], dtype=int32), 'currentDistance': 36.38113392402926}
episode index:181
target Thresh 10.638454817387785
target distance 8.0
model initialize at round 181
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([14., 16.,  0.]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 6.082762530298241}
done in step count: 99
reward sum = -0.770908186402471
running average episode reward sum: -0.966807397565198
{'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([84.74241211, 96.74287151,  0.        ]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 104.27596544093956}
episode index:182
target Thresh 10.659805685357174
target distance 10.0
model initialize at round 182
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 18.]), 'previousTarget': array([ 9., 18.]), 'currentState': array([20.99577689, 25.72158688,  0.        ]), 'targetState': array([ 9, 18], dtype=int32), 'currentDistance': 14.26609852862416}
done in step count: 99
reward sum = -1.3989196445686445
running average episode reward sum: -0.9691686666745064
{'scaleFactor': 20, 'currentTarget': array([ 9., 18.]), 'previousTarget': array([ 9., 18.]), 'currentState': array([127.01683152, 174.74190282,   0.        ]), 'targetState': array([ 9, 18], dtype=int32), 'currentDistance': 196.20396688269702}
episode index:183
target Thresh 10.68113521313047
target distance 9.0
model initialize at round 183
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 10.]), 'previousTarget': array([27., 10.]), 'currentState': array([20.,  3.,  0.]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 9.899494936611665}
done in step count: 99
reward sum = -0.3641599129609397
running average episode reward sum: -0.9658805756217153
{'scaleFactor': 20, 'currentTarget': array([27., 10.]), 'previousTarget': array([27., 10.]), 'currentState': array([ 72.21857992, -37.64587941,   0.        ]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 65.68751628124237}
episode index:184
target Thresh 10.702443422037202
target distance 8.0
model initialize at round 184
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 13.]), 'previousTarget': array([12., 13.]), 'currentState': array([21.99628973, 17.45374376,  0.        ]), 'targetState': array([12, 13], dtype=int32), 'currentDistance': 10.943566229069228}
done in step count: 99
reward sum = -1.0409203802442912
running average episode reward sum: -0.9662861961872428
{'scaleFactor': 20, 'currentTarget': array([12., 13.]), 'previousTarget': array([12., 13.]), 'currentState': array([113.8395507 , 142.95163581,   0.        ]), 'targetState': array([12, 13], dtype=int32), 'currentDistance': 165.10215545989212}
episode index:185
target Thresh 10.723730333385582
target distance 9.0
model initialize at round 185
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  7.]), 'previousTarget': array([11.,  7.]), 'currentState': array([21.98367393, 13.67480594,  0.        ]), 'targetState': array([11,  7], dtype=int32), 'currentDistance': 12.852786753471051}
done in step count: 99
reward sum = -0.5205319708537302
running average episode reward sum: -0.9638896680940519
{'scaleFactor': 20, 'currentTarget': array([11.,  7.]), 'previousTarget': array([11.,  7.]), 'currentState': array([81.79956692, 99.73701708,  0.        ]), 'targetState': array([11,  7], dtype=int32), 'currentDistance': 116.67361746731987}
episode index:186
target Thresh 10.744995968462522
target distance 10.0
model initialize at round 186
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 18.]), 'previousTarget': array([ 2., 18.]), 'currentState': array([13.95383954, 24.28088266,  0.        ]), 'targetState': array([ 2, 18], dtype=int32), 'currentDistance': 13.503472394359674}
done in step count: 99
reward sum = -1.0814360421860616
running average episode reward sum: -0.964518258329838
{'scaleFactor': 20, 'currentTarget': array([ 2., 18.]), 'previousTarget': array([ 2., 18.]), 'currentState': array([120.52323847, 160.67381012,   0.        ]), 'targetState': array([ 2, 18], dtype=int32), 'currentDistance': 185.4820049316114}
episode index:187
target Thresh 10.766240348533657
target distance 10.0
model initialize at round 187
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 13.]), 'previousTarget': array([ 9., 13.]), 'currentState': array([12.48502886,  2.24625319,  0.        ]), 'targetState': array([ 9, 13], dtype=int32), 'currentDistance': 11.304357415485885}
done in step count: 99
reward sum = -0.09324461565882641
running average episode reward sum: -0.9598838240603113
{'scaleFactor': 20, 'currentTarget': array([ 9., 13.]), 'previousTarget': array([ 9., 13.]), 'currentState': array([34.14919748, -5.89327674,  0.        ]), 'targetState': array([ 9, 13], dtype=int32), 'currentDistance': 31.455334046741164}
episode index:188
target Thresh 10.787463494843376
target distance 3.0
model initialize at round 188
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([25.96089542,  8.25856924,  0.        ]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 5.667973709201807}
done in step count: 99
reward sum = -1.440769611492573
running average episode reward sum: -0.9624281933059846
{'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([158.45448266, 154.48658575,   0.        ]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 198.70112001405752}
episode index:189
target Thresh 10.808665428614816
target distance 4.0
model initialize at round 189
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 19.]), 'previousTarget': array([23., 19.]), 'currentState': array([28.54025686, 14.4120611 ,  0.        ]), 'targetState': array([23, 19], dtype=int32), 'currentDistance': 7.1933044871624485}
done in step count: 99
reward sum = -1.2814862152834872
running average episode reward sum: -0.9641074460532346
{'scaleFactor': 20, 'currentTarget': array([23., 19.]), 'previousTarget': array([23., 19.]), 'currentState': array([149.39382916, 155.93680071,   0.        ]), 'targetState': array([23, 19], dtype=int32), 'currentDistance': 186.3520524076506}
episode index:190
target Thresh 10.829846171049923
target distance 9.0
model initialize at round 190
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([28.99268341, 19.39819071,  0.        ]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 13.717860996158194}
done in step count: 99
reward sum = -2.2015691594925935
running average episode reward sum: -0.9705863031916606
{'scaleFactor': 20, 'currentTarget': array([69.73846802, 65.05627994]), 'previousTarget': array([67.9909954 , 63.11439252]), 'currentState': array([205.27514452, 212.12704292,   0.        ]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 200.00000000000003}
episode index:191
target Thresh 10.851005743329438
target distance 2.0
model initialize at round 191
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 16.]), 'previousTarget': array([ 2., 16.]), 'currentState': array([ 5.98837376, 18.18706496,  0.        ]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 4.548667756902967}
done in step count: 99
reward sum = -1.9776949205318115
running average episode reward sum: -0.9758316605736406
{'scaleFactor': 20, 'currentTarget': array([37.31198766, 56.5148041 ]), 'previousTarget': array([35.55102213, 54.50197128]), 'currentState': array([168.72082031, 207.28522129,   0.        ]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 200.0}
episode index:192
target Thresh 10.87214416661293
target distance 1.0
model initialize at round 192
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'previousTarget': array([16., 23.]), 'currentState': array([17.49589303, 21.88411117,  0.        ]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 1.8662539621157743}
done in step count: 99
reward sum = -0.15005029532068553
running average episode reward sum: -0.9715530006500501
{'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'previousTarget': array([16., 23.]), 'currentState': array([9.83744278, 5.6802185 , 0.        ]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 18.38346928007926}
episode index:193
target Thresh 10.89326146203883
target distance 9.0
model initialize at round 193
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([ 6.18948406, 18.92552602,  0.        ]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 10.771028625227853}
done in step count: 99
reward sum = -0.32958117402385995
running average episode reward sum: -0.9682438675231111
{'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([ -0.39849413, -12.57571231,   0.        ]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 42.856370986895605}
episode index:194
target Thresh 10.91435765072443
target distance 7.0
model initialize at round 194
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 17.]), 'previousTarget': array([24., 17.]), 'currentState': array([19.99809361, 22.00011325,  0.        ]), 'targetState': array([24, 17], dtype=int32), 'currentDistance': 6.404403742354584}
done in step count: 99
reward sum = -0.45087302067485935
running average episode reward sum: -0.9655906836931201
{'scaleFactor': 20, 'currentTarget': array([24., 17.]), 'previousTarget': array([24., 17.]), 'currentState': array([ 15.16482037, -13.11508576,   0.        ]), 'targetState': array([24, 17], dtype=int32), 'currentDistance': 31.384371737047005}
episode index:195
target Thresh 10.935432753765927
target distance 10.0
model initialize at round 195
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([12.81693977,  4.64328271,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 11.325799617162144}
done in step count: 99
reward sum = -1.653213553502726
running average episode reward sum: -0.9690989636411282
{'scaleFactor': 20, 'currentTarget': array([23.82646539, 33.95456997]), 'previousTarget': array([22.2154604 , 32.01571101]), 'currentState': array([152.55001619, 187.02399039,   0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 200.0}
episode index:196
target Thresh 10.956486792238422
target distance 9.0
model initialize at round 196
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 18.]), 'previousTarget': array([21., 18.]), 'currentState': array([17.63526142,  7.81921601,  0.        ]), 'targetState': array([21, 18], dtype=int32), 'currentDistance': 10.722398440955613}
done in step count: 99
reward sum = -0.0409388632549268
running average episode reward sum: -0.9643874910503353
{'scaleFactor': 20, 'currentTarget': array([21., 18.]), 'previousTarget': array([21., 18.]), 'currentState': array([ 11.86209514, -13.75502054,   0.        ]), 'targetState': array([21, 18], dtype=int32), 'currentDistance': 33.04364741455555}
episode index:197
target Thresh 10.977519787195956
target distance 6.0
model initialize at round 197
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 18.]), 'previousTarget': array([14., 18.]), 'currentState': array([15.60821092, 11.75362879,  0.        ]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 6.450077182153326}
done in step count: 99
reward sum = -1.378801666216925
running average episode reward sum: -0.9664804919350151
{'scaleFactor': 20, 'currentTarget': array([16.38128738, 20.816723  ]), 'previousTarget': array([14.64735249, 18.7663321 ]), 'currentState': array([145.50356632, 173.54994492,   0.        ]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 200.0}
episode index:198
target Thresh 10.998531759671522
target distance 4.0
model initialize at round 198
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 26.]), 'previousTarget': array([27., 26.]), 'currentState': array([24.83208644, 24.02197313,  0.        ]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 2.9346958058624004}
done in step count: 99
reward sum = -1.3917517852492514
running average episode reward sum: -0.968617533609961
{'scaleFactor': 20, 'currentTarget': array([27., 26.]), 'previousTarget': array([27., 26.]), 'currentState': array([153.23812252, 178.0854342 ,   0.        ]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 197.65131639537574}
episode index:199
target Thresh 11.019522730677103
target distance 5.0
model initialize at round 199
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 21.]), 'previousTarget': array([ 3., 21.]), 'currentState': array([ 3.85798383, 15.35963058,  0.        ]), 'targetState': array([ 3, 21], dtype=int32), 'currentDistance': 5.70525226342106}
done in step count: 99
reward sum = -1.3555367306968755
running average episode reward sum: -0.9705521295953956
{'scaleFactor': 20, 'currentTarget': array([ 3.67563809, 21.80693526]), 'previousTarget': array([ 3., 21.]), 'currentState': array([132.07016253, 175.1524474 ,   0.        ]), 'targetState': array([ 3, 21], dtype=int32), 'currentDistance': 200.0}
episode index:200
target Thresh 11.040492721203663
target distance 6.0
model initialize at round 200
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 16.]), 'previousTarget': array([20., 16.]), 'currentState': array([26.86417359, 11.08255653,  0.        ]), 'targetState': array([20, 16], dtype=int32), 'currentDistance': 8.443821964117518}
done in step count: 99
reward sum = -1.5597981453175733
running average episode reward sum: -0.973483701812919
{'scaleFactor': 20, 'currentTarget': array([34.11369361, 33.85546036]), 'previousTarget': array([32.67439046, 31.99703221]), 'currentState': array([158.13605928, 190.75814617,   0.        ]), 'targetState': array([20, 16], dtype=int32), 'currentDistance': 200.0}
episode index:201
target Thresh 11.0614417522212
target distance 2.0
model initialize at round 201
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 13.]), 'previousTarget': array([ 3., 13.]), 'currentState': array([2.85601306, 9.90077758, 0.        ]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 3.1025653672139817}
done in step count: 99
reward sum = -1.264159422075033
running average episode reward sum: -0.9749226905270878
{'scaleFactor': 20, 'currentTarget': array([ 3., 13.]), 'previousTarget': array([ 3., 13.]), 'currentState': array([120.11632913, 162.047784  ,   0.        ]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 189.5559982306188}
episode index:202
target Thresh 11.082369844678741
target distance 5.0
model initialize at round 202
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([11.5906738 ,  1.48305285,  0.        ]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 5.741685206458833}
done in step count: 99
reward sum = -1.3348745694789375
running average episode reward sum: -0.9766958524923678
{'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([132.87979008, 158.84318622,   0.        ]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 195.3350864794854}
episode index:203
target Thresh 11.103277019504386
target distance 6.0
model initialize at round 203
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([10.99988544, 14.0035944 ,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 5.002249074649699}
done in step count: 2
reward sum = 0.9135652963315786
running average episode reward sum: -0.9674298664687211
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.28125215, 11.66888863,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.9818403512168824}
episode index:204
target Thresh 11.124163297605307
target distance 6.0
model initialize at round 204
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 23.]), 'previousTarget': array([ 6., 23.]), 'currentState': array([ 7.46298501, 16.66841793,  0.        ]), 'targetState': array([ 6, 23], dtype=int32), 'currentDistance': 6.498404160864305}
done in step count: 99
reward sum = -1.297682355720933
running average episode reward sum: -0.9690408542211708
{'scaleFactor': 20, 'currentTarget': array([ 6., 23.]), 'previousTarget': array([ 6., 23.]), 'currentState': array([131.55396992, 170.95613746,   0.        ]), 'targetState': array([ 6, 23], dtype=int32), 'currentDistance': 194.0484938749595}
episode index:205
target Thresh 11.145028699867787
target distance 7.0
model initialize at round 205
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 16.]), 'previousTarget': array([24., 16.]), 'currentState': array([18.9998368 , 21.99999881,  0.        ]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 7.810353237638819}
done in step count: 99
reward sum = -2.2696872236493895
running average episode reward sum: -0.9753546715484923
{'scaleFactor': 20, 'currentTarget': array([62.12596928, 63.64220116]), 'previousTarget': array([60.3690656 , 61.53032095]), 'currentState': array([187.08937645, 219.79644179,   0.        ]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 199.99999999999997}
episode index:206
target Thresh 11.16587324715723
target distance 6.0
model initialize at round 206
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([11.32625127, 17.79749125,  0.        ]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 8.251963248813242}
done in step count: 99
reward sum = -1.9341018705569075
running average episode reward sum: -0.9799863005292092
{'scaleFactor': 20, 'currentTarget': array([35.44579961, 54.71976198]), 'previousTarget': array([33.85382502, 52.68259524]), 'currentState': array([157.68790152, 213.01337689,   0.        ]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 200.0}
episode index:207
target Thresh 11.18669696031818
target distance 6.0
model initialize at round 207
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 14.]), 'previousTarget': array([25., 14.]), 'currentState': array([20.99956822, 12.99997485,  0.        ]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 4.1235306112307955}
done in step count: 99
reward sum = -1.8984162058351488
running average episode reward sum: -0.9844018289201032
{'scaleFactor': 20, 'currentTarget': array([45.60912111, 42.09806608]), 'previousTarget': array([44.13227644, 40.08883466]), 'currentState': array([163.89630751, 203.36846489,   0.        ]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 200.0}
episode index:208
target Thresh 11.207499860174359
target distance 2.0
model initialize at round 208
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 13.]), 'previousTarget': array([13., 13.]), 'currentState': array([11.90224123, 15.00946677,  0.        ]), 'targetState': array([13, 13], dtype=int32), 'currentDistance': 2.289766583303958}
done in step count: 99
reward sum = -1.5804761198502761
running average episode reward sum: -0.9872538590202476
{'scaleFactor': 20, 'currentTarget': array([24.52615952, 28.14813382]), 'previousTarget': array([22.86555307, 25.9989281 ]), 'currentState': array([145.63322467, 187.31181928,   0.        ]), 'targetState': array([13, 13], dtype=int32), 'currentDistance': 200.00000000000003}
episode index:209
target Thresh 11.228281967528666
target distance 10.0
model initialize at round 209
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 28.]), 'previousTarget': array([16., 28.]), 'currentState': array([ 8.        , 24.56133592,  0.        ]), 'targetState': array([16, 28], dtype=int32), 'currentDistance': 8.707721323413347}
done in step count: 6
reward sum = 0.8298105278001222
running average episode reward sum: -0.9786011714639601
{'scaleFactor': 20, 'currentTarget': array([16., 28.]), 'previousTarget': array([16., 28.]), 'currentState': array([16.14335111, 27.06846274,  0.        ]), 'targetState': array([16, 28], dtype=int32), 'currentDistance': 0.9425026248793927}
episode index:210
target Thresh 11.249043303163209
target distance 9.0
model initialize at round 210
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 5.]), 'previousTarget': array([6., 5.]), 'currentState': array([16.07831216,  7.16237393,  0.        ]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 10.30767854465556}
done in step count: 99
reward sum = -1.6846694399616446
running average episode reward sum: -0.9819474665753236
{'scaleFactor': 20, 'currentTarget': array([28.91277157, 33.36524376]), 'previousTarget': array([27.2561509 , 31.32965511]), 'currentState': array([154.58810102, 188.94709221,   0.        ]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 200.0}
episode index:211
target Thresh 11.269783887839324
target distance 6.0
model initialize at round 211
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([14.66804647, 16.31209892,  0.        ]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 5.8417736079158775}
done in step count: 99
reward sum = -0.8128641884759304
running average episode reward sum: -0.9811499039427793
{'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([108.6521772 , 134.87567972,   0.        ]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 146.03199996012725}
episode index:212
target Thresh 11.290503742297602
target distance 6.0
model initialize at round 212
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 19.]), 'previousTarget': array([20., 19.]), 'currentState': array([26.85032082, 17.91727401,  0.        ]), 'targetState': array([20, 19], dtype=int32), 'currentDistance': 6.9353580185341235}
done in step count: 99
reward sum = -1.1697576301004606
running average episode reward sum: -0.9820353862252098
{'scaleFactor': 20, 'currentTarget': array([20., 19.]), 'previousTarget': array([20., 19.]), 'currentState': array([132.70800751, 174.82583117,   0.        ]), 'targetState': array([20, 19], dtype=int32), 'currentDistance': 192.3142860481311}
episode index:213
target Thresh 11.311202887257895
target distance 3.0
model initialize at round 213
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 22.]), 'previousTarget': array([15., 22.]), 'currentState': array([18.44430208, 24.97675283,  0.        ]), 'targetState': array([15, 22], dtype=int32), 'currentDistance': 4.552392144389857}
done in step count: 99
reward sum = -1.0734345316494505
running average episode reward sum: -0.9824624850356034
{'scaleFactor': 20, 'currentTarget': array([15., 22.]), 'previousTarget': array([15., 22.]), 'currentState': array([127.1026644 , 162.94629679,   0.        ]), 'targetState': array([15, 22], dtype=int32), 'currentDistance': 180.0912711461929}
episode index:214
target Thresh 11.331881343419354
target distance 5.0
model initialize at round 214
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([10.995749  , 10.15148759,  0.        ]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 4.139993604694543}
done in step count: 5
reward sum = 0.9035302314483864
running average episode reward sum: -0.9736904258891662
{'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([14.12684351, 12.08487253,  0.        ]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 0.9238763796598418}
episode index:215
target Thresh 11.352539131460432
target distance 10.0
model initialize at round 215
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 14.]), 'previousTarget': array([15., 14.]), 'currentState': array([25.91532457, 23.73326945,  0.        ]), 'targetState': array([15, 14], dtype=int32), 'currentDistance': 14.624665626494705}
done in step count: 99
reward sum = -0.9401745835601995
running average episode reward sum: -0.973535259952458
{'scaleFactor': 20, 'currentTarget': array([15., 14.]), 'previousTarget': array([15., 14.]), 'currentState': array([108.93575084, 148.74034289,   0.        ]), 'targetState': array([15, 14], dtype=int32), 'currentDistance': 164.2525046641872}
episode index:216
target Thresh 11.373176272038918
target distance 7.0
model initialize at round 216
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  4.]), 'previousTarget': array([21.,  4.]), 'currentState': array([23.99979007,  9.33056819,  0.        ]), 'targetState': array([21,  4], dtype=int32), 'currentDistance': 6.116673749167356}
done in step count: 99
reward sum = -0.31119939480776415
running average episode reward sum: -0.9704830209425747
{'scaleFactor': 20, 'currentTarget': array([21.,  4.]), 'previousTarget': array([21.,  4.]), 'currentState': array([73.6524166, 70.7297569,  0.       ]), 'targetState': array([21,  4], dtype=int32), 'currentDistance': 85.00080841021476}
episode index:217
target Thresh 11.393792785791959
target distance 9.0
model initialize at round 217
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 16.]), 'previousTarget': array([ 4., 16.]), 'currentState': array([13.7284742 , 21.67128274,  0.        ]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 11.26084624506693}
done in step count: 99
reward sum = -0.33753511903890265
running average episode reward sum: -0.9675795901998973
{'scaleFactor': 20, 'currentTarget': array([ 4., 16.]), 'previousTarget': array([ 4., 16.]), 'currentState': array([65.2077814 , 90.57791303,  0.        ]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 96.47931185483161}
episode index:218
target Thresh 11.414388693336072
target distance 4.0
model initialize at round 218
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([ 6.44541001, 10.70426387,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 5.0588952299506404}
done in step count: 99
reward sum = -0.24245965981766812
running average episode reward sum: -0.9642685402894761
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([54.99979867, 70.26898695,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 81.89593257025133}
episode index:219
target Thresh 11.434964015267159
target distance 3.0
model initialize at round 219
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 12.]), 'previousTarget': array([14., 12.]), 'currentState': array([16.00375271, 14.48701108,  0.        ]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 3.1937828673584057}
done in step count: 99
reward sum = -0.4016935192081591
running average episode reward sum: -0.9617113811027428
{'scaleFactor': 20, 'currentTarget': array([14., 12.]), 'previousTarget': array([14., 12.]), 'currentState': array([77.98414356, 92.82666212,  0.        ]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 103.08695327751133}
episode index:220
target Thresh 11.455518772160548
target distance 10.0
model initialize at round 220
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 23.]), 'previousTarget': array([24., 23.]), 'currentState': array([20.95797932, 13.27085006,  0.        ]), 'targetState': array([24, 23], dtype=int32), 'currentDistance': 10.193637639107196}
done in step count: 15
reward sum = 0.8064785111808447
running average episode reward sum: -0.9537105218616406
{'scaleFactor': 20, 'currentTarget': array([24., 23.]), 'previousTarget': array([24., 23.]), 'currentState': array([24.43527942, 22.61624166,  0.        ]), 'targetState': array([24, 23], dtype=int32), 'currentDistance': 0.5802918537021271}
episode index:221
target Thresh 11.476052984570998
target distance 5.0
model initialize at round 221
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 17.]), 'previousTarget': array([ 5., 17.]), 'currentState': array([ 5.23026454, 12.82640266,  0.        ]), 'targetState': array([ 5, 17], dtype=int32), 'currentDistance': 4.179944554892303}
done in step count: 99
reward sum = -0.25851206935416643
running average episode reward sum: -0.9505789973007961
{'scaleFactor': 20, 'currentTarget': array([ 5., 17.]), 'previousTarget': array([ 5., 17.]), 'currentState': array([58.25563764, 75.15271487,  0.        ]), 'targetState': array([ 5, 17], dtype=int32), 'currentDistance': 78.85366946253289}
episode index:222
target Thresh 11.496566673032724
target distance 6.0
model initialize at round 222
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 14.]), 'previousTarget': array([27., 14.]), 'currentState': array([22.9998759 , 13.19606304,  0.        ]), 'targetState': array([27, 14], dtype=int32), 'currentDistance': 4.0801112023208335}
done in step count: 2
reward sum = 0.9253297347333014
running average episode reward sum: -0.9421668505203741
{'scaleFactor': 20, 'currentTarget': array([27., 14.]), 'previousTarget': array([27., 14.]), 'currentState': array([26.50966775, 13.50754209,  0.        ]), 'targetState': array([27, 14], dtype=int32), 'currentDistance': 0.6949392101260427}
episode index:223
target Thresh 11.517059858059412
target distance 8.0
model initialize at round 223
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  6.]), 'previousTarget': array([23.,  6.]), 'currentState': array([25.99913394, 12.10183024,  0.        ]), 'targetState': array([23,  6], dtype=int32), 'currentDistance': 6.799054106601877}
done in step count: 99
reward sum = -0.32596132841858594
running average episode reward sum: -0.9394159330109911
{'scaleFactor': 20, 'currentTarget': array([23.,  6.]), 'previousTarget': array([23.,  6.]), 'currentState': array([80.84101571, 72.83943387,  0.        ]), 'targetState': array([23,  6], dtype=int32), 'currentDistance': 88.39170220531949}
episode index:224
target Thresh 11.537532560144253
target distance 2.0
model initialize at round 224
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 13.]), 'previousTarget': array([ 4., 13.]), 'currentState': array([ 3.74322176, 13.35979199,  0.        ]), 'targetState': array([ 4, 13], dtype=int32), 'currentDistance': 0.4420241437489055}
done in step count: 0
reward sum = 0.9713544797897339
running average episode reward sum: -0.93092362006521
{'scaleFactor': 20, 'currentTarget': array([ 4., 13.]), 'previousTarget': array([ 4., 13.]), 'currentState': array([ 3.74322176, 13.35979199,  0.        ]), 'targetState': array([ 4, 13], dtype=int32), 'currentDistance': 0.4420241437489055}
episode index:225
target Thresh 11.557984799759947
target distance 6.0
model initialize at round 225
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 28.]), 'previousTarget': array([ 4., 28.]), 'currentState': array([10.79652452, 26.46868587,  0.        ]), 'targetState': array([ 4, 28], dtype=int32), 'currentDistance': 6.966898061082444}
done in step count: 99
reward sum = -0.3274246834739627
running average episode reward sum: -0.9282532707882577
{'scaleFactor': 20, 'currentTarget': array([ 4., 28.]), 'previousTarget': array([ 4., 28.]), 'currentState': array([ 67.46510302, 101.38511369,   0.        ]), 'targetState': array([ 4, 28], dtype=int32), 'currentDistance': 97.02161724374385}
episode index:226
target Thresh 11.57841659735874
target distance 3.0
model initialize at round 226
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 25.]), 'previousTarget': array([ 4., 25.]), 'currentState': array([ 7.58872229, 23.87338889,  0.        ]), 'targetState': array([ 4, 25], dtype=int32), 'currentDistance': 3.761406687121951}
done in step count: 99
reward sum = -0.1952734365164949
running average episode reward sum: -0.925024284734197
{'scaleFactor': 20, 'currentTarget': array([ 4., 25.]), 'previousTarget': array([ 4., 25.]), 'currentState': array([51.91472197, 69.55290282,  0.        ]), 'targetState': array([ 4, 25], dtype=int32), 'currentDistance': 65.427683214902}
episode index:227
target Thresh 11.598827973372423
target distance 1.0
model initialize at round 227
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 24.]), 'previousTarget': array([25., 24.]), 'currentState': array([24.98176789, 24.2855252 ,  0.        ]), 'targetState': array([25, 24], dtype=int32), 'currentDistance': 0.2861067128498614}
done in step count: 0
reward sum = 0.992628287593777
running average episode reward sum: -0.9166135278380216
{'scaleFactor': 20, 'currentTarget': array([25., 24.]), 'previousTarget': array([25., 24.]), 'currentState': array([24.98176789, 24.2855252 ,  0.        ]), 'targetState': array([25, 24], dtype=int32), 'currentDistance': 0.2861067128498614}
episode index:228
target Thresh 11.619218948212382
target distance 2.0
model initialize at round 228
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 21.]), 'previousTarget': array([ 4., 21.]), 'currentState': array([ 3.2490207 , 19.93943407,  0.        ]), 'targetState': array([ 4, 21], dtype=int32), 'currentDistance': 1.2995268396947672}
done in step count: 2
reward sum = 0.9702310803950408
running average episode reward sum: -0.9083740317322003
{'scaleFactor': 20, 'currentTarget': array([ 4., 21.]), 'previousTarget': array([ 4., 21.]), 'currentState': array([ 3.95069021, 20.21244458,  0.        ]), 'targetState': array([ 4, 21], dtype=int32), 'currentDistance': 0.7890975825096836}
episode index:229
target Thresh 11.639589542269594
target distance 5.0
model initialize at round 229
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 16.]), 'previousTarget': array([17., 16.]), 'currentState': array([22.67002463, 18.66386676,  0.        ]), 'targetState': array([17, 16], dtype=int32), 'currentDistance': 6.264612154768423}
done in step count: 99
reward sum = -0.13653169785028363
running average episode reward sum: -0.905018195497931
{'scaleFactor': 20, 'currentTarget': array([17., 16.]), 'previousTarget': array([17., 16.]), 'currentState': array([59.48832572, 50.50308538,  0.        ]), 'targetState': array([17, 16], dtype=int32), 'currentDistance': 54.73317753687131}
episode index:230
target Thresh 11.659939775914648
target distance 7.0
model initialize at round 230
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  6.]), 'previousTarget': array([19.,  6.]), 'currentState': array([26.81845891,  8.50180608,  0.        ]), 'targetState': array([19,  6], dtype=int32), 'currentDistance': 8.208978832069853}
done in step count: 99
reward sum = -0.13095369492765743
running average episode reward sum: -0.9016672669240338
{'scaleFactor': 20, 'currentTarget': array([19.,  6.]), 'previousTarget': array([19.,  6.]), 'currentState': array([62.98394922, 36.61124362,  0.        ]), 'targetState': array([19,  6], dtype=int32), 'currentDistance': 53.587648061828645}
episode index:231
target Thresh 11.680269669497786
target distance 8.0
model initialize at round 231
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 20.]), 'previousTarget': array([21., 20.]), 'currentState': array([22.9725585 , 26.01907372,  0.        ]), 'targetState': array([21, 20], dtype=int32), 'currentDistance': 6.3340536415062685}
done in step count: 99
reward sum = -0.1391723788468091
running average episode reward sum: -0.8983806510271491
{'scaleFactor': 20, 'currentTarget': array([21., 20.]), 'previousTarget': array([21., 20.]), 'currentState': array([57.84534897, 42.90316945,  0.        ]), 'targetState': array([21, 20], dtype=int32), 'currentDistance': 43.383578821857775}
episode index:232
target Thresh 11.700579243348898
target distance 5.0
model initialize at round 232
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([ 6.3035762 , 12.54016489,  0.        ]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 4.7715807582191605}
done in step count: 8
reward sum = 0.9106579133738808
running average episode reward sum: -0.8906165370168443
{'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([ 7.88279305, 16.27039227,  0.        ]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 0.7389620499236141}
episode index:233
target Thresh 11.72086851777756
target distance 10.0
model initialize at round 233
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 29.]), 'previousTarget': array([25., 29.]), 'currentState': array([23.59493589, 19.9968878 ,  0.        ]), 'targetState': array([25, 29], dtype=int32), 'currentDistance': 9.112092754812576}
done in step count: 13
reward sum = 0.8417293350599926
running average episode reward sum: -0.8832133495293365
{'scaleFactor': 20, 'currentTarget': array([25., 29.]), 'previousTarget': array([25., 29.]), 'currentState': array([24.7813027 , 28.13498914,  0.        ]), 'targetState': array([25, 29], dtype=int32), 'currentDistance': 0.8922288343478665}
episode index:234
target Thresh 11.741137513073053
target distance 1.0
model initialize at round 234
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  2.]), 'previousTarget': array([26.,  2.]), 'currentState': array([26.16265844,  2.76736452,  0.        ]), 'targetState': array([26,  2], dtype=int32), 'currentDistance': 0.7844144753940658}
done in step count: 0
reward sum = 0.9995971148852046
running average episode reward sum: -0.8752013901062958
{'scaleFactor': 20, 'currentTarget': array([26.,  2.]), 'previousTarget': array([26.,  2.]), 'currentState': array([26.16265844,  2.76736452,  0.        ]), 'targetState': array([26,  2], dtype=int32), 'currentDistance': 0.7844144753940658}
episode index:235
target Thresh 11.761386249504369
target distance 4.0
model initialize at round 235
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  4.]), 'previousTarget': array([12.,  4.]), 'currentState': array([13.64068413,  6.78487396,  0.        ]), 'targetState': array([12,  4], dtype=int32), 'currentDistance': 3.2322387587717953}
done in step count: 7
reward sum = 0.9049683287846314
running average episode reward sum: -0.867658298077097
{'scaleFactor': 20, 'currentTarget': array([12.,  4.]), 'previousTarget': array([12.,  4.]), 'currentState': array([12.24657106,  4.8077222 ,  0.        ]), 'targetState': array([12,  4], dtype=int32), 'currentDistance': 0.8445190628514928}
episode index:236
target Thresh 11.781614747320248
target distance 8.0
model initialize at round 236
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  5.]), 'previousTarget': array([21.,  5.]), 'currentState': array([14.94899297,  8.00899196,  0.        ]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 6.757863471662924}
done in step count: 3
reward sum = 0.8601321839889303
running average episode reward sum: -0.8603680428785062
{'scaleFactor': 20, 'currentTarget': array([21.,  5.]), 'previousTarget': array([21.,  5.]), 'currentState': array([20.05906975,  4.43931617,  0.        ]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 1.095315520424981}
episode index:237
target Thresh 11.80182302674919
target distance 7.0
model initialize at round 237
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([9.98174298, 2.59194839, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 9.641329948961083}
done in step count: 99
reward sum = -0.2606606019777938
running average episode reward sum: -0.8578482637150578
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([55.80444896, 65.58161976,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 78.80711682334434}
episode index:238
target Thresh 11.822011107999476
target distance 11.0
model initialize at round 238
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 10.]), 'previousTarget': array([18., 10.]), 'currentState': array([27.90718913, 20.35407627,  0.        ]), 'targetState': array([18, 10], dtype=int32), 'currentDistance': 14.330362584429963}
done in step count: 99
reward sum = -0.297832416957675
running average episode reward sum: -0.8555051011763241
{'scaleFactor': 20, 'currentTarget': array([18., 10.]), 'previousTarget': array([18., 10.]), 'currentState': array([76.89479095, 92.38160557,  0.        ]), 'targetState': array([18, 10], dtype=int32), 'currentDistance': 101.26858020663116}
episode index:239
target Thresh 11.842179011259187
target distance 4.0
model initialize at round 239
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([21.61856973,  2.86886519,  0.        ]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 4.755065963864621}
done in step count: 99
reward sum = -0.46190617523070676
running average episode reward sum: -0.8538651056515506
{'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([ 88.85863031, 100.43535098,   0.        ]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 120.26404146769148}
episode index:240
target Thresh 11.86232675669623
target distance 9.0
model initialize at round 240
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 27.]), 'previousTarget': array([18., 27.]), 'currentState': array([15.6321938 , 19.17821121,  0.        ]), 'targetState': array([18, 27], dtype=int32), 'currentDistance': 8.172324395447848}
done in step count: 9
reward sum = 0.8719199034870679
running average episode reward sum: -0.8467041720036725
{'scaleFactor': 20, 'currentTarget': array([18., 27.]), 'previousTarget': array([18., 27.]), 'currentState': array([17.67483916, 26.09211561,  0.        ]), 'targetState': array([18, 27], dtype=int32), 'currentDistance': 0.9643565925052192}
episode index:241
target Thresh 11.882454364458354
target distance 10.0
model initialize at round 241
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 2.]), 'previousTarget': array([8., 2.]), 'currentState': array([18.84393567,  8.50519395,  0.        ]), 'targetState': array([8, 2], dtype=int32), 'currentDistance': 12.645492837081553}
done in step count: 99
reward sum = -0.43954769664415144
running average episode reward sum: -0.8450217072294597
{'scaleFactor': 20, 'currentTarget': array([8., 2.]), 'previousTarget': array([8., 2.]), 'currentState': array([ 76.28517193, 110.89781618,   0.        ]), 'targetState': array([8, 2], dtype=int32), 'currentDistance': 128.53637257211375}
episode index:242
target Thresh 11.902561854673163
target distance 1.0
model initialize at round 242
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  7.]), 'previousTarget': array([13.,  7.]), 'currentState': array([13.56075004,  8.27606353,  0.        ]), 'targetState': array([13,  7], dtype=int32), 'currentDistance': 1.393835980757482}
done in step count: 5
reward sum = 0.9494399081596832
running average episode reward sum: -0.8376370915282698
{'scaleFactor': 20, 'currentTarget': array([13.,  7.]), 'previousTarget': array([13.,  7.]), 'currentState': array([13.31543532,  7.93338576,  0.        ]), 'targetState': array([13,  7], dtype=int32), 'currentDistance': 0.9852453542624702}
episode index:243
target Thresh 11.922649247448152
target distance 11.0
model initialize at round 243
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  9.]), 'previousTarget': array([24.,  9.]), 'currentState': array([14.91256356, 18.00001347,  0.        ]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 12.789907876111481}
done in step count: 6
reward sum = 0.7555119336392843
running average episode reward sum: -0.8311077922447961
{'scaleFactor': 20, 'currentTarget': array([24.,  9.]), 'previousTarget': array([24.,  9.]), 'currentState': array([23.69589919,  8.72814615,  0.        ]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 0.4078992754571694}
episode index:244
target Thresh 11.942716562870718
target distance 7.0
model initialize at round 244
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([ 6.92226148, 13.00853646,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 5.902094554653827}
done in step count: 3
reward sum = 0.8675638052613546
running average episode reward sum: -0.8241744387855874
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([11.69584191,  9.79143354,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.3687982033032437}
episode index:245
target Thresh 11.962763821008174
target distance 10.0
model initialize at round 245
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  4.]), 'previousTarget': array([12.,  4.]), 'currentState': array([22.60787565, 12.24808399,  0.        ]), 'targetState': array([12,  4], dtype=int32), 'currentDistance': 13.437184052912592}
done in step count: 99
reward sum = -0.333372354189477
running average episode reward sum: -0.8221793083604
{'scaleFactor': 20, 'currentTarget': array([12.,  4.]), 'previousTarget': array([12.,  4.]), 'currentState': array([81.56797323, 95.65610619,  0.        ]), 'targetState': array([12,  4], dtype=int32), 'currentDistance': 115.06756580602968}
episode index:246
target Thresh 11.982791041907781
target distance 11.0
model initialize at round 246
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 19.]), 'previousTarget': array([19., 19.]), 'currentState': array([21.7376669 ,  9.00456178,  0.        ]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 10.363571066698759}
done in step count: 99
reward sum = -0.5122228475527151
running average episode reward sum: -0.8209244239036887
{'scaleFactor': 20, 'currentTarget': array([19., 19.]), 'previousTarget': array([19., 19.]), 'currentState': array([ 84.02199128, 117.00450766,   0.        ]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 117.61268159194195}
episode index:247
target Thresh 12.002798245596761
target distance 1.0
model initialize at round 247
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 17.]), 'previousTarget': array([ 6., 17.]), 'currentState': array([ 6.04545092, 17.77496459,  0.        ]), 'targetState': array([ 6, 17], dtype=int32), 'currentDistance': 0.7762962680056463}
done in step count: 0
reward sum = 0.9997364663836731
running average episode reward sum: -0.8135830493460784
{'scaleFactor': 20, 'currentTarget': array([ 6., 17.]), 'previousTarget': array([ 6., 17.]), 'currentState': array([ 6.04545092, 17.77496459,  0.        ]), 'targetState': array([ 6, 17], dtype=int32), 'currentDistance': 0.7762962680056463}
episode index:248
target Thresh 12.02278545208232
target distance 11.0
model initialize at round 248
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([ 9.71307576, 11.6327455 ,  0.        ]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 11.837195629983212}
done in step count: 99
reward sum = -0.4992504545972048
running average episode reward sum: -0.8123206694474886
{'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([ 70.01327811, 119.65653062,   0.        ]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 117.87514945714067}
episode index:249
target Thresh 12.042752681351669
target distance 3.0
model initialize at round 249
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([6.0837641 , 7.98406202, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 3.6842376376955217}
done in step count: 8
reward sum = 0.8917263018425233
running average episode reward sum: -0.8055044815623286
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.23332617, 10.93047105,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.9592796688114832}
episode index:250
target Thresh 12.062699953372032
target distance 10.0
model initialize at round 250
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([26.94837201, 11.00267422,  0.        ]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 8.002840755198363}
done in step count: 7
reward sum = 0.8257039794109154
running average episode reward sum: -0.7990056430723953
{'scaleFactor': 20, 'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([27.3067609 ,  3.94775316,  0.        ]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 0.9961617849839287}
episode index:251
target Thresh 12.082627288090691
target distance 6.0
model initialize at round 251
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([9.86908638, 5.04909611, 0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 4.262047622821775}
done in step count: 3
reward sum = 0.8974285455428809
running average episode reward sum: -0.792273761371541
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.3541255 ,  3.10704385,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.102054699412218}
episode index:252
target Thresh 12.102534705434977
target distance 5.0
model initialize at round 252
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([4.13589096, 8.40401995, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 4.548146876544005}
done in step count: 19
reward sum = 0.8031056207048471
running average episode reward sum: -0.7859679140115553
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.41417023, 4.99839058, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.0808888580114913}
episode index:253
target Thresh 12.12242222531231
target distance 12.0
model initialize at round 253
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 24.]), 'previousTarget': array([19., 24.]), 'currentState': array([15.90337801, 13.61947131,  0.        ]), 'targetState': array([19, 24], dtype=int32), 'currentDistance': 10.832564036346607}
done in step count: 13
reward sum = 0.7873049462825591
running average episode reward sum: -0.7797739263726021
{'scaleFactor': 20, 'currentTarget': array([19., 24.]), 'previousTarget': array([19., 24.]), 'currentState': array([18.35677975, 23.03689384,  0.        ]), 'targetState': array([19, 24], dtype=int32), 'currentDistance': 1.158147552608962}
episode index:254
target Thresh 12.142289867610213
target distance 4.0
model initialize at round 254
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  5.]), 'previousTarget': array([27.,  5.]), 'currentState': array([23.73816985,  3.05836972,  0.        ]), 'targetState': array([27,  5], dtype=int32), 'currentDistance': 3.795979990011773}
done in step count: 5
reward sum = 0.9386755619342368
running average episode reward sum: -0.7730349087713988
{'scaleFactor': 20, 'currentTarget': array([27.,  5.]), 'previousTarget': array([27.,  5.]), 'currentState': array([26.53996524,  4.12041504,  0.        ]), 'targetState': array([27,  5], dtype=int32), 'currentDistance': 0.9926236353299559}
episode index:255
target Thresh 12.162137652196325
target distance 9.0
model initialize at round 255
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([17.9997927 ,  3.38013482,  0.        ]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 7.880756703954621}
done in step count: 14
reward sum = 0.7987278358279758
running average episode reward sum: -0.7668952105503075
{'scaleFactor': 20, 'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([24.71161504,  6.12772132,  0.        ]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 0.9187143039369837}
episode index:256
target Thresh 12.181965598918438
target distance 11.0
model initialize at round 256
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([25.75437105,  8.75035131,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 13.085556117218216}
done in step count: 99
reward sum = -0.263540142310186
running average episode reward sum: -0.7649366305182448
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([55.21310582, 70.98182633,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 79.49873459873722}
episode index:257
target Thresh 12.201773727604504
target distance 6.0
model initialize at round 257
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 27.]), 'previousTarget': array([27., 27.]), 'currentState': array([24.02671498, 22.36119008,  0.        ]), 'targetState': array([27, 27], dtype=int32), 'currentDistance': 5.509898481324602}
done in step count: 9
reward sum = 0.8856604648425054
running average episode reward sum: -0.7585389673579317
{'scaleFactor': 20, 'currentTarget': array([27., 27.]), 'previousTarget': array([27., 27.]), 'currentState': array([26.75268589, 26.08004192,  0.        ]), 'targetState': array([27, 27], dtype=int32), 'currentDistance': 0.9526211980796586}
episode index:258
target Thresh 12.221562058062641
target distance 10.0
model initialize at round 258
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  3.]), 'previousTarget': array([21.,  3.]), 'currentState': array([24.96335566, 11.18737578,  0.        ]), 'targetState': array([21,  3], dtype=int32), 'currentDistance': 9.096225058685349}
done in step count: 9
reward sum = 0.8362277874815056
running average episode reward sum: -0.752381566760096
{'scaleFactor': 20, 'currentTarget': array([21.,  3.]), 'previousTarget': array([21.,  3.]), 'currentState': array([21.44123349,  3.99001247,  0.        ]), 'targetState': array([21,  3], dtype=int32), 'currentDistance': 1.0838872980977883}
episode index:259
target Thresh 12.241330610081192
target distance 6.0
model initialize at round 259
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  8.]), 'previousTarget': array([10.,  8.]), 'currentState': array([16.40072718,  2.32762671,  0.        ]), 'targetState': array([10,  8], dtype=int32), 'currentDistance': 8.552492459062476}
done in step count: 99
reward sum = -0.3977449456778981
running average episode reward sum: -0.7510175797559338
{'scaleFactor': 20, 'currentTarget': array([10.,  8.]), 'previousTarget': array([10.,  8.]), 'currentState': array([63.15073196, 94.10303533,  0.        ]), 'targetState': array([10,  8], dtype=int32), 'currentDistance': 101.18662461421007}
episode index:260
target Thresh 12.261079403428703
target distance 4.0
model initialize at round 260
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 23.]), 'previousTarget': array([ 8., 23.]), 'currentState': array([ 5.93797886, 21.56935811,  0.        ]), 'targetState': array([ 8, 23], dtype=int32), 'currentDistance': 2.5097146018381338}
done in step count: 2
reward sum = 0.9447158244106446
running average episode reward sum: -0.7445205169047208
{'scaleFactor': 20, 'currentTarget': array([ 8., 23.]), 'previousTarget': array([ 8., 23.]), 'currentState': array([ 7.4543131, 22.0062539,  0.       ]), 'targetState': array([ 8, 23], dtype=int32), 'currentDistance': 1.133713150871031}
episode index:261
target Thresh 12.280808457853972
target distance 9.0
model initialize at round 261
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  3.]), 'previousTarget': array([26.,  3.]), 'currentState': array([22.98541772, 10.00002599,  0.        ]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 7.621552999768261}
done in step count: 6
reward sum = 0.8263206624911801
running average episode reward sum: -0.7385249398841257
{'scaleFactor': 20, 'currentTarget': array([26.,  3.]), 'previousTarget': array([26.,  3.]), 'currentState': array([26.30781516,  3.82326005,  0.        ]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 0.8789239365017735}
episode index:262
target Thresh 12.300517793086058
target distance 9.0
model initialize at round 262
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  3.]), 'previousTarget': array([12.,  3.]), 'currentState': array([21.87326688,  3.46005827,  0.        ]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 9.883979583810836}
done in step count: 99
reward sum = -0.441196338161994
running average episode reward sum: -0.73739441288138
{'scaleFactor': 20, 'currentTarget': array([12.,  3.]), 'previousTarget': array([12.,  3.]), 'currentState': array([ 69.83281705, 100.22767198,   0.        ]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 113.12760461946019}
episode index:263
target Thresh 12.320207428834294
target distance 4.0
model initialize at round 263
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  5.]), 'previousTarget': array([10.,  5.]), 'currentState': array([9.50080574, 7.06555867, 0.        ]), 'targetState': array([10,  5], dtype=int32), 'currentDistance': 2.125024127132423}
done in step count: 3
reward sum = 0.9397869235258639
running average episode reward sum: -0.7310414532737768
{'scaleFactor': 20, 'currentTarget': array([10.,  5.]), 'previousTarget': array([10.,  5.]), 'currentState': array([10.55776577,  5.8523737 ,  0.        ]), 'targetState': array([10,  5], dtype=int32), 'currentDistance': 1.0186479188703605}
episode index:264
target Thresh 12.339877384788323
target distance 10.0
model initialize at round 264
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 25.]), 'previousTarget': array([ 9., 25.]), 'currentState': array([19.73243874, 22.21528332,  0.        ]), 'targetState': array([ 9, 25], dtype=int32), 'currentDistance': 11.087826133474943}
done in step count: 99
reward sum = -0.4700472214811519
running average episode reward sum: -0.7300565693802196
{'scaleFactor': 20, 'currentTarget': array([ 9., 25.]), 'previousTarget': array([ 9., 25.]), 'currentState': array([ 66.18183549, 123.11954141,   0.        ]), 'targetState': array([ 9, 25], dtype=int32), 'currentDistance': 113.56586950496421}
episode index:265
target Thresh 12.359527680618093
target distance 7.0
model initialize at round 265
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  2.]), 'previousTarget': array([21.,  2.]), 'currentState': array([26.23651546,  9.56762779,  0.        ]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 9.202721582197535}
done in step count: 31
reward sum = 0.7043165526662879
running average episode reward sum: -0.72466418922215
{'scaleFactor': 20, 'currentTarget': array([21.,  2.]), 'previousTarget': array([21.,  2.]), 'currentState': array([21.6284472 ,  2.99139191,  0.        ]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 1.1737988775334809}
episode index:266
target Thresh 12.379158335973909
target distance 2.0
model initialize at round 266
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 23.]), 'previousTarget': array([19., 23.]), 'currentState': array([19.50460351, 24.1854949 ,  0.        ]), 'targetState': array([19, 23], dtype=int32), 'currentDistance': 1.2884187419959827}
done in step count: 1
reward sum = 0.9751356468437025
running average episode reward sum: -0.7182978977013041
{'scaleFactor': 20, 'currentTarget': array([19., 23.]), 'previousTarget': array([19., 23.]), 'currentState': array([19.58667582, 23.98713775,  0.        ]), 'targetState': array([19, 23], dtype=int32), 'currentDistance': 1.1483159225771433}
episode index:267
target Thresh 12.398769370486427
target distance 4.0
model initialize at round 267
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 15.]), 'previousTarget': array([ 4., 15.]), 'currentState': array([ 5.4946636 , 18.53455821,  0.        ]), 'targetState': array([ 4, 15], dtype=int32), 'currentDistance': 3.837593123677805}
done in step count: 17
reward sum = 0.8341932077027385
running average episode reward sum: -0.7125050204423339
{'scaleFactor': 20, 'currentTarget': array([ 4., 15.]), 'previousTarget': array([ 4., 15.]), 'currentState': array([ 4.55134509, 15.98767494,  0.        ]), 'targetState': array([ 4, 15], dtype=int32), 'currentDistance': 1.1311424257909177}
episode index:268
target Thresh 12.418360803766681
target distance 12.0
model initialize at round 268
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([5.99999392, 6.00005889, 0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 10.049887530502945}
done in step count: 8
reward sum = 0.7517817844655521
running average episode reward sum: -0.7070615750709291
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.91480079,  4.51567233,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.4917643750842145}
episode index:269
target Thresh 12.437932655406104
target distance 9.0
model initialize at round 269
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 29.]), 'previousTarget': array([12., 29.]), 'currentState': array([ 4.999704  , 23.17114806,  0.        ]), 'targetState': array([12, 29], dtype=int32), 'currentDistance': 9.109317151079505}
done in step count: 8
reward sum = 0.8221803659157166
running average episode reward sum: -0.7013977160302378
{'scaleFactor': 20, 'currentTarget': array([12., 29.]), 'previousTarget': array([12., 29.]), 'currentState': array([11.91733365, 28.4299643 ,  0.        ]), 'targetState': array([12, 29], dtype=int32), 'currentDistance': 0.5759986289612953}
episode index:270
target Thresh 12.457484944976557
target distance 6.0
model initialize at round 270
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 23.]), 'previousTarget': array([11., 23.]), 'currentState': array([ 8.82739478, 27.00509274,  0.        ]), 'targetState': array([11, 23], dtype=int32), 'currentDistance': 4.556421982570256}
done in step count: 3
reward sum = 0.9083281361304858
running average episode reward sum: -0.6954577682362868
{'scaleFactor': 20, 'currentTarget': array([11., 23.]), 'previousTarget': array([11., 23.]), 'currentState': array([11.61875933, 23.86171803,  0.        ]), 'targetState': array([11, 23], dtype=int32), 'currentDistance': 1.060858649703719}
episode index:271
target Thresh 12.477017692030326
target distance 8.0
model initialize at round 271
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 14.]), 'previousTarget': array([ 2., 14.]), 'currentState': array([ 5.47839865, 21.9458409 ,  0.        ]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 8.67384832158869}
done in step count: 99
reward sum = -0.2422186371404689
running average episode reward sum: -0.6937914479013757
{'scaleFactor': 20, 'currentTarget': array([ 2., 14.]), 'previousTarget': array([ 2., 14.]), 'currentState': array([46.49042721, 85.48458912,  0.        ]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 84.19883963206327}
episode index:272
target Thresh 12.496530916100156
target distance 6.0
model initialize at round 272
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 20.]), 'previousTarget': array([20., 20.]), 'currentState': array([17.90015221, 15.46241879,  0.        ]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 4.999900394158672}
done in step count: 5
reward sum = 0.9109447173629118
running average episode reward sum: -0.6879132934498581
{'scaleFactor': 20, 'currentTarget': array([20., 20.]), 'previousTarget': array([20., 20.]), 'currentState': array([19.6844798 , 19.20824003,  0.        ]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 0.8523126430061513}
episode index:273
target Thresh 12.516024636699282
target distance 7.0
model initialize at round 273
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 27.]), 'previousTarget': array([ 9., 27.]), 'currentState': array([ 3.99392307, 21.97968066,  0.        ]), 'targetState': array([ 9, 27], dtype=int32), 'currentDistance': 7.089739949423656}
done in step count: 6
reward sum = 0.8760211819922208
running average episode reward sum: -0.6822055033935002
{'scaleFactor': 20, 'currentTarget': array([ 9., 27.]), 'previousTarget': array([ 9., 27.]), 'currentState': array([ 8.70124315, 26.24670902,  0.        ]), 'targetState': array([ 9, 27], dtype=int32), 'currentDistance': 0.8103721115639841}
episode index:274
target Thresh 12.535498873321416
target distance 6.0
model initialize at round 274
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 27.]), 'previousTarget': array([ 4., 27.]), 'currentState': array([ 2.81011057, 22.29727006,  0.        ]), 'targetState': array([ 4, 27], dtype=int32), 'currentDistance': 4.850928339037221}
done in step count: 4
reward sum = 0.9208093824951576
running average episode reward sum: -0.6763763583539051
{'scaleFactor': 20, 'currentTarget': array([ 4., 27.]), 'previousTarget': array([ 4., 27.]), 'currentState': array([ 3.68722698, 26.13541096,  0.        ]), 'targetState': array([ 4, 27], dtype=int32), 'currentDistance': 0.9194243643646826}
episode index:275
target Thresh 12.554953645440804
target distance 9.0
model initialize at round 275
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 28.]), 'previousTarget': array([22., 28.]), 'currentState': array([14.99997723, 24.16320586,  0.        ]), 'targetState': array([22, 28], dtype=int32), 'currentDistance': 7.982562747839847}
done in step count: 5
reward sum = 0.847024652775271
running average episode reward sum: -0.6708567894730022
{'scaleFactor': 20, 'currentTarget': array([22., 28.]), 'previousTarget': array([22., 28.]), 'currentState': array([21.42302924, 27.09592593,  0.        ]), 'targetState': array([22, 28], dtype=int32), 'currentDistance': 1.0724948403225978}
episode index:276
target Thresh 12.57438897251222
target distance 12.0
model initialize at round 276
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([ 3.94964755, 14.00000095,  0.        ]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 12.235501166757581}
done in step count: 5
reward sum = 0.7783827283860327
running average episode reward sum: -0.6656248778561826
{'scaleFactor': 20, 'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([10.59465623,  4.5555824 ,  0.        ]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 0.6877320557158311}
episode index:277
target Thresh 12.593804873970985
target distance 7.0
model initialize at round 277
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 28.]), 'previousTarget': array([13., 28.]), 'currentState': array([14.76062298, 21.76316768,  0.        ]), 'targetState': array([13, 28], dtype=int32), 'currentDistance': 6.480576414780372}
done in step count: 6
reward sum = 0.8868564222675573
running average episode reward sum: -0.6600404127478239
{'scaleFactor': 20, 'currentTarget': array([13., 28.]), 'previousTarget': array([13., 28.]), 'currentState': array([12.85493939, 27.34446827,  0.        ]), 'targetState': array([13, 28], dtype=int32), 'currentDistance': 0.6713899286125179}
episode index:278
target Thresh 12.61320136923301
target distance 4.0
model initialize at round 278
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 29.]), 'previousTarget': array([19., 29.]), 'currentState': array([16.57173204, 26.90019077,  0.        ]), 'targetState': array([19, 29], dtype=int32), 'currentDistance': 3.210246729588154}
done in step count: 2
reward sum = 0.947364339658183
running average episode reward sum: -0.65427910539153
{'scaleFactor': 20, 'currentTarget': array([19., 29.]), 'previousTarget': array([19., 29.]), 'currentState': array([18.61062378, 28.13564432,  0.        ]), 'targetState': array([19, 29], dtype=int32), 'currentDistance': 0.9480108598559629}
episode index:279
target Thresh 12.632578477694786
target distance 9.0
model initialize at round 279
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 26.]), 'previousTarget': array([12., 26.]), 'currentState': array([ 6.87445819, 18.51884663,  0.        ]), 'targetState': array([12, 26], dtype=int32), 'currentDistance': 9.06856298081825}
done in step count: 6
reward sum = 0.8433658619792175
running average episode reward sum: -0.648930373365206
{'scaleFactor': 20, 'currentTarget': array([12., 26.]), 'previousTarget': array([12., 26.]), 'currentState': array([11.91186109, 25.42017204,  0.        ]), 'targetState': array([12, 26], dtype=int32), 'currentDistance': 0.5864886489272604}
episode index:280
target Thresh 12.651936218733425
target distance 5.0
model initialize at round 280
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([21.30776268, 20.01747668,  0.        ]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 4.307798133905027}
done in step count: 99
reward sum = -0.2770510105321687
running average episode reward sum: -0.6476069592625974
{'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([72.87912807, 87.1621943 ,  0.        ]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 87.36839987356335}
episode index:281
target Thresh 12.671274611706675
target distance 9.0
model initialize at round 281
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 25.]), 'previousTarget': array([ 4., 25.]), 'currentState': array([13.40639952, 28.5716778 ,  0.        ]), 'targetState': array([ 4, 25], dtype=int32), 'currentDistance': 10.061671542666124}
done in step count: 99
reward sum = -0.33219630573031705
running average episode reward sum: -0.646488481767802
{'scaleFactor': 20, 'currentTarget': array([ 4., 25.]), 'previousTarget': array([ 4., 25.]), 'currentState': array([ 53.68676598, 109.95335057,   0.        ]), 'targetState': array([ 4, 25], dtype=int32), 'currentDistance': 98.41669821167898}
episode index:282
target Thresh 12.690593675952922
target distance 7.0
model initialize at round 282
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 3.]), 'previousTarget': array([9., 3.]), 'currentState': array([9.13738823, 8.04855144, 0.        ]), 'targetState': array([9, 3], dtype=int32), 'currentDistance': 5.050420494405179}
done in step count: 99
reward sum = -0.3161412317439195
running average episode reward sum: -0.6453211769973997
{'scaleFactor': 20, 'currentTarget': array([9., 3.]), 'previousTarget': array([9., 3.]), 'currentState': array([56.6656977 , 80.99552964,  0.        ]), 'targetState': array([9, 3], dtype=int32), 'currentDistance': 91.40744707981601}
episode index:283
target Thresh 12.709893430791237
target distance 5.0
model initialize at round 283
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  7.]), 'previousTarget': array([21.,  7.]), 'currentState': array([24.13966505, 12.58707929,  0.        ]), 'targetState': array([21,  7], dtype=int32), 'currentDistance': 6.408818268220831}
done in step count: 99
reward sum = -0.33027191490744157
running average episode reward sum: -0.6442118486097589
{'scaleFactor': 20, 'currentTarget': array([21.,  7.]), 'previousTarget': array([21.,  7.]), 'currentState': array([70.42209843, 90.74061603,  0.        ]), 'targetState': array([21,  7], dtype=int32), 'currentDistance': 97.23700214574345}
episode index:284
target Thresh 12.729173895521374
target distance 10.0
model initialize at round 284
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 21.]), 'previousTarget': array([ 6., 21.]), 'currentState': array([16.22295035, 27.66100144,  0.        ]), 'targetState': array([ 6, 21], dtype=int32), 'currentDistance': 12.201543106507575}
done in step count: 99
reward sum = -0.2793371483305549
running average episode reward sum: -0.642931586503516
{'scaleFactor': 20, 'currentTarget': array([ 6., 21.]), 'previousTarget': array([ 6., 21.]), 'currentState': array([ 54.39468301, 101.93195592,   0.        ]), 'targetState': array([ 6, 21], dtype=int32), 'currentDistance': 94.29754415002527}
episode index:285
target Thresh 12.748435089423804
target distance 9.0
model initialize at round 285
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 14.]), 'previousTarget': array([12., 14.]), 'currentState': array([14.88254428, 23.11850109,  0.        ]), 'targetState': array([12, 14], dtype=int32), 'currentDistance': 9.563269505792329}
done in step count: 99
reward sum = -0.3722268186420325
running average episode reward sum: -0.6419850663361681
{'scaleFactor': 20, 'currentTarget': array([12., 14.]), 'previousTarget': array([12., 14.]), 'currentState': array([ 63.8793944 , 103.25643777,   0.        ]), 'targetState': array([12, 14], dtype=int32), 'currentDistance': 103.23847754876148}
episode index:286
target Thresh 12.767677031759717
target distance 10.0
model initialize at round 286
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 28.]), 'previousTarget': array([ 7., 28.]), 'currentState': array([ 3.10648489, 19.9266845 ,  0.        ]), 'targetState': array([ 7, 28], dtype=int32), 'currentDistance': 8.963140247371383}
done in step count: 5
reward sum = 0.8512962434326458
running average episode reward sum: -0.6367819955704231
{'scaleFactor': 20, 'currentTarget': array([ 7., 28.]), 'previousTarget': array([ 7., 28.]), 'currentState': array([ 6.71124709, 27.40696079,  0.        ]), 'targetState': array([ 7, 28], dtype=int32), 'currentDistance': 0.659601209146138}
episode index:287
target Thresh 12.786899741771059
target distance 5.0
model initialize at round 287
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 12.]), 'previousTarget': array([16., 12.]), 'currentState': array([19.75666285, 12.01783729,  0.        ]), 'targetState': array([16, 12], dtype=int32), 'currentDistance': 3.75670519263492}
done in step count: 4
reward sum = 0.9242478253617532
running average episode reward sum: -0.6313617531366309
{'scaleFactor': 20, 'currentTarget': array([16., 12.]), 'previousTarget': array([16., 12.]), 'currentState': array([16.54125565, 12.86590841,  0.        ]), 'targetState': array([16, 12], dtype=int32), 'currentDistance': 1.0211537903050476}
episode index:288
target Thresh 12.806103238680542
target distance 4.0
model initialize at round 288
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.00394964, 6.36265516, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 2.637347794645112}
done in step count: 2
reward sum = 0.9530569041567429
running average episode reward sum: -0.6258793356373459
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.84427759, 8.52384877, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.5009685226596143}
episode index:289
target Thresh 12.825287541691665
target distance 5.0
model initialize at round 289
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 4.]), 'previousTarget': array([7., 4.]), 'currentState': array([9.59826797, 8.47991437, 0.        ]), 'targetState': array([7, 4], dtype=int32), 'currentDistance': 5.178863697201861}
done in step count: 99
reward sum = -0.2943608918254861
running average episode reward sum: -0.6247361685897187
{'scaleFactor': 20, 'currentTarget': array([7., 4.]), 'previousTarget': array([7., 4.]), 'currentState': array([54.58546032, 84.06401945,  0.        ]), 'targetState': array([7, 4], dtype=int32), 'currentDistance': 93.13765749575835}
episode index:290
target Thresh 12.84445266998873
target distance 6.0
model initialize at round 290
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([10.952631  , 12.81225145,  0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 6.24994519152815}
done in step count: 99
reward sum = -0.32323010555771514
running average episode reward sum: -0.6237000652803304
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([56.98681378, 91.67342797,  0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 97.13161623130001}
episode index:291
target Thresh 12.863598642736868
target distance 9.0
model initialize at round 291
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([10.84453416, 17.80569911,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 8.020676084905766}
done in step count: 99
reward sum = -0.3326779501775249
running average episode reward sum: -0.6227034142012111
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([59.1495751 , 96.19947154,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 99.72626924104203}
episode index:292
target Thresh 12.882725479082058
target distance 2.0
model initialize at round 292
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 26.]), 'previousTarget': array([17., 26.]), 'currentState': array([16.60353029, 26.27137834,  0.        ]), 'targetState': array([17, 26], dtype=int32), 'currentDistance': 0.4804523236035064}
done in step count: 0
reward sum = 0.9844890050362639
running average episode reward sum: -0.6172181158420388
{'scaleFactor': 20, 'currentTarget': array([17., 26.]), 'previousTarget': array([17., 26.]), 'currentState': array([16.60353029, 26.27137834,  0.        ]), 'targetState': array([17, 26], dtype=int32), 'currentDistance': 0.4804523236035064}
episode index:293
target Thresh 12.901833198151127
target distance 12.0
model initialize at round 293
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([24.96684253, 18.53900358,  0.        ]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 11.705429218282491}
done in step count: 99
reward sum = -0.42100096994810066
running average episode reward sum: -0.6165507105838961
{'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([ 78.03337067, 102.62780926,   0.        ]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 110.33290438600476}
episode index:294
target Thresh 12.92092181905181
target distance 2.0
model initialize at round 294
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([12.90911901,  6.75135696,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.6580502939858555}
done in step count: 1
reward sum = 0.9792943755366061
running average episode reward sum: -0.6111410662241656
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.5348385 ,  7.35645372,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.7940573291378835}
episode index:295
target Thresh 12.939991360872718
target distance 3.0
model initialize at round 295
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 20.]), 'previousTarget': array([15., 20.]), 'currentState': array([13.8362447 , 19.35791591,  0.        ]), 'targetState': array([15, 20], dtype=int32), 'currentDistance': 1.3291344453841785}
done in step count: 1
reward sum = 0.9701960341269091
running average episode reward sum: -0.6057987111554121
{'scaleFactor': 20, 'currentTarget': array([15., 20.]), 'previousTarget': array([15., 20.]), 'currentState': array([14.49739218, 19.52654032,  0.        ]), 'targetState': array([15, 20], dtype=int32), 'currentDistance': 0.6904916276587347}
episode index:296
target Thresh 12.959041842683401
target distance 3.0
model initialize at round 296
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 12.]), 'previousTarget': array([ 3., 12.]), 'currentState': array([ 4.53669667, 12.47566709,  0.        ]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 1.6086316678648758}
done in step count: 1
reward sum = 0.9720244425574525
running average episode reward sum: -0.6004861752843249
{'scaleFactor': 20, 'currentTarget': array([ 3., 12.]), 'previousTarget': array([ 3., 12.]), 'currentState': array([ 3.42474556, 12.5353367 ,  0.        ]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 0.6833697184799117}
episode index:297
target Thresh 12.97807328353434
target distance 12.0
model initialize at round 297
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 18.]), 'previousTarget': array([14., 18.]), 'currentState': array([13.37082112,  7.87441957,  0.        ]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 10.145109418342017}
done in step count: 6
reward sum = 0.8201931464439074
running average episode reward sum: -0.5957187950100692
{'scaleFactor': 20, 'currentTarget': array([14., 18.]), 'previousTarget': array([14., 18.]), 'currentState': array([13.69676219, 17.53227717,  0.        ]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 0.5574206836068683}
episode index:298
target Thresh 12.997085702456975
target distance 8.0
model initialize at round 298
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 24.]), 'previousTarget': array([13., 24.]), 'currentState': array([17.2895925 , 16.84866822,  0.        ]), 'targetState': array([13, 24], dtype=int32), 'currentDistance': 8.339193613041775}
done in step count: 5
reward sum = 0.8593055683171349
running average episode reward sum: -0.5908524927915836
{'scaleFactor': 20, 'currentTarget': array([13., 24.]), 'previousTarget': array([13., 24.]), 'currentState': array([12.84339655, 23.51142323,  0.        ]), 'targetState': array([13, 24], dtype=int32), 'currentDistance': 0.5130613023520427}
episode index:299
target Thresh 13.016079118463729
target distance 10.0
model initialize at round 299
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 12.]), 'previousTarget': array([ 6., 12.]), 'currentState': array([ 6.87951577, 20.8098309 ,  0.        ]), 'targetState': array([ 6, 12], dtype=int32), 'currentDistance': 8.853624599098728}
done in step count: 14
reward sum = 0.7835748679620671
running average episode reward sum: -0.586271068255738
{'scaleFactor': 20, 'currentTarget': array([ 6., 12.]), 'previousTarget': array([ 6., 12.]), 'currentState': array([ 6.44485206, 12.58456665,  0.        ]), 'targetState': array([ 6, 12], dtype=int32), 'currentDistance': 0.7345825514564454}
episode index:300
target Thresh 13.035053550548021
target distance 10.0
model initialize at round 300
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 16.]), 'previousTarget': array([ 6., 16.]), 'currentState': array([13.96824765,  6.319686  ,  0.        ]), 'targetState': array([ 6, 16], dtype=int32), 'currentDistance': 12.538000234034717}
done in step count: 8
reward sum = 0.8003068992158744
running average episode reward sum: -0.5816644969352344
{'scaleFactor': 20, 'currentTarget': array([ 6., 16.]), 'previousTarget': array([ 6., 16.]), 'currentState': array([ 5.79522066, 15.54150251,  0.        ]), 'targetState': array([ 6, 16], dtype=int32), 'currentDistance': 0.5021499091509378}
episode index:301
target Thresh 13.054009017684283
target distance 9.0
model initialize at round 301
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 23.]), 'previousTarget': array([ 7., 23.]), 'currentState': array([15.05770731, 17.2003435 ,  0.        ]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 9.927873012604357}
done in step count: 6
reward sum = 0.8592826843676917
running average episode reward sum: -0.5768931486527743
{'scaleFactor': 20, 'currentTarget': array([ 7., 23.]), 'previousTarget': array([ 7., 23.]), 'currentState': array([ 6.99644727, 22.15981852,  0.        ]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 0.8401889962121242}
episode index:302
target Thresh 13.072945538827984
target distance 4.0
model initialize at round 302
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([6.49499786, 7.13366365, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 3.2327855859833683}
done in step count: 2
reward sum = 0.9450681492739464
running average episode reward sum: -0.5718701740721581
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([4.98156178, 9.2409358 , 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.7592881033870286}
episode index:303
target Thresh 13.091863132915645
target distance 10.0
model initialize at round 303
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 12.]), 'previousTarget': array([25., 12.]), 'currentState': array([16.99999988,  9.53496623,  0.        ]), 'targetState': array([25, 12], dtype=int32), 'currentDistance': 8.371164398854923}
done in step count: 5
reward sum = 0.8452570829765216
running average episode reward sum: -0.567208571252919
{'scaleFactor': 20, 'currentTarget': array([25., 12.]), 'previousTarget': array([25., 12.]), 'currentState': array([24.54878402, 11.44029476,  0.        ]), 'targetState': array([25, 12], dtype=int32), 'currentDistance': 0.7189338087091003}
episode index:304
target Thresh 13.110761818864866
target distance 10.0
model initialize at round 304
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 3.]), 'previousTarget': array([5., 3.]), 'currentState': array([13.1217587, 13.4716773,  0.       ]), 'targetState': array([5, 3], dtype=int32), 'currentDistance': 13.252131523506076}
done in step count: 99
reward sum = -0.35260560640917327
running average episode reward sum: -0.5665049549747428
{'scaleFactor': 20, 'currentTarget': array([5., 3.]), 'previousTarget': array([5., 3.]), 'currentState': array([63.51532401, 96.8906998 ,  0.        ]), 'targetState': array([5, 3], dtype=int32), 'currentDistance': 110.63230384492208}
episode index:305
target Thresh 13.129641615574329
target distance 8.0
model initialize at round 305
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([18.60606694,  3.18346958,  0.        ]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 6.851270190031241}
done in step count: 4
reward sum = 0.9041163956860798
running average episode reward sum: -0.5616990028484002
{'scaleFactor': 20, 'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([12.68349516,  4.58444063,  0.        ]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 0.7999095111676608}
episode index:306
target Thresh 13.148502541923836
target distance 13.0
model initialize at round 306
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 29.]), 'previousTarget': array([ 2., 29.]), 'currentState': array([15.15142529, 20.1870148 ,  0.        ]), 'targetState': array([ 2, 29], dtype=int32), 'currentDistance': 15.831256903995886}
done in step count: 99
reward sum = -0.27132251601413826
running average episode reward sum: -0.5607531510997544
{'scaleFactor': 20, 'currentTarget': array([ 2., 29.]), 'previousTarget': array([ 2., 29.]), 'currentState': array([71.4451955 , 81.95503518,  0.        ]), 'targetState': array([ 2, 29], dtype=int32), 'currentDistance': 87.33195823355027}
episode index:307
target Thresh 13.167344616774315
target distance 5.0
model initialize at round 307
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 18.]), 'previousTarget': array([ 2., 18.]), 'currentState': array([ 2.72512889, 14.39036846,  0.        ]), 'targetState': array([ 2, 18], dtype=int32), 'currentDistance': 3.681745747431759}
done in step count: 3
reward sum = 0.9309780342495322
running average episode reward sum: -0.5559098680304385
{'scaleFactor': 20, 'currentTarget': array([ 2., 18.]), 'previousTarget': array([ 2., 18.]), 'currentState': array([ 1.86399732, 17.63759822,  0.        ]), 'targetState': array([ 2, 18], dtype=int32), 'currentDistance': 0.3870811054693424}
episode index:308
target Thresh 13.186167858967838
target distance 4.0
model initialize at round 308
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 14.]), 'previousTarget': array([26., 14.]), 'currentState': array([24.10758618, 16.27484715,  0.        ]), 'targetState': array([26, 14], dtype=int32), 'currentDistance': 2.959080872713424}
done in step count: 1
reward sum = 0.9592770418574635
running average episode reward sum: -0.5510063505227107
{'scaleFactor': 20, 'currentTarget': array([26., 14.]), 'previousTarget': array([26., 14.]), 'currentState': array([25.00703339, 14.73220432,  0.        ]), 'targetState': array([26, 14], dtype=int32), 'currentDistance': 1.2337365444869293}
episode index:309
target Thresh 13.204972287327656
target distance 10.0
model initialize at round 309
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 24.]), 'previousTarget': array([ 6., 24.]), 'currentState': array([12.29654717, 14.67268831,  0.        ]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 11.253677166542221}
done in step count: 7
reward sum = 0.8077612918858761
running average episode reward sum: -0.5466232290955864
{'scaleFactor': 20, 'currentTarget': array([ 6., 24.]), 'previousTarget': array([ 6., 24.]), 'currentState': array([ 5.79845396, 23.60117882,  0.        ]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 0.44685471600923743}
episode index:310
target Thresh 13.223757920658194
target distance 8.0
model initialize at round 310
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([ 4.5555371 , 10.07227618,  0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 7.519831127678772}
done in step count: 6
reward sum = 0.8833896469734263
running average episode reward sum: -0.5420251169538853
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.44878916, 3.73326522, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.8597032037899719}
episode index:311
target Thresh 13.24252477774509
target distance 11.0
model initialize at round 311
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 26.]), 'previousTarget': array([ 4., 26.]), 'currentState': array([14.21281797, 19.01971382,  0.        ]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 12.370369680009638}
done in step count: 8
reward sum = 0.8162249826883916
running average episode reward sum: -0.5376717512499036
{'scaleFactor': 20, 'currentTarget': array([ 4., 26.]), 'previousTarget': array([ 4., 26.]), 'currentState': array([ 3.97927302, 25.45902323,  0.        ]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 0.5413736903115918}
episode index:312
target Thresh 13.261272877355196
target distance 2.0
model initialize at round 312
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 26.]), 'previousTarget': array([19., 26.]), 'currentState': array([18.95798498, 26.77804351,  0.        ]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 0.7791771061454262}
done in step count: 0
reward sum = 0.9879454351935419
running average episode reward sum: -0.5327975749353878
{'scaleFactor': 20, 'currentTarget': array([19., 26.]), 'previousTarget': array([19., 26.]), 'currentState': array([18.95798498, 26.77804351,  0.        ]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 0.7791771061454262}
episode index:313
target Thresh 13.28000223823662
target distance 7.0
model initialize at round 313
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 22.]), 'previousTarget': array([21., 22.]), 'currentState': array([17.81753528, 16.45969069,  0.        ]), 'targetState': array([21, 22], dtype=int32), 'currentDistance': 6.389296435175683}
done in step count: 3
reward sum = 0.896668398584115
running average episode reward sum: -0.5282451355292748
{'scaleFactor': 20, 'currentTarget': array([21., 22.]), 'previousTarget': array([21., 22.]), 'currentState': array([20.32966495, 21.00173235,  0.        ]), 'targetState': array([21, 22], dtype=int32), 'currentDistance': 1.202450575864421}
episode index:314
target Thresh 13.298712879118721
target distance 12.0
model initialize at round 314
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 25.]), 'previousTarget': array([27., 25.]), 'currentState': array([19.99660826, 14.00683701,  0.        ]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 13.03445926834255}
done in step count: 7
reward sum = 0.7903451403070685
running average episode reward sum: -0.5240591346536039
{'scaleFactor': 20, 'currentTarget': array([27., 25.]), 'previousTarget': array([27., 25.]), 'currentState': array([26.55256665, 24.44595194,  0.        ]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 0.7121557829663805}
episode index:315
target Thresh 13.317404818712145
target distance 9.0
model initialize at round 315
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([23.23424172, 10.13687944,  0.        ]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 8.502292891074843}
done in step count: 5
reward sum = 0.8536666734905053
running average episode reward sum: -0.5196992428556795
{'scaleFactor': 20, 'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([19.79386529, 17.38049686,  0.        ]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 0.6528978950953789}
episode index:316
target Thresh 13.33607807570883
target distance 10.0
model initialize at round 316
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 13.]), 'previousTarget': array([24., 13.]), 'currentState': array([23.45832705,  4.83506227,  0.        ]), 'targetState': array([24, 13], dtype=int32), 'currentDistance': 8.182885663953009}
done in step count: 5
reward sum = 0.8544901999811656
running average episode reward sum: -0.5153642603861627
{'scaleFactor': 20, 'currentTarget': array([24., 13.]), 'previousTarget': array([24., 13.]), 'currentState': array([23.70182907, 12.48959315,  0.        ]), 'targetState': array([24, 13], dtype=int32), 'currentDistance': 0.5911184811913288}
episode index:317
target Thresh 13.354732668782038
target distance 10.0
model initialize at round 317
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([16.6317153 ,  2.02444276,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 11.752277321224513}
done in step count: 7
reward sum = 0.817554995961292
running average episode reward sum: -0.5111726903976486
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([7.80223426, 9.24428952, 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.7811591518660244}
episode index:318
target Thresh 13.373368616586355
target distance 13.0
model initialize at round 318
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 24.]), 'previousTarget': array([17., 24.]), 'currentState': array([ 6.        , 19.37429929,  0.        ]), 'targetState': array([17, 24], dtype=int32), 'currentDistance': 11.933025897855297}
done in step count: 7
reward sum = 0.7715279469202707
running average episode reward sum: -0.5071516852649905
{'scaleFactor': 20, 'currentTarget': array([17., 24.]), 'previousTarget': array([17., 24.]), 'currentState': array([16.37789118, 23.17453402,  0.        ]), 'targetState': array([17, 24], dtype=int32), 'currentDistance': 1.0336408756503939}
episode index:319
target Thresh 13.391985937757742
target distance 4.0
model initialize at round 319
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 14.]), 'previousTarget': array([ 6., 14.]), 'currentState': array([ 3.96627712, 13.23109671,  0.        ]), 'targetState': array([ 6, 14], dtype=int32), 'currentDistance': 2.1742219308451993}
done in step count: 1
reward sum = 0.9608149141997648
running average episode reward sum: -0.5025642896416632
{'scaleFactor': 20, 'currentTarget': array([ 6., 14.]), 'previousTarget': array([ 6., 14.]), 'currentState': array([ 5.35679722, 13.28755065,  0.        ]), 'targetState': array([ 6, 14], dtype=int32), 'currentDistance': 0.9598405537377165}
episode index:320
target Thresh 13.41058465091351
target distance 1.0
model initialize at round 320
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 24.]), 'previousTarget': array([ 3., 24.]), 'currentState': array([ 3.34249538, 24.35422182,  0.        ]), 'targetState': array([ 3, 24], dtype=int32), 'currentDistance': 0.4927232336000812}
done in step count: 0
reward sum = 0.9957532915157855
running average episode reward sum: -0.49789663362559633
{'scaleFactor': 20, 'currentTarget': array([ 3., 24.]), 'previousTarget': array([ 3., 24.]), 'currentState': array([ 3.34249538, 24.35422182,  0.        ]), 'targetState': array([ 3, 24], dtype=int32), 'currentDistance': 0.4927232336000812}
episode index:321
target Thresh 13.429164774652385
target distance 13.0
model initialize at round 321
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 24.]), 'previousTarget': array([26., 24.]), 'currentState': array([14.9999584, 27.0242312,  0.       ]), 'targetState': array([26, 24], dtype=int32), 'currentDistance': 11.408193968117288}
done in step count: 6
reward sum = 0.7773357661624769
running average episode reward sum: -0.49393628455793154
{'scaleFactor': 20, 'currentTarget': array([26., 24.]), 'previousTarget': array([26., 24.]), 'currentState': array([25.37815869, 23.35839495,  0.        ]), 'targetState': array([26, 24], dtype=int32), 'currentDistance': 0.8935007867992536}
episode index:322
target Thresh 13.447726327554484
target distance 11.0
model initialize at round 322
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  5.]), 'previousTarget': array([21.,  5.]), 'currentState': array([26.690678 , 15.7214016,  0.       ]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 12.138050437233138}
done in step count: 34
reward sum = 0.6719692127696458
running average episode reward sum: -0.49032667001512165
{'scaleFactor': 20, 'currentTarget': array([21.,  5.]), 'previousTarget': array([21.,  5.]), 'currentState': array([21.49307871,  5.56373208,  0.        ]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 0.7489462354837662}
episode index:323
target Thresh 13.466269328181369
target distance 6.0
model initialize at round 323
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([8.44429493, 7.89992324, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 4.534492171084972}
done in step count: 3
reward sum = 0.9302501791375204
running average episode reward sum: -0.48594217356711966
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.41322601, 7.40838754, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.5809785867863698}
episode index:324
target Thresh 13.484793795076033
target distance 4.0
model initialize at round 324
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.68023759,  5.54338551,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 2.4773378396497363}
done in step count: 2
reward sum = 0.9572379847971544
running average episode reward sum: -0.4815016192336911
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.69929243,  7.51338714,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.5720289462818057}
episode index:325
target Thresh 13.503299746762952
target distance 5.0
model initialize at round 325
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 27.]), 'previousTarget': array([12., 27.]), 'currentState': array([15.26522458, 25.23337685,  0.        ]), 'targetState': array([12, 27], dtype=int32), 'currentDistance': 3.7124990102271314}
done in step count: 2
reward sum = 0.9449009531132465
running average episode reward sum: -0.477126151220357
{'scaleFactor': 20, 'currentTarget': array([12., 27.]), 'previousTarget': array([12., 27.]), 'currentState': array([12.68032932, 26.29129569,  0.        ]), 'targetState': array([12, 27], dtype=int32), 'currentDistance': 0.9824000118075098}
episode index:326
target Thresh 13.521787201748072
target distance 12.0
model initialize at round 326
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 15.]), 'previousTarget': array([27., 15.]), 'currentState': array([20.93654299,  4.33234382,  0.        ]), 'targetState': array([27, 15], dtype=int32), 'currentDistance': 12.270468588541744}
done in step count: 6
reward sum = 0.807589952150811
running average episode reward sum: -0.4731973557972036
{'scaleFactor': 20, 'currentTarget': array([27., 15.]), 'previousTarget': array([27., 15.]), 'currentState': array([26.39862031, 14.12411171,  0.        ]), 'targetState': array([27, 15], dtype=int32), 'currentDistance': 1.062467800786994}
episode index:327
target Thresh 13.54025617851886
target distance 6.0
model initialize at round 327
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.32156482, 8.41840124, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 4.430087297648227}
done in step count: 3
reward sum = 0.9248542907495132
running average episode reward sum: -0.46893500321626846
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.16517957, 4.21816093, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.27363932296627536}
episode index:328
target Thresh 13.558706695544284
target distance 12.0
model initialize at round 328
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 29.]), 'previousTarget': array([ 2., 29.]), 'currentState': array([13.5231792 , 19.03756461,  0.        ]), 'targetState': array([ 2, 29], dtype=int32), 'currentDistance': 15.232654986600714}
done in step count: 11
reward sum = 0.7596328728061307
running average episode reward sum: -0.46520075435297853
{'scaleFactor': 20, 'currentTarget': array([ 2., 29.]), 'previousTarget': array([ 2., 29.]), 'currentState': array([ 1.85267494, 28.35305428,  0.        ]), 'targetState': array([ 2, 29], dtype=int32), 'currentDistance': 0.6635084291025768}
episode index:329
target Thresh 13.577138771274868
target distance 5.0
model initialize at round 329
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  2.]), 'previousTarget': array([25.,  2.]), 'currentState': array([21.99134505,  0.92233133,  0.        ]), 'targetState': array([25,  2], dtype=int32), 'currentDistance': 3.1958370383952883}
done in step count: 2
reward sum = 0.9344731355577098
running average episode reward sum: -0.4609593183229461
{'scaleFactor': 20, 'currentTarget': array([25.,  2.]), 'previousTarget': array([25.,  2.]), 'currentState': array([24.51252568,  1.44805129,  0.        ]), 'targetState': array([25,  2], dtype=int32), 'currentDistance': 0.7363956781510976}
episode index:330
target Thresh 13.595552424142689
target distance 11.0
model initialize at round 330
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 24.]), 'previousTarget': array([ 6., 24.]), 'currentState': array([12.20597386, 13.68182236,  0.        ]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 12.04071847587776}
done in step count: 7
reward sum = 0.7980901614374347
running average episode reward sum: -0.45715554345962167
{'scaleFactor': 20, 'currentTarget': array([ 6., 24.]), 'previousTarget': array([ 6., 24.]), 'currentState': array([ 5.6491069 , 23.16422814,  0.        ]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 0.9064439111139081}
episode index:331
target Thresh 13.613947672561398
target distance 3.0
model initialize at round 331
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([17.54818034, 20.93222352,  0.        ]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 1.5496631961726253}
done in step count: 1
reward sum = 0.9748021696276009
running average episode reward sum: -0.4528424178177927
{'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([16.61382389, 21.18428706,  0.        ]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 0.6408911691172102}
episode index:332
target Thresh 13.632324534926251
target distance 11.0
model initialize at round 332
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 16.]), 'previousTarget': array([27., 16.]), 'currentState': array([27.06436825, 25.19427025,  0.        ]), 'targetState': array([27, 16], dtype=int32), 'currentDistance': 9.194495568539276}
done in step count: 6
reward sum = 0.8542199357199126
running average episode reward sum: -0.4489173056450068
{'scaleFactor': 20, 'currentTarget': array([27., 16.]), 'previousTarget': array([27., 16.]), 'currentState': array([27.64794094, 16.82152575,  0.        ]), 'targetState': array([27, 16], dtype=int32), 'currentDistance': 1.0462944280397608}
episode index:333
target Thresh 13.650683029614104
target distance 7.0
model initialize at round 333
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([14.28850019,  2.30307007,  0.        ]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 5.554080072362476}
done in step count: 3
reward sum = 0.9203436905702109
running average episode reward sum: -0.44481772182400314
{'scaleFactor': 20, 'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([9.84791279, 3.2276157 , 0.        ]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 1.146967133305404}
episode index:334
target Thresh 13.669023174983462
target distance 5.0
model initialize at round 334
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 23.]), 'previousTarget': array([12., 23.]), 'currentState': array([15.25759578, 18.82750249,  0.        ]), 'targetState': array([12, 23], dtype=int32), 'currentDistance': 5.293549445146627}
done in step count: 4
reward sum = 0.9061902931173854
running average episode reward sum: -0.44078486207790946
{'scaleFactor': 20, 'currentTarget': array([12., 23.]), 'previousTarget': array([12., 23.]), 'currentState': array([11.97435856, 22.62747991,  0.        ]), 'targetState': array([12, 23], dtype=int32), 'currentDistance': 0.37340152695773016}
episode index:335
target Thresh 13.687344989374466
target distance 13.0
model initialize at round 335
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 8.]), 'previousTarget': array([5., 8.]), 'currentState': array([18.01720273,  1.95083182,  0.        ]), 'targetState': array([5, 8], dtype=int32), 'currentDistance': 14.354093586002827}
done in step count: 99
reward sum = -0.2088446329979042
running average episode reward sum: -0.4400945637770761
{'scaleFactor': 20, 'currentTarget': array([5., 8.]), 'previousTarget': array([5., 8.]), 'currentState': array([67.45905523, 60.65822583,  0.        ]), 'targetState': array([5, 8], dtype=int32), 'currentDistance': 81.69468971354682}
episode index:336
target Thresh 13.705648491108935
target distance 10.0
model initialize at round 336
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 14.]), 'previousTarget': array([19., 14.]), 'currentState': array([12.28720009, 22.01351619,  0.        ]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 10.45361776026941}
done in step count: 5
reward sum = 0.815302789538497
running average episode reward sum: -0.4363693490788103
{'scaleFactor': 20, 'currentTarget': array([19., 14.]), 'previousTarget': array([19., 14.]), 'currentState': array([18.620924  , 13.97093523,  0.        ]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 0.3801886083875761}
episode index:337
target Thresh 13.723933698490367
target distance 5.0
model initialize at round 337
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 11.]), 'previousTarget': array([19., 11.]), 'currentState': array([15.63939798, 13.26447654,  0.        ]), 'targetState': array([19, 11], dtype=int32), 'currentDistance': 4.0523449920785515}
done in step count: 2
reward sum = 0.9159881653900884
running average episode reward sum: -0.43236829134369525
{'scaleFactor': 20, 'currentTarget': array([19., 11.]), 'previousTarget': array([19., 11.]), 'currentState': array([18.58517814, 10.9882648 ,  0.        ]), 'targetState': array([19, 11], dtype=int32), 'currentDistance': 0.41498782284237506}
episode index:338
target Thresh 13.742200629803975
target distance 9.0
model initialize at round 338
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([15.66897988, 11.73748899,  0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 8.142917068972684}
done in step count: 5
reward sum = 0.8904082023208345
running average episode reward sum: -0.4284662957871628
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([8.72710991, 9.66784894, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.9872745478023692}
episode index:339
target Thresh 13.76044930331669
target distance 8.0
model initialize at round 339
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 23.]), 'previousTarget': array([ 8., 23.]), 'currentState': array([12.23124993, 15.82830447,  0.        ]), 'targetState': array([ 8, 23], dtype=int32), 'currentDistance': 8.326865721157283}
done in step count: 5
reward sum = 0.8667072468712005
running average episode reward sum: -0.42465696183816765
{'scaleFactor': 20, 'currentTarget': array([ 8., 23.]), 'previousTarget': array([ 8., 23.]), 'currentState': array([ 7.67779103, 22.0398317 ,  0.        ]), 'targetState': array([ 8, 23], dtype=int32), 'currentDistance': 1.012789113850355}
episode index:340
target Thresh 13.77867973727719
target distance 11.0
model initialize at round 340
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 18.]), 'previousTarget': array([21., 18.]), 'currentState': array([12.35151184, 27.00073719,  0.        ]), 'targetState': array([21, 18], dtype=int32), 'currentDistance': 12.48237227011905}
done in step count: 6
reward sum = 0.7749136494737534
running average episode reward sum: -0.4211391594589538
{'scaleFactor': 20, 'currentTarget': array([21., 18.]), 'previousTarget': array([21., 18.]), 'currentState': array([20.38827908, 17.58533829,  0.        ]), 'targetState': array([21, 18], dtype=int32), 'currentDistance': 0.739017464942674}
episode index:341
target Thresh 13.79689194991591
target distance 4.0
model initialize at round 341
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 13.]), 'previousTarget': array([ 8., 13.]), 'currentState': array([ 5.95211852, 11.28859812,  0.        ]), 'targetState': array([ 8, 13], dtype=int32), 'currentDistance': 2.6688415027460213}
done in step count: 2
reward sum = 0.9515554565605403
running average episode reward sum: -0.4171254325115284
{'scaleFactor': 20, 'currentTarget': array([ 8., 13.]), 'previousTarget': array([ 8., 13.]), 'currentState': array([ 7.49652171, 12.35544187,  0.        ]), 'targetState': array([ 8, 13], dtype=int32), 'currentDistance': 0.8178909293276367}
episode index:342
target Thresh 13.81508595944506
target distance 13.0
model initialize at round 342
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'previousTarget': array([7., 3.]), 'currentState': array([19.78110619, 14.00313252,  0.        ]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 16.864922196542224}
done in step count: 99
reward sum = -0.2286734942874056
running average episode reward sum: -0.4165760099511082
{'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'previousTarget': array([7., 3.]), 'currentState': array([60.81135526, 70.37743719,  0.        ]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 86.22865531020688}
episode index:343
target Thresh 13.833261784058656
target distance 12.0
model initialize at round 343
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 27.]), 'previousTarget': array([ 3., 27.]), 'currentState': array([14.23581225, 20.16769877,  0.        ]), 'targetState': array([ 3, 27], dtype=int32), 'currentDistance': 13.15005007337932}
done in step count: 9
reward sum = 0.8069041835453239
running average episode reward sum: -0.4130193814816418
{'scaleFactor': 20, 'currentTarget': array([ 3., 27.]), 'previousTarget': array([ 3., 27.]), 'currentState': array([ 3.10796504, 26.61173402,  0.        ]), 'targetState': array([ 3, 27], dtype=int32), 'currentDistance': 0.4029974221404762}
episode index:344
target Thresh 13.851419441932524
target distance 9.0
model initialize at round 344
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 15.]), 'previousTarget': array([17., 15.]), 'currentState': array([24.8689723 , 23.74099544,  0.        ]), 'targetState': array([17, 15], dtype=int32), 'currentDistance': 11.76119578569867}
done in step count: 11
reward sum = 0.8341651248350067
running average episode reward sum: -0.4094043539271008
{'scaleFactor': 20, 'currentTarget': array([17., 15.]), 'previousTarget': array([17., 15.]), 'currentState': array([17.65170833, 15.80531147,  0.        ]), 'targetState': array([17, 15], dtype=int32), 'currentDistance': 1.035977953038532}
episode index:345
target Thresh 13.86955895122432
target distance 3.0
model initialize at round 345
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 23.]), 'previousTarget': array([17., 23.]), 'currentState': array([16.89034615, 24.34856558,  0.        ]), 'targetState': array([17, 23], dtype=int32), 'currentDistance': 1.3530162921637305}
done in step count: 1
reward sum = 0.9716614383608871
running average episode reward sum: -0.40541283429621067
{'scaleFactor': 20, 'currentTarget': array([17., 23.]), 'previousTarget': array([17., 23.]), 'currentState': array([17.16563592, 23.42010075,  0.        ]), 'targetState': array([17, 23], dtype=int32), 'currentDistance': 0.45157490770494696}
episode index:346
target Thresh 13.887680330073554
target distance 7.0
model initialize at round 346
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 14.]), 'previousTarget': array([ 3., 14.]), 'currentState': array([ 8.63676453, 16.63609347,  0.        ]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 6.222708662228325}
done in step count: 4
reward sum = 0.9143064080491388
running average episode reward sum: -0.4016096088139474
{'scaleFactor': 20, 'currentTarget': array([ 3., 14.]), 'previousTarget': array([ 3., 14.]), 'currentState': array([ 3.63602531, 14.70455477,  0.        ]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 0.9491710137190397}
episode index:347
target Thresh 13.905783596601612
target distance 9.0
model initialize at round 347
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([22.49093795,  8.72119272,  0.        ]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 7.295344784451538}
done in step count: 5
reward sum = 0.8688179411017516
running average episode reward sum: -0.3979589549348793
{'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([21.87742031, 15.61489767,  0.        ]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 0.4041405537234334}
episode index:348
target Thresh 13.92386876891176
target distance 1.0
model initialize at round 348
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 24.]), 'previousTarget': array([18., 24.]), 'currentState': array([17.51062465, 23.70327792,  0.        ]), 'targetState': array([18, 24], dtype=int32), 'currentDistance': 0.5723043170271683}
done in step count: 0
reward sum = 0.9982560922378552
running average episode reward sum: -0.39395833875386865
{'scaleFactor': 20, 'currentTarget': array([18., 24.]), 'previousTarget': array([18., 24.]), 'currentState': array([17.51062465, 23.70327792,  0.        ]), 'targetState': array([18, 24], dtype=int32), 'currentDistance': 0.5723043170271683}
episode index:349
target Thresh 13.941935865089167
target distance 8.0
model initialize at round 349
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([17.3228637 , 16.22148705,  0.        ]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 7.54914968938359}
done in step count: 5
reward sum = 0.8715963259508404
running average episode reward sum: -0.3903424682832838
{'scaleFactor': 20, 'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([14.02364721, 22.60475039,  0.        ]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 0.39595636250151356}
episode index:350
target Thresh 13.959984903200937
target distance 13.0
model initialize at round 350
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([18.86449544, 20.39922345,  0.        ]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 13.662444431789245}
done in step count: 11
reward sum = 0.8126733468189397
running average episode reward sum: -0.38691507279866205
{'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([ 6.87185499, 24.67491165,  0.        ]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 0.9304910324274626}
episode index:351
target Thresh 13.978015901296104
target distance 12.0
model initialize at round 351
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 3.]), 'previousTarget': array([6., 3.]), 'currentState': array([17.53266796,  8.3519733 ,  0.        ]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 12.714009931434221}
done in step count: 11
reward sum = 0.8249961183355596
running average episode reward sum: -0.3834721432783944
{'scaleFactor': 20, 'currentTarget': array([6., 3.]), 'previousTarget': array([6., 3.]), 'currentState': array([6.55686948, 3.75301275, 0.        ]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 0.9365531603561943}
episode index:352
target Thresh 13.99602887740567
target distance 8.0
model initialize at round 352
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 25.]), 'previousTarget': array([11., 25.]), 'currentState': array([17.62533927, 27.00127705,  0.        ]), 'targetState': array([11, 25], dtype=int32), 'currentDistance': 6.920999224694019}
done in step count: 5
reward sum = 0.8987848278031033
running average episode reward sum: -0.379839687269665
{'scaleFactor': 20, 'currentTarget': array([11., 25.]), 'previousTarget': array([11., 25.]), 'currentState': array([11.39219403, 25.47891534,  0.        ]), 'targetState': array([11, 25], dtype=int32), 'currentDistance': 0.6190121657943198}
episode index:353
target Thresh 14.014023849542614
target distance 6.0
model initialize at round 353
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 5.]), 'previousTarget': array([8., 5.]), 'currentState': array([10.92925111,  9.7490921 ,  0.        ]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 5.5798196988050055}
done in step count: 4
reward sum = 0.9242482624083912
running average episode reward sum: -0.3761558230050377
{'scaleFactor': 20, 'currentTarget': array([8., 5.]), 'previousTarget': array([8., 5.]), 'currentState': array([8.51762781, 5.65435839, 0.        ]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 0.8343401292873085}
episode index:354
target Thresh 14.032000835701911
target distance 5.0
model initialize at round 354
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  9.]), 'previousTarget': array([19.,  9.]), 'currentState': array([18.0181134 , 12.13162267,  0.        ]), 'targetState': array([19,  9], dtype=int32), 'currentDistance': 3.2819448279953316}
done in step count: 2
reward sum = 0.9411189601806813
running average episode reward sum: -0.37244518981296526
{'scaleFactor': 20, 'currentTarget': array([19.,  9.]), 'previousTarget': array([19.,  9.]), 'currentState': array([18.86591753,  9.33633924,  0.        ]), 'targetState': array([19,  9], dtype=int32), 'currentDistance': 0.36208036399565535}
episode index:355
target Thresh 14.049959853860546
target distance 5.0
model initialize at round 355
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 28.]), 'previousTarget': array([20., 28.]), 'currentState': array([23.41699028, 27.97815101,  0.        ]), 'targetState': array([20, 28], dtype=int32), 'currentDistance': 3.417060133067191}
done in step count: 2
reward sum = 0.9499224895656176
running average episode reward sum: -0.36873067385965463
{'scaleFactor': 20, 'currentTarget': array([20., 28.]), 'previousTarget': array([20., 28.]), 'currentState': array([20.7718662 , 28.03491932,  0.        ]), 'targetState': array([20, 28], dtype=int32), 'currentDistance': 0.7726556758888492}
episode index:356
target Thresh 14.06790092197754
target distance 2.0
model initialize at round 356
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  5.]), 'previousTarget': array([26.,  5.]), 'currentState': array([25.84353255,  4.02443659,  0.        ]), 'targetState': array([26,  5], dtype=int32), 'currentDistance': 0.9880313885226463}
done in step count: 0
reward sum = 0.9946302379431256
running average episode reward sum: -0.3649117357313556
{'scaleFactor': 20, 'currentTarget': array([26.,  5.]), 'previousTarget': array([26.,  5.]), 'currentState': array([25.84353255,  4.02443659,  0.        ]), 'targetState': array([26,  5], dtype=int32), 'currentDistance': 0.9880313885226463}
episode index:357
target Thresh 14.085824057993957
target distance 12.0
model initialize at round 357
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([13.73045641, 23.10838556,  0.        ]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 10.111978660682322}
done in step count: 6
reward sum = 0.8392740402849396
running average episode reward sum: -0.3615480883123156
{'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([14.28324479, 13.38249338,  0.        ]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 0.4759504110752542}
episode index:358
target Thresh 14.10372927983294
target distance 11.0
model initialize at round 358
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 23.]), 'previousTarget': array([20., 23.]), 'currentState': array([19.58663833, 13.85271239,  0.        ]), 'targetState': array([20, 23], dtype=int32), 'currentDistance': 9.156622654547052}
done in step count: 5
reward sum = 0.849631464398079
running average episode reward sum: -0.35817432911256525
{'scaleFactor': 20, 'currentTarget': array([20., 23.]), 'previousTarget': array([20., 23.]), 'currentState': array([19.76138345, 22.16578591,  0.        ]), 'targetState': array([20, 23], dtype=int32), 'currentDistance': 0.8676698723207207}
episode index:359
target Thresh 14.121616605399716
target distance 13.0
model initialize at round 359
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([14.75134061, 15.0133952 ,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 16.84908191406715}
done in step count: 23
reward sum = 0.7287703118110725
running average episode reward sum: -0.3551550384433329
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.64240813, 4.89305309, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.1001054611695418}
episode index:360
target Thresh 14.139486052581606
target distance 11.0
model initialize at round 360
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 14.]), 'previousTarget': array([27., 14.]), 'currentState': array([19.99765217,  4.28694344,  0.        ]), 'targetState': array([27, 14], dtype=int32), 'currentDistance': 11.973986091172588}
done in step count: 6
reward sum = 0.8037728367059309
running average episode reward sum: -0.3519447119193737
{'scaleFactor': 20, 'currentTarget': array([27., 14.]), 'previousTarget': array([27., 14.]), 'currentState': array([26.61339873, 13.21842235,  0.        ]), 'targetState': array([27, 14], dtype=int32), 'currentDistance': 0.8719656869702093}
episode index:361
target Thresh 14.157337639248059
target distance 11.0
model initialize at round 361
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 4.]), 'previousTarget': array([5., 4.]), 'currentState': array([14.91951442,  7.05959377,  0.        ]), 'targetState': array([5, 4], dtype=int32), 'currentDistance': 10.380649319794765}
done in step count: 7
reward sum = 0.8634588743989434
running average episode reward sum: -0.34858724344888115
{'scaleFactor': 20, 'currentTarget': array([5., 4.]), 'previousTarget': array([5., 4.]), 'currentState': array([5.69818044, 4.73932676, 0.        ]), 'targetState': array([5, 4], dtype=int32), 'currentDistance': 1.0168873984076667}
episode index:362
target Thresh 14.17517138325066
target distance 12.0
model initialize at round 362
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 10.]), 'previousTarget': array([17., 10.]), 'currentState': array([7.        , 2.69720221, 0.        ]), 'targetState': array([17, 10], dtype=int32), 'currentDistance': 12.382683700436173}
done in step count: 7
reward sum = 0.8005776999635432
running average episode reward sum: -0.34542149980311687
{'scaleFactor': 20, 'currentTarget': array([17., 10.]), 'previousTarget': array([17., 10.]), 'currentState': array([16.56754762,  9.33988693,  0.        ]), 'targetState': array([17, 10], dtype=int32), 'currentDistance': 0.7891541813956474}
episode index:363
target Thresh 14.192987302423166
target distance 4.0
model initialize at round 363
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 29.]), 'previousTarget': array([ 2., 29.]), 'currentState': array([ 4.4427017 , 26.68789059,  0.        ]), 'targetState': array([ 2, 29], dtype=int32), 'currentDistance': 3.3634270483498465}
done in step count: 2
reward sum = 0.9501052991681546
running average episode reward sum: -0.3418623602455035
{'scaleFactor': 20, 'currentTarget': array([ 2., 29.]), 'previousTarget': array([ 2., 29.]), 'currentState': array([ 2.47523135, 28.2005657 ,  0.        ]), 'targetState': array([ 2, 29], dtype=int32), 'currentDistance': 0.9300215278738235}
episode index:364
target Thresh 14.210785414581487
target distance 10.0
model initialize at round 364
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([13.99999964,  6.5279358 ,  0.        ]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 8.13430874274646}
done in step count: 4
reward sum = 0.8708500493036653
running average episode reward sum: -0.33853986049331397
{'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([21.14784491,  7.72143671,  0.        ]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 0.8965298675190682}
episode index:365
target Thresh 14.228565737523741
target distance 11.0
model initialize at round 365
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([4.48368406, 4.79678798, 0.        ]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 10.729811417426387}
done in step count: 6
reward sum = 0.8338281877738604
running average episode reward sum: -0.33533666910460586
{'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([ 9.70782432, 13.45704967,  0.        ]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 0.6165725354038298}
episode index:366
target Thresh 14.246328289030249
target distance 12.0
model initialize at round 366
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 11.]), 'previousTarget': array([17., 11.]), 'currentState': array([6.99999928, 4.55015868, 0.        ]), 'targetState': array([17, 11], dtype=int32), 'currentDistance': 11.899599462374098}
done in step count: 6
reward sum = 0.8158927324239641
running average episode reward sum: -0.3321998042503046
{'scaleFactor': 20, 'currentTarget': array([17., 11.]), 'previousTarget': array([17., 11.]), 'currentState': array([16.22276086, 10.13691545,  0.        ]), 'targetState': array([17, 11], dtype=int32), 'currentDistance': 1.1614713234704346}
episode index:367
target Thresh 14.264073086863572
target distance 9.0
model initialize at round 367
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([12.37362123,  9.04959524,  0.        ]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 8.634863913001755}
done in step count: 4
reward sum = 0.8434418561731333
running average episode reward sum: -0.3290051258252409
{'scaleFactor': 20, 'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([19.44434834,  4.87297863,  0.        ]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 0.5699852643247455}
episode index:368
target Thresh 14.2818001487685
target distance 5.0
model initialize at round 368
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 10.]), 'previousTarget': array([23., 10.]), 'currentState': array([25.51976085,  6.16415048,  0.        ]), 'targetState': array([23, 10], dtype=int32), 'currentDistance': 4.589437470301807}
done in step count: 3
reward sum = 0.9272864276862542
running average episode reward sum: -0.3256005416693832
{'scaleFactor': 20, 'currentTarget': array([23., 10.]), 'previousTarget': array([23., 10.]), 'currentState': array([23.21555117,  9.23586166,  0.        ]), 'targetState': array([23, 10], dtype=int32), 'currentDistance': 0.793958254581381}
episode index:369
target Thresh 14.299509492472101
target distance 9.0
model initialize at round 369
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([15.44705164, 14.01528513,  0.        ]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 10.30830996716909}
done in step count: 4
reward sum = 0.8190672870604332
running average episode reward sum: -0.32250684483497827
{'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([22.42634547,  7.59351897,  0.        ]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 0.8254358201433243}
episode index:370
target Thresh 14.317201135683721
target distance 12.0
model initialize at round 370
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 8.]), 'previousTarget': array([5., 8.]), 'currentState': array([16.18155515,  4.33314866,  0.        ]), 'targetState': array([5, 8], dtype=int32), 'currentDistance': 11.767454031274998}
done in step count: 8
reward sum = 0.8379293429368551
running average episode reward sum: -0.3193789844905798
{'scaleFactor': 20, 'currentTarget': array([5., 8.]), 'previousTarget': array([5., 8.]), 'currentState': array([5.47164142, 7.82206219, 0.        ]), 'targetState': array([5, 8], dtype=int32), 'currentDistance': 0.5040907586769295}
episode index:371
target Thresh 14.334875096095
target distance 14.0
model initialize at round 371
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.4394542 , 23.17466652,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 12.187564012765435}
done in step count: 7
reward sum = 0.8002327458456415
running average episode reward sum: -0.31636927553806304
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.338269  , 11.67315638,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.7533693821662905}
episode index:372
target Thresh 14.352531391379905
target distance 5.0
model initialize at round 372
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 12.]), 'previousTarget': array([26., 12.]), 'currentState': array([22.9899013 , 12.64311039,  0.        ]), 'targetState': array([26, 12], dtype=int32), 'currentDistance': 3.0780326732623995}
done in step count: 2
reward sum = 0.9375919642432174
running average episode reward sum: -0.3130074491579524
{'scaleFactor': 20, 'currentTarget': array([26., 12.]), 'previousTarget': array([26., 12.]), 'currentState': array([25.5086785 , 11.63293263,  0.        ]), 'targetState': array([26, 12], dtype=int32), 'currentDistance': 0.6132986810721056}
episode index:373
target Thresh 14.370170039194733
target distance 2.0
model initialize at round 373
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([15.85994911, 14.72492552,  0.        ]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 0.738330046758424}
done in step count: 0
reward sum = 0.9881733632170494
running average episode reward sum: -0.30952835607673584
{'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([15.85994911, 14.72492552,  0.        ]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 0.738330046758424}
episode index:374
target Thresh 14.387791057178124
target distance 9.0
model initialize at round 374
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([22.91050851, 14.51131359,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 8.65479451232809}
done in step count: 6
reward sum = 0.884614832372936
running average episode reward sum: -0.30634397424087007
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.67030299, 11.65880948,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.9398595796561048}
episode index:375
target Thresh 14.405394462951111
target distance 8.0
model initialize at round 375
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 14.]), 'previousTarget': array([19., 14.]), 'currentState': array([13.3799389 , 20.03801513,  0.        ]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 8.248800727288824}
done in step count: 4
reward sum = 0.8611625451678575
running average episode reward sum: -0.3032389037105277
{'scaleFactor': 20, 'currentTarget': array([19., 14.]), 'previousTarget': array([19., 14.]), 'currentState': array([18.47381324, 14.06140971,  0.        ]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 0.5297581179837164}
episode index:376
target Thresh 14.422980274117094
target distance 13.0
model initialize at round 376
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([26.11433471, 23.19991614,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 16.629418291324765}
done in step count: 99
reward sum = -0.2939399740753865
running average episode reward sum: -0.3032142381146785
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([77.80967311, 97.50331696,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 107.13200347681111}
episode index:377
target Thresh 14.440548508261884
target distance 8.0
model initialize at round 377
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 21.]), 'previousTarget': array([ 3., 21.]), 'currentState': array([10.04357338, 27.06752223,  0.        ]), 'targetState': array([ 3, 21], dtype=int32), 'currentDistance': 9.296598944616655}
done in step count: 6
reward sum = 0.8781489633072064
running average episode reward sum: -0.3000889386400174
{'scaleFactor': 20, 'currentTarget': array([ 3., 21.]), 'previousTarget': array([ 3., 21.]), 'currentState': array([ 3.70059007, 21.92957556,  0.        ]), 'targetState': array([ 3, 21], dtype=int32), 'currentDistance': 1.1640176881082007}
episode index:378
target Thresh 14.45809918295372
target distance 12.0
model initialize at round 378
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([20.64702579, 18.90161681,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 12.277383655568501}
done in step count: 9
reward sum = 0.8356289316632504
running average episode reward sum: -0.2970923215679771
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.58916835,  8.80858266,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.0004625301080785}
episode index:379
target Thresh 14.475632315743276
target distance 5.0
model initialize at round 379
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 21.]), 'previousTarget': array([ 3., 21.]), 'currentState': array([ 6.33553338, 17.01798224,  0.        ]), 'targetState': array([ 3, 21], dtype=int32), 'currentDistance': 5.194443987345448}
done in step count: 4
reward sum = 0.9105279860620131
running average episode reward sum: -0.2939143733900035
{'scaleFactor': 20, 'currentTarget': array([ 3., 21.]), 'previousTarget': array([ 3., 21.]), 'currentState': array([ 3.0223881 , 20.44907957,  0.        ]), 'targetState': array([ 3, 21], dtype=int32), 'currentDistance': 0.5513751388708097}
episode index:380
target Thresh 14.49314792416369
target distance 9.0
model initialize at round 380
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 14.]), 'previousTarget': array([15., 14.]), 'currentState': array([22.80036569, 17.32883674,  0.        ]), 'targetState': array([15, 14], dtype=int32), 'currentDistance': 8.48097039713994}
done in step count: 6
reward sum = 0.8815504225333297
running average episode reward sum: -0.2908291639518845
{'scaleFactor': 20, 'currentTarget': array([15., 14.]), 'previousTarget': array([15., 14.]), 'currentState': array([15.55504543, 14.75372672,  0.        ]), 'targetState': array([15, 14], dtype=int32), 'currentDistance': 0.9360445472661578}
episode index:381
target Thresh 14.51064602573057
target distance 4.0
model initialize at round 381
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 26.]), 'previousTarget': array([27., 26.]), 'currentState': array([24.90228641, 26.70838109,  0.        ]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 2.2140926060383386}
done in step count: 1
reward sum = 0.9604273457618684
running average episode reward sum: -0.28755362335053963
{'scaleFactor': 20, 'currentTarget': array([27., 26.]), 'previousTarget': array([27., 26.]), 'currentState': array([26.16471779, 25.90875986,  0.        ]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 0.8402506336223083}
episode index:382
target Thresh 14.528126637942012
target distance 6.0
model initialize at round 382
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 21.]), 'previousTarget': array([ 4., 21.]), 'currentState': array([ 8.65898502, 23.41568637,  0.        ]), 'targetState': array([ 4, 21], dtype=int32), 'currentDistance': 5.248016962419877}
done in step count: 3
reward sum = 0.9331161333188454
running average episode reward sum: -0.2843664960485308
{'scaleFactor': 20, 'currentTarget': array([ 4., 21.]), 'previousTarget': array([ 4., 21.]), 'currentState': array([ 4.92404938, 21.95810142,  0.        ]), 'targetState': array([ 4, 21], dtype=int32), 'currentDistance': 1.3310993899825152}
episode index:383
target Thresh 14.545589778278643
target distance 11.0
model initialize at round 383
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 16.]), 'previousTarget': array([15., 16.]), 'currentState': array([ 5.96490681, 21.20909715,  0.        ]), 'targetState': array([15, 16], dtype=int32), 'currentDistance': 10.429170724835624}
done in step count: 5
reward sum = 0.8137718719684353
running average episode reward sum: -0.2815067607151533
{'scaleFactor': 20, 'currentTarget': array([15., 16.]), 'previousTarget': array([15., 16.]), 'currentState': array([14.60331619, 16.19163501,  0.        ]), 'targetState': array([15, 16], dtype=int32), 'currentDistance': 0.44054741501855366}
episode index:384
target Thresh 14.563035464203594
target distance 7.0
model initialize at round 384
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 18.]), 'previousTarget': array([11., 18.]), 'currentState': array([ 5.90161812, 23.31101084,  0.        ]), 'targetState': array([11, 18], dtype=int32), 'currentDistance': 7.3620876033766445}
done in step count: 3
reward sum = 0.8639663613278581
running average episode reward sum: -0.2785315058527039
{'scaleFactor': 20, 'currentTarget': array([11., 18.]), 'previousTarget': array([11., 18.]), 'currentState': array([10.7082901 , 18.66221654,  0.        ]), 'targetState': array([11, 18], dtype=int32), 'currentDistance': 0.7236196632546794}
episode index:385
target Thresh 14.580463713162555
target distance 7.0
model initialize at round 385
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 27.]), 'previousTarget': array([11., 27.]), 'currentState': array([16.26765001, 25.22183068,  0.        ]), 'targetState': array([11, 27], dtype=int32), 'currentDistance': 5.5596782942952805}
done in step count: 4
reward sum = 0.9105655085613211
running average episode reward sum: -0.2754509436391961
{'scaleFactor': 20, 'currentTarget': array([11., 27.]), 'previousTarget': array([11., 27.]), 'currentState': array([11.41538066, 26.78233597,  0.        ]), 'targetState': array([11, 27], dtype=int32), 'currentDistance': 0.46895492460804594}
episode index:386
target Thresh 14.597874542583781
target distance 9.0
model initialize at round 386
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 29.]), 'previousTarget': array([ 7., 29.]), 'currentState': array([14.20165217, 22.59781915,  0.        ]), 'targetState': array([ 7, 29], dtype=int32), 'currentDistance': 9.635959402629549}
done in step count: 6
reward sum = 0.8471925569010061
running average episode reward sum: -0.272550056040901
{'scaleFactor': 20, 'currentTarget': array([ 7., 29.]), 'previousTarget': array([ 7., 29.]), 'currentState': array([ 7.1153447 , 28.51360309,  0.        ]), 'targetState': array([ 7, 29], dtype=int32), 'currentDistance': 0.49988634048862024}
episode index:387
target Thresh 14.615267969878097
target distance 12.0
model initialize at round 387
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 19.]), 'previousTarget': array([ 9., 19.]), 'currentState': array([13.1833545 ,  8.24889719,  0.        ]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 11.536319446592985}
done in step count: 7
reward sum = 0.7979790629837173
running average episode reward sum: -0.26979096037331174
{'scaleFactor': 20, 'currentTarget': array([ 9., 19.]), 'previousTarget': array([ 9., 19.]), 'currentState': array([ 8.76164508, 18.35884351,  0.        ]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 0.6840283028629967}
episode index:388
target Thresh 14.632644012438934
target distance 4.0
model initialize at round 388
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 7.]), 'previousTarget': array([5., 7.]), 'currentState': array([5.32034981, 4.35721302, 0.        ]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 2.6621320444927257}
done in step count: 2
reward sum = 0.9579801624866892
running average episode reward sum: -0.2666347364070906
{'scaleFactor': 20, 'currentTarget': array([5., 7.]), 'previousTarget': array([5., 7.]), 'currentState': array([4.83235411, 6.33644617, 0.        ]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 0.6844039989939571}
episode index:389
target Thresh 14.650002687642338
target distance 11.0
model initialize at round 389
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 15.]), 'previousTarget': array([18., 15.]), 'currentState': array([ 8.9998492 , 18.03717411,  0.        ]), 'targetState': array([18, 15], dtype=int32), 'currentDistance': 9.498796816768937}
done in step count: 5
reward sum = 0.8169833364597585
running average episode reward sum: -0.2638562285279449
{'scaleFactor': 20, 'currentTarget': array([18., 15.]), 'previousTarget': array([18., 15.]), 'currentState': array([17.25918663, 14.54524013,  0.        ]), 'targetState': array([18, 15], dtype=int32), 'currentDistance': 0.8692588748989818}
episode index:390
target Thresh 14.667344012846986
target distance 7.0
model initialize at round 390
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([21.89506078,  7.15045714,  0.        ]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 6.579262798074593}
done in step count: 3
reward sum = 0.8733736077608043
running average episode reward sum: -0.26094771232260283
{'scaleFactor': 20, 'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([26.49038541,  3.29568541,  0.        ]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 0.589183238977859}
episode index:391
target Thresh 14.684668005394197
target distance 8.0
model initialize at round 391
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([21.90967262, 12.00953466,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 8.534577503686114}
done in step count: 6
reward sum = 0.8790505166715994
running average episode reward sum: -0.2580395535751686
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.51215589,  7.71680382,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.8809718332535904}
episode index:392
target Thresh 14.701974682607972
target distance 2.0
model initialize at round 392
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 26.]), 'previousTarget': array([22., 26.]), 'currentState': array([22.37892967, 26.79765248,  0.        ]), 'targetState': array([22, 26], dtype=int32), 'currentDistance': 0.883083904239264}
done in step count: 0
reward sum = 0.9920538630033318
running average episode reward sum: -0.2548586542963429
{'scaleFactor': 20, 'currentTarget': array([22., 26.]), 'previousTarget': array([22., 26.]), 'currentState': array([22.37892967, 26.79765248,  0.        ]), 'targetState': array([22, 26], dtype=int32), 'currentDistance': 0.883083904239264}
episode index:393
target Thresh 14.719264061794991
target distance 3.0
model initialize at round 393
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([8.78928435, 6.65205601, 0.        ]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 1.259721163584854}
done in step count: 1
reward sum = 0.9708530097081546
running average episode reward sum: -0.25174771098668686
{'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([9.50017989, 6.57118054, 0.        ]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 0.6585637988037478}
episode index:394
target Thresh 14.73653616024463
target distance 7.0
model initialize at round 394
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  9.]), 'previousTarget': array([20.,  9.]), 'currentState': array([24.28653979,  3.02394414,  0.        ]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 7.354431796212845}
done in step count: 5
reward sum = 0.876164557310732
running average episode reward sum: -0.24889223688973136
{'scaleFactor': 20, 'currentTarget': array([20.,  9.]), 'previousTarget': array([20.,  9.]), 'currentState': array([19.91478294,  8.65648502,  0.        ]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 0.35392723518436947}
episode index:395
target Thresh 14.753790995228986
target distance 12.0
model initialize at round 395
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 29.]), 'previousTarget': array([15., 29.]), 'currentState': array([ 4.99999416, 22.03027822,  0.        ]), 'targetState': array([15, 29], dtype=int32), 'currentDistance': 12.189222228302167}
done in step count: 9
reward sum = 0.8026615324039245
running average episode reward sum: -0.24623679807838372
{'scaleFactor': 20, 'currentTarget': array([15., 29.]), 'previousTarget': array([15., 29.]), 'currentState': array([14.70262137, 28.5214899 ,  0.        ]), 'targetState': array([15, 29], dtype=int32), 'currentDistance': 0.5633879335307731}
episode index:396
target Thresh 14.77102858400291
target distance 10.0
model initialize at round 396
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 28.]), 'previousTarget': array([14., 28.]), 'currentState': array([22.45523167, 27.26695529,  0.        ]), 'targetState': array([14, 28], dtype=int32), 'currentDistance': 8.486948632318676}
done in step count: 6
reward sum = 0.8711981767358888
running average episode reward sum: -0.24342210040882636
{'scaleFactor': 20, 'currentTarget': array([14., 28.]), 'previousTarget': array([14., 28.]), 'currentState': array([14.36908048, 28.13789688,  0.        ]), 'targetState': array([14, 28], dtype=int32), 'currentDistance': 0.39399994198807775}
episode index:397
target Thresh 14.788248943803975
target distance 9.0
model initialize at round 397
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  4.]), 'previousTarget': array([25.,  4.]), 'currentState': array([21.33164433, 11.023628  ,  0.        ]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 7.923899518552712}
done in step count: 4
reward sum = 0.8624063891328848
running average episode reward sum: -0.24064363686726428
{'scaleFactor': 20, 'currentTarget': array([25.,  4.]), 'previousTarget': array([25.,  4.]), 'currentState': array([25.01111594,  4.33680248,  0.        ]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 0.3369858696976891}
episode index:398
target Thresh 14.805452091852548
target distance 4.0
model initialize at round 398
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 13.]), 'previousTarget': array([ 5., 13.]), 'currentState': array([ 8.05577689, 16.02360451,  0.        ]), 'targetState': array([ 5, 13], dtype=int32), 'currentDistance': 4.298832012623387}
done in step count: 3
reward sum = 0.9415581191435675
running average episode reward sum: -0.2376807251980642
{'scaleFactor': 20, 'currentTarget': array([ 5., 13.]), 'previousTarget': array([ 5., 13.]), 'currentState': array([ 5.62490234, 13.77226979,  0.        ]), 'targetState': array([ 5, 13], dtype=int32), 'currentDistance': 0.9934301954455468}
episode index:399
target Thresh 14.822638045351784
target distance 3.0
model initialize at round 399
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 16.]), 'previousTarget': array([19., 16.]), 'currentState': array([20.29151082, 17.75986171,  0.        ]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 2.1829139766296746}
done in step count: 1
reward sum = 0.9754956788248189
running average episode reward sum: -0.23464778418800702
{'scaleFactor': 20, 'currentTarget': array([19., 16.]), 'previousTarget': array([19., 16.]), 'currentState': array([19.75933033, 16.99406487,  0.        ]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 1.2508986826716577}
episode index:400
target Thresh 14.839806821487631
target distance 11.0
model initialize at round 400
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 13.]), 'previousTarget': array([17., 13.]), 'currentState': array([7.99969447, 2.31836253, 0.        ]), 'targetState': array([17, 13], dtype=int32), 'currentDistance': 13.967923204570642}
done in step count: 9
reward sum = 0.7878780584215946
running average episode reward sum: -0.23209784443087583
{'scaleFactor': 20, 'currentTarget': array([17., 13.]), 'previousTarget': array([17., 13.]), 'currentState': array([16.42439646, 12.33653083,  0.        ]), 'targetState': array([17, 13], dtype=int32), 'currentDistance': 0.8783568604723555}
episode index:401
target Thresh 14.856958437428876
target distance 7.0
model initialize at round 401
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 27.]), 'previousTarget': array([14., 27.]), 'currentState': array([ 8.99999213, 27.02343512,  0.        ]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 5.000062787890385}
done in step count: 3
reward sum = 0.903807118781388
running average episode reward sum: -0.2292722101940294
{'scaleFactor': 20, 'currentTarget': array([14., 27.]), 'previousTarget': array([14., 27.]), 'currentState': array([13.30479103, 26.77462411,  0.        ]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 0.7308281600674833}
episode index:402
target Thresh 14.874092910327125
target distance 14.0
model initialize at round 402
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 13.]), 'previousTarget': array([11., 13.]), 'currentState': array([24.63227352,  3.49713162,  0.        ]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 16.61756266684025}
done in step count: 11
reward sum = 0.7713449697417214
running average episode reward sum: -0.22678928915200522
{'scaleFactor': 20, 'currentTarget': array([11., 13.]), 'previousTarget': array([11., 13.]), 'currentState': array([11.12491998, 12.02117983,  0.        ]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 0.9867593102922461}
episode index:403
target Thresh 14.891210257316857
target distance 7.0
model initialize at round 403
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 16.]), 'previousTarget': array([20., 16.]), 'currentState': array([17.97154033, 21.04961157,  0.        ]), 'targetState': array([20, 16], dtype=int32), 'currentDistance': 5.441803526183391}
done in step count: 3
reward sum = 0.8831269472961155
running average episode reward sum: -0.2240419717350544
{'scaleFactor': 20, 'currentTarget': array([20., 16.]), 'previousTarget': array([20., 16.]), 'currentState': array([20.27148809, 16.57034886,  0.        ]), 'targetState': array([20, 16], dtype=int32), 'currentDistance': 0.6316673185151711}
episode index:404
target Thresh 14.908310495515419
target distance 11.0
model initialize at round 404
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  8.]), 'previousTarget': array([10.,  8.]), 'currentState': array([20.23292476, 13.5661315 ,  0.        ]), 'targetState': array([10,  8], dtype=int32), 'currentDistance': 11.648801184379069}
done in step count: 8
reward sum = 0.8537386327915626
running average episode reward sum: -0.22138078505721093
{'scaleFactor': 20, 'currentTarget': array([10.,  8.]), 'previousTarget': array([10.,  8.]), 'currentState': array([10.94792658,  8.97939253,  0.        ]), 'targetState': array([10,  8], dtype=int32), 'currentDistance': 1.3630020277810153}
episode index:405
target Thresh 14.925393642023057
target distance 12.0
model initialize at round 405
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([24.82087576,  7.37534687,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 10.838890341330776}
done in step count: 8
reward sum = 0.845408957110381
running average episode reward sum: -0.21875322411591144
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.57548919,  8.46447768,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.7395453526230666}
episode index:406
target Thresh 14.942459713922908
target distance 9.0
model initialize at round 406
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([9.53257346, 4.0171841 , 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 7.600942470341158}
done in step count: 5
reward sum = 0.897205002893869
running average episode reward sum: -0.2160113120102363
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.99761736, 3.30927729, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.044458159630976}
episode index:407
target Thresh 14.959508728281055
target distance 6.0
model initialize at round 407
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  7.]), 'previousTarget': array([27.,  7.]), 'currentState': array([22.99141538,  2.98796475,  0.        ]), 'targetState': array([27,  7], dtype=int32), 'currentDistance': 5.671435223714643}
done in step count: 3
reward sum = 0.9105992328769881
running average episode reward sum: -0.21325001165512056
{'scaleFactor': 20, 'currentTarget': array([27.,  7.]), 'previousTarget': array([27.,  7.]), 'currentState': array([26.19251937,  6.0394085 ,  0.        ]), 'targetState': array([27,  7], dtype=int32), 'currentDistance': 1.2548948136895812}
episode index:408
target Thresh 14.976540702146512
target distance 10.0
model initialize at round 408
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 10.]), 'previousTarget': array([23., 10.]), 'currentState': array([24.89615029, 18.05995834,  0.        ]), 'targetState': array([23, 10], dtype=int32), 'currentDistance': 8.279994827783092}
done in step count: 5
reward sum = 0.8684213428277395
running average episode reward sum: -0.21060533841677614
{'scaleFactor': 20, 'currentTarget': array([23., 10.]), 'previousTarget': array([23., 10.]), 'currentState': array([23.71146476, 10.72590089,  0.        ]), 'targetState': array([23, 10], dtype=int32), 'currentDistance': 1.0164222536855325}
episode index:409
target Thresh 14.993555652551251
target distance 8.0
model initialize at round 409
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 20.]), 'previousTarget': array([25., 20.]), 'currentState': array([20.99919093, 26.03309727,  0.        ]), 'targetState': array([25, 20], dtype=int32), 'currentDistance': 7.239111539207571}
done in step count: 3
reward sum = 0.8529209772990559
running average episode reward sum: -0.208011371793079
{'scaleFactor': 20, 'currentTarget': array([25., 20.]), 'previousTarget': array([25., 20.]), 'currentState': array([25.23802805, 20.93629968,  0.        ]), 'targetState': array([25, 20], dtype=int32), 'currentDistance': 0.9660820080183748}
episode index:410
target Thresh 15.010553596510224
target distance 3.0
model initialize at round 410
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 12.]), 'previousTarget': array([17., 12.]), 'currentState': array([15.85905337, 11.91734073,  0.        ]), 'targetState': array([17, 12], dtype=int32), 'currentDistance': 1.1439369565358444}
done in step count: 1
reward sum = 0.9710979900411539
running average episode reward sum: -0.20514249256720493
{'scaleFactor': 20, 'currentTarget': array([17., 12.]), 'previousTarget': array([17., 12.]), 'currentState': array([16.4174419 , 11.82300594,  0.        ]), 'targetState': array([17, 12], dtype=int32), 'currentDistance': 0.6088520594250967}
episode index:411
target Thresh 15.027534551021379
target distance 11.0
model initialize at round 411
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 28.]), 'previousTarget': array([ 4., 28.]), 'currentState': array([ 4.34635878, 18.76799607,  0.        ]), 'targetState': array([ 4, 28], dtype=int32), 'currentDistance': 9.238498845277004}
done in step count: 6
reward sum = 0.8329569951370193
running average episode reward sum: -0.20262283361646652
{'scaleFactor': 20, 'currentTarget': array([ 4., 28.]), 'previousTarget': array([ 4., 28.]), 'currentState': array([ 3.65489011, 27.50999767,  0.        ]), 'targetState': array([ 4, 28], dtype=int32), 'currentDistance': 0.5993355688482129}
episode index:412
target Thresh 15.044498533065667
target distance 3.0
model initialize at round 412
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 24.]), 'previousTarget': array([ 4., 24.]), 'currentState': array([ 3.72254613, 22.29490066,  0.        ]), 'targetState': array([ 4, 24], dtype=int32), 'currentDistance': 1.7275255203671744}
done in step count: 1
reward sum = 0.9767601740415773
running average episode reward sum: -0.19976718468751242
{'scaleFactor': 20, 'currentTarget': array([ 4., 24.]), 'previousTarget': array([ 4., 24.]), 'currentState': array([ 3.6457855 , 23.24218994,  0.        ]), 'targetState': array([ 4, 24], dtype=int32), 'currentDistance': 0.8365070211256193}
episode index:413
target Thresh 15.061445559607083
target distance 13.0
model initialize at round 413
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 24.]), 'previousTarget': array([19., 24.]), 'currentState': array([17.15688783, 12.95548129,  0.        ]), 'targetState': array([19, 24], dtype=int32), 'currentDistance': 11.197252162528743}
done in step count: 6
reward sum = 0.8263515019842529
running average episode reward sum: -0.19728863713516517
{'scaleFactor': 20, 'currentTarget': array([19., 24.]), 'previousTarget': array([19., 24.]), 'currentState': array([18.479685  , 23.16799831,  0.        ]), 'targetState': array([19, 24], dtype=int32), 'currentDistance': 0.9813024507747905}
episode index:414
target Thresh 15.07837564759264
target distance 6.0
model initialize at round 414
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([20.84118152, 20.09027684,  0.        ]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 4.251261563761327}
done in step count: 2
reward sum = 0.9143323856441891
running average episode reward sum: -0.19461003226099807
{'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([22.29692578, 16.92728579,  0.        ]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 0.9736651672220079}
episode index:415
target Thresh 15.09528881395244
target distance 6.0
model initialize at round 415
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 12.]), 'previousTarget': array([27., 12.]), 'currentState': array([24.96733129, 16.11553597,  0.        ]), 'targetState': array([27, 12], dtype=int32), 'currentDistance': 4.590139261617495}
done in step count: 2
reward sum = 0.9009965294737341
running average episode reward sum: -0.19197636264144344
{'scaleFactor': 20, 'currentTarget': array([27., 12.]), 'previousTarget': array([27., 12.]), 'currentState': array([27.30342412, 12.88912714,  0.        ]), 'targetState': array([27, 12], dtype=int32), 'currentDistance': 0.9394749912864264}
episode index:416
target Thresh 15.11218507559964
target distance 13.0
model initialize at round 416
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 19.]), 'previousTarget': array([14., 19.]), 'currentState': array([27.11418805, 25.90213576,  0.        ]), 'targetState': array([14, 19], dtype=int32), 'currentDistance': 14.819629088670899}
done in step count: 99
reward sum = -0.24294367139748108
running average episode reward sum: -0.19209858640344832
{'scaleFactor': 20, 'currentTarget': array([14., 19.]), 'previousTarget': array([14., 19.]), 'currentState': array([75.19953288, 93.14205334,  0.        ]), 'targetState': array([14, 19], dtype=int32), 'currentDistance': 96.1375415657992}
episode index:417
target Thresh 15.129064449430508
target distance 9.0
model initialize at round 417
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 28.]), 'previousTarget': array([18., 28.]), 'currentState': array([22.20132232, 19.98046088,  0.        ]), 'targetState': array([18, 28], dtype=int32), 'currentDistance': 9.053403607179522}
done in step count: 6
reward sum = 0.8450854911989955
running average episode reward sum: -0.18961728478238984
{'scaleFactor': 20, 'currentTarget': array([18., 28.]), 'previousTarget': array([18., 28.]), 'currentState': array([18.03260959, 27.54972088,  0.        ]), 'targetState': array([18, 28], dtype=int32), 'currentDistance': 0.4514583792494111}
episode index:418
target Thresh 15.145926952324423
target distance 7.0
model initialize at round 418
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([17.99996221,  3.35457155,  0.        ]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 5.012594027019626}
done in step count: 3
reward sum = 0.9099271805476914
running average episode reward sum: -0.18699307364795048
{'scaleFactor': 20, 'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([22.5256539 ,  2.55453722,  0.        ]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 0.6507236821989034}
episode index:419
target Thresh 15.162772601143885
target distance 1.0
model initialize at round 419
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  3.]), 'previousTarget': array([10.,  3.]), 'currentState': array([9.67395335, 2.75992835, 0.        ]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 0.4048960537493794}
done in step count: 0
reward sum = 0.9974407623237106
running average episode reward sum: -0.1841729930861132
{'scaleFactor': 20, 'currentTarget': array([10.,  3.]), 'previousTarget': array([10.,  3.]), 'currentState': array([9.67395335, 2.75992835, 0.        ]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 0.4048960537493794}
episode index:420
target Thresh 15.179601412734545
target distance 5.0
model initialize at round 420
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 19.]), 'previousTarget': array([18., 19.]), 'currentState': array([14.93983543, 16.32079634,  0.        ]), 'targetState': array([18, 19], dtype=int32), 'currentDistance': 4.067276662389176}
done in step count: 3
reward sum = 0.9311509602593151
running average episode reward sum: -0.181523767543725
{'scaleFactor': 20, 'currentTarget': array([18., 19.]), 'previousTarget': array([18., 19.]), 'currentState': array([17.49041891, 18.44078717,  0.        ]), 'targetState': array([18, 19], dtype=int32), 'currentDistance': 0.7565658461745793}
episode index:421
target Thresh 15.196413403925217
target distance 9.0
model initialize at round 421
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 26.]), 'previousTarget': array([18., 26.]), 'currentState': array([14.76223999, 18.97971678,  0.        ]), 'targetState': array([18, 26], dtype=int32), 'currentDistance': 7.730942140857201}
done in step count: 4
reward sum = 0.880520819908166
running average episode reward sum: -0.17900707420853096
{'scaleFactor': 20, 'currentTarget': array([18., 26.]), 'previousTarget': array([18., 26.]), 'currentState': array([17.03517792, 25.23903853,  0.        ]), 'targetState': array([18, 26], dtype=int32), 'currentDistance': 1.2287977932072076}
episode index:422
target Thresh 15.213208591527891
target distance 10.0
model initialize at round 422
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 11.]), 'previousTarget': array([20., 11.]), 'currentState': array([12.        , 11.54296654,  0.        ]), 'targetState': array([20, 11], dtype=int32), 'currentDistance': 8.01840462115661}
done in step count: 4
reward sum = 0.8676762742972431
running average episode reward sum: -0.17653264548865916
{'scaleFactor': 20, 'currentTarget': array([20., 11.]), 'previousTarget': array([20., 11.]), 'currentState': array([19.50413406, 11.1506778 ,  0.        ]), 'targetState': array([20, 11], dtype=int32), 'currentDistance': 0.5182536346570206}
episode index:423
target Thresh 15.22998699233776
target distance 7.0
model initialize at round 423
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([20.53181195,  7.74797365,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 5.582150845414531}
done in step count: 4
reward sum = 0.9188970327911321
running average episode reward sum: -0.17394908492667852
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.6420815 ,  7.29158296,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.7051874022176414}
episode index:424
target Thresh 15.246748623133218
target distance 6.0
model initialize at round 424
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 22.]), 'previousTarget': array([25., 22.]), 'currentState': array([20.99918747, 25.4685936 ,  0.        ]), 'targetState': array([25, 22], dtype=int32), 'currentDistance': 5.295058304528403}
done in step count: 2
reward sum = 0.8968388418366159
running average episode reward sum: -0.17142958392252958
{'scaleFactor': 20, 'currentTarget': array([25., 22.]), 'previousTarget': array([25., 22.]), 'currentState': array([24.56496906, 22.63191783,  0.        ]), 'targetState': array([25, 22], dtype=int32), 'currentDistance': 0.7671845055788346}
episode index:425
target Thresh 15.26349350067591
target distance 8.0
model initialize at round 425
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 11.]), 'previousTarget': array([20., 11.]), 'currentState': array([13.99999845, 10.3562845 ,  0.        ]), 'targetState': array([20, 11], dtype=int32), 'currentDistance': 6.03443354778749}
done in step count: 3
reward sum = 0.8858972172888846
running average episode reward sum: -0.16894759612625865
{'scaleFactor': 20, 'currentTarget': array([20., 11.]), 'previousTarget': array([20., 11.]), 'currentState': array([19.53217876, 11.12023435,  0.        ]), 'targetState': array([20, 11], dtype=int32), 'currentDistance': 0.48302485634893516}
episode index:426
target Thresh 15.280221641710703
target distance 13.0
model initialize at round 426
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 24.]), 'previousTarget': array([ 8., 24.]), 'currentState': array([20.02829468, 23.43869421,  0.        ]), 'targetState': array([ 8, 24], dtype=int32), 'currentDistance': 12.04138435398051}
done in step count: 8
reward sum = 0.8381121865603088
running average episode reward sum: -0.16658914230263672
{'scaleFactor': 20, 'currentTarget': array([ 8., 24.]), 'previousTarget': array([ 8., 24.]), 'currentState': array([ 8.80627489, 24.16205174,  0.        ]), 'targetState': array([ 8, 24], dtype=int32), 'currentDistance': 0.8223989103072528}
episode index:427
target Thresh 15.29693306296575
target distance 14.0
model initialize at round 427
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([11.36440152, 23.122082  ,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 12.883822123710367}
done in step count: 7
reward sum = 0.7962555520950911
running average episode reward sum: -0.16433950516619344
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.04000768, 11.42413169,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.4260144436619811}
episode index:428
target Thresh 15.313627781152462
target distance 10.0
model initialize at round 428
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([ 4.20521712, 10.01287353,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 8.103005030705576}
done in step count: 4
reward sum = 0.8698550789916836
running average episode reward sum: -0.16192879517981146
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.88792363, 2.97109127, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.9775374001429207}
episode index:429
target Thresh 15.330305812965566
target distance 13.0
model initialize at round 429
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 25.]), 'previousTarget': array([10., 25.]), 'currentState': array([14.14338577, 13.17268133,  0.        ]), 'targetState': array([10, 25], dtype=int32), 'currentDistance': 12.532083326606458}
done in step count: 7
reward sum = 0.7831276499162257
running average episode reward sum: -0.15973098949354161
{'scaleFactor': 20, 'currentTarget': array([10., 25.]), 'previousTarget': array([10., 25.]), 'currentState': array([ 9.59244827, 24.31265509,  0.        ]), 'targetState': array([10, 25], dtype=int32), 'currentDistance': 0.7990878795476686}
episode index:430
target Thresh 15.346967175083098
target distance 15.0
model initialize at round 430
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 17.]), 'previousTarget': array([22., 17.]), 'currentState': array([ 9.        , 26.00009739,  0.        ]), 'targetState': array([22, 17], dtype=int32), 'currentDistance': 15.81144373867544}
done in step count: 7
reward sum = 0.6999064883382168
running average episode reward sum: -0.1577364709834911
{'scaleFactor': 20, 'currentTarget': array([22., 17.]), 'previousTarget': array([22., 17.]), 'currentState': array([21.52470207, 16.89217177,  0.        ]), 'targetState': array([22, 17], dtype=int32), 'currentDistance': 0.48737567372401397}
episode index:431
target Thresh 15.36361188416641
target distance 13.0
model initialize at round 431
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 12.]), 'previousTarget': array([15., 12.]), 'currentState': array([ 4.       , 21.0001657,  0.       ]), 'targetState': array([15, 12], dtype=int32), 'currentDistance': 14.21277533221017}
done in step count: 6
reward sum = 0.7274016055272128
running average episode reward sum: -0.15568754025082748
{'scaleFactor': 20, 'currentTarget': array([15., 12.]), 'previousTarget': array([15., 12.]), 'currentState': array([14.74225807, 12.01226807,  0.        ]), 'targetState': array([15, 12], dtype=int32), 'currentDistance': 0.2580337322027119}
episode index:432
target Thresh 15.380239956860226
target distance 14.0
model initialize at round 432
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 17.]), 'previousTarget': array([24., 17.]), 'currentState': array([12.       ,  5.5871152,  0.       ]), 'targetState': array([24, 17], dtype=int32), 'currentDistance': 16.5606141038064}
done in step count: 9
reward sum = 0.733730532691638
running average episode reward sum: -0.15363345694149153
{'scaleFactor': 20, 'currentTarget': array([24., 17.]), 'previousTarget': array([24., 17.]), 'currentState': array([23.42150319, 16.66509855,  0.        ]), 'targetState': array([24, 17], dtype=int32), 'currentDistance': 0.6684441234248285}
episode index:433
target Thresh 15.39685140979261
target distance 15.0
model initialize at round 433
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 28.]), 'previousTarget': array([11., 28.]), 'currentState': array([ 8.06670213, 14.99722517,  0.        ]), 'targetState': array([11, 28], dtype=int32), 'currentDistance': 13.329530742274033}
done in step count: 7
reward sum = 0.7952222321408021
running average episode reward sum: -0.15144715351042634
{'scaleFactor': 20, 'currentTarget': array([11., 28.]), 'previousTarget': array([11., 28.]), 'currentState': array([10.49231394, 27.18249178,  0.        ]), 'targetState': array([11, 28], dtype=int32), 'currentDistance': 0.9623226226632152}
episode index:434
target Thresh 15.413446259575025
target distance 5.0
model initialize at round 434
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 14.]), 'previousTarget': array([13., 14.]), 'currentState': array([11.69195652, 17.07024336,  0.        ]), 'targetState': array([13, 14], dtype=int32), 'currentDistance': 3.3372701458618645}
done in step count: 2
reward sum = 0.9242430966892119
running average episode reward sum: -0.1489743023605421
{'scaleFactor': 20, 'currentTarget': array([13., 14.]), 'previousTarget': array([13., 14.]), 'currentState': array([12.90805329, 14.5553481 ,  0.        ]), 'targetState': array([13, 14], dtype=int32), 'currentDistance': 0.5629082581004503}
episode index:435
target Thresh 15.430024522802313
target distance 8.0
model initialize at round 435
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  3.]), 'previousTarget': array([11.,  3.]), 'currentState': array([16.63967526,  9.71342778,  0.        ]), 'targetState': array([11,  3], dtype=int32), 'currentDistance': 8.76789881434273}
done in step count: 5
reward sum = 0.8713353298686861
running average episode reward sum: -0.14663414265359434
{'scaleFactor': 20, 'currentTarget': array([11.,  3.]), 'previousTarget': array([11.,  3.]), 'currentState': array([11.46046799,  3.79950881,  0.        ]), 'targetState': array([11,  3], dtype=int32), 'currentDistance': 0.9226294548904926}
episode index:436
target Thresh 15.446586216052744
target distance 7.0
model initialize at round 436
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 20.]), 'previousTarget': array([ 4., 20.]), 'currentState': array([ 7.362625  , 14.17974734,  0.        ]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 6.721799454048353}
done in step count: 4
reward sum = 0.8915750902417692
running average episode reward sum: -0.1442583778185935
{'scaleFactor': 20, 'currentTarget': array([ 4., 20.]), 'previousTarget': array([ 4., 20.]), 'currentState': array([ 4.03979406, 19.30704474,  0.        ]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 0.6940969335459178}
episode index:437
target Thresh 15.463131355888013
target distance 11.0
model initialize at round 437
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 21.]), 'previousTarget': array([ 6., 21.]), 'currentState': array([ 9.2422775 , 11.32304382,  0.        ]), 'targetState': array([ 6, 21], dtype=int32), 'currentDistance': 10.20567706002107}
done in step count: 6
reward sum = 0.8215917890204454
running average episode reward sum: -0.14205324045138107
{'scaleFactor': 20, 'currentTarget': array([ 6., 21.]), 'previousTarget': array([ 6., 21.]), 'currentState': array([ 5.63256288, 20.48556417,  0.        ]), 'targetState': array([ 6, 21], dtype=int32), 'currentDistance': 0.6321821423715865}
episode index:438
target Thresh 15.479659958853258
target distance 6.0
model initialize at round 438
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'previousTarget': array([19.,  3.]), 'currentState': array([24.0084675 ,  7.06648821,  0.        ]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 6.451439605667412}
done in step count: 4
reward sum = 0.9169724593177413
running average episode reward sum: -0.13964088122639445
{'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'previousTarget': array([19.,  3.]), 'currentState': array([19.72930491,  3.96154726,  0.        ]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 1.206838338764353}
episode index:439
target Thresh 15.496172041477084
target distance 8.0
model initialize at round 439
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([23.8237313 , 19.09018576,  0.        ]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 6.357386135458224}
done in step count: 4
reward sum = 0.8961990845044107
running average episode reward sum: -0.13728669948609717
{'scaleFactor': 20, 'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([22.17188367, 13.56734335,  0.        ]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 0.5928089729815329}
episode index:440
target Thresh 15.512667620271579
target distance 7.0
model initialize at round 440
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 26.]), 'previousTarget': array([15., 26.]), 'currentState': array([ 9.99996066, 26.2602874 ,  0.        ]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 5.006809655351004}
done in step count: 3
reward sum = 0.9014277486827648
running average episode reward sum: -0.13493133792562356
{'scaleFactor': 20, 'currentTarget': array([15., 26.]), 'previousTarget': array([15., 26.]), 'currentState': array([14.51371557, 25.9577533 ,  0.        ]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 0.48811610824571333}
episode index:441
target Thresh 15.52914671173232
target distance 15.0
model initialize at round 441
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 13.]), 'previousTarget': array([27., 13.]), 'currentState': array([14.        ,  5.74328172,  0.        ]), 'targetState': array([27, 13], dtype=int32), 'currentDistance': 14.88824906311221}
done in step count: 7
reward sum = 0.7688827904674538
running average episode reward sum: -0.13288650958084286
{'scaleFactor': 20, 'currentTarget': array([27., 13.]), 'previousTarget': array([27., 13.]), 'currentState': array([26.06344235, 12.32769993,  0.        ]), 'targetState': array([27, 13], dtype=int32), 'currentDistance': 1.1528779717128848}
episode index:442
target Thresh 15.545609332338401
target distance 6.0
model initialize at round 442
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 23.]), 'previousTarget': array([17., 23.]), 'currentState': array([13.95911813, 27.05606747,  0.        ]), 'targetState': array([17, 23], dtype=int32), 'currentDistance': 5.069383182917717}
done in step count: 2
reward sum = 0.8955367773550561
running average episode reward sum: -0.13056501231913653
{'scaleFactor': 20, 'currentTarget': array([17., 23.]), 'previousTarget': array([17., 23.]), 'currentState': array([16.82355773, 23.90494668,  0.        ]), 'targetState': array([17, 23], dtype=int32), 'currentDistance': 0.9219871883367653}
episode index:443
target Thresh 15.562055498552443
target distance 6.0
model initialize at round 443
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  3.]), 'previousTarget': array([21.,  3.]), 'currentState': array([21.82067159,  7.10408878,  0.        ]), 'targetState': array([21,  3], dtype=int32), 'currentDistance': 4.185337094735015}
done in step count: 2
reward sum = 0.9362426669731491
running average episode reward sum: -0.12816229232073048
{'scaleFactor': 20, 'currentTarget': array([21.,  3.]), 'previousTarget': array([21.,  3.]), 'currentState': array([21.2529868 ,  3.98165476,  0.        ]), 'targetState': array([21,  3], dtype=int32), 'currentDistance': 1.0137299428238185}
episode index:444
target Thresh 15.57848522682061
target distance 15.0
model initialize at round 444
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 19.]), 'previousTarget': array([ 2., 19.]), 'currentState': array([16.70519704,  8.53314036,  0.        ]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 18.049874530629513}
done in step count: 13
reward sum = 0.7450916745714242
running average episode reward sum: -0.12619992385580428
{'scaleFactor': 20, 'currentTarget': array([ 2., 19.]), 'previousTarget': array([ 2., 19.]), 'currentState': array([ 1.82763636, 18.52285641,  0.        ]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 0.5073216192561416}
episode index:445
target Thresh 15.594898533572639
target distance 9.0
model initialize at round 445
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 27.]), 'previousTarget': array([14., 27.]), 'currentState': array([21.51773942, 28.14625996,  0.        ]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 7.604624765123624}
done in step count: 6
reward sum = 0.8879840176838377
running average episode reward sum: -0.12392596882993065
{'scaleFactor': 20, 'currentTarget': array([14., 27.]), 'previousTarget': array([14., 27.]), 'currentState': array([14.58688146, 27.52579881,  0.        ]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 0.7879684244119239}
episode index:446
target Thresh 15.611295435221834
target distance 6.0
model initialize at round 446
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 13.]), 'previousTarget': array([11., 13.]), 'currentState': array([ 6.99472308, 16.21710682,  0.        ]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 5.137316369370042}
done in step count: 2
reward sum = 0.897180967968338
running average episode reward sum: -0.12164161326662355
{'scaleFactor': 20, 'currentTarget': array([11., 13.]), 'previousTarget': array([11., 13.]), 'currentState': array([10.43577826, 13.60449791,  0.        ]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 0.8269001718909682}
episode index:447
target Thresh 15.627675948165095
target distance 15.0
model initialize at round 447
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 2.]), 'previousTarget': array([9., 2.]), 'currentState': array([24.32414135,  2.16007331,  0.        ]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 15.324977379514978}
done in step count: 99
reward sum = -0.12262643312957405
running average episode reward sum: -0.12164381152524621
{'scaleFactor': 20, 'currentTarget': array([9., 2.]), 'previousTarget': array([9., 2.]), 'currentState': array([70.76655239, 29.45258275,  0.        ]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 67.59253874129818}
episode index:448
target Thresh 15.644040088782944
target distance 3.0
model initialize at round 448
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 19.]), 'previousTarget': array([16., 19.]), 'currentState': array([17.82634807, 20.21640146,  0.        ]), 'targetState': array([16, 19], dtype=int32), 'currentDistance': 2.1943517871782743}
done in step count: 1
reward sum = 0.9734242443784841
running average episode reward sum: -0.11920490716911318
{'scaleFactor': 20, 'currentTarget': array([16., 19.]), 'previousTarget': array([16., 19.]), 'currentState': array([16.84646386, 19.6025098 ,  0.        ]), 'targetState': array([16, 19], dtype=int32), 'currentDistance': 1.0389990951645984}
episode index:449
target Thresh 15.660387873439518
target distance 5.0
model initialize at round 449
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 12.]), 'previousTarget': array([15., 12.]), 'currentState': array([18.79204297, 13.38721663,  0.        ]), 'targetState': array([15, 12], dtype=int32), 'currentDistance': 4.037816224544463}
done in step count: 3
reward sum = 0.9418813720272718
running average episode reward sum: -0.11684693765978788
{'scaleFactor': 20, 'currentTarget': array([15., 12.]), 'previousTarget': array([15., 12.]), 'currentState': array([15.64532584, 12.24408355,  0.        ]), 'targetState': array([15, 12], dtype=int32), 'currentDistance': 0.68994363557171}
episode index:450
target Thresh 15.6767193184826
target distance 14.0
model initialize at round 450
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 29.]), 'previousTarget': array([ 4., 29.]), 'currentState': array([ 9.14083838, 16.06283283,  0.        ]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 13.921153458709618}
done in step count: 8
reward sum = 0.7617266663969942
running average episode reward sum: -0.11489888088804334
{'scaleFactor': 20, 'currentTarget': array([ 4., 29.]), 'previousTarget': array([ 4., 29.]), 'currentState': array([ 3.60742721, 28.27962971,  0.        ]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 0.8203942675948102}
episode index:451
target Thresh 15.693034440243647
target distance 3.0
model initialize at round 451
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.74144405,  3.35685408,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.381269045540982}
done in step count: 1
reward sum = 0.9696569242817722
running average episode reward sum: -0.11249942114209242
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.94199505,  2.46972448,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.47329235930323316}
episode index:452
target Thresh 15.70933325503777
target distance 4.0
model initialize at round 452
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 14.]), 'previousTarget': array([21., 14.]), 'currentState': array([23.58493352, 10.81266862,  0.        ]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 4.103774191600191}
done in step count: 3
reward sum = 0.9384665889585438
running average episode reward sum: -0.11017940787476212
{'scaleFactor': 20, 'currentTarget': array([21., 14.]), 'previousTarget': array([21., 14.]), 'currentState': array([21.3537409 , 13.23536497,  0.        ]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 0.8424959041732032}
episode index:453
target Thresh 15.725615779163792
target distance 13.0
model initialize at round 453
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 12.]), 'previousTarget': array([ 7., 12.]), 'currentState': array([ 6.39707148, 23.01231539,  0.        ]), 'targetState': array([ 7, 12], dtype=int32), 'currentDistance': 11.028808326518647}
done in step count: 6
reward sum = 0.8134908433597579
running average episode reward sum: -0.1081448919028799
{'scaleFactor': 20, 'currentTarget': array([ 7., 12.]), 'previousTarget': array([ 7., 12.]), 'currentState': array([ 7.01754714, 12.60986841,  0.        ]), 'targetState': array([ 7, 12], dtype=int32), 'currentDistance': 0.6101207883374196}
episode index:454
target Thresh 15.74188202890424
target distance 11.0
model initialize at round 454
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 23.]), 'previousTarget': array([13., 23.]), 'currentState': array([18.16667914, 12.85682786,  0.        ]), 'targetState': array([13, 23], dtype=int32), 'currentDistance': 11.38325588471665}
done in step count: 7
reward sum = 0.8066090405064272
running average episode reward sum: -0.10613444369978252
{'scaleFactor': 20, 'currentTarget': array([13., 23.]), 'previousTarget': array([13., 23.]), 'currentState': array([13.01291126, 22.62760746,  0.        ]), 'targetState': array([13, 23], dtype=int32), 'currentDistance': 0.37261629167472016}
episode index:455
target Thresh 15.758132020525355
target distance 13.0
model initialize at round 455
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  8.]), 'previousTarget': array([25.,  8.]), 'currentState': array([18.98385406, 19.00459278,  0.        ]), 'targetState': array([25,  8], dtype=int32), 'currentDistance': 12.54173330121612}
done in step count: 6
reward sum = 0.7711267028977087
running average episode reward sum: -0.10421062539584065
{'scaleFactor': 20, 'currentTarget': array([25.,  8.]), 'previousTarget': array([25.,  8.]), 'currentState': array([24.93047112,  8.54071087,  0.        ]), 'targetState': array([25,  8], dtype=int32), 'currentDistance': 0.5451628251612821}
episode index:456
target Thresh 15.774365770277146
target distance 13.0
model initialize at round 456
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([23.11010456, 21.73600841,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 14.265612124485498}
done in step count: 8
reward sum = 0.8037882275977184
running average episode reward sum: -0.1022237570085462
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.71400309, 10.96471077,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.2001946844699247}
episode index:457
target Thresh 15.790583294393347
target distance 13.0
model initialize at round 457
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 29.]), 'previousTarget': array([27., 29.]), 'currentState': array([23.07660925, 17.99633551,  0.        ]), 'targetState': array([27, 29], dtype=int32), 'currentDistance': 11.682192740815536}
done in step count: 6
reward sum = 0.8162980915493481
running average episode reward sum: -0.10021825078898748
{'scaleFactor': 20, 'currentTarget': array([27., 29.]), 'previousTarget': array([27., 29.]), 'currentState': array([26.59762555, 28.21646339,  0.        ]), 'targetState': array([27, 29], dtype=int32), 'currentDistance': 0.8808148607969993}
episode index:458
target Thresh 15.806784609091498
target distance 9.0
model initialize at round 458
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 7.]), 'previousTarget': array([7., 7.]), 'currentState': array([ 5.91676563, 14.01566386,  0.        ]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 7.098798217134674}
done in step count: 4
reward sum = 0.8723411385303269
running average episode reward sum: -0.09809938501704998
{'scaleFactor': 20, 'currentTarget': array([7., 7.]), 'previousTarget': array([7., 7.]), 'currentState': array([6.72514218, 7.24382865, 0.        ]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 0.3674224181540002}
episode index:459
target Thresh 15.822969730572904
target distance 15.0
model initialize at round 459
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 22.]), 'previousTarget': array([ 8., 22.]), 'currentState': array([7.54310584, 8.9037255 , 0.        ]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 13.104241982063497}
done in step count: 7
reward sum = 0.7895977734456632
running average episode reward sum: -0.0961696085856093
{'scaleFactor': 20, 'currentTarget': array([ 8., 22.]), 'previousTarget': array([ 8., 22.]), 'currentState': array([ 7.60455284, 21.15843129,  0.        ]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 0.9298474849274628}
episode index:460
target Thresh 15.839138675022696
target distance 6.0
model initialize at round 460
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([25.31335211,  2.5859068 ,  0.        ]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 5.501003432216017}
done in step count: 4
reward sum = 0.9150790095177892
running average episode reward sum: -0.09397601071553685
{'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([21.38887382,  5.20505339,  0.        ]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 0.8849649476871521}
episode index:461
target Thresh 15.855291458609813
target distance 13.0
model initialize at round 461
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([27.09141191, 15.631327  ,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 16.864464998096768}
done in step count: 99
reward sum = -0.12012254647574944
running average episode reward sum: -0.09403260494878407
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([72.36542093, 43.00940174,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 69.65082182611592}
episode index:462
target Thresh 15.871428097487044
target distance 10.0
model initialize at round 462
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'previousTarget': array([27.,  8.]), 'currentState': array([18.96907258, 16.07955813,  0.        ]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 11.39188548317839}
done in step count: 5
reward sum = 0.7956348716657644
running average episode reward sum: -0.09211107692153883
{'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'previousTarget': array([27.,  8.]), 'currentState': array([26.46645349,  8.40019715,  0.        ]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 0.6669554955470413}
episode index:463
target Thresh 15.887548607791029
target distance 9.0
model initialize at round 463
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 15.]), 'previousTarget': array([23., 15.]), 'currentState': array([26.22664768, 22.15200543,  0.        ]), 'targetState': array([23, 15], dtype=int32), 'currentDistance': 7.8461733951850405}
done in step count: 5
reward sum = 0.8762458715156245
running average episode reward sum: -0.09002410073956219
{'scaleFactor': 20, 'currentTarget': array([23., 15.]), 'previousTarget': array([23., 15.]), 'currentState': array([23.32290497, 15.62993282,  0.        ]), 'targetState': array([23, 15], dtype=int32), 'currentDistance': 0.7078721502466891}
episode index:464
target Thresh 15.90365300564228
target distance 13.0
model initialize at round 464
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 13.]), 'previousTarget': array([15., 13.]), 'currentState': array([ 3.99999809, 16.90720566,  0.        ]), 'targetState': array([15, 13], dtype=int32), 'currentDistance': 11.673315638724342}
done in step count: 6
reward sum = 0.815885063180727
running average episode reward sum: -0.08807590898919597
{'scaleFactor': 20, 'currentTarget': array([15., 13.]), 'previousTarget': array([15., 13.]), 'currentState': array([14.17615342, 12.92439348,  0.        ]), 'targetState': array([15, 13], dtype=int32), 'currentDistance': 0.827308607281244}
episode index:465
target Thresh 15.919741307145195
target distance 15.0
model initialize at round 465
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 18.]), 'previousTarget': array([ 3., 18.]), 'currentState': array([16.86981773, 14.43481553,  0.        ]), 'targetState': array([ 3, 18], dtype=int32), 'currentDistance': 14.32069775888882}
done in step count: 10
reward sum = 0.8026607812465347
running average episode reward sum: -0.0861644568642266
{'scaleFactor': 20, 'currentTarget': array([ 3., 18.]), 'previousTarget': array([ 3., 18.]), 'currentState': array([ 3.45780998, 17.53364493,  0.        ]), 'targetState': array([ 3, 18], dtype=int32), 'currentDistance': 0.6535113111728463}
episode index:466
target Thresh 15.935813528388078
target distance 10.0
model initialize at round 466
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 16.]), 'previousTarget': array([17., 16.]), 'currentState': array([21.3147763 ,  7.01711583,  0.        ]), 'targetState': array([17, 16], dtype=int32), 'currentDistance': 9.965415320894394}
done in step count: 6
reward sum = 0.8448474631844646
running average episode reward sum: -0.08417085532236643
{'scaleFactor': 20, 'currentTarget': array([17., 16.]), 'previousTarget': array([17., 16.]), 'currentState': array([17.06018227, 15.13054997,  0.        ]), 'targetState': array([17, 16], dtype=int32), 'currentDistance': 0.8715304156546714}
episode index:467
target Thresh 15.951869685443146
target distance 14.0
model initialize at round 467
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'previousTarget': array([ 5., 22.]), 'currentState': array([5.51770961, 9.82648373, 0.        ]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 12.184519756821194}
done in step count: 7
reward sum = 0.8033850341446247
running average episode reward sum: -0.0822743683790609
{'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'previousTarget': array([ 5., 22.]), 'currentState': array([ 4.57615369, 21.23507816,  0.        ]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 0.874500498085594}
episode index:468
target Thresh 15.967909794366566
target distance 7.0
model initialize at round 468
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([17.37971199,  2.4805972 ,  0.        ]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 5.940428564216342}
done in step count: 4
reward sum = 0.9160322639444659
running average episode reward sum: -0.0801457828090747
{'scaleFactor': 20, 'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([12.80951929,  4.07721949,  0.        ]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 1.2275362963921181}
episode index:469
target Thresh 15.983933871198445
target distance 15.0
model initialize at round 469
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 26.]), 'previousTarget': array([25., 26.]), 'currentState': array([22.22320199, 12.99633276,  0.        ]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 13.296840555409421}
done in step count: 7
reward sum = 0.7959338241528837
running average episode reward sum: -0.07828178364532586
{'scaleFactor': 20, 'currentTarget': array([25., 26.]), 'previousTarget': array([25., 26.]), 'currentState': array([24.70040847, 25.03139031,  0.        ]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 1.0138835315446229}
episode index:470
target Thresh 15.999941931962859
target distance 9.0
model initialize at round 470
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 16.]), 'previousTarget': array([15., 16.]), 'currentState': array([16.55685487, 23.06377316,  0.        ]), 'targetState': array([15, 16], dtype=int32), 'currentDistance': 7.2333041057784815}
done in step count: 4
reward sum = 0.8876293104820029
running average episode reward sum: -0.07623101699112769
{'scaleFactor': 20, 'currentTarget': array([15., 16.]), 'previousTarget': array([15., 16.]), 'currentState': array([15.17683665, 16.83509374,  0.        ]), 'targetState': array([15, 16], dtype=int32), 'currentDistance': 0.8536115928100646}
episode index:471
target Thresh 16.01593399266787
target distance 2.0
model initialize at round 471
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([ 7.6818341 , 23.63257778,  0.        ]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 1.368415297167877}
done in step count: 1
reward sum = 0.9854584456889448
running average episode reward sum: -0.0739816749091784
{'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([ 8.23920459, 23.66203125,  0.        ]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 0.8324857572672784}
episode index:472
target Thresh 16.031910069305546
target distance 11.0
model initialize at round 472
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 19.]), 'previousTarget': array([21., 19.]), 'currentState': array([11.99997532, 20.86165616,  0.        ]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 9.190549919856217}
done in step count: 5
reward sum = 0.8523378114742817
running average episode reward sum: -0.07202328276037617
{'scaleFactor': 20, 'currentTarget': array([21., 19.]), 'previousTarget': array([21., 19.]), 'currentState': array([20.21801126, 18.56464979,  0.        ]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 0.8950062537912552}
episode index:473
target Thresh 16.04787017785196
target distance 10.0
model initialize at round 473
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([ 5.99900556, 15.24465735,  0.        ]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 10.471416636513117}
done in step count: 9
reward sum = 0.8235198206557549
running average episode reward sum: -0.07013395131856999
{'scaleFactor': 20, 'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([13.34928358, 21.65117797,  0.        ]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 0.7383147508204156}
episode index:474
target Thresh 16.063814334267217
target distance 13.0
model initialize at round 474
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 23.]), 'previousTarget': array([13., 23.]), 'currentState': array([ 5.43533108, 11.76683462,  0.        ]), 'targetState': array([13, 23], dtype=int32), 'currentDistance': 13.542829115564066}
done in step count: 10
reward sum = 0.7909116509409206
running average episode reward sum: -0.0683212237348658
{'scaleFactor': 20, 'currentTarget': array([13., 23.]), 'previousTarget': array([13., 23.]), 'currentState': array([12.40975687, 22.6111033 ,  0.        ]), 'targetState': array([13, 23], dtype=int32), 'currentDistance': 0.7068434054050822}
episode index:475
target Thresh 16.079742554495482
target distance 13.0
model initialize at round 475
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 5.]), 'previousTarget': array([6., 5.]), 'currentState': array([13.28763396, 16.61820436,  0.        ]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 13.714673933752923}
done in step count: 8
reward sum = 0.810911211267693
running average episode reward sum: -0.0664740967705747
{'scaleFactor': 20, 'currentTarget': array([6., 5.]), 'previousTarget': array([6., 5.]), 'currentState': array([6.55302703, 5.93979919, 0.        ]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 1.0904409277132938}
episode index:476
target Thresh 16.09565485446498
target distance 13.0
model initialize at round 476
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 21.]), 'previousTarget': array([ 4., 21.]), 'currentState': array([14.24599457,  9.07823634,  0.        ]), 'targetState': array([ 4, 21], dtype=int32), 'currentDistance': 15.71969635249159}
done in step count: 10
reward sum = 0.743402617456034
running average episode reward sum: -0.06477624202376839
{'scaleFactor': 20, 'currentTarget': array([ 4., 21.]), 'previousTarget': array([ 4., 21.]), 'currentState': array([ 3.75215104, 20.4982965 ,  0.        ]), 'targetState': array([ 4, 21], dtype=int32), 'currentDistance': 0.5595851217943202}
episode index:477
target Thresh 16.111551250088006
target distance 11.0
model initialize at round 477
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 22.]), 'previousTarget': array([ 7., 22.]), 'currentState': array([16.4654727, 21.2606315,  0.       ]), 'targetState': array([ 7, 22], dtype=int32), 'currentDistance': 9.494305618469596}
done in step count: 7
reward sum = 0.8652002631273369
running average episode reward sum: -0.06283068448161126
{'scaleFactor': 20, 'currentTarget': array([ 7., 22.]), 'previousTarget': array([ 7., 22.]), 'currentState': array([ 7.79310083, 21.77857928,  0.        ]), 'targetState': array([ 7, 22], dtype=int32), 'currentDistance': 0.8234294554265599}
episode index:478
target Thresh 16.12743175726095
target distance 12.0
model initialize at round 478
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([16.93670338, 12.04596901,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 10.230948799359279}
done in step count: 6
reward sum = 0.8371632596741928
running average episode reward sum: -0.06095178271928182
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.06927396,  2.68351442,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.687015893683684}
episode index:479
target Thresh 16.14329639186434
target distance 13.0
model initialize at round 479
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([ 3.43659925, 15.07525766,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 11.08957855881941}
done in step count: 7
reward sum = 0.8037476372466504
running average episode reward sum: -0.059150325594352786
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.19312157, 4.63156876, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.660435491161946}
episode index:480
target Thresh 16.159145169762795
target distance 5.0
model initialize at round 480
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 15.]), 'previousTarget': array([19., 15.]), 'currentState': array([22.54838276, 10.87956178,  0.        ]), 'targetState': array([19, 15], dtype=int32), 'currentDistance': 5.437741379870312}
done in step count: 5
reward sum = 0.9100904653767611
running average episode reward sum: -0.05713527197487022
{'scaleFactor': 20, 'currentTarget': array([19., 15.]), 'previousTarget': array([19., 15.]), 'currentState': array([18.94531831, 14.36752468,  0.        ]), 'targetState': array([19, 15], dtype=int32), 'currentDistance': 0.6348347131931183}
episode index:481
target Thresh 16.1749781068051
target distance 4.0
model initialize at round 481
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 12.]), 'previousTarget': array([11., 12.]), 'currentState': array([13.74286377, 10.95399706,  0.        ]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 2.935544895497277}
done in step count: 2
reward sum = 0.9625476357643864
running average episode reward sum: -0.05501974727001699
{'scaleFactor': 20, 'currentTarget': array([11., 12.]), 'previousTarget': array([11., 12.]), 'currentState': array([11.81043983, 11.21402188,  0.        ]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 1.1289704667865863}
episode index:482
target Thresh 16.190795218824192
target distance 15.0
model initialize at round 482
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 24.]), 'previousTarget': array([27., 24.]), 'currentState': array([14.        , 15.93788904,  0.        ]), 'targetState': array([27, 24], dtype=int32), 'currentDistance': 15.296981177315601}
done in step count: 8
reward sum = 0.7480158055119769
running average episode reward sum: -0.05335714778185552
{'scaleFactor': 20, 'currentTarget': array([27., 24.]), 'previousTarget': array([27., 24.]), 'currentState': array([26.43006825, 23.64052373,  0.        ]), 'targetState': array([27, 24], dtype=int32), 'currentDistance': 0.6738288966987745}
episode index:483
target Thresh 16.206596521637188
target distance 15.0
model initialize at round 483
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 29.]), 'previousTarget': array([ 4., 29.]), 'currentState': array([17.8146565 , 18.94411796,  0.        ]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 17.086997911849398}
done in step count: 11
reward sum = 0.754852038498828
running average episode reward sum: -0.051687294091192944
{'scaleFactor': 20, 'currentTarget': array([ 4., 29.]), 'previousTarget': array([ 4., 29.]), 'currentState': array([ 4.17514262, 28.27063113,  0.        ]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 0.7501025782505862}
episode index:484
target Thresh 16.222382031045385
target distance 8.0
model initialize at round 484
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 29.]), 'previousTarget': array([ 7., 29.]), 'currentState': array([ 7.07708222, 22.7564888 ,  0.        ]), 'targetState': array([ 7, 29], dtype=int32), 'currentDistance': 6.243987009279686}
done in step count: 4
reward sum = 0.9022470828591906
running average episode reward sum: -0.049720419087171536
{'scaleFactor': 20, 'currentTarget': array([ 7., 29.]), 'previousTarget': array([ 7., 29.]), 'currentState': array([ 6.7505226 , 28.13440925,  0.        ]), 'targetState': array([ 7, 29], dtype=int32), 'currentDistance': 0.9008253561650265}
episode index:485
target Thresh 16.2381517628343
target distance 2.0
model initialize at round 485
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 28.]), 'previousTarget': array([19., 28.]), 'currentState': array([20.04162788, 27.90350727,  0.        ]), 'targetState': array([19, 28], dtype=int32), 'currentDistance': 1.046087709276777}
done in step count: 1
reward sum = 0.9836203813030666
running average episode reward sum: -0.047594203448508496
{'scaleFactor': 20, 'currentTarget': array([19., 28.]), 'previousTarget': array([19., 28.]), 'currentState': array([19.45189852, 27.96572948,  0.        ]), 'targetState': array([19, 28], dtype=int32), 'currentDistance': 0.4531961346192981}
episode index:486
target Thresh 16.253905732773667
target distance 15.0
model initialize at round 486
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  2.]), 'previousTarget': array([23.,  2.]), 'currentState': array([19.3947649 , 15.01161301,  0.        ]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 13.501844069688248}
done in step count: 7
reward sum = 0.778039709016151
running average episode reward sum: -0.04589885660566525
{'scaleFactor': 20, 'currentTarget': array([23.,  2.]), 'previousTarget': array([23.,  2.]), 'currentState': array([22.9910472 ,  2.72796595,  0.        ]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 0.7280210013890646}
episode index:487
target Thresh 16.26964395661745
target distance 13.0
model initialize at round 487
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 22.]), 'previousTarget': array([ 2., 22.]), 'currentState': array([10.18565679,  9.95251113,  0.        ]), 'targetState': array([ 2, 22], dtype=int32), 'currentDistance': 14.565265704679005}
done in step count: 9
reward sum = 0.7642358993097933
running average episode reward sum: -0.04423874440092046
{'scaleFactor': 20, 'currentTarget': array([ 2., 22.]), 'previousTarget': array([ 2., 22.]), 'currentState': array([ 1.76581436, 21.25657398,  0.        ]), 'targetState': array([ 2, 22], dtype=int32), 'currentDistance': 0.7794390098898231}
episode index:488
target Thresh 16.28536645010388
target distance 13.0
model initialize at round 488
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([13.61456442,  3.18836831,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 11.6428885009563}
done in step count: 9
reward sum = 0.8369730807700425
running average episode reward sum: -0.0424366752287917
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.82007587, 3.88744313, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.8277641453437548}
episode index:489
target Thresh 16.301073228955456
target distance 7.0
model initialize at round 489
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 24.]), 'previousTarget': array([15., 24.]), 'currentState': array([20.39590847, 19.63316137,  0.        ]), 'targetState': array([15, 24], dtype=int32), 'currentDistance': 6.94154938938384}
done in step count: 6
reward sum = 0.8886919836788776
running average episode reward sum: -0.040536412659592375
{'scaleFactor': 20, 'currentTarget': array([15., 24.]), 'previousTarget': array([15., 24.]), 'currentState': array([15.06478867, 23.37852722,  0.        ]), 'targetState': array([15, 24], dtype=int32), 'currentDistance': 0.6248407660402218}
episode index:490
target Thresh 16.316764308878945
target distance 12.0
model initialize at round 490
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'previousTarget': array([26., 11.]), 'currentState': array([27.95607245, 21.05722284,  0.        ]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 10.24567961365176}
done in step count: 6
reward sum = 0.8390827570015196
running average episode reward sum: -0.03874492758899947
{'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'previousTarget': array([26., 11.]), 'currentState': array([26.50244185, 11.71708202,  0.        ]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 0.8755880532063357}
episode index:491
target Thresh 16.33243970556544
target distance 9.0
model initialize at round 491
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 25.]), 'previousTarget': array([ 3., 25.]), 'currentState': array([ 6.53881359, 17.20584178,  0.        ]), 'targetState': array([ 3, 25], dtype=int32), 'currentDistance': 8.55991261607285}
done in step count: 6
reward sum = 0.8673364273294062
running average episode reward sum: -0.036903298818840115
{'scaleFactor': 20, 'currentTarget': array([ 3., 25.]), 'previousTarget': array([ 3., 25.]), 'currentState': array([ 2.97877008, 24.31926876,  0.        ]), 'targetState': array([ 3, 25], dtype=int32), 'currentDistance': 0.6810622046181195}
episode index:492
target Thresh 16.348099434690337
target distance 10.0
model initialize at round 492
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([3.99986196, 7.65150166, 0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 8.337724701771059}
done in step count: 5
reward sum = 0.8418914701904971
running average episode reward sum: -0.03512075364843578
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([11.23789966,  9.44283996,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.9440467335404967}
episode index:493
target Thresh 16.363743511913363
target distance 12.0
model initialize at round 493
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 26.]), 'previousTarget': array([ 5., 26.]), 'currentState': array([14.2557075 , 14.92222536,  0.        ]), 'targetState': array([ 5, 26], dtype=int32), 'currentDistance': 14.435553762510335}
done in step count: 9
reward sum = 0.7746569075702986
running average episode reward sum: -0.03348152761358004
{'scaleFactor': 20, 'currentTarget': array([ 5., 26.]), 'previousTarget': array([ 5., 26.]), 'currentState': array([ 4.96655431, 25.10589504,  0.        ]), 'targetState': array([ 5, 26], dtype=int32), 'currentDistance': 0.8947302884609637}
episode index:494
target Thresh 16.379371952878596
target distance 15.0
model initialize at round 494
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 14.]), 'previousTarget': array([ 7., 14.]), 'currentState': array([20.81396329, 10.342134  ,  0.        ]), 'targetState': array([ 7, 14], dtype=int32), 'currentDistance': 14.290051279509225}
done in step count: 10
reward sum = 0.8066689978449176
running average episode reward sum: -0.031784253824775
{'scaleFactor': 20, 'currentTarget': array([ 7., 14.]), 'previousTarget': array([ 7., 14.]), 'currentState': array([ 7.69449735, 13.34027133,  0.        ]), 'targetState': array([ 7, 14], dtype=int32), 'currentDistance': 0.9578979491745937}
episode index:495
target Thresh 16.394984773214482
target distance 16.0
model initialize at round 495
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 16.]), 'previousTarget': array([21., 16.]), 'currentState': array([7.        , 8.55953079, 0.        ]), 'targetState': array([21, 16], dtype=int32), 'currentDistance': 15.854355300553856}
done in step count: 8
reward sum = 0.7614857439937495
running average episode reward sum: -0.03018491915175378
{'scaleFactor': 20, 'currentTarget': array([21., 16.]), 'previousTarget': array([21., 16.]), 'currentState': array([20.03603965, 15.07501009,  0.        ]), 'targetState': array([21, 16], dtype=int32), 'currentDistance': 1.3359737599039136}
episode index:496
target Thresh 16.410581988533842
target distance 15.0
model initialize at round 496
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([19.69870251, 23.550071  ,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 15.584429554375342}
done in step count: 10
reward sum = 0.7781963184842531
running average episode reward sum: -0.028558397546852358
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([12.41169804, 10.72895938,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.8371840053953841}
episode index:497
target Thresh 16.426163614433886
target distance 15.0
model initialize at round 497
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 24.]), 'previousTarget': array([20., 24.]), 'currentState': array([ 7.        , 19.04634964,  0.        ]), 'targetState': array([20, 24], dtype=int32), 'currentDistance': 13.911816985676332}
done in step count: 8
reward sum = 0.7655227441401304
running average episode reward sum: -0.026963857101697773
{'scaleFactor': 20, 'currentTarget': array([20., 24.]), 'previousTarget': array([20., 24.]), 'currentState': array([19.30209059, 23.40498933,  0.        ]), 'targetState': array([20, 24], dtype=int32), 'currentDistance': 0.9171233551270774}
episode index:498
target Thresh 16.44172966649625
target distance 2.0
model initialize at round 498
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([17.16765022, 16.31974173,  0.        ]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 1.7621365627817518}
done in step count: 1
reward sum = 0.9799994158958403
running average episode reward sum: -0.024945894630760825
{'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([16.55092424, 15.63218129,  0.        ]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 0.8385527409258547}
episode index:499
target Thresh 16.457280160286984
target distance 6.0
model initialize at round 499
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([8.27587986, 9.94560516, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 7.231453428126558}
done in step count: 6
reward sum = 0.9016922041158625
running average episode reward sum: -0.023092618433267577
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.68377608, 5.79150522, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.0459590038928692}
episode index:500
target Thresh 16.47281511135658
target distance 6.0
model initialize at round 500
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 22.]), 'previousTarget': array([19., 22.]), 'currentState': array([24.14442748, 24.45080781,  0.        ]), 'targetState': array([19, 22], dtype=int32), 'currentDistance': 5.698385122150763}
done in step count: 5
reward sum = 0.9218028950746623
running average episode reward sum: -0.02120659944422979
{'scaleFactor': 20, 'currentTarget': array([19., 22.]), 'previousTarget': array([19., 22.]), 'currentState': array([19.622495  , 22.52081609,  0.        ]), 'targetState': array([19, 22], dtype=int32), 'currentDistance': 0.8116337947441371}
episode index:501
target Thresh 16.48833453524
target distance 9.0
model initialize at round 501
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 22.]), 'previousTarget': array([21., 22.]), 'currentState': array([19.29949948, 14.94629037,  0.        ]), 'targetState': array([21, 22], dtype=int32), 'currentDistance': 7.2557922725722275}
done in step count: 4
reward sum = 0.8913562659218537
running average episode reward sum: -0.01938874513075154
{'scaleFactor': 20, 'currentTarget': array([21., 22.]), 'previousTarget': array([21., 22.]), 'currentState': array([20.60309857, 21.0457207 ,  0.        ]), 'targetState': array([21, 22], dtype=int32), 'currentDistance': 1.0335278096348592}
episode index:502
target Thresh 16.50383844745666
target distance 14.0
model initialize at round 502
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'previousTarget': array([19.,  3.]), 'currentState': array([6.99998987, 8.72237265, 0.        ]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 13.294577539174087}
done in step count: 7
reward sum = 0.7930291122392885
running average episode reward sum: -0.017773600285085456
{'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'previousTarget': array([19.,  3.]), 'currentState': array([18.59193236,  3.56412704,  0.        ]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 0.6962460213940048}
episode index:503
target Thresh 16.51932686351047
target distance 9.0
model initialize at round 503
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 18.]), 'previousTarget': array([17., 18.]), 'currentState': array([21.09007549, 25.39254808,  0.        ]), 'targetState': array([17, 18], dtype=int32), 'currentDistance': 8.448578857734875}
done in step count: 6
reward sum = 0.8738571583665089
running average episode reward sum: -0.016004491636967214
{'scaleFactor': 20, 'currentTarget': array([17., 18.]), 'previousTarget': array([17., 18.]), 'currentState': array([17.33059743, 18.72277409,  0.        ]), 'targetState': array([17, 18], dtype=int32), 'currentDistance': 0.7947937115180925}
episode index:504
target Thresh 16.534799798889864
target distance 11.0
model initialize at round 504
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 18.]), 'previousTarget': array([13., 18.]), 'currentState': array([22.3537308, 12.7022801,  0.       ]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 10.749796087920947}
done in step count: 8
reward sum = 0.8391351966467485
running average episode reward sum: -0.014311145719573717
{'scaleFactor': 20, 'currentTarget': array([13., 18.]), 'previousTarget': array([13., 18.]), 'currentState': array([13.64968956, 17.37598029,  0.        ]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 0.90083134898891}
episode index:505
target Thresh 16.550257269067764
target distance 15.0
model initialize at round 505
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([ 5.99999118, 24.56660962,  0.        ]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 14.564360311183998}
done in step count: 8
reward sum = 0.7635841416204867
running average episode reward sum: -0.012773803254474784
{'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([18.23323947, 17.95594162,  0.        ]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 0.7680252915774757}
episode index:506
target Thresh 16.565699289501644
target distance 5.0
model initialize at round 506
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 26.]), 'previousTarget': array([19., 26.]), 'currentState': array([15.7304554 , 23.23699021,  0.        ]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 4.280671090108266}
done in step count: 3
reward sum = 0.9275838299708721
running average episode reward sum: -0.010919054470992838
{'scaleFactor': 20, 'currentTarget': array([19., 26.]), 'previousTarget': array([19., 26.]), 'currentState': array([18.25721473, 25.47788313,  0.        ]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 0.9079295073646083}
episode index:507
target Thresh 16.581125875633525
target distance 6.0
model initialize at round 507
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 24.]), 'previousTarget': array([26., 24.]), 'currentState': array([23.1695559 , 19.79282546,  0.        ]), 'targetState': array([26, 24], dtype=int32), 'currentDistance': 5.0706736628926246}
done in step count: 3
reward sum = 0.919358746129894
running average episode reward sum: -0.009087798957998966
{'scaleFactor': 20, 'currentTarget': array([26., 24.]), 'previousTarget': array([26., 24.]), 'currentState': array([25.23520046, 23.1876654 ,  0.        ]), 'targetState': array([26, 24], dtype=int32), 'currentDistance': 1.1157086657452204}
episode index:508
target Thresh 16.59653704289
target distance 3.0
model initialize at round 508
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([14.9418695 ,  9.12947336,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 2.128068085914851}
done in step count: 2
reward sum = 0.9681842083970782
running average episode reward sum: -0.007167814660641251
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13.47876334,  9.64213392,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.5977310998887472}
episode index:509
target Thresh 16.611932806682233
target distance 2.0
model initialize at round 509
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([23.07047725, 13.27652407,  0.        ]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 1.2920290099658351}
done in step count: 1
reward sum = 0.9830892263988285
running average episode reward sum: -0.005226134187975624
{'scaleFactor': 20, 'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([22.46814728, 13.56520501,  0.        ]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 0.6389120119319991}
episode index:510
target Thresh 16.62731318240599
target distance 5.0
model initialize at round 510
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([23.24074674, 12.28249073,  0.        ]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 4.288149247216203}
done in step count: 2
reward sum = 0.9238174757823523
running average episode reward sum: -0.0034080449316736127
{'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([25.43004173,  9.43259871,  0.        ]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 0.7155376111894093}
episode index:511
target Thresh 16.64267818544165
target distance 12.0
model initialize at round 511
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([ 4.98816907, 14.38814747,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 14.42741717763096}
done in step count: 7
reward sum = 0.7600654901349759
running average episode reward sum: -0.0019168856834965628
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.42369163,  4.26852125,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.6357947768232465}
episode index:512
target Thresh 16.658027831154214
target distance 8.0
model initialize at round 512
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  5.]), 'previousTarget': array([25.,  5.]), 'currentState': array([18.99843001,  4.81161022,  0.        ]), 'targetState': array([25,  5], dtype=int32), 'currentDistance': 6.004526043701047}
done in step count: 4
reward sum = 0.8962240536448709
running average episode reward sum: -0.00016612361852898496
{'scaleFactor': 20, 'currentTarget': array([25.,  5.]), 'previousTarget': array([25.,  5.]), 'currentState': array([24.15292603,  4.52232558,  0.        ]), 'targetState': array([25,  5], dtype=int32), 'currentDistance': 0.972474765937134}
episode index:513
target Thresh 16.67336213489333
target distance 12.0
model initialize at round 513
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 22.]), 'previousTarget': array([25., 22.]), 'currentState': array([16.95050108, 11.40794516,  0.        ]), 'targetState': array([25, 22], dtype=int32), 'currentDistance': 13.303610735798378}
done in step count: 7
reward sum = 0.7966960609478161
running average episode reward sum: 0.0013841919156467838
{'scaleFactor': 20, 'currentTarget': array([25., 22.]), 'previousTarget': array([25., 22.]), 'currentState': array([24.14678836, 21.12399566,  0.        ]), 'targetState': array([25, 22], dtype=int32), 'currentDistance': 1.2228465584560584}
episode index:514
target Thresh 16.68868111199331
target distance 7.0
model initialize at round 514
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 29.]), 'previousTarget': array([ 3., 29.]), 'currentState': array([ 8.40687561, 25.65778273,  0.        ]), 'targetState': array([ 3, 29], dtype=int32), 'currentDistance': 6.35647072853357}
done in step count: 5
reward sum = 0.903981052007642
running average episode reward sum: 0.0031368071779613376
{'scaleFactor': 20, 'currentTarget': array([ 3., 29.]), 'previousTarget': array([ 3., 29.]), 'currentState': array([ 3.47483188, 28.19191012,  0.        ]), 'targetState': array([ 3, 29], dtype=int32), 'currentDistance': 0.9372697429536693}
episode index:515
target Thresh 16.703984777773123
target distance 15.0
model initialize at round 515
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 28.]), 'previousTarget': array([12., 28.]), 'currentState': array([25.46067762, 14.33053553,  0.        ]), 'targetState': array([12, 28], dtype=int32), 'currentDistance': 19.18447551950628}
done in step count: 12
reward sum = 0.7008355423857884
running average episode reward sum: 0.0044889365097594515
{'scaleFactor': 20, 'currentTarget': array([12., 28.]), 'previousTarget': array([12., 28.]), 'currentState': array([12.09295627, 27.59350955,  0.        ]), 'targetState': array([12, 28], dtype=int32), 'currentDistance': 0.4169836339748701}
episode index:516
target Thresh 16.719273147536434
target distance 16.0
model initialize at round 516
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([23.88710213, 25.61827135,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 16.61024495207202}
done in step count: 11
reward sum = 0.7688571056237361
running average episode reward sum: 0.005967404921972172
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.26131957, 11.63563919,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.6872591203561935}
episode index:517
target Thresh 16.734546236571624
target distance 3.0
model initialize at round 517
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 24.]), 'previousTarget': array([ 3., 24.]), 'currentState': array([ 3.51089269, 25.60062313,  0.        ]), 'targetState': array([ 3, 24], dtype=int32), 'currentDistance': 1.680180272099832}
done in step count: 1
reward sum = 0.9743818466289452
running average episode reward sum: 0.007836930871213432
{'scaleFactor': 20, 'currentTarget': array([ 3., 24.]), 'previousTarget': array([ 3., 24.]), 'currentState': array([ 3.17721313, 24.69278944,  0.        ]), 'targetState': array([ 3, 24], dtype=int32), 'currentDistance': 0.7150955852694136}
episode index:518
target Thresh 16.749804060151774
target distance 13.0
model initialize at round 518
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  5.]), 'previousTarget': array([19.,  5.]), 'currentState': array([24.32008952, 16.28766441,  0.        ]), 'targetState': array([19,  5], dtype=int32), 'currentDistance': 12.478570447015878}
done in step count: 8
reward sum = 0.8167694025838219
running average episode reward sum: 0.009395567618251215
{'scaleFactor': 20, 'currentTarget': array([19.,  5.]), 'previousTarget': array([19.,  5.]), 'currentState': array([19.51026586,  5.7591154 ,  0.        ]), 'targetState': array([19,  5], dtype=int32), 'currentDistance': 0.9146734019173547}
episode index:519
target Thresh 16.765046633534716
target distance 15.0
model initialize at round 519
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 10.]), 'previousTarget': array([17., 10.]), 'currentState': array([ 3.99998248, 17.87350464,  0.        ]), 'targetState': array([17, 10], dtype=int32), 'currentDistance': 15.198438436674888}
done in step count: 8
reward sum = 0.7504921061171561
running average episode reward sum: 0.010820753269210647
{'scaleFactor': 20, 'currentTarget': array([17., 10.]), 'previousTarget': array([17., 10.]), 'currentState': array([16.22749591,  9.66073033,  0.        ]), 'targetState': array([17, 10], dtype=int32), 'currentDistance': 0.8437218016326583}
episode index:520
target Thresh 16.780273971963023
target distance 4.0
model initialize at round 520
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 22.]), 'previousTarget': array([19., 22.]), 'currentState': array([20.33672935, 19.05541921,  0.        ]), 'targetState': array([19, 22], dtype=int32), 'currentDistance': 3.233790561827571}
done in step count: 3
reward sum = 0.9493382555838928
running average episode reward sum: 0.012622130432962436
{'scaleFactor': 20, 'currentTarget': array([19., 22.]), 'previousTarget': array([19., 22.]), 'currentState': array([19.01249725, 21.48137456,  0.        ]), 'targetState': array([19, 22], dtype=int32), 'currentDistance': 0.5187759885791458}
episode index:521
target Thresh 16.795486090664035
target distance 6.0
model initialize at round 521
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 25.]), 'previousTarget': array([ 8., 25.]), 'currentState': array([12.99633396, 26.55041218,  0.        ]), 'targetState': array([ 8, 25], dtype=int32), 'currentDistance': 5.231360332539798}
done in step count: 5
reward sum = 0.9237869357582766
running average episode reward sum: 0.014367656879945796
{'scaleFactor': 20, 'currentTarget': array([ 8., 25.]), 'previousTarget': array([ 8., 25.]), 'currentState': array([ 8.62492442, 25.22208361,  0.        ]), 'targetState': array([ 8, 25], dtype=int32), 'currentDistance': 0.6632131359046011}
episode index:522
target Thresh 16.81068300484987
target distance 13.0
model initialize at round 522
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([12.89687858, 11.96380615,  0.        ]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 11.091188011896188}
done in step count: 7
reward sum = 0.8257060284392281
running average episode reward sum: 0.015918973077955898
{'scaleFactor': 20, 'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([13.78854313, 22.61897808,  0.        ]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 0.4357656568690806}
episode index:523
target Thresh 16.825864729717438
target distance 9.0
model initialize at round 523
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 14.]), 'previousTarget': array([11., 14.]), 'currentState': array([17.97869802, 21.94298732,  0.        ]), 'targetState': array([11, 14], dtype=int32), 'currentDistance': 10.573233828768645}
done in step count: 8
reward sum = 0.8540634043426337
running average episode reward sum: 0.01751848535136177
{'scaleFactor': 20, 'currentTarget': array([11., 14.]), 'previousTarget': array([11., 14.]), 'currentState': array([11.48183256, 14.77007002,  0.        ]), 'targetState': array([11, 14], dtype=int32), 'currentDistance': 0.9083889308805284}
episode index:524
target Thresh 16.841031280448476
target distance 3.0
model initialize at round 524
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 26.]), 'previousTarget': array([12., 26.]), 'currentState': array([10.72873777, 24.33950853,  0.        ]), 'targetState': array([12, 26], dtype=int32), 'currentDistance': 2.091253108539528}
done in step count: 1
reward sum = 0.9732108601343911
running average episode reward sum: 0.01933885177951992
{'scaleFactor': 20, 'currentTarget': array([12., 26.]), 'previousTarget': array([12., 26.]), 'currentState': array([11.34568453, 25.15332156,  0.        ]), 'targetState': array([12, 26], dtype=int32), 'currentDistance': 1.0700435085044973}
episode index:525
target Thresh 16.856182672209535
target distance 16.0
model initialize at round 525
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([15.99969208, 21.69362736,  0.        ]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 17.231025144549445}
done in step count: 9
reward sum = 0.6967408499367175
running average episode reward sum: 0.020626688277917634
{'scaleFactor': 20, 'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([24.82245893,  7.60520762,  0.        ]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 0.6307115811030632}
episode index:526
target Thresh 16.871318920152
target distance 13.0
model initialize at round 526
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 29.]), 'previousTarget': array([27., 29.]), 'currentState': array([16.        , 28.35211968,  0.        ]), 'targetState': array([27, 29], dtype=int32), 'currentDistance': 11.01906297756649}
done in step count: 6
reward sum = 0.801953053420243
running average episode reward sum: 0.022109281001147854
{'scaleFactor': 20, 'currentTarget': array([27., 29.]), 'previousTarget': array([27., 29.]), 'currentState': array([26.10742891, 28.6819654 ,  0.        ]), 'targetState': array([27, 29], dtype=int32), 'currentDistance': 0.9475384732967261}
episode index:527
target Thresh 16.886440039412122
target distance 5.0
model initialize at round 527
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 25.]), 'previousTarget': array([ 5., 25.]), 'currentState': array([ 4.7394143 , 21.55670929,  0.        ]), 'targetState': array([ 5, 25], dtype=int32), 'currentDistance': 3.453137098547734}
done in step count: 3
reward sum = 0.9423376839002102
running average episode reward sum: 0.02385213782482032
{'scaleFactor': 20, 'currentTarget': array([ 5., 25.]), 'previousTarget': array([ 5., 25.]), 'currentState': array([ 4.73657265, 24.53209031,  0.        ]), 'targetState': array([ 5, 25], dtype=int32), 'currentDistance': 0.5369668989388179}
episode index:528
target Thresh 16.90154604511103
target distance 2.0
model initialize at round 528
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 25.]), 'previousTarget': array([27., 25.]), 'currentState': array([25.85341072, 24.57392633,  0.        ]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 1.2231948935944787}
done in step count: 1
reward sum = 0.983692037243094
running average episode reward sum: 0.0256665799787301
{'scaleFactor': 20, 'currentTarget': array([27., 25.]), 'previousTarget': array([27., 25.]), 'currentState': array([26.43922091, 24.68403236,  0.        ]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 0.6436681937879734}
episode index:529
target Thresh 16.91663695235472
target distance 14.0
model initialize at round 529
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([15.64563355, 15.60698831,  0.        ]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 14.744214683289234}
done in step count: 10
reward sum = 0.793475135079537
running average episode reward sum: 0.027115275365712753
{'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([8.30215594, 3.68710822, 0.        ]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 0.750610363160247}
episode index:530
target Thresh 16.931712776234107
target distance 9.0
model initialize at round 530
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  7.]), 'previousTarget': array([11.,  7.]), 'currentState': array([3.99942553, 9.03376997, 0.        ]), 'targetState': array([11,  7], dtype=int32), 'currentDistance': 7.290011191704652}
done in step count: 4
reward sum = 0.8742675812266123
running average episode reward sum: 0.028710665772230456
{'scaleFactor': 20, 'currentTarget': array([11.,  7.]), 'previousTarget': array([11.,  7.]), 'currentState': array([10.15370756,  7.19790053,  0.        ]), 'targetState': array([11,  7], dtype=int32), 'currentDistance': 0.8691234139454191}
episode index:531
target Thresh 16.946773531825013
target distance 7.0
model initialize at round 531
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([14.00962782, 27.13284361,  0.        ]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 5.132852643201924}
done in step count: 3
reward sum = 0.913123541954739
running average episode reward sum: 0.03037309599061863
{'scaleFactor': 20, 'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([14.15774934, 22.70197099,  0.        ]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 0.7194776795227193}
episode index:532
target Thresh 16.961819234188198
target distance 15.0
model initialize at round 532
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 3.]), 'previousTarget': array([9., 3.]), 'currentState': array([24.53494406, 12.99419299,  0.        ]), 'targetState': array([9, 3], dtype=int32), 'currentDistance': 18.472097345300185}
done in step count: 99
reward sum = -0.10209723457240721
running average episode reward sum: 0.030124558785059482
{'scaleFactor': 20, 'currentTarget': array([9., 3.]), 'previousTarget': array([9., 3.]), 'currentState': array([54.53140553, 32.53511432,  0.        ]), 'targetState': array([9, 3], dtype=int32), 'currentDistance': 54.27183309619945}
episode index:533
target Thresh 16.976849898369366
target distance 9.0
model initialize at round 533
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 17.]), 'previousTarget': array([21., 17.]), 'currentState': array([19.27414143,  9.95021296,  0.        ]), 'targetState': array([21, 17], dtype=int32), 'currentDistance': 7.257967013830078}
done in step count: 4
reward sum = 0.8910119703567967
running average episode reward sum: 0.03173670749586798
{'scaleFactor': 20, 'currentTarget': array([21., 17.]), 'previousTarget': array([21., 17.]), 'currentState': array([20.71182956, 16.02725708,  0.        ]), 'targetState': array([21, 17], dtype=int32), 'currentDistance': 1.0145299318478966}
episode index:534
target Thresh 16.99186553939918
target distance 7.0
model initialize at round 534
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.66522661,  5.75627446,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 5.254401083299707}
done in step count: 4
reward sum = 0.9141848472615566
running average episode reward sum: 0.03338614327113095
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.75423082, 10.51873291,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.5403892191754653}
episode index:535
target Thresh 17.00686617229328
target distance 5.0
model initialize at round 535
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'previousTarget': array([27.,  8.]), 'currentState': array([23.91223073,  9.65598774,  0.        ]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 3.5038000025671407}
done in step count: 2
reward sum = 0.9376303280619778
running average episode reward sum: 0.0350731660039497
{'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'previousTarget': array([27.,  8.]), 'currentState': array([26.47967219,  8.1681276 ,  0.        ]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 0.546816161843197}
episode index:536
target Thresh 17.021851812052304
target distance 9.0
model initialize at round 536
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 12.]), 'previousTarget': array([14., 12.]), 'currentState': array([ 7.93850231, 20.42764777,  0.        ]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 10.38108863034716}
done in step count: 5
reward sum = 0.8421471999720043
running average episode reward sum: 0.03657609716590138
{'scaleFactor': 20, 'currentTarget': array([14., 12.]), 'previousTarget': array([14., 12.]), 'currentState': array([13.3945781 , 12.86935776,  0.        ]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 1.0593953940750218}
episode index:537
target Thresh 17.036822473661893
target distance 14.0
model initialize at round 537
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 21.]), 'previousTarget': array([ 2., 21.]), 'currentState': array([14.8922385 , 24.04341928,  0.        ]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 13.246592557756479}
done in step count: 12
reward sum = 0.8124138272737721
running average episode reward sum: 0.03801817473115765
{'scaleFactor': 20, 'currentTarget': array([ 2., 21.]), 'previousTarget': array([ 2., 21.]), 'currentState': array([ 2.53345847, 21.4529539 ,  0.        ]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 0.6998179576040255}
episode index:538
target Thresh 17.051778172092703
target distance 11.0
model initialize at round 538
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 17.]), 'previousTarget': array([12., 17.]), 'currentState': array([10.86093974, 26.16195166,  0.        ]), 'targetState': array([12, 17], dtype=int32), 'currentDistance': 9.23248701671662}
done in step count: 5
reward sum = 0.8561951796153502
running average episode reward sum: 0.03953612835802999
{'scaleFactor': 20, 'currentTarget': array([12., 17.]), 'previousTarget': array([12., 17.]), 'currentState': array([11.95305826, 17.81567723,  0.        ]), 'targetState': array([12, 17], dtype=int32), 'currentDistance': 0.8170268438159312}
episode index:539
target Thresh 17.066718922300446
target distance 11.0
model initialize at round 539
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 28.]), 'previousTarget': array([ 4., 28.]), 'currentState': array([ 4.83387375, 18.83955193,  0.        ]), 'targetState': array([ 4, 28], dtype=int32), 'currentDistance': 9.198323453303576}
done in step count: 6
reward sum = 0.8484990283047993
running average episode reward sum: 0.04103420780237586
{'scaleFactor': 20, 'currentTarget': array([ 4., 28.]), 'previousTarget': array([ 4., 28.]), 'currentState': array([ 3.6749639, 27.4892686,  0.       ]), 'targetState': array([ 4, 28], dtype=int32), 'currentDistance': 0.6053883310637974}
episode index:540
target Thresh 17.081644739225865
target distance 10.0
model initialize at round 540
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([22.67177331,  9.7217598 ,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 9.08887390195053}
done in step count: 8
reward sum = 0.873365774591339
running average episode reward sum: 0.04257271347111701
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.98552769,  7.4527698 ,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.0845576634868592}
episode index:541
target Thresh 17.09655563779478
target distance 12.0
model initialize at round 541
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 25.]), 'previousTarget': array([18., 25.]), 'currentState': array([22.37373936, 14.37950468,  0.        ]), 'targetState': array([18, 25], dtype=int32), 'currentDistance': 11.4858398401388}
done in step count: 7
reward sum = 0.8174748997327734
running average episode reward sum: 0.04400242230185807
{'scaleFactor': 20, 'currentTarget': array([18., 25.]), 'previousTarget': array([18., 25.]), 'currentState': array([17.75963631, 24.32160228,  0.        ]), 'targetState': array([18, 25], dtype=int32), 'currentDistance': 0.7197208917372956}
episode index:542
target Thresh 17.11145163291809
target distance 5.0
model initialize at round 542
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 10.]), 'previousTarget': array([26., 10.]), 'currentState': array([22.58176041, 11.82013369,  0.        ]), 'targetState': array([26, 10], dtype=int32), 'currentDistance': 3.8726281197349484}
done in step count: 2
reward sum = 0.9338675517772996
running average episode reward sum: 0.04564121627879259
{'scaleFactor': 20, 'currentTarget': array([26., 10.]), 'previousTarget': array([26., 10.]), 'currentState': array([25.12531292,  9.8778615 ,  0.        ]), 'targetState': array([26, 10], dtype=int32), 'currentDistance': 0.8831734220876714}
episode index:543
target Thresh 17.126332739491794
target distance 16.0
model initialize at round 543
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 14.]), 'previousTarget': array([24., 14.]), 'currentState': array([ 9.99998975, 24.91248322,  0.        ]), 'targetState': array([24, 14], dtype=int32), 'currentDistance': 17.750565539750582}
done in step count: 9
reward sum = 0.7186778720790592
running average episode reward sum: 0.04687841601371955
{'scaleFactor': 20, 'currentTarget': array([24., 14.]), 'previousTarget': array([24., 14.]), 'currentState': array([23.3030228 , 14.70834705,  0.        ]), 'targetState': array([24, 14], dtype=int32), 'currentDistance': 0.993746829558482}
episode index:544
target Thresh 17.141198972396992
target distance 5.0
model initialize at round 544
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 7.]), 'previousTarget': array([5., 7.]), 'currentState': array([ 7.53373384, 10.63626754,  0.        ]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 4.4319576756972605}
done in step count: 4
reward sum = 0.9319275834517263
running average episode reward sum: 0.048502359440211314
{'scaleFactor': 20, 'currentTarget': array([5., 7.]), 'previousTarget': array([5., 7.]), 'currentState': array([5.31718281, 7.61068842, 0.        ]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 0.6881462624037022}
episode index:545
target Thresh 17.15605034649993
target distance 9.0
model initialize at round 545
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([17.18325707, 10.95261908,  0.        ]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 7.277783505528082}
done in step count: 4
reward sum = 0.8909093991934692
running average episode reward sum: 0.05004522947638944
{'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([18.39893144, 17.09775507,  0.        ]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 1.0841260604614107}
episode index:546
target Thresh 17.170886876651977
target distance 7.0
model initialize at round 546
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 17.]), 'previousTarget': array([13., 17.]), 'currentState': array([ 9.45525017, 22.29452467,  0.        ]), 'targetState': array([13, 17], dtype=int32), 'currentDistance': 6.3715965666226255}
done in step count: 3
reward sum = 0.9060004138423168
running average episode reward sum: 0.05161004699808218
{'scaleFactor': 20, 'currentTarget': array([13., 17.]), 'previousTarget': array([13., 17.]), 'currentState': array([12.29606304, 17.69054079,  0.        ]), 'targetState': array([13, 17], dtype=int32), 'currentDistance': 0.9860901749306974}
episode index:547
target Thresh 17.185708577689667
target distance 7.0
model initialize at round 547
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([21.2333616 , 26.65082252,  0.        ]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 7.392742168799528}
done in step count: 4
reward sum = 0.8813009019934832
running average episode reward sum: 0.0531240814050081
{'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([25.69809079, 21.24917829,  0.        ]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 0.39145752061407457}
episode index:548
target Thresh 17.200515464434698
target distance 14.0
model initialize at round 548
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 26.]), 'previousTarget': array([12., 26.]), 'currentState': array([ 9.03473234, 13.99548578,  0.        ]), 'targetState': array([12, 26], dtype=int32), 'currentDistance': 12.365321422436455}
done in step count: 7
reward sum = 0.8073467571229105
running average episode reward sum: 0.05449789320048697
{'scaleFactor': 20, 'currentTarget': array([12., 26.]), 'previousTarget': array([12., 26.]), 'currentState': array([11.41612965, 25.31696415,  0.        ]), 'targetState': array([12, 26], dtype=int32), 'currentDistance': 0.8985780768782721}
episode index:549
target Thresh 17.21530755169396
target distance 13.0
model initialize at round 549
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([25.01818902, 15.5557383 ,  0.        ]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 16.75997781572762}
done in step count: 21
reward sum = 0.737462503754158
running average episode reward sum: 0.05573964703785729
{'scaleFactor': 20, 'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([12.50023631,  5.36781521,  0.        ]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 0.6209061115683893}
episode index:550
target Thresh 17.23008485425954
target distance 6.0
model initialize at round 550
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([10.57041088, 18.41211307,  0.        ]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 5.1062465496457}
done in step count: 4
reward sum = 0.9252910912945214
running average episode reward sum: 0.05731778033051912
{'scaleFactor': 20, 'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([ 8.62895009, 14.86250746,  0.        ]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 1.067472406011504}
episode index:551
target Thresh 17.244847386908745
target distance 11.0
model initialize at round 551
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([ 5.99998915, 20.00010836,  0.        ]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 9.055407886957074}
done in step count: 5
reward sum = 0.8491608806805877
running average episode reward sum: 0.05875227870071851
{'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([14.26283044, 18.64874265,  0.        ]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 0.8165786517557684}
episode index:552
target Thresh 17.25959516440411
target distance 13.0
model initialize at round 552
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 27.]), 'previousTarget': array([22., 27.]), 'currentState': array([24.4734875 , 15.64460218,  0.        ]), 'targetState': array([22, 27], dtype=int32), 'currentDistance': 11.621669417769692}
done in step count: 7
reward sum = 0.8090339341782233
running average episode reward sum: 0.060109026721473495
{'scaleFactor': 20, 'currentTarget': array([22., 27.]), 'previousTarget': array([22., 27.]), 'currentState': array([21.61373078, 26.37009746,  0.        ]), 'targetState': array([22, 27], dtype=int32), 'currentDistance': 0.7389053565887764}
episode index:553
target Thresh 17.274328201493404
target distance 7.0
model initialize at round 553
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 16.]), 'previousTarget': array([10., 16.]), 'currentState': array([ 6.33620143, 21.36582649,  0.        ]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 6.497346678496582}
done in step count: 3
reward sum = 0.8920912283039522
running average episode reward sum: 0.061610799648517685
{'scaleFactor': 20, 'currentTarget': array([10., 16.]), 'previousTarget': array([10., 16.]), 'currentState': array([ 9.55249113, 16.5716697 ,  0.        ]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 0.7259961662840934}
episode index:554
target Thresh 17.289046512909678
target distance 8.0
model initialize at round 554
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  2.]), 'previousTarget': array([23.,  2.]), 'currentState': array([16.99790657,  2.47560239,  0.        ]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 6.020907176356895}
done in step count: 4
reward sum = 0.8980181463621995
running average episode reward sum: 0.06311783991286665
{'scaleFactor': 20, 'currentTarget': array([23.,  2.]), 'previousTarget': array([23.,  2.]), 'currentState': array([22.167027  ,  1.61858484,  0.        ]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 0.9161449380490703}
episode index:555
target Thresh 17.303750113371237
target distance 7.0
model initialize at round 555
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([11.252056 , 15.2560662,  0.       ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 7.563220463874931}
done in step count: 7
reward sum = 0.8990344632213838
running average episode reward sum: 0.06462128707709061
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.97474051, 11.9315657 ,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.3483077206573466}
episode index:556
target Thresh 17.31843901758169
target distance 8.0
model initialize at round 556
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  2.]), 'previousTarget': array([21.,  2.]), 'currentState': array([23.35948598,  8.17091036,  0.        ]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 6.606610988982448}
done in step count: 4
reward sum = 0.89865079213779
running average episode reward sum: 0.06611864705026961
{'scaleFactor': 20, 'currentTarget': array([21.,  2.]), 'previousTarget': array([21.,  2.]), 'currentState': array([21.47038889,  2.66921347,  0.        ]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 0.8179928977078009}
episode index:557
target Thresh 17.333113240229927
target distance 16.0
model initialize at round 557
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  7.]), 'previousTarget': array([17.,  7.]), 'currentState': array([23.27467078, 21.44322705,  0.        ]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 15.747326791846481}
done in step count: 10
reward sum = 0.7655582954973121
running average episode reward sum: 0.06737212312275535
{'scaleFactor': 20, 'currentTarget': array([17.,  7.]), 'previousTarget': array([17.,  7.]), 'currentState': array([17.06099907,  7.69137615,  0.        ]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 0.6940618605858015}
episode index:558
target Thresh 17.34777279599019
target distance 14.0
model initialize at round 558
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 16.]), 'previousTarget': array([17., 16.]), 'currentState': array([ 4.99999785, 16.46877325,  0.        ]), 'targetState': array([17, 16], dtype=int32), 'currentDistance': 12.009154835141667}
done in step count: 7
reward sum = 0.8047384036399221
running average episode reward sum: 0.06869120412546943
{'scaleFactor': 20, 'currentTarget': array([17., 16.]), 'previousTarget': array([17., 16.]), 'currentState': array([16.60264385, 15.40653026,  0.        ]), 'targetState': array([17, 16], dtype=int32), 'currentDistance': 0.714211623413021}
episode index:559
target Thresh 17.362417699522023
target distance 17.0
model initialize at round 559
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 16.]), 'previousTarget': array([ 5., 16.]), 'currentState': array([20.68913937, 12.47875723,  0.        ]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 16.079435458953764}
done in step count: 12
reward sum = 0.7726607870530419
running average episode reward sum: 0.06994829266641153
{'scaleFactor': 20, 'currentTarget': array([ 5., 16.]), 'previousTarget': array([ 5., 16.]), 'currentState': array([ 5.50375611, 16.18443555,  0.        ]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 0.5364575332741709}
episode index:560
target Thresh 17.37704796547034
target distance 7.0
model initialize at round 560
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 16.]), 'previousTarget': array([ 7., 16.]), 'currentState': array([12.51372147, 14.39872193,  0.        ]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 5.741534276831178}
done in step count: 4
reward sum = 0.9214882168112238
running average episode reward sum: 0.07146618914438803
{'scaleFactor': 20, 'currentTarget': array([ 7., 16.]), 'previousTarget': array([ 7., 16.]), 'currentState': array([ 7.89311928, 15.14756444,  0.        ]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 1.2346288603061806}
episode index:561
target Thresh 17.3916636084654
target distance 15.0
model initialize at round 561
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  4.]), 'previousTarget': array([13.,  4.]), 'currentState': array([17.72098809, 17.15949321,  0.        ]), 'targetState': array([13,  4], dtype=int32), 'currentDistance': 13.980700629113723}
done in step count: 9
reward sum = 0.7916215471933625
running average episode reward sum: 0.07274760437223318
{'scaleFactor': 20, 'currentTarget': array([13.,  4.]), 'previousTarget': array([13.,  4.]), 'currentState': array([13.50625868,  4.65234238,  0.        ]), 'targetState': array([13,  4], dtype=int32), 'currentDistance': 0.8257411410788215}
episode index:562
target Thresh 17.406264643122853
target distance 7.0
model initialize at round 562
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  6.]), 'previousTarget': array([19.,  6.]), 'currentState': array([25.00583321,  8.37763405,  0.        ]), 'targetState': array([19,  6], dtype=int32), 'currentDistance': 6.4593479699935505}
done in step count: 6
reward sum = 0.90655345316988
running average episode reward sum: 0.07422860943226453
{'scaleFactor': 20, 'currentTarget': array([19.,  6.]), 'previousTarget': array([19.,  6.]), 'currentState': array([19.59940666,  6.26350027,  0.        ]), 'targetState': array([19,  6], dtype=int32), 'currentDistance': 0.6547676974071536}
episode index:563
target Thresh 17.420851084043733
target distance 14.0
model initialize at round 563
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 24.]), 'previousTarget': array([ 5., 24.]), 'currentState': array([ 3.23670468, 11.98741162,  0.        ]), 'targetState': array([ 5, 24], dtype=int32), 'currentDistance': 12.141313356662558}
done in step count: 7
reward sum = 0.8127902117670714
running average episode reward sum: 0.07553811581938298
{'scaleFactor': 20, 'currentTarget': array([ 5., 24.]), 'previousTarget': array([ 5., 24.]), 'currentState': array([ 4.60435983, 23.48663139,  0.        ]), 'targetState': array([ 5, 24], dtype=int32), 'currentDistance': 0.6481346060337501}
episode index:564
target Thresh 17.43542294581448
target distance 17.0
model initialize at round 564
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([11.9996736 , 18.63431692,  0.        ]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 21.66660235364942}
done in step count: 10
reward sum = 0.6621205790375156
running average episode reward sum: 0.07657631486932658
{'scaleFactor': 20, 'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([26.01125365,  3.53622639,  0.        ]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 1.1247924600080286}
episode index:565
target Thresh 17.449980243006962
target distance 7.0
model initialize at round 565
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 12.]), 'previousTarget': array([13., 12.]), 'currentState': array([ 7.5198853 , 17.45660114,  0.        ]), 'targetState': array([13, 12], dtype=int32), 'currentDistance': 7.733443808565806}
done in step count: 4
reward sum = 0.869049883663782
running average episode reward sum: 0.0779764448495288
{'scaleFactor': 20, 'currentTarget': array([13., 12.]), 'previousTarget': array([13., 12.]), 'currentState': array([12.42866504, 12.13261294,  0.        ]), 'targetState': array([13, 12], dtype=int32), 'currentDistance': 0.5865235094551191}
episode index:566
target Thresh 17.464522990178473
target distance 6.0
model initialize at round 566
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([6.25904071, 8.82815838, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 5.825157482838553}
done in step count: 5
reward sum = 0.9130876402378771
running average episode reward sum: 0.07944930410065464
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.01589802, 4.63292187, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.6331215098701732}
episode index:567
target Thresh 17.479051201871766
target distance 10.0
model initialize at round 567
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  2.]), 'previousTarget': array([23.,  2.]), 'currentState': array([14.9502629 ,  6.67286086,  0.        ]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 9.307733125560329}
done in step count: 5
reward sum = 0.8395648453673205
running average episode reward sum: 0.08078753568739172
{'scaleFactor': 20, 'currentTarget': array([23.,  2.]), 'previousTarget': array([23.,  2.]), 'currentState': array([22.49292511,  1.7004306 ,  0.        ]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 0.5889539633470057}
episode index:568
target Thresh 17.493564892615048
target distance 1.0
model initialize at round 568
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 13.]), 'previousTarget': array([27., 13.]), 'currentState': array([26.69936877, 12.3959218 ,  0.        ]), 'targetState': array([27, 13], dtype=int32), 'currentDistance': 0.6747515167686616}
done in step count: 0
reward sum = 0.996770646512401
running average episode reward sum: 0.08239734783295412
{'scaleFactor': 20, 'currentTarget': array([27., 13.]), 'previousTarget': array([27., 13.]), 'currentState': array([26.69936877, 12.3959218 ,  0.        ]), 'targetState': array([27, 13], dtype=int32), 'currentDistance': 0.6747515167686616}
episode index:569
target Thresh 17.50806407692201
target distance 7.0
model initialize at round 569
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([16.99261534, 13.45172441,  0.        ]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 5.027718772406796}
done in step count: 3
reward sum = 0.913401913942731
running average episode reward sum: 0.08385525058051513
{'scaleFactor': 20, 'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([21.44121164, 12.74343809,  0.        ]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 0.6148727035011141}
episode index:570
target Thresh 17.522548769291852
target distance 4.0
model initialize at round 570
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 20.]), 'previousTarget': array([19., 20.]), 'currentState': array([18.23954704, 22.15262175,  0.        ]), 'targetState': array([19, 20], dtype=int32), 'currentDistance': 2.2829956372834257}
done in step count: 1
reward sum = 0.9643031397621372
running average episode reward sum: 0.08539719084177892
{'scaleFactor': 20, 'currentTarget': array([19., 20.]), 'previousTarget': array([19., 20.]), 'currentState': array([18.61678174, 20.91014957,  0.        ]), 'targetState': array([19, 20], dtype=int32), 'currentDistance': 0.9875365721336283}
episode index:571
target Thresh 17.537018984209247
target distance 12.0
model initialize at round 571
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 21.]), 'previousTarget': array([24., 21.]), 'currentState': array([24.87070942, 10.8658452 ,  0.        ]), 'targetState': array([24, 21], dtype=int32), 'currentDistance': 10.17149095925614}
done in step count: 6
reward sum = 0.8376374753284156
running average episode reward sum: 0.08671229623423807
{'scaleFactor': 20, 'currentTarget': array([24., 21.]), 'previousTarget': array([24., 21.]), 'currentState': array([23.7219231 , 20.36698836,  0.        ]), 'targetState': array([24, 21], dtype=int32), 'currentDistance': 0.6913974961386823}
episode index:572
target Thresh 17.551474736144424
target distance 5.0
model initialize at round 572
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 29.]), 'previousTarget': array([ 5., 29.]), 'currentState': array([ 7.21612412, 25.1345861 ,  0.        ]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 4.455629108702285}
done in step count: 4
reward sum = 0.9317738406800674
running average episode reward sum: 0.08818709823152572
{'scaleFactor': 20, 'currentTarget': array([ 5., 29.]), 'previousTarget': array([ 5., 29.]), 'currentState': array([ 5.01635051, 28.34424818,  0.        ]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 0.6559556343834958}
episode index:573
target Thresh 17.56591603955313
target distance 11.0
model initialize at round 573
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([23.12969732, 25.15623426,  0.        ]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 9.225662141295869}
done in step count: 6
reward sum = 0.8506212919973845
running average episode reward sum: 0.08951538079906207
{'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([22.46711795, 16.66831601,  0.        ]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 0.8153805662087859}
episode index:574
target Thresh 17.58034290887668
target distance 11.0
model initialize at round 574
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 29.]), 'previousTarget': array([22., 29.]), 'currentState': array([17.0651226 , 19.90204287,  0.        ]), 'targetState': array([22, 29], dtype=int32), 'currentDistance': 10.350161299539778}
done in step count: 6
reward sum = 0.837938847881275
running average episode reward sum: 0.09081698682877026
{'scaleFactor': 20, 'currentTarget': array([22., 29.]), 'previousTarget': array([22., 29.]), 'currentState': array([21.16769707, 28.30406812,  0.        ]), 'targetState': array([22, 29], dtype=int32), 'currentDistance': 1.0849190512789784}
episode index:575
target Thresh 17.59475535854193
target distance 14.0
model initialize at round 575
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 12.]), 'previousTarget': array([ 4., 12.]), 'currentState': array([10.98705649, 24.57838297,  0.        ]), 'targetState': array([ 4, 12], dtype=int32), 'currentDistance': 14.388699613215943}
done in step count: 9
reward sum = 0.7840085434638446
running average episode reward sum: 0.09202044439237282
{'scaleFactor': 20, 'currentTarget': array([ 4., 12.]), 'previousTarget': array([ 4., 12.]), 'currentState': array([ 4.28189147, 12.51018918,  0.        ]), 'targetState': array([ 4, 12], dtype=int32), 'currentDistance': 0.5828857461377978}
episode index:576
target Thresh 17.609153402961347
target distance 16.0
model initialize at round 576
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 27.]), 'previousTarget': array([21., 27.]), 'currentState': array([ 7.        , 24.94418864,  0.        ]), 'targetState': array([21, 27], dtype=int32), 'currentDistance': 14.150136406923986}
done in step count: 8
reward sum = 0.7770574097388053
running average episode reward sum: 0.09320768350042555
{'scaleFactor': 20, 'currentTarget': array([21., 27.]), 'previousTarget': array([21., 27.]), 'currentState': array([20.0313977 , 26.58255204,  0.        ]), 'targetState': array([21, 27], dtype=int32), 'currentDistance': 1.054728977430928}
episode index:577
target Thresh 17.62353705653296
target distance 9.0
model initialize at round 577
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'previousTarget': array([26., 11.]), 'currentState': array([18.98712933,  6.72287011,  0.        ]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 8.214267776779243}
done in step count: 6
reward sum = 0.8651421639853942
running average episode reward sum: 0.0945432102832715
{'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'previousTarget': array([26., 11.]), 'currentState': array([25.3038063 , 10.61859965,  0.        ]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 0.793821066461374}
episode index:578
target Thresh 17.63790633364043
target distance 15.0
model initialize at round 578
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 28.]), 'previousTarget': array([17., 28.]), 'currentState': array([ 8.32661259, 14.75459135,  0.        ]), 'targetState': array([17, 28], dtype=int32), 'currentDistance': 15.83251399970339}
done in step count: 12
reward sum = 0.7450100050790447
running average episode reward sum: 0.09566664170778924
{'scaleFactor': 20, 'currentTarget': array([17., 28.]), 'previousTarget': array([17., 28.]), 'currentState': array([16.32696736, 27.63445944,  0.        ]), 'targetState': array([17, 28], dtype=int32), 'currentDistance': 0.7658934915368266}
episode index:579
target Thresh 17.652261248653044
target distance 12.0
model initialize at round 579
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 17.]), 'previousTarget': array([ 7., 17.]), 'currentState': array([18.27884877, 24.27788776,  0.        ]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 13.423117364663664}
done in step count: 12
reward sum = 0.816855474213553
running average episode reward sum: 0.09691007072935091
{'scaleFactor': 20, 'currentTarget': array([ 7., 17.]), 'previousTarget': array([ 7., 17.]), 'currentState': array([ 7.69848067, 17.99643077,  0.        ]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 1.2168605174251028}
episode index:580
target Thresh 17.66660181592571
target distance 9.0
model initialize at round 580
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 10.]), 'previousTarget': array([18., 10.]), 'currentState': array([17.61114264, 17.15460026,  0.        ]), 'targetState': array([18, 10], dtype=int32), 'currentDistance': 7.165159800607394}
done in step count: 4
reward sum = 0.8877717243174211
running average episode reward sum: 0.09827127839473487
{'scaleFactor': 20, 'currentTarget': array([18., 10.]), 'previousTarget': array([18., 10.]), 'currentState': array([17.89996634, 10.74012554,  0.        ]), 'targetState': array([18, 10], dtype=int32), 'currentDistance': 0.7468551015319499}
episode index:581
target Thresh 17.680928049798993
target distance 6.0
model initialize at round 581
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 18.]), 'previousTarget': array([10., 18.]), 'currentState': array([ 5.83303308, 14.58501309,  0.        ]), 'targetState': array([10, 18], dtype=int32), 'currentDistance': 5.387554998009897}
done in step count: 4
reward sum = 0.916723853462248
running average episode reward sum: 0.09967755429691273
{'scaleFactor': 20, 'currentTarget': array([10., 18.]), 'previousTarget': array([10., 18.]), 'currentState': array([ 9.37499917, 17.30178294,  0.        ]), 'targetState': array([10, 18], dtype=int32), 'currentDistance': 0.9370875688758277}
episode index:582
target Thresh 17.695239964599132
target distance 11.0
model initialize at round 582
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 14.]), 'previousTarget': array([20., 14.]), 'currentState': array([10.98467875, 20.87238073,  0.        ]), 'targetState': array([20, 14], dtype=int32), 'currentDistance': 11.336032563116424}
done in step count: 5
reward sum = 0.8169481638280802
running average episode reward sum: 0.10090786409027665
{'scaleFactor': 20, 'currentTarget': array([20., 14.]), 'previousTarget': array([20., 14.]), 'currentState': array([19.10367703, 14.29944682,  0.        ]), 'targetState': array([20, 14], dtype=int32), 'currentDistance': 0.9450202418126656}
episode index:583
target Thresh 17.70953757463805
target distance 8.0
model initialize at round 583
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 20.]), 'previousTarget': array([ 2., 20.]), 'currentState': array([ 4.7070111 , 26.23179364,  0.        ]), 'targetState': array([ 2, 20], dtype=int32), 'currentDistance': 6.794347732652306}
done in step count: 6
reward sum = 0.8908504499173583
running average episode reward sum: 0.10226050550436411
{'scaleFactor': 20, 'currentTarget': array([ 2., 20.]), 'previousTarget': array([ 2., 20.]), 'currentState': array([ 2.25528413, 20.68682763,  0.        ]), 'targetState': array([ 2, 20], dtype=int32), 'currentDistance': 0.7327360919086349}
episode index:584
target Thresh 17.723820894213343
target distance 6.0
model initialize at round 584
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 27.]), 'previousTarget': array([11., 27.]), 'currentState': array([15.44795883, 23.70945001,  0.        ]), 'targetState': array([11, 27], dtype=int32), 'currentDistance': 5.532816370229252}
done in step count: 4
reward sum = 0.9217174916970979
running average episode reward sum: 0.10366128667734313
{'scaleFactor': 20, 'currentTarget': array([11., 27.]), 'previousTarget': array([11., 27.]), 'currentState': array([11.79393595, 26.06048572,  0.        ]), 'targetState': array([11, 27], dtype=int32), 'currentDistance': 1.2300493409179565}
episode index:585
target Thresh 17.738089937608347
target distance 9.0
model initialize at round 585
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 14.]), 'previousTarget': array([12., 14.]), 'currentState': array([ 9.18247008, 21.36262786,  0.        ]), 'targetState': array([12, 14], dtype=int32), 'currentDistance': 7.883321881916063}
done in step count: 4
reward sum = 0.8739232831437055
running average episode reward sum: 0.10497572694435056
{'scaleFactor': 20, 'currentTarget': array([12., 14.]), 'previousTarget': array([12., 14.]), 'currentState': array([11.78234971, 14.83188534,  0.        ]), 'targetState': array([12, 14], dtype=int32), 'currentDistance': 0.8598865424614238}
episode index:586
target Thresh 17.7523447190921
target distance 10.0
model initialize at round 586
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([17.29048443, 14.94891495,  0.        ]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 10.263905806220363}
done in step count: 7
reward sum = 0.84826439273065
running average episode reward sum: 0.106241976800886
{'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([ 9.73987365, 20.16152394,  0.        ]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 1.1182375064325463}
episode index:587
target Thresh 17.76658525291938
target distance 5.0
model initialize at round 587
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 18.]), 'previousTarget': array([ 3., 18.]), 'currentState': array([ 2.11219686, 21.06571651,  0.        ]), 'targetState': array([ 3, 18], dtype=int32), 'currentDistance': 3.1916785729727213}
done in step count: 2
reward sum = 0.9409878826953064
running average episode reward sum: 0.10766161269526425
{'scaleFactor': 20, 'currentTarget': array([ 3., 18.]), 'previousTarget': array([ 3., 18.]), 'currentState': array([ 2.18538152, 18.28055871,  0.        ]), 'targetState': array([ 3, 18], dtype=int32), 'currentDistance': 0.8615778828088358}
episode index:588
target Thresh 17.780811553330732
target distance 12.0
model initialize at round 588
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  3.]), 'previousTarget': array([20.,  3.]), 'currentState': array([11.93414927, 13.4092629 ,  0.        ]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 13.168549734272933}
done in step count: 6
reward sum = 0.7914815300666581
running average episode reward sum: 0.10882259727484217
{'scaleFactor': 20, 'currentTarget': array([20.,  3.]), 'previousTarget': array([20.,  3.]), 'currentState': array([19.23223913,  3.89019787,  0.        ]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 1.1755462607509952}
episode index:589
target Thresh 17.79502363455245
target distance 2.0
model initialize at round 589
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 16.]), 'previousTarget': array([21., 16.]), 'currentState': array([20.1232543, 15.3969534,  0.       ]), 'targetState': array([21, 16], dtype=int32), 'currentDistance': 1.064118518278141}
done in step count: 0
reward sum = 0.9929036391158431
running average episode reward sum: 0.11032103971864046
{'scaleFactor': 20, 'currentTarget': array([21., 16.]), 'previousTarget': array([21., 16.]), 'currentState': array([20.1232543, 15.3969534,  0.       ]), 'targetState': array([21, 16], dtype=int32), 'currentDistance': 1.064118518278141}
episode index:590
target Thresh 17.80922151079662
target distance 2.0
model initialize at round 590
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([ 7.85401243, 22.95105606,  0.        ]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 1.5535671513812623}
done in step count: 1
reward sum = 0.9794788263653482
running average episode reward sum: 0.111791695872019
{'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([ 8.37288636, 23.40483016,  0.        ]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 0.8645800477553556}
episode index:591
target Thresh 17.823405196261117
target distance 12.0
model initialize at round 591
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'previousTarget': array([22., 18.]), 'currentState': array([11.99913645,  9.31279778,  0.        ]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 13.247065868156406}
done in step count: 7
reward sum = 0.7849993502002388
running average episode reward sum: 0.11292887096378963
{'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'previousTarget': array([22., 18.]), 'currentState': array([21.34927839, 17.42823958,  0.        ]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 0.866226636931525}
episode index:592
target Thresh 17.837574705129626
target distance 6.0
model initialize at round 592
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 24.]), 'previousTarget': array([11., 24.]), 'currentState': array([12.35941595, 19.514094  ,  0.        ]), 'targetState': array([11, 24], dtype=int32), 'currentDistance': 4.687362201237428}
done in step count: 4
reward sum = 0.923974151867848
running average episode reward sum: 0.11429656958251487
{'scaleFactor': 20, 'currentTarget': array([11., 24.]), 'previousTarget': array([11., 24.]), 'currentState': array([10.96636346, 23.44821817,  0.        ]), 'targetState': array([11, 24], dtype=int32), 'currentDistance': 0.5528061217583253}
episode index:593
target Thresh 17.851730051571664
target distance 9.0
model initialize at round 593
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 20.]), 'previousTarget': array([13., 20.]), 'currentState': array([16.75240874, 27.37155128,  0.        ]), 'targetState': array([13, 20], dtype=int32), 'currentDistance': 8.27165881655632}
done in step count: 6
reward sum = 0.8722921959734055
running average episode reward sum: 0.11557265649563084
{'scaleFactor': 20, 'currentTarget': array([13., 20.]), 'previousTarget': array([13., 20.]), 'currentState': array([13.4427633 , 20.78747422,  0.        ]), 'targetState': array([13, 20], dtype=int32), 'currentDistance': 0.903412960975065}
episode index:594
target Thresh 17.865871249742575
target distance 7.0
model initialize at round 594
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  3.]), 'previousTarget': array([21.,  3.]), 'currentState': array([25.14919037,  8.9146024 ,  0.        ]), 'targetState': array([21,  3], dtype=int32), 'currentDistance': 7.224839253040783}
done in step count: 6
reward sum = 0.900043117928375
running average episode reward sum: 0.11689109424593798
{'scaleFactor': 20, 'currentTarget': array([21.,  3.]), 'previousTarget': array([21.,  3.]), 'currentState': array([21.55466041,  3.86009771,  0.        ]), 'targetState': array([21,  3], dtype=int32), 'currentDistance': 1.0234335515751352}
episode index:595
target Thresh 17.879998313783556
target distance 1.0
model initialize at round 595
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([22.52233863,  2.90567667,  0.        ]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 0.4868852802030276}
done in step count: 0
reward sum = 0.9985913274321602
running average episode reward sum: 0.11837045705329742
{'scaleFactor': 20, 'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([22.52233863,  2.90567667,  0.        ]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 0.4868852802030276}
episode index:596
target Thresh 17.894111257821677
target distance 6.0
model initialize at round 596
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 28.]), 'previousTarget': array([24., 28.]), 'currentState': array([19.95797098, 28.56109318,  0.        ]), 'targetState': array([24, 28], dtype=int32), 'currentDistance': 4.080787201304957}
done in step count: 3
reward sum = 0.927561302492587
running average episode reward sum: 0.11972588560512203
{'scaleFactor': 20, 'currentTarget': array([24., 28.]), 'previousTarget': array([24., 28.]), 'currentState': array([23.04875004, 27.62194386,  0.        ]), 'targetState': array([24, 28], dtype=int32), 'currentDistance': 1.0236224509485963}
episode index:597
target Thresh 17.908210095969878
target distance 8.0
model initialize at round 597
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 6.]), 'previousTarget': array([7., 6.]), 'currentState': array([14.13794851, 12.04419196,  0.        ]), 'targetState': array([7, 6], dtype=int32), 'currentDistance': 9.353211500973572}
done in step count: 10
reward sum = 0.8591432666984207
running average episode reward sum: 0.12096236951999377
{'scaleFactor': 20, 'currentTarget': array([7., 6.]), 'previousTarget': array([7., 6.]), 'currentState': array([7.62463945, 6.73812583, 0.        ]), 'targetState': array([7, 6], dtype=int32), 'currentDistance': 0.9669561451277721}
episode index:598
target Thresh 17.922294842327
target distance 15.0
model initialize at round 598
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 12.]), 'previousTarget': array([ 3., 12.]), 'currentState': array([ 4.58765006, 25.70677578,  0.        ]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 13.798417848486798}
done in step count: 8
reward sum = 0.7914413332427536
running average episode reward sum: 0.12208170001034897
{'scaleFactor': 20, 'currentTarget': array([ 3., 12.]), 'previousTarget': array([ 3., 12.]), 'currentState': array([ 3.48477777, 12.85661727,  0.        ]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 0.9842777238724897}
episode index:599
target Thresh 17.93636551097779
target distance 11.0
model initialize at round 599
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 16.]), 'previousTarget': array([ 5., 16.]), 'currentState': array([14.26568496,  9.14623737,  0.        ]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 11.525058783473511}
done in step count: 8
reward sum = 0.8226529217347222
running average episode reward sum: 0.12324931871322295
{'scaleFactor': 20, 'currentTarget': array([ 5., 16.]), 'previousTarget': array([ 5., 16.]), 'currentState': array([ 5.33813602, 15.1418891 ,  0.        ]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 0.9223287325920859}
episode index:600
target Thresh 17.95042211599292
target distance 9.0
model initialize at round 600
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 29.]), 'previousTarget': array([ 8., 29.]), 'currentState': array([15.47022891, 29.11939808,  0.        ]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 7.471183030546509}
done in step count: 7
reward sum = 0.8872903097228146
running average episode reward sum: 0.1245206015601607
{'scaleFactor': 20, 'currentTarget': array([ 8., 29.]), 'previousTarget': array([ 8., 29.]), 'currentState': array([ 8.73368752, 29.01601766,  0.        ]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 0.7338623456969858}
episode index:601
target Thresh 17.964464671429
target distance 16.0
model initialize at round 601
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 29.]), 'previousTarget': array([22., 29.]), 'currentState': array([ 7.99999583, 14.67838454,  0.        ]), 'targetState': array([22, 29], dtype=int32), 'currentDistance': 20.02770047056382}
done in step count: 10
reward sum = 0.6734091117312537
running average episode reward sum: 0.12543237649399974
{'scaleFactor': 20, 'currentTarget': array([22., 29.]), 'previousTarget': array([22., 29.]), 'currentState': array([21.25282317, 28.42624897,  0.        ]), 'targetState': array([22, 29], dtype=int32), 'currentDistance': 0.9420527885616351}
episode index:602
target Thresh 17.978493191328575
target distance 8.0
model initialize at round 602
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 17.]), 'previousTarget': array([24., 17.]), 'currentState': array([22.87880216, 10.9000653 ,  0.        ]), 'targetState': array([24, 17], dtype=int32), 'currentDistance': 6.202119630461204}
done in step count: 4
reward sum = 0.900352127545651
running average episode reward sum: 0.12671748387551157
{'scaleFactor': 20, 'currentTarget': array([24., 17.]), 'previousTarget': array([24., 17.]), 'currentState': array([23.69594231, 16.52115464,  0.        ]), 'targetState': array([24, 17], dtype=int32), 'currentDistance': 0.5672247827081734}
episode index:603
target Thresh 17.992507689720178
target distance 6.0
model initialize at round 603
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([19.33303887, 24.44262046,  0.        ]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 6.34767194783935}
done in step count: 7
reward sum = 0.9083681408865735
running average episode reward sum: 0.12801160747983453
{'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([14.66906774, 21.81724808,  0.        ]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 1.056194139043828}
episode index:604
target Thresh 18.006508180618297
target distance 9.0
model initialize at round 604
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([4.33717144, 3.84144592, 0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 7.16649014876031}
done in step count: 5
reward sum = 0.8846523528877683
running average episode reward sum: 0.12926225334001293
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.80809875, 10.4442938 ,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.5879077091080578}
episode index:605
target Thresh 18.02049467802344
target distance 3.0
model initialize at round 605
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 24.]), 'previousTarget': array([12., 24.]), 'currentState': array([ 9.87432784, 22.46948338,  0.        ]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 2.6193440545741}
done in step count: 2
reward sum = 0.9659063495403561
running average episode reward sum: 0.13064285415882537
{'scaleFactor': 20, 'currentTarget': array([12., 24.]), 'previousTarget': array([12., 24.]), 'currentState': array([11.45401913, 23.45477536,  0.        ]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 0.7715989993271059}
episode index:606
target Thresh 18.03446719592209
target distance 4.0
model initialize at round 606
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 23.]), 'previousTarget': array([23., 23.]), 'currentState': array([23.02491872, 20.45289683,  0.        ]), 'targetState': array([23, 23], dtype=int32), 'currentDistance': 2.54722505558796}
done in step count: 2
reward sum = 0.9606329821046243
running average episode reward sum: 0.13201021845527647
{'scaleFactor': 20, 'currentTarget': array([23., 23.]), 'previousTarget': array([23., 23.]), 'currentState': array([22.90072211, 22.31179035,  0.        ]), 'targetState': array([23, 23], dtype=int32), 'currentDistance': 0.6953334632431635}
episode index:607
target Thresh 18.04842574828677
target distance 15.0
model initialize at round 607
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 24.]), 'previousTarget': array([27., 24.]), 'currentState': array([13.99990463, 27.41028678,  0.        ]), 'targetState': array([27, 24], dtype=int32), 'currentDistance': 13.439960398476476}
done in step count: 8
reward sum = 0.7749178813257658
running average episode reward sum: 0.13306763237447133
{'scaleFactor': 20, 'currentTarget': array([27., 24.]), 'previousTarget': array([27., 24.]), 'currentState': array([26.2002027 , 23.66722668,  0.        ]), 'targetState': array([27, 24], dtype=int32), 'currentDistance': 0.8662642773899754}
episode index:608
target Thresh 18.06237034907604
target distance 8.0
model initialize at round 608
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([11.98654354, 25.45387542,  0.        ]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 7.122740190977309}
done in step count: 4
reward sum = 0.8837891860770519
running average episode reward sum: 0.134300344285313
{'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([14.6880157 , 19.33596867,  0.        ]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 0.45848571757528556}
episode index:609
target Thresh 18.076301012234495
target distance 2.0
model initialize at round 609
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 22.]), 'previousTarget': array([11., 22.]), 'currentState': array([10.42876211, 20.93758303,  0.        ]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 1.206251445118901}
done in step count: 1
reward sum = 0.9833693807917535
running average episode reward sum: 0.13569226073860227
{'scaleFactor': 20, 'currentTarget': array([11., 22.]), 'previousTarget': array([11., 22.]), 'currentState': array([10.63398921, 21.41062176,  0.        ]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 0.6937799376708145}
episode index:610
target Thresh 18.090217751692805
target distance 13.0
model initialize at round 610
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 17.]), 'previousTarget': array([13., 17.]), 'currentState': array([14.57578444,  5.7974906 ,  0.        ]), 'targetState': array([13, 17], dtype=int32), 'currentDistance': 11.312794241297322}
done in step count: 7
reward sum = 0.8161803257076063
running average episode reward sum: 0.13680598915917347
{'scaleFactor': 20, 'currentTarget': array([13., 17.]), 'previousTarget': array([13., 17.]), 'currentState': array([12.88117759, 16.39819556,  0.        ]), 'targetState': array([13, 17], dtype=int32), 'currentDistance': 0.6134226474955684}
episode index:611
target Thresh 18.104120581367706
target distance 15.0
model initialize at round 611
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 26.]), 'previousTarget': array([23., 26.]), 'currentState': array([ 9.9999994 , 27.59931886,  0.        ]), 'targetState': array([23, 26], dtype=int32), 'currentDistance': 13.098008868504891}
done in step count: 8
reward sum = 0.7897484978384481
running average episode reward sum: 0.1378728886831592
{'scaleFactor': 20, 'currentTarget': array([23., 26.]), 'previousTarget': array([23., 26.]), 'currentState': array([22.28191966, 25.58280478,  0.        ]), 'targetState': array([23, 26], dtype=int32), 'currentDistance': 0.8304765050300327}
episode index:612
target Thresh 18.11800951516203
target distance 7.0
model initialize at round 612
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 19.]), 'previousTarget': array([ 5., 19.]), 'currentState': array([10.24357116, 12.92308402,  0.        ]), 'targetState': array([ 5, 19], dtype=int32), 'currentDistance': 8.026452912327713}
done in step count: 6
reward sum = 0.8697237932792733
running average episode reward sum: 0.13906677270370751
{'scaleFactor': 20, 'currentTarget': array([ 5., 19.]), 'previousTarget': array([ 5., 19.]), 'currentState': array([ 5.13180375, 18.505243  ,  0.        ]), 'targetState': array([ 5, 19], dtype=int32), 'currentDistance': 0.512012416344746}
episode index:613
target Thresh 18.13188456696471
target distance 13.0
model initialize at round 613
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([4.99999082, 2.91296029, 0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 13.08534805723013}
done in step count: 7
reward sum = 0.7881912464362916
running average episode reward sum: 0.14012397868698537
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.32494378,  9.29290517,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.9775909175942378}
episode index:614
target Thresh 18.145745750650804
target distance 12.0
model initialize at round 614
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 4.]), 'previousTarget': array([8., 4.]), 'currentState': array([18.29312587,  5.39622945,  0.        ]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 10.387391241161762}
done in step count: 9
reward sum = 0.8446409837203748
running average episode reward sum: 0.14126953479273072
{'scaleFactor': 20, 'currentTarget': array([8., 4.]), 'previousTarget': array([8., 4.]), 'currentState': array([8.78914285, 4.61345628, 0.        ]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 0.9995374126867812}
episode index:615
target Thresh 18.159593080081493
target distance 15.0
model initialize at round 615
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 26.]), 'previousTarget': array([ 4., 26.]), 'currentState': array([17.1414957, 12.6758709,  0.       ]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 18.714468353436814}
done in step count: 11
reward sum = 0.6935864392167473
running average episode reward sum: 0.1421661531440684
{'scaleFactor': 20, 'currentTarget': array([ 4., 26.]), 'previousTarget': array([ 4., 26.]), 'currentState': array([ 3.96140131, 25.5702858 ,  0.        ]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 0.43144426610346176}
episode index:616
target Thresh 18.17342656910411
target distance 11.0
model initialize at round 616
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 23.]), 'previousTarget': array([11., 23.]), 'currentState': array([20.28387833, 22.63225132,  0.        ]), 'targetState': array([11, 23], dtype=int32), 'currentDistance': 9.291159016488173}
done in step count: 8
reward sum = 0.8568786046101858
running average episode reward sum: 0.1433245201642728
{'scaleFactor': 20, 'currentTarget': array([11., 23.]), 'previousTarget': array([11., 23.]), 'currentState': array([11.75315368, 23.4522008 ,  0.        ]), 'targetState': array([11, 23], dtype=int32), 'currentDistance': 0.8784793852678142}
episode index:617
target Thresh 18.187246231552145
target distance 11.0
model initialize at round 617
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  8.]), 'previousTarget': array([17.,  8.]), 'currentState': array([7.9999944 , 4.78726828, 0.        ]), 'targetState': array([17,  8], dtype=int32), 'currentDistance': 9.556241204023067}
done in step count: 6
reward sum = 0.8457674683504015
running average episode reward sum: 0.14446115923900765
{'scaleFactor': 20, 'currentTarget': array([17.,  8.]), 'previousTarget': array([17.,  8.]), 'currentState': array([16.35590857,  7.47457379,  0.        ]), 'targetState': array([17,  8], dtype=int32), 'currentDistance': 0.8312198673220452}
episode index:618
target Thresh 18.20105208124526
target distance 9.0
model initialize at round 618
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 13.]), 'previousTarget': array([21., 13.]), 'currentState': array([18.56071383,  5.96829247,  0.        ]), 'targetState': array([21, 13], dtype=int32), 'currentDistance': 7.442783602357161}
done in step count: 4
reward sum = 0.8862464949752393
running average episode reward sum: 0.14565952003987392
{'scaleFactor': 20, 'currentTarget': array([21., 13.]), 'previousTarget': array([21., 13.]), 'currentState': array([20.39128315, 12.15347874,  0.        ]), 'targetState': array([21, 13], dtype=int32), 'currentDistance': 1.0426573928461567}
episode index:619
target Thresh 18.214844131989302
target distance 5.0
model initialize at round 619
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 10.]), 'previousTarget': array([25., 10.]), 'currentState': array([23.68269667, 13.07401502,  0.        ]), 'targetState': array([25, 10], dtype=int32), 'currentDistance': 3.3443768348947938}
done in step count: 3
reward sum = 0.9285026491721304
running average episode reward sum: 0.14692217024815177
{'scaleFactor': 20, 'currentTarget': array([25., 10.]), 'previousTarget': array([25., 10.]), 'currentState': array([24.38194233, 10.03815277,  0.        ]), 'targetState': array([25, 10], dtype=int32), 'currentDistance': 0.6192341358729317}
episode index:620
target Thresh 18.22862239757633
target distance 5.0
model initialize at round 620
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([16.88524985, 19.13751161,  0.        ]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 4.821149579746235}
done in step count: 4
reward sum = 0.9268822781613152
running average episode reward sum: 0.14817814465702964
{'scaleFactor': 20, 'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([14.40071553, 22.39311683,  0.        ]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 0.7272414425671834}
episode index:621
target Thresh 18.242386891784612
target distance 8.0
model initialize at round 621
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 10.]), 'previousTarget': array([17., 10.]), 'currentState': array([11.92744339, 16.69701147,  0.        ]), 'targetState': array([17, 10], dtype=int32), 'currentDistance': 8.401237602346155}
done in step count: 4
reward sum = 0.8565191855183709
running average episode reward sum: 0.1493169566198292
{'scaleFactor': 20, 'currentTarget': array([17., 10.]), 'previousTarget': array([17., 10.]), 'currentState': array([16.02893299, 10.20726025,  0.        ]), 'targetState': array([17, 10], dtype=int32), 'currentDistance': 0.9929390475792791}
episode index:622
target Thresh 18.256137628378635
target distance 5.0
model initialize at round 622
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([14.50411975, 11.133371  ,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 3.472127806753988}
done in step count: 3
reward sum = 0.9237441625271275
running average episode reward sum: 0.15056001794552307
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.47555476,  7.85557485,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.5439682254595958}
episode index:623
target Thresh 18.26987462110914
target distance 17.0
model initialize at round 623
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  3.]), 'previousTarget': array([22.,  3.]), 'currentState': array([13.99794936, 18.48723352,  0.        ]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 17.432361185608304}
done in step count: 9
reward sum = 0.7115070130012765
running average episode reward sum: 0.15145897146324064
{'scaleFactor': 20, 'currentTarget': array([22.,  3.]), 'previousTarget': array([22.,  3.]), 'currentState': array([21.94777898,  3.92238832,  0.        ]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 0.9238653789577802}
episode index:624
target Thresh 18.283597883713124
target distance 16.0
model initialize at round 624
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 23.]), 'previousTarget': array([11., 23.]), 'currentState': array([25.22906089, 17.3367573 ,  0.        ]), 'targetState': array([11, 23], dtype=int32), 'currentDistance': 15.31464957535443}
done in step count: 10
reward sum = 0.764741479455333
running average episode reward sum: 0.152440223476028
{'scaleFactor': 20, 'currentTarget': array([11., 23.]), 'previousTarget': array([11., 23.]), 'currentState': array([11.73876959, 22.84947692,  0.        ]), 'targetState': array([11, 23], dtype=int32), 'currentDistance': 0.75394807938742}
episode index:625
target Thresh 18.29730742991385
target distance 14.0
model initialize at round 625
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([19.978782  , 14.21543109,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 12.847079940221684}
done in step count: 8
reward sum = 0.8166497362490085
running average episode reward sum: 0.15350126103636824
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.60955331,  2.83246678,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.0317733151536232}
episode index:626
target Thresh 18.311003273420862
target distance 5.0
model initialize at round 626
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 26.]), 'previousTarget': array([24., 26.]), 'currentState': array([20.78670645, 25.54116404,  0.        ]), 'targetState': array([24, 26], dtype=int32), 'currentDistance': 3.2458875352600676}
done in step count: 3
reward sum = 0.9392377327783459
running average episode reward sum: 0.15475442925286262
{'scaleFactor': 20, 'currentTarget': array([24., 26.]), 'previousTarget': array([24., 26.]), 'currentState': array([23.48930508, 25.69181001,  0.        ]), 'targetState': array([24, 26], dtype=int32), 'currentDistance': 0.5964816593924744}
episode index:627
target Thresh 18.324685427930007
target distance 13.0
model initialize at round 627
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([13.60740924, 19.68241405,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 11.924902041524069}
done in step count: 7
reward sum = 0.8250107167679197
running average episode reward sum: 0.15582171633489297
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.38240694,  8.2654615 ,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.6722283869101469}
episode index:628
target Thresh 18.33835390712344
target distance 4.0
model initialize at round 628
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 27.]), 'previousTarget': array([18., 27.]), 'currentState': array([19.43819499, 24.14024734,  0.        ]), 'targetState': array([18, 27], dtype=int32), 'currentDistance': 3.201029534019237}
done in step count: 3
reward sum = 0.9491381537722913
running average episode reward sum: 0.15708295073463446
{'scaleFactor': 20, 'currentTarget': array([18., 27.]), 'previousTarget': array([18., 27.]), 'currentState': array([18.09332404, 26.56326324,  0.        ]), 'targetState': array([18, 27], dtype=int32), 'currentDistance': 0.4465964345180706}
episode index:629
target Thresh 18.35200872466964
target distance 13.0
model initialize at round 629
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 18.]), 'previousTarget': array([24., 18.]), 'currentState': array([20.83782971,  6.99465752,  0.        ]), 'targetState': array([24, 18], dtype=int32), 'currentDistance': 11.450628110655304}
done in step count: 6
reward sum = 0.8235645227687758
running average episode reward sum: 0.15814085799183153
{'scaleFactor': 20, 'currentTarget': array([24., 18.]), 'previousTarget': array([24., 18.]), 'currentState': array([23.56171325, 17.27851915,  0.        ]), 'targetState': array([24, 18], dtype=int32), 'currentDistance': 0.8441740864760553}
episode index:630
target Thresh 18.36564989422343
target distance 9.0
model initialize at round 630
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  6.]), 'previousTarget': array([10.,  6.]), 'currentState': array([18.00966716, 12.13348824,  0.        ]), 'targetState': array([10,  6], dtype=int32), 'currentDistance': 10.088332168793515}
done in step count: 11
reward sum = 0.8475111776411165
running average episode reward sum: 0.15923336246037237
{'scaleFactor': 20, 'currentTarget': array([10.,  6.]), 'previousTarget': array([10.,  6.]), 'currentState': array([10.48432267,  6.69938138,  0.        ]), 'targetState': array([10,  6], dtype=int32), 'currentDistance': 0.8507072130643059}
episode index:631
target Thresh 18.37927742942598
target distance 15.0
model initialize at round 631
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 28.]), 'previousTarget': array([12., 28.]), 'currentState': array([25.15439606, 17.52730894,  0.        ]), 'targetState': array([12, 28], dtype=int32), 'currentDistance': 16.814142667630204}
done in step count: 10
reward sum = 0.7404143428801094
running average episode reward sum: 0.16015295261926438
{'scaleFactor': 20, 'currentTarget': array([12., 28.]), 'previousTarget': array([12., 28.]), 'currentState': array([12.46130145, 27.17974526,  0.        ]), 'targetState': array([12, 28], dtype=int32), 'currentDistance': 0.9410721904417649}
episode index:632
target Thresh 18.39289134390482
target distance 12.0
model initialize at round 632
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  7.]), 'previousTarget': array([17.,  7.]), 'currentState': array([16.69748926, 17.29296982,  0.        ]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 10.297414263863834}
done in step count: 6
reward sum = 0.8435335270527554
running average episode reward sum: 0.1612325427842462
{'scaleFactor': 20, 'currentTarget': array([17.,  7.]), 'previousTarget': array([17.,  7.]), 'currentState': array([17.2121415 ,  7.65407234,  0.        ]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 0.6876151894254748}
episode index:633
target Thresh 18.406491651273875
target distance 7.0
model initialize at round 633
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 26.]), 'previousTarget': array([16., 26.]), 'currentState': array([21.76736307, 26.28216141,  0.        ]), 'targetState': array([16, 26], dtype=int32), 'currentDistance': 5.774261152886263}
done in step count: 6
reward sum = 0.909253727184179
running average episode reward sum: 0.16241238692367826
{'scaleFactor': 20, 'currentTarget': array([16., 26.]), 'previousTarget': array([16., 26.]), 'currentState': array([16.34888196, 26.0690881 ,  0.        ]), 'targetState': array([16, 26], dtype=int32), 'currentDistance': 0.3556568402679373}
episode index:634
target Thresh 18.420078365133442
target distance 7.0
model initialize at round 634
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  2.]), 'previousTarget': array([18.,  2.]), 'currentState': array([23.23087859,  8.01401871,  0.        ]), 'targetState': array([18,  2], dtype=int32), 'currentDistance': 7.970602984523468}
done in step count: 9
reward sum = 0.8789768771631505
running average episode reward sum: 0.16354083493980343
{'scaleFactor': 20, 'currentTarget': array([18.,  2.]), 'previousTarget': array([18.,  2.]), 'currentState': array([18.66529939,  2.82318246,  0.        ]), 'targetState': array([18,  2], dtype=int32), 'currentDistance': 1.0584198794342843}
episode index:635
target Thresh 18.433651499070244
target distance 9.0
model initialize at round 635
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 20.]), 'previousTarget': array([ 2., 20.]), 'currentState': array([ 3.11526042, 12.81478703,  0.        ]), 'targetState': array([ 2, 20], dtype=int32), 'currentDistance': 7.271251008628875}
done in step count: 5
reward sum = 0.8826639395393446
running average episode reward sum: 0.1646715316451486
{'scaleFactor': 20, 'currentTarget': array([ 2., 20.]), 'previousTarget': array([ 2., 20.]), 'currentState': array([ 1.86755054, 19.45661545,  0.        ]), 'targetState': array([ 2, 20], dtype=int32), 'currentDistance': 0.5592938669717581}
episode index:636
target Thresh 18.447211066657417
target distance 4.0
model initialize at round 636
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 24.]), 'previousTarget': array([15., 24.]), 'currentState': array([18.56699154, 27.42839921,  0.        ]), 'targetState': array([15, 24], dtype=int32), 'currentDistance': 4.94745891757298}
done in step count: 6
reward sum = 0.9240867835003269
running average episode reward sum: 0.16586370629484276
{'scaleFactor': 20, 'currentTarget': array([15., 24.]), 'previousTarget': array([15., 24.]), 'currentState': array([15.46115902, 24.56241551,  0.        ]), 'targetState': array([15, 24], dtype=int32), 'currentDistance': 0.7273093211934031}
episode index:637
target Thresh 18.460757081454528
target distance 17.0
model initialize at round 637
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 29.]), 'previousTarget': array([14., 29.]), 'currentState': array([ 7.8986702 , 13.99769831,  0.        ]), 'targetState': array([14, 29], dtype=int32), 'currentDistance': 16.195532762380594}
done in step count: 8
reward sum = 0.7559244326790753
running average episode reward sum: 0.16678856636754533
{'scaleFactor': 20, 'currentTarget': array([14., 29.]), 'previousTarget': array([14., 29.]), 'currentState': array([13.50647119, 28.01144457,  0.        ]), 'targetState': array([14, 29], dtype=int32), 'currentDistance': 1.104903855666297}
episode index:638
target Thresh 18.47428955700759
target distance 3.0
model initialize at round 638
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  7.]), 'previousTarget': array([17.,  7.]), 'currentState': array([16.5073148 ,  5.32006645,  0.        ]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 1.7506899870359751}
done in step count: 1
reward sum = 0.9756990554268438
running average episode reward sum: 0.1680544669764018
{'scaleFactor': 20, 'currentTarget': array([17.,  7.]), 'previousTarget': array([17.,  7.]), 'currentState': array([16.68312781,  6.2355032 ,  0.        ]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 0.8275647103525099}
episode index:639
target Thresh 18.487808506849085
target distance 13.0
model initialize at round 639
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([24.14544427,  9.22080767,  0.        ]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 13.777036901409367}
done in step count: 8
reward sum = 0.779957157809486
running average episode reward sum: 0.1690105649308285
{'scaleFactor': 20, 'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([17.2336953 , 20.29267496,  0.        ]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 0.744931007913812}
episode index:640
target Thresh 18.50131394449796
target distance 3.0
model initialize at round 640
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'previousTarget': array([25.,  3.]), 'currentState': array([24.17256394,  4.30719888,  0.        ]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 1.547067985023672}
done in step count: 1
reward sum = 0.9698568812453232
running average episode reward sum: 0.17025993515908822
{'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'previousTarget': array([25.,  3.]), 'currentState': array([24.51002836,  3.29189861,  0.        ]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 0.5703306087255525}
episode index:641
target Thresh 18.51480588345965
target distance 13.0
model initialize at round 641
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  8.]), 'previousTarget': array([24.,  8.]), 'currentState': array([26.61815906, 19.14595234,  0.        ]), 'targetState': array([24,  8], dtype=int32), 'currentDistance': 11.449323582395285}
done in step count: 8
reward sum = 0.8249935965240249
running average episode reward sum: 0.171279769522585
{'scaleFactor': 20, 'currentTarget': array([24.,  8.]), 'previousTarget': array([24.,  8.]), 'currentState': array([24.4859741,  8.5478577,  0.       ]), 'targetState': array([24,  8], dtype=int32), 'currentDistance': 0.7323379569226088}
episode index:642
target Thresh 18.528284337226104
target distance 18.0
model initialize at round 642
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  4.]), 'previousTarget': array([22.,  4.]), 'currentState': array([ 5.99999738, 18.48525715,  0.        ]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 21.582927479573915}
done in step count: 10
reward sum = 0.6494435401138186
running average episode reward sum: 0.17202341457793685
{'scaleFactor': 20, 'currentTarget': array([22.,  4.]), 'previousTarget': array([22.,  4.]), 'currentState': array([21.22505224,  4.02022403,  0.        ]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 0.7752116138001877}
episode index:643
target Thresh 18.541749319275766
target distance 8.0
model initialize at round 643
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 24.]), 'previousTarget': array([12., 24.]), 'currentState': array([17.28743684, 17.02829742,  0.        ]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 8.749950009665412}
done in step count: 7
reward sum = 0.8600866524508182
running average episode reward sum: 0.1730918357547581
{'scaleFactor': 20, 'currentTarget': array([12., 24.]), 'previousTarget': array([12., 24.]), 'currentState': array([12.48468488, 23.33451581,  0.        ]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 0.8232792019917556}
episode index:644
target Thresh 18.555200843073635
target distance 1.0
model initialize at round 644
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  3.]), 'previousTarget': array([20.,  3.]), 'currentState': array([19.59472567,  2.29143003,  0.        ]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 0.81628346269458}
done in step count: 0
reward sum = 0.9978068496676815
running average episode reward sum: 0.17437046368330525
{'scaleFactor': 20, 'currentTarget': array([20.,  3.]), 'previousTarget': array([20.,  3.]), 'currentState': array([19.59472567,  2.29143003,  0.        ]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 0.81628346269458}
episode index:645
target Thresh 18.568638922071223
target distance 10.0
model initialize at round 645
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 11.]), 'previousTarget': array([23., 11.]), 'currentState': array([14.96058476, 19.51189661,  0.        ]), 'targetState': array([23, 11], dtype=int32), 'currentDistance': 11.708312487695364}
done in step count: 6
reward sum = 0.8065255169351742
running average episode reward sum: 0.17534903187719358
{'scaleFactor': 20, 'currentTarget': array([23., 11.]), 'previousTarget': array([23., 11.]), 'currentState': array([22.28494981, 11.37268126,  0.        ]), 'targetState': array([23, 11], dtype=int32), 'currentDistance': 0.8063424190984432}
episode index:646
target Thresh 18.582063569706612
target distance 16.0
model initialize at round 646
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 17.]), 'previousTarget': array([ 7., 17.]), 'currentState': array([21.22367263, 10.49309361,  0.        ]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 15.641377618696787}
done in step count: 10
reward sum = 0.7609762898968725
running average episode reward sum: 0.1762541744707325
{'scaleFactor': 20, 'currentTarget': array([ 7., 17.]), 'previousTarget': array([ 7., 17.]), 'currentState': array([ 7.98467571, 16.2225218 ,  0.        ]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 1.254614921514292}
episode index:647
target Thresh 18.595474799404453
target distance 8.0
model initialize at round 647
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  8.]), 'previousTarget': array([11.,  8.]), 'currentState': array([4.99422145, 8.31839955, 0.        ]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 6.014212689735655}
done in step count: 4
reward sum = 0.8960850717843398
running average episode reward sum: 0.1773650246209078
{'scaleFactor': 20, 'currentTarget': array([11.,  8.]), 'previousTarget': array([11.,  8.]), 'currentState': array([10.24094856,  7.58028815,  0.        ]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 0.867362167346843}
episode index:648
target Thresh 18.60887262457598
target distance 14.0
model initialize at round 648
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([14.13836706, 11.5154767 ,  0.        ]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 13.500597688552356}
done in step count: 8
reward sum = 0.7728653365010268
running average episode reward sum: 0.17828259058682477
{'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([ 9.33344072, 23.51975292,  0.        ]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 0.5846537223773772}
episode index:649
target Thresh 18.622257058619013
target distance 7.0
model initialize at round 649
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([25.93347275, 15.80251634,  0.        ]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 5.197909412353904}
done in step count: 4
reward sum = 0.9166837314224062
running average episode reward sum: 0.17941859234195645
{'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([26.03314726, 20.24978513,  0.        ]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 0.750946801680222}
episode index:650
target Thresh 18.63562811491799
target distance 10.0
model initialize at round 650
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([ 7.98764122, 18.03410017,  0.        ]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 8.970610766025233}
done in step count: 5
reward sum = 0.8524261968174826
running average episode reward sum: 0.18045239818600486
{'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([15.34295124, 13.91604292,  0.        ]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 0.6623910220285292}
episode index:651
target Thresh 18.648985806843967
target distance 9.0
model initialize at round 651
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([17.24109292, 22.03658307,  0.        ]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 10.046024192553665}
done in step count: 6
reward sum = 0.8499455100153718
running average episode reward sum: 0.18147922811212353
{'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([10.65461445, 28.12957549,  0.        ]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 1.0891092241824902}
episode index:652
target Thresh 18.662330147754638
target distance 3.0
model initialize at round 652
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  9.]), 'previousTarget': array([24.,  9.]), 'currentState': array([26.29319179, 10.69941506,  0.        ]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 2.854249485836026}
done in step count: 2
reward sum = 0.9706560119578995
running average episode reward sum: 0.1826877683630359
{'scaleFactor': 20, 'currentTarget': array([24.,  9.]), 'previousTarget': array([24.,  9.]), 'currentState': array([24.89992523,  9.8816897 ,  0.        ]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 1.2598579863443788}
episode index:653
target Thresh 18.67566115099435
target distance 16.0
model initialize at round 653
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 17.]), 'previousTarget': array([11., 17.]), 'currentState': array([25.31378055, 16.5198611 ,  0.        ]), 'targetState': array([11, 17], dtype=int32), 'currentDistance': 14.321831128934274}
done in step count: 12
reward sum = 0.7865512172926642
running average episode reward sum: 0.18361110696996197
{'scaleFactor': 20, 'currentTarget': array([11., 17.]), 'previousTarget': array([11., 17.]), 'currentState': array([11.92919403, 17.83740513,  0.        ]), 'targetState': array([11, 17], dtype=int32), 'currentDistance': 1.250859263543284}
episode index:654
target Thresh 18.688978829894097
target distance 11.0
model initialize at round 654
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 21.]), 'previousTarget': array([15., 21.]), 'currentState': array([ 5.99958587, 16.4084563 ,  0.        ]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 10.103946167465203}
done in step count: 6
reward sum = 0.83998747468496
running average episode reward sum: 0.1846132082947177
{'scaleFactor': 20, 'currentTarget': array([15., 21.]), 'previousTarget': array([15., 21.]), 'currentState': array([14.44312376, 20.51179293,  0.        ]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 0.7405790263389218}
episode index:655
target Thresh 18.702283197771564
target distance 10.0
model initialize at round 655
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  5.]), 'previousTarget': array([17.,  5.]), 'currentState': array([20.18265653, 13.3766638 ,  0.        ]), 'targetState': array([17,  5], dtype=int32), 'currentDistance': 8.960903919373491}
done in step count: 7
reward sum = 0.86482451399726
running average episode reward sum: 0.1856501157729228
{'scaleFactor': 20, 'currentTarget': array([17.,  5.]), 'previousTarget': array([17.,  5.]), 'currentState': array([17.4076477 ,  5.78336552,  0.        ]), 'targetState': array([17,  5], dtype=int32), 'currentDistance': 0.8830844700025278}
episode index:656
target Thresh 18.715574267931125
target distance 13.0
model initialize at round 656
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  2.]), 'previousTarget': array([11.,  2.]), 'currentState': array([19.76802158, 14.190961  ,  0.        ]), 'targetState': array([11,  2], dtype=int32), 'currentDistance': 15.016581923786516}
done in step count: 11
reward sum = 0.7887537307548861
running average episode reward sum: 0.18656808170135808
{'scaleFactor': 20, 'currentTarget': array([11.,  2.]), 'previousTarget': array([11.,  2.]), 'currentState': array([11.7752493 ,  2.88877475,  0.        ]), 'targetState': array([11,  2], dtype=int32), 'currentDistance': 1.1793778197598488}
episode index:657
target Thresh 18.72885205366385
target distance 17.0
model initialize at round 657
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 16.]), 'previousTarget': array([20., 16.]), 'currentState': array([5.        , 6.29032031, 0.        ]), 'targetState': array([20, 16], dtype=int32), 'currentDistance': 17.868348545412353}
done in step count: 10
reward sum = 0.7193172651620245
running average episode reward sum: 0.1873777309163439
{'scaleFactor': 20, 'currentTarget': array([20., 16.]), 'previousTarget': array([20., 16.]), 'currentState': array([19.65957105, 15.35738331,  0.        ]), 'targetState': array([20, 16], dtype=int32), 'currentDistance': 0.7272194154143088}
episode index:658
target Thresh 18.742116568247518
target distance 2.0
model initialize at round 658
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  2.]), 'previousTarget': array([13.,  2.]), 'currentState': array([14.47215629,  3.38844955,  0.        ]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 2.0236195994078714}
done in step count: 1
reward sum = 0.9843096649944489
running average episode reward sum: 0.18858703582389796
{'scaleFactor': 20, 'currentTarget': array([13.,  2.]), 'previousTarget': array([13.,  2.]), 'currentState': array([13.99641773,  2.8746106 ,  0.        ]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 1.3258174841854622}
episode index:659
target Thresh 18.755367824946646
target distance 6.0
model initialize at round 659
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([17.51916206, 27.22073603,  0.        ]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 7.597185958812721}
done in step count: 11
reward sum = 0.8720181050663504
running average episode reward sum: 0.18962253744396226
{'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([12.87048829, 22.77045274,  0.        ]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 1.1624746351172814}
episode index:660
target Thresh 18.7686058370125
target distance 11.0
model initialize at round 660
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([22.486485  , 26.50286925,  0.        ]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 9.82966335054163}
done in step count: 6
reward sum = 0.830145156900654
running average episode reward sum: 0.19059155804828404
{'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([25.09103101, 17.38008308,  0.        ]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 0.3908321845767788}
episode index:661
target Thresh 18.781830617683084
target distance 17.0
model initialize at round 661
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 22.]), 'previousTarget': array([20., 22.]), 'currentState': array([ 5.        , 13.64539099,  0.        ]), 'targetState': array([20, 22], dtype=int32), 'currentDistance': 17.169726024415173}
done in step count: 9
reward sum = 0.7314059408695354
running average episode reward sum: 0.19140849820360317
{'scaleFactor': 20, 'currentTarget': array([20., 22.]), 'previousTarget': array([20., 22.]), 'currentState': array([19.34456944, 21.49519518,  0.        ]), 'targetState': array([20, 22], dtype=int32), 'currentDistance': 0.8272950616278515}
episode index:662
target Thresh 18.795042180183188
target distance 15.0
model initialize at round 662
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([23.50913942,  7.99494052,  0.        ]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 13.241448505796232}
done in step count: 7
reward sum = 0.8022528333931035
running average episode reward sum: 0.19232983204250137
{'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([25.58932522, 20.09833169,  0.        ]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 0.9907873217717076}
episode index:663
target Thresh 18.808240537724373
target distance 18.0
model initialize at round 663
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 26.]), 'previousTarget': array([13., 26.]), 'currentState': array([12.29630351,  9.97914076,  0.        ]), 'targetState': array([13, 26], dtype=int32), 'currentDistance': 16.036306294923275}
done in step count: 9
reward sum = 0.7542649955509918
running average episode reward sum: 0.1931761199393515
{'scaleFactor': 20, 'currentTarget': array([13., 26.]), 'previousTarget': array([13., 26.]), 'currentState': array([12.69704639, 25.54688716,  0.        ]), 'targetState': array([13, 26], dtype=int32), 'currentDistance': 0.545061588416118}
episode index:664
target Thresh 18.821425703504993
target distance 16.0
model initialize at round 664
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([11.97903895, 18.03628063,  0.        ]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 18.642728277457376}
done in step count: 11
reward sum = 0.7238528858155971
running average episode reward sum: 0.1939741301136015
{'scaleFactor': 20, 'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([22.13373882,  3.79594958,  0.        ]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 1.176411565078707}
episode index:665
target Thresh 18.834597690710222
target distance 7.0
model initialize at round 665
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.40785158,  9.0006336 ,  0.        ]), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.444565316799969}
done in step count: 5
reward sum = 0.891624994484178
running average episode reward sum: 0.19502165393397775
{'scaleFactor': 20, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.66503382, 14.09462136,  0.        ]), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1233790363728968}
episode index:666
target Thresh 18.847756512512042
target distance 9.0
model initialize at round 666
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  3.]), 'previousTarget': array([21.,  3.]), 'currentState': array([24.11647433, 10.50293565,  0.        ]), 'targetState': array([21,  3], dtype=int32), 'currentDistance': 8.124435709001338}
done in step count: 6
reward sum = 0.884301911631047
running average episode reward sum: 0.196055057618681
{'scaleFactor': 20, 'currentTarget': array([21.,  3.]), 'previousTarget': array([21.,  3.]), 'currentState': array([21.67354026,  3.9328422 ,  0.        ]), 'targetState': array([21,  3], dtype=int32), 'currentDistance': 1.150587262471362}
episode index:667
target Thresh 18.860902182069278
target distance 16.0
model initialize at round 667
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 8.3759889 , 25.40531921,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 15.055314693227752}
done in step count: 10
reward sum = 0.7793679364549858
running average episode reward sum: 0.1969282804911904
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.37990995, 11.51537028,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.6402640808797335}
episode index:668
target Thresh 18.874034712527603
target distance 15.0
model initialize at round 668
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 17.]), 'previousTarget': array([23., 17.]), 'currentState': array([21.76210879,  3.98521531,  0.        ]), 'targetState': array([23, 17], dtype=int32), 'currentDistance': 13.073522680473062}
done in step count: 7
reward sum = 0.804415009505357
running average episode reward sum: 0.19783633240302026
{'scaleFactor': 20, 'currentTarget': array([23., 17.]), 'previousTarget': array([23., 17.]), 'currentState': array([22.46847882, 16.17608917,  0.        ]), 'targetState': array([23, 17], dtype=int32), 'currentDistance': 0.9804814262430149}
episode index:669
target Thresh 18.887154117019545
target distance 12.0
model initialize at round 669
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 18.]), 'previousTarget': array([25., 18.]), 'currentState': array([14.99287271, 20.84642839,  0.        ]), 'targetState': array([25, 18], dtype=int32), 'currentDistance': 10.404073774356064}
done in step count: 6
reward sum = 0.8254614937892889
running average episode reward sum: 0.19877308637523855
{'scaleFactor': 20, 'currentTarget': array([25., 18.]), 'previousTarget': array([25., 18.]), 'currentState': array([24.26313794, 17.3715907 ,  0.        ]), 'targetState': array([25, 18], dtype=int32), 'currentDistance': 0.9684337582577687}
episode index:670
target Thresh 18.900260408664515
target distance 3.0
model initialize at round 670
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 14.]), 'previousTarget': array([ 9., 14.]), 'currentState': array([11.34676754, 15.59602854,  0.        ]), 'targetState': array([ 9, 14], dtype=int32), 'currentDistance': 2.8380671236923263}
done in step count: 2
reward sum = 0.970455250738466
running average episode reward sum: 0.1999231343102061
{'scaleFactor': 20, 'currentTarget': array([ 9., 14.]), 'previousTarget': array([ 9., 14.]), 'currentState': array([ 9.89484197, 14.8174966 ,  0.        ]), 'targetState': array([ 9, 14], dtype=int32), 'currentDistance': 1.212040773599575}
episode index:671
target Thresh 18.913353600568797
target distance 12.0
model initialize at round 671
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 13.]), 'previousTarget': array([ 5., 13.]), 'currentState': array([ 2.43098655, 23.74470615,  0.        ]), 'targetState': array([ 5, 13], dtype=int32), 'currentDistance': 11.047558120311441}
done in step count: 6
reward sum = 0.8475237774913249
running average episode reward sum: 0.20088682574351133
{'scaleFactor': 20, 'currentTarget': array([ 5., 13.]), 'previousTarget': array([ 5., 13.]), 'currentState': array([ 4.67737348, 13.94867754,  0.        ]), 'targetState': array([ 5, 13], dtype=int32), 'currentDistance': 1.0020363983104927}
episode index:672
target Thresh 18.926433705825588
target distance 9.0
model initialize at round 672
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 16.]), 'previousTarget': array([25., 16.]), 'currentState': array([25.51522714,  8.87704813,  0.        ]), 'targetState': array([25, 16], dtype=int32), 'currentDistance': 7.141561613447039}
done in step count: 5
reward sum = 0.8838099655449158
running average episode reward sum: 0.20190157037917464
{'scaleFactor': 20, 'currentTarget': array([25., 16.]), 'previousTarget': array([25., 16.]), 'currentState': array([24.81541388, 15.55766493,  0.        ]), 'targetState': array([25, 16], dtype=int32), 'currentDistance': 0.4793040247248525}
episode index:673
target Thresh 18.939500737515
target distance 3.0
model initialize at round 673
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 26.]), 'previousTarget': array([23., 26.]), 'currentState': array([21.74510628, 27.34962332,  0.        ]), 'targetState': array([23, 26], dtype=int32), 'currentDistance': 1.8428894056541008}
done in step count: 1
reward sum = 0.9657082565499323
running average episode reward sum: 0.20303481472067428
{'scaleFactor': 20, 'currentTarget': array([23., 26.]), 'previousTarget': array([23., 26.]), 'currentState': array([22.04805475, 26.12341034,  0.        ]), 'targetState': array([23, 26], dtype=int32), 'currentDistance': 0.9599113829001453}
episode index:674
target Thresh 18.952554708704064
target distance 16.0
model initialize at round 674
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 19.]), 'previousTarget': array([19., 19.]), 'currentState': array([ 4.99999833, 20.72977483,  0.        ]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 14.106458368491152}
done in step count: 9
reward sum = 0.7639148264678365
running average episode reward sum: 0.2038657480714108
{'scaleFactor': 20, 'currentTarget': array([19., 19.]), 'previousTarget': array([19., 19.]), 'currentState': array([18.48282948, 18.69408602,  0.        ]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 0.6008732906593844}
episode index:675
target Thresh 18.965595632446743
target distance 7.0
model initialize at round 675
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 23.]), 'previousTarget': array([20., 23.]), 'currentState': array([24.45552659, 17.05710101,  0.        ]), 'targetState': array([20, 23], dtype=int32), 'currentDistance': 7.427635262145295}
done in step count: 5
reward sum = 0.8907716539927379
running average episode reward sum: 0.20488188106833588
{'scaleFactor': 20, 'currentTarget': array([20., 23.]), 'previousTarget': array([20., 23.]), 'currentState': array([20.44391394, 22.09223396,  0.        ]), 'targetState': array([20, 23], dtype=int32), 'currentDistance': 1.010494321613597}
episode index:676
target Thresh 18.97862352178397
target distance 12.0
model initialize at round 676
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 15.]), 'previousTarget': array([13., 15.]), 'currentState': array([23.45389569, 18.87832382,  0.        ]), 'targetState': array([13, 15], dtype=int32), 'currentDistance': 11.15012693804334}
done in step count: 12
reward sum = 0.8292042468315645
running average episode reward sum: 0.20580407067803042
{'scaleFactor': 20, 'currentTarget': array([13., 15.]), 'previousTarget': array([13., 15.]), 'currentState': array([13.86980736, 15.56032936,  0.        ]), 'targetState': array([13, 15], dtype=int32), 'currentDistance': 1.0346660537183077}
episode index:677
target Thresh 18.991638389743635
target distance 9.0
model initialize at round 677
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 25.]), 'previousTarget': array([15., 25.]), 'currentState': array([ 7.98578346, 20.75118375,  0.        ]), 'targetState': array([15, 25], dtype=int32), 'currentDistance': 8.200711752199519}
done in step count: 5
reward sum = 0.8754753510844261
running average episode reward sum: 0.2067917864308422
{'scaleFactor': 20, 'currentTarget': array([15., 25.]), 'previousTarget': array([15., 25.]), 'currentState': array([14.0594241, 24.1812281,  0.       ]), 'targetState': array([15, 25], dtype=int32), 'currentDistance': 1.2470246354818806}
episode index:678
target Thresh 19.004640249340603
target distance 2.0
model initialize at round 678
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 18.]), 'previousTarget': array([25., 18.]), 'currentState': array([25.0461798, 18.8255558,  0.       ]), 'targetState': array([25, 18], dtype=int32), 'currentDistance': 0.8268463915701972}
done in step count: 0
reward sum = 0.9930927414489741
running average episode reward sum: 0.20794981434692192
{'scaleFactor': 20, 'currentTarget': array([25., 18.]), 'previousTarget': array([25., 18.]), 'currentState': array([25.0461798, 18.8255558,  0.       ]), 'targetState': array([25, 18], dtype=int32), 'currentDistance': 0.8268463915701972}
episode index:679
target Thresh 19.01762911357674
target distance 10.0
model initialize at round 679
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([12.55598068, 18.02183968,  0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 11.152330424102948}
done in step count: 11
reward sum = 0.8273830158628857
running average episode reward sum: 0.2088607455256219
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.51618281, 9.81073007, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.9611076628611926}
episode index:680
target Thresh 19.030604995440907
target distance 14.0
model initialize at round 680
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 12.]), 'previousTarget': array([ 3., 12.]), 'currentState': array([15.30646014,  8.96191221,  0.        ]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 12.675919636098309}
done in step count: 11
reward sum = 0.8000247112703911
running average episode reward sum: 0.20972882770733228
{'scaleFactor': 20, 'currentTarget': array([ 3., 12.]), 'previousTarget': array([ 3., 12.]), 'currentState': array([ 3.59440881, 12.0034036 ,  0.        ]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 0.5944185545895646}
episode index:681
target Thresh 19.043567907908987
target distance 9.0
model initialize at round 681
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 15.]), 'previousTarget': array([11., 15.]), 'currentState': array([18.46713579, 11.64271986,  0.        ]), 'targetState': array([11, 15], dtype=int32), 'currentDistance': 8.187151321891797}
done in step count: 7
reward sum = 0.8740027599116325
running average episode reward sum: 0.21070283640557905
{'scaleFactor': 20, 'currentTarget': array([11., 15.]), 'previousTarget': array([11., 15.]), 'currentState': array([11.30453432, 14.43600138,  0.        ]), 'targetState': array([11, 15], dtype=int32), 'currentDistance': 0.6409645842818756}
episode index:682
target Thresh 19.056517863943895
target distance 8.0
model initialize at round 682
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 16.]), 'previousTarget': array([20., 16.]), 'currentState': array([24.44755483,  9.06982195,  0.        ]), 'targetState': array([20, 16], dtype=int32), 'currentDistance': 8.23456809366627}
done in step count: 6
reward sum = 0.8706450823366401
running average episode reward sum: 0.2116690768827841
{'scaleFactor': 20, 'currentTarget': array([20., 16.]), 'previousTarget': array([20., 16.]), 'currentState': array([20.25603545, 15.62164277,  0.        ]), 'targetState': array([20, 16], dtype=int32), 'currentDistance': 0.45684608447424524}
episode index:683
target Thresh 19.069454876495595
target distance 9.0
model initialize at round 683
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([19.99172032, 17.59831017,  0.        ]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 7.408380262732146}
done in step count: 5
reward sum = 0.8781939068615545
running average episode reward sum: 0.21264352838860104
{'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([26.41433263, 19.43180377,  0.        ]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 0.8159983000631053}
episode index:684
target Thresh 19.08237895850109
target distance 9.0
model initialize at round 684
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'previousTarget': array([10., 12.]), 'currentState': array([ 8.61577338, 19.26536894,  0.        ]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 7.396057676073014}
done in step count: 4
reward sum = 0.8881675045708985
running average episode reward sum: 0.21362969477718832
{'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'previousTarget': array([10., 12.]), 'currentState': array([ 9.60345281, 12.73642743,  0.        ]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 0.8364060164764606}
episode index:685
target Thresh 19.095290122884464
target distance 15.0
model initialize at round 685
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 29.]), 'previousTarget': array([18., 29.]), 'currentState': array([12.66439849, 15.99589157,  0.        ]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 14.056154505795002}
done in step count: 8
reward sum = 0.7827017548321945
running average episode reward sum: 0.21445924588514026
{'scaleFactor': 20, 'currentTarget': array([18., 29.]), 'previousTarget': array([18., 29.]), 'currentState': array([17.41669211, 28.43895692,  0.        ]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 0.8093314764045569}
episode index:686
target Thresh 19.10818838255689
target distance 15.0
model initialize at round 686
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 14.]), 'previousTarget': array([19., 14.]), 'currentState': array([ 5.99999654, 20.89799631,  0.        ]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 14.716740227618077}
done in step count: 8
reward sum = 0.7539558558856081
running average episode reward sum: 0.21524453934947863
{'scaleFactor': 20, 'currentTarget': array([19., 14.]), 'previousTarget': array([19., 14.]), 'currentState': array([18.20811132, 13.58550254,  0.        ]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 0.8938097316019012}
episode index:687
target Thresh 19.121073750416624
target distance 4.0
model initialize at round 687
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  3.]), 'previousTarget': array([13.,  3.]), 'currentState': array([16.098445  ,  2.04125959,  0.        ]), 'targetState': array([13,  3], dtype=int32), 'currentDistance': 3.2433847401847897}
done in step count: 3
reward sum = 0.9573205278958249
running average episode reward sum: 0.21632313817004017
{'scaleFactor': 20, 'currentTarget': array([13.,  3.]), 'previousTarget': array([13.,  3.]), 'currentState': array([13.84378022,  2.53296391,  0.        ]), 'targetState': array([13,  3], dtype=int32), 'currentDistance': 0.9644105829431839}
episode index:688
target Thresh 19.13394623934903
target distance 11.0
model initialize at round 688
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 27.]), 'previousTarget': array([ 7., 27.]), 'currentState': array([ 5.96925806, 17.95949614,  0.        ]), 'targetState': array([ 7, 27], dtype=int32), 'currentDistance': 9.099073523771027}
done in step count: 6
reward sum = 0.8550007547222727
running average episode reward sum: 0.217250101329042
{'scaleFactor': 20, 'currentTarget': array([ 7., 27.]), 'previousTarget': array([ 7., 27.]), 'currentState': array([ 6.53922038, 26.58449978,  0.        ]), 'targetState': array([ 7, 27], dtype=int32), 'currentDistance': 0.6204500767918949}
episode index:689
target Thresh 19.146805862226607
target distance 7.0
model initialize at round 689
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 4.85390139, 15.3092947 ,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 5.72663509208948}
done in step count: 3
reward sum = 0.9118690454280688
running average episode reward sum: 0.21825679545092463
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 6.22130546, 10.5784564 ,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.9700396877687016}
episode index:690
target Thresh 19.159652631908976
target distance 12.0
model initialize at round 690
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 17.]), 'previousTarget': array([19., 17.]), 'currentState': array([17.18354811,  6.979895  ,  0.        ]), 'targetState': array([19, 17], dtype=int32), 'currentDistance': 10.183417980928288}
done in step count: 6
reward sum = 0.842612862089839
running average episode reward sum: 0.21916034981653812
{'scaleFactor': 20, 'currentTarget': array([19., 17.]), 'previousTarget': array([19., 17.]), 'currentState': array([18.66045353, 16.39412796,  0.        ]), 'targetState': array([19, 17], dtype=int32), 'currentDistance': 0.6945305820210586}
episode index:691
target Thresh 19.172486561242906
target distance 14.0
model initialize at round 691
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 18.]), 'previousTarget': array([26., 18.]), 'currentState': array([18.61360872,  5.74933052,  0.        ]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 14.305162664743838}
done in step count: 9
reward sum = 0.794925641836143
running average episode reward sum: 0.21999238058535256
{'scaleFactor': 20, 'currentTarget': array([26., 18.]), 'previousTarget': array([26., 18.]), 'currentState': array([25.28813004, 17.3275007 ,  0.        ]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 0.9792926733836129}
episode index:692
target Thresh 19.18530766306233
target distance 4.0
model initialize at round 692
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([13.29572523, 13.95211762,  0.        ]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 3.29607304076784}
done in step count: 4
reward sum = 0.9496934781220439
running average episode reward sum: 0.22104534032205775
{'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([10.69573259, 14.02340422,  0.        ]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 0.6961261374721108}
episode index:693
target Thresh 19.198115950188345
target distance 19.0
model initialize at round 693
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 26.]), 'previousTarget': array([ 6., 26.]), 'currentState': array([21.27002144,  8.63060069,  0.        ]), 'targetState': array([ 6, 26], dtype=int32), 'currentDistance': 23.127247719870393}
done in step count: 13
reward sum = 0.6389804145675142
running average episode reward sum: 0.22164755224460164
{'scaleFactor': 20, 'currentTarget': array([ 6., 26.]), 'previousTarget': array([ 6., 26.]), 'currentState': array([ 6.33983859, 25.42997211,  0.        ]), 'targetState': array([ 6, 26], dtype=int32), 'currentDistance': 0.6636430237711023}
episode index:694
target Thresh 19.210911435429246
target distance 14.0
model initialize at round 694
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([23.42270184,  3.88745368,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 13.433601558015946}
done in step count: 10
reward sum = 0.8075080423251958
running average episode reward sum: 0.22249051697853056
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([11.88022918,  8.47995637,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 1.022374087146595}
episode index:695
target Thresh 19.223694131580515
target distance 8.0
model initialize at round 695
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 27.]), 'previousTarget': array([14., 27.]), 'currentState': array([19.51438487, 19.90415484,  0.        ]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 8.986626676519931}
done in step count: 7
reward sum = 0.8649976934279321
running average episode reward sum: 0.22341365947342914
{'scaleFactor': 20, 'currentTarget': array([14., 27.]), 'previousTarget': array([14., 27.]), 'currentState': array([14.43132219, 26.45236766,  0.        ]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 0.6970939716180142}
episode index:696
target Thresh 19.236464051424853
target distance 14.0
model initialize at round 696
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 10.]), 'previousTarget': array([26., 10.]), 'currentState': array([18.55624139, 23.02873188,  0.        ]), 'targetState': array([26, 10], dtype=int32), 'currentDistance': 15.005245638306342}
done in step count: 11
reward sum = 0.765502434647864
running average episode reward sum: 0.22419140520538672
{'scaleFactor': 20, 'currentTarget': array([26., 10.]), 'previousTarget': array([26., 10.]), 'currentState': array([25.29037153,  9.8020134 ,  0.        ]), 'targetState': array([26, 10], dtype=int32), 'currentDistance': 0.7367301174424213}
episode index:697
target Thresh 19.24922120773218
target distance 8.0
model initialize at round 697
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 14.]), 'previousTarget': array([21., 14.]), 'currentState': array([20.03491077,  7.88880169,  0.        ]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 6.1869331628424975}
done in step count: 4
reward sum = 0.9045259801302925
running average episode reward sum: 0.22516609657347397
{'scaleFactor': 20, 'currentTarget': array([21., 14.]), 'previousTarget': array([21., 14.]), 'currentState': array([20.6164584 , 13.21714276,  0.        ]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 0.8717623624407436}
episode index:698
target Thresh 19.26196561325965
target distance 18.0
model initialize at round 698
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([20.58151245,  5.37853566,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 16.660603283805617}
done in step count: 16
reward sum = 0.7511525505558106
running average episode reward sum: 0.22591858077087362
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.67778242, 7.13164102, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.6904479435876325}
episode index:699
target Thresh 19.274697280751674
target distance 11.0
model initialize at round 699
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 22.]), 'previousTarget': array([25., 22.]), 'currentState': array([15.99983907, 24.19181718,  0.        ]), 'targetState': array([25, 22], dtype=int32), 'currentDistance': 9.263204594480628}
done in step count: 5
reward sum = 0.8546592564438029
running average episode reward sum: 0.22681678173612063
{'scaleFactor': 20, 'currentTarget': array([25., 22.]), 'previousTarget': array([25., 22.]), 'currentState': array([24.01573378, 21.44546258,  0.        ]), 'targetState': array([25, 22], dtype=int32), 'currentDistance': 1.1297308263590626}
episode index:700
target Thresh 19.287416222939918
target distance 17.0
model initialize at round 700
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 25.]), 'previousTarget': array([12., 25.]), 'currentState': array([5.41554794, 9.99120808, 0.        ]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 16.38959560121957}
done in step count: 9
reward sum = 0.7623504375609472
running average episode reward sum: 0.2275807384491375
{'scaleFactor': 20, 'currentTarget': array([12., 25.]), 'previousTarget': array([12., 25.]), 'currentState': array([11.10434595, 24.07865459,  0.        ]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 1.284941069198677}
episode index:701
target Thresh 19.30012245254332
target distance 4.0
model initialize at round 701
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 25.]), 'previousTarget': array([11., 25.]), 'currentState': array([ 8.68069375, 24.73915523,  0.        ]), 'targetState': array([11, 25], dtype=int32), 'currentDistance': 2.3339283395507895}
done in step count: 2
reward sum = 0.9585982122099233
running average episode reward sum: 0.22862207388184516
{'scaleFactor': 20, 'currentTarget': array([11., 25.]), 'previousTarget': array([11., 25.]), 'currentState': array([10.1671651 , 24.24809518,  0.        ]), 'targetState': array([11, 25], dtype=int32), 'currentDistance': 1.1220404777820128}
episode index:702
target Thresh 19.31281598226812
target distance 19.0
model initialize at round 702
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([19.75230062, 11.85865702,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 17.980992665826594}
done in step count: 19
reward sum = 0.731387361086029
running average episode reward sum: 0.2293372449873988
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.83043662, 9.25599474, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.8689984352851883}
episode index:703
target Thresh 19.325496824807846
target distance 14.0
model initialize at round 703
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 11.]), 'previousTarget': array([17., 11.]), 'currentState': array([ 4.99920154, 17.70395923,  0.        ]), 'targetState': array([17, 11], dtype=int32), 'currentDistance': 13.746353446688575}
done in step count: 8
reward sum = 0.7858883324704811
running average episode reward sum: 0.2301278005093918
{'scaleFactor': 20, 'currentTarget': array([17., 11.]), 'previousTarget': array([17., 11.]), 'currentState': array([16.17531505, 10.723941  ,  0.        ]), 'targetState': array([17, 11], dtype=int32), 'currentDistance': 0.8696630585587928}
episode index:704
target Thresh 19.33816499284334
target distance 8.0
model initialize at round 704
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([16.71400619,  6.35684505,  0.        ]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 6.744740717604919}
done in step count: 7
reward sum = 0.8966806877603328
running average episode reward sum: 0.23107326559769098
{'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([10.7048648 ,  6.82414641,  0.        ]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 0.7264701442927984}
episode index:705
target Thresh 19.350820499042772
target distance 18.0
model initialize at round 705
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 29.]), 'previousTarget': array([23., 29.]), 'currentState': array([20.93743926, 12.99549007,  0.        ]), 'targetState': array([23, 29], dtype=int32), 'currentDistance': 16.13686756346304}
done in step count: 9
reward sum = 0.7554926072005661
running average episode reward sum: 0.23181606919769507
{'scaleFactor': 20, 'currentTarget': array([23., 29.]), 'previousTarget': array([23., 29.]), 'currentState': array([22.76627703, 28.31486082,  0.        ]), 'targetState': array([23, 29], dtype=int32), 'currentDistance': 0.7239075378192964}
episode index:706
target Thresh 19.36346335606165
target distance 7.0
model initialize at round 706
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 5.]), 'previousTarget': array([7., 5.]), 'currentState': array([ 2.92448384, 10.33271742,  0.        ]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 6.7117588658179725}
done in step count: 4
reward sum = 0.8873414650159448
running average episode reward sum: 0.2327432621196445
{'scaleFactor': 20, 'currentTarget': array([7., 5.]), 'previousTarget': array([7., 5.]), 'currentState': array([6.13365343, 4.83310908, 0.        ]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 0.8822748747261234}
episode index:707
target Thresh 19.37609357654283
target distance 9.0
model initialize at round 707
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 5.]), 'previousTarget': array([9., 5.]), 'currentState': array([16.82735014,  3.52740288,  0.        ]), 'targetState': array([9, 5], dtype=int32), 'currentDistance': 7.964669013365696}
done in step count: 9
reward sum = 0.8756305555238828
running average episode reward sum: 0.23365129501993298
{'scaleFactor': 20, 'currentTarget': array([9., 5.]), 'previousTarget': array([9., 5.]), 'currentState': array([9.86018533, 4.68774642, 0.        ]), 'targetState': array([9, 5], dtype=int32), 'currentDistance': 0.9151071491435463}
episode index:708
target Thresh 19.388711173116533
target distance 16.0
model initialize at round 708
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 14.]), 'previousTarget': array([ 6., 14.]), 'currentState': array([21.17481029, 22.35602087,  0.        ]), 'targetState': array([ 6, 14], dtype=int32), 'currentDistance': 17.323335478227804}
done in step count: 20
reward sum = 0.7391225935926642
running average episode reward sum: 0.23436423056093825
{'scaleFactor': 20, 'currentTarget': array([ 6., 14.]), 'previousTarget': array([ 6., 14.]), 'currentState': array([ 6.38416871, 14.42335302,  0.        ]), 'targetState': array([ 6, 14], dtype=int32), 'currentDistance': 0.5716759430126004}
episode index:709
target Thresh 19.401316158400356
target distance 12.0
model initialize at round 709
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 19.]), 'previousTarget': array([16., 19.]), 'currentState': array([7.77475452, 8.1493969 , 0.        ]), 'targetState': array([16, 19], dtype=int32), 'currentDistance': 13.615808857696654}
done in step count: 8
reward sum = 0.798789669247277
running average episode reward sum: 0.2351591959675387
{'scaleFactor': 20, 'currentTarget': array([16., 19.]), 'previousTarget': array([16., 19.]), 'currentState': array([15.41684562, 18.31752551,  0.        ]), 'targetState': array([16, 19], dtype=int32), 'currentDistance': 0.897686173571873}
episode index:710
target Thresh 19.413908544999295
target distance 12.0
model initialize at round 710
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 29.]), 'previousTarget': array([25., 29.]), 'currentState': array([14.99995959, 26.74795979,  0.        ]), 'targetState': array([25, 29], dtype=int32), 'currentDistance': 10.250487468228071}
done in step count: 7
reward sum = 0.8249994914959259
running average episode reward sum: 0.23598878850695978
{'scaleFactor': 20, 'currentTarget': array([25., 29.]), 'previousTarget': array([25., 29.]), 'currentState': array([24.46178567, 28.52176209,  0.        ]), 'targetState': array([25, 29], dtype=int32), 'currentDistance': 0.7199903896166102}
episode index:711
target Thresh 19.426488345505724
target distance 16.0
model initialize at round 711
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  2.]), 'previousTarget': array([27.,  2.]), 'currentState': array([22.14320481, 16.68930793,  0.        ]), 'targetState': array([27,  2], dtype=int32), 'currentDistance': 15.471400290240743}
done in step count: 10
reward sum = 0.7780420325993012
running average episode reward sum: 0.23675009924304455
{'scaleFactor': 20, 'currentTarget': array([27.,  2.]), 'previousTarget': array([27.,  2.]), 'currentState': array([26.30065939,  1.87429303,  0.        ]), 'targetState': array([27,  2], dtype=int32), 'currentDistance': 0.71054875529267}
episode index:712
target Thresh 19.439055572499452
target distance 15.0
model initialize at round 712
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 28.]), 'previousTarget': array([27., 28.]), 'currentState': array([16.8201412, 14.5634079,  0.       ]), 'targetState': array([27, 28], dtype=int32), 'currentDistance': 16.857388071144435}
done in step count: 11
reward sum = 0.7531805294826711
running average episode reward sum: 0.23747440559681682
{'scaleFactor': 20, 'currentTarget': array([27., 28.]), 'previousTarget': array([27., 28.]), 'currentState': array([26.00438678, 27.16056341,  0.        ]), 'targetState': array([27, 28], dtype=int32), 'currentDistance': 1.3022670495442277}
episode index:713
target Thresh 19.45161023854771
target distance 6.0
model initialize at round 713
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([13.75100827,  8.59995168,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 5.322810490623598}
done in step count: 5
reward sum = 0.9196390937565962
running average episode reward sum: 0.23842981832533192
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.55326319, 10.5903253 ,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.6884282952495487}
episode index:714
target Thresh 19.464152356205155
target distance 17.0
model initialize at round 714
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([12.87743735, 23.79939294,  0.        ]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 18.763983899173766}
done in step count: 12
reward sum = 0.7117236325936717
running average episode reward sum: 0.239091767715917
{'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([22.31218061,  7.82317397,  0.        ]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 0.7101851590951163}
episode index:715
target Thresh 19.47668193801391
target distance 19.0
model initialize at round 715
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 7.]), 'previousTarget': array([8., 7.]), 'currentState': array([25.94191968,  9.97322608,  0.        ]), 'targetState': array([8, 7], dtype=int32), 'currentDistance': 18.18660373200786}
done in step count: 19
reward sum = 0.7353506305057056
running average episode reward sum: 0.2397848666862938
{'scaleFactor': 20, 'currentTarget': array([8., 7.]), 'previousTarget': array([8., 7.]), 'currentState': array([8.67189533, 6.99335678, 0.        ]), 'targetState': array([8, 7], dtype=int32), 'currentDistance': 0.6719281660536914}
episode index:716
target Thresh 19.489198996503564
target distance 12.0
model initialize at round 716
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([14.89522886, 19.6979754 ,  0.        ]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 14.71574250023138}
done in step count: 9
reward sum = 0.7631510267119885
running average episode reward sum: 0.24051480554267554
{'scaleFactor': 20, 'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([24.06753755,  8.52003768,  0.        ]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 1.0487373625535286}
episode index:717
target Thresh 19.501703544191173
target distance 4.0
model initialize at round 717
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([20.70834702,  5.67213321,  0.        ]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 2.6622042030345567}
done in step count: 2
reward sum = 0.9517452967762534
running average episode reward sum: 0.24150537725748555
{'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([21.5709537 ,  7.57418746,  0.        ]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 0.604480811477728}
episode index:718
target Thresh 19.51419559358128
target distance 9.0
model initialize at round 718
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([21.9602915 ,  8.59959483,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 8.314336162043553}
done in step count: 8
reward sum = 0.8808589177635889
running average episode reward sum: 0.2423946033221672
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.90644729, 10.47746922,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.0462720024730865}
episode index:719
target Thresh 19.526675157165947
target distance 17.0
model initialize at round 719
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 19.]), 'previousTarget': array([27., 19.]), 'currentState': array([11.99999189, 23.47803587,  0.        ]), 'targetState': array([27, 19], dtype=int32), 'currentDistance': 15.654170320239691}
done in step count: 10
reward sum = 0.7509545521691605
running average episode reward sum: 0.2431009365844547
{'scaleFactor': 20, 'currentTarget': array([27., 19.]), 'previousTarget': array([27., 19.]), 'currentState': array([26.39338803, 18.65312745,  0.        ]), 'targetState': array([27, 19], dtype=int32), 'currentDistance': 0.6987836900680852}
episode index:720
target Thresh 19.539142247424724
target distance 3.0
model initialize at round 720
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 17.]), 'previousTarget': array([ 6., 17.]), 'currentState': array([ 8.3355152 , 16.79942082,  0.        ]), 'targetState': array([ 6, 17], dtype=int32), 'currentDistance': 2.344112467973108}
done in step count: 2
reward sum = 0.972015901756974
running average episode reward sum: 0.24411191434474944
{'scaleFactor': 20, 'currentTarget': array([ 6., 17.]), 'previousTarget': array([ 6., 17.]), 'currentState': array([ 6.83236551, 16.73948836,  0.        ]), 'targetState': array([ 6, 17], dtype=int32), 'currentDistance': 0.8721804077908546}
episode index:721
target Thresh 19.551596876824714
target distance 12.0
model initialize at round 721
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  4.]), 'previousTarget': array([25.,  4.]), 'currentState': array([14.96643567, 10.91858673,  0.        ]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 12.187668173236146}
done in step count: 7
reward sum = 0.8065874307330849
running average episode reward sum: 0.24489096630650614
{'scaleFactor': 20, 'currentTarget': array([25.,  4.]), 'previousTarget': array([25.,  4.]), 'currentState': array([24.19990957,  3.80062556,  0.        ]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 0.8245573757518417}
episode index:722
target Thresh 19.564039057820544
target distance 7.0
model initialize at round 722
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([16.13679086,  9.83184934,  0.        ]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 5.169960639872834}
done in step count: 4
reward sum = 0.9119222931300048
running average episode reward sum: 0.24581355458703658
{'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([16.22716702, 14.58938134,  0.        ]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 0.46926808992798347}
episode index:723
target Thresh 19.576468802854393
target distance 18.0
model initialize at round 723
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([ 3.53584725, 26.75411248,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 17.957882834024844}
done in step count: 12
reward sum = 0.7422248471743248
running average episode reward sum: 0.24649920554364885
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([9.31772974, 9.78874287, 0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.7142284514045039}
episode index:724
target Thresh 19.588886124356012
target distance 10.0
model initialize at round 724
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  9.]), 'previousTarget': array([18.,  9.]), 'currentState': array([22.58013463, 17.78071272,  0.        ]), 'targetState': array([18,  9], dtype=int32), 'currentDistance': 9.903461474025077}
done in step count: 9
reward sum = 0.8465555688636401
running average episode reward sum: 0.24732686949305574
{'scaleFactor': 20, 'currentTarget': array([18.,  9.]), 'previousTarget': array([18.,  9.]), 'currentState': array([18.58435999,  9.7310161 ,  0.        ]), 'targetState': array([18,  9], dtype=int32), 'currentDistance': 0.935874528930735}
episode index:725
target Thresh 19.601291034742722
target distance 4.0
model initialize at round 725
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 21.]), 'previousTarget': array([ 4., 21.]), 'currentState': array([ 5.60452229, 18.25771463,  0.        ]), 'targetState': array([ 4, 21], dtype=int32), 'currentDistance': 3.1772033027875053}
done in step count: 2
reward sum = 0.9606450939298944
running average episode reward sum: 0.24830940148263814
{'scaleFactor': 20, 'currentTarget': array([ 4., 21.]), 'previousTarget': array([ 4., 21.]), 'currentState': array([ 4.52683169, 20.03583723,  0.        ]), 'targetState': array([ 4, 21], dtype=int32), 'currentDistance': 1.0987090001625592}
episode index:726
target Thresh 19.61368354641943
target distance 12.0
model initialize at round 726
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 23.]), 'previousTarget': array([26., 23.]), 'currentState': array([15.98763096, 14.81726438,  0.        ]), 'targetState': array([26, 23], dtype=int32), 'currentDistance': 12.93076548333269}
done in step count: 8
reward sum = 0.8002176551471433
running average episode reward sum: 0.24906856001587682
{'scaleFactor': 20, 'currentTarget': array([26., 23.]), 'previousTarget': array([26., 23.]), 'currentState': array([25.46324229, 22.53437155,  0.        ]), 'targetState': array([26, 23], dtype=int32), 'currentDistance': 0.7105763052586122}
episode index:727
target Thresh 19.626063671778653
target distance 10.0
model initialize at round 727
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 11.]), 'previousTarget': array([27., 11.]), 'currentState': array([18.98673916, 10.97122316,  0.        ]), 'targetState': array([27, 11], dtype=int32), 'currentDistance': 8.013312512208339}
done in step count: 6
reward sum = 0.8635067244836794
running average episode reward sum: 0.24991256848355234
{'scaleFactor': 20, 'currentTarget': array([27., 11.]), 'previousTarget': array([27., 11.]), 'currentState': array([26.45240605, 10.57690425,  0.        ]), 'targetState': array([27, 11], dtype=int32), 'currentDistance': 0.6920037191009356}
episode index:728
target Thresh 19.638431423200515
target distance 6.0
model initialize at round 728
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 2.]), 'previousTarget': array([6., 2.]), 'currentState': array([4.23508966, 6.22785401, 0.        ]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 4.581447158738835}
done in step count: 2
reward sum = 0.9234970667956938
running average episode reward sum: 0.25083655270620275
{'scaleFactor': 20, 'currentTarget': array([6., 2.]), 'previousTarget': array([6., 2.]), 'currentState': array([5.56907693, 2.91949451, 0.        ]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 1.0154628744988579}
episode index:729
target Thresh 19.650786813052775
target distance 13.0
model initialize at round 729
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 28.]), 'previousTarget': array([11., 28.]), 'currentState': array([22.86409044, 25.72679961,  0.        ]), 'targetState': array([11, 28], dtype=int32), 'currentDistance': 12.079904058270841}
done in step count: 11
reward sum = 0.8250344038491965
running average episode reward sum: 0.25162312510502877
{'scaleFactor': 20, 'currentTarget': array([11., 28.]), 'previousTarget': array([11., 28.]), 'currentState': array([11.78197747, 28.1457213 ,  0.        ]), 'targetState': array([11, 28], dtype=int32), 'currentDistance': 0.7954391663675532}
episode index:730
target Thresh 19.66312985369082
target distance 4.0
model initialize at round 730
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 19.]), 'previousTarget': array([19., 19.]), 'currentState': array([16.3343401 , 18.16118662,  0.        ]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 2.794521535401632}
done in step count: 3
reward sum = 0.9513187731096433
running average episode reward sum: 0.2525803010940912
{'scaleFactor': 20, 'currentTarget': array([19., 19.]), 'previousTarget': array([19., 19.]), 'currentState': array([18.48598921, 18.58572948,  0.        ]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 0.6601720632363696}
episode index:731
target Thresh 19.675460557457683
target distance 11.0
model initialize at round 731
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  3.]), 'previousTarget': array([17.,  3.]), 'currentState': array([21.60231185, 12.70643938,  0.        ]), 'targetState': array([17,  3], dtype=int32), 'currentDistance': 10.742264180259975}
done in step count: 9
reward sum = 0.8373868774497373
running average episode reward sum: 0.25337921718200873
{'scaleFactor': 20, 'currentTarget': array([17.,  3.]), 'previousTarget': array([17.,  3.]), 'currentState': array([17.60908039,  3.94103056,  0.        ]), 'targetState': array([17,  3], dtype=int32), 'currentDistance': 1.1209448866526297}
episode index:732
target Thresh 19.687778936684083
target distance 4.0
model initialize at round 732
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  9.]), 'previousTarget': array([23.,  9.]), 'currentState': array([26.1617673 ,  8.97296132,  0.        ]), 'targetState': array([23,  9], dtype=int32), 'currentDistance': 3.1618829159889854}
done in step count: 3
reward sum = 0.9549489775023404
running average episode reward sum: 0.25433633827385094
{'scaleFactor': 20, 'currentTarget': array([23.,  9.]), 'previousTarget': array([23.,  9.]), 'currentState': array([23.4911837 ,  8.96852384,  0.        ]), 'targetState': array([23,  9], dtype=int32), 'currentDistance': 0.4921911967444238}
episode index:733
target Thresh 19.70008500368839
target distance 4.0
model initialize at round 733
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  2.]), 'previousTarget': array([26.,  2.]), 'currentState': array([23.59501207,  3.58305788,  0.        ]), 'targetState': array([26,  2], dtype=int32), 'currentDistance': 2.87924281750726}
done in step count: 2
reward sum = 0.9397953753293768
running average episode reward sum: 0.2552702061717467
{'scaleFactor': 20, 'currentTarget': array([26.,  2.]), 'previousTarget': array([26.,  2.]), 'currentState': array([25.34553766,  1.85944462,  0.        ]), 'targetState': array([26,  2], dtype=int32), 'currentDistance': 0.6693853647517873}
episode index:734
target Thresh 19.712378770776677
target distance 12.0
model initialize at round 734
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([ 6.8681376 , 17.43858373,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 13.232203763815813}
done in step count: 8
reward sum = 0.7803273384865995
running average episode reward sum: 0.25598456961707305
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.25669986,  6.61470181,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.8372274452772736}
episode index:735
target Thresh 19.72466025024271
target distance 14.0
model initialize at round 735
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 26.]), 'previousTarget': array([ 7., 26.]), 'currentState': array([19.80283856, 24.61720061,  0.        ]), 'targetState': array([ 7, 26], dtype=int32), 'currentDistance': 12.877298219673252}
done in step count: 13
reward sum = 0.802745932063812
running average episode reward sum: 0.2567274519030061
{'scaleFactor': 20, 'currentTarget': array([ 7., 26.]), 'previousTarget': array([ 7., 26.]), 'currentState': array([ 7.3014397 , 26.04990333,  0.        ]), 'targetState': array([ 7, 26], dtype=int32), 'currentDistance': 0.3055425278233296}
episode index:736
target Thresh 19.73692945436797
target distance 15.0
model initialize at round 736
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([ 7.70389628, 18.09464467,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 16.354948643079403}
done in step count: 9
reward sum = 0.7564938265578064
running average episode reward sum: 0.2574055609595255
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.25372085,  4.11892569,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.7556956331689513}
episode index:737
target Thresh 19.749186395421667
target distance 16.0
model initialize at round 737
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([24.3053751 , 11.84026444,  0.        ]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 14.540413188325632}
done in step count: 9
reward sum = 0.7813963197249506
running average episode reward sum: 0.2581155755377984
{'scaleFactor': 20, 'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([21.10651032, 25.45965922,  0.        ]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 0.5507382389618808}
episode index:738
target Thresh 19.761431085660732
target distance 16.0
model initialize at round 738
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 12.]), 'previousTarget': array([18., 12.]), 'currentState': array([ 3.99961472, 17.91114511,  0.        ]), 'targetState': array([18, 12], dtype=int32), 'currentDistance': 15.19711895643394}
done in step count: 8
reward sum = 0.7631760527352329
running average episode reward sum: 0.2587990132606637
{'scaleFactor': 20, 'currentTarget': array([18., 12.]), 'previousTarget': array([18., 12.]), 'currentState': array([17.2424174 , 11.60801782,  0.        ]), 'targetState': array([18, 12], dtype=int32), 'currentDistance': 0.8529838421121598}
episode index:739
target Thresh 19.773663537329867
target distance 14.0
model initialize at round 739
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 16.]), 'previousTarget': array([27., 16.]), 'currentState': array([15.9911617,  2.6961742,  0.       ]), 'targetState': array([27, 16], dtype=int32), 'currentDistance': 17.268071734478493}
done in step count: 13
reward sum = 0.7463794165896519
running average episode reward sum: 0.25945790569759475
{'scaleFactor': 20, 'currentTarget': array([27., 16.]), 'previousTarget': array([27., 16.]), 'currentState': array([26.44592154, 15.46126863,  0.        ]), 'targetState': array([27, 16], dtype=int32), 'currentDistance': 0.7728094362046138}
episode index:740
target Thresh 19.78588376266152
target distance 7.0
model initialize at round 740
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 27.]), 'previousTarget': array([17., 27.]), 'currentState': array([11.40545094, 21.84994912,  0.        ]), 'targetState': array([17, 27], dtype=int32), 'currentDistance': 7.604078066055333}
done in step count: 6
reward sum = 0.8882729989279737
running average episode reward sum: 0.2603065090622781
{'scaleFactor': 20, 'currentTarget': array([17., 27.]), 'previousTarget': array([17., 27.]), 'currentState': array([16.46256119, 26.46295106,  0.        ]), 'targetState': array([17, 27], dtype=int32), 'currentDistance': 0.7597776213454723}
episode index:741
target Thresh 19.798091773875917
target distance 13.0
model initialize at round 741
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([25.84263206,  4.78822356,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 12.047401752945989}
done in step count: 11
reward sum = 0.8244889462985652
running average episode reward sum: 0.2610668627512758
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.91782635,  7.09330971,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.9225572716094302}
episode index:742
target Thresh 19.810287583181072
target distance 9.0
model initialize at round 742
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([13.00203621, 27.44944096,  0.        ]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 8.454459473782169}
done in step count: 4
reward sum = 0.868344235441238
running average episode reward sum: 0.2618841943430523
{'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([16.71150029, 20.74381423,  0.        ]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 0.7978042949667254}
episode index:743
target Thresh 19.82247120277279
target distance 15.0
model initialize at round 743
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([19.66040528, 18.40434146,  0.        ]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 13.40864253128904}
done in step count: 8
reward sum = 0.7861519592711981
running average episode reward sum: 0.2625888553174181
{'scaleFactor': 20, 'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([20.21998234,  5.89385605,  0.        ]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 0.9205274932951852}
episode index:744
target Thresh 19.8346426448347
target distance 5.0
model initialize at round 744
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 23.]), 'previousTarget': array([ 2., 23.]), 'currentState': array([ 5.77318442, 19.79234183,  0.        ]), 'targetState': array([ 2, 23], dtype=int32), 'currentDistance': 4.952372320770415}
done in step count: 4
reward sum = 0.9287859957349145
running average episode reward sum: 0.2634830796669718
{'scaleFactor': 20, 'currentTarget': array([ 2., 23.]), 'previousTarget': array([ 2., 23.]), 'currentState': array([ 2.66220051, 22.45511258,  0.        ]), 'targetState': array([ 2, 23], dtype=int32), 'currentDistance': 0.8575615548833897}
episode index:745
target Thresh 19.846801921538244
target distance 16.0
model initialize at round 745
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 17.]), 'previousTarget': array([24., 17.]), 'currentState': array([ 9.99999988, 15.94262308,  0.        ]), 'targetState': array([24, 17], dtype=int32), 'currentDistance': 14.039873549690409}
done in step count: 8
reward sum = 0.7807737839211947
running average episode reward sum: 0.26417649884157535
{'scaleFactor': 20, 'currentTarget': array([24., 17.]), 'previousTarget': array([24., 17.]), 'currentState': array([23.04764819, 16.34326831,  0.        ]), 'targetState': array([24, 17], dtype=int32), 'currentDistance': 1.156836408742854}
episode index:746
target Thresh 19.858949045042692
target distance 16.0
model initialize at round 746
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 28.]), 'previousTarget': array([10., 28.]), 'currentState': array([24.97148836, 23.07387877,  0.        ]), 'targetState': array([10, 28], dtype=int32), 'currentDistance': 15.761095584774068}
done in step count: 14
reward sum = 0.7726055058397765
running average episode reward sum: 0.264857126695656
{'scaleFactor': 20, 'currentTarget': array([10., 28.]), 'previousTarget': array([10., 28.]), 'currentState': array([10.90766084, 28.07843228,  0.        ]), 'targetState': array([10, 28], dtype=int32), 'currentDistance': 0.9110432630859506}
episode index:747
target Thresh 19.871084027495172
target distance 18.0
model initialize at round 747
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 26.]), 'previousTarget': array([17., 26.]), 'currentState': array([13.21460128,  9.99793267,  0.        ]), 'targetState': array([17, 26], dtype=int32), 'currentDistance': 16.443704030297805}
done in step count: 9
reward sum = 0.7436856947007023
running average episode reward sum: 0.2654972718400478
{'scaleFactor': 20, 'currentTarget': array([17., 26.]), 'previousTarget': array([17., 26.]), 'currentState': array([17.10081552, 25.32820183,  0.        ]), 'targetState': array([17, 26], dtype=int32), 'currentDistance': 0.6793206530833924}
episode index:748
target Thresh 19.883206881030674
target distance 9.0
model initialize at round 748
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([23.13546848,  7.41963297,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 9.258455758834847}
done in step count: 12
reward sum = 0.8522538149428668
running average episode reward sum: 0.26628065841294873
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.64455986,  3.55952561,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.8535375331901898}
episode index:749
target Thresh 19.895317617772044
target distance 15.0
model initialize at round 749
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 17.]), 'previousTarget': array([24., 17.]), 'currentState': array([25.02809442,  3.94054556,  0.        ]), 'targetState': array([24, 17], dtype=int32), 'currentDistance': 13.0998598630456}
done in step count: 8
reward sum = 0.8026589553872288
running average episode reward sum: 0.2669958294755811
{'scaleFactor': 20, 'currentTarget': array([24., 17.]), 'previousTarget': array([24., 17.]), 'currentState': array([24.14437206, 16.40577024,  0.        ]), 'targetState': array([24, 17], dtype=int32), 'currentDistance': 0.6115163911419054}
episode index:750
target Thresh 19.90741624983002
target distance 9.0
model initialize at round 750
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 16.]), 'previousTarget': array([24., 16.]), 'currentState': array([19.20077348, 23.50039971,  0.        ]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 8.904413007308866}
done in step count: 4
reward sum = 0.8679122997577542
running average episode reward sum: 0.2677959845625081
{'scaleFactor': 20, 'currentTarget': array([24., 16.]), 'previousTarget': array([24., 16.]), 'currentState': array([23.32351077, 16.9332329 ,  0.        ]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 1.1526323506723932}
episode index:751
target Thresh 19.919502789303245
target distance 18.0
model initialize at round 751
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 29.]), 'previousTarget': array([16., 29.]), 'currentState': array([10.87557274, 12.99712777,  0.        ]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 16.803323311121297}
done in step count: 9
reward sum = 0.7412582029374547
running average episode reward sum: 0.26842558857630455
{'scaleFactor': 20, 'currentTarget': array([16., 29.]), 'previousTarget': array([16., 29.]), 'currentState': array([15.84565096, 28.43904114,  0.        ]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 0.5818062156515352}
episode index:752
target Thresh 19.93157724827825
target distance 12.0
model initialize at round 752
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  2.]), 'previousTarget': array([19.,  2.]), 'currentState': array([13.71600097, 12.55301023,  0.        ]), 'targetState': array([19,  2], dtype=int32), 'currentDistance': 11.801977398728095}
done in step count: 6
reward sum = 0.825769736861516
running average episode reward sum: 0.26916575344786525
{'scaleFactor': 20, 'currentTarget': array([19.,  2.]), 'previousTarget': array([19.,  2.]), 'currentState': array([18.47774701,  2.71833956,  0.        ]), 'targetState': array([19,  2], dtype=int32), 'currentDistance': 0.8881215661816906}
episode index:753
target Thresh 19.943639638829495
target distance 4.0
model initialize at round 753
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 7.]), 'previousTarget': array([5., 7.]), 'currentState': array([4.24039619, 9.22815812, 0.        ]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 2.3540787039902287}
done in step count: 1
reward sum = 0.9633064945958495
running average episode reward sum: 0.27008636451039575
{'scaleFactor': 20, 'currentTarget': array([5., 7.]), 'previousTarget': array([5., 7.]), 'currentState': array([4.23998937, 7.75740373, 0.        ]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 1.0729755681311368}
episode index:754
target Thresh 19.955689973019375
target distance 1.0
model initialize at round 754
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 26.]), 'previousTarget': array([18., 26.]), 'currentState': array([17.10628433, 26.05299693,  0.        ]), 'targetState': array([18, 26], dtype=int32), 'currentDistance': 0.8952856402751062}
done in step count: 0
reward sum = 0.9954594439762752
running average episode reward sum: 0.27104712355604593
{'scaleFactor': 20, 'currentTarget': array([18., 26.]), 'previousTarget': array([18., 26.]), 'currentState': array([17.10628433, 26.05299693,  0.        ]), 'targetState': array([18, 26], dtype=int32), 'currentDistance': 0.8952856402751062}
episode index:755
target Thresh 19.967728262898227
target distance 8.0
model initialize at round 755
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([9.04786556, 4.91273403, 0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 6.161279652544372}
done in step count: 4
reward sum = 0.9013006548874989
running average episode reward sum: 0.2718807922482833
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.02701332, 10.38080865,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.6197803204407673}
episode index:756
target Thresh 19.979754520504336
target distance 4.0
model initialize at round 756
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.24301629,  4.23831105,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 2.3628501238248982}
done in step count: 1
reward sum = 0.9633975609853273
running average episode reward sum: 0.27279428864027416
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.53689598,  2.79148197,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.9170109318464581}
episode index:757
target Thresh 19.991768757863966
target distance 18.0
model initialize at round 757
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 24.]), 'previousTarget': array([26., 24.]), 'currentState': array([27.12803692,  7.95817006,  0.        ]), 'targetState': array([26, 24], dtype=int32), 'currentDistance': 16.0814419510703}
done in step count: 9
reward sum = 0.7671139032113027
running average episode reward sum: 0.2734464253349589
{'scaleFactor': 20, 'currentTarget': array([26., 24.]), 'previousTarget': array([26., 24.]), 'currentState': array([26.25982565, 23.08681995,  0.        ]), 'targetState': array([26, 24], dtype=int32), 'currentDistance': 0.9494246569916316}
episode index:758
target Thresh 20.003770986991356
target distance 1.0
model initialize at round 758
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([16.20019341, 17.59575528,  0.        ]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 0.6284916483128667}
done in step count: 0
reward sum = 0.9959844782012435
running average episode reward sum: 0.27439838587891974
{'scaleFactor': 20, 'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([16.20019341, 17.59575528,  0.        ]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 0.6284916483128667}
episode index:759
target Thresh 20.01576121988873
target distance 11.0
model initialize at round 759
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([ 8.37402833, 12.09030533,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 11.934927679238314}
done in step count: 11
reward sum = 0.8142391920478335
running average episode reward sum: 0.275108702729142
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.70255776, 2.86636341, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 1.1154250152557075}
episode index:760
target Thresh 20.027739468546322
target distance 6.0
model initialize at round 760
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  5.]), 'previousTarget': array([17.,  5.]), 'currentState': array([12.65661752,  6.1471535 ,  0.        ]), 'targetState': array([17,  5], dtype=int32), 'currentDistance': 4.492319277745459}
done in step count: 3
reward sum = 0.9264286033399779
running average episode reward sum: 0.27596457644873573
{'scaleFactor': 20, 'currentTarget': array([17.,  5.]), 'previousTarget': array([17.,  5.]), 'currentState': array([16.09987748,  4.48549025,  0.        ]), 'targetState': array([17,  5], dtype=int32), 'currentDistance': 1.036793538884844}
episode index:761
target Thresh 20.03970574494239
target distance 6.0
model initialize at round 761
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([14.6572237 ,  7.58573723,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 5.245798064733779}
done in step count: 4
reward sum = 0.9258330313938514
running average episode reward sum: 0.2768174221901335
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.69733614,  9.36403871,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.943781996048402}
episode index:762
target Thresh 20.0516600610432
target distance 18.0
model initialize at round 762
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([13.89309448, 29.23853189,  0.        ]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 19.574272693568027}
done in step count: 15
reward sum = 0.7119158449466656
running average episode reward sum: 0.277387669140011
{'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([20.69595728, 11.57303706,  0.        ]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 0.6487013551543253}
episode index:763
target Thresh 20.063602428803083
target distance 14.0
model initialize at round 763
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 14.]), 'previousTarget': array([23., 14.]), 'currentState': array([10.99999237, 16.62418205,  0.        ]), 'targetState': array([23, 14], dtype=int32), 'currentDistance': 12.28358720053875}
done in step count: 7
reward sum = 0.8014132350840365
running average episode reward sum: 0.27807356647763404
{'scaleFactor': 20, 'currentTarget': array([23., 14.]), 'previousTarget': array([23., 14.]), 'currentState': array([22.02308553, 13.55442615,  0.        ]), 'targetState': array([23, 14], dtype=int32), 'currentDistance': 1.073730846102344}
episode index:764
target Thresh 20.075532860164394
target distance 14.0
model initialize at round 764
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 29.]), 'previousTarget': array([ 9., 29.]), 'currentState': array([ 5.77978635, 16.98978031,  0.        ]), 'targetState': array([ 9, 29], dtype=int32), 'currentDistance': 12.434434166493507}
done in step count: 7
reward sum = 0.8052405660965061
running average episode reward sum: 0.2787626736666783
{'scaleFactor': 20, 'currentTarget': array([ 9., 29.]), 'previousTarget': array([ 9., 29.]), 'currentState': array([ 9.04045519, 28.38808221,  0.        ]), 'targetState': array([ 9, 29], dtype=int32), 'currentDistance': 0.6132536250147762}
episode index:765
target Thresh 20.087451367057575
target distance 18.0
model initialize at round 765
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 10.]), 'previousTarget': array([23., 10.]), 'currentState': array([ 6.99999809, 12.62640098,  0.        ]), 'targetState': array([23, 10], dtype=int32), 'currentDistance': 16.214130970517854}
done in step count: 9
reward sum = 0.7536159444339215
running average episode reward sum: 0.2793825865527974
{'scaleFactor': 20, 'currentTarget': array([23., 10.]), 'previousTarget': array([23., 10.]), 'currentState': array([22.3054142 ,  9.80054962,  0.        ]), 'targetState': array([23, 10], dtype=int32), 'currentDistance': 0.7226547494023288}
episode index:766
target Thresh 20.099357961401132
target distance 20.0
model initialize at round 766
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 27.]), 'previousTarget': array([25., 27.]), 'currentState': array([25.2192679 ,  8.97921073,  0.        ]), 'targetState': array([25, 27], dtype=int32), 'currentDistance': 18.022123187083107}
done in step count: 10
reward sum = 0.7365871152574106
running average episode reward sum: 0.2799786811143419
{'scaleFactor': 20, 'currentTarget': array([25., 27.]), 'previousTarget': array([25., 27.]), 'currentState': array([25.20133541, 26.26006311,  0.        ]), 'targetState': array([25, 27], dtype=int32), 'currentDistance': 0.7668393219750614}
episode index:767
target Thresh 20.111252655101655
target distance 17.0
model initialize at round 767
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 2.]), 'previousTarget': array([8., 2.]), 'currentState': array([10.54888535, 17.276456  ,  0.        ]), 'targetState': array([8, 2], dtype=int32), 'currentDistance': 15.487637791993196}
done in step count: 9
reward sum = 0.7705967014189803
running average episode reward sum: 0.28061750666161356
{'scaleFactor': 20, 'currentTarget': array([8., 2.]), 'previousTarget': array([8., 2.]), 'currentState': array([7.94757334, 2.48076689, 0.        ]), 'targetState': array([8, 2], dtype=int32), 'currentDistance': 0.48361695523273834}
episode index:768
target Thresh 20.123135460053845
target distance 15.0
model initialize at round 768
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  6.]), 'previousTarget': array([24.,  6.]), 'currentState': array([16.68636012, 19.21307755,  0.        ]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 15.102143771583787}
done in step count: 9
reward sum = 0.7593033971277783
running average episode reward sum: 0.2812399850627399
{'scaleFactor': 20, 'currentTarget': array([24.,  6.]), 'previousTarget': array([24.,  6.]), 'currentState': array([23.43275239,  6.17275   ,  0.        ]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 0.5929691535572764}
episode index:769
target Thresh 20.135006388140503
target distance 13.0
model initialize at round 769
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([ 7.03443456, 20.56103516,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 12.222243786108853}
done in step count: 7
reward sum = 0.8154721862336749
running average episode reward sum: 0.28193379311620864
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.50319877,  9.29713857,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.5788806420116972}
episode index:770
target Thresh 20.14686545123256
target distance 16.0
model initialize at round 770
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 26.]), 'previousTarget': array([11., 26.]), 'currentState': array([25.7971369 , 23.79633319,  0.        ]), 'targetState': array([11, 26], dtype=int32), 'currentDistance': 14.960327800998169}
done in step count: 15
reward sum = 0.7735858410831195
running average episode reward sum: 0.2825714741122747
{'scaleFactor': 20, 'currentTarget': array([11., 26.]), 'previousTarget': array([11., 26.]), 'currentState': array([11.79097813, 26.24075312,  0.        ]), 'targetState': array([11, 26], dtype=int32), 'currentDistance': 0.826806187001345}
episode index:771
target Thresh 20.158712661189085
target distance 16.0
model initialize at round 771
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  2.]), 'previousTarget': array([18.,  2.]), 'currentState': array([3.9999994 , 1.97505603, 0.        ]), 'targetState': array([18,  2], dtype=int32), 'currentDistance': 14.000022817515227}
done in step count: 8
reward sum = 0.7827212403557329
running average episode reward sum: 0.2832193365037817
{'scaleFactor': 20, 'currentTarget': array([18.,  2.]), 'previousTarget': array([18.,  2.]), 'currentState': array([17.0792082 ,  1.48311494,  0.        ]), 'targetState': array([18,  2], dtype=int32), 'currentDistance': 1.0559487278227484}
episode index:772
target Thresh 20.17054802985728
target distance 9.0
model initialize at round 772
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 28.]), 'previousTarget': array([23., 28.]), 'currentState': array([15.99453735, 28.23874678,  0.        ]), 'targetState': array([23, 28], dtype=int32), 'currentDistance': 7.009529721379355}
done in step count: 4
reward sum = 0.8891623020546643
running average episode reward sum: 0.28400322132338185
{'scaleFactor': 20, 'currentTarget': array([23., 28.]), 'previousTarget': array([23., 28.]), 'currentState': array([22.1744622 , 27.51956245,  0.        ]), 'targetState': array([23, 28], dtype=int32), 'currentDistance': 0.9551611927980526}
episode index:773
target Thresh 20.18237156907252
target distance 2.0
model initialize at round 773
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.06631899, 3.00968157, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0663629395060816}
done in step count: 1
reward sum = 0.9834378663173303
running average episode reward sum: 0.2849068836554154
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.4151473 , 3.15458961, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.4429957488240523}
episode index:774
target Thresh 20.194183290658344
target distance 11.0
model initialize at round 774
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([ 6.56811386, 13.97901356,  0.        ]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 9.343033045643262}
done in step count: 5
reward sum = 0.8574489434536189
running average episode reward sum: 0.2856456476035421
{'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([ 8.89267379, 22.06075764,  0.        ]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 0.9453545005255487}
episode index:775
target Thresh 20.205983206426474
target distance 13.0
model initialize at round 775
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 6.]), 'previousTarget': array([7., 6.]), 'currentState': array([ 5.95409703, 17.17649651,  0.        ]), 'targetState': array([7, 6], dtype=int32), 'currentDistance': 11.225327931765188}
done in step count: 6
reward sum = 0.8329532331200991
running average episode reward sum: 0.28635094088384694
{'scaleFactor': 20, 'currentTarget': array([7., 6.]), 'previousTarget': array([7., 6.]), 'currentState': array([6.44631629, 6.6434387 , 0.        ]), 'targetState': array([7, 6], dtype=int32), 'currentDistance': 0.8488692493524492}
episode index:776
target Thresh 20.21777132817683
target distance 11.0
model initialize at round 776
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 16.]), 'previousTarget': array([12., 16.]), 'currentState': array([22.040407  ,  6.07257009,  0.        ]), 'targetState': array([12, 16], dtype=int32), 'currentDistance': 14.119618885510107}
done in step count: 10
reward sum = 0.8081396724173794
running average episode reward sum: 0.28702248365287336
{'scaleFactor': 20, 'currentTarget': array([12., 16.]), 'previousTarget': array([12., 16.]), 'currentState': array([12.64314306, 15.50772554,  0.        ]), 'targetState': array([12, 16], dtype=int32), 'currentDistance': 0.8099179834540283}
episode index:777
target Thresh 20.22954766769753
target distance 15.0
model initialize at round 777
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([24.92521644,  4.30547905,  0.        ]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 16.913007579014245}
done in step count: 11
reward sum = 0.759451922816527
running average episode reward sum: 0.2876297194358601
{'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([15.37798369, 17.55501878,  0.        ]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 0.5838492555509872}
episode index:778
target Thresh 20.24131223676492
target distance 11.0
model initialize at round 778
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 13.]), 'previousTarget': array([ 5., 13.]), 'currentState': array([14.7146709 , 16.13853849,  0.        ]), 'targetState': array([ 5, 13], dtype=int32), 'currentDistance': 10.209077064585252}
done in step count: 11
reward sum = 0.8483908744715133
running average episode reward sum: 0.28834956687493024
{'scaleFactor': 20, 'currentTarget': array([ 5., 13.]), 'previousTarget': array([ 5., 13.]), 'currentState': array([ 5.80354941, 13.44639533,  0.        ]), 'targetState': array([ 5, 13], dtype=int32), 'currentDistance': 0.9192172992969615}
episode index:779
target Thresh 20.253065047143565
target distance 7.0
model initialize at round 779
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 12.]), 'previousTarget': array([26., 12.]), 'currentState': array([20.8067565 ,  7.04374194,  0.        ]), 'targetState': array([26, 12], dtype=int32), 'currentDistance': 7.178737495569866}
done in step count: 4
reward sum = 0.8929116124816783
running average episode reward sum: 0.28912464642057994
{'scaleFactor': 20, 'currentTarget': array([26., 12.]), 'previousTarget': array([26., 12.]), 'currentState': array([25.18892699, 11.12470001,  0.        ]), 'targetState': array([26, 12], dtype=int32), 'currentDistance': 1.1933103085818044}
episode index:780
target Thresh 20.264806110586278
target distance 16.0
model initialize at round 780
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 24.]), 'previousTarget': array([11., 24.]), 'currentState': array([26.15448236, 17.20402265,  0.        ]), 'targetState': array([11, 24], dtype=int32), 'currentDistance': 16.608541294688845}
done in step count: 15
reward sum = 0.7529446013758156
running average episode reward sum: 0.28971852600438947
{'scaleFactor': 20, 'currentTarget': array([11., 24.]), 'previousTarget': array([11., 24.]), 'currentState': array([11.34282923, 23.94084162,  0.        ]), 'targetState': array([11, 24], dtype=int32), 'currentDistance': 0.3478959514334786}
episode index:781
target Thresh 20.276535438834124
target distance 7.0
model initialize at round 781
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 25.]), 'previousTarget': array([ 8., 25.]), 'currentState': array([ 7.10737979, 19.86225283,  0.        ]), 'targetState': array([ 8, 25], dtype=int32), 'currentDistance': 5.214711575254561}
done in step count: 3
reward sum = 0.9217807513582061
running average episode reward sum: 0.29052678971967566
{'scaleFactor': 20, 'currentTarget': array([ 8., 25.]), 'previousTarget': array([ 8., 25.]), 'currentState': array([ 7.95276756, 24.04950857,  0.        ]), 'targetState': array([ 8, 25], dtype=int32), 'currentDistance': 0.9516642574745617}
episode index:782
target Thresh 20.28825304361643
target distance 4.0
model initialize at round 782
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 27.]), 'previousTarget': array([13., 27.]), 'currentState': array([10.7274574 , 27.49244648,  0.        ]), 'targetState': array([13, 27], dtype=int32), 'currentDistance': 2.3252856572529583}
done in step count: 2
reward sum = 0.954758080254069
running average episode reward sum: 0.29137510554411294
{'scaleFactor': 20, 'currentTarget': array([13., 27.]), 'previousTarget': array([13., 27.]), 'currentState': array([12.11185485, 26.53235792,  0.        ]), 'targetState': array([13, 27], dtype=int32), 'currentDistance': 1.0037384703168999}
episode index:783
target Thresh 20.299958936650803
target distance 12.0
model initialize at round 783
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([17.65812981, 10.27149841,  0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 10.73370575736501}
done in step count: 11
reward sum = 0.8403764276855339
running average episode reward sum: 0.2920753623325586
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([7.90014011, 9.27979067, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.9426213601101884}
episode index:784
target Thresh 20.31165312964314
target distance 17.0
model initialize at round 784
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 29.]), 'previousTarget': array([ 2., 29.]), 'currentState': array([18.16163594, 19.24070835,  0.        ]), 'targetState': array([ 2, 29], dtype=int32), 'currentDistance': 18.87967821741131}
done in step count: 15
reward sum = 0.7379277640688936
running average episode reward sum: 0.2926433271755348
{'scaleFactor': 20, 'currentTarget': array([ 2., 29.]), 'previousTarget': array([ 2., 29.]), 'currentState': array([ 2.69733882, 28.89423381,  0.        ]), 'targetState': array([ 2, 29], dtype=int32), 'currentDistance': 0.705314055278734}
episode index:785
target Thresh 20.323335634287638
target distance 15.0
model initialize at round 785
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([12.99999332, 13.67356557,  0.        ]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 14.45766046805787}
done in step count: 8
reward sum = 0.7808201548096492
running average episode reward sum: 0.29326441728702857
{'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([25.25756508, 19.31707734,  0.        ]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 1.0087581326048567}
episode index:786
target Thresh 20.33500646226679
target distance 20.0
model initialize at round 786
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([21.04551494,  4.62443638,  0.        ]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 19.67995475593019}
done in step count: 12
reward sum = 0.7195666052239018
running average episode reward sum: 0.2938060973225265
{'scaleFactor': 20, 'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([14.4233591 , 22.46558601,  0.        ]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 0.6817853331554343}
episode index:787
target Thresh 20.346665625251433
target distance 12.0
model initialize at round 787
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 24.]), 'previousTarget': array([ 3., 24.]), 'currentState': array([ 2.99811474, 13.92464149,  0.        ]), 'targetState': array([ 3, 24], dtype=int32), 'currentDistance': 10.075358686397873}
done in step count: 6
reward sum = 0.8491173260394808
running average episode reward sum: 0.2945108070036394
{'scaleFactor': 20, 'currentTarget': array([ 3., 24.]), 'previousTarget': array([ 3., 24.]), 'currentState': array([ 3.20802815, 23.13466716,  0.        ]), 'targetState': array([ 3, 24], dtype=int32), 'currentDistance': 0.8899868760494224}
episode index:788
target Thresh 20.358313134900733
target distance 6.0
model initialize at round 788
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 22.]), 'previousTarget': array([27., 22.]), 'currentState': array([22.94670022, 21.4243353 ,  0.        ]), 'targetState': array([27, 22], dtype=int32), 'currentDistance': 4.093974717812938}
done in step count: 3
reward sum = 0.9304888175729102
running average episode reward sum: 0.2953168627838286
{'scaleFactor': 20, 'currentTarget': array([27., 22.]), 'previousTarget': array([27., 22.]), 'currentState': array([26.11601013, 21.42230949,  0.        ]), 'targetState': array([27, 22], dtype=int32), 'currentDistance': 1.056013455794603}
episode index:789
target Thresh 20.369949002862192
target distance 5.0
model initialize at round 789
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 12.]), 'previousTarget': array([22., 12.]), 'currentState': array([18.63494575, 10.57389444,  0.        ]), 'targetState': array([22, 12], dtype=int32), 'currentDistance': 3.654773203482984}
done in step count: 3
reward sum = 0.940179183659178
running average episode reward sum: 0.2961331442026582
{'scaleFactor': 20, 'currentTarget': array([22., 12.]), 'previousTarget': array([22., 12.]), 'currentState': array([21.33366382, 11.62246501,  0.        ]), 'targetState': array([22, 12], dtype=int32), 'currentDistance': 0.7658567545300636}
episode index:790
target Thresh 20.38157324077169
target distance 10.0
model initialize at round 790
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([ 7.9948864, 16.3186045,  0.       ]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 8.179788176127566}
done in step count: 5
reward sum = 0.8732656738441311
running average episode reward sum: 0.2968627681339369
{'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([15.13528812, 17.15323468,  0.        ]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 1.210263670166692}
episode index:791
target Thresh 20.39318586025346
target distance 15.0
model initialize at round 791
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([ 4.4102771 , 15.97897887,  0.        ]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 14.170109150056307}
done in step count: 7
reward sum = 0.792570935712175
running average episode reward sum: 0.29748866228491955
{'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([ 9.215808 , 28.0211612,  0.       ]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 1.2542258510586188}
episode index:792
target Thresh 20.40478687292012
target distance 15.0
model initialize at round 792
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 4.]), 'previousTarget': array([5., 4.]), 'currentState': array([18.68975174,  3.65786678,  0.        ]), 'targetState': array([5, 4], dtype=int32), 'currentDistance': 13.694026360523978}
done in step count: 13
reward sum = 0.7984591773475047
running average episode reward sum: 0.29812040316141714
{'scaleFactor': 20, 'currentTarget': array([5., 4.]), 'previousTarget': array([5., 4.]), 'currentState': array([5.86721534, 4.2633072 , 0.        ]), 'targetState': array([5, 4], dtype=int32), 'currentDistance': 0.9063074075175888}
episode index:793
target Thresh 20.41637629037269
target distance 4.0
model initialize at round 793
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  3.]), 'previousTarget': array([11.,  3.]), 'currentState': array([14.63921618,  5.43743753,  0.        ]), 'targetState': array([11,  3], dtype=int32), 'currentDistance': 4.3800680556519955}
done in step count: 4
reward sum = 0.9450429691211384
running average episode reward sum: 0.29893516709839413
{'scaleFactor': 20, 'currentTarget': array([11.,  3.]), 'previousTarget': array([11.,  3.]), 'currentState': array([11.86599341,  3.72944266,  0.        ]), 'targetState': array([11,  3], dtype=int32), 'currentDistance': 1.132268155172832}
episode index:794
target Thresh 20.427954124200582
target distance 18.0
model initialize at round 794
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([25.95679164, 18.93935537,  0.        ]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 17.69584433522459}
done in step count: 15
reward sum = 0.7504744514421651
running average episode reward sum: 0.29950314104096487
{'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([ 9.86728483, 24.19950624,  0.        ]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 0.8899357971203247}
episode index:795
target Thresh 20.439520385981638
target distance 13.0
model initialize at round 795
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 29.]), 'previousTarget': array([16., 29.]), 'currentState': array([13.44347253, 17.98133326,  0.        ]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 11.31135931277578}
done in step count: 7
reward sum = 0.8215127793794262
running average episode reward sum: 0.3001589320439026
{'scaleFactor': 20, 'currentTarget': array([16., 29.]), 'previousTarget': array([16., 29.]), 'currentState': array([15.8056621 , 28.52099454,  0.        ]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 0.5169269263959291}
episode index:796
target Thresh 20.451075087282113
target distance 14.0
model initialize at round 796
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 14.]), 'previousTarget': array([19., 14.]), 'currentState': array([ 6.99160552, 22.10510719,  0.        ]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 14.487729308416233}
done in step count: 8
reward sum = 0.7721854026097581
running average episode reward sum: 0.30075118608476314
{'scaleFactor': 20, 'currentTarget': array([19., 14.]), 'previousTarget': array([19., 14.]), 'currentState': array([18.23019594, 13.71383664,  0.        ]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 0.8212720377787809}
episode index:797
target Thresh 20.462618239656717
target distance 18.0
model initialize at round 797
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 27.]), 'previousTarget': array([14., 27.]), 'currentState': array([21.92904186, 10.46869111,  0.        ]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 18.334499678297163}
done in step count: 11
reward sum = 0.745100663497822
running average episode reward sum: 0.30130801500382715
{'scaleFactor': 20, 'currentTarget': array([14., 27.]), 'previousTarget': array([14., 27.]), 'currentState': array([14.71563643, 26.2293604 ,  0.        ]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 1.0516752795546516}
episode index:798
target Thresh 20.474149854648594
target distance 7.0
model initialize at round 798
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 27.]), 'previousTarget': array([16., 27.]), 'currentState': array([22.05711746, 28.20512798,  0.        ]), 'targetState': array([16, 27], dtype=int32), 'currentDistance': 6.175840461741996}
done in step count: 6
reward sum = 0.9144070196117056
running average episode reward sum: 0.30207534792573937
{'scaleFactor': 20, 'currentTarget': array([16., 27.]), 'previousTarget': array([16., 27.]), 'currentState': array([16.92795408, 27.19846159,  0.        ]), 'targetState': array([16, 27], dtype=int32), 'currentDistance': 0.9489392888566888}
episode index:799
target Thresh 20.485669943789368
target distance 13.0
model initialize at round 799
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([ 9.64512753, 20.38889289,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 11.872744017073096}
done in step count: 7
reward sum = 0.8200006036218912
running average episode reward sum: 0.3027227544953595
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([12.73022731,  9.36786604,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.45618277586844447}
episode index:800
target Thresh 20.497178518599128
target distance 20.0
model initialize at round 800
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  4.]), 'previousTarget': array([13.,  4.]), 'currentState': array([13.93465006, 22.26663017,  0.        ]), 'targetState': array([13,  4], dtype=int32), 'currentDistance': 18.29052619823984}
done in step count: 11
reward sum = 0.725490018878785
running average episode reward sum: 0.30325055382667465
{'scaleFactor': 20, 'currentTarget': array([13.,  4.]), 'previousTarget': array([13.,  4.]), 'currentState': array([13.08457195,  4.51553959,  0.        ]), 'targetState': array([13,  4], dtype=int32), 'currentDistance': 0.5224303582837713}
episode index:801
target Thresh 20.508675590586442
target distance 7.0
model initialize at round 801
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 21.]), 'previousTarget': array([ 5., 21.]), 'currentState': array([10.50620717, 27.10551977,  0.        ]), 'targetState': array([ 5, 21], dtype=int32), 'currentDistance': 8.221659750875476}
done in step count: 9
reward sum = 0.8776526888313182
running average episode reward sum: 0.30396676596508443
{'scaleFactor': 20, 'currentTarget': array([ 5., 21.]), 'previousTarget': array([ 5., 21.]), 'currentState': array([ 5.85266244, 21.5458647 ,  0.        ]), 'targetState': array([ 5, 21], dtype=int32), 'currentDistance': 1.0124235851152972}
episode index:802
target Thresh 20.520161171248397
target distance 9.0
model initialize at round 802
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.31799012,  9.05295688,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.287237077438727}
done in step count: 11
reward sum = 0.8484095079704651
running average episode reward sum: 0.3046447768517661
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.66835892, 3.60921031, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9043455386726931}
episode index:803
target Thresh 20.53163527207056
target distance 6.0
model initialize at round 803
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([24.52262038, 13.41914088,  0.        ]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 6.495372213058979}
done in step count: 7
reward sum = 0.9052158460294794
running average episode reward sum: 0.30539175579352945
{'scaleFactor': 20, 'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([19.51345599, 10.43957762,  0.        ]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 0.6759182925924017}
episode index:804
target Thresh 20.543097904527045
target distance 10.0
model initialize at round 804
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 16.]), 'previousTarget': array([15., 16.]), 'currentState': array([ 6.98392916, 10.13551573,  0.        ]), 'targetState': array([15, 16], dtype=int32), 'currentDistance': 9.932248861941819}
done in step count: 6
reward sum = 0.844503459472038
running average episode reward sum: 0.30606145977325433
{'scaleFactor': 20, 'currentTarget': array([15., 16.]), 'previousTarget': array([15., 16.]), 'currentState': array([14.47060701, 15.49696501,  0.        ]), 'targetState': array([15, 16], dtype=int32), 'currentDistance': 0.7302747020116845}
episode index:805
target Thresh 20.554549080080477
target distance 18.0
model initialize at round 805
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 7.]), 'previousTarget': array([5., 7.]), 'currentState': array([21.70064139, 14.4616158 ,  0.        ]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 18.29172308229283}
done in step count: 17
reward sum = 0.7423888380109719
running average episode reward sum: 0.30660280887776764
{'scaleFactor': 20, 'currentTarget': array([5., 7.]), 'previousTarget': array([5., 7.]), 'currentState': array([5.77046353, 7.62267077, 0.        ]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 0.9906224975779324}
episode index:806
target Thresh 20.565988810182034
target distance 17.0
model initialize at round 806
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 17.]), 'previousTarget': array([20., 17.]), 'currentState': array([ 4.99946845, 22.16537905,  0.        ]), 'targetState': array([20, 17], dtype=int32), 'currentDistance': 15.8649641541501}
done in step count: 8
reward sum = 0.7307253311272129
running average episode reward sum: 0.3071283634282626
{'scaleFactor': 20, 'currentTarget': array([20., 17.]), 'previousTarget': array([20., 17.]), 'currentState': array([19.07289249, 16.46465458,  0.        ]), 'targetState': array([20, 17], dtype=int32), 'currentDistance': 1.070571370597405}
episode index:807
target Thresh 20.577417106271447
target distance 7.0
model initialize at round 807
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([16.64925981,  9.49916512,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 5.671417100809265}
done in step count: 5
reward sum = 0.9139985178753953
running average episode reward sum: 0.3078794403520833
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([11.55893129, 10.14062247,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.5763496051805773}
episode index:808
target Thresh 20.588833979777018
target distance 20.0
model initialize at round 808
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 26.]), 'previousTarget': array([22., 26.]), 'currentState': array([4.99999988, 7.76901901, 0.        ]), 'targetState': array([22, 26], dtype=int32), 'currentDistance': 24.927267639983093}
done in step count: 13
reward sum = 0.6123636120993394
running average episode reward sum: 0.308255811392562
{'scaleFactor': 20, 'currentTarget': array([22., 26.]), 'previousTarget': array([22., 26.]), 'currentState': array([21.31681895, 25.24487931,  0.        ]), 'targetState': array([22, 26], dtype=int32), 'currentDistance': 1.0183042801884545}
episode index:809
target Thresh 20.600239442115615
target distance 11.0
model initialize at round 809
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([23.82596922,  2.49621999,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 10.43197706270538}
done in step count: 8
reward sum = 0.8530473555651225
running average episode reward sum: 0.3089283935458614
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.48206198,  5.8229264 ,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.513555073692269}
episode index:810
target Thresh 20.611633504692705
target distance 12.0
model initialize at round 810
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 25.]), 'previousTarget': array([ 3., 25.]), 'currentState': array([13.84692657, 22.59738815,  0.        ]), 'targetState': array([ 3, 25], dtype=int32), 'currentDistance': 11.109831669450784}
done in step count: 9
reward sum = 0.8430980460599874
running average episode reward sum: 0.3095870490976667
{'scaleFactor': 20, 'currentTarget': array([ 3., 25.]), 'previousTarget': array([ 3., 25.]), 'currentState': array([ 3.66608071, 25.12753938,  0.        ]), 'targetState': array([ 3, 25], dtype=int32), 'currentDistance': 0.6781812518766256}
episode index:811
target Thresh 20.623016178902354
target distance 15.0
model initialize at round 811
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 19.]), 'previousTarget': array([14., 19.]), 'currentState': array([14.90761561,  5.91644311,  0.        ]), 'targetState': array([14, 19], dtype=int32), 'currentDistance': 13.115000076293363}
done in step count: 8
reward sum = 0.80338056323322
running average episode reward sum: 0.31019516918896667
{'scaleFactor': 20, 'currentTarget': array([14., 19.]), 'previousTarget': array([14., 19.]), 'currentState': array([14.20852504, 18.405496  ,  0.        ]), 'targetState': array([14, 19], dtype=int32), 'currentDistance': 0.6300140434785841}
episode index:812
target Thresh 20.63438747612723
target distance 13.0
model initialize at round 812
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 20.]), 'previousTarget': array([ 7., 20.]), 'currentState': array([18.93133163, 10.84979904,  0.        ]), 'targetState': array([ 7, 20], dtype=int32), 'currentDistance': 15.036051750263175}
done in step count: 11
reward sum = 0.7963019217978037
running average episode reward sum: 0.3107930864738484
{'scaleFactor': 20, 'currentTarget': array([ 7., 20.]), 'previousTarget': array([ 7., 20.]), 'currentState': array([ 7.53136849, 19.2984215 ,  0.        ]), 'targetState': array([ 7, 20], dtype=int32), 'currentDistance': 0.8800936683947703}
episode index:813
target Thresh 20.645747407738636
target distance 15.0
model initialize at round 813
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 26.]), 'previousTarget': array([ 4., 26.]), 'currentState': array([ 4.81921935, 12.90584254,  0.        ]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 13.11975913889685}
done in step count: 8
reward sum = 0.8011443183274939
running average episode reward sum: 0.3113954835645777
{'scaleFactor': 20, 'currentTarget': array([ 4., 26.]), 'previousTarget': array([ 4., 26.]), 'currentState': array([ 4.23126805, 25.53157878,  0.        ]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 0.5224015241061326}
episode index:814
target Thresh 20.657095985096504
target distance 14.0
model initialize at round 814
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 12.]), 'previousTarget': array([23., 12.]), 'currentState': array([10.97431684, 22.12785876,  0.        ]), 'targetState': array([23, 12], dtype=int32), 'currentDistance': 15.722295589280344}
done in step count: 9
reward sum = 0.7586239424782713
running average episode reward sum: 0.3119442301399319
{'scaleFactor': 20, 'currentTarget': array([23., 12.]), 'previousTarget': array([23., 12.]), 'currentState': array([22.28436351, 11.8637495 ,  0.        ]), 'targetState': array([23, 12], dtype=int32), 'currentDistance': 0.7284914453834874}
episode index:815
target Thresh 20.66843321954941
target distance 12.0
model initialize at round 815
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'previousTarget': array([27.,  8.]), 'currentState': array([16.91774392, 17.42293334,  0.        ]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 13.80012899824992}
done in step count: 8
reward sum = 0.7792063112453076
running average episode reward sum: 0.3125168552393257
{'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'previousTarget': array([27.,  8.]), 'currentState': array([26.4058432 ,  7.94388431,  0.        ]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 0.5968008673947961}
episode index:816
target Thresh 20.679759122434593
target distance 3.0
model initialize at round 816
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.79965027, 5.27510285, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 1.9012392574830541}
done in step count: 1
reward sum = 0.9776143424297724
running average episode reward sum: 0.31333092805106433
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.50212963, 6.12995881, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 1.0045426021913415}
episode index:817
target Thresh 20.691073705077955
target distance 3.0
model initialize at round 817
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  4.]), 'previousTarget': array([24.,  4.]), 'currentState': array([25.83602905,  3.99951746,  0.        ]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 1.836029116145327}
done in step count: 1
reward sum = 0.9785077225362978
running average episode reward sum: 0.3141441026164497
{'scaleFactor': 20, 'currentTarget': array([24.,  4.]), 'previousTarget': array([24.,  4.]), 'currentState': array([24.86182338,  4.06340534,  0.        ]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 0.8641526345202052}
episode index:818
target Thresh 20.702376978794078
target distance 11.0
model initialize at round 818
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([17.89974117, 27.77209847,  0.        ]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 11.967465282971338}
done in step count: 7
reward sum = 0.8168254637420421
running average episode reward sum: 0.3147578771721586
{'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([26.27952999, 19.87000285,  0.        ]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 0.7321040198388732}
episode index:819
target Thresh 20.71366895488624
target distance 17.0
model initialize at round 819
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 10.]), 'previousTarget': array([20., 10.]), 'currentState': array([ 4.99998176, 13.95843649,  0.        ]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 15.513534948851966}
done in step count: 8
reward sum = 0.7621450199486025
running average episode reward sum: 0.3153034712487152
{'scaleFactor': 20, 'currentTarget': array([20., 10.]), 'previousTarget': array([20., 10.]), 'currentState': array([19.01577985,  9.72686312,  0.        ]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 1.0214171799071534}
episode index:820
target Thresh 20.724949644646415
target distance 20.0
model initialize at round 820
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 18.]), 'previousTarget': array([27., 18.]), 'currentState': array([ 9.        , 18.32509488,  0.        ]), 'targetState': array([27, 18], dtype=int32), 'currentDistance': 18.002935501749164}
done in step count: 10
reward sum = 0.7217597317824807
running average episode reward sum: 0.3157985458656869
{'scaleFactor': 20, 'currentTarget': array([27., 18.]), 'previousTarget': array([27., 18.]), 'currentState': array([26.18613148, 17.65561524,  0.        ]), 'targetState': array([27, 18], dtype=int32), 'currentDistance': 0.883732333575078}
episode index:821
target Thresh 20.7362190593553
target distance 15.0
model initialize at round 821
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([14.32646465, 19.73725605,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 17.80452204700474}
done in step count: 15
reward sum = 0.7218032150044678
running average episode reward sum: 0.31629246882084355
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.90028244, 6.75616008, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 1.1757068215437672}
episode index:822
target Thresh 20.747477210282298
target distance 12.0
model initialize at round 822
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([ 3.9998219 , 13.23949295,  0.        ]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 10.003045480239676}
done in step count: 6
reward sum = 0.8411575788479849
running average episode reward sum: 0.3169302150055667
{'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([13.26461369, 12.56196217,  0.        ]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 0.8559615465442435}
episode index:823
target Thresh 20.758724108685577
target distance 18.0
model initialize at round 823
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([23.51307487, 22.80446327,  0.        ]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 20.370097662443047}
done in step count: 14
reward sum = 0.7004429679037016
running average episode reward sum: 0.317395643103744
{'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([12.6125899 ,  6.82078081,  0.        ]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 1.0241813923683039}
episode index:824
target Thresh 20.769959765812022
target distance 19.0
model initialize at round 824
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 24.]), 'previousTarget': array([ 5., 24.]), 'currentState': array([15.67599714,  6.3501631 ,  0.        ]), 'targetState': array([ 5, 24], dtype=int32), 'currentDistance': 20.627497604212415}
done in step count: 13
reward sum = 0.7130015030665826
running average episode reward sum: 0.3178751653582445
{'scaleFactor': 20, 'currentTarget': array([ 5., 24.]), 'previousTarget': array([ 5., 24.]), 'currentState': array([ 5.55541059, 23.25555861,  0.        ]), 'targetState': array([ 5, 24], dtype=int32), 'currentDistance': 0.9288024067843244}
episode index:825
target Thresh 20.7811841928973
target distance 13.0
model initialize at round 825
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 4.76992846, 21.57176507,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 15.445391239221697}
done in step count: 9
reward sum = 0.7654334862992234
running average episode reward sum: 0.31841700351919
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.38942891, 10.14141905,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.6267347127207525}
episode index:826
target Thresh 20.792397401165832
target distance 16.0
model initialize at round 826
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 24.]), 'previousTarget': array([ 2., 24.]), 'currentState': array([16.55688643, 25.99274634,  0.        ]), 'targetState': array([ 2, 24], dtype=int32), 'currentDistance': 14.692650565153622}
done in step count: 13
reward sum = 0.7929191342845381
running average episode reward sum: 0.31899076667610093
{'scaleFactor': 20, 'currentTarget': array([ 2., 24.]), 'previousTarget': array([ 2., 24.]), 'currentState': array([ 2.81788939, 24.49205129,  0.        ]), 'targetState': array([ 2, 24], dtype=int32), 'currentDistance': 0.9544933351403845}
episode index:827
target Thresh 20.803599401830837
target distance 16.0
model initialize at round 827
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([20.36368853, 27.69507527,  0.        ]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 16.01379933003042}
done in step count: 14
reward sum = 0.7649641038730597
running average episode reward sum: 0.3195293818176432
{'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([14.78282225, 13.61221474,  0.        ]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 0.9937895013334098}
episode index:828
target Thresh 20.814790206094305
target distance 8.0
model initialize at round 828
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([ 9.14626193, 14.1617415 ,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 9.912833437967398}
done in step count: 5
reward sum = 0.8606929156621281
running average episode reward sum: 0.3201821725701697
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.13534909,  7.38575184,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.9467975879103192}
episode index:829
target Thresh 20.825969825147048
target distance 16.0
model initialize at round 829
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 29.]), 'previousTarget': array([ 6., 29.]), 'currentState': array([ 3.76171666, 14.9880743 ,  0.        ]), 'targetState': array([ 6, 29], dtype=int32), 'currentDistance': 14.189572722261413}
done in step count: 8
reward sum = 0.7844395586079855
running average episode reward sum: 0.320741518818408
{'scaleFactor': 20, 'currentTarget': array([ 6., 29.]), 'previousTarget': array([ 6., 29.]), 'currentState': array([ 6.15384128, 28.1826809 ,  0.        ]), 'targetState': array([ 6, 29], dtype=int32), 'currentDistance': 0.831671596052464}
episode index:830
target Thresh 20.837138270168687
target distance 5.0
model initialize at round 830
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([13.85676032, 17.69465256,  0.        ]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 3.9393905657275288}
done in step count: 3
reward sum = 0.9310084591934358
running average episode reward sum: 0.3214758954012901
{'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([15.67555884, 20.52614671,  0.        ]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 0.5742812985752546}
episode index:831
target Thresh 20.848295552327667
target distance 15.0
model initialize at round 831
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 28.]), 'previousTarget': array([11., 28.]), 'currentState': array([21.63903022, 13.90764719,  0.        ]), 'targetState': array([11, 28], dtype=int32), 'currentDistance': 17.657388584777525}
done in step count: 12
reward sum = 0.7552064878433309
running average episode reward sum: 0.3219972062095137
{'scaleFactor': 20, 'currentTarget': array([11., 28.]), 'previousTarget': array([11., 28.]), 'currentState': array([11.67038092, 27.23280013,  0.        ]), 'targetState': array([11, 28], dtype=int32), 'currentDistance': 1.0188259049903479}
episode index:832
target Thresh 20.859441682781267
target distance 15.0
model initialize at round 832
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 16.]), 'previousTarget': array([ 2., 16.]), 'currentState': array([15.59833241, 22.56189486,  0.        ]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 15.098778374539695}
done in step count: 15
reward sum = 0.7795671707390784
running average episode reward sum: 0.32254650988842076
{'scaleFactor': 20, 'currentTarget': array([ 2., 16.]), 'previousTarget': array([ 2., 16.]), 'currentState': array([ 2.46627522, 16.36551194,  0.        ]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 0.5924622804553897}
episode index:833
target Thresh 20.870576672675625
target distance 11.0
model initialize at round 833
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 16.]), 'previousTarget': array([13., 16.]), 'currentState': array([16.38302928,  6.55572832,  0.        ]), 'targetState': array([13, 16], dtype=int32), 'currentDistance': 10.031906835637205}
done in step count: 6
reward sum = 0.8589352195033548
running average episode reward sum: 0.3231896618184147
{'scaleFactor': 20, 'currentTarget': array([13., 16.]), 'previousTarget': array([13., 16.]), 'currentState': array([13.48776561, 15.12511283,  0.        ]), 'targetState': array([13, 16], dtype=int32), 'currentDistance': 1.0016700294785337}
episode index:834
target Thresh 20.881700533145725
target distance 10.0
model initialize at round 834
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([7.97448945, 6.70192885, 0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 8.203985702382788}
done in step count: 5
reward sum = 0.8542494838493317
running average episode reward sum: 0.3238256616052781
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.28911287,  4.34115882,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.9692431144900228}
episode index:835
target Thresh 20.892813275315433
target distance 4.0
model initialize at round 835
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 17.]), 'previousTarget': array([ 3., 17.]), 'currentState': array([ 6.82452911, 20.23018014,  0.        ]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 5.006104941821336}
done in step count: 5
reward sum = 0.9291191373075858
running average episode reward sum: 0.32454969686329516
{'scaleFactor': 20, 'currentTarget': array([ 3., 17.]), 'previousTarget': array([ 3., 17.]), 'currentState': array([ 3.46793851, 17.56016889,  0.        ]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 0.7299011160887134}
episode index:836
target Thresh 20.903914910297487
target distance 20.0
model initialize at round 836
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 24.]), 'previousTarget': array([ 4., 24.]), 'currentState': array([22.60587358, 28.68780732,  0.        ]), 'targetState': array([ 4, 24], dtype=int32), 'currentDistance': 19.187341382681588}
done in step count: 17
reward sum = 0.7291622562841207
running average episode reward sum: 0.32503310493906673
{'scaleFactor': 20, 'currentTarget': array([ 4., 24.]), 'previousTarget': array([ 4., 24.]), 'currentState': array([ 4.40228885, 24.4134699 ,  0.        ]), 'targetState': array([ 4, 24], dtype=int32), 'currentDistance': 0.5768827289660666}
episode index:837
target Thresh 20.91500544919353
target distance 8.0
model initialize at round 837
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 23.]), 'previousTarget': array([18., 23.]), 'currentState': array([11.64889073, 26.73328966,  0.        ]), 'targetState': array([18, 23], dtype=int32), 'currentDistance': 7.367091732184034}
done in step count: 4
reward sum = 0.8930877733332274
running average episode reward sum: 0.3257109744717566
{'scaleFactor': 20, 'currentTarget': array([18., 23.]), 'previousTarget': array([18., 23.]), 'currentState': array([17.00664288, 23.04542118,  0.        ]), 'targetState': array([18, 23], dtype=int32), 'currentDistance': 0.9943950198952604}
episode index:838
target Thresh 20.9260849030941
target distance 13.0
model initialize at round 838
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 13.]), 'previousTarget': array([25., 13.]), 'currentState': array([13.99954689,  5.67610914,  0.        ]), 'targetState': array([25, 13], dtype=int32), 'currentDistance': 13.215496438830542}
done in step count: 9
reward sum = 0.7985236942720729
running average episode reward sum: 0.32627451764195964
{'scaleFactor': 20, 'currentTarget': array([25., 13.]), 'previousTarget': array([25., 13.]), 'currentState': array([24.41368753, 12.39071697,  0.        ]), 'targetState': array([25, 13], dtype=int32), 'currentDistance': 0.8455697052697634}
episode index:839
target Thresh 20.93715328307865
target distance 6.0
model initialize at round 839
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  4.]), 'previousTarget': array([21.,  4.]), 'currentState': array([25.57870448,  3.48104307,  0.        ]), 'targetState': array([21,  4], dtype=int32), 'currentDistance': 4.608020288195939}
done in step count: 3
reward sum = 0.9358022002989557
running average episode reward sum: 0.32700014583559894
{'scaleFactor': 20, 'currentTarget': array([21.,  4.]), 'previousTarget': array([21.,  4.]), 'currentState': array([21.87707055,  3.99210946,  0.        ]), 'targetState': array([21,  4], dtype=int32), 'currentDistance': 0.8771060389167463}
episode index:840
target Thresh 20.94821060021556
target distance 7.0
model initialize at round 840
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  7.]), 'previousTarget': array([26.,  7.]), 'currentState': array([22.84365785, 12.33981943,  0.        ]), 'targetState': array([26,  7], dtype=int32), 'currentDistance': 6.202916033537103}
done in step count: 3
reward sum = 0.9048089533608619
running average episode reward sum: 0.32768719554728176
{'scaleFactor': 20, 'currentTarget': array([26.,  7.]), 'previousTarget': array([26.,  7.]), 'currentState': array([25.34364253,  7.55050445,  0.        ]), 'targetState': array([26,  7], dtype=int32), 'currentDistance': 0.8566564479499781}
episode index:841
target Thresh 20.959256865562146
target distance 2.0
model initialize at round 841
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 14.]), 'previousTarget': array([11., 14.]), 'currentState': array([12.29359102, 15.29286343,  0.        ]), 'targetState': array([11, 14], dtype=int32), 'currentDistance': 1.8288995539565875}
done in step count: 1
reward sum = 0.9817331505764587
running average episode reward sum: 0.3284639722159625
{'scaleFactor': 20, 'currentTarget': array([11., 14.]), 'previousTarget': array([11., 14.]), 'currentState': array([11.67688268, 14.763152  ,  0.        ]), 'targetState': array([11, 14], dtype=int32), 'currentDistance': 1.0200838927923495}
episode index:842
target Thresh 20.970292090164683
target distance 13.0
model initialize at round 842
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([12.99568117,  6.07168919,  0.        ]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 11.733443051109711}
done in step count: 7
reward sum = 0.811991864070381
running average episode reward sum: 0.32903755215885033
{'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([23.16059476,  1.60933222,  0.        ]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 0.9258630938080434}
episode index:843
target Thresh 20.98131628505839
target distance 8.0
model initialize at round 843
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([5.98521578, 8.32263425, 0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 6.052805426825817}
done in step count: 4
reward sum = 0.9012482047355767
running average episode reward sum: 0.3297155268656948
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([11.13271868,  8.53563672,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.98377341752953}
episode index:844
target Thresh 20.99232946126746
target distance 17.0
model initialize at round 844
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 29.]), 'previousTarget': array([ 7., 29.]), 'currentState': array([18.60344744, 12.8950066 ,  0.        ]), 'targetState': array([ 7, 29], dtype=int32), 'currentDistance': 19.84970541189936}
done in step count: 14
reward sum = 0.7245337949062542
running average episode reward sum: 0.33018276741958896
{'scaleFactor': 20, 'currentTarget': array([ 7., 29.]), 'previousTarget': array([ 7., 29.]), 'currentState': array([ 7.59535637, 28.37824339,  0.        ]), 'targetState': array([ 7, 29], dtype=int32), 'currentDistance': 0.8608312839073118}
episode index:845
target Thresh 21.003331629805075
target distance 6.0
model initialize at round 845
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 28.]), 'previousTarget': array([18., 28.]), 'currentState': array([13.91614556, 28.10634499,  0.        ]), 'targetState': array([18, 28], dtype=int32), 'currentDistance': 4.085238832500281}
done in step count: 3
reward sum = 0.9314706311554035
running average episode reward sum: 0.330893509575305
{'scaleFactor': 20, 'currentTarget': array([18., 28.]), 'previousTarget': array([18., 28.]), 'currentState': array([17.11165583, 27.47439546,  0.        ]), 'targetState': array([18, 28], dtype=int32), 'currentDistance': 1.0321896594209323}
episode index:846
target Thresh 21.01432280167341
target distance 8.0
model initialize at round 846
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 20.]), 'previousTarget': array([19., 20.]), 'currentState': array([19.43870926, 26.16702282,  0.        ]), 'targetState': array([19, 20], dtype=int32), 'currentDistance': 6.182607567139031}
done in step count: 4
reward sum = 0.9007172011572615
running average episode reward sum: 0.3315662648192034
{'scaleFactor': 20, 'currentTarget': array([19., 20.]), 'previousTarget': array([19., 20.]), 'currentState': array([18.95239881, 20.62527156,  0.        ]), 'targetState': array([19, 20], dtype=int32), 'currentDistance': 0.6270808526731484}
episode index:847
target Thresh 21.025302987863622
target distance 9.0
model initialize at round 847
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 26.]), 'previousTarget': array([14., 26.]), 'currentState': array([21.60880876, 23.36700594,  0.        ]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 8.051498517734187}
done in step count: 6
reward sum = 0.8833414040289473
running average episode reward sum: 0.3322169430494036
{'scaleFactor': 20, 'currentTarget': array([14., 26.]), 'previousTarget': array([14., 26.]), 'currentState': array([14.57875472, 25.83335033,  0.        ]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 0.6022699907781488}
episode index:848
target Thresh 21.036272199355913
target distance 6.0
model initialize at round 848
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([10.79151487, 13.3421123 ,  0.        ]), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.523266372492945}
done in step count: 4
reward sum = 0.9233894279132749
running average episode reward sum: 0.33291325928599236
{'scaleFactor': 20, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.43381453, 14.51030745,  0.        ]), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7485751724041024}
episode index:849
target Thresh 21.04723044711949
target distance 3.0
model initialize at round 849
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([21.58848894,  4.3401053 ,  0.        ]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 1.946341615639458}
done in step count: 1
reward sum = 0.966278511333883
running average episode reward sum: 0.33365839487663695
{'scaleFactor': 20, 'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([22.1600334 ,  3.18569708,  0.        ]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 0.8602483862500478}
episode index:850
target Thresh 21.058177742112598
target distance 5.0
model initialize at round 850
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 21.]), 'previousTarget': array([ 4., 21.]), 'currentState': array([ 4.58434877, 24.32431662,  0.        ]), 'targetState': array([ 4, 21], dtype=int32), 'currentDistance': 3.3752843551561607}
done in step count: 2
reward sum = 0.949195799268199
running average episode reward sum: 0.3343817055750994
{'scaleFactor': 20, 'currentTarget': array([ 4., 21.]), 'previousTarget': array([ 4., 21.]), 'currentState': array([ 4.15811357, 21.867598  ,  0.        ]), 'targetState': array([ 4, 21], dtype=int32), 'currentDistance': 0.8818878530935739}
episode index:851
target Thresh 21.069114095282536
target distance 6.0
model initialize at round 851
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 14.]), 'previousTarget': array([21., 14.]), 'currentState': array([19.65424186,  9.79188442,  0.        ]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 4.418065378293329}
done in step count: 3
reward sum = 0.9263949031811303
running average episode reward sum: 0.33507655674599846
{'scaleFactor': 20, 'currentTarget': array([21., 14.]), 'previousTarget': array([21., 14.]), 'currentState': array([20.93868639, 13.37464255,  0.        ]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 0.6283560283633896}
episode index:852
target Thresh 21.08003951756566
target distance 15.0
model initialize at round 852
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([11.99978757, 22.17209151,  0.        ]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 13.991284925902903}
done in step count: 9
reward sum = 0.7807297753659512
running average episode reward sum: 0.33559901069514264
{'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([24.12546587, 16.36663277,  0.        ]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 1.0797981276674895}
episode index:853
target Thresh 21.09095401988739
target distance 20.0
model initialize at round 853
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  2.]), 'previousTarget': array([23.,  2.]), 'currentState': array([ 6.98900831, 21.12421882,  0.        ]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 24.94168399511708}
done in step count: 14
reward sum = 0.6120079018696101
running average episode reward sum: 0.3359226745021385
{'scaleFactor': 20, 'currentTarget': array([23.,  2.]), 'previousTarget': array([23.,  2.]), 'currentState': array([22.55337202,  2.83062202,  0.        ]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 0.9430850898537957}
episode index:854
target Thresh 21.101857613162235
target distance 10.0
model initialize at round 854
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 24.]), 'previousTarget': array([12., 24.]), 'currentState': array([ 3.99337542, 21.14596933,  0.        ]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 8.50008989446642}
done in step count: 6
reward sum = 0.8629458279199701
running average episode reward sum: 0.3365390758511652
{'scaleFactor': 20, 'currentTarget': array([12., 24.]), 'previousTarget': array([12., 24.]), 'currentState': array([11.58364815, 23.56016397,  0.        ]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 0.6056439515867819}
episode index:855
target Thresh 21.112750308293784
target distance 13.0
model initialize at round 855
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 18.]), 'previousTarget': array([23., 18.]), 'currentState': array([11.99984086, 11.81523246,  0.        ]), 'targetState': array([23, 18], dtype=int32), 'currentDistance': 12.619621657514651}
done in step count: 7
reward sum = 0.8093182930807827
running average episode reward sum: 0.33709138802082594
{'scaleFactor': 20, 'currentTarget': array([23., 18.]), 'previousTarget': array([23., 18.]), 'currentState': array([22.2392692 , 17.12607479,  0.        ]), 'targetState': array([23, 18], dtype=int32), 'currentDistance': 1.1586443050642021}
episode index:856
target Thresh 21.123632116174733
target distance 8.0
model initialize at round 856
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 12.]), 'previousTarget': array([16., 12.]), 'currentState': array([18.26884031,  5.56110895,  0.        ]), 'targetState': array([16, 12], dtype=int32), 'currentDistance': 6.8269286183963285}
done in step count: 5
reward sum = 0.8924604014594687
running average episode reward sum: 0.3377394265429247
{'scaleFactor': 20, 'currentTarget': array([16., 12.]), 'previousTarget': array([16., 12.]), 'currentState': array([16.2391023 , 11.55202377,  0.        ]), 'targetState': array([16, 12], dtype=int32), 'currentDistance': 0.5077919020874555}
episode index:857
target Thresh 21.134503047686888
target distance 10.0
model initialize at round 857
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 16.]), 'previousTarget': array([24., 16.]), 'currentState': array([15.81840932, 21.26449811,  0.        ]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 9.728996165747219}
done in step count: 6
reward sum = 0.852300729301604
running average episode reward sum: 0.33833914834101175
{'scaleFactor': 20, 'currentTarget': array([24., 16.]), 'previousTarget': array([24., 16.]), 'currentState': array([23.2306878 , 15.86712265,  0.        ]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 0.780703308358424}
episode index:858
target Thresh 21.145363113701187
target distance 11.0
model initialize at round 858
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([22.30413657, 15.2091732 ,  0.        ]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 9.301056030271805}
done in step count: 5
reward sum = 0.8625441470527516
running average episode reward sum: 0.33894939863054807
{'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([20.92914218,  6.94877851,  0.        ]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 0.9514207750647359}
episode index:859
target Thresh 21.156212325077696
target distance 3.0
model initialize at round 859
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 26.]), 'previousTarget': array([ 7., 26.]), 'currentState': array([ 6.60810167, 24.25744581,  0.        ]), 'targetState': array([ 7, 26], dtype=int32), 'currentDistance': 1.7860793379755382}
done in step count: 1
reward sum = 0.9762605844922738
running average episode reward sum: 0.33969045814899196
{'scaleFactor': 20, 'currentTarget': array([ 7., 26.]), 'previousTarget': array([ 7., 26.]), 'currentState': array([ 6.87481818, 25.11408907,  0.        ]), 'targetState': array([ 7, 26], dtype=int32), 'currentDistance': 0.8947114964297759}
episode index:860
target Thresh 21.16705069266563
target distance 10.0
model initialize at round 860
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 29.]), 'previousTarget': array([ 2., 29.]), 'currentState': array([10.58907866, 20.43821087,  0.        ]), 'targetState': array([ 2, 29], dtype=int32), 'currentDistance': 12.127510269393177}
done in step count: 8
reward sum = 0.8375862802777468
running average episode reward sum: 0.3402687343651693
{'scaleFactor': 20, 'currentTarget': array([ 2., 29.]), 'previousTarget': array([ 2., 29.]), 'currentState': array([ 2.92019087, 28.03332624,  0.        ]), 'targetState': array([ 2, 29], dtype=int32), 'currentDistance': 1.3346195712680284}
episode index:861
target Thresh 21.177878227303353
target distance 5.0
model initialize at round 861
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'previousTarget': array([13., 22.]), 'currentState': array([ 8.65709341, 17.72955477,  0.        ]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 6.0907750023076925}
done in step count: 5
reward sum = 0.9171675703410066
running average episode reward sum: 0.34093799055539653
{'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'previousTarget': array([13., 22.]), 'currentState': array([12.49240917, 21.44007975,  0.        ]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 0.7557507121561698}
episode index:862
target Thresh 21.188694939818397
target distance 8.0
model initialize at round 862
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 26.]), 'previousTarget': array([ 3., 26.]), 'currentState': array([ 9.60903394, 25.44835085,  0.        ]), 'targetState': array([ 3, 26], dtype=int32), 'currentDistance': 6.632016770011353}
done in step count: 5
reward sum = 0.9040177490824755
running average episode reward sum: 0.34159045841000496
{'scaleFactor': 20, 'currentTarget': array([ 3., 26.]), 'previousTarget': array([ 3., 26.]), 'currentState': array([ 3.68196267, 26.18917269,  0.        ]), 'targetState': array([ 3, 26], dtype=int32), 'currentDistance': 0.7077142008581526}
episode index:863
target Thresh 21.199500841027483
target distance 10.0
model initialize at round 863
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 24.]), 'previousTarget': array([25., 24.]), 'currentState': array([16.99724925, 21.3143706 ,  0.        ]), 'targetState': array([25, 24], dtype=int32), 'currentDistance': 8.441363924021243}
done in step count: 5
reward sum = 0.8740738140915019
running average episode reward sum: 0.34220675859019184
{'scaleFactor': 20, 'currentTarget': array([25., 24.]), 'previousTarget': array([25., 24.]), 'currentState': array([24.03629899, 23.20627967,  0.        ]), 'targetState': array([25, 24], dtype=int32), 'currentDistance': 1.2484837227685655}
episode index:864
target Thresh 21.210295941736508
target distance 9.0
model initialize at round 864
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 6.]), 'previousTarget': array([9., 6.]), 'currentState': array([16.51648068,  2.30040541,  0.        ]), 'targetState': array([9, 6], dtype=int32), 'currentDistance': 8.377617918356675}
done in step count: 6
reward sum = 0.8787459092629433
running average episode reward sum: 0.34282703506495804
{'scaleFactor': 20, 'currentTarget': array([9., 6.]), 'previousTarget': array([9., 6.]), 'currentState': array([9.65623736, 5.72046795, 0.        ]), 'targetState': array([9, 6], dtype=int32), 'currentDistance': 0.713292115419354}
episode index:865
target Thresh 21.221080252740578
target distance 6.0
model initialize at round 865
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([13.81995285,  5.72049981,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 7.148780850760953}
done in step count: 5
reward sum = 0.8999185637156365
running average episode reward sum: 0.3434703278232152
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.56088108, 10.43311441,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.7974627619570219}
episode index:866
target Thresh 21.231853784824
target distance 8.0
model initialize at round 866
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([12.59585226, 18.40937179,  0.        ]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 9.324250486766438}
done in step count: 6
reward sum = 0.8756897227875708
running average episode reward sum: 0.34408419102386606
{'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([ 6.98167795, 24.17860818,  0.        ]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 1.2799906717814207}
episode index:867
target Thresh 21.24261654876031
target distance 20.0
model initialize at round 867
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 19.]), 'previousTarget': array([ 7., 19.]), 'currentState': array([25.98706019, 10.78577262,  0.        ]), 'targetState': array([ 7, 19], dtype=int32), 'currentDistance': 20.687725492514346}
done in step count: 15
reward sum = 0.7252857778745812
running average episode reward sum: 0.34452336335894757
{'scaleFactor': 20, 'currentTarget': array([ 7., 19.]), 'previousTarget': array([ 7., 19.]), 'currentState': array([ 7.79972625, 18.71047455,  0.        ]), 'targetState': array([ 7, 19], dtype=int32), 'currentDistance': 0.8505216382776816}
episode index:868
target Thresh 21.253368555312274
target distance 17.0
model initialize at round 868
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 12.]), 'previousTarget': array([ 9., 12.]), 'currentState': array([13.86685309, 27.37114167,  0.        ]), 'targetState': array([ 9, 12], dtype=int32), 'currentDistance': 16.12322099601585}
done in step count: 10
reward sum = 0.7786467465289164
running average episode reward sum: 0.34502292996788886
{'scaleFactor': 20, 'currentTarget': array([ 9., 12.]), 'previousTarget': array([ 9., 12.]), 'currentState': array([ 9.57209256, 12.92524248,  0.        ]), 'targetState': array([ 9, 12], dtype=int32), 'currentDistance': 1.0878251486467303}
episode index:869
target Thresh 21.264109815231897
target distance 17.0
model initialize at round 869
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([ 6.99879456, 24.7465229 ,  0.        ]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 17.364844591404047}
done in step count: 11
reward sum = 0.7301529930937456
running average episode reward sum: 0.34546560820136685
{'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([21.27805915, 15.82646519,  0.        ]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 0.7425044878731946}
episode index:870
target Thresh 21.274840339260443
target distance 10.0
model initialize at round 870
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([21.99544322, 15.32237792,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 9.980027539898524}
done in step count: 10
reward sum = 0.853420035588628
running average episode reward sum: 0.34604879353705825
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.31401306, 11.44545107,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.5450053728307593}
episode index:871
target Thresh 21.28556013812843
target distance 9.0
model initialize at round 871
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([ 4.73872685, 10.35941607,  0.        ]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 9.024933699577055}
done in step count: 6
reward sum = 0.8588018178284333
running average episode reward sum: 0.3466368130603282
{'scaleFactor': 20, 'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([11.28809112,  4.68777704,  0.        ]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 0.7773656953216359}
episode index:872
target Thresh 21.296269222555665
target distance 8.0
model initialize at round 872
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 20.]), 'previousTarget': array([ 9., 20.]), 'currentState': array([13.0681479 , 13.25898623,  0.        ]), 'targetState': array([ 9, 20], dtype=int32), 'currentDistance': 7.873442315922787}
done in step count: 5
reward sum = 0.8896057099351372
running average episode reward sum: 0.3472587705596121
{'scaleFactor': 20, 'currentTarget': array([ 9., 20.]), 'previousTarget': array([ 9., 20.]), 'currentState': array([ 9.64491248, 19.2687428 ,  0.        ]), 'targetState': array([ 9, 20], dtype=int32), 'currentDistance': 0.975012410921871}
episode index:873
target Thresh 21.30696760325123
target distance 8.0
model initialize at round 873
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 23.]), 'previousTarget': array([25., 23.]), 'currentState': array([18.94680476, 19.55447531,  0.        ]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 6.965113997458307}
done in step count: 4
reward sum = 0.8963858665838432
running average episode reward sum: 0.3478870624314933
{'scaleFactor': 20, 'currentTarget': array([25., 23.]), 'previousTarget': array([25., 23.]), 'currentState': array([24.09056157, 22.28848702,  0.        ]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 1.1546986555008174}
episode index:874
target Thresh 21.317655290913507
target distance 10.0
model initialize at round 874
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  3.]), 'previousTarget': array([11.,  3.]), 'currentState': array([13.98766517, 11.28009892,  0.        ]), 'targetState': array([11,  3], dtype=int32), 'currentDistance': 8.802623541055546}
done in step count: 7
reward sum = 0.8650967980736871
running average episode reward sum: 0.34847815927222725
{'scaleFactor': 20, 'currentTarget': array([11.,  3.]), 'previousTarget': array([11.,  3.]), 'currentState': array([11.13738701,  3.63269979,  0.        ]), 'targetState': array([11,  3], dtype=int32), 'currentDistance': 0.647444369095133}
episode index:875
target Thresh 21.328332296230187
target distance 12.0
model initialize at round 875
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'previousTarget': array([13., 22.]), 'currentState': array([21.77844   , 10.71678966,  0.        ]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 14.295868085643825}
done in step count: 10
reward sum = 0.8051958381070746
running average episode reward sum: 0.348999526485509
{'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'previousTarget': array([13., 22.]), 'currentState': array([13.50433442, 21.15443856,  0.        ]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 0.9845442417818986}
episode index:876
target Thresh 21.33899862987827
target distance 10.0
model initialize at round 876
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  8.]), 'previousTarget': array([10.,  8.]), 'currentState': array([ 5.92229915, 16.82062769,  0.        ]), 'targetState': array([10,  8], dtype=int32), 'currentDistance': 9.717567446831714}
done in step count: 6
reward sum = 0.854228435217684
running average episode reward sum: 0.34957561418075667
{'scaleFactor': 20, 'currentTarget': array([10.,  8.]), 'previousTarget': array([10.,  8.]), 'currentState': array([9.98530795, 8.51333767, 0.        ]), 'targetState': array([10,  8], dtype=int32), 'currentDistance': 0.5135478767082544}
episode index:877
target Thresh 21.349654302524094
target distance 14.0
model initialize at round 877
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 22.]), 'previousTarget': array([22., 22.]), 'currentState': array([ 9.99999154, 19.43021661,  0.        ]), 'targetState': array([22, 22], dtype=int32), 'currentDistance': 12.272081722430011}
done in step count: 7
reward sum = 0.81591787882113
running average episode reward sum: 0.35010675571223776
{'scaleFactor': 20, 'currentTarget': array([22., 22.]), 'previousTarget': array([22., 22.]), 'currentState': array([21.10129923, 21.21480434,  0.        ]), 'targetState': array([22, 22], dtype=int32), 'currentDistance': 1.1933965389040955}
episode index:878
target Thresh 21.360299324823337
target distance 12.0
model initialize at round 878
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 28.]), 'previousTarget': array([ 9., 28.]), 'currentState': array([19.70491242, 28.32335812,  0.        ]), 'targetState': array([ 9, 28], dtype=int32), 'currentDistance': 10.709795071811758}
done in step count: 9
reward sum = 0.8436856296772405
running average episode reward sum: 0.35066827889081
{'scaleFactor': 20, 'currentTarget': array([ 9., 28.]), 'previousTarget': array([ 9., 28.]), 'currentState': array([ 9.44300389, 28.33797266,  0.        ]), 'targetState': array([ 9, 28], dtype=int32), 'currentDistance': 0.5572054964318267}
episode index:879
target Thresh 21.370933707421017
target distance 7.0
model initialize at round 879
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 15.]), 'previousTarget': array([ 8., 15.]), 'currentState': array([11.28217942,  9.39610934,  0.        ]), 'targetState': array([ 8, 15], dtype=int32), 'currentDistance': 6.494327695312904}
done in step count: 4
reward sum = 0.9103293118650838
running average episode reward sum: 0.3513042573373717
{'scaleFactor': 20, 'currentTarget': array([ 8., 15.]), 'previousTarget': array([ 8., 15.]), 'currentState': array([ 8.81980169, 14.29549021,  0.        ]), 'targetState': array([ 8, 15], dtype=int32), 'currentDistance': 1.08092962713185}
episode index:880
target Thresh 21.381557460951516
target distance 7.0
model initialize at round 880
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 23.]), 'previousTarget': array([18., 23.]), 'currentState': array([23.50144768, 22.40053326,  0.        ]), 'targetState': array([18, 23], dtype=int32), 'currentDistance': 5.534011828969316}
done in step count: 4
reward sum = 0.9179393102179987
running average episode reward sum: 0.35194742992860967
{'scaleFactor': 20, 'currentTarget': array([18., 23.]), 'previousTarget': array([18., 23.]), 'currentState': array([18.65329319, 22.91208334,  0.        ]), 'targetState': array([18, 23], dtype=int32), 'currentDistance': 0.6591823231581873}
episode index:881
target Thresh 21.392170596038593
target distance 15.0
model initialize at round 881
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([11.92341566, 25.78373277,  0.        ]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 18.325806351340407}
done in step count: 13
reward sum = 0.7238138750462021
running average episode reward sum: 0.35236904721332346
{'scaleFactor': 20, 'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([23.24983555, 11.88472134,  0.        ]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 0.7589702699087479}
episode index:882
target Thresh 21.402773123295383
target distance 5.0
model initialize at round 882
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 24.]), 'previousTarget': array([16., 24.]), 'currentState': array([20.59610352, 28.25399041,  0.        ]), 'targetState': array([16, 24], dtype=int32), 'currentDistance': 6.2626353863645905}
done in step count: 6
reward sum = 0.9126528302860499
running average episode reward sum: 0.3530035701839607
{'scaleFactor': 20, 'currentTarget': array([16., 24.]), 'previousTarget': array([16., 24.]), 'currentState': array([16.3768526 , 24.75659254,  0.        ]), 'targetState': array([16, 24], dtype=int32), 'currentDistance': 0.8452515353238982}
episode index:883
target Thresh 21.413365053324412
target distance 6.0
model initialize at round 883
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 19.]), 'previousTarget': array([25., 19.]), 'currentState': array([20.86781335, 18.98385367,  0.        ]), 'targetState': array([25, 19], dtype=int32), 'currentDistance': 4.132218196639818}
done in step count: 3
reward sum = 0.9342004341089953
running average episode reward sum: 0.35366103269971305
{'scaleFactor': 20, 'currentTarget': array([25., 19.]), 'previousTarget': array([25., 19.]), 'currentState': array([24.05994624, 18.42468845,  0.        ]), 'targetState': array([25, 19], dtype=int32), 'currentDistance': 1.102127237535063}
episode index:884
target Thresh 21.42394639671761
target distance 20.0
model initialize at round 884
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  4.]), 'previousTarget': array([27.,  4.]), 'currentState': array([ 8.99967694, 17.34414792,  0.        ]), 'targetState': array([27,  4], dtype=int32), 'currentDistance': 22.4070951684892}
done in step count: 12
reward sum = 0.6437461374552249
running average episode reward sum: 0.35398881247909786
{'scaleFactor': 20, 'currentTarget': array([27.,  4.]), 'previousTarget': array([27.,  4.]), 'currentState': array([26.34598488,  3.84553152,  0.        ]), 'targetState': array([27,  4], dtype=int32), 'currentDistance': 0.6720091461678425}
episode index:885
target Thresh 21.43451716405633
target distance 8.0
model initialize at round 885
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 27.]), 'previousTarget': array([12., 27.]), 'currentState': array([18.5100553, 24.2948882,  0.       ]), 'targetState': array([12, 27], dtype=int32), 'currentDistance': 7.04971275406846}
done in step count: 5
reward sum = 0.8977926634731663
running average episode reward sum: 0.3546025865772853
{'scaleFactor': 20, 'currentTarget': array([12., 27.]), 'previousTarget': array([12., 27.]), 'currentState': array([12.72774178, 26.8366721 ,  0.        ]), 'targetState': array([12, 27], dtype=int32), 'currentDistance': 0.7458445542136015}
episode index:886
target Thresh 21.445077365911327
target distance 15.0
model initialize at round 886
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  2.]), 'previousTarget': array([21.,  2.]), 'currentState': array([ 7.98916626, 11.3133074 ,  0.        ]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 16.00060903300476}
done in step count: 10
reward sum = 0.7531930311557531
running average episode reward sum: 0.3550519557369003
{'scaleFactor': 20, 'currentTarget': array([21.,  2.]), 'previousTarget': array([21.,  2.]), 'currentState': array([20.26354942,  1.66495913,  0.        ]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 0.8090808634510278}
episode index:887
target Thresh 21.45562701284281
target distance 9.0
model initialize at round 887
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([24.51014304, 14.28269789,  0.        ]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 9.438738896991897}
done in step count: 7
reward sum = 0.8617215743947173
running average episode reward sum: 0.3556225296317852
{'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([17.61896399, 19.7461389 ,  0.        ]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 0.6690006536478509}
episode index:888
target Thresh 21.466166115400434
target distance 3.0
model initialize at round 888
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 29.]), 'previousTarget': array([19., 29.]), 'currentState': array([17.02264988, 27.43897116,  0.        ]), 'targetState': array([19, 29], dtype=int32), 'currentDistance': 2.5192706313374984}
done in step count: 1
reward sum = 0.9763979320714538
running average episode reward sum: 0.3563208146738996
{'scaleFactor': 20, 'currentTarget': array([19., 29.]), 'previousTarget': array([19., 29.]), 'currentState': array([18.059448  , 28.08852756,  0.        ]), 'targetState': array([19, 29], dtype=int32), 'currentDistance': 1.3097404575208642}
episode index:889
target Thresh 21.476694684123288
target distance 14.0
model initialize at round 889
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 26.]), 'previousTarget': array([23., 26.]), 'currentState': array([10.99972081, 17.7999931 ,  0.        ]), 'targetState': array([23, 26], dtype=int32), 'currentDistance': 14.53433224438322}
done in step count: 9
reward sum = 0.7838834732234896
running average episode reward sum: 0.356801222155416
{'scaleFactor': 20, 'currentTarget': array([23., 26.]), 'previousTarget': array([23., 26.]), 'currentState': array([22.22157305, 25.14450747,  0.        ]), 'targetState': array([23, 26], dtype=int32), 'currentDistance': 1.156639953386344}
episode index:890
target Thresh 21.487212729539955
target distance 9.0
model initialize at round 890
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 13.]), 'previousTarget': array([27., 13.]), 'currentState': array([19.95716912,  4.95230746,  0.        ]), 'targetState': array([27, 13], dtype=int32), 'currentDistance': 10.694242471633762}
done in step count: 8
reward sum = 0.8585871749584555
running average episode reward sum: 0.35736439381961693
{'scaleFactor': 20, 'currentTarget': array([27., 13.]), 'previousTarget': array([27., 13.]), 'currentState': array([26.23092484, 12.13910836,  0.        ]), 'targetState': array([27, 13], dtype=int32), 'currentDistance': 1.1543877210793472}
episode index:891
target Thresh 21.49772026216847
target distance 12.0
model initialize at round 891
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'previousTarget': array([25.,  3.]), 'currentState': array([14.9984597 ,  3.79298355,  0.        ]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 10.032927356595339}
done in step count: 6
reward sum = 0.8426915380409769
running average episode reward sum: 0.3579084825463225
{'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'previousTarget': array([25.,  3.]), 'currentState': array([24.20201379,  2.50991517,  0.        ]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 0.9364641626800003}
episode index:892
target Thresh 21.508217292516377
target distance 10.0
model initialize at round 892
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 23.]), 'previousTarget': array([11., 23.]), 'currentState': array([19.56467414, 20.23800546,  0.        ]), 'targetState': array([11, 23], dtype=int32), 'currentDistance': 8.999014221038495}
done in step count: 6
reward sum = 0.8759800340517602
running average episode reward sum: 0.35848862986043833
{'scaleFactor': 20, 'currentTarget': array([11., 23.]), 'previousTarget': array([11., 23.]), 'currentState': array([11.92448241, 22.84212571,  0.        ]), 'targetState': array([11, 23], dtype=int32), 'currentDistance': 0.9378656661447283}
episode index:893
target Thresh 21.5187038310807
target distance 15.0
model initialize at round 893
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([ 4.9237287 , 13.86101651,  0.        ]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 16.99854554563108}
done in step count: 10
reward sum = 0.7391870189690732
running average episode reward sum: 0.3589144669847209
{'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([17.2899541 ,  2.80150247,  0.        ]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 0.7372695956017811}
episode index:894
target Thresh 21.529179888347983
target distance 12.0
model initialize at round 894
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 20.]), 'previousTarget': array([24., 20.]), 'currentState': array([13.97959363, 11.78344929,  0.        ]), 'targetState': array([24, 20], dtype=int32), 'currentDistance': 12.958404580108239}
done in step count: 9
reward sum = 0.8085060176672533
running average episode reward sum: 0.3594168039128578
{'scaleFactor': 20, 'currentTarget': array([24., 20.]), 'previousTarget': array([24., 20.]), 'currentState': array([23.28237224, 19.24837768,  0.        ]), 'targetState': array([24, 20], dtype=int32), 'currentDistance': 1.039194745009232}
episode index:895
target Thresh 21.539645474794277
target distance 1.0
model initialize at round 895
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([8.84467207, 9.42992127, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.4571204043357496}
done in step count: 0
reward sum = 0.9982544173171561
running average episode reward sum: 0.36012979232067505
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([8.84467207, 9.42992127, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.4571204043357496}
episode index:896
target Thresh 21.55010060088518
target distance 10.0
model initialize at round 896
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 27.]), 'previousTarget': array([ 4., 27.]), 'currentState': array([ 8.15282309, 18.34473181,  0.        ]), 'targetState': array([ 4, 27], dtype=int32), 'currentDistance': 9.599979536536553}
done in step count: 6
reward sum = 0.8669509897305738
running average episode reward sum: 0.36069481037798823
{'scaleFactor': 20, 'currentTarget': array([ 4., 27.]), 'previousTarget': array([ 4., 27.]), 'currentState': array([ 4.86755615, 26.2091248 ,  0.        ]), 'targetState': array([ 4, 27], dtype=int32), 'currentDistance': 1.1739409090654085}
episode index:897
target Thresh 21.56054527707581
target distance 15.0
model initialize at round 897
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 21.]), 'previousTarget': array([ 4., 21.]), 'currentState': array([17.72782469, 19.38520646,  0.        ]), 'targetState': array([ 4, 21], dtype=int32), 'currentDistance': 13.82247187868533}
done in step count: 10
reward sum = 0.812241059796308
running average episode reward sum: 0.3611976458450465
{'scaleFactor': 20, 'currentTarget': array([ 4., 21.]), 'previousTarget': array([ 4., 21.]), 'currentState': array([ 4.80311453, 21.01885543,  0.        ]), 'targetState': array([ 4, 21], dtype=int32), 'currentDistance': 0.8033358457473674}
episode index:898
target Thresh 21.570979513810848
target distance 7.0
model initialize at round 898
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 19.]), 'previousTarget': array([ 7., 19.]), 'currentState': array([13.43774062, 25.06672698,  0.        ]), 'targetState': array([ 7, 19], dtype=int32), 'currentDistance': 8.845884954180635}
done in step count: 8
reward sum = 0.8777220689154939
running average episode reward sum: 0.36177220026447965
{'scaleFactor': 20, 'currentTarget': array([ 7., 19.]), 'previousTarget': array([ 7., 19.]), 'currentState': array([ 7.82534644, 19.56714928,  0.        ]), 'targetState': array([ 7, 19], dtype=int32), 'currentDistance': 1.00142650936298}
episode index:899
target Thresh 21.581403321524526
target distance 14.0
model initialize at round 899
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([19.62905839, 23.3980794 ,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 12.918298552683138}
done in step count: 8
reward sum = 0.8205621658659306
running average episode reward sum: 0.3622819668929257
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.26681802, 11.92889237,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.9664537736587121}
episode index:900
target Thresh 21.591816710640664
target distance 2.0
model initialize at round 900
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([14.85139196, 12.925336  ,  0.        ]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 1.3710473984859126}
done in step count: 1
reward sum = 0.9837388613513717
running average episode reward sum: 0.3629717081742336
{'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([14.56722777, 13.47024345,  0.        ]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 0.7761374468993839}
episode index:901
target Thresh 21.602219691572643
target distance 11.0
model initialize at round 901
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 18.]), 'previousTarget': array([18., 18.]), 'currentState': array([9.69743311, 7.98456544, 0.        ]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 13.00928692836709}
done in step count: 10
reward sum = 0.8151547635631087
running average episode reward sum: 0.36347301976557383
{'scaleFactor': 20, 'currentTarget': array([18., 18.]), 'previousTarget': array([18., 18.]), 'currentState': array([17.33495355, 17.29565471,  0.        ]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 0.9687048400000987}
episode index:902
target Thresh 21.612612274723446
target distance 19.0
model initialize at round 902
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([15.2159586 , 21.82446051,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 21.059656206988514}
done in step count: 14
reward sum = 0.7133614390185529
running average episode reward sum: 0.3638604930980799
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.16148469, 4.85735029, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.8724258272677019}
episode index:903
target Thresh 21.622994470485658
target distance 2.0
model initialize at round 903
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 20.]), 'previousTarget': array([25., 20.]), 'currentState': array([26.03818327, 21.20176864,  0.        ]), 'targetState': array([25, 20], dtype=int32), 'currentDistance': 1.5881033852322761}
done in step count: 1
reward sum = 0.9788999357010904
running average episode reward sum: 0.36454084646379115
{'scaleFactor': 20, 'currentTarget': array([25., 20.]), 'previousTarget': array([25., 20.]), 'currentState': array([25.38738692, 20.71099743,  0.        ]), 'targetState': array([25, 20], dtype=int32), 'currentDistance': 0.8096826373769567}
episode index:904
target Thresh 21.633366289241472
target distance 7.0
model initialize at round 904
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 12.]), 'previousTarget': array([16., 12.]), 'currentState': array([21.5497272 ,  8.52864116,  0.        ]), 'targetState': array([16, 12], dtype=int32), 'currentDistance': 6.545976182115219}
done in step count: 5
reward sum = 0.9016402213707646
running average episode reward sum: 0.3651343264360641
{'scaleFactor': 20, 'currentTarget': array([16., 12.]), 'previousTarget': array([16., 12.]), 'currentState': array([16.65522379, 11.86663139,  0.        ]), 'targetState': array([16, 12], dtype=int32), 'currentDistance': 0.6686594030740446}
episode index:905
target Thresh 21.643727741362717
target distance 7.0
model initialize at round 905
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 20.]), 'previousTarget': array([22., 20.]), 'currentState': array([22.85524058, 14.78511477,  0.        ]), 'targetState': array([22, 20], dtype=int32), 'currentDistance': 5.284549599351387}
done in step count: 4
reward sum = 0.9141300706417876
running average episode reward sum: 0.36574028200362
{'scaleFactor': 20, 'currentTarget': array([22., 20.]), 'previousTarget': array([22., 20.]), 'currentState': array([22.52896679, 19.48352849,  0.        ]), 'targetState': array([22, 20], dtype=int32), 'currentDistance': 0.7392893091409206}
episode index:906
target Thresh 21.65407883721084
target distance 10.0
model initialize at round 906
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([14.56627011,  6.19986135,  0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 9.012311585879168}
done in step count: 6
reward sum = 0.876560083814677
running average episode reward sum: 0.3663034791390236
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.90268552, 8.68084127, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.9574463171761826}
episode index:907
target Thresh 21.664419587136937
target distance 1.0
model initialize at round 907
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 27.]), 'previousTarget': array([11., 27.]), 'currentState': array([10.85164984, 27.37818319,  0.        ]), 'targetState': array([11, 27], dtype=int32), 'currentDistance': 0.4062392071923043}
done in step count: 0
reward sum = 0.9979566803368385
running average episode reward sum: 0.36699913244430754
{'scaleFactor': 20, 'currentTarget': array([11., 27.]), 'previousTarget': array([11., 27.]), 'currentState': array([10.85164984, 27.37818319,  0.        ]), 'targetState': array([11, 27], dtype=int32), 'currentDistance': 0.4062392071923043}
episode index:908
target Thresh 21.674750001481762
target distance 5.0
model initialize at round 908
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 28.]), 'previousTarget': array([18., 28.]), 'currentState': array([14.79153955, 28.50003973,  0.        ]), 'targetState': array([18, 28], dtype=int32), 'currentDistance': 3.247192354695812}
done in step count: 2
reward sum = 0.9479124052776701
running average episode reward sum: 0.367638200951275
{'scaleFactor': 20, 'currentTarget': array([18., 28.]), 'previousTarget': array([18., 28.]), 'currentState': array([17.0442549 , 27.61746001,  0.        ]), 'targetState': array([18, 28], dtype=int32), 'currentDistance': 1.029458857907036}
episode index:909
target Thresh 21.685070090575724
target distance 11.0
model initialize at round 909
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([25.8586328 , 10.99553382,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 13.34587087910712}
done in step count: 13
reward sum = 0.8074755253055425
running average episode reward sum: 0.3681215386703456
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.97342226,  2.61478278,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.1513074140228285}
episode index:910
target Thresh 21.695379864738918
target distance 8.0
model initialize at round 910
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 20.]), 'previousTarget': array([18., 20.]), 'currentState': array([16.5662632 , 13.87121725,  0.        ]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 6.2942496932964636}
done in step count: 4
reward sum = 0.9006630517083521
running average episode reward sum: 0.3687061067417375
{'scaleFactor': 20, 'currentTarget': array([18., 20.]), 'previousTarget': array([18., 20.]), 'currentState': array([18.35342951, 19.12864858,  0.        ]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 0.940300865706412}
episode index:911
target Thresh 21.705679334281122
target distance 9.0
model initialize at round 911
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([6.60955083, 9.27494216, 0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 9.079854275665214}
done in step count: 6
reward sum = 0.860895035481878
running average episode reward sum: 0.3692457875846543
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.15410802,  3.69676715,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.8986008047207842}
episode index:912
target Thresh 21.7159685095018
target distance 18.0
model initialize at round 912
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 11.]), 'previousTarget': array([20., 11.]), 'currentState': array([11.15950423, 27.35915935,  0.        ]), 'targetState': array([20, 11], dtype=int32), 'currentDistance': 18.59506547518191}
done in step count: 11
reward sum = 0.7340342381242216
running average episode reward sum: 0.36964533681854206
{'scaleFactor': 20, 'currentTarget': array([20., 11.]), 'previousTarget': array([20., 11.]), 'currentState': array([19.03933395, 11.17433572,  0.        ]), 'targetState': array([20, 11], dtype=int32), 'currentDistance': 0.9763565932948564}
episode index:913
target Thresh 21.726247400690127
target distance 9.0
model initialize at round 913
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 18.]), 'previousTarget': array([27., 18.]), 'currentState': array([19.59749603, 26.28019345,  0.        ]), 'targetState': array([27, 18], dtype=int32), 'currentDistance': 11.106694761407603}
done in step count: 7
reward sum = 0.8282907025052901
running average episode reward sum: 0.3701471369998186
{'scaleFactor': 20, 'currentTarget': array([27., 18.]), 'previousTarget': array([27., 18.]), 'currentState': array([26.20169073, 17.80959404,  0.        ]), 'targetState': array([27, 18], dtype=int32), 'currentDistance': 0.8207022094146942}
episode index:914
target Thresh 21.736516018125002
target distance 21.0
model initialize at round 914
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([24.47685277, 13.56902131,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 22.197513956292592}
done in step count: 19
reward sum = 0.7134200883239699
running average episode reward sum: 0.3705222986952548
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.90424734, 5.56199028, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.0646578457863165}
episode index:915
target Thresh 21.746774372075038
target distance 16.0
model initialize at round 915
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 14.]), 'previousTarget': array([21., 14.]), 'currentState': array([ 6.96409678, 26.98701024,  0.        ]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 19.12247405699861}
done in step count: 12
reward sum = 0.7120723197682254
running average episode reward sum: 0.3708951698972996
{'scaleFactor': 20, 'currentTarget': array([21., 14.]), 'previousTarget': array([21., 14.]), 'currentState': array([20.23866075, 13.84359872,  0.        ]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 0.7772379356974066}
episode index:916
target Thresh 21.757022472798592
target distance 13.0
model initialize at round 916
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 29.]), 'previousTarget': array([ 3., 29.]), 'currentState': array([14.6617918 , 25.42211303,  0.        ]), 'targetState': array([ 3, 29], dtype=int32), 'currentDistance': 12.198305750484439}
done in step count: 8
reward sum = 0.8351230771125407
running average episode reward sum: 0.3714014162519509
{'scaleFactor': 20, 'currentTarget': array([ 3., 29.]), 'previousTarget': array([ 3., 29.]), 'currentState': array([ 3.90267688, 28.6346193 ,  0.        ]), 'targetState': array([ 3, 29], dtype=int32), 'currentDistance': 0.9738216517852967}
episode index:917
target Thresh 21.767260330543767
target distance 12.0
model initialize at round 917
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([ 3.62800229, 27.47776949,  0.        ]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 11.353326278328197}
done in step count: 6
reward sum = 0.8364047254401957
running average episode reward sum: 0.3719079558044436
{'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([ 7.7929652 , 17.82550645,  0.        ]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 0.8510724433752332}
episode index:918
target Thresh 21.777487955548416
target distance 8.0
model initialize at round 918
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([15.96922696, 15.95420317,  0.        ]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 6.030946928451771}
done in step count: 4
reward sum = 0.9018291151531908
running average episode reward sum: 0.3724845838342028
{'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([21.18070298, 15.53882521,  0.        ]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 0.9401754002345242}
episode index:919
target Thresh 21.787705358040174
target distance 18.0
model initialize at round 919
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([12.97421539, 20.79642451,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 19.53470882476348}
done in step count: 13
reward sum = 0.7246423997622355
running average episode reward sum: 0.3728673640689072
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.25803834, 4.51518786, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.5761964218514163}
episode index:920
target Thresh 21.79791254823644
target distance 19.0
model initialize at round 920
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  4.]), 'previousTarget': array([21.,  4.]), 'currentState': array([ 3.99975884, 12.3115564 ,  0.        ]), 'targetState': array([21,  4], dtype=int32), 'currentDistance': 18.92327057575106}
done in step count: 11
reward sum = 0.7136082478802191
running average episode reward sum: 0.3732373324552387
{'scaleFactor': 20, 'currentTarget': array([21.,  4.]), 'previousTarget': array([21.,  4.]), 'currentState': array([20.27197111,  3.76310226,  0.        ]), 'targetState': array([21,  4], dtype=int32), 'currentDistance': 0.7656021208325909}
episode index:921
target Thresh 21.808109536344396
target distance 16.0
model initialize at round 921
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([23.75581217, 17.82169485,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 16.728277276546432}
done in step count: 12
reward sum = 0.7623379772581517
running average episode reward sum: 0.3736593505081703
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.09590441,  3.58098739,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.588849732661317}
episode index:922
target Thresh 21.818296332561047
target distance 12.0
model initialize at round 922
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  2.]), 'previousTarget': array([25.,  2.]), 'currentState': array([14.92558801,  7.44599146,  0.        ]), 'targetState': array([25,  2], dtype=int32), 'currentDistance': 11.4521875575599}
done in step count: 7
reward sum = 0.822570381943188
running average episode reward sum: 0.37414571132229274
{'scaleFactor': 20, 'currentTarget': array([25.,  2.]), 'previousTarget': array([25.,  2.]), 'currentState': array([24.11515963,  1.63809252,  0.        ]), 'targetState': array([25,  2], dtype=int32), 'currentDistance': 0.9559913726825588}
episode index:923
target Thresh 21.828472947073177
target distance 9.0
model initialize at round 923
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 15.]), 'previousTarget': array([12., 15.]), 'currentState': array([19.57188082, 15.30629945,  0.        ]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 7.578073532565216}
done in step count: 6
reward sum = 0.8899251081293015
running average episode reward sum: 0.37470391413268994
{'scaleFactor': 20, 'currentTarget': array([12., 15.]), 'previousTarget': array([12., 15.]), 'currentState': array([12.67235327, 15.26910717,  0.        ]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 0.7242082467809604}
episode index:924
target Thresh 21.83863939005741
target distance 16.0
model initialize at round 924
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([16.88788724,  4.43132225,  0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 14.9703018121941}
done in step count: 12
reward sum = 0.7901346737758934
running average episode reward sum: 0.3751530284674393
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.42172468, 6.28889719, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.5111881179625826}
episode index:925
target Thresh 21.848795671680183
target distance 2.0
model initialize at round 925
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  9.]), 'previousTarget': array([24.,  9.]), 'currentState': array([22.67938536,  8.98190832,  0.        ]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 1.32073855304751}
done in step count: 1
reward sum = 0.9805218788890391
running average episode reward sum: 0.3758067745262099
{'scaleFactor': 20, 'currentTarget': array([24.,  9.]), 'previousTarget': array([24.,  9.]), 'currentState': array([23.19796395,  8.61770946,  0.        ]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 0.8884862853510964}
episode index:926
target Thresh 21.85894180209778
target distance 9.0
model initialize at round 926
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([22.49053311, 11.00153416,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 10.397478774788521}
done in step count: 10
reward sum = 0.8518901003327206
running average episode reward sum: 0.37632034877195586
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.65899059,  5.61691022,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.9026886609334871}
episode index:927
target Thresh 21.86907779145634
target distance 8.0
model initialize at round 927
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([11.53055251, 15.46048975,  0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 6.938416519390798}
done in step count: 5
reward sum = 0.8946465882956395
running average episode reward sum: 0.3768788899783391
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([9.03306136, 9.5189293 , 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.5199814173703569}
episode index:928
target Thresh 21.879203649891842
target distance 14.0
model initialize at round 928
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 24.]), 'previousTarget': array([17., 24.]), 'currentState': array([ 4.99930692, 15.99868691,  0.        ]), 'targetState': array([17, 24], dtype=int32), 'currentDistance': 14.42351017045391}
done in step count: 10
reward sum = 0.779156964600681
running average episode reward sum: 0.3773119126636162
{'scaleFactor': 20, 'currentTarget': array([17., 24.]), 'previousTarget': array([17., 24.]), 'currentState': array([16.34575126, 23.28669593,  0.        ]), 'targetState': array([17, 24], dtype=int32), 'currentDistance': 0.9679070819899244}
episode index:929
target Thresh 21.889319387530158
target distance 21.0
model initialize at round 929
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([23.23820716, 12.39751351,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 21.911258800882226}
done in step count: 19
reward sum = 0.7120773871313498
running average episode reward sum: 0.3776718755393879
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.78723174, 4.53421086, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.9513753525486429}
episode index:930
target Thresh 21.89942501448701
target distance 10.0
model initialize at round 930
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 27.]), 'previousTarget': array([15., 27.]), 'currentState': array([ 6.96679759, 28.66988403,  0.        ]), 'targetState': array([15, 27], dtype=int32), 'currentDistance': 8.20492861738482}
done in step count: 5
reward sum = 0.870401071516949
running average episode reward sum: 0.37820112279607704
{'scaleFactor': 20, 'currentTarget': array([15., 27.]), 'previousTarget': array([15., 27.]), 'currentState': array([14.11646205, 26.62677352,  0.        ]), 'targetState': array([15, 27], dtype=int32), 'currentDistance': 0.9591336247823914}
episode index:931
target Thresh 21.909520540868037
target distance 14.0
model initialize at round 931
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 29.]), 'previousTarget': array([21., 29.]), 'currentState': array([ 8.99941957, 20.77525312,  0.        ]), 'targetState': array([21, 29], dtype=int32), 'currentDistance': 14.548552915862965}
done in step count: 10
reward sum = 0.7824740483262544
running average episode reward sum: 0.3786348920294785
{'scaleFactor': 20, 'currentTarget': array([21., 29.]), 'previousTarget': array([21., 29.]), 'currentState': array([20.36207354, 28.16225243,  0.        ]), 'targetState': array([21, 29], dtype=int32), 'currentDistance': 1.052982034502783}
episode index:932
target Thresh 21.919605976768768
target distance 6.0
model initialize at round 932
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 17.]), 'previousTarget': array([12., 17.]), 'currentState': array([10.73883677, 12.7299192 ,  0.        ]), 'targetState': array([12, 17], dtype=int32), 'currentDistance': 4.452428863476577}
done in step count: 4
reward sum = 0.919405772316092
running average episode reward sum: 0.37921449640277605
{'scaleFactor': 20, 'currentTarget': array([12., 17.]), 'previousTarget': array([12., 17.]), 'currentState': array([12.15458407, 16.39610907,  0.        ]), 'targetState': array([12, 17], dtype=int32), 'currentDistance': 0.6233622417227404}
episode index:933
target Thresh 21.929681332274633
target distance 5.0
model initialize at round 933
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([11.5147503 , 13.25664485,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 4.096608561815219}
done in step count: 2
reward sum = 0.9362406681838223
running average episode reward sum: 0.3798108841669956
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.04109263, 10.31834996,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.0103712345481015}
episode index:934
target Thresh 21.93974661746099
target distance 4.0
model initialize at round 934
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'previousTarget': array([13., 22.]), 'currentState': array([15.95330811, 19.90192121,  0.        ]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 3.622701114314911}
done in step count: 3
reward sum = 0.9454257018279134
running average episode reward sum: 0.38041581980085754
{'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'previousTarget': array([13., 22.]), 'currentState': array([13.66245037, 21.77632758,  0.        ]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 0.6991922827732329}
episode index:935
target Thresh 21.94980184239313
target distance 4.0
model initialize at round 935
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 14.]), 'previousTarget': array([23., 14.]), 'currentState': array([25.59037519, 14.06727527,  0.        ]), 'targetState': array([23, 14], dtype=int32), 'currentDistance': 2.591248649048186}
done in step count: 2
reward sum = 0.9603998775997414
running average episode reward sum: 0.3810354608882495
{'scaleFactor': 20, 'currentTarget': array([23., 14.]), 'previousTarget': array([23., 14.]), 'currentState': array([23.65315211, 14.25119291,  0.        ]), 'targetState': array([23, 14], dtype=int32), 'currentDistance': 0.6997896483677642}
episode index:936
target Thresh 21.959847017126272
target distance 8.0
model initialize at round 936
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([ 3.60649395, 13.42083794,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 7.251141414717146}
done in step count: 5
reward sum = 0.8869585622548476
running average episode reward sum: 0.3815754001639876
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([9.18885767, 9.6938163 , 0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.8670065326321816}
episode index:937
target Thresh 21.969882151705598
target distance 11.0
model initialize at round 937
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  5.]), 'previousTarget': array([23.,  5.]), 'currentState': array([13.77891278,  9.39169484,  0.        ]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 10.213492696938296}
done in step count: 7
reward sum = 0.8393470476345323
running average episode reward sum: 0.3820634296389029
{'scaleFactor': 20, 'currentTarget': array([23.,  5.]), 'previousTarget': array([23.,  5.]), 'currentState': array([22.26061374,  4.55726099,  0.        ]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 0.8618061690887396}
episode index:938
target Thresh 21.979907256166232
target distance 13.0
model initialize at round 938
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 17.]), 'previousTarget': array([ 4., 17.]), 'currentState': array([15.74051213, 18.19312054,  0.        ]), 'targetState': array([ 4, 17], dtype=int32), 'currentDistance': 11.800981389605406}
done in step count: 10
reward sum = 0.830948586094348
running average episode reward sum: 0.3825414755989193
{'scaleFactor': 20, 'currentTarget': array([ 4., 17.]), 'previousTarget': array([ 4., 17.]), 'currentState': array([ 4.41582298, 17.46543559,  0.        ]), 'targetState': array([ 4, 17], dtype=int32), 'currentDistance': 0.6241306317394306}
episode index:939
target Thresh 21.98992234053329
target distance 13.0
model initialize at round 939
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 29.]), 'previousTarget': array([21., 29.]), 'currentState': array([ 9.95799923, 16.69532758,  0.        ]), 'targetState': array([21, 29], dtype=int32), 'currentDistance': 16.53271739253297}
done in step count: 13
reward sum = 0.7562615698600674
running average episode reward sum: 0.38293905016728225
{'scaleFactor': 20, 'currentTarget': array([21., 29.]), 'previousTarget': array([21., 29.]), 'currentState': array([20.42071503, 28.32737601,  0.        ]), 'targetState': array([21, 29], dtype=int32), 'currentDistance': 0.8876903212226003}
episode index:940
target Thresh 21.999927414821858
target distance 9.0
model initialize at round 940
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([16.68114007, 12.29061943,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 7.7888132100958085}
done in step count: 6
reward sum = 0.8927246596553458
running average episode reward sum: 0.38348079895526105
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.82911688, 11.41752741,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.9283124174225322}
episode index:941
target Thresh 22.009922489037006
target distance 20.0
model initialize at round 941
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 22.]), 'previousTarget': array([ 9., 22.]), 'currentState': array([17.77028465,  3.26874113,  0.        ]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 20.682793614825325}
done in step count: 13
reward sum = 0.7093561602329141
running average episode reward sum: 0.38382673882922885
{'scaleFactor': 20, 'currentTarget': array([ 9., 22.]), 'previousTarget': array([ 9., 22.]), 'currentState': array([ 9.78954637, 21.32036829,  0.        ]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 1.041769039896922}
episode index:942
target Thresh 22.01990757317381
target distance 13.0
model initialize at round 942
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 14.]), 'previousTarget': array([ 2., 14.]), 'currentState': array([13.70196247, 10.27146533,  0.        ]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 12.281608056295541}
done in step count: 9
reward sum = 0.8287839563462601
running average episode reward sum: 0.38429859165798497
{'scaleFactor': 20, 'currentTarget': array([ 2., 14.]), 'previousTarget': array([ 2., 14.]), 'currentState': array([ 2.84483606, 13.8452706 ,  0.        ]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 0.8588883223300738}
episode index:943
target Thresh 22.02988267721736
target distance 9.0
model initialize at round 943
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 19.]), 'previousTarget': array([19., 19.]), 'currentState': array([12.71646237, 10.92322224,  0.        ]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 10.233141454313033}
done in step count: 7
reward sum = 0.8682477443295766
running average episode reward sum: 0.3848112496586964
{'scaleFactor': 20, 'currentTarget': array([19., 19.]), 'previousTarget': array([19., 19.]), 'currentState': array([18.34103996, 18.01365989,  0.        ]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 1.1862104129864375}
episode index:944
target Thresh 22.03984781114275
target distance 3.0
model initialize at round 944
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 23.]), 'previousTarget': array([ 6., 23.]), 'currentState': array([ 4.45748317, 24.01245254,  0.        ]), 'targetState': array([ 6, 23], dtype=int32), 'currentDistance': 1.845106586287996}
done in step count: 1
reward sum = 0.9686967800832045
running average episode reward sum: 0.38542911794485984
{'scaleFactor': 20, 'currentTarget': array([ 6., 23.]), 'previousTarget': array([ 6., 23.]), 'currentState': array([ 5.11406904, 23.15119725,  0.        ]), 'targetState': array([ 6, 23], dtype=int32), 'currentDistance': 0.8987403782905192}
episode index:945
target Thresh 22.04980298491512
target distance 18.0
model initialize at round 945
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([25.27040115,  3.97655964,  0.        ]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 16.116518022985648}
done in step count: 9
reward sum = 0.7632078455579074
running average episode reward sum: 0.38582846120872144
{'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([27.70750371, 19.05581957,  0.        ]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 1.1798466751164003}
episode index:946
target Thresh 22.05974820848965
target distance 17.0
model initialize at round 946
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  6.]), 'previousTarget': array([26.,  6.]), 'currentState': array([10.99999487,  7.03342842,  0.        ]), 'targetState': array([26,  6], dtype=int32), 'currentDistance': 15.03556211405796}
done in step count: 8
reward sum = 0.7736345076990654
running average episode reward sum: 0.38623797128949267
{'scaleFactor': 20, 'currentTarget': array([26.,  6.]), 'previousTarget': array([26.,  6.]), 'currentState': array([25.07050437,  5.90646881,  0.        ]), 'targetState': array([26,  6], dtype=int32), 'currentDistance': 0.9341896029957636}
episode index:947
target Thresh 22.06968349181156
target distance 16.0
model initialize at round 947
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 23.]), 'previousTarget': array([11., 23.]), 'currentState': array([3.06696224, 8.26971626, 0.        ]), 'targetState': array([11, 23], dtype=int32), 'currentDistance': 16.73064096338675}
done in step count: 15
reward sum = 0.7614304660986931
running average episode reward sum: 0.38663374396334205
{'scaleFactor': 20, 'currentTarget': array([11., 23.]), 'previousTarget': array([11., 23.]), 'currentState': array([10.42930266, 22.23691422,  0.        ]), 'targetState': array([11, 23], dtype=int32), 'currentDistance': 0.9528879065238982}
episode index:948
target Thresh 22.079608844816136
target distance 20.0
model initialize at round 948
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([23.5023303 ,  4.24672008,  0.        ]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 23.70660439931283}
done in step count: 16
reward sum = 0.6682543699905399
running average episode reward sum: 0.3869304991014108
{'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([ 9.55408531, 22.38686407,  0.        ]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 0.8264055958814911}
episode index:949
target Thresh 22.089524277428723
target distance 3.0
model initialize at round 949
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  5.]), 'previousTarget': array([24.,  5.]), 'currentState': array([23.97878104,  6.50596547,  0.        ]), 'targetState': array([24,  5], dtype=int32), 'currentDistance': 1.5061149508318121}
done in step count: 1
reward sum = 0.9748966614580974
running average episode reward sum: 0.38754941085125993
{'scaleFactor': 20, 'currentTarget': array([24.,  5.]), 'previousTarget': array([24.,  5.]), 'currentState': array([23.97591338,  5.61375988,  0.        ]), 'targetState': array([24,  5], dtype=int32), 'currentDistance': 0.6142323257526987}
episode index:950
target Thresh 22.099429799564767
target distance 17.0
model initialize at round 950
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 20.]), 'previousTarget': array([10., 20.]), 'currentState': array([25.82713306, 13.50300723,  0.        ]), 'targetState': array([10, 20], dtype=int32), 'currentDistance': 17.10874208981372}
done in step count: 13
reward sum = 0.7631783277076922
running average episode reward sum: 0.387944393939437
{'scaleFactor': 20, 'currentTarget': array([10., 20.]), 'previousTarget': array([10., 20.]), 'currentState': array([10.51785558, 19.87294161,  0.        ]), 'targetState': array([10, 20], dtype=int32), 'currentDistance': 0.533215003709156}
episode index:951
target Thresh 22.109325421129785
target distance 10.0
model initialize at round 951
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 18.]), 'previousTarget': array([ 4., 18.]), 'currentState': array([7.3124097, 9.5191586, 0.       ]), 'targetState': array([ 4, 18], dtype=int32), 'currentDistance': 9.104764073597572}
done in step count: 6
reward sum = 0.867946695991941
running average episode reward sum: 0.3884485980382317
{'scaleFactor': 20, 'currentTarget': array([ 4., 18.]), 'previousTarget': array([ 4., 18.]), 'currentState': array([ 4.74364899, 17.40794867,  0.        ]), 'targetState': array([ 4, 18], dtype=int32), 'currentDistance': 0.9505464722315146}
episode index:952
target Thresh 22.1192111520194
target distance 13.0
model initialize at round 952
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 12.]), 'previousTarget': array([19., 12.]), 'currentState': array([15.55990577, 23.44110966,  0.        ]), 'targetState': array([19, 12], dtype=int32), 'currentDistance': 11.947101678606636}
done in step count: 7
reward sum = 0.8249624348277301
running average episode reward sum: 0.3889066398396897
{'scaleFactor': 20, 'currentTarget': array([19., 12.]), 'previousTarget': array([19., 12.]), 'currentState': array([18.85241571, 12.54866201,  0.        ]), 'targetState': array([19, 12], dtype=int32), 'currentDistance': 0.5681646946249926}
episode index:953
target Thresh 22.12908700211934
target distance 10.0
model initialize at round 953
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 15.]), 'previousTarget': array([12., 15.]), 'currentState': array([19.82641292, 23.94349217,  0.        ]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 11.884392765457662}
done in step count: 11
reward sum = 0.8270515983304253
running average episode reward sum: 0.38936591128464854
{'scaleFactor': 20, 'currentTarget': array([12., 15.]), 'previousTarget': array([12., 15.]), 'currentState': array([12.45066139, 15.72393832,  0.        ]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 0.8527498899935404}
episode index:954
target Thresh 22.138952981305465
target distance 8.0
model initialize at round 954
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([12.7921803 , 18.57184941,  0.        ]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 9.351728892915393}
done in step count: 7
reward sum = 0.8686244850452579
running average episode reward sum: 0.3898677527231413
{'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([ 6.57580101, 24.57958636,  0.        ]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 0.712947712025679}
episode index:955
target Thresh 22.148809099443746
target distance 7.0
model initialize at round 955
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 20.]), 'previousTarget': array([21., 20.]), 'currentState': array([20.31618026, 14.78056777,  0.        ]), 'targetState': array([21, 20], dtype=int32), 'currentDistance': 5.264036691693038}
done in step count: 4
reward sum = 0.9123611819074611
running average episode reward sum: 0.39041429396705796
{'scaleFactor': 20, 'currentTarget': array([21., 20.]), 'previousTarget': array([21., 20.]), 'currentState': array([21.44218076, 19.53152114,  0.        ]), 'targetState': array([21, 20], dtype=int32), 'currentDistance': 0.6442020352614887}
episode index:956
target Thresh 22.158655366390306
target distance 20.0
model initialize at round 956
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  2.]), 'previousTarget': array([23.,  2.]), 'currentState': array([23.4047156 , 20.29225409,  0.        ]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 18.296730703035227}
done in step count: 11
reward sum = 0.7333962747958007
running average episode reward sum: 0.3907726868414871
{'scaleFactor': 20, 'currentTarget': array([23.,  2.]), 'previousTarget': array([23.,  2.]), 'currentState': array([22.83238576,  2.62203681,  0.        ]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 0.64422382259252}
episode index:957
target Thresh 22.168491791991414
target distance 13.0
model initialize at round 957
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  2.]), 'previousTarget': array([11.,  2.]), 'currentState': array([22.80984545,  2.29180506,  0.        ]), 'targetState': array([11,  2], dtype=int32), 'currentDistance': 11.81344994855769}
done in step count: 9
reward sum = 0.8397576152214291
running average episode reward sum: 0.39124135586902353
{'scaleFactor': 20, 'currentTarget': array([11.,  2.]), 'previousTarget': array([11.,  2.]), 'currentState': array([11.78562993,  2.49783019,  0.        ]), 'targetState': array([11,  2], dtype=int32), 'currentDistance': 0.9300802554995998}
episode index:958
target Thresh 22.178318386083497
target distance 3.0
model initialize at round 958
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 23.]), 'previousTarget': array([26., 23.]), 'currentState': array([24.12362504, 20.89382905,  0.        ]), 'targetState': array([26, 23], dtype=int32), 'currentDistance': 2.8207692338629524}
done in step count: 2
reward sum = 0.9586523806619167
running average episode reward sum: 0.3918330253422174
{'scaleFactor': 20, 'currentTarget': array([26., 23.]), 'previousTarget': array([26., 23.]), 'currentState': array([25.48347667, 22.44107115,  0.        ]), 'targetState': array([26, 23], dtype=int32), 'currentDistance': 0.761050463663898}
episode index:959
target Thresh 22.188135158493143
target distance 14.0
model initialize at round 959
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 11.]), 'previousTarget': array([25., 11.]), 'currentState': array([12.95136595, 21.80006121,  0.        ]), 'targetState': array([25, 11], dtype=int32), 'currentDistance': 16.180571828177413}
done in step count: 10
reward sum = 0.7593398325954714
running average episode reward sum: 0.3922158449331062
{'scaleFactor': 20, 'currentTarget': array([25., 11.]), 'previousTarget': array([25., 11.]), 'currentState': array([24.05138975, 11.57023999,  0.        ]), 'targetState': array([25, 11], dtype=int32), 'currentDistance': 1.1068130133128768}
episode index:960
target Thresh 22.19794211903713
target distance 17.0
model initialize at round 960
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 19.]), 'previousTarget': array([20., 19.]), 'currentState': array([4.99993145, 4.36692855, 0.        ]), 'targetState': array([20, 19], dtype=int32), 'currentDistance': 20.95540112943151}
done in step count: 14
reward sum = 0.6929688577427464
running average episode reward sum: 0.3925288033231267
{'scaleFactor': 20, 'currentTarget': array([20., 19.]), 'previousTarget': array([20., 19.]), 'currentState': array([19.5712882 , 18.41273656,  0.        ]), 'targetState': array([20, 19], dtype=int32), 'currentDistance': 0.7270984467109716}
episode index:961
target Thresh 22.20773927752242
target distance 14.0
model initialize at round 961
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  5.]), 'previousTarget': array([25.,  5.]), 'currentState': array([21.22179933, 17.44513357,  0.        ]), 'targetState': array([25,  5], dtype=int32), 'currentDistance': 13.00600437520388}
done in step count: 8
reward sum = 0.8109861351447968
running average episode reward sum: 0.3929637901545421
{'scaleFactor': 20, 'currentTarget': array([25.,  5.]), 'previousTarget': array([25.,  5.]), 'currentState': array([24.830784  ,  5.51363879,  0.        ]), 'targetState': array([25,  5], dtype=int32), 'currentDistance': 0.5407946615419902}
episode index:962
target Thresh 22.217526643746176
target distance 11.0
model initialize at round 962
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'previousTarget': array([ 2., 17.]), 'currentState': array([ 8.61394429, 26.95027995,  0.        ]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 11.947900660942969}
done in step count: 9
reward sum = 0.8247660002441373
running average episode reward sum: 0.39341218289606816
{'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'previousTarget': array([ 2., 17.]), 'currentState': array([ 2.00462863, 17.61005363,  0.        ]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 0.6100711877074116}
episode index:963
target Thresh 22.227304227495754
target distance 18.0
model initialize at round 963
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 4.]), 'previousTarget': array([7., 4.]), 'currentState': array([24.24526948, 13.37446475,  0.        ]), 'targetState': array([7, 4], dtype=int32), 'currentDistance': 19.62854830855947}
done in step count: 17
reward sum = 0.7402424585984807
running average episode reward sum: 0.39377196533974285
{'scaleFactor': 20, 'currentTarget': array([7., 4.]), 'previousTarget': array([7., 4.]), 'currentState': array([7.73843336, 4.49906516, 0.        ]), 'targetState': array([7, 4], dtype=int32), 'currentDistance': 0.8912630719333198}
episode index:964
target Thresh 22.237072038548746
target distance 21.0
model initialize at round 964
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 28.]), 'previousTarget': array([ 9., 28.]), 'currentState': array([16.06772858,  8.38627887,  0.        ]), 'targetState': array([ 9, 28], dtype=int32), 'currentDistance': 20.848281558004476}
done in step count: 13
reward sum = 0.7106420042183014
running average episode reward sum: 0.39410032807433204
{'scaleFactor': 20, 'currentTarget': array([ 9., 28.]), 'previousTarget': array([ 9., 28.]), 'currentState': array([ 9.64198898, 27.29813713,  0.        ]), 'targetState': array([ 9, 28], dtype=int32), 'currentDistance': 0.9511894333940024}
episode index:965
target Thresh 22.246830086672965
target distance 18.0
model initialize at round 965
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 23.]), 'previousTarget': array([ 5., 23.]), 'currentState': array([21.77284086, 17.61805189,  0.        ]), 'targetState': array([ 5, 23], dtype=int32), 'currentDistance': 17.615151317175417}
done in step count: 13
reward sum = 0.7622559542124633
running average episode reward sum: 0.39448144155894704
{'scaleFactor': 20, 'currentTarget': array([ 5., 23.]), 'previousTarget': array([ 5., 23.]), 'currentState': array([ 5.74327654, 22.79082876,  0.        ]), 'targetState': array([ 5, 23], dtype=int32), 'currentDistance': 0.7721480528013034}
episode index:966
target Thresh 22.256578381626458
target distance 19.0
model initialize at round 966
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([10.24964969, 22.45673096,  0.        ]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 17.54426350934305}
done in step count: 10
reward sum = 0.7569514050256677
running average episode reward sum: 0.3948562812316117
{'scaleFactor': 20, 'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([11.94912774,  5.91155803,  0.        ]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 0.9129764687484128}
episode index:967
target Thresh 22.26631693315752
target distance 18.0
model initialize at round 967
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 29.]), 'previousTarget': array([25., 29.]), 'currentState': array([16.47281992, 12.29850578,  0.        ]), 'targetState': array([25, 29], dtype=int32), 'currentDistance': 18.752405422245424}
done in step count: 12
reward sum = 0.7313670712829445
running average episode reward sum: 0.3952039163453011
{'scaleFactor': 20, 'currentTarget': array([25., 29.]), 'previousTarget': array([25., 29.]), 'currentState': array([24.53468105, 28.38106585,  0.        ]), 'targetState': array([25, 29], dtype=int32), 'currentDistance': 0.774339209272312}
episode index:968
target Thresh 22.276045751004702
target distance 21.0
model initialize at round 968
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([25.86375928, 17.49130738,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 20.897511913558418}
done in step count: 18
reward sum = 0.7198512325807843
running average episode reward sum: 0.3955389496953893
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.61624253, 11.75940343,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.9779818110022134}
episode index:969
target Thresh 22.285764844896832
target distance 4.0
model initialize at round 969
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([10.82570803,  7.73538023,  0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 3.0957856897607248}
done in step count: 2
reward sum = 0.95861128493333
running average episode reward sum: 0.39611943663893356
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([8.94072723, 8.79432198, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.9629492067707113}
episode index:970
target Thresh 22.29547422455299
target distance 3.0
model initialize at round 970
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 14.]), 'previousTarget': array([ 7., 14.]), 'currentState': array([ 5.67297798, 12.18616986,  0.        ]), 'targetState': array([ 7, 14], dtype=int32), 'currentDistance': 2.247435693826188}
done in step count: 1
reward sum = 0.9749194462135368
running average episode reward sum: 0.39671552315754804
{'scaleFactor': 20, 'currentTarget': array([ 7., 14.]), 'previousTarget': array([ 7., 14.]), 'currentState': array([ 6.31312114, 13.0568639 ,  0.        ]), 'targetState': array([ 7, 14], dtype=int32), 'currentDistance': 1.1667511576553218}
episode index:971
target Thresh 22.305173899682565
target distance 16.0
model initialize at round 971
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([ 6.51688802, 16.45031261,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 14.66786487436016}
done in step count: 9
reward sum = 0.7923690225245676
running average episode reward sum: 0.3971225740828227
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.04715689, 2.70362198, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.7052004454304687}
episode index:972
target Thresh 22.314863879985232
target distance 12.0
model initialize at round 972
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 15.]), 'previousTarget': array([12., 15.]), 'currentState': array([7.28620848, 4.86830437, 0.        ]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 11.174573226311697}
done in step count: 8
reward sum = 0.8322340497591352
running average episode reward sum: 0.39756975956655993
{'scaleFactor': 20, 'currentTarget': array([12., 15.]), 'previousTarget': array([12., 15.]), 'currentState': array([11.51877162, 14.36557895,  0.        ]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 0.7962856424924601}
episode index:973
target Thresh 22.324544175150976
target distance 15.0
model initialize at round 973
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([10.65030581, 17.86149406,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 15.374250714423825}
done in step count: 11
reward sum = 0.7921833236929939
running average episode reward sum: 0.39797490696299365
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.16492546, 4.63095105, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.6521500055140591}
episode index:974
target Thresh 22.33421479486008
target distance 11.0
model initialize at round 974
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([ 6.99115872, 21.87820947,  0.        ]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 9.05154533852593}
done in step count: 6
reward sum = 0.8537756292597707
running average episode reward sum: 0.398442394883298
{'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([15.38402155, 20.78973368,  0.        ]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 0.6508773897400346}
episode index:975
target Thresh 22.343875748783184
target distance 10.0
model initialize at round 975
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 18.]), 'previousTarget': array([11., 18.]), 'currentState': array([19.69421005, 18.26582441,  0.        ]), 'targetState': array([11, 18], dtype=int32), 'currentDistance': 8.698272877552036}
done in step count: 6
reward sum = 0.8824154911768924
running average episode reward sum: 0.39893826895736934
{'scaleFactor': 20, 'currentTarget': array([11., 18.]), 'previousTarget': array([11., 18.]), 'currentState': array([11.88511157, 18.21959455,  0.        ]), 'targetState': array([11, 18], dtype=int32), 'currentDistance': 0.9119453144057864}
episode index:976
target Thresh 22.353527046581227
target distance 6.0
model initialize at round 976
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 22.]), 'previousTarget': array([20., 22.]), 'currentState': array([18.78897125, 26.31046462,  0.        ]), 'targetState': array([20, 22], dtype=int32), 'currentDistance': 4.477353670428284}
done in step count: 3
reward sum = 0.9231887240732459
running average episode reward sum: 0.3994748610301594
{'scaleFactor': 20, 'currentTarget': array([20., 22.]), 'previousTarget': array([20., 22.]), 'currentState': array([19.8657985, 22.4860065,  0.       ]), 'targetState': array([20, 22], dtype=int32), 'currentDistance': 0.5041947621500106}
episode index:977
target Thresh 22.36316869790551
target distance 6.0
model initialize at round 977
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 26.]), 'previousTarget': array([ 4., 26.]), 'currentState': array([ 9.19514775, 28.57815185,  0.        ]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 5.799691987031983}
done in step count: 4
reward sum = 0.9303761380735299
running average episode reward sum: 0.40001770487171706
{'scaleFactor': 20, 'currentTarget': array([ 4., 26.]), 'previousTarget': array([ 4., 26.]), 'currentState': array([ 4.95263541, 26.77774099,  0.        ]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 1.229794803824336}
episode index:978
target Thresh 22.37280071239769
target distance 18.0
model initialize at round 978
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([26.33710468, 24.79224277,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 20.83707198559241}
done in step count: 14
reward sum = 0.7173127541370615
running average episode reward sum: 0.4003418060456347
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.13632491,  8.88171715,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.8921937044638845}
episode index:979
target Thresh 22.38242309968978
target distance 5.0
model initialize at round 979
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([22.9893769 ,  6.09894228,  0.        ]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 5.579729330984665}
done in step count: 4
reward sum = 0.9199883798897723
running average episode reward sum: 0.4008720576515981
{'scaleFactor': 20, 'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([19.74188864,  9.61959371,  0.        ]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 0.8337311936612788}
episode index:980
target Thresh 22.392035869404168
target distance 14.0
model initialize at round 980
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([24.81359172,  8.85472447,  0.        ]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 15.82648627545618}
done in step count: 11
reward sum = 0.7849384781692308
running average episode reward sum: 0.4012635626674163
{'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([16.83853814, 21.22296566,  0.        ]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 1.1432097714024472}
episode index:981
target Thresh 22.40163903115363
target distance 11.0
model initialize at round 981
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 27.]), 'previousTarget': array([ 6., 27.]), 'currentState': array([ 3.42075297, 17.9264338 ,  0.        ]), 'targetState': array([ 6, 27], dtype=int32), 'currentDistance': 9.433033381971452}
done in step count: 6
reward sum = 0.8527854904912994
running average episode reward sum: 0.40172336096458927
{'scaleFactor': 20, 'currentTarget': array([ 6., 27.]), 'previousTarget': array([ 6., 27.]), 'currentState': array([ 6.19307134, 26.28707552,  0.        ]), 'targetState': array([ 6, 27], dtype=int32), 'currentDistance': 0.738605345809994}
episode index:982
target Thresh 22.411232594541318
target distance 19.0
model initialize at round 982
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'previousTarget': array([26., 11.]), 'currentState': array([ 8.98714697, 28.41435251,  0.        ]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 24.345365910470083}
done in step count: 17
reward sum = 0.6370420243997681
running average episode reward sum: 0.4019627492285111
{'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'previousTarget': array([26., 11.]), 'currentState': array([25.63041216, 11.47130371,  0.        ]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 0.5989343477739945}
episode index:983
target Thresh 22.420816569160802
target distance 5.0
model initialize at round 983
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([19.73306036, 18.78870457,  0.        ]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 4.338843983325119}
done in step count: 3
reward sum = 0.9380615638754636
running average episode reward sum: 0.40250756509705476
{'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([16.78495723, 20.72632283,  0.        ]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 0.8312984100809994}
episode index:984
target Thresh 22.430390964596057
target distance 21.0
model initialize at round 984
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  2.]), 'previousTarget': array([10.,  2.]), 'currentState': array([ 8.06353263, 21.32983088,  0.        ]), 'targetState': array([10,  2], dtype=int32), 'currentDistance': 19.426586625614213}
done in step count: 12
reward sum = 0.725727299034503
running average episode reward sum: 0.40283570695892024
{'scaleFactor': 20, 'currentTarget': array([10.,  2.]), 'previousTarget': array([10.,  2.]), 'currentState': array([10.24492674,  2.8081789 ,  0.        ]), 'targetState': array([10,  2], dtype=int32), 'currentDistance': 0.8444774970347179}
episode index:985
target Thresh 22.43995579042148
target distance 3.0
model initialize at round 985
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  5.]), 'previousTarget': array([25.,  5.]), 'currentState': array([26.60571617,  3.22458291,  0.        ]), 'targetState': array([25,  5], dtype=int32), 'currentDistance': 2.393831710158247}
done in step count: 2
reward sum = 0.964373780201503
running average episode reward sum: 0.4034052181893894
{'scaleFactor': 20, 'currentTarget': array([25.,  5.]), 'previousTarget': array([25.,  5.]), 'currentState': array([25.54412454,  4.55177563,  0.        ]), 'targetState': array([25,  5], dtype=int32), 'currentDistance': 0.7049656736594345}
episode index:986
target Thresh 22.449511056201896
target distance 5.0
model initialize at round 986
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 23.]), 'previousTarget': array([19., 23.]), 'currentState': array([15.50072956, 24.15318   ,  0.        ]), 'targetState': array([19, 23], dtype=int32), 'currentDistance': 3.6843883788218124}
done in step count: 3
reward sum = 0.9380581594985343
running average episode reward sum: 0.4039469131653865
{'scaleFactor': 20, 'currentTarget': array([19., 23.]), 'previousTarget': array([19., 23.]), 'currentState': array([18.15045804, 22.66248399,  0.        ]), 'targetState': array([19, 23], dtype=int32), 'currentDistance': 0.9141327049857295}
episode index:987
target Thresh 22.45905677149257
target distance 7.0
model initialize at round 987
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 20.]), 'previousTarget': array([ 8., 20.]), 'currentState': array([13.80226469, 20.95043742,  0.        ]), 'targetState': array([ 8, 20], dtype=int32), 'currentDistance': 5.8795924035761615}
done in step count: 5
reward sum = 0.9144059580624984
running average episode reward sum: 0.40446357211771156
{'scaleFactor': 20, 'currentTarget': array([ 8., 20.]), 'previousTarget': array([ 8., 20.]), 'currentState': array([ 8.50678319, 20.32825442,  0.        ]), 'targetState': array([ 8, 20], dtype=int32), 'currentDistance': 0.6038047375003733}
episode index:988
target Thresh 22.468592945839227
target distance 16.0
model initialize at round 988
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  3.]), 'previousTarget': array([22.,  3.]), 'currentState': array([7.99996185, 3.2333525 , 0.        ]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 14.00198277047394}
done in step count: 8
reward sum = 0.788259968769984
running average episode reward sum: 0.40485163723060563
{'scaleFactor': 20, 'currentTarget': array([22.,  3.]), 'previousTarget': array([22.,  3.]), 'currentState': array([21.19915789,  2.62859396,  0.        ]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 0.8827743327115595}
episode index:989
target Thresh 22.47811958877803
target distance 15.0
model initialize at round 989
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([ 4.6322605 , 22.83592296,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 15.67534208164978}
done in step count: 11
reward sum = 0.7781740553400376
running average episode reward sum: 0.40522873058223136
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([11.68278466,  9.45388532,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.5537485477167837}
episode index:990
target Thresh 22.487636709835627
target distance 15.0
model initialize at round 990
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  5.]), 'previousTarget': array([18.,  5.]), 'currentState': array([14.11258598, 18.44608879,  0.        ]), 'targetState': array([18,  5], dtype=int32), 'currentDistance': 13.996760035917601}
done in step count: 9
reward sum = 0.7983372838201194
running average episode reward sum: 0.4056254092434199
{'scaleFactor': 20, 'currentTarget': array([18.,  5.]), 'previousTarget': array([18.,  5.]), 'currentState': array([17.8072365,  5.4481374,  0.       ]), 'targetState': array([18,  5], dtype=int32), 'currentDistance': 0.4878369574087633}
episode index:991
target Thresh 22.497144318529145
target distance 3.0
model initialize at round 991
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([14.60813725,  6.17557025,  0.        ]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 2.4320052074296976}
done in step count: 2
reward sum = 0.9651457980191055
running average episode reward sum: 0.4061894418933954
{'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([13.63863826,  7.55432248,  0.        ]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 0.7787729292673802}
episode index:992
target Thresh 22.50664242436619
target distance 9.0
model initialize at round 992
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 7.88610625, 19.10907793,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 9.467374461512664}
done in step count: 7
reward sum = 0.862862992508097
running average episode reward sum: 0.4066493346936116
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.02706268, 11.53041714,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.5311070851038945}
episode index:993
target Thresh 22.516131036844868
target distance 13.0
model initialize at round 993
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 24.]), 'previousTarget': array([14., 24.]), 'currentState': array([15.89138652, 12.76922488,  0.        ]), 'targetState': array([14, 24], dtype=int32), 'currentDistance': 11.388926758156346}
done in step count: 8
reward sum = 0.8286583223960055
running average episode reward sum: 0.40707389101926794
{'scaleFactor': 20, 'currentTarget': array([14., 24.]), 'previousTarget': array([14., 24.]), 'currentState': array([14.42807517, 23.51207972,  0.        ]), 'targetState': array([14, 24], dtype=int32), 'currentDistance': 0.6490874799997984}
episode index:994
target Thresh 22.52561016545379
target distance 14.0
model initialize at round 994
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([16.73202276,  2.38735136,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 13.91424551788678}
done in step count: 10
reward sum = 0.8136845916733514
running average episode reward sum: 0.40748254498977454
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.88610673, 7.48161823, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.0265986558217959}
episode index:995
target Thresh 22.53507981967209
target distance 11.0
model initialize at round 995
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 27.]), 'previousTarget': array([ 6., 27.]), 'currentState': array([15.61910415, 18.37483412,  0.        ]), 'targetState': array([ 6, 27], dtype=int32), 'currentDistance': 12.91977751522092}
done in step count: 10
reward sum = 0.8211918426086008
running average episode reward sum: 0.4078979157705163
{'scaleFactor': 20, 'currentTarget': array([ 6., 27.]), 'previousTarget': array([ 6., 27.]), 'currentState': array([ 6.73971838, 26.41610104,  0.        ]), 'targetState': array([ 6, 27], dtype=int32), 'currentDistance': 0.9424018651494458}
episode index:996
target Thresh 22.54454000896942
target distance 16.0
model initialize at round 996
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 26.]), 'previousTarget': array([19., 26.]), 'currentState': array([16.45331496, 11.97078991,  0.        ]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 14.25848310516167}
done in step count: 9
reward sum = 0.7811912589359672
running average episode reward sum: 0.40827233236346056
{'scaleFactor': 20, 'currentTarget': array([19., 26.]), 'previousTarget': array([19., 26.]), 'currentState': array([19.27778292, 25.33993119,  0.        ]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 0.7161383854012738}
episode index:997
target Thresh 22.553990742805972
target distance 10.0
model initialize at round 997
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 27.]), 'previousTarget': array([ 4., 27.]), 'currentState': array([12.57435799, 23.15844189,  0.        ]), 'targetState': array([ 4, 27], dtype=int32), 'currentDistance': 9.395593840768795}
done in step count: 7
reward sum = 0.8678752167568761
running average episode reward sum: 0.40873285629571854
{'scaleFactor': 20, 'currentTarget': array([ 4., 27.]), 'previousTarget': array([ 4., 27.]), 'currentState': array([ 4.8483578 , 26.83637185,  0.        ]), 'targetState': array([ 4, 27], dtype=int32), 'currentDistance': 0.8639937044620968}
episode index:998
target Thresh 22.563432030632477
target distance 10.0
model initialize at round 998
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([12.08017778, 11.63203812,  0.        ]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 10.466918224261704}
done in step count: 6
reward sum = 0.8486614785042292
running average episode reward sum: 0.40917322528691824
{'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([17.09024924,  3.32517731,  0.        ]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 0.9661194174471127}
episode index:999
target Thresh 22.57286388189023
target distance 17.0
model initialize at round 999
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'previousTarget': array([10., 12.]), 'currentState': array([17.06712344, 27.91794455,  0.        ]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 17.41623358723012}
done in step count: 13
reward sum = 0.7661737861131128
running average episode reward sum: 0.4095302258477444
{'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'previousTarget': array([10., 12.]), 'currentState': array([ 9.90462802, 12.67957544,  0.        ]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 0.6862350897574637}

Process finished with exit code 0
