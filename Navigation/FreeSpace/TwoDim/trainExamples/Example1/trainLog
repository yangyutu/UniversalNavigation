/home/yangyutu/anaconda3/bin/python /home/yangyutu/Dropbox/UniversalNavigationProject/activeParticleModel/Navigation/FreeSpace/TwoDim/DDPGHER_MLP.py
episode index:0
target Thresh 6.399999999999999
target distance 3.0
model initialize at round 0
at step 0:
{'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([15.02964886,  7.01294537,  0.        ]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 3.130057940703636}
done in step count: 99
reward sum = -0.13070098383493886
running average episode reward sum: -0.13070098383493886
{'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([-8.8306389 , 35.00425314,  0.        ]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 38.06721517708735}
episode index:1
target Thresh 6.425587204265597
target distance 2.0
model initialize at round 1
at step 0:
{'currentTarget': array([9., 8.]), 'previousTarget': array([9., 8.]), 'currentState': array([ 8.24132282, 10.73915559,  0.        ]), 'targetState': array([9, 8], dtype=int32), 'currentDistance': 2.8422815517749065}
done in step count: 99
reward sum = -2.346620274622802
running average episode reward sum: -1.2386606292288704
{'currentTarget': array([-44.03738272,  61.72338794]), 'previousTarget': array([-42.04686964,  59.71402278]), 'currentState': array([-184.54711694,  204.05052727,    0.        ]), 'targetState': array([9, 8], dtype=int32), 'currentDistance': 200.00000000000003}
episode index:2
target Thresh 6.451148834116271
target distance 4.0
model initialize at round 2
at step 0:
{'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 8.29317689, 10.46956873,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 2.340759500204698}
done in step count: 99
reward sum = -2.4905323924516445
running average episode reward sum: -1.655951216969795
{'currentTarget': array([-6.96595041, 33.14138792]), 'previousTarget': array([-6.86934934, 32.33317158]), 'currentState': array([-104.72549796,  207.62081611,    0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 200.0}
episode index:3
target Thresh 6.476684915113651
target distance 5.0
model initialize at round 3
at step 0:
{'currentTarget': array([18.,  9.]), 'previousTarget': array([18.,  9.]), 'currentState': array([23.12387943, 13.67438307,  0.        ]), 'targetState': array([18,  9], dtype=int32), 'currentDistance': 6.9357045470191325}
done in step count: 99
reward sum = -0.8999878228929691
running average episode reward sum: -1.4669603684505885
{'currentTarget': array([18.,  9.]), 'previousTarget': array([18.,  9.]), 'currentState': array([ 18.43770885, -26.66510551,   0.        ]), 'targetState': array([18,  9], dtype=int32), 'currentDistance': 35.667791358345525}
episode index:4
target Thresh 6.502195472793815
target distance 3.0
model initialize at round 4
at step 0:
{'currentTarget': array([ 2., 26.]), 'previousTarget': array([ 2., 26.]), 'currentState': array([ 4.41604728, 23.14495634,  0.        ]), 'targetState': array([ 2, 26], dtype=int32), 'currentDistance': 3.7401281745393447}
done in step count: 99
reward sum = -0.6217805294392008
running average episode reward sum: -1.297924400648311
{'currentTarget': array([ 2., 26.]), 'previousTarget': array([ 2., 26.]), 'currentState': array([-27.94322304, 130.03354707,   0.        ]), 'targetState': array([ 2, 26], dtype=int32), 'currentDistance': 108.2569883332902}
episode index:5
target Thresh 6.527680532667333
target distance 3.0
model initialize at round 5
at step 0:
{'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([10.35201377, 24.50987464,  0.        ]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 3.8775093329030885}
done in step count: 99
reward sum = -1.0766340218323893
running average episode reward sum: -1.2610426708456575
{'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([  8.22314019, 198.39650154,   0.        ]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 177.43670259853678}
episode index:6
target Thresh 6.553140120219254
target distance 6.0
model initialize at round 6
at step 0:
{'currentTarget': array([ 8., 13.]), 'previousTarget': array([ 8., 13.]), 'currentState': array([ 1.61941844, 14.49253407,  0.        ]), 'targetState': array([ 8, 13], dtype=int32), 'currentDistance': 6.55282221404045}
done in step count: 99
reward sum = -0.9539510140325933
running average episode reward sum: -1.2171724341580767
{'currentTarget': array([ 8., 13.]), 'previousTarget': array([ 8., 13.]), 'currentState': array([-32.61568889, 181.71289524,   0.        ]), 'targetState': array([ 8, 13], dtype=int32), 'currentDistance': 173.53292254112364}
episode index:7
target Thresh 6.578574260909178
target distance 6.0
model initialize at round 7
at step 0:
{'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.85843357, 10.3958106 ,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 6.397377145073312}
done in step count: 99
reward sum = -0.9538602840783504
running average episode reward sum: -1.184258415398111
{'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([-22.95796252, 180.48266211,   0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 180.31090099978545}
episode index:8
target Thresh 6.603982980171246
target distance 4.0
model initialize at round 8
at step 0:
{'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([12.77229714, 15.24517088,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 4.600273137119253}
done in step count: 99
reward sum = -0.6715323028295191
running average episode reward sum: -1.127288847334934
{'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([-20.3042733 , 157.16726191,   0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 149.48185837068172}
episode index:9
target Thresh 6.6293663034141765
target distance 3.0
model initialize at round 9
at step 0:
{'currentTarget': array([12.,  4.]), 'previousTarget': array([12.,  4.]), 'currentState': array([9.83962895, 7.37221643, 0.        ]), 'targetState': array([12,  4], dtype=int32), 'currentDistance': 4.004877869253709}
done in step count: 99
reward sum = -0.4870320465687199
running average episode reward sum: -1.0632631672583126
{'currentTarget': array([12.,  4.]), 'previousTarget': array([12.,  4.]), 'currentState': array([-13.4936639 , 131.40367559,   0.        ]), 'targetState': array([12,  4], dtype=int32), 'currentDistance': 129.92930174407292}
episode index:10
target Thresh 6.6547242560212965
target distance 4.0
model initialize at round 10
at step 0:
{'currentTarget': array([16., 19.]), 'previousTarget': array([16., 19.]), 'currentState': array([19.55688208, 16.31284231,  0.        ]), 'targetState': array([16, 19], dtype=int32), 'currentDistance': 4.457827566099193}
done in step count: 6
reward sum = 0.9321952511516488
running average episode reward sum: -0.8818578564937707
{'currentTarget': array([16., 19.]), 'previousTarget': array([16., 19.]), 'currentState': array([16.98642692, 18.070687  ,  0.        ]), 'targetState': array([16, 19], dtype=int32), 'currentDistance': 1.355234565431724}
episode index:11
target Thresh 6.680056863350558
target distance 5.0
model initialize at round 11
at step 0:
{'currentTarget': array([27., 28.]), 'previousTarget': array([27., 28.]), 'currentState': array([26.06206131, 23.09893851,  0.        ]), 'targetState': array([27, 28], dtype=int32), 'currentDistance': 4.990003278260804}
done in step count: 99
reward sum = -1.6531403592122351
running average episode reward sum: -0.9461313983869761
{'currentTarget': array([27., 28.]), 'previousTarget': array([27., 28.]), 'currentState': array([-163.14238524,   41.46850773,    0.        ]), 'targetState': array([27, 28], dtype=int32), 'currentDistance': 190.61880118937808}
episode index:12
target Thresh 6.705364150734578
target distance 6.0
model initialize at round 12
at step 0:
{'currentTarget': array([20.,  4.]), 'previousTarget': array([20.,  4.]), 'currentState': array([21.40835267, 10.26172051,  0.        ]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 6.418146225836999}
done in step count: 99
reward sum = -0.1978561875088014
running average episode reward sum: -0.8885717667809627
{'currentTarget': array([20.,  4.]), 'previousTarget': array([20.,  4.]), 'currentState': array([22.20238348, 87.09378138,  0.        ]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 83.12296311698132}
episode index:13
target Thresh 6.730646143480637
target distance 6.0
model initialize at round 13
at step 0:
{'currentTarget': array([19., 17.]), 'previousTarget': array([19., 17.]), 'currentState': array([24.64493072, 12.51004899,  0.        ]), 'targetState': array([19, 17], dtype=int32), 'currentDistance': 7.212829053158912}
done in step count: 13
reward sum = 0.8585031082632719
running average episode reward sum: -0.7637807042778031
{'currentTarget': array([19., 17.]), 'previousTarget': array([19., 17.]), 'currentState': array([19.63793987, 17.11915609,  0.        ]), 'targetState': array([19, 17], dtype=int32), 'currentDistance': 0.6489726130960469}
episode index:14
target Thresh 6.755902866870734
target distance 5.0
model initialize at round 14
at step 0:
{'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([18.53198275, 10.33794382,  0.        ]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 4.887723647327882}
done in step count: 99
reward sum = -0.9605844268345004
running average episode reward sum: -0.7769009524482496
{'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([14.26116041, 91.62845074,  0.        ]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 76.84304615304933}
episode index:15
target Thresh 6.781134346161593
target distance 3.0
model initialize at round 15
at step 0:
{'currentTarget': array([25., 21.]), 'previousTarget': array([25., 21.]), 'currentState': array([24.50668406, 18.31060526,  0.        ]), 'targetState': array([25, 21], dtype=int32), 'currentDistance': 2.734264927087722}
done in step count: 99
reward sum = -0.5201774127973577
running average episode reward sum: -0.7608557312200689
{'currentTarget': array([25., 21.]), 'previousTarget': array([25., 21.]), 'currentState': array([-94.41385074,  16.91498445,   0.        ]), 'targetState': array([25, 21], dtype=int32), 'currentDistance': 119.48370223800852}
episode index:16
target Thresh 6.806340606584698
target distance 2.0
model initialize at round 16
at step 0:
{'currentTarget': array([14., 26.]), 'previousTarget': array([14., 26.]), 'currentState': array([15.35571295, 27.31212243,  0.        ]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 1.8866962888120509}
done in step count: 99
reward sum = -0.07240925726412319
running average episode reward sum: -0.7203588798108956
{'currentTarget': array([14., 26.]), 'previousTarget': array([14., 26.]), 'currentState': array([-2.93773593, 21.64972845,  0.        ]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 17.48747439937328}
episode index:17
target Thresh 6.831521673346312
target distance 5.0
model initialize at round 17
at step 0:
{'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([16.49375528,  9.46846223,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 4.52508220118676}
done in step count: 99
reward sum = -0.06069998815265505
running average episode reward sum: -0.68371116360766
{'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([7.69844123, 8.24415216, 0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 4.646117676703536}
episode index:18
target Thresh 6.8566775716275
target distance 6.0
model initialize at round 18
at step 0:
{'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([13.15706921, 10.82285078,  0.        ]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 8.98505943437299}
done in step count: 99
reward sum = -0.027121301452280814
running average episode reward sum: -0.6491538024415874
{'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([17.56272781,  8.04895809,  0.        ]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 3.9033884185338827}
episode index:19
target Thresh 6.881808326584164
target distance 5.0
model initialize at round 19
at step 0:
{'currentTarget': array([ 5., 19.]), 'previousTarget': array([ 5., 19.]), 'currentState': array([ 9.46179461, 20.59556407,  0.        ]), 'targetState': array([ 5, 19], dtype=int32), 'currentDistance': 4.738505658712174}
done in step count: 99
reward sum = -0.04712718886790244
running average episode reward sum: -0.6190524717629031
{'currentTarget': array([ 5., 19.]), 'previousTarget': array([ 5., 19.]), 'currentState': array([ 3.38039973, 46.50825779,  0.        ]), 'targetState': array([ 5, 19], dtype=int32), 'currentDistance': 27.55589503987793}
episode index:20
target Thresh 6.9069139633470655
target distance 5.0
model initialize at round 20
at step 0:
{'currentTarget': array([23., 19.]), 'previousTarget': array([23., 19.]), 'currentState': array([18.68749624, 22.84569691,  0.        ]), 'targetState': array([23, 19], dtype=int32), 'currentDistance': 5.778154841350411}
done in step count: 99
reward sum = -0.024387427664328748
running average episode reward sum: -0.5907350887105901
{'currentTarget': array([23., 19.]), 'previousTarget': array([23., 19.]), 'currentState': array([22.20572163, 39.85485599,  0.        ]), 'targetState': array([23, 19], dtype=int32), 'currentDistance': 20.86997595432318}
episode index:21
target Thresh 6.931994507021834
target distance 2.0
model initialize at round 21
at step 0:
{'currentTarget': array([21., 15.]), 'previousTarget': array([21., 15.]), 'currentState': array([22.67500231, 15.20147833,  0.        ]), 'targetState': array([21, 15], dtype=int32), 'currentDistance': 1.6870762418539846}
done in step count: 3
reward sum = 0.967366156229615
running average episode reward sum: -0.5199123048496718
{'currentTarget': array([21., 15.]), 'previousTarget': array([21., 15.]), 'currentState': array([21.66792229, 15.77890261,  0.        ]), 'targetState': array([21, 15], dtype=int32), 'currentDistance': 1.026065032732732}
episode index:22
target Thresh 6.957049982689021
target distance 5.0
model initialize at round 22
at step 0:
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([ 6.29642826, 12.44772375,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 6.938082576383277}
done in step count: 99
reward sum = -0.02171301691831775
running average episode reward sum: -0.4982514662439607
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([ 5.89350421, 13.72392519,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 7.76984844986577}
episode index:23
target Thresh 6.982080415404106
target distance 3.0
model initialize at round 23
at step 0:
{'currentTarget': array([17.,  2.]), 'previousTarget': array([17.,  2.]), 'currentState': array([19.71802723,  3.12294455,  0.        ]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 2.9408632244121398}
done in step count: 99
reward sum = -0.007556159974834442
running average episode reward sum: -0.4778058284827471
{'currentTarget': array([17.,  2.]), 'previousTarget': array([17.,  2.]), 'currentState': array([20.75463493,  7.7131438 ,  0.        ]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 6.83646806421665}
episode index:24
target Thresh 7.007085830197521
target distance 4.0
model initialize at round 24
at step 0:
{'currentTarget': array([ 9., 19.]), 'previousTarget': array([ 9., 19.]), 'currentState': array([ 4.94382405, 21.99864142,  0.        ]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 5.044245604719144}
done in step count: 99
reward sum = -0.0039515700054109934
running average episode reward sum: -0.45885165814365364
{'currentTarget': array([ 9., 19.]), 'previousTarget': array([ 9., 19.]), 'currentState': array([11.55943816, 26.51150385,  0.        ]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 7.935578987510488}
episode index:25
target Thresh 7.032066252074685
target distance 7.0
model initialize at round 25
at step 0:
{'currentTarget': array([27., 16.]), 'previousTarget': array([27., 16.]), 'currentState': array([21.5681302 , 22.94121767,  0.        ]), 'targetState': array([27, 16], dtype=int32), 'currentDistance': 8.813949869179938}
done in step count: 99
reward sum = -0.03375189059634263
running average episode reward sum: -0.4425016670841417
{'currentTarget': array([27., 16.]), 'previousTarget': array([27., 16.]), 'currentState': array([36.15608122, 33.9360001 ,  0.        ]), 'targetState': array([27, 16], dtype=int32), 'currentDistance': 20.13787285293529}
episode index:26
target Thresh 7.057021706016016
target distance 5.0
model initialize at round 26
at step 0:
{'currentTarget': array([ 3., 12.]), 'previousTarget': array([ 3., 12.]), 'currentState': array([ 2.9303725 , 17.11362866,  0.        ]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 5.114102667449643}
done in step count: 99
reward sum = -0.006970180887666158
running average episode reward sum: -0.426370871299087
{'currentTarget': array([ 3., 12.]), 'previousTarget': array([ 3., 12.]), 'currentState': array([ 3.84613937, 15.32907372,  0.        ]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 3.4349212017629753}
episode index:27
target Thresh 7.081952216976976
target distance 5.0
model initialize at round 27
at step 0:
{'currentTarget': array([27., 22.]), 'previousTarget': array([27., 22.]), 'currentState': array([21.67747751, 24.96519351,  0.        ]), 'targetState': array([27, 22], dtype=int32), 'currentDistance': 6.0927512868715645}
done in step count: 99
reward sum = -0.07944252905321579
running average episode reward sum: -0.4139805733617345
{'currentTarget': array([27., 22.]), 'previousTarget': array([27., 22.]), 'currentState': array([ 6.58711792, 29.851363  ,  0.        ]), 'targetState': array([27, 22], dtype=int32), 'currentDistance': 21.870748867538666}
episode index:28
target Thresh 7.10685780988808
target distance 3.0
model initialize at round 28
at step 0:
{'currentTarget': array([24., 17.]), 'previousTarget': array([24., 17.]), 'currentState': array([26.57982579, 17.22417982,  0.        ]), 'targetState': array([24, 17], dtype=int32), 'currentDistance': 2.589547777426901}
done in step count: 99
reward sum = -0.032234018241871894
running average episode reward sum: -0.4008168990472565
{'currentTarget': array([24., 17.]), 'previousTarget': array([24., 17.]), 'currentState': array([ 9.4892051 , 20.98359892,  0.        ]), 'targetState': array([24, 17], dtype=int32), 'currentDistance': 15.047665232668342}
episode index:29
target Thresh 7.131738509654916
target distance 7.0
model initialize at round 29
at step 0:
{'currentTarget': array([5., 4.]), 'previousTarget': array([5., 4.]), 'currentState': array([11.9446079,  7.9811714,  0.       ]), 'targetState': array([5, 4], dtype=int32), 'currentDistance': 8.00483008021734}
done in step count: 99
reward sum = -0.003571162750477972
running average episode reward sum: -0.3875753745040305
{'currentTarget': array([5., 4.]), 'previousTarget': array([5., 4.]), 'currentState': array([5.11654008, 8.53605762, 0.        ]), 'targetState': array([5, 4], dtype=int32), 'currentDistance': 4.53755444102037}
episode index:30
target Thresh 7.15659434115819
target distance 6.0
model initialize at round 30
at step 0:
{'currentTarget': array([20., 20.]), 'previousTarget': array([20., 20.]), 'currentState': array([14.44551286, 25.92286087,  0.        ]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 8.119889669331409}
done in step count: 99
reward sum = -0.019721898694455457
running average episode reward sum: -0.37570913334888295
{'currentTarget': array([20., 20.]), 'previousTarget': array([20., 20.]), 'currentState': array([18.57453556, 23.49000472,  0.        ]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 3.7698914855045396}
episode index:31
target Thresh 7.1814253292537344
target distance 6.0
model initialize at round 31
at step 0:
{'currentTarget': array([20.,  2.]), 'previousTarget': array([20.,  2.]), 'currentState': array([16.02425027,  7.70368323,  0.        ]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 6.9525957944817725}
done in step count: 99
reward sum = -0.03454273150310852
running average episode reward sum: -0.3650476832912025
{'currentTarget': array([20.,  2.]), 'previousTarget': array([20.,  2.]), 'currentState': array([22.69224278,  4.89016234,  0.        ]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 3.949836649716479}
episode index:32
target Thresh 7.206231498772539
target distance 6.0
model initialize at round 32
at step 0:
{'currentTarget': array([25., 24.]), 'previousTarget': array([25., 24.]), 'currentState': array([19.08675549, 27.93979714,  0.        ]), 'targetState': array([25, 24], dtype=int32), 'currentDistance': 7.10552335323539}
done in step count: 99
reward sum = -0.013945889001257369
running average episode reward sum: -0.354408234979386
{'currentTarget': array([25., 24.]), 'previousTarget': array([25., 24.]), 'currentState': array([ 5.01845431, 27.87603866,  0.        ]), 'targetState': array([25, 24], dtype=int32), 'currentDistance': 20.35401296670555}
episode index:33
target Thresh 7.231012874520779
target distance 2.0
model initialize at round 33
at step 0:
{'currentTarget': array([ 2., 22.]), 'previousTarget': array([ 2., 22.]), 'currentState': array([ 3.51954567, 20.34777838,  0.        ]), 'targetState': array([ 2, 22], dtype=int32), 'currentDistance': 2.2447394811483603}
done in step count: 2
reward sum = 0.9751580200465421
running average episode reward sum: -0.31530334512568226
{'currentTarget': array([ 2., 22.]), 'previousTarget': array([ 2., 22.]), 'currentState': array([ 2.59287384, 21.00528726,  0.        ]), 'targetState': array([ 2, 22], dtype=int32), 'currentDistance': 1.1579951759164433}
episode index:34
target Thresh 7.2557694812798275
target distance 3.0
model initialize at round 34
at step 0:
{'currentTarget': array([26., 19.]), 'previousTarget': array([26., 19.]), 'currentState': array([22.7384508 , 20.11643045,  0.        ]), 'targetState': array([26, 19], dtype=int32), 'currentDistance': 3.447335227747602}
done in step count: 99
reward sum = -0.057474151611291636
running average episode reward sum: -0.30793679673955676
{'currentTarget': array([26., 19.]), 'previousTarget': array([26., 19.]), 'currentState': array([-18.61021166,  17.5656914 ,   0.        ]), 'targetState': array([26, 19], dtype=int32), 'currentDistance': 44.633263661908885}
episode index:35
target Thresh 7.280501343806296
target distance 2.0
model initialize at round 35
at step 0:
{'currentTarget': array([23., 25.]), 'previousTarget': array([23., 25.]), 'currentState': array([24.70921436, 27.18405522,  0.        ]), 'targetState': array([23, 25], dtype=int32), 'currentDistance': 2.773357342381808}
done in step count: 99
reward sum = -0.003338484282832299
running average episode reward sum: -0.2994757325046477
{'currentTarget': array([23., 25.]), 'previousTarget': array([23., 25.]), 'currentState': array([24.84728762, 27.69984066,  0.        ]), 'targetState': array([23, 25], dtype=int32), 'currentDistance': 3.2713317053809026}
episode index:36
target Thresh 7.305208486832047
target distance 2.0
model initialize at round 36
at step 0:
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([13.88960442,  5.07808051,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 2.1118394994892986}
done in step count: 99
reward sum = -0.055022458896724524
running average episode reward sum: -0.2928688872720011
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([-24.02616645,   9.35997619,   0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 40.26292826886938}
episode index:37
target Thresh 7.329890935064228
target distance 3.0
model initialize at round 37
at step 0:
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([12.54077545, 12.28998247,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 2.8494902430170077}
done in step count: 99
reward sum = -0.0059648005854484034
running average episode reward sum: -0.28531877972761815
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([11.85414136, 13.41384662,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 3.043763410439613}
episode index:38
target Thresh 7.354548713185292
target distance 5.0
model initialize at round 38
at step 0:
{'currentTarget': array([24.,  6.]), 'previousTarget': array([24.,  6.]), 'currentState': array([19.18329404,  2.95780364,  0.        ]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 5.6969829705202475}
done in step count: 99
reward sum = -0.006812147739917751
running average episode reward sum: -0.2781775840356258
{'currentTarget': array([24.,  6.]), 'previousTarget': array([24.,  6.]), 'currentState': array([19.48448284,  7.88228613,  0.        ]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 4.892125946133062}
episode index:39
target Thresh 7.379181845853012
target distance 7.0
model initialize at round 39
at step 0:
{'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([ 8.8970592 , 11.90353191,  0.        ]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 6.669327596927507}
done in step count: 99
reward sum = -0.08421945789392066
running average episode reward sum: -0.2733286308820832
{'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([13.38758018,  7.76366365,  0.        ]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 2.2440784842670123}
episode index:40
target Thresh 7.403790357700526
target distance 6.0
model initialize at round 40
at step 0:
{'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([28.88814592, 16.52457011,  0.        ]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 6.660758607536421}
done in step count: 99
reward sum = -0.13916518228578428
running average episode reward sum: -0.27005635164802716
{'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([71.55615538, 35.89681774,  0.        ]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 53.22260622008126}
episode index:41
target Thresh 7.428374273336345
target distance 7.0
model initialize at round 41
at step 0:
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.99670696, 10.12308669,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 5.219141884617725}
done in step count: 99
reward sum = -0.13930653951823888
running average episode reward sum: -0.2669432608830322
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([14.56217132,  3.28178069,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 2.2404528347497146}
episode index:42
target Thresh 7.452933617344396
target distance 5.0
model initialize at round 42
at step 0:
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([6.37673599, 2.48791382, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 4.406591669701777}
done in step count: 27
reward sum = 0.756309084785779
running average episode reward sum: -0.24314669470468778
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.89364132, 2.06705335, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.291891739825553}
episode index:43
target Thresh 7.477468414284015
target distance 5.0
model initialize at round 43
at step 0:
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 3.98986387, 13.10146177,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 3.684918361441736}
done in step count: 36
reward sum = 0.6038664571593808
running average episode reward sum: -0.22389639579868623
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.96815475, 9.21891767, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.243950654771757}
episode index:44
target Thresh 7.501978688690002
target distance 1.0
model initialize at round 44
at step 0:
{'currentTarget': array([26., 27.]), 'previousTarget': array([26., 27.]), 'currentState': array([27.55743742, 26.53986418,  0.        ]), 'targetState': array([26, 27], dtype=int32), 'currentDistance': 1.6239877731837218}
done in step count: 5
reward sum = 0.9276994402930134
running average episode reward sum: -0.19830537721887068
{'currentTarget': array([26., 27.]), 'previousTarget': array([26., 27.]), 'currentState': array([26.96719622, 26.19974825,  0.        ]), 'targetState': array([26, 27], dtype=int32), 'currentDistance': 1.2553371586048523}
episode index:45
target Thresh 7.526464465072639
target distance 7.0
model initialize at round 45
at step 0:
{'currentTarget': array([ 3., 16.]), 'previousTarget': array([ 3., 16.]), 'currentState': array([ 9.99654627, 21.03543186,  0.        ]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 8.620164367479031}
done in step count: 71
reward sum = 0.3822967886754901
running average episode reward sum: -0.1856835910037759
{'currentTarget': array([ 3., 16.]), 'previousTarget': array([ 3., 16.]), 'currentState': array([ 3.9841395 , 15.10394564,  0.        ]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 1.3309560381577457}
episode index:46
target Thresh 7.550925767917704
target distance 5.0
model initialize at round 46
at step 0:
{'currentTarget': array([8., 6.]), 'previousTarget': array([8., 6.]), 'currentState': array([4.99499261, 9.03343844, 0.        ]), 'targetState': array([8, 6], dtype=int32), 'currentDistance': 4.269873321953903}
done in step count: 2
reward sum = 0.8702992912044395
running average episode reward sum: -0.16321587010572877
{'currentTarget': array([8., 6.]), 'previousTarget': array([8., 6.]), 'currentState': array([8.82030308, 5.29800498, 0.        ]), 'targetState': array([8, 6], dtype=int32), 'currentDistance': 1.0796731673941486}
episode index:47
target Thresh 7.575362621686498
target distance 3.0
model initialize at round 47
at step 0:
{'currentTarget': array([18.,  5.]), 'previousTarget': array([18.,  5.]), 'currentState': array([18.98665953,  6.04319429,  0.        ]), 'targetState': array([18,  5], dtype=int32), 'currentDistance': 1.435879993891607}
done in step count: 9
reward sum = 0.8461005176443364
running average episode reward sum: -0.14218844536093575
{'currentTarget': array([18.,  5.]), 'previousTarget': array([18.,  5.]), 'currentState': array([18.89822914,  4.38172251,  0.        ]), 'targetState': array([18,  5], dtype=int32), 'currentDistance': 1.0904506585048213}
episode index:48
target Thresh 7.599775050815879
target distance 7.0
model initialize at round 48
at step 0:
{'currentTarget': array([19.,  5.]), 'previousTarget': array([19.,  5.]), 'currentState': array([20.99859965, 10.01934445,  0.        ]), 'targetState': array([19,  5], dtype=int32), 'currentDistance': 5.402612261955162}
done in step count: 37
reward sum = 0.5659317017622335
running average episode reward sum: -0.1277370137869935
{'currentTarget': array([19.,  5.]), 'previousTarget': array([19.,  5.]), 'currentState': array([19.98898641,  4.06642224,  0.        ]), 'targetState': array([19,  5], dtype=int32), 'currentDistance': 1.360022630508907}
episode index:49
target Thresh 7.624163079718272
target distance 6.0
model initialize at round 49
at step 0:
{'currentTarget': array([ 7., 15.]), 'previousTarget': array([ 7., 15.]), 'currentState': array([12.99817717, 19.01676762,  0.        ]), 'targetState': array([ 7, 15], dtype=int32), 'currentDistance': 7.218902374543285}
done in step count: 79
reward sum = 0.35210439001885946
running average episode reward sum: -0.11814018571087644
{'currentTarget': array([ 7., 15.]), 'previousTarget': array([ 7., 15.]), 'currentState': array([ 7.99566167, 14.28415546,  0.        ]), 'targetState': array([ 7, 15], dtype=int32), 'currentDistance': 1.2262852726333486}
episode index:50
target Thresh 7.648526732781718
target distance 7.0
model initialize at round 50
at step 0:
{'currentTarget': array([18., 23.]), 'previousTarget': array([18., 23.]), 'currentState': array([23.26871437, 16.43148336,  0.        ]), 'targetState': array([18, 23], dtype=int32), 'currentDistance': 8.420496539368488}
done in step count: 99
reward sum = -0.01848730314844112
running average episode reward sum: -0.11618620762141692
{'currentTarget': array([18., 23.]), 'previousTarget': array([18., 23.]), 'currentState': array([17.54014776, 18.98998713,  0.        ]), 'targetState': array([18, 23], dtype=int32), 'currentDistance': 4.036293760810987}
episode index:51
target Thresh 7.672866034369868
target distance 5.0
model initialize at round 51
at step 0:
{'currentTarget': array([12.,  3.]), 'previousTarget': array([12.,  3.]), 'currentState': array([17.98775196,  6.08718169,  0.        ]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 6.736754731638808}
done in step count: 99
reward sum = -0.05523946755306292
running average episode reward sum: -0.11501415492779474
{'currentTarget': array([12.,  3.]), 'previousTarget': array([12.,  3.]), 'currentState': array([19.20200701,  7.41324351,  0.        ]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 8.44663384378643}
episode index:52
target Thresh 7.697181008822028
target distance 4.0
model initialize at round 52
at step 0:
{'currentTarget': array([12.,  2.]), 'previousTarget': array([12.,  2.]), 'currentState': array([10.99748087,  4.01413548,  0.        ]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 2.2498414030625185}
done in step count: 1
reward sum = 0.9137286983502574
running average episode reward sum: -0.0956039124131145
{'currentTarget': array([12.,  2.]), 'previousTarget': array([12.,  2.]), 'currentState': array([12.9549408 ,  2.12655258,  0.        ]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 0.963289925011103}
episode index:53
target Thresh 7.721471680453163
target distance 1.0
model initialize at round 53
at step 0:
{'currentTarget': array([19., 15.]), 'previousTarget': array([19., 15.]), 'currentState': array([17.76755983, 14.10118475,  0.        ]), 'targetState': array([19, 15], dtype=int32), 'currentDistance': 1.525377867477878}
done in step count: 99
reward sum = -0.0026268616022333993
running average episode reward sum: -0.09388211517587595
{'currentTarget': array([19., 15.]), 'previousTarget': array([19., 15.]), 'currentState': array([18.33583935, 13.71705715,  0.        ]), 'targetState': array([19, 15], dtype=int32), 'currentDistance': 1.4446631876371503}
episode index:54
target Thresh 7.745738073553962
target distance 7.0
model initialize at round 54
at step 0:
{'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([12.99993372,  9.00597203,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 5.836107746562455}
done in step count: 11
reward sum = 0.7534768652937198
running average episode reward sum: -0.07847558825824695
{'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.93602648,  3.72116193,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.9766761237534893}
episode index:55
target Thresh 7.769980212390813
target distance 4.0
model initialize at round 55
at step 0:
{'currentTarget': array([ 6., 19.]), 'previousTarget': array([ 6., 19.]), 'currentState': array([ 5.1838374 , 15.13609788,  0.        ]), 'targetState': array([ 6, 19], dtype=int32), 'currentDistance': 3.9491595306052223}
done in step count: 99
reward sum = -0.005393521978395399
running average episode reward sum: -0.07717055136039246
{'currentTarget': array([ 6., 19.]), 'previousTarget': array([ 6., 19.]), 'currentState': array([ 5.70289154, 17.59846788,  0.        ]), 'targetState': array([ 6, 19], dtype=int32), 'currentDistance': 1.4326778141059424}
episode index:56
target Thresh 7.794198121205852
target distance 6.0
model initialize at round 56
at step 0:
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([ 8.8790184 , 12.14507602,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 7.193365875179196}
done in step count: 99
reward sum = -1.7980432398454658
running average episode reward sum: -0.10736130028118322
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([13.21568263, 14.459989  ,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 12.086837034399386}
episode index:57
target Thresh 7.818391824217002
target distance 5.0
model initialize at round 57
at step 0:
{'currentTarget': array([13., 24.]), 'previousTarget': array([13., 24.]), 'currentState': array([15.99966931, 27.00807297,  0.        ]), 'targetState': array([13, 24], dtype=int32), 'currentDistance': 4.248119465905545}
done in step count: 30
reward sum = 0.6622905093812548
running average episode reward sum: -0.09409144149389981
{'currentTarget': array([13., 24.]), 'previousTarget': array([13., 24.]), 'currentState': array([13.87672789, 23.34760532,  0.        ]), 'targetState': array([13, 24], dtype=int32), 'currentDistance': 1.0928268851905831}
episode index:58
target Thresh 7.8425613456179555
target distance 6.0
model initialize at round 58
at step 0:
{'currentTarget': array([20., 12.]), 'previousTarget': array([20., 12.]), 'currentState': array([18.15363753,  5.60234547,  0.        ]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 6.658756483384756}
done in step count: 99
reward sum = -0.027895438973476728
running average episode reward sum: -0.09296947534948587
{'currentTarget': array([20., 12.]), 'previousTarget': array([20., 12.]), 'currentState': array([20.9136949 ,  9.94048186,  0.        ]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 2.2530986130398327}
episode index:59
target Thresh 7.866706709578246
target distance 5.0
model initialize at round 59
at step 0:
{'currentTarget': array([ 5., 25.]), 'previousTarget': array([ 5., 25.]), 'currentState': array([ 5.64519447, 19.8520823 ,  0.        ]), 'targetState': array([ 5, 25], dtype=int32), 'currentDistance': 5.1881916483489166}
done in step count: 99
reward sum = -0.0076950526012650134
running average episode reward sum: -0.09154823497034885
{'currentTarget': array([ 5., 25.]), 'previousTarget': array([ 5., 25.]), 'currentState': array([ 6.29085682, 22.46064701,  0.        ]), 'targetState': array([ 5, 25], dtype=int32), 'currentDistance': 2.8486180740581535}
episode index:60
target Thresh 7.890827940243231
target distance 7.0
model initialize at round 60
at step 0:
{'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([8.99988389, 9.01084626, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 7.078659416047287}
done in step count: 99
reward sum = -0.0944289074043875
running average episode reward sum: -0.09159545910861178
{'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([6.74508624, 0.96586315, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 4.091635966738978}
episode index:61
target Thresh 7.914925061734152
target distance 4.0
model initialize at round 61
at step 0:
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([3.98213541, 6.17265843, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 4.310151304923539}
done in step count: 99
reward sum = -0.003962295515031893
running average episode reward sum: -0.09018202098613468
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([4.34504112, 6.96209173, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 3.8377212686854247}
episode index:62
target Thresh 7.938998098148122
target distance 7.0
model initialize at round 62
at step 0:
{'currentTarget': array([26., 18.]), 'previousTarget': array([26., 18.]), 'currentState': array([28.9998939 , 23.02308333,  0.        ]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 5.8507033394067305}
done in step count: 99
reward sum = -0.09193524799210474
running average episode reward sum: -0.09020984998622944
{'currentTarget': array([26., 18.]), 'previousTarget': array([26., 18.]), 'currentState': array([28.86289939, 14.58023363,  0.        ]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 4.459932165269758}
episode index:63
target Thresh 7.963047073558187
target distance 4.0
model initialize at round 63
at step 0:
{'currentTarget': array([17.,  8.]), 'previousTarget': array([17.,  8.]), 'currentState': array([20.73165616,  5.09967005,  0.        ]), 'targetState': array([17,  8], dtype=int32), 'currentDistance': 4.726221697085709}
done in step count: 99
reward sum = -0.003026316517437927
running average episode reward sum: -0.08884760727577957
{'currentTarget': array([17.,  8.]), 'previousTarget': array([17.,  8.]), 'currentState': array([20.87685092,  4.28618692,  0.        ]), 'targetState': array([17,  8], dtype=int32), 'currentDistance': 5.36864794052544}
episode index:64
target Thresh 7.987072012013325
target distance 6.0
model initialize at round 64
at step 0:
{'currentTarget': array([15., 13.]), 'previousTarget': array([15., 13.]), 'currentState': array([10.97271156,  8.27514613,  0.        ]), 'targetState': array([15, 13], dtype=int32), 'currentDistance': 6.2083247561237185}
done in step count: 99
reward sum = -0.14711414741238632
running average episode reward sum: -0.08974401558557353
{'currentTarget': array([15., 13.]), 'previousTarget': array([15., 13.]), 'currentState': array([18.52467881,  9.81924115,  0.        ]), 'targetState': array([15, 13], dtype=int32), 'currentDistance': 4.747692868644481}
episode index:65
target Thresh 8.011072937538472
target distance 7.0
model initialize at round 65
at step 0:
{'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([22.80879532,  6.33966011,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 7.199562702269131}
done in step count: 99
reward sum = -0.015514344954822441
running average episode reward sum: -0.08861932360631973
{'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([19.47752134,  0.93583109,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 4.634898681099966}
episode index:66
target Thresh 8.035049874134558
target distance 6.0
model initialize at round 66
at step 0:
{'currentTarget': array([11.,  5.]), 'previousTarget': array([11.,  5.]), 'currentState': array([16.91812706,  9.27318454,  0.        ]), 'targetState': array([11,  5], dtype=int32), 'currentDistance': 7.2996119069711165}
done in step count: 99
reward sum = -0.05122985626997561
running average episode reward sum: -0.08806127185503101
{'currentTarget': array([11.,  5.]), 'previousTarget': array([11.,  5.]), 'currentState': array([14.40503384,  2.3275453 ,  0.        ]), 'targetState': array([11,  5], dtype=int32), 'currentDistance': 4.32854127365136}
episode index:67
target Thresh 8.059002845778519
target distance 6.0
model initialize at round 67
at step 0:
{'currentTarget': array([15., 16.]), 'previousTarget': array([15., 16.]), 'currentState': array([10.97638214, 17.36498988,  0.        ]), 'targetState': array([15, 16], dtype=int32), 'currentDistance': 4.2488466760223895}
done in step count: 99
reward sum = -0.11361678791664
running average episode reward sum: -0.08843708826770173
{'currentTarget': array([15., 16.]), 'previousTarget': array([15., 16.]), 'currentState': array([18.32379032, 13.57622712,  0.        ]), 'targetState': array([15, 16], dtype=int32), 'currentDistance': 4.113667109670346}
episode index:68
target Thresh 8.08293187642333
target distance 7.0
model initialize at round 68
at step 0:
{'currentTarget': array([19., 19.]), 'previousTarget': array([19., 19.]), 'currentState': array([25.79370955, 23.26849097,  0.        ]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 8.023372393325966}
done in step count: 99
reward sum = -0.021965060734592475
running average episode reward sum: -0.08747372554983057
{'currentTarget': array([19., 19.]), 'previousTarget': array([19., 19.]), 'currentState': array([22.71444869, 16.57815701,  0.        ]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 4.434236407586256}
episode index:69
target Thresh 8.10683698999803
target distance 8.0
model initialize at round 69
at step 0:
{'currentTarget': array([14., 20.]), 'previousTarget': array([14., 20.]), 'currentState': array([ 9.88014996, 10.91920853,  0.        ]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 9.971656739528678}
done in step count: 99
reward sum = -0.10466775581343243
running average episode reward sum: -0.08771935455359632
{'currentTarget': array([14., 20.]), 'previousTarget': array([14., 20.]), 'currentState': array([17.93538585, 17.4988424 ,  0.        ]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 4.662944469397174}
episode index:70
target Thresh 8.130718210407721
target distance 6.0
model initialize at round 70
at step 0:
{'currentTarget': array([27., 19.]), 'previousTarget': array([27., 19.]), 'currentState': array([25.98672044, 23.32705271,  0.        ]), 'targetState': array([27, 19], dtype=int32), 'currentDistance': 4.444110781397583}
done in step count: 6
reward sum = 0.8733735856534182
running average episode reward sum: -0.07418283426899049
{'currentTarget': array([27., 19.]), 'previousTarget': array([27., 19.]), 'currentState': array([26.60334425, 19.97992519,  0.        ]), 'targetState': array([27, 19], dtype=int32), 'currentDistance': 1.0571608939289525}
episode index:71
target Thresh 8.15457556153364
target distance 8.0
model initialize at round 71
at step 0:
{'currentTarget': array([27., 18.]), 'previousTarget': array([27., 18.]), 'currentState': array([20.73886192, 16.91945481,  0.        ]), 'targetState': array([27, 18], dtype=int32), 'currentDistance': 6.353694041845554}
done in step count: 99
reward sum = -0.11714876561188799
running average episode reward sum: -0.07477958331541962
{'currentTarget': array([27., 18.]), 'previousTarget': array([27., 18.]), 'currentState': array([29.80715758, 13.79163775,  0.        ]), 'targetState': array([27, 18], dtype=int32), 'currentDistance': 5.058700085220262}
episode index:72
target Thresh 8.178409067233133
target distance 2.0
model initialize at round 72
at step 0:
{'currentTarget': array([18., 24.]), 'previousTarget': array([18., 24.]), 'currentState': array([18.0729605 , 21.37709945,  0.        ]), 'targetState': array([18, 24], dtype=int32), 'currentDistance': 2.6239151103005183}
done in step count: 99
reward sum = -0.02088964914320039
running average episode reward sum: -0.07404136503908784
{'currentTarget': array([18., 24.]), 'previousTarget': array([18., 24.]), 'currentState': array([20.58648088, 19.29685407,  0.        ]), 'targetState': array([18, 24], dtype=int32), 'currentDistance': 5.367444920549556}
episode index:73
target Thresh 8.202218751339704
target distance 8.0
model initialize at round 73
at step 0:
{'currentTarget': array([26., 13.]), 'previousTarget': array([26., 13.]), 'currentState': array([24.84847355,  4.31074196,  0.        ]), 'targetState': array([26, 13], dtype=int32), 'currentDistance': 8.765227802852158}
done in step count: 99
reward sum = -0.0607189579082876
running average episode reward sum: -0.07386133251029324
{'currentTarget': array([26., 13.]), 'previousTarget': array([26., 13.]), 'currentState': array([28.66426549,  7.65139418,  0.        ]), 'targetState': array([26, 13], dtype=int32), 'currentDistance': 5.975440970509638}
episode index:74
target Thresh 8.226004637663046
target distance 2.0
model initialize at round 74
at step 0:
{'currentTarget': array([24., 13.]), 'previousTarget': array([24., 13.]), 'currentState': array([23.52578187, 11.0843854 ,  0.        ]), 'targetState': array([24, 13], dtype=int32), 'currentDistance': 1.973439167956313}
done in step count: 99
reward sum = -0.024980988083044386
running average episode reward sum: -0.07320959458459658
{'currentTarget': array([24., 13.]), 'previousTarget': array([24., 13.]), 'currentState': array([26.19196861,  7.74404764,  0.        ]), 'targetState': array([24, 13], dtype=int32), 'currentDistance': 5.694713478448045}
episode index:75
target Thresh 8.249766749989046
target distance 5.0
model initialize at round 75
at step 0:
{'currentTarget': array([ 3., 23.]), 'previousTarget': array([ 3., 23.]), 'currentState': array([ 7.81797452, 26.32801002,  0.        ]), 'targetState': array([ 3, 23], dtype=int32), 'currentDistance': 5.855640802873275}
done in step count: 11
reward sum = 0.8813779878125856
running average episode reward sum: -0.06064923165831787
{'currentTarget': array([ 3., 23.]), 'previousTarget': array([ 3., 23.]), 'currentState': array([ 3.98230724, 23.02745767,  0.        ]), 'targetState': array([ 3, 23], dtype=int32), 'currentDistance': 0.9826909168629605}
episode index:76
target Thresh 8.273505112079814
target distance 3.0
model initialize at round 76
at step 0:
{'currentTarget': array([16., 13.]), 'previousTarget': array([16., 13.]), 'currentState': array([15.88211811, 14.46770036,  0.        ]), 'targetState': array([16, 13], dtype=int32), 'currentDistance': 1.4724267364380152}
done in step count: 2
reward sum = 0.9487053085471472
running average episode reward sum: -0.047540731136168975
{'currentTarget': array([16., 13.]), 'previousTarget': array([16., 13.]), 'currentState': array([15.74662382, 13.60254371,  0.        ]), 'targetState': array([16, 13], dtype=int32), 'currentDistance': 0.6536500710120466}
episode index:77
target Thresh 8.297219747673719
target distance 8.0
model initialize at round 77
at step 0:
{'currentTarget': array([26., 10.]), 'previousTarget': array([26., 10.]), 'currentState': array([24.98819256, 16.11211336,  0.        ]), 'targetState': array([26, 10], dtype=int32), 'currentDistance': 6.195295310387541}
done in step count: 9
reward sum = 0.8402286769298903
running average episode reward sum: -0.03615907205839898
{'currentTarget': array([26., 10.]), 'previousTarget': array([26., 10.]), 'currentState': array([25.54837719, 10.54995862,  0.        ]), 'targetState': array([26, 10], dtype=int32), 'currentDistance': 0.7116302733360405}
episode index:78
target Thresh 8.320910680485394
target distance 8.0
model initialize at round 78
at step 0:
{'currentTarget': array([6., 3.]), 'previousTarget': array([6., 3.]), 'currentState': array([13.55375299,  4.69318712,  0.        ]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 7.741192855597939}
done in step count: 99
reward sum = -0.01912266162375661
running average episode reward sum: -0.035943421293403506
{'currentTarget': array([6., 3.]), 'previousTarget': array([6., 3.]), 'currentState': array([ 7.545056  , -2.01280253,  0.        ]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 5.245511151920079}
episode index:79
target Thresh 8.344577934205777
target distance 8.0
model initialize at round 79
at step 0:
{'currentTarget': array([25., 15.]), 'previousTarget': array([25., 15.]), 'currentState': array([18.96865225, 18.56895554,  0.        ]), 'targetState': array([25, 15], dtype=int32), 'currentDistance': 7.008180887514864}
done in step count: 3
reward sum = 0.8612695027146141
running average episode reward sum: -0.02472825974330329
{'currentTarget': array([25., 15.]), 'previousTarget': array([25., 15.]), 'currentState': array([24.4322567 , 14.35455632,  0.        ]), 'targetState': array([25, 15], dtype=int32), 'currentDistance': 0.8596103754718709}
episode index:80
target Thresh 8.368221532502123
target distance 6.0
model initialize at round 80
at step 0:
{'currentTarget': array([21., 23.]), 'previousTarget': array([21., 23.]), 'currentState': array([16.9767617 , 26.33254313,  0.        ]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 5.224202348055485}
done in step count: 2
reward sum = 0.8954218903399389
running average episode reward sum: -0.013368381347213878
{'currentTarget': array([21., 23.]), 'previousTarget': array([21., 23.]), 'currentState': array([20.45467412, 23.47775698,  0.        ]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 0.7250048547440361}
episode index:81
target Thresh 8.391841499018032
target distance 7.0
model initialize at round 81
at step 0:
{'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([13.92607069,  7.42008609,  0.        ]), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 7.655613568439951}
done in step count: 99
reward sum = -0.048211498525425896
running average episode reward sum: -0.013793297410362804
{'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([16.32647625, 10.32701353,  0.        ]), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 4.85760659169851}
episode index:82
target Thresh 8.415437857373472
target distance 4.0
model initialize at round 82
at step 0:
{'currentTarget': array([20., 27.]), 'previousTarget': array([20., 27.]), 'currentState': array([23.77979948, 24.90012656,  0.        ]), 'targetState': array([20, 27], dtype=int32), 'currentDistance': 4.323927906244948}
done in step count: 99
reward sum = -0.00550059966783545
running average episode reward sum: -0.013693385389368498
{'currentTarget': array([20., 27.]), 'previousTarget': array([20., 27.]), 'currentState': array([20.65593493, 23.11417376,  0.        ]), 'targetState': array([20, 27], dtype=int32), 'currentDistance': 3.940798927998136}
episode index:83
target Thresh 8.439010631164805
target distance 8.0
model initialize at round 83
at step 0:
{'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([4.97397554, 6.51511884, 0.        ]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 6.529838709194549}
done in step count: 99
reward sum = -0.1021399100005557
running average episode reward sum: -0.014746320206168348
{'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([10.71016483,  1.29371161,  0.        ]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 2.721764372854228}
episode index:84
target Thresh 8.462559843964808
target distance 6.0
model initialize at round 84
at step 0:
{'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([17.99408517,  9.01079929,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 5.393003355083774}
done in step count: 8
reward sum = 0.9022536784532557
running average episode reward sum: -0.0039580849278221825
{'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.66686103,  4.74305025,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.9984123907690126}
episode index:85
target Thresh 8.486085519322689
target distance 7.0
model initialize at round 85
at step 0:
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([11.30449596, 11.57999086,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 8.62159947742148}
done in step count: 99
reward sum = -0.05934191169599983
running average episode reward sum: -0.004602082913498667
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([2.58247022, 4.84683019, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 2.5778927220441386}
episode index:86
target Thresh 8.509587680764135
target distance 7.0
model initialize at round 86
at step 0:
{'currentTarget': array([ 3., 24.]), 'previousTarget': array([ 3., 24.]), 'currentState': array([ 9.6992816 , 18.36411062,  0.        ]), 'targetState': array([ 3, 24], dtype=int32), 'currentDistance': 8.754634379252618}
done in step count: 24
reward sum = 0.770034286802315
running average episode reward sum: 0.004301783405073904
{'currentTarget': array([ 3., 24.]), 'previousTarget': array([ 3., 24.]), 'currentState': array([ 2.94251757, 23.04633132,  0.        ]), 'targetState': array([ 3, 24], dtype=int32), 'currentDistance': 0.9553994838875275}
episode index:87
target Thresh 8.533066351791302
target distance 2.0
model initialize at round 87
at step 0:
{'currentTarget': array([ 5., 21.]), 'previousTarget': array([ 5., 21.]), 'currentState': array([ 4.61379009, 19.18006481,  0.        ]), 'targetState': array([ 5, 21], dtype=int32), 'currentDistance': 1.8604628935438612}
done in step count: 99
reward sum = -0.002151783724913779
running average episode reward sum: 0.004228447414960408
{'currentTarget': array([ 5., 21.]), 'previousTarget': array([ 5., 21.]), 'currentState': array([ 4.027713  , 17.86578596,  0.        ]), 'targetState': array([ 5, 21], dtype=int32), 'currentDistance': 3.281560550231581}
episode index:88
target Thresh 8.556521555882863
target distance 6.0
model initialize at round 88
at step 0:
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([13.42771652, 12.90333819,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 5.537090185976538}
done in step count: 99
reward sum = -0.025098704545128827
running average episode reward sum: 0.0038989288536110905
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.01525346,  5.17458593,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 2.992104684121781}
episode index:89
target Thresh 8.579953316494027
target distance 5.0
model initialize at round 89
at step 0:
{'currentTarget': array([14., 26.]), 'previousTarget': array([14., 26.]), 'currentState': array([12.75567192, 20.90803671,  0.        ]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 5.241797643297827}
done in step count: 99
reward sum = -0.007068480350632727
running average episode reward sum: 0.0037770687513417147
{'currentTarget': array([14., 26.]), 'previousTarget': array([14., 26.]), 'currentState': array([12.35920578, 23.73529271,  0.        ]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 2.796623820744698}
episode index:90
target Thresh 8.603361657056556
target distance 2.0
model initialize at round 90
at step 0:
{'currentTarget': array([20.,  4.]), 'previousTarget': array([20.,  4.]), 'currentState': array([21.59240782,  3.9200402 ,  0.        ]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 1.5944140752935927}
done in step count: 2
reward sum = 0.9777370863680123
running average episode reward sum: 0.014479926087788645
{'currentTarget': array([20.,  4.]), 'previousTarget': array([20.,  4.]), 'currentState': array([20.81334639,  3.89042819,  0.        ]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 0.8206938065430595}
episode index:91
target Thresh 8.626746600978795
target distance 8.0
model initialize at round 91
at step 0:
{'currentTarget': array([13., 29.]), 'previousTarget': array([13., 29.]), 'currentState': array([ 7.04198277, 22.05289698,  0.        ]), 'targetState': array([13, 29], dtype=int32), 'currentDistance': 9.152060409159574}
done in step count: 99
reward sum = -0.04173241348385023
running average episode reward sum: 0.013868922396792572
{'currentTarget': array([13., 29.]), 'previousTarget': array([13., 29.]), 'currentState': array([11.22581083, 27.59307499,  0.        ]), 'targetState': array([13, 29], dtype=int32), 'currentDistance': 2.264328866982613}
episode index:92
target Thresh 8.650108171645684
target distance 8.0
model initialize at round 92
at step 0:
{'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 5.6760602 , 16.24900734,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 6.667142490128824}
done in step count: 99
reward sum = -0.0488052132128278
running average episode reward sum: 0.013195006960129986
{'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([8.94280556, 6.86494946, 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 3.2737477302652813}
episode index:93
target Thresh 8.6734463924188
target distance 8.0
model initialize at round 93
at step 0:
{'currentTarget': array([17., 19.]), 'previousTarget': array([17., 19.]), 'currentState': array([13.47474015, 11.45784816,  0.        ]), 'targetState': array([17, 19], dtype=int32), 'currentDistance': 8.32535352898702}
done in step count: 99
reward sum = -0.06043314358616824
running average episode reward sum: 0.012411728762828941
{'currentTarget': array([17., 19.]), 'previousTarget': array([17., 19.]), 'currentState': array([17.72876879, 16.30845944,  0.        ]), 'targetState': array([17, 19], dtype=int32), 'currentDistance': 2.7884573801052066}
episode index:94
target Thresh 8.696761286636363
target distance 4.0
model initialize at round 94
at step 0:
{'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([11.94843286, 16.93527157,  0.        ]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 3.6844491431607445}
done in step count: 99
reward sum = -0.02212431300113632
running average episode reward sum: 0.01204819148110299
{'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([15.51877568, 16.68127366,  0.        ]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 2.376051360421583}
episode index:95
target Thresh 8.720052877613274
target distance 8.0
model initialize at round 95
at step 0:
{'currentTarget': array([24., 25.]), 'previousTarget': array([24., 25.]), 'currentState': array([16.37051415, 23.96226782,  0.        ]), 'targetState': array([24, 25], dtype=int32), 'currentDistance': 7.69973651193711}
done in step count: 99
reward sum = -0.05471689179847139
running average episode reward sum: 0.011352721863607423
{'currentTarget': array([24., 25.]), 'previousTarget': array([24., 25.]), 'currentState': array([24.26618942, 22.95083145,  0.        ]), 'targetState': array([24, 25], dtype=int32), 'currentDistance': 2.066385379350652}
episode index:96
target Thresh 8.743321188641119
target distance 6.0
model initialize at round 96
at step 0:
{'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([24.70185876,  5.21784794,  0.        ]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 6.047539711682982}
done in step count: 99
reward sum = -0.04185850630313765
running average episode reward sum: 0.01080415250106366
{'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([20.47433868,  8.29528594,  0.        ]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 1.783919799458119}
episode index:97
target Thresh 8.766566242988219
target distance 7.0
model initialize at round 97
at step 0:
{'currentTarget': array([27.,  2.]), 'previousTarget': array([27.,  2.]), 'currentState': array([21.5583005 ,  7.57856321,  0.        ]), 'targetState': array([27,  2], dtype=int32), 'currentDistance': 7.793103427161297}
done in step count: 6
reward sum = 0.8761077491601348
running average episode reward sum: 0.01963378103840112
{'currentTarget': array([27.,  2.]), 'previousTarget': array([27.,  2.]), 'currentState': array([26.34762871,  2.85550416,  0.        ]), 'targetState': array([27,  2], dtype=int32), 'currentDistance': 1.0758604259140794}
episode index:98
target Thresh 8.789788063899621
target distance 6.0
model initialize at round 98
at step 0:
{'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([15.50528094,  5.70800826,  0.        ]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 5.654852863227265}
done in step count: 8
reward sum = 0.9059286135810933
running average episode reward sum: 0.02858625409438791
{'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([10.65213001,  6.33412634,  0.        ]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 0.9320199999262894}
episode index:99
target Thresh 8.812986674597155
target distance 7.0
model initialize at round 99
at step 0:
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([20.00350648,  5.52168965,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 6.494929733974554}
done in step count: 7
reward sum = 0.9017513371983317
running average episode reward sum: 0.037317904925427345
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.611803  ,  7.53063363,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.7711080960284488}
episode index:100
target Thresh 8.836162098279434
target distance 4.0
model initialize at round 100
at step 0:
{'currentTarget': array([5., 3.]), 'previousTarget': array([5., 3.]), 'currentState': array([6.03753254, 6.2827608 , 0.        ]), 'targetState': array([5, 3], dtype=int32), 'currentDistance': 3.4428174855054428}
done in step count: 7
reward sum = 0.9204028400241541
running average episode reward sum: 0.04606132012442463
{'currentTarget': array([5., 3.]), 'previousTarget': array([5., 3.]), 'currentState': array([5.96853095, 2.86387148, 0.        ]), 'targetState': array([5, 3], dtype=int32), 'currentDistance': 0.9780507002021532}
episode index:101
target Thresh 8.859314358121878
target distance 6.0
model initialize at round 101
at step 0:
{'currentTarget': array([7., 7.]), 'previousTarget': array([7., 7.]), 'currentState': array([13.35104841, 10.83125317,  0.        ]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 7.41716366041852}
done in step count: 15
reward sum = 0.8039529996968602
running average episode reward sum: 0.05349163070846812
{'currentTarget': array([7., 7.]), 'previousTarget': array([7., 7.]), 'currentState': array([7.74168802, 6.74602034, 0.        ]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 0.7839686097470872}
episode index:102
target Thresh 8.882443477276752
target distance 5.0
model initialize at round 102
at step 0:
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([2.483385  , 9.34908539, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 4.605937993810668}
done in step count: 7
reward sum = 0.9181443806393015
running average episode reward sum: 0.06188631760100048
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.98551853, 5.7460078 , 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.7461483436868914}
episode index:103
target Thresh 8.905549478873176
target distance 5.0
model initialize at round 103
at step 0:
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([6.11097479, 7.44752446, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 4.3943479631256235}
done in step count: 5
reward sum = 0.9345439071673988
running average episode reward sum: 0.07027725596221585
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.97995943, 8.7306539 , 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.0163010402310653}
episode index:104
target Thresh 8.928632386017156
target distance 7.0
model initialize at round 104
at step 0:
{'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([15.66524279, 17.47263682,  0.        ]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 7.224150840799978}
done in step count: 99
reward sum = -0.02826064380120499
running average episode reward sum: 0.06933879977399279
{'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([21.62366236, 11.78238798,  0.        ]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 2.2493183598818276}
episode index:105
target Thresh 8.9516922217916
target distance 8.0
model initialize at round 105
at step 0:
{'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([14.31130803, 14.81428933,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 9.604958244315757}
done in step count: 17
reward sum = 0.7838925210583786
running average episode reward sum: 0.07607987261629832
{'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([6.77097165, 9.78655806, 0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.7999717183343344}
episode index:106
target Thresh 8.974729009256343
target distance 6.0
model initialize at round 106
at step 0:
{'currentTarget': array([14., 26.]), 'previousTarget': array([14., 26.]), 'currentState': array([17.00690341, 20.89038229,  0.        ]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 5.928714975466566}
done in step count: 99
reward sum = -0.027095129357587892
running average episode reward sum: 0.07511562026140219
{'currentTarget': array([14., 26.]), 'previousTarget': array([14., 26.]), 'currentState': array([13.69513719, 23.94285753,  0.        ]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 2.0796096913193374}
episode index:107
target Thresh 8.997742771448177
target distance 7.0
model initialize at round 107
at step 0:
{'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([11.96516603, 13.27732772,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 6.598893281036655}
done in step count: 10
reward sum = 0.8781520605818292
running average episode reward sum: 0.08255114285696169
{'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.72075301,  7.78390688,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.0648919659704272}
episode index:108
target Thresh 9.020733531380866
target distance 8.0
model initialize at round 108
at step 0:
{'currentTarget': array([20.,  3.]), 'previousTarget': array([20.,  3.]), 'currentState': array([26.36141026, 10.06574273,  0.        ]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 9.507484464388886}
done in step count: 18
reward sum = 0.783892235117102
running average episode reward sum: 0.08898546480430243
{'currentTarget': array([20.,  3.]), 'previousTarget': array([20.,  3.]), 'currentState': array([20.88476166,  2.95304019,  0.        ]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 0.8860070078530229}
episode index:109
target Thresh 9.043701312045172
target distance 7.0
model initialize at round 109
at step 0:
{'currentTarget': array([ 5., 12.]), 'previousTarget': array([ 5., 12.]), 'currentState': array([8.10478061, 5.90765923, 0.        ]), 'targetState': array([ 5, 12], dtype=int32), 'currentDistance': 6.837856293410225}
done in step count: 99
reward sum = -0.02607403042443045
running average episode reward sum: 0.08793946939313213
{'currentTarget': array([ 5., 12.]), 'previousTarget': array([ 5., 12.]), 'currentState': array([5.02787154, 9.46695382, 0.        ]), 'targetState': array([ 5, 12], dtype=int32), 'currentDistance': 2.5331995090649895}
episode index:110
target Thresh 9.066646136408878
target distance 7.0
model initialize at round 110
at step 0:
{'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([21.37534732, 18.62998983,  0.        ]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 6.40639841035808}
done in step count: 11
reward sum = 0.8767419794467938
running average episode reward sum: 0.09504579831253449
{'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([15.85894023, 17.9851861 ,  0.        ]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 0.8590679649646974}
episode index:111
target Thresh 9.089568027416803
target distance 8.0
model initialize at round 111
at step 0:
{'currentTarget': array([14., 16.]), 'previousTarget': array([14., 16.]), 'currentState': array([12.59532452,  9.00775218,  0.        ]), 'targetState': array([14, 16], dtype=int32), 'currentDistance': 7.13194523180183}
done in step count: 99
reward sum = -0.026664357669566152
running average episode reward sum: 0.09395910049126574
{'currentTarget': array([14., 16.]), 'previousTarget': array([14., 16.]), 'currentState': array([15.6252714 , 12.69236558,  0.        ]), 'targetState': array([14, 16], dtype=int32), 'currentDistance': 3.6853700793797546}
episode index:112
target Thresh 9.112467007990855
target distance 3.0
model initialize at round 112
at step 0:
{'currentTarget': array([24., 22.]), 'previousTarget': array([24., 22.]), 'currentState': array([25.94597813, 24.62779769,  0.        ]), 'targetState': array([24, 22], dtype=int32), 'currentDistance': 3.2698855618823766}
done in step count: 8
reward sum = 0.9183473211859685
running average episode reward sum: 0.10125457147086488
{'currentTarget': array([24., 22.]), 'previousTarget': array([24., 22.]), 'currentState': array([24.24713083, 22.96822061,  0.        ]), 'targetState': array([24, 22], dtype=int32), 'currentDistance': 0.9992621235925621}
episode index:113
target Thresh 9.135343101030003
target distance 6.0
model initialize at round 113
at step 0:
{'currentTarget': array([ 5., 26.]), 'previousTarget': array([ 5., 26.]), 'currentState': array([10.18530601, 27.23512873,  0.        ]), 'targetState': array([ 5, 26], dtype=int32), 'currentDistance': 5.330379107119398}
done in step count: 10
reward sum = 0.8905618020611548
running average episode reward sum: 0.10817831910762181
{'currentTarget': array([ 5., 26.]), 'previousTarget': array([ 5., 26.]), 'currentState': array([ 5.89791825, 26.81073988,  0.        ]), 'targetState': array([ 5, 26], dtype=int32), 'currentDistance': 1.209775328244298}
episode index:114
target Thresh 9.158196329410345
target distance 9.0
model initialize at round 114
at step 0:
{'currentTarget': array([21.,  3.]), 'previousTarget': array([21.,  3.]), 'currentState': array([15.80873096, 10.3043642 ,  0.        ]), 'targetState': array([21,  3], dtype=int32), 'currentDistance': 8.961194712614981}
done in step count: 7
reward sum = 0.8445977626617538
running average episode reward sum: 0.11458196644287515
{'currentTarget': array([21.,  3.]), 'previousTarget': array([21.,  3.]), 'currentState': array([21.63849446,  3.96237183,  0.        ]), 'targetState': array([21,  3], dtype=int32), 'currentDistance': 1.1549176202332834}
episode index:115
target Thresh 9.181026715985116
target distance 6.0
model initialize at round 115
at step 0:
{'currentTarget': array([10.,  8.]), 'previousTarget': array([10.,  8.]), 'currentState': array([13.26163006, 13.19936174,  0.        ]), 'targetState': array([10,  8], dtype=int32), 'currentDistance': 6.137718888687355}
done in step count: 16
reward sum = 0.8315419418478174
running average episode reward sum: 0.1207626558860212
{'currentTarget': array([10.,  8.]), 'previousTarget': array([10.,  8.]), 'currentState': array([10.3046866,  8.9371321,  0.       ]), 'targetState': array([10,  8], dtype=int32), 'currentDistance': 0.9854189417515564}
episode index:116
target Thresh 9.203834283584698
target distance 8.0
model initialize at round 116
at step 0:
{'currentTarget': array([13., 21.]), 'previousTarget': array([13., 21.]), 'currentState': array([19.60811019, 17.23295867,  0.        ]), 'targetState': array([13, 21], dtype=int32), 'currentDistance': 7.606426271534993}
done in step count: 6
reward sum = 0.8819181308613002
running average episode reward sum: 0.12726825823623725
{'currentTarget': array([13., 21.]), 'previousTarget': array([13., 21.]), 'currentState': array([13.81095707, 20.84443389,  0.        ]), 'targetState': array([13, 21], dtype=int32), 'currentDistance': 0.8257434172276569}
episode index:117
target Thresh 9.22661905501667
target distance 2.0
model initialize at round 117
at step 0:
{'currentTarget': array([22., 12.]), 'previousTarget': array([22., 12.]), 'currentState': array([23.3054024 , 11.32453373,  0.        ]), 'targetState': array([22, 12], dtype=int32), 'currentDistance': 1.4698061443233996}
done in step count: 1
reward sum = 0.9859433483812541
running average episode reward sum: 0.13454516577983908
{'currentTarget': array([22., 12.]), 'previousTarget': array([22., 12.]), 'currentState': array([22.85281074, 11.46931331,  0.        ]), 'targetState': array([22, 12], dtype=int32), 'currentDistance': 1.0044473721878735}
episode index:118
target Thresh 9.249381053065793
target distance 6.0
model initialize at round 118
at step 0:
{'currentTarget': array([15., 28.]), 'previousTarget': array([15., 28.]), 'currentState': array([18.43052524, 22.54244995,  0.        ]), 'targetState': array([15, 28], dtype=int32), 'currentDistance': 6.44618926035087}
done in step count: 99
reward sum = -0.011842143792146606
running average episode reward sum: 0.13331502032125098
{'currentTarget': array([15., 28.]), 'previousTarget': array([15., 28.]), 'currentState': array([16.43897871, 25.35111297,  0.        ]), 'targetState': array([15, 28], dtype=int32), 'currentDistance': 3.0145086223802946}
episode index:119
target Thresh 9.272120300494077
target distance 9.0
model initialize at round 119
at step 0:
{'currentTarget': array([13., 27.]), 'previousTarget': array([13., 27.]), 'currentState': array([20.62285542, 20.13124108,  0.        ]), 'targetState': array([13, 27], dtype=int32), 'currentDistance': 10.260983085772555}
done in step count: 99
reward sum = -0.0621629403975724
running average episode reward sum: 0.13168603731526077
{'currentTarget': array([13., 27.]), 'previousTarget': array([13., 27.]), 'currentState': array([14.91551132, 23.83537107,  0.        ]), 'targetState': array([13, 27], dtype=int32), 'currentDistance': 3.6991971930328957}
episode index:120
target Thresh 9.294836820040768
target distance 5.0
model initialize at round 120
at step 0:
{'currentTarget': array([ 4., 16.]), 'previousTarget': array([ 4., 16.]), 'currentState': array([ 8.37068224, 19.15300126,  0.        ]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 5.389274553044845}
done in step count: 14
reward sum = 0.8586244065929368
running average episode reward sum: 0.1376937924332581
{'currentTarget': array([ 4., 16.]), 'previousTarget': array([ 4., 16.]), 'currentState': array([ 4.70206916, 16.7834189 ,  0.        ]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 1.0519725713082575}
episode index:121
target Thresh 9.317530634422383
target distance 9.0
model initialize at round 121
at step 0:
{'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([22.25719133, 11.19026154,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 10.942906841533071}
done in step count: 99
reward sum = -0.031705055270305725
running average episode reward sum: 0.13630527728814693
{'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([17.09580714, -1.28303894,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 4.768315233795886}
episode index:122
target Thresh 9.340201766332743
target distance 5.0
model initialize at round 122
at step 0:
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([5.20031197, 6.33812398, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 4.864225778373055}
done in step count: 12
reward sum = 0.8760414383032912
running average episode reward sum: 0.14231939241835134
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.39720149, 2.71405402, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.8170937322836391}
episode index:123
target Thresh 9.362850238442984
target distance 8.0
model initialize at round 123
at step 0:
{'currentTarget': array([18., 18.]), 'previousTarget': array([18., 18.]), 'currentState': array([20.9524399 ,  9.66621688,  0.        ]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 8.841314522049379}
done in step count: 99
reward sum = -0.01495438656254068
running average episode reward sum: 0.1410510554910861
{'currentTarget': array([18., 18.]), 'previousTarget': array([18., 18.]), 'currentState': array([19.98887425, 10.98816966,  0.        ]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 7.288441912367844}
episode index:124
target Thresh 9.385476073401577
target distance 5.0
model initialize at round 124
at step 0:
{'currentTarget': array([ 6., 19.]), 'previousTarget': array([ 6., 19.]), 'currentState': array([ 5.44568124, 23.19231695,  0.        ]), 'targetState': array([ 6, 19], dtype=int32), 'currentDistance': 4.228804876919637}
done in step count: 6
reward sum = 0.9257399151113839
running average episode reward sum: 0.14732856636804845
{'currentTarget': array([ 6., 19.]), 'previousTarget': array([ 6., 19.]), 'currentState': array([ 6.24916629, 19.74979618,  0.        ]), 'targetState': array([ 6, 19], dtype=int32), 'currentDistance': 0.7901127483414133}
episode index:125
target Thresh 9.408079293834355
target distance 5.0
model initialize at round 125
at step 0:
{'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([3.96239553, 5.06798148, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 6.390479517310048}
done in step count: 99
reward sum = -0.07228061272712256
running average episode reward sum: 0.14558563637522964
{'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([11.23411737,  0.67588722,  0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 8.618708372187752}
episode index:126
target Thresh 9.430659922344546
target distance 6.0
model initialize at round 126
at step 0:
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([20.06814975, 10.60164249,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 5.103735448427944}
done in step count: 12
reward sum = 0.8694424987360437
running average episode reward sum: 0.15128529670877933
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.94396716, 10.00984756,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.9440185275407332}
episode index:127
target Thresh 9.453217981512779
target distance 1.0
model initialize at round 127
at step 0:
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.9139449 , 7.80122876, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.21659982782985826}
done in step count: 0
reward sum = 0.9886382620388392
running average episode reward sum: 0.15782711675042044
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.9139449 , 7.80122876, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.21659982782985826}
episode index:128
target Thresh 9.475753493897116
target distance 6.0
model initialize at round 128
at step 0:
{'currentTarget': array([19., 20.]), 'previousTarget': array([19., 20.]), 'currentState': array([15.85547537, 25.35337502,  0.        ]), 'targetState': array([19, 20], dtype=int32), 'currentDistance': 6.208595594224962}
done in step count: 5
reward sum = 0.9146648833130445
running average episode reward sum: 0.16369407618113846
{'currentTarget': array([19., 20.]), 'previousTarget': array([19., 20.]), 'currentState': array([19.23013151, 20.93254989,  0.        ]), 'targetState': array([19, 20], dtype=int32), 'currentDistance': 0.9605258013234179}
episode index:129
target Thresh 9.49826648203307
target distance 4.0
model initialize at round 129
at step 0:
{'currentTarget': array([22.,  2.]), 'previousTarget': array([22.,  2.]), 'currentState': array([18.32415864,  2.62308583,  0.        ]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 3.72827650003095}
done in step count: 99
reward sum = -0.197809272941169
running average episode reward sum: 0.16091328118788994
{'currentTarget': array([22.,  2.]), 'previousTarget': array([22.,  2.]), 'currentState': array([20.60209649, -5.54209649,  0.        ]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 7.670551065133478}
episode index:130
target Thresh 9.52075696843363
target distance 3.0
model initialize at round 130
at step 0:
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([6.43783844, 5.38958383, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 2.513098514067512}
done in step count: 5
reward sum = 0.9451973477411633
running average episode reward sum: 0.16690018245928898
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.8142288 , 5.82685116, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.8324356218515644}
episode index:131
target Thresh 9.543224975589283
target distance 9.0
model initialize at round 131
at step 0:
{'currentTarget': array([18.,  5.]), 'previousTarget': array([18.,  5.]), 'currentState': array([23.19311979, 13.13571298,  0.        ]), 'targetState': array([18,  5], dtype=int32), 'currentDistance': 9.651855723592403}
done in step count: 22
reward sum = 0.7763803469265637
running average episode reward sum: 0.17151745643252592
{'currentTarget': array([18.,  5.]), 'previousTarget': array([18.,  5.]), 'currentState': array([18.37213228,  5.95697683,  0.        ]), 'targetState': array([18,  5], dtype=int32), 'currentDistance': 1.0267848311292247}
episode index:132
target Thresh 9.565670525968045
target distance 2.0
model initialize at round 132
at step 0:
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 3.56608659, 11.17751554,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.9593800159987453}
done in step count: 4
reward sum = 0.9577296740227179
running average episode reward sum: 0.17742882648959504
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.51000364, 10.90217894,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.0363544532834348}
episode index:133
target Thresh 9.588093642015458
target distance 9.0
model initialize at round 133
at step 0:
{'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([15.51322192,  7.5716722 ,  0.        ]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 8.827353968526943}
done in step count: 99
reward sum = -0.11754164836247187
running average episode reward sum: 0.17522755428920647
{'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([30.34549456,  7.10767998,  0.        ]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 6.973579881582094}
episode index:134
target Thresh 9.610494346154649
target distance 2.0
model initialize at round 134
at step 0:
{'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([27.26002836,  8.05011082,  0.        ]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 3.79866460074932}
done in step count: 99
reward sum = -0.030251274104801818
running average episode reward sum: 0.1737054888936953
{'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([26.3273579 ,  8.76746415,  0.        ]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 2.6335792069539314}
episode index:135
target Thresh 9.632872660786319
target distance 7.0
model initialize at round 135
at step 0:
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.36953118, 11.07938474,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 6.090605237895903}
done in step count: 99
reward sum = -0.028857593292121442
running average episode reward sum: 0.17221605446585841
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.20471541,  3.66373452,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.3518556983724968}
episode index:136
target Thresh 9.655228608288784
target distance 1.0
model initialize at round 136
at step 0:
{'currentTarget': array([26.,  3.]), 'previousTarget': array([26.,  3.]), 'currentState': array([26.60383621,  2.18857434,  0.        ]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 1.0114493374063078}
done in step count: 0
reward sum = 0.9990374698330357
running average episode reward sum: 0.1782512472787575
{'currentTarget': array([26.,  3.]), 'previousTarget': array([26.,  3.]), 'currentState': array([26.60383621,  2.18857434,  0.        ]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 1.0114493374063078}
episode index:137
target Thresh 9.677562211018
target distance 5.0
model initialize at round 137
at step 0:
{'currentTarget': array([27., 25.]), 'previousTarget': array([27., 25.]), 'currentState': array([25.8990696 , 21.80214834,  0.        ]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 3.3820560272995785}
done in step count: 99
reward sum = -0.028250758904085814
running average episode reward sum: 0.17675485592960646
{'currentTarget': array([27., 25.]), 'previousTarget': array([27., 25.]), 'currentState': array([27.01850498, 23.74195879,  0.        ]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 1.2581773015527769}
episode index:138
target Thresh 9.69987349130756
target distance 6.0
model initialize at round 138
at step 0:
{'currentTarget': array([20., 12.]), 'previousTarget': array([20., 12.]), 'currentState': array([15.64137495,  7.72232199,  0.        ]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 6.107056700743226}
done in step count: 9
reward sum = 0.8650029149320442
running average episode reward sum: 0.1817062808145161
{'currentTarget': array([20., 12.]), 'previousTarget': array([20., 12.]), 'currentState': array([19.33947226, 11.58481074,  0.        ]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 0.780178833402176}
episode index:139
target Thresh 9.72216247146876
target distance 7.0
model initialize at round 139
at step 0:
{'currentTarget': array([ 3., 19.]), 'previousTarget': array([ 3., 19.]), 'currentState': array([ 2.46955961, 13.95914507,  0.        ]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 5.068686757101316}
done in step count: 4
reward sum = 0.9120645317844783
running average episode reward sum: 0.1869231254643015
{'currentTarget': array([ 3., 19.]), 'previousTarget': array([ 3., 19.]), 'currentState': array([ 2.64375518, 18.00750089,  0.        ]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 1.054497446505031}
episode index:140
target Thresh 9.744429173790568
target distance 7.0
model initialize at round 140
at step 0:
{'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([12.5648914 , 21.23287112,  0.        ]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 7.11678673606628}
done in step count: 8
reward sum = 0.891487958602891
running average episode reward sum: 0.1919200391745043
{'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([16.17402592, 15.80274153,  0.        ]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 0.8213884476576883}
episode index:141
target Thresh 9.766673620539702
target distance 8.0
model initialize at round 141
at step 0:
{'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([6.47508568, 6.77914   , 0.        ]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 9.763372100993749}
done in step count: 17
reward sum = 0.7620297815989301
running average episode reward sum: 0.19593489651552137
{'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([13.24650797, 12.70350812,  0.        ]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 0.809726919314499}
episode index:142
target Thresh 9.788895833960598
target distance 8.0
model initialize at round 142
at step 0:
{'currentTarget': array([22.,  3.]), 'previousTarget': array([22.,  3.]), 'currentState': array([20.37519357, 10.29512888,  0.        ]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 7.473881277601619}
done in step count: 11
reward sum = 0.8706998292051413
running average episode reward sum: 0.20065353240845576
{'currentTarget': array([22.,  3.]), 'previousTarget': array([22.,  3.]), 'currentState': array([22.3459692 ,  3.95937306,  0.        ]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 1.0198486905707158}
episode index:143
target Thresh 9.81109583627548
target distance 6.0
model initialize at round 143
at step 0:
{'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([19.64639974,  5.55587935,  0.        ]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 6.36576804504272}
done in step count: 13
reward sum = 0.8281098017012457
running average episode reward sum: 0.20501086761187792
{'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([24.07585996,  8.68999781,  0.        ]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 0.9747492824302666}
episode index:144
target Thresh 9.833273649684351
target distance 8.0
model initialize at round 144
at step 0:
{'currentTarget': array([18., 27.]), 'previousTarget': array([18., 27.]), 'currentState': array([10.49096206, 25.3003068 ,  0.        ]), 'targetState': array([18, 27], dtype=int32), 'currentDistance': 7.699000441228284}
done in step count: 29
reward sum = 0.7118627090975456
running average episode reward sum: 0.2085063975531584
{'currentTarget': array([18., 27.]), 'previousTarget': array([18., 27.]), 'currentState': array([17.09129504, 26.2924521 ,  0.        ]), 'targetState': array([18, 27], dtype=int32), 'currentDistance': 1.151680829588291}
episode index:145
target Thresh 9.85542929636502
target distance 1.0
model initialize at round 145
at step 0:
{'currentTarget': array([ 3., 26.]), 'previousTarget': array([ 3., 26.]), 'currentState': array([ 3.45762229, 27.17401174,  0.        ]), 'targetState': array([ 3, 26], dtype=int32), 'currentDistance': 1.260048300208858}
done in step count: 4
reward sum = 0.9561342283366765
running average episode reward sum: 0.2136271361201688
{'currentTarget': array([ 3., 26.]), 'previousTarget': array([ 3., 26.]), 'currentState': array([ 2.15837564, 26.78640812,  0.        ]), 'targetState': array([ 3, 26], dtype=int32), 'currentDistance': 1.1518547242805894}
episode index:146
target Thresh 9.877562798473143
target distance 7.0
model initialize at round 146
at step 0:
{'currentTarget': array([13.,  2.]), 'previousTarget': array([13.,  2.]), 'currentState': array([17.35012883,  8.73621783,  0.        ]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 8.01874375700667}
done in step count: 17
reward sum = 0.8227944266619042
running average episode reward sum: 0.2177711312939221
{'currentTarget': array([13.,  2.]), 'previousTarget': array([13.,  2.]), 'currentState': array([12.36901746,  2.93139371,  0.        ]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 1.1250036509470756}
episode index:147
target Thresh 9.89967417814222
target distance 7.0
model initialize at round 147
at step 0:
{'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([14.13031289, 15.36480725,  0.        ]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 7.62072881673973}
done in step count: 12
reward sum = 0.8514969918342207
running average episode reward sum: 0.22205306278405923
{'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([ 8.61163555, 20.25507675,  0.        ]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 0.840081898645167}
episode index:148
target Thresh 9.921763457483635
target distance 3.0
model initialize at round 148
at step 0:
{'currentTarget': array([ 5., 24.]), 'previousTarget': array([ 5., 24.]), 'currentState': array([ 5.61198497, 26.86281447,  0.        ]), 'targetState': array([ 5, 24], dtype=int32), 'currentDistance': 2.927495909027213}
done in step count: 11
reward sum = 0.8880345276382394
running average episode reward sum: 0.22652273704482553
{'currentTarget': array([ 5., 24.]), 'previousTarget': array([ 5., 24.]), 'currentState': array([ 4.07494594, 24.20494352,  0.        ]), 'targetState': array([ 5, 24], dtype=int32), 'currentDistance': 0.9474844953363444}
episode index:149
target Thresh 9.943830658586666
target distance 5.0
model initialize at round 149
at step 0:
{'currentTarget': array([ 6., 22.]), 'previousTarget': array([ 6., 22.]), 'currentState': array([10.21401411, 23.27349129,  0.        ]), 'targetState': array([ 6, 22], dtype=int32), 'currentDistance': 4.402237501313576}
done in step count: 10
reward sum = 0.8915129131051268
running average episode reward sum: 0.23095600488522752
{'currentTarget': array([ 6., 22.]), 'previousTarget': array([ 6., 22.]), 'currentState': array([ 6.00023755, 22.95354189,  0.        ]), 'targetState': array([ 6, 22], dtype=int32), 'currentDistance': 0.9535419175144827}
episode index:150
target Thresh 9.965875803518518
target distance 7.0
model initialize at round 150
at step 0:
{'currentTarget': array([21., 28.]), 'previousTarget': array([21., 28.]), 'currentState': array([17.93945196, 22.78029943,  0.        ]), 'targetState': array([21, 28], dtype=int32), 'currentDistance': 6.05080394492427}
done in step count: 11
reward sum = 0.8416383508057442
running average episode reward sum: 0.23500025883172102
{'currentTarget': array([21., 28.]), 'previousTarget': array([21., 28.]), 'currentState': array([20.04751219, 27.82468855,  0.        ]), 'targetState': array([21, 28], dtype=int32), 'currentDistance': 0.9684870348804687}
episode index:151
target Thresh 9.987898914324337
target distance 5.0
model initialize at round 151
at step 0:
{'currentTarget': array([23., 28.]), 'previousTarget': array([23., 28.]), 'currentState': array([20.32498959, 24.52158868,  0.        ]), 'targetState': array([23, 28], dtype=int32), 'currentDistance': 4.3880549220164955}
done in step count: 5
reward sum = 0.9187602334327312
running average episode reward sum: 0.23949867971725397
{'currentTarget': array([23., 28.]), 'previousTarget': array([23., 28.]), 'currentState': array([22.38262707, 27.27600441,  0.        ]), 'targetState': array([23, 28], dtype=int32), 'currentDistance': 0.9514824997053445}
episode index:152
target Thresh 10.009900013027238
target distance 9.0
model initialize at round 152
at step 0:
{'currentTarget': array([14., 27.]), 'previousTarget': array([14., 27.]), 'currentState': array([21.93472767, 27.49971882,  0.        ]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 7.950447916411475}
done in step count: 15
reward sum = 0.8252003870781149
running average episode reward sum: 0.24332679545163866
{'currentTarget': array([14., 27.]), 'previousTarget': array([14., 27.]), 'currentState': array([13.85939546, 27.84988429,  0.        ]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 0.8614365544518201}
episode index:153
target Thresh 10.031879121628318
target distance 9.0
model initialize at round 153
at step 0:
{'currentTarget': array([17.,  3.]), 'previousTarget': array([17.,  3.]), 'currentState': array([11.87306148, 10.78704822,  0.        ]), 'targetState': array([17,  3], dtype=int32), 'currentDistance': 9.323283681055722}
done in step count: 12
reward sum = 0.834206413456487
running average episode reward sum: 0.24716367608803377
{'currentTarget': array([17.,  3.]), 'previousTarget': array([17.,  3.]), 'currentState': array([16.44521444,  3.81769213,  0.        ]), 'targetState': array([17,  3], dtype=int32), 'currentDistance': 0.988133311520318}
episode index:154
target Thresh 10.053836262106685
target distance 9.0
model initialize at round 154
at step 0:
{'currentTarget': array([11., 25.]), 'previousTarget': array([11., 25.]), 'currentState': array([19.14420134, 22.12939525,  0.        ]), 'targetState': array([11, 25], dtype=int32), 'currentDistance': 8.635298899829975}
done in step count: 15
reward sum = 0.8024800769561717
running average episode reward sum: 0.25074636254524757
{'currentTarget': array([11., 25.]), 'previousTarget': array([11., 25.]), 'currentState': array([10.43317294, 25.81436395,  0.        ]), 'targetState': array([11, 25], dtype=int32), 'currentDistance': 0.9922104370848102}
episode index:155
target Thresh 10.075771456419492
target distance 7.0
model initialize at round 155
at step 0:
{'currentTarget': array([9., 5.]), 'previousTarget': array([9., 5.]), 'currentState': array([15.09038872,  2.91767174,  0.        ]), 'targetState': array([9, 5], dtype=int32), 'currentDistance': 6.436530562750299}
done in step count: 10
reward sum = 0.8643601855332353
running average episode reward sum: 0.25467978448747824
{'currentTarget': array([9., 5.]), 'previousTarget': array([9., 5.]), 'currentState': array([8.7024069 , 5.99198745, 0.        ]), 'targetState': array([9, 5], dtype=int32), 'currentDistance': 1.035664404632611}
episode index:156
target Thresh 10.097684726501925
target distance 9.0
model initialize at round 156
at step 0:
{'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([17.03561366, 12.81835666,  0.        ]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 11.930402384868488}
done in step count: 25
reward sum = 0.739274790085642
running average episode reward sum: 0.2577663768798233
{'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([8.61070427, 4.67725544, 0.        ]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 0.7811696942199765}
episode index:157
target Thresh 10.11957609426726
target distance 9.0
model initialize at round 157
at step 0:
{'currentTarget': array([26., 18.]), 'previousTarget': array([26., 18.]), 'currentState': array([22.70516869, 10.79455113,  0.        ]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 7.923030145863479}
done in step count: 15
reward sum = 0.7976675095167941
running average episode reward sum: 0.2611834726560066
{'currentTarget': array([26., 18.]), 'previousTarget': array([26., 18.]), 'currentState': array([25.73315136, 17.00330182,  0.        ]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 1.0318020461719875}
episode index:158
target Thresh 10.141445581606867
target distance 1.0
model initialize at round 158
at step 0:
{'currentTarget': array([ 3., 19.]), 'previousTarget': array([ 3., 19.]), 'currentState': array([ 2.68927255, 18.19103439,  0.        ]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 0.8665892349821869}
done in step count: 0
reward sum = 0.9993347715418629
running average episode reward sum: 0.2658259336552887
{'currentTarget': array([ 3., 19.]), 'previousTarget': array([ 3., 19.]), 'currentState': array([ 2.68927255, 18.19103439,  0.        ]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 0.8665892349821869}
episode index:159
target Thresh 10.163293210390233
target distance 2.0
model initialize at round 159
at step 0:
{'currentTarget': array([27., 28.]), 'previousTarget': array([27., 28.]), 'currentState': array([26.90212493, 26.67645574,  0.        ]), 'targetState': array([27, 28], dtype=int32), 'currentDistance': 1.327158222807494}
done in step count: 1
reward sum = 0.9867700917758486
running average episode reward sum: 0.2703318346435422
{'currentTarget': array([27., 28.]), 'previousTarget': array([27., 28.]), 'currentState': array([26.68139872, 27.03963038,  0.        ]), 'targetState': array([27, 28], dtype=int32), 'currentDistance': 1.0118382166508577}
episode index:160
target Thresh 10.185119002464987
target distance 8.0
model initialize at round 160
at step 0:
{'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([20.54945308, 14.39031798,  0.        ]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 7.786011816199311}
done in step count: 15
reward sum = 0.8371722517324097
running average episode reward sum: 0.2738525825757712
{'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([22.70680628,  7.77465902,  0.        ]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 0.8282868813504131}
episode index:161
target Thresh 10.20692297965693
target distance 9.0
model initialize at round 161
at step 0:
{'currentTarget': array([ 3., 29.]), 'previousTarget': array([ 3., 29.]), 'currentState': array([11.64419267, 22.55063605,  0.        ]), 'targetState': array([ 3, 29], dtype=int32), 'currentDistance': 10.785006358992064}
done in step count: 14
reward sum = 0.792229059712665
running average episode reward sum: 0.2770524373729125
{'currentTarget': array([ 3., 29.]), 'previousTarget': array([ 3., 29.]), 'currentState': array([ 2.64709085, 29.82802691,  0.        ]), 'targetState': array([ 3, 29], dtype=int32), 'currentDistance': 0.9000963409815417}
episode index:162
target Thresh 10.228705163770034
target distance 8.0
model initialize at round 162
at step 0:
{'currentTarget': array([17., 15.]), 'previousTarget': array([17., 15.]), 'currentState': array([ 9.67660928, 13.16310835,  0.        ]), 'targetState': array([17, 15], dtype=int32), 'currentDistance': 7.550246526384323}
done in step count: 13
reward sum = 0.8365107163980434
running average episode reward sum: 0.28048469675343485
{'currentTarget': array([17., 15.]), 'previousTarget': array([17., 15.]), 'currentState': array([16.22888333, 14.46680967,  0.        ]), 'targetState': array([17, 15], dtype=int32), 'currentDistance': 0.9375035228482024}
episode index:163
target Thresh 10.250465576586489
target distance 7.0
model initialize at round 163
at step 0:
{'currentTarget': array([7., 7.]), 'previousTarget': array([7., 7.]), 'currentState': array([12.91690779,  5.91187787,  0.        ]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 6.016128949105127}
done in step count: 7
reward sum = 0.8952821492585996
running average episode reward sum: 0.2842334617077346
{'currentTarget': array([7., 7.]), 'previousTarget': array([7., 7.]), 'currentState': array([7.58547744, 7.94097742, 0.        ]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 1.1082519333446093}
episode index:164
target Thresh 10.272204239866706
target distance 8.0
model initialize at round 164
at step 0:
{'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([13.86143303, 14.88590565,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 8.423261671905452}
done in step count: 20
reward sum = 0.7873629468891956
running average episode reward sum: 0.28728273131489496
{'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 6.60208255, 10.99177855,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 1.06862667900479}
episode index:165
target Thresh 10.293921175349354
target distance 6.0
model initialize at round 165
at step 0:
{'currentTarget': array([ 5., 14.]), 'previousTarget': array([ 5., 14.]), 'currentState': array([10.18369609, 11.86082989,  0.        ]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 5.6077405366285165}
done in step count: 7
reward sum = 0.9033839580219244
running average episode reward sum: 0.2909941844878289
{'currentTarget': array([ 5., 14.]), 'previousTarget': array([ 5., 14.]), 'currentState': array([ 5.55096915, 14.7075927 ,  0.        ]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 0.8968023369890687}
episode index:166
target Thresh 10.315616404751367
target distance 6.0
model initialize at round 166
at step 0:
{'currentTarget': array([20.,  7.]), 'previousTarget': array([20.,  7.]), 'currentState': array([24.97574747, 11.11067389,  0.        ]), 'targetState': array([20,  7], dtype=int32), 'currentDistance': 6.454122920446852}
done in step count: 17
reward sum = 0.819920554728016
running average episode reward sum: 0.2941614082617222
{'currentTarget': array([20.,  7.]), 'previousTarget': array([20.,  7.]), 'currentState': array([19.72113968,  7.8506351 ,  0.        ]), 'targetState': array([20,  7], dtype=int32), 'currentDistance': 0.8951777225884188}
episode index:167
target Thresh 10.337289949767978
target distance 9.0
model initialize at round 167
at step 0:
{'currentTarget': array([13., 14.]), 'previousTarget': array([13., 14.]), 'currentState': array([16.63183326, 22.11163151,  0.        ]), 'targetState': array([13, 14], dtype=int32), 'currentDistance': 8.887563141329657}
done in step count: 20
reward sum = 0.7911156250885255
running average episode reward sum: 0.2971194690761675
{'currentTarget': array([13., 14.]), 'previousTarget': array([13., 14.]), 'currentState': array([12.14049859, 14.99376731,  0.        ]), 'targetState': array([13, 14], dtype=int32), 'currentDistance': 1.31389349901459}
episode index:168
target Thresh 10.358941832072734
target distance 2.0
model initialize at round 168
at step 0:
{'currentTarget': array([12., 19.]), 'previousTarget': array([12., 19.]), 'currentState': array([10.13139808, 18.80457187,  0.        ]), 'targetState': array([12, 19], dtype=int32), 'currentDistance': 1.8787935714041297}
done in step count: 5
reward sum = 0.946270684660849
running average episode reward sum: 0.30096060052933127
{'currentTarget': array([12., 19.]), 'previousTarget': array([12., 19.]), 'currentState': array([11.02985951, 18.87473467,  0.        ]), 'targetState': array([12, 19], dtype=int32), 'currentDistance': 0.9781942375705353}
episode index:169
target Thresh 10.38057207331752
target distance 7.0
model initialize at round 169
at step 0:
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([9.15509486, 5.80189586, 0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 7.320880392344548}
done in step count: 6
reward sum = 0.8838298984222456
running average episode reward sum: 0.30438924345811313
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.72945905, 10.3886877 ,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.9517422076401982}
episode index:170
target Thresh 10.402180695132575
target distance 8.0
model initialize at round 170
at step 0:
{'currentTarget': array([14., 27.]), 'previousTarget': array([14., 27.]), 'currentState': array([15.20314115, 20.91996932,  0.        ]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 6.1979288233914955}
done in step count: 13
reward sum = 0.8234151906958741
running average episode reward sum: 0.3074244829156439
{'currentTarget': array([14., 27.]), 'previousTarget': array([14., 27.]), 'currentState': array([13.64701156, 26.02437207,  0.        ]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 1.0375214230178977}
episode index:171
target Thresh 10.42376771912653
target distance 6.0
model initialize at round 171
at step 0:
{'currentTarget': array([24., 27.]), 'previousTarget': array([24., 27.]), 'currentState': array([18.68806893, 26.93141538,  0.        ]), 'targetState': array([24, 27], dtype=int32), 'currentDistance': 5.312373817946694}
done in step count: 9
reward sum = 0.8856664660777788
running average episode reward sum: 0.3107863549107726
{'currentTarget': array([24., 27.]), 'previousTarget': array([24., 27.]), 'currentState': array([23.17700115, 26.66463554,  0.        ]), 'targetState': array([24, 27], dtype=int32), 'currentDistance': 0.8887049171326586}
episode index:172
target Thresh 10.4453331668864
target distance 4.0
model initialize at round 172
at step 0:
{'currentTarget': array([21., 17.]), 'previousTarget': array([21., 17.]), 'currentState': array([18.90647   , 20.21914715,  0.        ]), 'targetState': array([21, 17], dtype=int32), 'currentDistance': 3.8400229431296995}
done in step count: 5
reward sum = 0.9332871974728479
running average episode reward sum: 0.3143846256770274
{'currentTarget': array([21., 17.]), 'previousTarget': array([21., 17.]), 'currentState': array([20.56567659, 17.94088466,  0.        ]), 'targetState': array([21, 17], dtype=int32), 'currentDistance': 1.0362918389507614}
episode index:173
target Thresh 10.466877059977644
target distance 9.0
model initialize at round 173
at step 0:
{'currentTarget': array([15., 24.]), 'previousTarget': array([15., 24.]), 'currentState': array([10.41652605, 16.63874698,  0.        ]), 'targetState': array([15, 24], dtype=int32), 'currentDistance': 8.671578837031497}
done in step count: 9
reward sum = 0.8464877126688457
running average episode reward sum: 0.3174426893953712
{'currentTarget': array([15., 24.]), 'previousTarget': array([15., 24.]), 'currentState': array([14.15336655, 23.70319551,  0.        ]), 'targetState': array([15, 24], dtype=int32), 'currentDistance': 0.8971516601002806}
episode index:174
target Thresh 10.488399419944155
target distance 2.0
model initialize at round 174
at step 0:
{'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([15.28510106, 21.47062641,  0.        ]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 1.389863704875262}
done in step count: 1
reward sum = 0.9828659251495182
running average episode reward sum: 0.3212451078853949
{'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([14.58146644, 21.9246707 ,  0.        ]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 0.5863256097475796}
episode index:175
target Thresh 10.50990026830829
target distance 3.0
model initialize at round 175
at step 0:
{'currentTarget': array([23., 28.]), 'previousTarget': array([23., 28.]), 'currentState': array([23.68823949, 26.13217974,  0.        ]), 'targetState': array([23, 28], dtype=int32), 'currentDistance': 1.9905843672855663}
done in step count: 2
reward sum = 0.9680211935273265
running average episode reward sum: 0.3249199720083604
{'currentTarget': array([23., 28.]), 'previousTarget': array([23., 28.]), 'currentState': array([22.74561369, 27.21167618,  0.        ]), 'targetState': array([23, 28], dtype=int32), 'currentDistance': 0.8283518800551065}
episode index:176
target Thresh 10.531379626570903
target distance 10.0
model initialize at round 176
at step 0:
{'currentTarget': array([20., 10.]), 'previousTarget': array([20., 10.]), 'currentState': array([24.77829258, 19.15465158,  0.        ]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 10.326651228063515}
done in step count: 25
reward sum = 0.7459114347047109
running average episode reward sum: 0.32729845484845277
{'currentTarget': array([20., 10.]), 'previousTarget': array([20., 10.]), 'currentState': array([19.1569851 , 10.84050854,  0.        ]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 1.190432162768101}
episode index:177
target Thresh 10.552837516211355
target distance 8.0
model initialize at round 177
at step 0:
{'currentTarget': array([15., 24.]), 'previousTarget': array([15., 24.]), 'currentState': array([ 7.57821989, 24.01467764,  0.        ]), 'targetState': array([15, 24], dtype=int32), 'currentDistance': 7.421794622973206}
done in step count: 13
reward sum = 0.8413734151787708
running average episode reward sum: 0.3301865164233422
{'currentTarget': array([15., 24.]), 'previousTarget': array([15., 24.]), 'currentState': array([14.11253951, 23.90313848,  0.        ]), 'targetState': array([15, 24], dtype=int32), 'currentDistance': 0.8927307917073707}
episode index:178
target Thresh 10.574273958687535
target distance 8.0
model initialize at round 178
at step 0:
{'currentTarget': array([26., 12.]), 'previousTarget': array([26., 12.]), 'currentState': array([20.41923293,  5.36991131,  0.        ]), 'targetState': array([26, 12], dtype=int32), 'currentDistance': 8.666200841990685}
done in step count: 7
reward sum = 0.8753260985374797
running average episode reward sum: 0.3332319889491195
{'currentTarget': array([26., 12.]), 'previousTarget': array([26., 12.]), 'currentState': array([25.20348313, 11.28476303,  0.        ]), 'targetState': array([26, 12], dtype=int32), 'currentDistance': 1.070515316371942}
episode index:179
target Thresh 10.595688975435891
target distance 7.0
model initialize at round 179
at step 0:
{'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([9.76132128, 3.52632236, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 7.254745147237117}
done in step count: 7
reward sum = 0.8910357041586364
running average episode reward sum: 0.33633089847806125
{'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([5.83316496, 8.34765291, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 1.0581684997459662}
episode index:180
target Thresh 10.617082587871437
target distance 4.0
model initialize at round 180
at step 0:
{'currentTarget': array([21., 27.]), 'previousTarget': array([21., 27.]), 'currentState': array([17.57717317, 24.13466692,  0.        ]), 'targetState': array([21, 27], dtype=int32), 'currentDistance': 4.463841075320366}
done in step count: 5
reward sum = 0.9257575075089426
running average episode reward sum: 0.33958739908044183
{'currentTarget': array([21., 27.]), 'previousTarget': array([21., 27.]), 'currentState': array([20.34572494, 26.3737176 ,  0.        ]), 'targetState': array([21, 27], dtype=int32), 'currentDistance': 0.9057071809885054}
episode index:181
target Thresh 10.638454817387785
target distance 9.0
model initialize at round 181
at step 0:
{'currentTarget': array([20.,  9.]), 'previousTarget': array([20.,  9.]), 'currentState': array([11.51512295, 12.39104387,  0.        ]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 9.137413041539928}
done in step count: 17
reward sum = 0.8182889424131298
running average episode reward sum: 0.3422176273405116
{'currentTarget': array([20.,  9.]), 'previousTarget': array([20.,  9.]), 'currentState': array([19.10505164,  9.83626653,  0.        ]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 1.2248568373174586}
episode index:182
target Thresh 10.659805685357174
target distance 9.0
model initialize at round 182
at step 0:
{'currentTarget': array([26.,  8.]), 'previousTarget': array([26.,  8.]), 'currentState': array([21.81720835, 16.29584974,  0.        ]), 'targetState': array([26,  8], dtype=int32), 'currentDistance': 9.290687213915884}
done in step count: 50
reward sum = 0.5771603166363236
running average episode reward sum: 0.3435014671727291
{'currentTarget': array([26.,  8.]), 'previousTarget': array([26.,  8.]), 'currentState': array([25.07752946,  8.99489801,  0.        ]), 'targetState': array([26,  8], dtype=int32), 'currentDistance': 1.3567512484558824}
episode index:183
target Thresh 10.68113521313047
target distance 4.0
model initialize at round 183
at step 0:
{'currentTarget': array([22., 23.]), 'previousTarget': array([22., 23.]), 'currentState': array([18.64376271, 21.27274919,  0.        ]), 'targetState': array([22, 23], dtype=int32), 'currentDistance': 3.774615760330034}
done in step count: 7
reward sum = 0.9076137986927657
running average episode reward sum: 0.346567295061425
{'currentTarget': array([22., 23.]), 'previousTarget': array([22., 23.]), 'currentState': array([21.01246272, 23.32346421,  0.        ]), 'targetState': array([22, 23], dtype=int32), 'currentDistance': 1.0391626262146256}
episode index:184
target Thresh 10.702443422037202
target distance 2.0
model initialize at round 184
at step 0:
{'currentTarget': array([25., 27.]), 'previousTarget': array([25., 27.]), 'currentState': array([25.14211434, 25.93734372,  0.        ]), 'targetState': array([25, 27], dtype=int32), 'currentDistance': 1.0721170004228855}
done in step count: 1
reward sum = 0.9768141346106989
running average episode reward sum: 0.3499740347346643
{'currentTarget': array([25., 27.]), 'previousTarget': array([25., 27.]), 'currentState': array([24.41312557, 26.64550793,  0.        ]), 'targetState': array([25, 27], dtype=int32), 'currentDistance': 0.6856283378644363}
episode index:185
target Thresh 10.723730333385582
target distance 2.0
model initialize at round 185
at step 0:
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([ 1.47433674, 10.23813516,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 2.2990369395484977}
done in step count: 35
reward sum = 0.7008512545962496
running average episode reward sum: 0.3518604714005868
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.13272167, 8.9393851 , 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.2785210445177877}
episode index:186
target Thresh 10.744995968462522
target distance 10.0
model initialize at round 186
at step 0:
{'currentTarget': array([ 7., 22.]), 'previousTarget': array([ 7., 22.]), 'currentState': array([ 3.02052605, 13.70132315,  0.        ]), 'targetState': array([ 7, 22], dtype=int32), 'currentDistance': 9.203491206355194}
done in step count: 15
reward sum = 0.7857235762639844
running average episode reward sum: 0.354180594956006
{'currentTarget': array([ 7., 22.]), 'previousTarget': array([ 7., 22.]), 'currentState': array([ 6.01567636, 22.64026528,  0.        ]), 'targetState': array([ 7, 22], dtype=int32), 'currentDistance': 1.1742370550004841}
episode index:187
target Thresh 10.766240348533657
target distance 5.0
model initialize at round 187
at step 0:
{'currentTarget': array([20., 29.]), 'previousTarget': array([20., 29.]), 'currentState': array([23.7484355 , 26.30333328,  0.        ]), 'targetState': array([20, 29], dtype=int32), 'currentDistance': 4.617659587142315}
done in step count: 3
reward sum = 0.9227113951196124
running average episode reward sum: 0.35720469495687623
{'currentTarget': array([20., 29.]), 'previousTarget': array([20., 29.]), 'currentState': array([20.4220891 , 29.10763627,  0.        ]), 'targetState': array([20, 29], dtype=int32), 'currentDistance': 0.4355970333766068}
episode index:188
target Thresh 10.787463494843376
target distance 10.0
model initialize at round 188
at step 0:
{'currentTarget': array([24., 22.]), 'previousTarget': array([24., 22.]), 'currentState': array([14.14104199, 15.44577026,  0.        ]), 'targetState': array([24, 22], dtype=int32), 'currentDistance': 11.838791340283429}
done in step count: 18
reward sum = 0.7431090773855653
running average episode reward sum: 0.3592465170861286
{'currentTarget': array([24., 22.]), 'previousTarget': array([24., 22.]), 'currentState': array([23.15903331, 22.25418965,  0.        ]), 'targetState': array([24, 22], dtype=int32), 'currentDistance': 0.8785427394255463}
episode index:189
target Thresh 10.808665428614816
target distance 10.0
model initialize at round 189
at step 0:
{'currentTarget': array([22., 28.]), 'previousTarget': array([22., 28.]), 'currentState': array([12.05305493, 21.40183485,  0.        ]), 'targetState': array([22, 28], dtype=int32), 'currentDistance': 11.936393916484034}
done in step count: 19
reward sum = 0.7272682585445721
running average episode reward sum: 0.3611834736201204
{'currentTarget': array([22., 28.]), 'previousTarget': array([22., 28.]), 'currentState': array([21.02264453, 28.60496999,  0.        ]), 'targetState': array([22, 28], dtype=int32), 'currentDistance': 1.14944003606826}
episode index:190
target Thresh 10.829846171049923
target distance 10.0
model initialize at round 190
at step 0:
{'currentTarget': array([22., 21.]), 'previousTarget': array([22., 21.]), 'currentState': array([12.42386952, 22.05121279,  0.        ]), 'targetState': array([22, 21], dtype=int32), 'currentDistance': 9.633655759196666}
done in step count: 18
reward sum = 0.7939720069998628
running average episode reward sum: 0.3634493821718468
{'currentTarget': array([22., 21.]), 'previousTarget': array([22., 21.]), 'currentState': array([21.19643262, 21.64776213,  0.        ]), 'targetState': array([22, 21], dtype=int32), 'currentDistance': 1.032141616665889}
episode index:191
target Thresh 10.851005743329438
target distance 9.0
model initialize at round 191
at step 0:
{'currentTarget': array([19., 27.]), 'previousTarget': array([19., 27.]), 'currentState': array([12.22905168, 19.48138726,  0.        ]), 'targetState': array([19, 27], dtype=int32), 'currentDistance': 10.118066946366126}
done in step count: 10
reward sum = 0.8389120662781513
running average episode reward sum: 0.3659257503182338
{'currentTarget': array([19., 27.]), 'previousTarget': array([19., 27.]), 'currentState': array([18.00977084, 26.75332685,  0.        ]), 'targetState': array([19, 27], dtype=int32), 'currentDistance': 1.020490778790216}
episode index:192
target Thresh 10.87214416661293
target distance 8.0
model initialize at round 192
at step 0:
{'currentTarget': array([26.,  2.]), 'previousTarget': array([26.,  2.]), 'currentState': array([18.95000976,  8.77623415,  0.        ]), 'targetState': array([26,  2], dtype=int32), 'currentDistance': 9.77853320253677}
done in step count: 12
reward sum = 0.8325737517887025
running average episode reward sum: 0.3683436156108269
{'currentTarget': array([26.,  2.]), 'previousTarget': array([26.,  2.]), 'currentState': array([25.36181469,  2.92875005,  0.        ]), 'targetState': array([26,  2], dtype=int32), 'currentDistance': 1.126879388923827}
episode index:193
target Thresh 10.89326146203883
target distance 7.0
model initialize at round 193
at step 0:
{'currentTarget': array([27., 23.]), 'previousTarget': array([27., 23.]), 'currentState': array([23.3935006 , 17.41032326,  0.        ]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 6.652166863656915}
done in step count: 6
reward sum = 0.8977454367332524
running average episode reward sum: 0.37107249097743733
{'currentTarget': array([27., 23.]), 'previousTarget': array([27., 23.]), 'currentState': array([26.01121452, 22.53647915,  0.        ]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 1.092038597897243}
episode index:194
target Thresh 10.91435765072443
target distance 1.0
model initialize at round 194
at step 0:
{'currentTarget': array([13., 28.]), 'previousTarget': array([13., 28.]), 'currentState': array([13.18683857, 29.43373883,  0.        ]), 'targetState': array([13, 28], dtype=int32), 'currentDistance': 1.4458615688525376}
done in step count: 10
reward sum = 0.898608586340444
running average episode reward sum: 0.3737778042869912
{'currentTarget': array([13., 28.]), 'previousTarget': array([13., 28.]), 'currentState': array([12.4619417 , 28.96288559,  0.        ]), 'targetState': array([13, 28], dtype=int32), 'currentDistance': 1.103021030900911}
episode index:195
target Thresh 10.935432753765927
target distance 9.0
model initialize at round 195
at step 0:
{'currentTarget': array([ 6., 26.]), 'previousTarget': array([ 6., 26.]), 'currentState': array([ 3.71686155, 18.69091833,  0.        ]), 'targetState': array([ 6, 26], dtype=int32), 'currentDistance': 7.657375275617099}
done in step count: 7
reward sum = 0.8781445860053038
running average episode reward sum: 0.3763511041937173
{'currentTarget': array([ 6., 26.]), 'previousTarget': array([ 6., 26.]), 'currentState': array([ 5.92611031, 25.02547631,  0.        ]), 'targetState': array([ 6, 26], dtype=int32), 'currentDistance': 0.977320886436436}
episode index:196
target Thresh 10.956486792238422
target distance 9.0
model initialize at round 196
at step 0:
{'currentTarget': array([23., 13.]), 'previousTarget': array([23., 13.]), 'currentState': array([20.83565922,  5.63308418,  0.        ]), 'targetState': array([23, 13], dtype=int32), 'currentDistance': 7.6782693191439755}
done in step count: 7
reward sum = 0.8804484383583092
running average episode reward sum: 0.37890997391028886
{'currentTarget': array([23., 13.]), 'previousTarget': array([23., 13.]), 'currentState': array([22.62164968, 12.21445221,  0.        ]), 'targetState': array([23, 13], dtype=int32), 'currentDistance': 0.8719141572910457}
episode index:197
target Thresh 10.977519787195956
target distance 2.0
model initialize at round 197
at step 0:
{'currentTarget': array([21.,  9.]), 'previousTarget': array([21.,  9.]), 'currentState': array([21.89165163,  8.87136149,  0.        ]), 'targetState': array([21,  9], dtype=int32), 'currentDistance': 0.9008831751161178}
done in step count: 0
reward sum = 0.9900614652882905
running average episode reward sum: 0.38199659760411714
{'currentTarget': array([21.,  9.]), 'previousTarget': array([21.,  9.]), 'currentState': array([21.89165163,  8.87136149,  0.        ]), 'targetState': array([21,  9], dtype=int32), 'currentDistance': 0.9008831751161178}
episode index:198
target Thresh 10.998531759671522
target distance 10.0
model initialize at round 198
at step 0:
{'currentTarget': array([18., 18.]), 'previousTarget': array([18., 18.]), 'currentState': array([10.83743724,  9.26751614,  0.        ]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 11.294183445052104}
done in step count: 16
reward sum = 0.7822743270058669
running average episode reward sum: 0.3840080434805078
{'currentTarget': array([18., 18.]), 'previousTarget': array([18., 18.]), 'currentState': array([17.23015538, 18.28195877,  0.        ]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 0.8198545514161137}
episode index:199
target Thresh 11.019522730677103
target distance 8.0
model initialize at round 199
at step 0:
{'currentTarget': array([ 5., 20.]), 'previousTarget': array([ 5., 20.]), 'currentState': array([ 2.95543539, 13.67217851,  0.        ]), 'targetState': array([ 5, 20], dtype=int32), 'currentDistance': 6.649930022388004}
done in step count: 6
reward sum = 0.8938769458552308
running average episode reward sum: 0.38655738799238143
{'currentTarget': array([ 5., 20.]), 'previousTarget': array([ 5., 20.]), 'currentState': array([ 4.66359711, 19.26637343,  0.        ]), 'targetState': array([ 5, 20], dtype=int32), 'currentDistance': 0.8070779745169451}
episode index:200
target Thresh 11.040492721203663
target distance 10.0
model initialize at round 200
at step 0:
{'currentTarget': array([23., 24.]), 'previousTarget': array([23., 24.]), 'currentState': array([13.55787683, 27.80151224,  0.        ]), 'targetState': array([23, 24], dtype=int32), 'currentDistance': 10.178663240663163}
done in step count: 31
reward sum = 0.7069587148325266
running average episode reward sum: 0.38815142444432243
{'currentTarget': array([23., 24.]), 'previousTarget': array([23., 24.]), 'currentState': array([22.19561151, 24.92628779,  0.        ]), 'targetState': array([23, 24], dtype=int32), 'currentDistance': 1.2268047583107278}
episode index:201
target Thresh 11.0614417522212
target distance 5.0
model initialize at round 201
at step 0:
{'currentTarget': array([23., 14.]), 'previousTarget': array([23., 14.]), 'currentState': array([23.91985773, 10.43861735,  0.        ]), 'targetState': array([23, 14], dtype=int32), 'currentDistance': 3.6782583702152967}
done in step count: 3
reward sum = 0.9420620864083054
running average episode reward sum: 0.3908935564342431
{'currentTarget': array([23., 14.]), 'previousTarget': array([23., 14.]), 'currentState': array([22.57038026, 13.30604553,  0.        ]), 'targetState': array([23, 14], dtype=int32), 'currentDistance': 0.8161776287895833}
episode index:202
target Thresh 11.082369844678741
target distance 2.0
model initialize at round 202
at step 0:
{'currentTarget': array([10., 18.]), 'previousTarget': array([10., 18.]), 'currentState': array([ 9.32598346, 16.53294933,  0.        ]), 'targetState': array([10, 18], dtype=int32), 'currentDistance': 1.6144769935830914}
done in step count: 1
reward sum = 0.9869072372016886
running average episode reward sum: 0.3938295844183192
{'currentTarget': array([10., 18.]), 'previousTarget': array([10., 18.]), 'currentState': array([ 9.39595816, 17.00798771,  0.        ]), 'targetState': array([10, 18], dtype=int32), 'currentDistance': 1.1614451897546503}
episode index:203
target Thresh 11.103277019504386
target distance 8.0
model initialize at round 203
at step 0:
{'currentTarget': array([8., 6.]), 'previousTarget': array([8., 6.]), 'currentState': array([14.58397293,  4.89542365,  0.        ]), 'targetState': array([8, 6], dtype=int32), 'currentDistance': 6.675985954619223}
done in step count: 99
reward sum = -0.05801685733999454
running average episode reward sum: 0.3916146508802883
{'currentTarget': array([8., 6.]), 'previousTarget': array([8., 6.]), 'currentState': array([6.91450991, 9.02387956, 0.        ]), 'targetState': array([8, 6], dtype=int32), 'currentDistance': 3.21280817121663}
episode index:204
target Thresh 11.124163297605307
target distance 6.0
model initialize at round 204
at step 0:
{'currentTarget': array([ 7., 19.]), 'previousTarget': array([ 7., 19.]), 'currentState': array([ 2.87782264, 24.71134374,  0.        ]), 'targetState': array([ 7, 19], dtype=int32), 'currentDistance': 7.043563975145547}
done in step count: 99
reward sum = -0.015537759413364152
running average episode reward sum: 0.38962854156178267
{'currentTarget': array([ 7., 19.]), 'previousTarget': array([ 7., 19.]), 'currentState': array([ 5.14703563, 24.41273076,  0.        ]), 'targetState': array([ 7, 19], dtype=int32), 'currentDistance': 5.72111276071804}
episode index:205
target Thresh 11.145028699867787
target distance 4.0
model initialize at round 205
at step 0:
{'currentTarget': array([17., 25.]), 'previousTarget': array([17., 25.]), 'currentState': array([19.18522161, 29.06819743,  0.        ]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 4.6179458440220165}
done in step count: 99
reward sum = -0.018150784637472567
running average episode reward sum: 0.3876490302695533
{'currentTarget': array([17., 25.]), 'previousTarget': array([17., 25.]), 'currentState': array([15.13734673, 31.58334097,  0.        ]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 6.841772838776257}
episode index:206
target Thresh 11.16587324715723
target distance 8.0
model initialize at round 206
at step 0:
{'currentTarget': array([21., 19.]), 'previousTarget': array([21., 19.]), 'currentState': array([13.49831203, 18.68553531,  0.        ]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 7.5082761334083195}
done in step count: 99
reward sum = -0.02435204786896734
running average episode reward sum: 0.3856586868969034
{'currentTarget': array([21., 19.]), 'previousTarget': array([21., 19.]), 'currentState': array([19.33930452, 25.227005  ,  0.        ]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 6.444649001353673}
episode index:207
target Thresh 11.18669696031818
target distance 11.0
model initialize at round 207
at step 0:
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([5.23977254, 2.9238447 , 0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 11.506412863678118}
done in step count: 99
reward sum = -0.04313726266359824
running average episode reward sum: 0.3835971679086318
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([14.25041621,  9.9046776 ,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 3.390898903312003}
episode index:208
target Thresh 11.207499860174359
target distance 5.0
model initialize at round 208
at step 0:
{'currentTarget': array([ 5., 20.]), 'previousTarget': array([ 5., 20.]), 'currentState': array([ 8.81417537, 18.83676153,  0.        ]), 'targetState': array([ 5, 20], dtype=int32), 'currentDistance': 3.9876130029410466}
done in step count: 99
reward sum = -0.0410169623236999
running average episode reward sum: 0.3815655213524963
{'currentTarget': array([ 5., 20.]), 'previousTarget': array([ 5., 20.]), 'currentState': array([ 2.73789669, 21.12091613,  0.        ]), 'targetState': array([ 5, 20], dtype=int32), 'currentDistance': 2.524591917197176}
episode index:209
target Thresh 11.228281967528666
target distance 2.0
model initialize at round 209
at step 0:
{'currentTarget': array([11., 16.]), 'previousTarget': array([11., 16.]), 'currentState': array([ 8.87426153, 17.07640658,  0.        ]), 'targetState': array([11, 16], dtype=int32), 'currentDistance': 2.3827327132091733}
done in step count: 99
reward sum = -0.0008665316874421312
running average episode reward sum: 0.37974441633802036
{'currentTarget': array([11., 16.]), 'previousTarget': array([11., 16.]), 'currentState': array([ 9.15108172, 16.17065556,  0.        ]), 'targetState': array([11, 16], dtype=int32), 'currentDistance': 1.8567773457920815}
episode index:210
target Thresh 11.249043303163209
target distance 10.0
model initialize at round 210
at step 0:
{'currentTarget': array([27., 23.]), 'previousTarget': array([27., 23.]), 'currentState': array([25.65743327, 14.18876362,  0.        ]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 8.91293285018656}
done in step count: 84
reward sum = 0.3540205066038863
running average episode reward sum: 0.3796225020738776
{'currentTarget': array([27., 23.]), 'previousTarget': array([27., 23.]), 'currentState': array([26.01195762, 22.02653506,  0.        ]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 1.3870334258292454}
episode index:211
target Thresh 11.269783887839324
target distance 8.0
model initialize at round 211
at step 0:
{'currentTarget': array([16., 29.]), 'previousTarget': array([16., 29.]), 'currentState': array([16.62755799, 22.43880582,  0.        ]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 6.591137847222425}
done in step count: 99
reward sum = -0.05129446984272175
running average episode reward sum: 0.3775898748478559
{'currentTarget': array([16., 29.]), 'previousTarget': array([16., 29.]), 'currentState': array([16.02005154, 27.42421899,  0.        ]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 1.5759085773591177}
episode index:212
target Thresh 11.290503742297602
target distance 9.0
model initialize at round 212
at step 0:
{'currentTarget': array([10., 19.]), 'previousTarget': array([10., 19.]), 'currentState': array([ 8.72607338, 26.599617  ,  0.        ]), 'targetState': array([10, 19], dtype=int32), 'currentDistance': 7.705651668464627}
done in step count: 6
reward sum = 0.8876678271772447
running average episode reward sum: 0.3799846070184164
{'currentTarget': array([10., 19.]), 'previousTarget': array([10., 19.]), 'currentState': array([10.56702735, 19.86257017,  0.        ]), 'targetState': array([10, 19], dtype=int32), 'currentDistance': 1.032253508994056}
episode index:213
target Thresh 11.311202887257895
target distance 5.0
model initialize at round 213
at step 0:
{'currentTarget': array([22., 21.]), 'previousTarget': array([22., 21.]), 'currentState': array([18.39557135, 22.22049719,  0.        ]), 'targetState': array([22, 21], dtype=int32), 'currentDistance': 3.805459141612909}
done in step count: 99
reward sum = -0.039270982621243844
running average episode reward sum: 0.37802546874907217
{'currentTarget': array([22., 21.]), 'previousTarget': array([22., 21.]), 'currentState': array([21.3939249 , 19.35171341,  0.        ]), 'targetState': array([22, 21], dtype=int32), 'currentDistance': 1.7561821397565975}
episode index:214
target Thresh 11.331881343419354
target distance 5.0
model initialize at round 214
at step 0:
{'currentTarget': array([7., 8.]), 'previousTarget': array([7., 8.]), 'currentState': array([7.64473605, 4.02261066, 0.        ]), 'targetState': array([7, 8], dtype=int32), 'currentDistance': 4.029306454389169}
done in step count: 99
reward sum = -0.02028768819762462
running average episode reward sum: 0.37617284941443635
{'currentTarget': array([7., 8.]), 'previousTarget': array([7., 8.]), 'currentState': array([5.14274647, 5.38185264, 0.        ]), 'targetState': array([7, 8], dtype=int32), 'currentDistance': 3.2099978655935604}
episode index:215
target Thresh 11.352539131460432
target distance 6.0
model initialize at round 215
at step 0:
{'currentTarget': array([27., 10.]), 'previousTarget': array([27., 10.]), 'currentState': array([21.81027144,  6.09658003,  0.        ]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 6.493840926602727}
done in step count: 99
reward sum = -0.035005489422447904
running average episode reward sum: 0.3742692459938952
{'currentTarget': array([27., 10.]), 'previousTarget': array([27., 10.]), 'currentState': array([23.62942255,  8.45789591,  0.        ]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 3.7065991677075965}
episode index:216
target Thresh 11.373176272038918
target distance 4.0
model initialize at round 216
at step 0:
{'currentTarget': array([25., 26.]), 'previousTarget': array([25., 26.]), 'currentState': array([24.54818797, 22.61178023,  0.        ]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 3.4182111322748088}
done in step count: 99
reward sum = -0.030564433844062575
running average episode reward sum: 0.37240365299925027
{'currentTarget': array([25., 26.]), 'previousTarget': array([25., 26.]), 'currentState': array([21.25302368, 25.13012243,  0.        ]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 3.846624304900651}
episode index:217
target Thresh 11.393792785791959
target distance 8.0
model initialize at round 217
at step 0:
{'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([10.63842022,  7.71713114,  0.        ]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 7.367012392333345}
done in step count: 99
reward sum = -0.03163043271162151
running average episode reward sum: 0.3705502856336041
{'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([14.85190534,  8.06790939,  0.        ]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 3.14882703475259}
episode index:218
target Thresh 11.414388693336072
target distance 10.0
model initialize at round 218
at step 0:
{'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([ 7.18711933, 23.33550137,  0.        ]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 9.370824982756163}
done in step count: 99
reward sum = -0.03384803725315001
running average episode reward sum: 0.36870371794918966
{'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([ 5.18677151, 14.97437766,  0.        ]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 2.9771910177112533}
episode index:219
target Thresh 11.434964015267159
target distance 11.0
model initialize at round 219
at step 0:
{'currentTarget': array([ 7., 17.]), 'previousTarget': array([ 7., 17.]), 'currentState': array([16.62045619, 27.56116369,  0.        ]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 14.286054593459019}
done in step count: 99
reward sum = -0.05439990351308472
running average episode reward sum: 0.3667805196698157
{'currentTarget': array([ 7., 17.]), 'previousTarget': array([ 7., 17.]), 'currentState': array([ 4.54306711, 17.63940428,  0.        ]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 2.538770778043871}
episode index:220
target Thresh 11.455518772160548
target distance 8.0
model initialize at round 220
at step 0:
{'currentTarget': array([19., 17.]), 'previousTarget': array([19., 17.]), 'currentState': array([25.86780429, 13.40666178,  0.        ]), 'targetState': array([19, 17], dtype=int32), 'currentDistance': 7.751052530003852}
done in step count: 7
reward sum = 0.8904544514761094
running average episode reward sum: 0.36915008497210655
{'currentTarget': array([19., 17.]), 'previousTarget': array([19., 17.]), 'currentState': array([19.34739429, 16.01778358,  0.        ]), 'targetState': array([19, 17], dtype=int32), 'currentDistance': 1.0418406223710672}
episode index:221
target Thresh 11.476052984570998
target distance 7.0
model initialize at round 221
at step 0:
{'currentTarget': array([14., 17.]), 'previousTarget': array([14., 17.]), 'currentState': array([10.09537613, 22.96453023,  0.        ]), 'targetState': array([14, 17], dtype=int32), 'currentDistance': 7.128934590456505}
done in step count: 99
reward sum = -0.032863463893809536
running average episode reward sum: 0.3673392131303682
{'currentTarget': array([14., 17.]), 'previousTarget': array([14., 17.]), 'currentState': array([12.15655509, 17.23657666,  0.        ]), 'targetState': array([14, 17], dtype=int32), 'currentDistance': 1.8585633285993226}
episode index:222
target Thresh 11.496566673032724
target distance 11.0
model initialize at round 222
at step 0:
{'currentTarget': array([14., 25.]), 'previousTarget': array([14., 25.]), 'currentState': array([ 3.22022849, 20.82589638,  0.        ]), 'targetState': array([14, 25], dtype=int32), 'currentDistance': 11.559697865841994}
done in step count: 91
reward sum = 0.345887020823195
running average episode reward sum: 0.3672430149585872
{'currentTarget': array([14., 25.]), 'previousTarget': array([14., 25.]), 'currentState': array([13.00027456, 24.74875635,  0.        ]), 'targetState': array([14, 25], dtype=int32), 'currentDistance': 1.0308124601248998}
episode index:223
target Thresh 11.517059858059412
target distance 5.0
model initialize at round 223
at step 0:
{'currentTarget': array([10., 28.]), 'previousTarget': array([10., 28.]), 'currentState': array([11.74556422, 23.26200762,  0.        ]), 'targetState': array([10, 28], dtype=int32), 'currentDistance': 5.049313439486419}
done in step count: 14
reward sum = 0.8426952990917481
running average episode reward sum: 0.36936556979846735
{'currentTarget': array([10., 28.]), 'previousTarget': array([10., 28.]), 'currentState': array([ 9.0232279, 27.732279 ,  0.       ]), 'targetState': array([10, 28], dtype=int32), 'currentDistance': 1.012797248377701}
episode index:224
target Thresh 11.537532560144253
target distance 5.0
model initialize at round 224
at step 0:
{'currentTarget': array([19., 22.]), 'previousTarget': array([19., 22.]), 'currentState': array([23.15116304, 23.71825156,  0.        ]), 'targetState': array([19, 22], dtype=int32), 'currentDistance': 4.492721113992833}
done in step count: 6
reward sum = 0.9233072779080291
running average episode reward sum: 0.37182753294562093
{'currentTarget': array([19., 22.]), 'previousTarget': array([19., 22.]), 'currentState': array([19.65553001, 21.52897155,  0.        ]), 'targetState': array([19, 22], dtype=int32), 'currentDistance': 0.8072096297681938}
episode index:225
target Thresh 11.557984799759947
target distance 5.0
model initialize at round 225
at step 0:
{'currentTarget': array([19., 26.]), 'previousTarget': array([19., 26.]), 'currentState': array([23.11824387, 22.22101557,  0.        ]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 5.589334119679268}
done in step count: 21
reward sum = 0.7764518971218528
running average episode reward sum: 0.37361790623843616
{'currentTarget': array([19., 26.]), 'previousTarget': array([19., 26.]), 'currentState': array([18.01906061, 26.07503766,  0.        ]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 0.9838052374036028}
episode index:226
target Thresh 11.57841659735874
target distance 4.0
model initialize at round 226
at step 0:
{'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([25.02350722, 12.22462994,  0.        ]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 3.2247156200477978}
done in step count: 4
reward sum = 0.9489602547997615
running average episode reward sum: 0.3761524540294552
{'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([24.7468229 ,  9.62683505,  0.        ]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 0.676033151566004}
episode index:227
target Thresh 11.598827973372423
target distance 7.0
model initialize at round 227
at step 0:
{'currentTarget': array([17., 12.]), 'previousTarget': array([17., 12.]), 'currentState': array([17.08921155,  6.17008865,  0.        ]), 'targetState': array([17, 12], dtype=int32), 'currentDistance': 5.830593885944687}
done in step count: 11
reward sum = 0.8748116719796428
running average episode reward sum: 0.37833955586257006
{'currentTarget': array([17., 12.]), 'previousTarget': array([17., 12.]), 'currentState': array([16.08659546, 11.29532588,  0.        ]), 'targetState': array([17, 12], dtype=int32), 'currentDistance': 1.1536348939701497}
episode index:228
target Thresh 11.619218948212382
target distance 3.0
model initialize at round 228
at step 0:
{'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.06259984,  6.32717818,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 2.673554790909361}
done in step count: 5
reward sum = 0.9401952953135527
running average episode reward sum: 0.38079307437545645
{'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([12.31396502,  8.03799668,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 1.1815643810221141}
episode index:229
target Thresh 11.639589542269594
target distance 7.0
model initialize at round 229
at step 0:
{'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([16.38825262, 13.84618311,  0.        ]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 6.390104155779163}
done in step count: 14
reward sum = 0.8556158072117475
running average episode reward sum: 0.38285752103996207
{'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([10.76603922, 14.16638259,  0.        ]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 0.7839000289440123}
episode index:230
target Thresh 11.659939775914648
target distance 6.0
model initialize at round 230
at step 0:
{'currentTarget': array([25.,  6.]), 'previousTarget': array([25.,  6.]), 'currentState': array([19.72141904,  5.39515209,  0.        ]), 'targetState': array([25,  6], dtype=int32), 'currentDistance': 5.313121303079815}
done in step count: 6
reward sum = 0.9180450380159981
running average episode reward sum: 0.38517435011778045
{'currentTarget': array([25.,  6.]), 'previousTarget': array([25.,  6.]), 'currentState': array([24.15954703,  6.02441657,  0.        ]), 'targetState': array([25,  6], dtype=int32), 'currentDistance': 0.8408075655457775}
episode index:231
target Thresh 11.680269669497786
target distance 10.0
model initialize at round 231
at step 0:
{'currentTarget': array([20.,  6.]), 'previousTarget': array([20.,  6.]), 'currentState': array([11.30677795, 13.2916162 ,  0.        ]), 'targetState': array([20,  6], dtype=int32), 'currentDistance': 11.346355201906224}
done in step count: 14
reward sum = 0.7693960320777946
running average episode reward sum: 0.3868304780572633
{'currentTarget': array([20.,  6.]), 'previousTarget': array([20.,  6.]), 'currentState': array([19.03630433,  6.70582065,  0.        ]), 'targetState': array([20,  6], dtype=int32), 'currentDistance': 1.1945259033236555}
episode index:232
target Thresh 11.700579243348898
target distance 3.0
model initialize at round 232
at step 0:
{'currentTarget': array([11.,  6.]), 'previousTarget': array([11.,  6.]), 'currentState': array([10.61512041,  4.35082531,  0.        ]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 1.6934903179649143}
done in step count: 1
reward sum = 0.9754850078179095
running average episode reward sum: 0.38935689234808146
{'currentTarget': array([11.,  6.]), 'previousTarget': array([11.,  6.]), 'currentState': array([10.47656187,  5.31016564,  0.        ]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 0.865943942027978}
episode index:233
target Thresh 11.72086851777756
target distance 8.0
model initialize at round 233
at step 0:
{'currentTarget': array([10.,  8.]), 'previousTarget': array([10.,  8.]), 'currentState': array([17.09069014,  6.33842492,  0.        ]), 'targetState': array([10,  8], dtype=int32), 'currentDistance': 7.28276859080215}
done in step count: 10
reward sum = 0.853208293712581
running average episode reward sum: 0.39133916329408364
{'currentTarget': array([10.,  8.]), 'previousTarget': array([10.,  8.]), 'currentState': array([9.70548457, 8.99138924, 0.        ]), 'targetState': array([10,  8], dtype=int32), 'currentDistance': 1.034210790063583}
episode index:234
target Thresh 11.741137513073053
target distance 6.0
model initialize at round 234
at step 0:
{'currentTarget': array([21.,  7.]), 'previousTarget': array([21.,  7.]), 'currentState': array([26.07280236,  6.35668004,  0.        ]), 'targetState': array([21,  7], dtype=int32), 'currentDistance': 5.113431764401788}
done in step count: 7
reward sum = 0.8937820987864823
running average episode reward sum: 0.39347721833873217
{'currentTarget': array([21.,  7.]), 'previousTarget': array([21.,  7.]), 'currentState': array([21.1091882 ,  7.87949687,  0.        ]), 'targetState': array([21,  7], dtype=int32), 'currentDistance': 0.8862487262971084}
episode index:235
target Thresh 11.761386249504369
target distance 7.0
model initialize at round 235
at step 0:
{'currentTarget': array([ 2., 13.]), 'previousTarget': array([ 2., 13.]), 'currentState': array([ 3.63482794, 19.12556106,  0.        ]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 6.339965353256414}
done in step count: 7
reward sum = 0.9059411497593471
running average episode reward sum: 0.3956486756752602
{'currentTarget': array([ 2., 13.]), 'previousTarget': array([ 2., 13.]), 'currentState': array([ 1.82556038, 13.97994575,  0.        ]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 0.9953506183628043}
episode index:236
target Thresh 11.781614747320248
target distance 11.0
model initialize at round 236
at step 0:
{'currentTarget': array([26.,  5.]), 'previousTarget': array([26.,  5.]), 'currentState': array([22.28952503, 14.62611198,  0.        ]), 'targetState': array([26,  5], dtype=int32), 'currentDistance': 10.316474999786}
done in step count: 15
reward sum = 0.8008654346751586
running average episode reward sum: 0.39735845102969014
{'currentTarget': array([26.,  5.]), 'previousTarget': array([26.,  5.]), 'currentState': array([25.00865709,  5.23048962,  0.        ]), 'targetState': array([26,  5], dtype=int32), 'currentDistance': 1.0177849602408109}
episode index:237
target Thresh 11.80182302674919
target distance 4.0
model initialize at round 237
at step 0:
{'currentTarget': array([26.,  3.]), 'previousTarget': array([26.,  3.]), 'currentState': array([25.96019394,  6.01175529,  0.        ]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 3.0120183325172443}
done in step count: 3
reward sum = 0.9566284069857505
running average episode reward sum: 0.3997083247942114
{'currentTarget': array([26.,  3.]), 'previousTarget': array([26.,  3.]), 'currentState': array([25.77143601,  3.76737303,  0.        ]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 0.8006889894746654}
episode index:238
target Thresh 11.822011107999476
target distance 7.0
model initialize at round 238
at step 0:
{'currentTarget': array([24.,  9.]), 'previousTarget': array([24.,  9.]), 'currentState': array([17.74006969,  4.21935391,  0.        ]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 7.876630276718523}
done in step count: 9
reward sum = 0.8632395708225432
running average episode reward sum: 0.40164778607466467
{'currentTarget': array([24.,  9.]), 'previousTarget': array([24.,  9.]), 'currentState': array([23.15062086,  8.71594963,  0.        ]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 0.8956168433386842}
episode index:239
target Thresh 11.842179011259187
target distance 9.0
model initialize at round 239
at step 0:
{'currentTarget': array([21., 18.]), 'previousTarget': array([21., 18.]), 'currentState': array([14.64255297, 25.15528631,  0.        ]), 'targetState': array([21, 18], dtype=int32), 'currentDistance': 9.571585810305136}
done in step count: 7
reward sum = 0.8376130877270629
running average episode reward sum: 0.40346430816488305
{'currentTarget': array([21., 18.]), 'previousTarget': array([21., 18.]), 'currentState': array([20.15191507, 17.45770565,  0.        ]), 'targetState': array([21, 18], dtype=int32), 'currentDistance': 1.0066435358650028}
episode index:240
target Thresh 11.86232675669623
target distance 10.0
model initialize at round 240
at step 0:
{'currentTarget': array([12., 25.]), 'previousTarget': array([12., 25.]), 'currentState': array([15.74685955, 16.97447658,  0.        ]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 8.857086582084928}
done in step count: 5
reward sum = 0.8441676711123125
running average episode reward sum: 0.40529295282441596
{'currentTarget': array([12., 25.]), 'previousTarget': array([12., 25.]), 'currentState': array([11.64414269, 24.42655587,  0.        ]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 0.6748870979406066}
episode index:241
target Thresh 11.882454364458354
target distance 11.0
model initialize at round 241
at step 0:
{'currentTarget': array([7., 5.]), 'previousTarget': array([7., 5.]), 'currentState': array([ 9.91467171, 14.74864912,  0.        ]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 10.175041565412217}
done in step count: 10
reward sum = 0.851233172933886
running average episode reward sum: 0.4071356810066865
{'currentTarget': array([7., 5.]), 'previousTarget': array([7., 5.]), 'currentState': array([6.97117469, 5.53585315, 0.        ]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 0.5366278914893384}
episode index:242
target Thresh 11.902561854673163
target distance 9.0
model initialize at round 242
at step 0:
{'currentTarget': array([ 5., 20.]), 'previousTarget': array([ 5., 20.]), 'currentState': array([13.25642818, 23.95328849,  0.        ]), 'targetState': array([ 5, 20], dtype=int32), 'currentDistance': 9.154075388924177}
done in step count: 12
reward sum = 0.8453420940480606
running average episode reward sum: 0.4089389995788732
{'currentTarget': array([ 5., 20.]), 'previousTarget': array([ 5., 20.]), 'currentState': array([ 4.41629431, 20.77766676,  0.        ]), 'targetState': array([ 5, 20], dtype=int32), 'currentDistance': 0.9723568914051002}
episode index:243
target Thresh 11.922649247448152
target distance 5.0
model initialize at round 243
at step 0:
{'currentTarget': array([24.,  9.]), 'previousTarget': array([24.,  9.]), 'currentState': array([19.66816759,  8.8996821 ,  0.        ]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 4.332993849404867}
done in step count: 7
reward sum = 0.9124438802755757
running average episode reward sum: 0.41100254417189247
{'currentTarget': array([24.,  9.]), 'previousTarget': array([24.,  9.]), 'currentState': array([23.07191214,  8.56577303,  0.        ]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 1.0246463474544982}
episode index:244
target Thresh 11.942716562870718
target distance 8.0
model initialize at round 244
at step 0:
{'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([23.33511353, 26.12699533,  0.        ]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 7.139430270366807}
done in step count: 6
reward sum = 0.8725973076944301
running average episode reward sum: 0.41288660443116815
{'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([26.14046147, 19.85406432,  0.        ]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 0.8718392630195811}
episode index:245
target Thresh 11.962763821008174
target distance 6.0
model initialize at round 245
at step 0:
{'currentTarget': array([26., 26.]), 'previousTarget': array([26., 26.]), 'currentState': array([22.52717978, 21.28327584,  0.        ]), 'targetState': array([26, 26], dtype=int32), 'currentDistance': 5.85730032295847}
done in step count: 5
reward sum = 0.9150596365596272
running average episode reward sum: 0.41492795822030826
{'currentTarget': array([26., 26.]), 'previousTarget': array([26., 26.]), 'currentState': array([25.19715852, 25.32475498,  0.        ]), 'targetState': array([26, 26], dtype=int32), 'currentDistance': 1.0490520873819964}
episode index:246
target Thresh 11.982791041907781
target distance 11.0
model initialize at round 246
at step 0:
{'currentTarget': array([25., 18.]), 'previousTarget': array([25., 18.]), 'currentState': array([25.31919086, 27.78667152,  0.        ]), 'targetState': array([25, 18], dtype=int32), 'currentDistance': 9.791875317387019}
done in step count: 12
reward sum = 0.8413522760729393
running average episode reward sum: 0.4166543724626266
{'currentTarget': array([25., 18.]), 'previousTarget': array([25., 18.]), 'currentState': array([25.52589893, 18.55693227,  0.        ]), 'targetState': array([25, 18], dtype=int32), 'currentDistance': 0.7659916717151419}
episode index:247
target Thresh 12.002798245596761
target distance 6.0
model initialize at round 247
at step 0:
{'currentTarget': array([12., 26.]), 'previousTarget': array([12., 26.]), 'currentState': array([17.14003074, 26.94643941,  0.        ]), 'targetState': array([12, 26], dtype=int32), 'currentDistance': 5.226438900930249}
done in step count: 5
reward sum = 0.9303294434281997
running average episode reward sum: 0.4187256429100685
{'currentTarget': array([12., 26.]), 'previousTarget': array([12., 26.]), 'currentState': array([12.99468553, 26.59897982,  0.        ]), 'targetState': array([12, 26], dtype=int32), 'currentDistance': 1.161109867623842}
episode index:248
target Thresh 12.02278545208232
target distance 9.0
model initialize at round 248
at step 0:
{'currentTarget': array([24., 20.]), 'previousTarget': array([24., 20.]), 'currentState': array([24.15289889, 27.81107783,  0.        ]), 'targetState': array([24, 20], dtype=int32), 'currentDistance': 7.812574158837505}
done in step count: 9
reward sum = 0.876063466394392
running average episode reward sum: 0.4205623409963509
{'currentTarget': array([24., 20.]), 'previousTarget': array([24., 20.]), 'currentState': array([24.51677255, 20.49023658,  0.        ]), 'targetState': array([24, 20], dtype=int32), 'currentDistance': 0.7123101688201816}
episode index:249
target Thresh 12.042752681351669
target distance 4.0
model initialize at round 249
at step 0:
{'currentTarget': array([19.,  7.]), 'previousTarget': array([19.,  7.]), 'currentState': array([19.85702634,  4.80960023,  0.        ]), 'targetState': array([19,  7], dtype=int32), 'currentDistance': 2.352093807604017}
done in step count: 1
reward sum = 0.9574019643441491
running average episode reward sum: 0.4227096994897421
{'currentTarget': array([19.,  7.]), 'previousTarget': array([19.,  7.]), 'currentState': array([19.14618003,  6.01494026,  0.        ]), 'targetState': array([19,  7], dtype=int32), 'currentDistance': 0.9958470213792552}
episode index:250
target Thresh 12.062699953372032
target distance 8.0
model initialize at round 250
at step 0:
{'currentTarget': array([25., 20.]), 'previousTarget': array([25., 20.]), 'currentState': array([17.70625949, 16.83234465,  0.        ]), 'targetState': array([25, 20], dtype=int32), 'currentDistance': 7.951898581444377}
done in step count: 10
reward sum = 0.8587942310184625
running average episode reward sum: 0.42444708806156967
{'currentTarget': array([25., 20.]), 'previousTarget': array([25., 20.]), 'currentState': array([24.7474665 , 19.00674311,  0.        ]), 'targetState': array([25, 20], dtype=int32), 'currentDistance': 1.024857263614953}
episode index:251
target Thresh 12.082627288090691
target distance 5.0
model initialize at round 251
at step 0:
{'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([13.09961188,  3.88876723,  0.        ]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 4.101120612892555}
done in step count: 4
reward sum = 0.9433861452806144
running average episode reward sum: 0.42650637003466113
{'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([9.85223031, 3.99502135, 0.        ]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 0.8522448526948578}
episode index:252
target Thresh 12.102534705434977
target distance 12.0
model initialize at round 252
at step 0:
{'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([18.01442512, 15.06796843,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 11.471123916588375}
done in step count: 15
reward sum = 0.8183160704463245
running average episode reward sum: 0.4280550249769997
{'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.12995285,  4.55866379,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.5735790861910957}
episode index:253
target Thresh 12.12242222531231
target distance 1.0
model initialize at round 253
at step 0:
{'currentTarget': array([25., 11.]), 'previousTarget': array([25., 11.]), 'currentState': array([24.27689296, 11.30531108,  0.        ]), 'targetState': array([25, 11], dtype=int32), 'currentDistance': 0.7849195175822251}
done in step count: 0
reward sum = 0.9972036879108134
running average episode reward sum: 0.43029576774445566
{'currentTarget': array([25., 11.]), 'previousTarget': array([25., 11.]), 'currentState': array([24.27689296, 11.30531108,  0.        ]), 'targetState': array([25, 11], dtype=int32), 'currentDistance': 0.7849195175822251}
episode index:254
target Thresh 12.142289867610213
target distance 7.0
model initialize at round 254
at step 0:
{'currentTarget': array([17.,  5.]), 'previousTarget': array([17.,  5.]), 'currentState': array([10.57795364,  5.47474629,  0.        ]), 'targetState': array([17,  5], dtype=int32), 'currentDistance': 6.439570135495085}
done in step count: 6
reward sum = 0.9108431026245809
running average episode reward sum: 0.43218026709692675
{'currentTarget': array([17.,  5.]), 'previousTarget': array([17.,  5.]), 'currentState': array([16.0076614 ,  4.22387448,  0.        ]), 'targetState': array([17,  5], dtype=int32), 'currentDistance': 1.2598042372347542}
episode index:255
target Thresh 12.162137652196325
target distance 4.0
model initialize at round 255
at step 0:
{'currentTarget': array([ 8., 26.]), 'previousTarget': array([ 8., 26.]), 'currentState': array([ 4.97112495, 26.67152119,  0.        ]), 'targetState': array([ 8, 26], dtype=int32), 'currentDistance': 3.102422406864448}
done in step count: 3
reward sum = 0.9451389029414803
running average episode reward sum: 0.43418401176819454
{'currentTarget': array([ 8., 26.]), 'previousTarget': array([ 8., 26.]), 'currentState': array([ 7.00797448, 25.26458642,  0.        ]), 'targetState': array([ 8, 26], dtype=int32), 'currentDistance': 1.2348877585967024}
episode index:256
target Thresh 12.181965598918438
target distance 12.0
model initialize at round 256
at step 0:
{'currentTarget': array([8., 4.]), 'previousTarget': array([8., 4.]), 'currentState': array([14.80255735, 15.04111695,  0.        ]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 12.968463673726102}
done in step count: 16
reward sum = 0.8011983219786515
running average episode reward sum: 0.4356120830141496
{'currentTarget': array([8., 4.]), 'previousTarget': array([8., 4.]), 'currentState': array([8.51156879, 4.51828969, 0.        ]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 0.7282354200068444}
episode index:257
target Thresh 12.201773727604504
target distance 1.0
model initialize at round 257
at step 0:
{'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([21.37063223, 10.37349412,  0.        ]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 0.7279271051725001}
done in step count: 0
reward sum = 0.997321991641217
running average episode reward sum: 0.4377892532026266
{'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([21.37063223, 10.37349412,  0.        ]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 0.7279271051725001}
episode index:258
target Thresh 12.221562058062641
target distance 10.0
model initialize at round 258
at step 0:
{'currentTarget': array([ 6., 17.]), 'previousTarget': array([ 6., 17.]), 'currentState': array([13.14213479,  8.98740602,  0.        ]), 'targetState': array([ 6, 17], dtype=int32), 'currentDistance': 10.733673726107645}
done in step count: 5
reward sum = 0.8035787063054429
running average episode reward sum: 0.4392015676933711
{'currentTarget': array([ 6., 17.]), 'previousTarget': array([ 6., 17.]), 'currentState': array([ 6.30321962, 16.44482219,  0.        ]), 'targetState': array([ 6, 17], dtype=int32), 'currentDistance': 0.6325855942557945}
episode index:259
target Thresh 12.241330610081192
target distance 4.0
model initialize at round 259
at step 0:
{'currentTarget': array([ 8., 21.]), 'previousTarget': array([ 8., 21.]), 'currentState': array([ 7.44543138, 17.85152543,  0.        ]), 'targetState': array([ 8, 21], dtype=int32), 'currentDistance': 3.1969420539555533}
done in step count: 4
reward sum = 0.9472850572250273
running average episode reward sum: 0.4411557349608005
{'currentTarget': array([ 8., 21.]), 'previousTarget': array([ 8., 21.]), 'currentState': array([ 7.69836411, 20.12338832,  0.        ]), 'targetState': array([ 8, 21], dtype=int32), 'currentDistance': 0.9270556865912194}
episode index:260
target Thresh 12.261079403428703
target distance 6.0
model initialize at round 260
at step 0:
{'currentTarget': array([ 8., 22.]), 'previousTarget': array([ 8., 22.]), 'currentState': array([13.22598642, 22.07129392,  0.        ]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 5.226472701227431}
done in step count: 7
reward sum = 0.913258956625181
running average episode reward sum: 0.4429645595648786
{'currentTarget': array([ 8., 22.]), 'previousTarget': array([ 8., 22.]), 'currentState': array([ 8.57202306, 21.22926534,  0.        ]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 0.959813678022797}
episode index:261
target Thresh 12.280808457853972
target distance 2.0
model initialize at round 261
at step 0:
{'currentTarget': array([11., 17.]), 'previousTarget': array([11., 17.]), 'currentState': array([11.01411432, 15.45284086,  0.        ]), 'targetState': array([11, 17], dtype=int32), 'currentDistance': 1.5472235146265694}
done in step count: 3
reward sum = 0.9687494920904309
running average episode reward sum: 0.4449713722844418
{'currentTarget': array([11., 17.]), 'previousTarget': array([11., 17.]), 'currentState': array([11.04108477, 16.00061505,  0.        ]), 'targetState': array([11, 17], dtype=int32), 'currentDistance': 1.0002290964462182}
episode index:262
target Thresh 12.300517793086058
target distance 4.0
model initialize at round 262
at step 0:
{'currentTarget': array([ 4., 25.]), 'previousTarget': array([ 4., 25.]), 'currentState': array([ 7.24244744, 25.72978213,  0.        ]), 'targetState': array([ 4, 25], dtype=int32), 'currentDistance': 3.3235594379856566}
done in step count: 4
reward sum = 0.9478324704854615
running average episode reward sum: 0.4468833916692366
{'currentTarget': array([ 4., 25.]), 'previousTarget': array([ 4., 25.]), 'currentState': array([ 4.61942774, 24.87762371,  0.        ]), 'targetState': array([ 4, 25], dtype=int32), 'currentDistance': 0.6314005724317886}
episode index:263
target Thresh 12.320207428834294
target distance 9.0
model initialize at round 263
at step 0:
{'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([16.04003471, 15.01007319,  0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 10.038084376478446}
done in step count: 14
reward sum = 0.8261764232701291
running average episode reward sum: 0.4483201076980278
{'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([8.44578791, 9.76595943, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.8862396449546648}
episode index:264
target Thresh 12.339877384788323
target distance 3.0
model initialize at round 264
at step 0:
{'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([8.76011705, 4.18868595, 0.        ]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 2.2478162738396903}
done in step count: 3
reward sum = 0.9601243014461456
running average episode reward sum: 0.4502514442782094
{'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([10.06080058,  3.3748952 ,  0.        ]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 1.1282072315260712}
episode index:265
target Thresh 12.359527680618093
target distance 8.0
model initialize at round 265
at step 0:
{'currentTarget': array([12., 12.]), 'previousTarget': array([12., 12.]), 'currentState': array([19.14802778, 15.05083917,  0.        ]), 'targetState': array([12, 12], dtype=int32), 'currentDistance': 7.7718672646454205}
done in step count: 9
reward sum = 0.8809536094608403
running average episode reward sum: 0.4518706253503246
{'currentTarget': array([12., 12.]), 'previousTarget': array([12., 12.]), 'currentState': array([12.56223327, 12.52476215,  0.        ]), 'targetState': array([12, 12], dtype=int32), 'currentDistance': 0.7690783822811731}
episode index:266
target Thresh 12.379158335973909
target distance 6.0
model initialize at round 266
at step 0:
{'currentTarget': array([ 5., 23.]), 'previousTarget': array([ 5., 23.]), 'currentState': array([ 7.40132618, 28.48766547,  0.        ]), 'targetState': array([ 5, 23], dtype=int32), 'currentDistance': 5.990061750888588}
done in step count: 8
reward sum = 0.9014467432645734
running average episode reward sum: 0.4535544310353966
{'currentTarget': array([ 5., 23.]), 'previousTarget': array([ 5., 23.]), 'currentState': array([ 5.26352823, 23.56557399,  0.        ]), 'targetState': array([ 5, 23], dtype=int32), 'currentDistance': 0.6239559803428498}
episode index:267
target Thresh 12.398769370486427
target distance 12.0
model initialize at round 267
at step 0:
{'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([16.29237312, 13.67768738,  0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 12.222865862534501}
done in step count: 13
reward sum = 0.8296063902602745
running average episode reward sum: 0.4549576099877283
{'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([5.95335072, 9.6232401 , 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 1.1389933366110794}
episode index:268
target Thresh 12.418360803766681
target distance 10.0
model initialize at round 268
at step 0:
{'currentTarget': array([21., 24.]), 'previousTarget': array([21., 24.]), 'currentState': array([17.61686176, 14.73497814,  0.        ]), 'targetState': array([21, 24], dtype=int32), 'currentDistance': 9.863379462647151}
done in step count: 10
reward sum = 0.8524326198061519
running average episode reward sum: 0.4564352122547113
{'currentTarget': array([21., 24.]), 'previousTarget': array([21., 24.]), 'currentState': array([20.57922433, 23.12564194,  0.        ]), 'targetState': array([21, 24], dtype=int32), 'currentDistance': 0.9703371480219308}
episode index:269
target Thresh 12.437932655406104
target distance 3.0
model initialize at round 269
at step 0:
{'currentTarget': array([17.,  6.]), 'previousTarget': array([17.,  6.]), 'currentState': array([16.27015164,  7.84093547,  0.        ]), 'targetState': array([17,  6], dtype=int32), 'currentDistance': 1.9803338155970478}
done in step count: 1
reward sum = 0.9782244107448063
running average episode reward sum: 0.4583677648417116
{'currentTarget': array([17.,  6.]), 'previousTarget': array([17.,  6.]), 'currentState': array([16.61089024,  6.92874885,  0.        ]), 'targetState': array([17,  6], dtype=int32), 'currentDistance': 1.0069661487760917}
episode index:270
target Thresh 12.457484944976557
target distance 10.0
model initialize at round 270
at step 0:
{'currentTarget': array([18., 26.]), 'previousTarget': array([18., 26.]), 'currentState': array([ 8.70121282, 16.97794544,  0.        ]), 'targetState': array([18, 26], dtype=int32), 'currentDistance': 12.956269192208643}
done in step count: 14
reward sum = 0.8074064514815332
running average episode reward sum: 0.45965573047506886
{'currentTarget': array([18., 26.]), 'previousTarget': array([18., 26.]), 'currentState': array([17.54517792, 25.11927352,  0.        ]), 'targetState': array([18, 26], dtype=int32), 'currentDistance': 0.991232690739751}
episode index:271
target Thresh 12.477017692030326
target distance 7.0
model initialize at round 271
at step 0:
{'currentTarget': array([11.,  7.]), 'previousTarget': array([11.,  7.]), 'currentState': array([17.14200044,  6.08101964,  0.        ]), 'targetState': array([11,  7], dtype=int32), 'currentDistance': 6.210369897805235}
done in step count: 6
reward sum = 0.9146327991358344
running average episode reward sum: 0.46132844028632164
{'currentTarget': array([11.,  7.]), 'previousTarget': array([11.,  7.]), 'currentState': array([11.82840121,  6.70398927,  0.        ]), 'targetState': array([11,  7], dtype=int32), 'currentDistance': 0.8796993309811345}
episode index:272
target Thresh 12.496530916100156
target distance 10.0
model initialize at round 272
at step 0:
{'currentTarget': array([12., 12.]), 'previousTarget': array([12., 12.]), 'currentState': array([2.61943203, 8.87080687, 0.        ]), 'targetState': array([12, 12], dtype=int32), 'currentDistance': 9.888726157967525}
done in step count: 10
reward sum = 0.8492021312497372
running average episode reward sum: 0.4627492230371034
{'currentTarget': array([12., 12.]), 'previousTarget': array([12., 12.]), 'currentState': array([11.07916015, 11.53892415,  0.        ]), 'targetState': array([12, 12], dtype=int32), 'currentDistance': 1.0298237526518397}
episode index:273
target Thresh 12.516024636699282
target distance 10.0
model initialize at round 273
at step 0:
{'currentTarget': array([27., 17.]), 'previousTarget': array([27., 17.]), 'currentState': array([17.55113846, 10.66583604,  0.        ]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 11.375527131096284}
done in step count: 12
reward sum = 0.8258502506990005
running average episode reward sum: 0.4640744092694461
{'currentTarget': array([27., 17.]), 'previousTarget': array([27., 17.]), 'currentState': array([26.05336136, 16.53851933,  0.        ]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 1.0531330083707175}
episode index:274
target Thresh 12.535498873321416
target distance 12.0
model initialize at round 274
at step 0:
{'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([ 9.02964015, 16.58570492,  0.        ]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 10.994552575771634}
done in step count: 8
reward sum = 0.8405326474548549
running average episode reward sum: 0.46544334831739304
{'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([11.19509029,  6.1675663 ,  0.        ]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 0.8221667158807472}
episode index:275
target Thresh 12.554953645440804
target distance 11.0
model initialize at round 275
at step 0:
{'currentTarget': array([23., 19.]), 'previousTarget': array([23., 19.]), 'currentState': array([12.61610514, 14.9774161 ,  0.        ]), 'targetState': array([23, 19], dtype=int32), 'currentDistance': 11.135818502771913}
done in step count: 12
reward sum = 0.8189011962061489
running average episode reward sum: 0.46672399269380155
{'currentTarget': array([23., 19.]), 'previousTarget': array([23., 19.]), 'currentState': array([22.21129212, 18.50375194,  0.        ]), 'targetState': array([23, 19], dtype=int32), 'currentDistance': 0.9318381059652109}
episode index:276
target Thresh 12.57438897251222
target distance 11.0
model initialize at round 276
at step 0:
{'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([14.40966082, 24.41136849,  0.        ]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 14.722691062691013}
done in step count: 18
reward sum = 0.7741835146703422
running average episode reward sum: 0.4678339548670021
{'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([ 4.17675223, 14.63240606,  0.        ]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 0.6566420418275533}
episode index:277
target Thresh 12.593804873970985
target distance 11.0
model initialize at round 277
at step 0:
{'currentTarget': array([20., 20.]), 'previousTarget': array([20., 20.]), 'currentState': array([18.63972172, 10.85444021,  0.        ]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 9.246167899541781}
done in step count: 7
reward sum = 0.8553336224308182
running average episode reward sum: 0.4692278385632747
{'currentTarget': array([20., 20.]), 'previousTarget': array([20., 20.]), 'currentState': array([19.6942573 , 19.01748621,  0.        ]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 1.02898587674471}
episode index:278
target Thresh 12.61320136923301
target distance 6.0
model initialize at round 278
at step 0:
{'currentTarget': array([ 7., 13.]), 'previousTarget': array([ 7., 13.]), 'currentState': array([12.12817788, 16.58219764,  0.        ]), 'targetState': array([ 7, 13], dtype=int32), 'currentDistance': 6.255425508119539}
done in step count: 6
reward sum = 0.9138804925413488
running average episode reward sum: 0.47082157567430727
{'currentTarget': array([ 7., 13.]), 'previousTarget': array([ 7., 13.]), 'currentState': array([ 7.62413535, 13.91484895,  0.        ]), 'targetState': array([ 7, 13], dtype=int32), 'currentDistance': 1.1074716867016599}
episode index:279
target Thresh 12.632578477694786
target distance 10.0
model initialize at round 279
at step 0:
{'currentTarget': array([13., 14.]), 'previousTarget': array([13., 14.]), 'currentState': array([ 3.84478813, 19.59697628,  0.        ]), 'targetState': array([13, 14], dtype=int32), 'currentDistance': 10.730519455777078}
done in step count: 11
reward sum = 0.8278373043710572
running average episode reward sum: 0.47209663184822426
{'currentTarget': array([13., 14.]), 'previousTarget': array([13., 14.]), 'currentState': array([12.33257434, 13.815575  ,  0.        ]), 'targetState': array([13, 14], dtype=int32), 'currentDistance': 0.6924374316173743}
episode index:280
target Thresh 12.651936218733425
target distance 12.0
model initialize at round 280
at step 0:
{'currentTarget': array([16., 29.]), 'previousTarget': array([16., 29.]), 'currentState': array([25.39825678, 18.99056089,  0.        ]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 13.730116597852817}
done in step count: 7
reward sum = 0.7683741058596454
running average episode reward sum: 0.4731510000831403
{'currentTarget': array([16., 29.]), 'previousTarget': array([16., 29.]), 'currentState': array([16.33499545, 28.28484237,  0.        ]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 0.7897293103000573}
episode index:281
target Thresh 12.671274611706675
target distance 10.0
model initialize at round 281
at step 0:
{'currentTarget': array([13., 28.]), 'previousTarget': array([13., 28.]), 'currentState': array([10.62352306, 19.3884424 ,  0.        ]), 'targetState': array([13, 28], dtype=int32), 'currentDistance': 8.933452132894233}
done in step count: 7
reward sum = 0.8667891005035995
running average episode reward sum: 0.47454687987186533
{'currentTarget': array([13., 28.]), 'previousTarget': array([13., 28.]), 'currentState': array([12.72198348, 27.03926408,  0.        ]), 'targetState': array([13, 28], dtype=int32), 'currentDistance': 1.0001533318028164}
episode index:282
target Thresh 12.690593675952922
target distance 10.0
model initialize at round 282
at step 0:
{'currentTarget': array([6., 2.]), 'previousTarget': array([6., 2.]), 'currentState': array([15.19652557,  8.73344812,  0.        ]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 11.398043963140923}
done in step count: 14
reward sum = 0.8169521358028158
running average episode reward sum: 0.47575679243699237
{'currentTarget': array([6., 2.]), 'previousTarget': array([6., 2.]), 'currentState': array([6.29782128, 2.47870901, 0.        ]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 0.5637905912669788}
episode index:283
target Thresh 12.709893430791237
target distance 5.0
model initialize at round 283
at step 0:
{'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([24.78576946, 16.23576474,  0.        ]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 4.308031638351225}
done in step count: 5
reward sum = 0.9333451797268297
running average episode reward sum: 0.4773680191528017
{'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([24.15459883, 12.44687331,  0.        ]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 0.4728599704024874}
episode index:284
target Thresh 12.729173895521374
target distance 9.0
model initialize at round 284
at step 0:
{'currentTarget': array([23.,  9.]), 'previousTarget': array([23.,  9.]), 'currentState': array([14.45156381, 16.31017733,  0.        ]), 'targetState': array([23,  9], dtype=int32), 'currentDistance': 11.247864415731001}
done in step count: 11
reward sum = 0.8163897785639339
running average episode reward sum: 0.47855756918582326
{'currentTarget': array([23.,  9.]), 'previousTarget': array([23.,  9.]), 'currentState': array([22.37959775,  9.08116192,  0.        ]), 'targetState': array([23,  9], dtype=int32), 'currentDistance': 0.6256885841836414}
episode index:285
target Thresh 12.748435089423804
target distance 5.0
model initialize at round 285
at step 0:
{'currentTarget': array([16., 27.]), 'previousTarget': array([16., 27.]), 'currentState': array([19.8414582 , 23.61075497,  0.        ]), 'targetState': array([16, 27], dtype=int32), 'currentDistance': 5.122868630823448}
done in step count: 3
reward sum = 0.9241884543805343
running average episode reward sum: 0.4801157191340565
{'currentTarget': array([16., 27.]), 'previousTarget': array([16., 27.]), 'currentState': array([16.81965607, 26.14665258,  0.        ]), 'targetState': array([16, 27], dtype=int32), 'currentDistance': 1.1832319722374116}
episode index:286
target Thresh 12.767677031759717
target distance 8.0
model initialize at round 286
at step 0:
{'currentTarget': array([ 2., 12.]), 'previousTarget': array([ 2., 12.]), 'currentState': array([ 9.19273663, 13.0573781 ,  0.        ]), 'targetState': array([ 2, 12], dtype=int32), 'currentDistance': 7.270041857457543}
done in step count: 7
reward sum = 0.9008546633173161
running average episode reward sum: 0.4815817084866114
{'currentTarget': array([ 2., 12.]), 'previousTarget': array([ 2., 12.]), 'currentState': array([ 2.96137124, 12.0672558 ,  0.        ]), 'targetState': array([ 2, 12], dtype=int32), 'currentDistance': 0.9637209191235752}
episode index:287
target Thresh 12.786899741771059
target distance 10.0
model initialize at round 287
at step 0:
{'currentTarget': array([20., 12.]), 'previousTarget': array([20., 12.]), 'currentState': array([16.58102584,  2.80877286,  0.        ]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 9.806530508680789}
done in step count: 9
reward sum = 0.858070938551081
running average episode reward sum: 0.4828889627576686
{'currentTarget': array([20., 12.]), 'previousTarget': array([20., 12.]), 'currentState': array([19.5924606 , 11.21652907,  0.        ]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 0.883127999012348}
episode index:288
target Thresh 12.806103238680542
target distance 6.0
model initialize at round 288
at step 0:
{'currentTarget': array([12., 26.]), 'previousTarget': array([12., 26.]), 'currentState': array([16.73755991, 21.71738553,  0.        ]), 'targetState': array([12, 26], dtype=int32), 'currentDistance': 6.386333896605498}
done in step count: 4
reward sum = 0.9004979527684523
running average episode reward sum: 0.48433397656393434
{'currentTarget': array([12., 26.]), 'previousTarget': array([12., 26.]), 'currentState': array([12.71191907, 25.21147788,  0.        ]), 'targetState': array([12, 26], dtype=int32), 'currentDistance': 1.0623539437092535}
episode index:289
target Thresh 12.825287541691665
target distance 11.0
model initialize at round 289
at step 0:
{'currentTarget': array([ 7., 20.]), 'previousTarget': array([ 7., 20.]), 'currentState': array([17.17802328, 26.65834606,  0.        ]), 'targetState': array([ 7, 20], dtype=int32), 'currentDistance': 12.162472202474698}
done in step count: 13
reward sum = 0.8221831601633267
running average episode reward sum: 0.48549897374875983
{'currentTarget': array([ 7., 20.]), 'previousTarget': array([ 7., 20.]), 'currentState': array([ 7.48502149, 20.57474753,  0.        ]), 'targetState': array([ 7, 20], dtype=int32), 'currentDistance': 0.7520509084965628}
episode index:290
target Thresh 12.84445266998873
target distance 8.0
model initialize at round 290
at step 0:
{'currentTarget': array([11., 29.]), 'previousTarget': array([11., 29.]), 'currentState': array([18.11657238, 24.824812  ,  0.        ]), 'targetState': array([11, 29], dtype=int32), 'currentDistance': 8.250927058068658}
done in step count: 8
reward sum = 0.8864825914573295
running average episode reward sum: 0.48687692432507795
{'currentTarget': array([11., 29.]), 'previousTarget': array([11., 29.]), 'currentState': array([11.99703813, 28.03623956,  0.        ]), 'targetState': array([11, 29], dtype=int32), 'currentDistance': 1.386693622208472}
episode index:291
target Thresh 12.863598642736868
target distance 3.0
model initialize at round 291
at step 0:
{'currentTarget': array([21.,  2.]), 'previousTarget': array([21.,  2.]), 'currentState': array([23.1639182,  4.5962927,  0.       ]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 3.379833985657736}
done in step count: 4
reward sum = 0.9458484915113498
running average episode reward sum: 0.4884487447606474
{'currentTarget': array([21.,  2.]), 'previousTarget': array([21.,  2.]), 'currentState': array([21.36388658,  2.46603549,  0.        ]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 0.5912719480399902}
episode index:292
target Thresh 12.882725479082058
target distance 12.0
model initialize at round 292
at step 0:
{'currentTarget': array([ 4., 28.]), 'previousTarget': array([ 4., 28.]), 'currentState': array([ 9.06291071, 17.95376158,  0.        ]), 'targetState': array([ 4, 28], dtype=int32), 'currentDistance': 11.24988761075119}
done in step count: 8
reward sum = 0.8231187900912696
running average episode reward sum: 0.4895909633453935
{'currentTarget': array([ 4., 28.]), 'previousTarget': array([ 4., 28.]), 'currentState': array([ 4.67187995, 27.19535875,  0.        ]), 'targetState': array([ 4, 28], dtype=int32), 'currentDistance': 1.048270103306955}
episode index:293
target Thresh 12.901833198151127
target distance 7.0
model initialize at round 293
at step 0:
{'currentTarget': array([13., 19.]), 'previousTarget': array([13., 19.]), 'currentState': array([ 6.45047259, 22.81061447,  0.        ]), 'targetState': array([13, 19], dtype=int32), 'currentDistance': 7.57740667145359}
done in step count: 10
reward sum = 0.8714359703489866
running average episode reward sum: 0.490889755886222
{'currentTarget': array([13., 19.]), 'previousTarget': array([13., 19.]), 'currentState': array([12.25645101, 19.35027472,  0.        ]), 'targetState': array([13, 19], dtype=int32), 'currentDistance': 0.8219230368242563}
episode index:294
target Thresh 12.92092181905181
target distance 8.0
model initialize at round 294
at step 0:
{'currentTarget': array([20., 19.]), 'previousTarget': array([20., 19.]), 'currentState': array([12.78365308, 22.82055104,  0.        ]), 'targetState': array([20, 19], dtype=int32), 'currentDistance': 8.165309124304649}
done in step count: 12
reward sum = 0.8503692767435294
running average episode reward sum: 0.4921083305331959
{'currentTarget': array([20., 19.]), 'previousTarget': array([20., 19.]), 'currentState': array([19.4085848 , 19.45870474,  0.        ]), 'targetState': array([20, 19], dtype=int32), 'currentDistance': 0.7484530542668759}
episode index:295
target Thresh 12.939991360872718
target distance 9.0
model initialize at round 295
at step 0:
{'currentTarget': array([25., 10.]), 'previousTarget': array([25., 10.]), 'currentState': array([16.82718343,  8.46137524,  0.        ]), 'targetState': array([25, 10], dtype=int32), 'currentDistance': 8.31638725195292}
done in step count: 13
reward sum = 0.8191370186178022
running average episode reward sum: 0.4932131571821304
{'currentTarget': array([25., 10.]), 'previousTarget': array([25., 10.]), 'currentState': array([24.09366912, 10.70192934,  0.        ]), 'targetState': array([25, 10], dtype=int32), 'currentDistance': 1.1463596583643516}
episode index:296
target Thresh 12.959041842683401
target distance 8.0
model initialize at round 296
at step 0:
{'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([20.87540793, 27.20743778,  0.        ]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 8.060630680248243}
done in step count: 10
reward sum = 0.856903131970518
running average episode reward sum: 0.49443770255178826
{'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([13.89736254, 23.81600707,  0.        ]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 0.8224366197091459}
episode index:297
target Thresh 12.97807328353434
target distance 11.0
model initialize at round 297
at step 0:
{'currentTarget': array([20., 26.]), 'previousTarget': array([20., 26.]), 'currentState': array([13.65974736, 15.58859056,  0.        ]), 'targetState': array([20, 26], dtype=int32), 'currentDistance': 12.190006152190788}
done in step count: 12
reward sum = 0.827238188552158
running average episode reward sum: 0.4955544827061519
{'currentTarget': array([20., 26.]), 'previousTarget': array([20., 26.]), 'currentState': array([19.47080064, 25.33571944,  0.        ]), 'targetState': array([20, 26], dtype=int32), 'currentDistance': 0.8493059671183275}
episode index:298
target Thresh 12.997085702456975
target distance 7.0
model initialize at round 298
at step 0:
{'currentTarget': array([23., 21.]), 'previousTarget': array([23., 21.]), 'currentState': array([23.3415347 , 15.51872659,  0.        ]), 'targetState': array([23, 21], dtype=int32), 'currentDistance': 5.491903511428161}
done in step count: 4
reward sum = 0.9171613483451013
running average episode reward sum: 0.4969645391129711
{'currentTarget': array([23., 21.]), 'previousTarget': array([23., 21.]), 'currentState': array([22.96984047, 20.22113723,  0.        ]), 'targetState': array([23, 21], dtype=int32), 'currentDistance': 0.7794464822775456}
episode index:299
target Thresh 13.016079118463729
target distance 12.0
model initialize at round 299
at step 0:
{'currentTarget': array([8., 7.]), 'previousTarget': array([8., 7.]), 'currentState': array([10.15563571, 18.01858252,  0.        ]), 'targetState': array([8, 7], dtype=int32), 'currentDistance': 11.227463032539319}
done in step count: 13
reward sum = 0.8294934794372444
running average episode reward sum: 0.498072968914052
{'currentTarget': array([8., 7.]), 'previousTarget': array([8., 7.]), 'currentState': array([8.0363424 , 7.56134272, 0.        ]), 'targetState': array([8, 7], dtype=int32), 'currentDistance': 0.5625179243047552}
episode index:300
target Thresh 13.035053550548021
target distance 1.0
model initialize at round 300
at step 0:
{'currentTarget': array([25., 12.]), 'previousTarget': array([25., 12.]), 'currentState': array([24.42188412, 12.58272991,  0.        ]), 'targetState': array([25, 12], dtype=int32), 'currentDistance': 0.8208484113639325}
done in step count: 0
reward sum = 0.9982394971396071
running average episode reward sum: 0.49973465173207704
{'currentTarget': array([25., 12.]), 'previousTarget': array([25., 12.]), 'currentState': array([24.42188412, 12.58272991,  0.        ]), 'targetState': array([25, 12], dtype=int32), 'currentDistance': 0.8208484113639325}
episode index:301
target Thresh 13.054009017684283
target distance 13.0
model initialize at round 301
at step 0:
{'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([11.91804288,  6.92224169,  0.        ]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 12.559457026176796}
done in step count: 13
reward sum = 0.782696560627741
running average episode reward sum: 0.5006716116953077
{'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([ 6.60461417, 17.62278605,  0.        ]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 0.7126350120692785}
episode index:302
target Thresh 13.072945538827984
target distance 10.0
model initialize at round 302
at step 0:
{'currentTarget': array([21.,  5.]), 'previousTarget': array([21.,  5.]), 'currentState': array([18.43762657, 13.80853343,  0.        ]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 9.173658963662099}
done in step count: 9
reward sum = 0.8666677528126951
running average episode reward sum: 0.5018795197518008
{'currentTarget': array([21.,  5.]), 'previousTarget': array([21.,  5.]), 'currentState': array([21.0081837 ,  5.84794998,  0.        ]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 0.8479894718197638}
episode index:303
target Thresh 13.091863132915645
target distance 3.0
model initialize at round 303
at step 0:
{'currentTarget': array([ 3., 19.]), 'previousTarget': array([ 3., 19.]), 'currentState': array([ 5.16220182, 21.4944452 ,  0.        ]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 3.3011170227117104}
done in step count: 3
reward sum = 0.9560497554704207
running average episode reward sum: 0.5033735007903488
{'currentTarget': array([ 3., 19.]), 'previousTarget': array([ 3., 19.]), 'currentState': array([ 3.49214792, 19.82664335,  0.        ]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 0.9620544693518012}
episode index:304
target Thresh 13.110761818864866
target distance 1.0
model initialize at round 304
at step 0:
{'currentTarget': array([20.,  8.]), 'previousTarget': array([20.,  8.]), 'currentState': array([19.26623744,  7.90574938,  0.        ]), 'targetState': array([20,  8], dtype=int32), 'currentDistance': 0.7397909684874395}
done in step count: 0
reward sum = 0.9996011722184259
running average episode reward sum: 0.5050004767622441
{'currentTarget': array([20.,  8.]), 'previousTarget': array([20.,  8.]), 'currentState': array([19.26623744,  7.90574938,  0.        ]), 'targetState': array([20,  8], dtype=int32), 'currentDistance': 0.7397909684874395}
episode index:305
target Thresh 13.129641615574329
target distance 7.0
model initialize at round 305
at step 0:
{'currentTarget': array([ 3., 14.]), 'previousTarget': array([ 3., 14.]), 'currentState': array([ 9.14486098, 16.61659762,  0.        ]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 6.678764827261997}
done in step count: 7
reward sum = 0.9026046792898251
running average episode reward sum: 0.5062998369012232
{'currentTarget': array([ 3., 14.]), 'previousTarget': array([ 3., 14.]), 'currentState': array([ 3.8007499 , 14.10289531,  0.        ]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 0.8073337874211948}
episode index:306
target Thresh 13.148502541923836
target distance 13.0
model initialize at round 306
at step 0:
{'currentTarget': array([24., 29.]), 'previousTarget': array([24., 29.]), 'currentState': array([11.84898031, 29.43633711,  0.        ]), 'targetState': array([24, 29], dtype=int32), 'currentDistance': 12.158851493546305}
done in step count: 23
reward sum = 0.7235123747120812
running average episode reward sum: 0.5070073695976753
{'currentTarget': array([24., 29.]), 'previousTarget': array([24., 29.]), 'currentState': array([23.30706134, 29.07413463,  0.        ]), 'targetState': array([24, 29], dtype=int32), 'currentDistance': 0.6968930503988374}
episode index:307
target Thresh 13.167344616774315
target distance 7.0
model initialize at round 307
at step 0:
{'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([19.92062092, 28.59582931,  0.        ]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 8.863335507505411}
done in step count: 9
reward sum = 0.8669880518260187
running average episode reward sum: 0.5081761380464686
{'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([14.31967515, 22.64341441,  0.        ]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 0.7184527126623118}
episode index:308
target Thresh 13.186167858967838
target distance 2.0
model initialize at round 308
at step 0:
{'currentTarget': array([21.,  7.]), 'previousTarget': array([21.,  7.]), 'currentState': array([19.70191532,  5.4252964 ,  0.        ]), 'targetState': array([21,  7], dtype=int32), 'currentDistance': 2.040763403031393}
done in step count: 2
reward sum = 0.9743673681818763
running average episode reward sum: 0.5096848475291074
{'currentTarget': array([21.,  7.]), 'previousTarget': array([21.,  7.]), 'currentState': array([20.36196826,  6.10201043,  0.        ]), 'targetState': array([21,  7], dtype=int32), 'currentDistance': 1.1015760373022678}
episode index:309
target Thresh 13.204972287327656
target distance 12.0
model initialize at round 309
at step 0:
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([14.3379243 , 14.29932261,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 14.050881935787348}
done in step count: 15
reward sum = 0.7914482379062215
running average episode reward sum: 0.5105937616916142
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.2000917 , 6.98085855, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 1.0010595308869519}
episode index:310
target Thresh 13.223757920658194
target distance 10.0
model initialize at round 310
at step 0:
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([12.85541636,  3.22411072,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 12.553632368175624}
done in step count: 18
reward sum = 0.7886550104872442
running average episode reward sum: 0.5114878493083204
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.44909295, 10.80381247,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.49007553197666914}
episode index:311
target Thresh 13.24252477774509
target distance 4.0
model initialize at round 311
at step 0:
{'currentTarget': array([24., 26.]), 'previousTarget': array([24., 26.]), 'currentState': array([24.98438569, 23.43845558,  0.        ]), 'targetState': array([24, 26], dtype=int32), 'currentDistance': 2.7441802025489195}
done in step count: 2
reward sum = 0.959555493383879
running average episode reward sum: 0.5129239635521523
{'currentTarget': array([24., 26.]), 'previousTarget': array([24., 26.]), 'currentState': array([24.19495086, 25.25669247,  0.        ]), 'targetState': array([24, 26], dtype=int32), 'currentDistance': 0.768447737383973}
episode index:312
target Thresh 13.261272877355196
target distance 8.0
model initialize at round 312
at step 0:
{'currentTarget': array([24., 26.]), 'previousTarget': array([24., 26.]), 'currentState': array([21.68847799, 18.94243681,  0.        ]), 'targetState': array([24, 26], dtype=int32), 'currentDistance': 7.4264616143102264}
done in step count: 6
reward sum = 0.8948675262596689
running average episode reward sum: 0.5141442305256587
{'currentTarget': array([24., 26.]), 'previousTarget': array([24., 26.]), 'currentState': array([23.29422016, 25.33645403,  0.        ]), 'targetState': array([24, 26], dtype=int32), 'currentDistance': 0.968719998773054}
episode index:313
target Thresh 13.28000223823662
target distance 6.0
model initialize at round 313
at step 0:
{'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([23.65920871, 16.1902808 ,  0.        ]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 5.449035537759233}
done in step count: 6
reward sum = 0.919054575209499
running average episode reward sum: 0.5154337539163716
{'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([22.22058784, 11.68097538,  0.        ]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 0.7158117496618014}
episode index:314
target Thresh 13.298712879118721
target distance 10.0
model initialize at round 314
at step 0:
{'currentTarget': array([19., 22.]), 'previousTarget': array([19., 22.]), 'currentState': array([ 9.74209785, 15.77072459,  0.        ]), 'targetState': array([19, 22], dtype=int32), 'currentDistance': 11.158522492245396}
done in step count: 11
reward sum = 0.8264930861656206
running average episode reward sum: 0.51642124386002
{'currentTarget': array([19., 22.]), 'previousTarget': array([19., 22.]), 'currentState': array([18.18222684, 21.68151246,  0.        ]), 'targetState': array([19, 22], dtype=int32), 'currentDistance': 0.8776031333742014}
episode index:315
target Thresh 13.317404818712145
target distance 13.0
model initialize at round 315
at step 0:
{'currentTarget': array([25., 12.]), 'previousTarget': array([25., 12.]), 'currentState': array([12.83412039, 15.37167215,  0.        ]), 'targetState': array([25, 12], dtype=int32), 'currentDistance': 12.624452452578142}
done in step count: 23
reward sum = 0.743984234398291
running average episode reward sum: 0.5171413799060272
{'currentTarget': array([25., 12.]), 'previousTarget': array([25., 12.]), 'currentState': array([24.10611603, 11.93339992,  0.        ]), 'targetState': array([25, 12], dtype=int32), 'currentDistance': 0.8963616061946442}
episode index:316
target Thresh 13.33607807570883
target distance 9.0
model initialize at round 316
at step 0:
{'currentTarget': array([26., 28.]), 'previousTarget': array([26., 28.]), 'currentState': array([17.77797055, 25.44588256,  0.        ]), 'targetState': array([26, 28], dtype=int32), 'currentDistance': 8.609604180204602}
done in step count: 13
reward sum = 0.8138967106683763
running average episode reward sum: 0.5180775165961292
{'currentTarget': array([26., 28.]), 'previousTarget': array([26., 28.]), 'currentState': array([25.34493434, 27.99721296,  0.        ]), 'targetState': array([26, 28], dtype=int32), 'currentDistance': 0.6550715845487803}
episode index:317
target Thresh 13.354732668782038
target distance 4.0
model initialize at round 317
at step 0:
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([ 2.21310045, 11.82715285,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 2.9346216335117954}
done in step count: 2
reward sum = 0.9640505042940554
running average episode reward sum: 0.5194799473750535
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.68989858, 9.9905411 , 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.0379472829945364}
episode index:318
target Thresh 13.373368616586355
target distance 8.0
model initialize at round 318
at step 0:
{'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([11.34990221, 10.33041078,  0.        ]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 8.059576106470752}
done in step count: 8
reward sum = 0.8859083622277519
running average episode reward sum: 0.5206286257915196
{'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([8.21311761, 3.81012547, 0.        ]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 0.8376887220007018}
episode index:319
target Thresh 13.391985937757742
target distance 13.0
model initialize at round 319
at step 0:
{'currentTarget': array([13., 28.]), 'previousTarget': array([13., 28.]), 'currentState': array([25.57393837, 19.84192789,  0.        ]), 'targetState': array([13, 28], dtype=int32), 'currentDistance': 14.988597891176143}
done in step count: 23
reward sum = 0.7501364440315624
running average episode reward sum: 0.5213458377235197
{'currentTarget': array([13., 28.]), 'previousTarget': array([13., 28.]), 'currentState': array([13.93829361, 27.66703933,  0.        ]), 'targetState': array([13, 28], dtype=int32), 'currentDistance': 0.9956192523893533}
episode index:320
target Thresh 13.41058465091351
target distance 4.0
model initialize at round 320
at step 0:
{'currentTarget': array([23., 16.]), 'previousTarget': array([23., 16.]), 'currentState': array([19.86315852, 15.80566406,  0.        ]), 'targetState': array([23, 16], dtype=int32), 'currentDistance': 3.1428555330989125}
done in step count: 4
reward sum = 0.9457443124799539
running average episode reward sum: 0.5226679513520444
{'currentTarget': array([23., 16.]), 'previousTarget': array([23., 16.]), 'currentState': array([22.177827  , 15.71587212,  0.        ]), 'targetState': array([23, 16], dtype=int32), 'currentDistance': 0.8698833777730249}
episode index:321
target Thresh 13.429164774652385
target distance 8.0
model initialize at round 321
at step 0:
{'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([17.83173871,  8.19533992,  0.        ]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 7.267242079070784}
done in step count: 11
reward sum = 0.8616378694293146
running average episode reward sum: 0.52372065296098
{'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([24.03549317,  7.18839158,  0.        ]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 0.9827333425094757}
episode index:322
target Thresh 13.447726327554484
target distance 1.0
model initialize at round 322
at step 0:
{'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.60537553,  2.59207988,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.567562595392346}
done in step count: 0
reward sum = 0.9974685648603905
running average episode reward sum: 0.5251873647625263
{'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.60537553,  2.59207988,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.567562595392346}
episode index:323
target Thresh 13.466269328181369
target distance 3.0
model initialize at round 323
at step 0:
{'currentTarget': array([5., 7.]), 'previousTarget': array([5., 7.]), 'currentState': array([2.87999606, 6.48188365, 0.        ]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 2.1823980519730832}
done in step count: 3
reward sum = 0.9612463127023415
running average episode reward sum: 0.5265332257129578
{'currentTarget': array([5., 7.]), 'previousTarget': array([5., 7.]), 'currentState': array([4.40830559, 6.49159253, 0.        ]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 0.7801156553199694}
episode index:324
target Thresh 13.484793795076033
target distance 1.0
model initialize at round 324
at step 0:
{'currentTarget': array([6., 5.]), 'previousTarget': array([6., 5.]), 'currentState': array([5.33433056, 4.18686573, 0.        ]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 1.050858288569069}
done in step count: 0
reward sum = 0.9992665213795099
running average episode reward sum: 0.5279877896996241
{'currentTarget': array([6., 5.]), 'previousTarget': array([6., 5.]), 'currentState': array([5.33433056, 4.18686573, 0.        ]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 1.050858288569069}
episode index:325
target Thresh 13.503299746762952
target distance 7.0
model initialize at round 325
at step 0:
{'currentTarget': array([17., 16.]), 'previousTarget': array([17., 16.]), 'currentState': array([23.48199451, 12.20745514,  0.        ]), 'targetState': array([17, 16], dtype=int32), 'currentDistance': 7.50996999418634}
done in step count: 10
reward sum = 0.8793382770272481
running average episode reward sum: 0.5290655519306904
{'currentTarget': array([17., 16.]), 'previousTarget': array([17., 16.]), 'currentState': array([17.59125113, 15.96864344,  0.        ]), 'targetState': array([17, 16], dtype=int32), 'currentDistance': 0.592082036739636}
episode index:326
target Thresh 13.521787201748072
target distance 4.0
model initialize at round 326
at step 0:
{'currentTarget': array([15., 28.]), 'previousTarget': array([15., 28.]), 'currentState': array([11.89074427, 26.05300868,  0.        ]), 'targetState': array([15, 28], dtype=int32), 'currentDistance': 3.6685482712919972}
done in step count: 3
reward sum = 0.9490623152839098
running average episode reward sum: 0.5303499457024128
{'currentTarget': array([15., 28.]), 'previousTarget': array([15., 28.]), 'currentState': array([14.01503187, 27.38633844,  0.        ]), 'targetState': array([15, 28], dtype=int32), 'currentDistance': 1.160492445454346}
episode index:327
target Thresh 13.54025617851886
target distance 11.0
model initialize at round 327
at step 0:
{'currentTarget': array([ 2., 18.]), 'previousTarget': array([ 2., 18.]), 'currentState': array([9.10765301, 8.92073345, 0.        ]), 'targetState': array([ 2, 18], dtype=int32), 'currentDistance': 11.530473204475678}
done in step count: 13
reward sum = 0.7992356700209919
running average episode reward sum: 0.5311697192521646
{'currentTarget': array([ 2., 18.]), 'previousTarget': array([ 2., 18.]), 'currentState': array([ 2.80202509, 17.78151152,  0.        ]), 'targetState': array([ 2, 18], dtype=int32), 'currentDistance': 0.8312529484759191}
episode index:328
target Thresh 13.558706695544284
target distance 10.0
model initialize at round 328
at step 0:
{'currentTarget': array([16., 28.]), 'previousTarget': array([16., 28.]), 'currentState': array([ 6.31015638, 24.49675572,  0.        ]), 'targetState': array([16, 28], dtype=int32), 'currentDistance': 10.303678467650663}
done in step count: 13
reward sum = 0.7950027622498115
running average episode reward sum: 0.5319716433950146
{'currentTarget': array([16., 28.]), 'previousTarget': array([16., 28.]), 'currentState': array([15.33945712, 27.57899529,  0.        ]), 'targetState': array([16, 28], dtype=int32), 'currentDistance': 0.7833018938177968}
episode index:329
target Thresh 13.577138771274868
target distance 10.0
model initialize at round 329
at step 0:
{'currentTarget': array([ 6., 26.]), 'previousTarget': array([ 6., 26.]), 'currentState': array([15.40354365, 19.93907708,  0.        ]), 'targetState': array([ 6, 26], dtype=int32), 'currentDistance': 11.18755647355268}
done in step count: 13
reward sum = 0.8294515242465073
running average episode reward sum: 0.532873097579413
{'currentTarget': array([ 6., 26.]), 'previousTarget': array([ 6., 26.]), 'currentState': array([ 6.53944981, 25.95680365,  0.        ]), 'targetState': array([ 6, 26], dtype=int32), 'currentDistance': 0.5411765176456282}
episode index:330
target Thresh 13.595552424142689
target distance 9.0
model initialize at round 330
at step 0:
{'currentTarget': array([27.,  2.]), 'previousTarget': array([27.,  2.]), 'currentState': array([19.364786  ,  9.15177488,  0.        ]), 'targetState': array([27,  2], dtype=int32), 'currentDistance': 10.461566652329443}
done in step count: 9
reward sum = 0.8276230829194668
running average episode reward sum: 0.533763580918809
{'currentTarget': array([27.,  2.]), 'previousTarget': array([27.,  2.]), 'currentState': array([26.06030583,  1.72319148,  0.        ]), 'targetState': array([27,  2], dtype=int32), 'currentDistance': 0.9796162922496694}
episode index:331
target Thresh 13.613947672561398
target distance 13.0
model initialize at round 331
at step 0:
{'currentTarget': array([17., 16.]), 'previousTarget': array([17., 16.]), 'currentState': array([ 5.49862808, 27.05568457,  0.        ]), 'targetState': array([17, 16], dtype=int32), 'currentDistance': 15.953360689163933}
done in step count: 12
reward sum = 0.748338289877559
running average episode reward sum: 0.5344098902831426
{'currentTarget': array([17., 16.]), 'previousTarget': array([17., 16.]), 'currentState': array([16.21522558, 15.62290703,  0.        ]), 'targetState': array([17, 16], dtype=int32), 'currentDistance': 0.870672156595532}
episode index:332
target Thresh 13.632324534926251
target distance 1.0
model initialize at round 332
at step 0:
{'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([25.2993806 ,  8.22661796,  0.        ]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 1.0435455522087707}
done in step count: 0
reward sum = 0.9992950777788079
running average episode reward sum: 0.5358059418972437
{'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([25.2993806 ,  8.22661796,  0.        ]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 1.0435455522087707}
episode index:333
target Thresh 13.650683029614104
target distance 6.0
model initialize at round 333
at step 0:
{'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([18.97507478, 13.7553215 ,  0.        ]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 5.183470408574271}
done in step count: 3
reward sum = 0.9245706872308205
running average episode reward sum: 0.5369699082006377
{'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([16.82024647, 17.37371904,  0.        ]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 1.0320039304046924}
episode index:334
target Thresh 13.669023174983462
target distance 7.0
model initialize at round 334
at step 0:
{'currentTarget': array([10., 15.]), 'previousTarget': array([10., 15.]), 'currentState': array([ 3.91723913, 16.00236582,  0.        ]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 6.164796593131137}
done in step count: 6
reward sum = 0.9088186558281574
running average episode reward sum: 0.5380799044622124
{'currentTarget': array([10., 15.]), 'previousTarget': array([10., 15.]), 'currentState': array([ 9.30787224, 14.6564411 ,  0.        ]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 0.7727053500917801}
episode index:335
target Thresh 13.687344989374466
target distance 3.0
model initialize at round 335
at step 0:
{'currentTarget': array([ 6., 20.]), 'previousTarget': array([ 6., 20.]), 'currentState': array([ 6.14394607, 21.83961546,  0.        ]), 'targetState': array([ 6, 20], dtype=int32), 'currentDistance': 1.8452386095631992}
done in step count: 1
reward sum = 0.9796098146077971
running average episode reward sum: 0.5393939815757409
{'currentTarget': array([ 6., 20.]), 'previousTarget': array([ 6., 20.]), 'currentState': array([ 6.21561377, 20.99530029,  0.        ]), 'targetState': array([ 6, 20], dtype=int32), 'currentDistance': 1.0183869450856258}
episode index:336
target Thresh 13.705648491108935
target distance 11.0
model initialize at round 336
at step 0:
{'currentTarget': array([27., 25.]), 'previousTarget': array([27., 25.]), 'currentState': array([16.51882768, 18.0671339 ,  0.        ]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 12.566606764800627}
done in step count: 11
reward sum = 0.8036503835421415
running average episode reward sum: 0.540178125201754
{'currentTarget': array([27., 25.]), 'previousTarget': array([27., 25.]), 'currentState': array([26.09668022, 24.55273975,  0.        ]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 1.0079823139249142}
episode index:337
target Thresh 13.723933698490367
target distance 7.0
model initialize at round 337
at step 0:
{'currentTarget': array([18.,  7.]), 'previousTarget': array([18.,  7.]), 'currentState': array([14.5039205 , 12.16909945,  0.        ]), 'targetState': array([18,  7], dtype=int32), 'currentDistance': 6.24036545619374}
done in step count: 4
reward sum = 0.9019642982406941
running average episode reward sum: 0.5412484984947686
{'currentTarget': array([18.,  7.]), 'previousTarget': array([18.,  7.]), 'currentState': array([17.15568525,  7.34242123,  0.        ]), 'targetState': array([18,  7], dtype=int32), 'currentDistance': 0.9111090522518017}
episode index:338
target Thresh 13.742200629803975
target distance 13.0
model initialize at round 338
at step 0:
{'currentTarget': array([20., 26.]), 'previousTarget': array([20., 26.]), 'currentState': array([ 7.67815119, 15.60463446,  0.        ]), 'targetState': array([20, 26], dtype=int32), 'currentDistance': 16.121153268952625}
done in step count: 22
reward sum = 0.7197136022125689
running average episode reward sum: 0.5417749442284494
{'currentTarget': array([20., 26.]), 'previousTarget': array([20., 26.]), 'currentState': array([19.26103571, 25.39982038,  0.        ]), 'targetState': array([20, 26], dtype=int32), 'currentDistance': 0.9519893931344636}
episode index:339
target Thresh 13.76044930331669
target distance 12.0
model initialize at round 339
at step 0:
{'currentTarget': array([ 6., 16.]), 'previousTarget': array([ 6., 16.]), 'currentState': array([ 8.87470409, 26.884866  ,  0.        ]), 'targetState': array([ 6, 16], dtype=int32), 'currentDistance': 11.258074055620403}
done in step count: 11
reward sum = 0.8400205740627981
running average episode reward sum: 0.5426521372573739
{'currentTarget': array([ 6., 16.]), 'previousTarget': array([ 6., 16.]), 'currentState': array([ 6.41502126, 16.62766761,  0.        ]), 'targetState': array([ 6, 16], dtype=int32), 'currentDistance': 0.7524687807996877}
episode index:340
target Thresh 13.77867973727719
target distance 8.0
model initialize at round 340
at step 0:
{'currentTarget': array([ 2., 18.]), 'previousTarget': array([ 2., 18.]), 'currentState': array([ 3.86735375, 24.99457359,  0.        ]), 'targetState': array([ 2, 18], dtype=int32), 'currentDistance': 7.239549004317245}
done in step count: 7
reward sum = 0.897642372376704
running average episode reward sum: 0.5436931643398354
{'currentTarget': array([ 2., 18.]), 'previousTarget': array([ 2., 18.]), 'currentState': array([ 2.76890191, 18.6105293 ,  0.        ]), 'targetState': array([ 2, 18], dtype=int32), 'currentDistance': 0.9818126974837967}
episode index:341
target Thresh 13.79689194991591
target distance 7.0
model initialize at round 341
at step 0:
{'currentTarget': array([25., 12.]), 'previousTarget': array([25., 12.]), 'currentState': array([19.17044067, 13.52665007,  0.        ]), 'targetState': array([25, 12], dtype=int32), 'currentDistance': 6.026144901766068}
done in step count: 5
reward sum = 0.9072557048824722
running average episode reward sum: 0.5447562127039951
{'currentTarget': array([25., 12.]), 'previousTarget': array([25., 12.]), 'currentState': array([24.36545104, 11.49754086,  0.        ]), 'targetState': array([25, 12], dtype=int32), 'currentDistance': 0.8093933353546269}
episode index:342
target Thresh 13.81508595944506
target distance 6.0
model initialize at round 342
at step 0:
{'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([22.99959362,  7.6766901 ,  0.        ]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 4.43754677683253}
done in step count: 4
reward sum = 0.9286824348134488
running average episode reward sum: 0.5458755311358011
{'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([23.39232942, 11.23503554,  0.        ]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 0.9769514623576236}
episode index:343
target Thresh 13.833261784058656
target distance 13.0
model initialize at round 343
at step 0:
{'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([24.65962473, 14.95479   ,  0.        ]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 11.635700114788508}
done in step count: 7
reward sum = 0.8151242821157986
running average episode reward sum: 0.5466582309933011
{'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([21.45424451, 25.69772404,  0.        ]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 0.5456270056891551}
episode index:344
target Thresh 13.851419441932524
target distance 6.0
model initialize at round 344
at step 0:
{'currentTarget': array([21., 20.]), 'previousTarget': array([21., 20.]), 'currentState': array([26.1480301 , 20.61135721,  0.        ]), 'targetState': array([21, 20], dtype=int32), 'currentDistance': 5.184204044445851}
done in step count: 6
reward sum = 0.9207867185788033
running average episode reward sum: 0.5477426613920996
{'currentTarget': array([21., 20.]), 'previousTarget': array([21., 20.]), 'currentState': array([21.8844173 , 20.09466811,  0.        ]), 'targetState': array([21, 20], dtype=int32), 'currentDistance': 0.8894695071015677}
episode index:345
target Thresh 13.86955895122432
target distance 9.0
model initialize at round 345
at step 0:
{'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([13.834512  , 13.16893148,  0.        ]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 8.207671364800191}
done in step count: 7
reward sum = 0.8743767003314867
running average episode reward sum: 0.5486866904063752
{'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([21.11780238, 13.56277198,  0.        ]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 0.9846019414138413}
episode index:346
target Thresh 13.887680330073554
target distance 6.0
model initialize at round 346
at step 0:
{'currentTarget': array([ 5., 16.]), 'previousTarget': array([ 5., 16.]), 'currentState': array([ 6.1152961 , 20.73426223,  0.        ]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 4.863858984845735}
done in step count: 4
reward sum = 0.9333099120997413
running average episode reward sum: 0.5497951146763849
{'currentTarget': array([ 5., 16.]), 'previousTarget': array([ 5., 16.]), 'currentState': array([ 5.90357272, 16.86544633,  0.        ]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 1.2511758533798156}
episode index:347
target Thresh 13.905783596601612
target distance 11.0
model initialize at round 347
at step 0:
{'currentTarget': array([24., 20.]), 'previousTarget': array([24., 20.]), 'currentState': array([23.29997361, 10.83869982,  0.        ]), 'targetState': array([24, 20], dtype=int32), 'currentDistance': 9.188006202751376}
done in step count: 7
reward sum = 0.8539208111194341
running average episode reward sum: 0.5506690390914512
{'currentTarget': array([24., 20.]), 'previousTarget': array([24., 20.]), 'currentState': array([23.54542468, 19.27092063,  0.        ]), 'targetState': array([24, 20], dtype=int32), 'currentDistance': 0.859183006493796}
episode index:348
target Thresh 13.92386876891176
target distance 8.0
model initialize at round 348
at step 0:
{'currentTarget': array([5., 2.]), 'previousTarget': array([5., 2.]), 'currentState': array([10.34702277,  9.49728799,  0.        ]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 9.208690444763173}
done in step count: 10
reward sum = 0.8703232280474411
running average episode reward sum: 0.551584953672987
{'currentTarget': array([5., 2.]), 'previousTarget': array([5., 2.]), 'currentState': array([5.94445258, 2.86357719, 0.        ]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 1.2797485069498067}
episode index:349
target Thresh 13.941935865089167
target distance 13.0
model initialize at round 349
at step 0:
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([ 8.99474108, 16.02675295,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 13.063802398174413}
done in step count: 7
reward sum = 0.7916530651827606
running average episode reward sum: 0.5522708625630149
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.371548  ,  5.29016751,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.6922059695156076}
episode index:350
target Thresh 13.959984903200937
target distance 6.0
model initialize at round 350
at step 0:
{'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([18.03447819,  7.18980956,  0.        ]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 6.496992460005916}
done in step count: 5
reward sum = 0.880807186335226
running average episode reward sum: 0.5532068634854429
{'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([22.45701993,  2.69545993,  0.        ]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 0.6225528207974405}
episode index:351
target Thresh 13.978015901296104
target distance 9.0
model initialize at round 351
at step 0:
{'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([4.03902102, 8.17695844, 0.        ]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 8.571478946917864}
done in step count: 7
reward sum = 0.8710108216337309
running average episode reward sum: 0.5541097156392732
{'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([11.43602699,  4.58062867,  0.        ]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 0.7028071338120626}
episode index:352
target Thresh 13.99602887740567
target distance 10.0
model initialize at round 352
at step 0:
{'currentTarget': array([17., 19.]), 'previousTarget': array([17., 19.]), 'currentState': array([26.21649086, 18.77328165,  0.        ]), 'targetState': array([17, 19], dtype=int32), 'currentDistance': 9.219278988667265}
done in step count: 12
reward sum = 0.8532348392718624
running average episode reward sum: 0.5549570955929066
{'currentTarget': array([17., 19.]), 'previousTarget': array([17., 19.]), 'currentState': array([17.9274199 , 18.86649044,  0.        ]), 'targetState': array([17, 19], dtype=int32), 'currentDistance': 0.9369805089078631}
episode index:353
target Thresh 14.014023849542614
target distance 8.0
model initialize at round 353
at step 0:
{'currentTarget': array([ 5., 26.]), 'previousTarget': array([ 5., 26.]), 'currentState': array([12.41459155, 23.67264453,  0.        ]), 'targetState': array([ 5, 26], dtype=int32), 'currentDistance': 7.7712773318830815}
done in step count: 10
reward sum = 0.8752482625723847
running average episode reward sum: 0.5558618729007583
{'currentTarget': array([ 5., 26.]), 'previousTarget': array([ 5., 26.]), 'currentState': array([ 5.78452322, 26.31144409,  0.        ]), 'targetState': array([ 5, 26], dtype=int32), 'currentDistance': 0.8440818082893363}
episode index:354
target Thresh 14.032000835701911
target distance 6.0
model initialize at round 354
at step 0:
{'currentTarget': array([18., 22.]), 'previousTarget': array([18., 22.]), 'currentState': array([14.84763026, 17.40019536,  0.        ]), 'targetState': array([18, 22], dtype=int32), 'currentDistance': 5.576346266792413}
done in step count: 5
reward sum = 0.9096456700956065
running average episode reward sum: 0.5568584469773635
{'currentTarget': array([18., 22.]), 'previousTarget': array([18., 22.]), 'currentState': array([17.58931522, 21.19715178,  0.        ]), 'targetState': array([18, 22], dtype=int32), 'currentDistance': 0.9017911355122987}
episode index:355
target Thresh 14.049959853860546
target distance 11.0
model initialize at round 355
at step 0:
{'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([13.28409201, 14.96182525,  0.        ]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 10.002102135308647}
done in step count: 8
reward sum = 0.8227385097235284
running average episode reward sum: 0.5576053010862011
{'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([ 9.70906368, 24.01772454,  0.        ]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 0.7092851716728052}
episode index:356
target Thresh 14.06790092197754
target distance 10.0
model initialize at round 356
at step 0:
{'currentTarget': array([ 5., 15.]), 'previousTarget': array([ 5., 15.]), 'currentState': array([14.4995423 , 12.95960639,  0.        ]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 9.716198325421379}
done in step count: 13
reward sum = 0.8453032222924551
running average episode reward sum: 0.5584111776161906
{'currentTarget': array([ 5., 15.]), 'previousTarget': array([ 5., 15.]), 'currentState': array([ 5.99517256, 15.14522997,  0.        ]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 1.0057137602063906}
episode index:357
target Thresh 14.085824057993957
target distance 13.0
model initialize at round 357
at step 0:
{'currentTarget': array([10.,  6.]), 'previousTarget': array([10.,  6.]), 'currentState': array([19.54219633, 18.26837415,  0.        ]), 'targetState': array([10,  6], dtype=int32), 'currentDistance': 15.542410206521057}
done in step count: 20
reward sum = 0.7631066134446497
running average episode reward sum: 0.5589829525766054
{'currentTarget': array([10.,  6.]), 'previousTarget': array([10.,  6.]), 'currentState': array([10.77741137,  6.45314887,  0.        ]), 'targetState': array([10,  6], dtype=int32), 'currentDistance': 0.8998401749066902}
episode index:358
target Thresh 14.10372927983294
target distance 8.0
model initialize at round 358
at step 0:
{'currentTarget': array([ 8., 28.]), 'previousTarget': array([ 8., 28.]), 'currentState': array([ 3.87148207, 20.75246853,  0.        ]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 8.34094555338339}
done in step count: 7
reward sum = 0.8802876341715665
running average episode reward sum: 0.5598779516896832
{'currentTarget': array([ 8., 28.]), 'previousTarget': array([ 8., 28.]), 'currentState': array([ 7.63612128, 27.3788572 ,  0.        ]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 0.7198792303197694}
episode index:359
target Thresh 14.121616605399716
target distance 13.0
model initialize at round 359
at step 0:
{'currentTarget': array([19., 11.]), 'previousTarget': array([19., 11.]), 'currentState': array([22.0293934 , 22.60423756,  0.        ]), 'targetState': array([19, 11], dtype=int32), 'currentDistance': 11.993146112145592}
done in step count: 11
reward sum = 0.8299013296915542
running average episode reward sum: 0.5606280166285773
{'currentTarget': array([19., 11.]), 'previousTarget': array([19., 11.]), 'currentState': array([19.77301493, 11.79363924,  0.        ]), 'targetState': array([19, 11], dtype=int32), 'currentDistance': 1.10788778167852}
episode index:360
target Thresh 14.139486052581606
target distance 12.0
model initialize at round 360
at step 0:
{'currentTarget': array([6., 3.]), 'previousTarget': array([6., 3.]), 'currentState': array([17.26639497, 12.20970023,  0.        ]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 14.551640249483148}
done in step count: 14
reward sum = 0.7989997675497259
running average episode reward sum: 0.5612883261879157
{'currentTarget': array([6., 3.]), 'previousTarget': array([6., 3.]), 'currentState': array([6.92015371, 3.87202495, 0.        ]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 1.2677185666728181}
episode index:361
target Thresh 14.157337639248059
target distance 10.0
model initialize at round 361
at step 0:
{'currentTarget': array([26., 24.]), 'previousTarget': array([26., 24.]), 'currentState': array([16.72032875, 18.87454033,  0.        ]), 'targetState': array([26, 24], dtype=int32), 'currentDistance': 10.60106765284776}
done in step count: 9
reward sum = 0.843813919378229
running average episode reward sum: 0.5620687836276679
{'currentTarget': array([26., 24.]), 'previousTarget': array([26., 24.]), 'currentState': array([25.28732443, 23.45954973,  0.        ]), 'targetState': array([26, 24], dtype=int32), 'currentDistance': 0.8944232608587656}
episode index:362
target Thresh 14.17517138325066
target distance 9.0
model initialize at round 362
at step 0:
{'currentTarget': array([21., 13.]), 'previousTarget': array([21., 13.]), 'currentState': array([21.01242538, 20.28954029,  0.        ]), 'targetState': array([21, 13], dtype=int32), 'currentDistance': 7.289550880663307}
done in step count: 5
reward sum = 0.8924515318008791
running average episode reward sum: 0.5629789289394398
{'currentTarget': array([21., 13.]), 'previousTarget': array([21., 13.]), 'currentState': array([21.43529918, 13.77945757,  0.        ]), 'targetState': array([21, 13], dtype=int32), 'currentDistance': 0.892770672965161}
episode index:363
target Thresh 14.192987302423166
target distance 14.0
model initialize at round 363
at step 0:
{'currentTarget': array([21., 23.]), 'previousTarget': array([21., 23.]), 'currentState': array([ 7.70621365, 19.9502278 ,  0.        ]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 13.639129954925787}
done in step count: 13
reward sum = 0.773664462036776
running average episode reward sum: 0.5635577353490479
{'currentTarget': array([21., 23.]), 'previousTarget': array([21., 23.]), 'currentState': array([20.33924681, 22.66106114,  0.        ]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 0.7426131755400126}
episode index:364
target Thresh 14.210785414581487
target distance 4.0
model initialize at round 364
at step 0:
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.08865182, 4.58847845, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 2.5899961024703133}
done in step count: 2
reward sum = 0.9611168691528268
running average episode reward sum: 0.5646469384553595
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.33325267, 2.74906838, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.819854119952504}
episode index:365
target Thresh 14.228565737523741
target distance 12.0
model initialize at round 365
at step 0:
{'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([15.04170181, 20.65883064,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 11.399387010242615}
done in step count: 11
reward sum = 0.8374940155800052
running average episode reward sum: 0.56539242227264
{'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([11.95715731, 10.66009361,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 1.162701032704399}
episode index:366
target Thresh 14.246328289030249
target distance 8.0
model initialize at round 366
at step 0:
{'currentTarget': array([ 8., 28.]), 'previousTarget': array([ 8., 28.]), 'currentState': array([ 6.31706506, 21.44263554,  0.        ]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 6.7698817362634935}
done in step count: 5
reward sum = 0.9037938604724594
running average episode reward sum: 0.5663144970361272
{'currentTarget': array([ 8., 28.]), 'previousTarget': array([ 8., 28.]), 'currentState': array([ 7.62983859, 27.15521222,  0.        ]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 0.9223263304253289}
episode index:367
target Thresh 14.264073086863572
target distance 3.0
model initialize at round 367
at step 0:
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([13.97421503,  8.39007819,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 2.0629992084164708}
done in step count: 2
reward sum = 0.9700416131440615
running average episode reward sum: 0.5674115815907683
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.0852474 ,  7.98371023,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.9148976339175803}
episode index:368
target Thresh 14.2818001487685
target distance 12.0
model initialize at round 368
at step 0:
{'currentTarget': array([ 7., 26.]), 'previousTarget': array([ 7., 26.]), 'currentState': array([17.99041688, 18.77425086,  0.        ]), 'targetState': array([ 7, 26], dtype=int32), 'currentDistance': 13.152973574655965}
done in step count: 13
reward sum = 0.7896474165963171
running average episode reward sum: 0.5680138467262847
{'currentTarget': array([ 7., 26.]), 'previousTarget': array([ 7., 26.]), 'currentState': array([ 7.6141122 , 26.27086939,  0.        ]), 'targetState': array([ 7, 26], dtype=int32), 'currentDistance': 0.671195961071956}
episode index:369
target Thresh 14.299509492472101
target distance 11.0
model initialize at round 369
at step 0:
{'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([5.43102771, 9.50371832, 0.        ]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 13.560603884118754}
done in step count: 99
reward sum = -1.103703590059909
running average episode reward sum: 0.5634956914917274
{'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([ -61.79892138, -138.54234952,    0.        ]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 174.8089796313465}
episode index:370
target Thresh 14.317201135683721
target distance 8.0
model initialize at round 370
at step 0:
{'currentTarget': array([16., 29.]), 'previousTarget': array([16., 29.]), 'currentState': array([23.57353562, 26.93172163,  0.        ]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 7.850873660322567}
done in step count: 30
reward sum = 0.48249426161503395
running average episode reward sum: 0.563277358796642
{'currentTarget': array([16., 29.]), 'previousTarget': array([16., 29.]), 'currentState': array([16.12848495, 28.71103024,  0.        ]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 0.31624658285270496}
episode index:371
target Thresh 14.334875096095
target distance 7.0
model initialize at round 371
at step 0:
{'currentTarget': array([25., 27.]), 'previousTarget': array([25., 27.]), 'currentState': array([25.10505784, 21.6938957 ,  0.        ]), 'targetState': array([25, 27], dtype=int32), 'currentDistance': 5.307144243077544}
done in step count: 15
reward sum = 0.8213060166026885
running average episode reward sum: 0.5639709842208517
{'currentTarget': array([25., 27.]), 'previousTarget': array([25., 27.]), 'currentState': array([25.28700724, 26.12631482,  0.        ]), 'targetState': array([25, 27], dtype=int32), 'currentDistance': 0.9196189118918112}
episode index:372
target Thresh 14.352531391379905
target distance 9.0
model initialize at round 372
at step 0:
{'currentTarget': array([10., 19.]), 'previousTarget': array([10., 19.]), 'currentState': array([ 4.61400908, 11.33917332,  0.        ]), 'targetState': array([10, 19], dtype=int32), 'currentDistance': 9.364676376309339}
done in step count: 7
reward sum = 0.8495778699079283
running average episode reward sum: 0.5647366863272514
{'currentTarget': array([10., 19.]), 'previousTarget': array([10., 19.]), 'currentState': array([10.4436627 , 18.33997995,  0.        ]), 'targetState': array([10, 19], dtype=int32), 'currentDistance': 0.7952754606600442}
episode index:373
target Thresh 14.370170039194733
target distance 8.0
model initialize at round 373
at step 0:
{'currentTarget': array([ 4., 26.]), 'previousTarget': array([ 4., 26.]), 'currentState': array([ 5.92875433, 19.85530245,  0.        ]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 6.440295118072418}
done in step count: 6
reward sum = 0.8796678877799036
running average episode reward sum: 0.5655787483632211
{'currentTarget': array([ 4., 26.]), 'previousTarget': array([ 4., 26.]), 'currentState': array([ 3.87379304, 25.1647049 ,  0.        ]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 0.8447757650975743}
episode index:374
target Thresh 14.387791057178124
target distance 11.0
model initialize at round 374
at step 0:
{'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([10.07141063, 11.82373244,  0.        ]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 11.77729142949553}
done in step count: 41
reward sum = 0.6192979770155175
running average episode reward sum: 0.5657219996396272
{'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([15.62783948, 21.01402989,  0.        ]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 1.0538693032020252}
episode index:375
target Thresh 14.405394462951111
target distance 10.0
model initialize at round 375
at step 0:
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([23.72577226, 11.83234459,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 10.495491651038611}
done in step count: 14
reward sum = 0.8205303954234116
running average episode reward sum: 0.5663996815433074
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.9753117 ,  6.62423915,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 1.1579755686850262}
episode index:376
target Thresh 14.422980274117094
target distance 10.0
model initialize at round 376
at step 0:
{'currentTarget': array([ 2., 19.]), 'previousTarget': array([ 2., 19.]), 'currentState': array([11.56791982, 26.96515298,  0.        ]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 12.449447845975007}
done in step count: 13
reward sum = 0.8223576423366825
running average episode reward sum: 0.5670786151263137
{'currentTarget': array([ 2., 19.]), 'previousTarget': array([ 2., 19.]), 'currentState': array([ 2.84585613, 19.70575249,  0.        ]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 1.1016166178679563}
episode index:377
target Thresh 14.440548508261884
target distance 12.0
model initialize at round 377
at step 0:
{'currentTarget': array([14., 20.]), 'previousTarget': array([14., 20.]), 'currentState': array([ 3.69193602, 10.11572351,  0.        ]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 14.281285123078845}
done in step count: 12
reward sum = 0.7627288483898412
running average episode reward sum: 0.5675962083360057
{'currentTarget': array([14., 20.]), 'previousTarget': array([14., 20.]), 'currentState': array([13.57783598, 19.19927827,  0.        ]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 0.9051948641708649}
episode index:378
target Thresh 14.45809918295372
target distance 8.0
model initialize at round 378
at step 0:
{'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([25.98596168, 11.54274279,  0.        ]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 7.154279547650706}
done in step count: 8
reward sum = 0.8910189175790489
running average episode reward sum: 0.568449566407887
{'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([19.98956424,  9.53821395,  0.        ]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 1.0920090406142555}
episode index:379
target Thresh 14.475632315743276
target distance 10.0
model initialize at round 379
at step 0:
{'currentTarget': array([24., 26.]), 'previousTarget': array([24., 26.]), 'currentState': array([25.29199111, 17.78393888,  0.        ]), 'targetState': array([24, 26], dtype=int32), 'currentDistance': 8.317024785944374}
done in step count: 8
reward sum = 0.8580774017433875
running average episode reward sum: 0.5692117449219278
{'currentTarget': array([24., 26.]), 'previousTarget': array([24., 26.]), 'currentState': array([23.61999382, 25.19680443,  0.        ]), 'targetState': array([24, 26], dtype=int32), 'currentDistance': 0.8885537747108234}
episode index:380
target Thresh 14.49314792416369
target distance 8.0
model initialize at round 380
at step 0:
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([10.90635884,  4.49717212,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 7.0679759511168205}
done in step count: 8
reward sum = 0.881018699731464
running average episode reward sum: 0.5700301358794332
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.03071436, 5.15184981, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.848706139076357}
episode index:381
target Thresh 14.51064602573057
target distance 11.0
model initialize at round 381
at step 0:
{'currentTarget': array([14., 29.]), 'previousTarget': array([14., 29.]), 'currentState': array([10.53644907, 18.81148493,  0.        ]), 'targetState': array([14, 29], dtype=int32), 'currentDistance': 10.76113489760104}
done in step count: 8
reward sum = 0.834988461389021
running average episode reward sum: 0.5707237440613955
{'currentTarget': array([14., 29.]), 'previousTarget': array([14., 29.]), 'currentState': array([13.59927665, 28.04018879,  0.        ]), 'targetState': array([14, 29], dtype=int32), 'currentDistance': 1.040104209425213}
episode index:382
target Thresh 14.528126637942012
target distance 3.0
model initialize at round 382
at step 0:
{'currentTarget': array([5., 6.]), 'previousTarget': array([5., 6.]), 'currentState': array([7.23382062, 4.17847058, 0.        ]), 'targetState': array([5, 6], dtype=int32), 'currentDistance': 2.8823469553994245}
done in step count: 3
reward sum = 0.958830807854112
running average episode reward sum: 0.5717370784316114
{'currentTarget': array([5., 6.]), 'previousTarget': array([5., 6.]), 'currentState': array([5.26843572, 5.23721635, 0.        ]), 'targetState': array([5, 6], dtype=int32), 'currentDistance': 0.808638748426847}
episode index:383
target Thresh 14.545589778278643
target distance 13.0
model initialize at round 383
at step 0:
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([ 6.1932855, 14.7506423,  0.       ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 12.476427280968991}
done in step count: 13
reward sum = 0.8149691131628226
running average episode reward sum: 0.5723704951887238
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.61081214, 3.59195536, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.8505895755813109}
episode index:384
target Thresh 14.563035464203594
target distance 7.0
model initialize at round 384
at step 0:
{'currentTarget': array([13., 18.]), 'previousTarget': array([13., 18.]), 'currentState': array([ 9.68454659, 11.40091896,  0.        ]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 7.385127072692369}
done in step count: 6
reward sum = 0.8774836693081781
running average episode reward sum: 0.5731629969396835
{'currentTarget': array([13., 18.]), 'previousTarget': array([13., 18.]), 'currentState': array([12.70961104, 17.10955164,  0.        ]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 0.9366023879188864}
episode index:385
target Thresh 14.580463713162555
target distance 4.0
model initialize at round 385
at step 0:
{'currentTarget': array([19., 16.]), 'previousTarget': array([19., 16.]), 'currentState': array([22.06747949, 17.46053505,  0.        ]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 3.397439191476278}
done in step count: 3
reward sum = 0.9542379656215183
running average episode reward sum: 0.5741502377911909
{'currentTarget': array([19., 16.]), 'previousTarget': array([19., 16.]), 'currentState': array([19.85526961, 16.32740968,  0.        ]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 0.9157964888039308}
episode index:386
target Thresh 14.597874542583781
target distance 14.0
model initialize at round 386
at step 0:
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([10.51753247, 23.39442968,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 13.180075961870319}
done in step count: 11
reward sum = 0.8079177526363219
running average episode reward sum: 0.5747542882171472
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.58338287, 11.52014822,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.6664262880650298}
episode index:387
target Thresh 14.615267969878097
target distance 13.0
model initialize at round 387
at step 0:
{'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([21.17777234, 16.67890644,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 13.212180999946126}
done in step count: 15
reward sum = 0.8004497717777563
running average episode reward sum: 0.5753359776077673
{'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.87070814,  5.99761248,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.3241462585806172}
episode index:388
target Thresh 14.632644012438934
target distance 13.0
model initialize at round 388
at step 0:
{'currentTarget': array([ 2., 19.]), 'previousTarget': array([ 2., 19.]), 'currentState': array([13.6699425 , 19.15141231,  0.        ]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 11.670924710516305}
done in step count: 13
reward sum = 0.7818097794846515
running average episode reward sum: 0.575866758589456
{'currentTarget': array([ 2., 19.]), 'previousTarget': array([ 2., 19.]), 'currentState': array([ 1.96620393, 18.22493282,  0.        ]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 0.7758036571084671}
episode index:389
target Thresh 14.650002687642338
target distance 7.0
model initialize at round 389
at step 0:
{'currentTarget': array([14., 18.]), 'previousTarget': array([14., 18.]), 'currentState': array([ 8.58369851, 14.08877245,  0.        ]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 6.680869913283831}
done in step count: 4
reward sum = 0.9049050028300247
running average episode reward sum: 0.5767104463952011
{'currentTarget': array([14., 18.]), 'previousTarget': array([14., 18.]), 'currentState': array([13.1360856 , 17.12641737,  0.        ]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 1.228614953884683}
episode index:390
target Thresh 14.667344012846986
target distance 4.0
model initialize at round 390
at step 0:
{'currentTarget': array([23., 14.]), 'previousTarget': array([23., 14.]), 'currentState': array([24.71014994, 17.24447745,  0.        ]), 'targetState': array([23, 14], dtype=int32), 'currentDistance': 3.6675941377741745}
done in step count: 4
reward sum = 0.9480835120319645
running average episode reward sum: 0.5776602496321237
{'currentTarget': array([23., 14.]), 'previousTarget': array([23., 14.]), 'currentState': array([23.48339084, 14.79503822,  0.        ]), 'targetState': array([23, 14], dtype=int32), 'currentDistance': 0.9304582089638908}
episode index:391
target Thresh 14.684668005394197
target distance 8.0
model initialize at round 391
at step 0:
{'currentTarget': array([18., 13.]), 'previousTarget': array([18., 13.]), 'currentState': array([11.44422174, 13.17151804,  0.        ]), 'targetState': array([18, 13], dtype=int32), 'currentDistance': 6.558021584197887}
done in step count: 6
reward sum = 0.8943448858424199
running average episode reward sum: 0.5784681186020479
{'currentTarget': array([18., 13.]), 'previousTarget': array([18., 13.]), 'currentState': array([17.29208055, 13.02234837,  0.        ]), 'targetState': array([18, 13], dtype=int32), 'currentDistance': 0.7082721197056622}
episode index:392
target Thresh 14.701974682607972
target distance 3.0
model initialize at round 392
at step 0:
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([ 4.3873651 , 11.39364386,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 3.3806867709850508}
done in step count: 4
reward sum = 0.9482369007874956
running average episode reward sum: 0.5794090060885249
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.69098136, 9.38427076, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.7906448333637408}
episode index:393
target Thresh 14.719264061794991
target distance 9.0
model initialize at round 393
at step 0:
{'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([21.62827572, 16.79034865,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 10.903216157091688}
done in step count: 10
reward sum = 0.8457344059925149
running average episode reward sum: 0.5800849588801593
{'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.59963226,  9.78124782,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.9848385726745493}
episode index:394
target Thresh 14.73653616024463
target distance 13.0
model initialize at round 394
at step 0:
{'currentTarget': array([19., 27.]), 'previousTarget': array([19., 27.]), 'currentState': array([ 9.56711578, 13.364043  ,  0.        ]), 'targetState': array([19, 27], dtype=int32), 'currentDistance': 16.58067031262982}
done in step count: 12
reward sum = 0.742886852617463
running average episode reward sum: 0.5804971155731651
{'currentTarget': array([19., 27.]), 'previousTarget': array([19., 27.]), 'currentState': array([18.57440663, 26.10301992,  0.        ]), 'targetState': array([19, 27], dtype=int32), 'currentDistance': 0.9928257545162507}
episode index:395
target Thresh 14.753790995228986
target distance 3.0
model initialize at round 395
at step 0:
{'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 9.12920648, 10.58586326,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 2.208337834377202}
done in step count: 2
reward sum = 0.9696380102080038
running average episode reward sum: 0.5814797946000206
{'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 7.71818757, 10.0104859 ,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.7182641162370413}
episode index:396
target Thresh 14.77102858400291
target distance 9.0
model initialize at round 396
at step 0:
{'currentTarget': array([20., 28.]), 'previousTarget': array([20., 28.]), 'currentState': array([21.56227681, 20.66265583,  0.        ]), 'targetState': array([20, 28], dtype=int32), 'currentDistance': 7.5018216651520175}
done in step count: 5
reward sum = 0.8881643629435415
running average episode reward sum: 0.582252299809954
{'currentTarget': array([20., 28.]), 'previousTarget': array([20., 28.]), 'currentState': array([19.60603534, 27.21131045,  0.        ]), 'targetState': array([20, 28], dtype=int32), 'currentDistance': 0.8816118011824939}
episode index:397
target Thresh 14.788248943803975
target distance 14.0
model initialize at round 397
at step 0:
{'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([10.45865214,  3.22739255,  0.        ]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 17.585995842015645}
done in step count: 12
reward sum = 0.7289858767952213
running average episode reward sum: 0.5826209771390627
{'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([19.33430574, 17.23624176,  0.        ]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 1.0131512733791908}
episode index:398
target Thresh 14.805452091852548
target distance 6.0
model initialize at round 398
at step 0:
{'currentTarget': array([13., 18.]), 'previousTarget': array([13., 18.]), 'currentState': array([ 8.92707551, 22.65792859,  0.        ]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 6.187488396127513}
done in step count: 5
reward sum = 0.9101969868174817
running average episode reward sum: 0.5834419696445223
{'currentTarget': array([13., 18.]), 'previousTarget': array([13., 18.]), 'currentState': array([12.07318038, 18.51894367,  0.        ]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 1.0622133225620027}
episode index:399
target Thresh 14.822638045351784
target distance 10.0
model initialize at round 399
at step 0:
{'currentTarget': array([ 2., 14.]), 'previousTarget': array([ 2., 14.]), 'currentState': array([10.44443655,  4.82112518,  0.        ]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 12.472379549196672}
done in step count: 9
reward sum = 0.8138719725520027
running average episode reward sum: 0.584018044651791
{'currentTarget': array([ 2., 14.]), 'previousTarget': array([ 2., 14.]), 'currentState': array([ 1.83383036, 13.33679557,  0.        ]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 0.6837049571230766}
episode index:400
target Thresh 14.839806821487631
target distance 14.0
model initialize at round 400
at step 0:
{'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([12.02892113, 24.22871888,  0.        ]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 15.166470304873393}
done in step count: 10
reward sum = 0.7742386620115798
running average episode reward sum: 0.5844924102811171
{'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([20.24285978, 12.28202158,  0.        ]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 0.8079588382669819}
episode index:401
target Thresh 14.856958437428876
target distance 8.0
model initialize at round 401
at step 0:
{'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([4.39654756, 9.97959696, 0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 6.603483964190512}
done in step count: 5
reward sum = 0.904019774872317
running average episode reward sum: 0.5852872544716424
{'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([10.01784021,  9.9736536 ,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.9825131003417362}
episode index:402
target Thresh 14.874092910327125
target distance 2.0
model initialize at round 402
at step 0:
{'currentTarget': array([20., 25.]), 'previousTarget': array([20., 25.]), 'currentState': array([19.20078982, 24.04686594,  0.        ]), 'targetState': array([20, 25], dtype=int32), 'currentDistance': 1.2438655238286367}
done in step count: 0
reward sum = 0.9943187754760202
running average episode reward sum: 0.5863022210250032
{'currentTarget': array([20., 25.]), 'previousTarget': array([20., 25.]), 'currentState': array([19.20078982, 24.04686594,  0.        ]), 'targetState': array([20, 25], dtype=int32), 'currentDistance': 1.2438655238286367}
episode index:403
target Thresh 14.891210257316857
target distance 2.0
model initialize at round 403
at step 0:
{'currentTarget': array([22.,  7.]), 'previousTarget': array([22.,  7.]), 'currentState': array([21.2485728 ,  8.18562752,  0.        ]), 'targetState': array([22,  7], dtype=int32), 'currentDistance': 1.4036935058681201}
done in step count: 1
reward sum = 0.9839721057695221
running average episode reward sum: 0.5872865524228856
{'currentTarget': array([22.,  7.]), 'previousTarget': array([22.,  7.]), 'currentState': array([21.51597425,  7.54224682,  0.        ]), 'targetState': array([22,  7], dtype=int32), 'currentDistance': 0.7268511096337126}
episode index:404
target Thresh 14.908310495515419
target distance 8.0
model initialize at round 404
at step 0:
{'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([ 2.37102631, 11.00112134,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 7.0293177445617925}
done in step count: 8
reward sum = 0.8905829275917355
running average episode reward sum: 0.5880354323615742
{'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.27897585, 4.55752957, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.6234314283466691}
episode index:405
target Thresh 14.925393642023057
target distance 6.0
model initialize at round 405
at step 0:
{'currentTarget': array([21., 24.]), 'previousTarget': array([21., 24.]), 'currentState': array([23.15093666, 19.32437146,  0.        ]), 'targetState': array([21, 24], dtype=int32), 'currentDistance': 5.146652387724247}
done in step count: 4
reward sum = 0.9211031405784565
running average episode reward sum: 0.5888557961749162
{'currentTarget': array([21., 24.]), 'previousTarget': array([21., 24.]), 'currentState': array([20.98230186, 23.30665839,  0.        ]), 'targetState': array([21, 24], dtype=int32), 'currentDistance': 0.6935674561166961}
episode index:406
target Thresh 14.942459713922908
target distance 10.0
model initialize at round 406
at step 0:
{'currentTarget': array([5., 6.]), 'previousTarget': array([5., 6.]), 'currentState': array([ 3.56393754, 14.87702239,  0.        ]), 'targetState': array([5, 6], dtype=int32), 'currentDistance': 8.992430250469743}
done in step count: 9
reward sum = 0.8682239447981182
running average episode reward sum: 0.5895422044024917
{'currentTarget': array([5., 6.]), 'previousTarget': array([5., 6.]), 'currentState': array([5.27494625, 6.84716326, 0.        ]), 'targetState': array([5, 6], dtype=int32), 'currentDistance': 0.8906632525627396}
episode index:407
target Thresh 14.959508728281055
target distance 12.0
model initialize at round 407
at step 0:
{'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([12.5196749 , 17.75877428,  0.        ]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 14.320296425674671}
done in step count: 10
reward sum = 0.7896401144972869
running average episode reward sum: 0.5900326404566456
{'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([22.17186466,  8.26425904,  0.        ]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 0.8692761264957694}
episode index:408
target Thresh 14.976540702146512
target distance 10.0
model initialize at round 408
at step 0:
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([11.68958521,  5.03947711,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 9.180064644944636}
done in step count: 10
reward sum = 0.8290908786130289
running average episode reward sum: 0.5906171349264656
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.02335247, 7.5254635 , 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.4751107504194745}
episode index:409
target Thresh 14.993555652551251
target distance 2.0
model initialize at round 409
at step 0:
{'currentTarget': array([ 4., 20.]), 'previousTarget': array([ 4., 20.]), 'currentState': array([ 5.28741813, 18.47943318,  0.        ]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 1.9923776979547734}
done in step count: 1
reward sum = 0.9830190212115086
running average episode reward sum: 0.5915742126978926
{'currentTarget': array([ 4., 20.]), 'previousTarget': array([ 4., 20.]), 'currentState': array([ 4.67215127, 19.01484472,  0.        ]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 1.1926098527308029}
episode index:410
target Thresh 15.010553596510224
target distance 3.0
model initialize at round 410
at step 0:
{'currentTarget': array([ 6., 24.]), 'previousTarget': array([ 6., 24.]), 'currentState': array([ 8.2270183 , 22.20344581,  0.        ]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 2.8613314142106336}
done in step count: 2
reward sum = 0.9700042180062252
running average episode reward sum: 0.5924949669687157
{'currentTarget': array([ 6., 24.]), 'previousTarget': array([ 6., 24.]), 'currentState': array([ 6.77710521, 23.0313888 ,  0.        ]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 1.2418132545796874}
episode index:411
target Thresh 15.027534551021379
target distance 13.0
model initialize at round 411
at step 0:
{'currentTarget': array([24.,  5.]), 'previousTarget': array([24.,  5.]), 'currentState': array([12.55150867,  8.78277566,  0.        ]), 'targetState': array([24,  5], dtype=int32), 'currentDistance': 12.05725281748003}
done in step count: 9
reward sum = 0.8250980397299491
running average episode reward sum: 0.5930595375336701
{'currentTarget': array([24.,  5.]), 'previousTarget': array([24.,  5.]), 'currentState': array([23.06396133,  5.03793007,  0.        ]), 'targetState': array([24,  5], dtype=int32), 'currentDistance': 0.9368068571074075}
episode index:412
target Thresh 15.044498533065667
target distance 10.0
model initialize at round 412
at step 0:
{'currentTarget': array([13., 17.]), 'previousTarget': array([13., 17.]), 'currentState': array([ 4.41897631, 24.82135093,  0.        ]), 'targetState': array([13, 17], dtype=int32), 'currentDistance': 11.610663117353637}
done in step count: 8
reward sum = 0.8295178312399896
running average episode reward sum: 0.5936320757750898
{'currentTarget': array([13., 17.]), 'previousTarget': array([13., 17.]), 'currentState': array([12.05700272, 17.29309553,  0.        ]), 'targetState': array([13, 17], dtype=int32), 'currentDistance': 0.987496254770714}
episode index:413
target Thresh 15.061445559607083
target distance 10.0
model initialize at round 413
at step 0:
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([18.62297726, 14.97773409,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 9.49621531205127}
done in step count: 8
reward sum = 0.8518348822304944
running average episode reward sum: 0.5942557540515521
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.51130301, 10.89637927,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.5216972499138571}
episode index:414
target Thresh 15.07837564759264
target distance 7.0
model initialize at round 414
at step 0:
{'currentTarget': array([ 3., 22.]), 'previousTarget': array([ 3., 22.]), 'currentState': array([ 7.79573226, 15.73804402,  0.        ]), 'targetState': array([ 3, 22], dtype=int32), 'currentDistance': 7.887403917555039}
done in step count: 6
reward sum = 0.885040445998709
running average episode reward sum: 0.5949564400562439
{'currentTarget': array([ 3., 22.]), 'previousTarget': array([ 3., 22.]), 'currentState': array([ 3.01162687, 21.34396267,  0.        ]), 'targetState': array([ 3, 22], dtype=int32), 'currentDistance': 0.6561403533310108}
episode index:415
target Thresh 15.09528881395244
target distance 9.0
model initialize at round 415
at step 0:
{'currentTarget': array([17., 19.]), 'previousTarget': array([17., 19.]), 'currentState': array([13.93847293, 11.06195104,  0.        ]), 'targetState': array([17, 19], dtype=int32), 'currentDistance': 8.507970926902678}
done in step count: 6
reward sum = 0.8770685933531991
running average episode reward sum: 0.5956345942709
{'currentTarget': array([17., 19.]), 'previousTarget': array([17., 19.]), 'currentState': array([16.4430913, 18.4597894,  0.       ]), 'targetState': array([17, 19], dtype=int32), 'currentDistance': 0.775870346170758}
episode index:416
target Thresh 15.11218507559964
target distance 12.0
model initialize at round 416
at step 0:
{'currentTarget': array([18., 28.]), 'previousTarget': array([18., 28.]), 'currentState': array([ 7.37180352, 23.18659097,  0.        ]), 'targetState': array([18, 28], dtype=int32), 'currentDistance': 11.667367606573931}
done in step count: 9
reward sum = 0.8140688784785127
running average episode reward sum: 0.5961584174944194
{'currentTarget': array([18., 28.]), 'previousTarget': array([18., 28.]), 'currentState': array([17.03202501, 27.24911965,  0.        ]), 'targetState': array([18, 28], dtype=int32), 'currentDistance': 1.2250701514612437}
episode index:417
target Thresh 15.129064449430508
target distance 4.0
model initialize at round 417
at step 0:
{'currentTarget': array([19.,  6.]), 'previousTarget': array([19.,  6.]), 'currentState': array([16.58645129,  6.37324849,  0.        ]), 'targetState': array([19,  6], dtype=int32), 'currentDistance': 2.442239095437516}
done in step count: 3
reward sum = 0.951654092331936
running average episode reward sum: 0.5970088856160404
{'currentTarget': array([19.,  6.]), 'previousTarget': array([19.,  6.]), 'currentState': array([18.26682699,  5.95498251,  0.        ]), 'targetState': array([19,  6], dtype=int32), 'currentDistance': 0.7345537698858754}
episode index:418
target Thresh 15.145926952324423
target distance 14.0
model initialize at round 418
at step 0:
{'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([ 7.41386008, 10.65957001,  0.        ]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 12.80189558482426}
done in step count: 10
reward sum = 0.8147095193416187
running average episode reward sum: 0.5975284575342399
{'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([19.19366553, 12.54744839,  0.        ]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 0.924650329071746}
episode index:419
target Thresh 15.162772601143885
target distance 1.0
model initialize at round 419
at step 0:
{'currentTarget': array([12.,  3.]), 'previousTarget': array([12.,  3.]), 'currentState': array([12.04380277,  3.37236303,  0.        ]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 0.3749305396989814}
done in step count: 0
reward sum = 0.9980207656783727
running average episode reward sum: 0.5984820106488687
{'currentTarget': array([12.,  3.]), 'previousTarget': array([12.,  3.]), 'currentState': array([12.04380277,  3.37236303,  0.        ]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 0.3749305396989814}
episode index:420
target Thresh 15.179601412734545
target distance 15.0
model initialize at round 420
at step 0:
{'currentTarget': array([17., 27.]), 'previousTarget': array([17., 27.]), 'currentState': array([ 3.5122354 , 29.26740107,  0.        ]), 'targetState': array([17, 27], dtype=int32), 'currentDistance': 13.67702092679437}
done in step count: 10
reward sum = 0.803050426034464
running average episode reward sum: 0.5989679213742503
{'currentTarget': array([17., 27.]), 'previousTarget': array([17., 27.]), 'currentState': array([16.02389449, 27.01977445,  0.        ]), 'targetState': array([17, 27], dtype=int32), 'currentDistance': 0.9763057911936538}
episode index:421
target Thresh 15.196413403925217
target distance 12.0
model initialize at round 421
at step 0:
{'currentTarget': array([13., 28.]), 'previousTarget': array([13., 28.]), 'currentState': array([23.59513474, 23.84827232,  0.        ]), 'targetState': array([13, 28], dtype=int32), 'currentDistance': 11.379530867043487}
done in step count: 10
reward sum = 0.7974221467082117
running average episode reward sum: 0.5994381920503971
{'currentTarget': array([13., 28.]), 'previousTarget': array([13., 28.]), 'currentState': array([13.14417082, 27.47322789,  0.        ]), 'targetState': array([13, 28], dtype=int32), 'currentDistance': 0.5461447456135998}
episode index:422
target Thresh 15.213208591527891
target distance 2.0
model initialize at round 422
at step 0:
{'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([21.22155797, 15.66233152,  0.        ]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 1.3895635754517905}
done in step count: 1
reward sum = 0.9846308030180847
running average episode reward sum: 0.6003488128801079
{'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([20.66744566, 15.43787409,  0.        ]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 0.7982589996113924}
episode index:423
target Thresh 15.22998699233776
target distance 10.0
model initialize at round 423
at step 0:
{'currentTarget': array([12., 20.]), 'previousTarget': array([12., 20.]), 'currentState': array([20.55602992, 11.80413607,  0.        ]), 'targetState': array([12, 20], dtype=int32), 'currentDistance': 11.84811518842448}
done in step count: 8
reward sum = 0.8304042430149112
running average episode reward sum: 0.6008913964417466
{'currentTarget': array([12., 20.]), 'previousTarget': array([12., 20.]), 'currentState': array([12.20835888, 19.40453181,  0.        ]), 'targetState': array([12, 20], dtype=int32), 'currentDistance': 0.6308690781022558}
episode index:424
target Thresh 15.246748623133218
target distance 5.0
model initialize at round 424
at step 0:
{'currentTarget': array([16., 16.]), 'previousTarget': array([16., 16.]), 'currentState': array([19.90870881, 11.63397884,  0.        ]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 5.860046527548206}
done in step count: 4
reward sum = 0.9223525505716597
running average episode reward sum: 0.6016477756279347
{'currentTarget': array([16., 16.]), 'previousTarget': array([16., 16.]), 'currentState': array([16.37119871, 15.07874912,  0.        ]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 0.9932228695403741}
episode index:425
target Thresh 15.26349350067591
target distance 7.0
model initialize at round 425
at step 0:
{'currentTarget': array([19.,  5.]), 'previousTarget': array([19.,  5.]), 'currentState': array([25.14096224, 11.01227146,  0.        ]), 'targetState': array([19,  5], dtype=int32), 'currentDistance': 8.594115744287583}
done in step count: 7
reward sum = 0.8805920720407933
running average episode reward sum: 0.6023025744458053
{'currentTarget': array([19.,  5.]), 'previousTarget': array([19.,  5.]), 'currentState': array([19.74104264,  5.54792798,  0.        ]), 'targetState': array([19,  5], dtype=int32), 'currentDistance': 0.9216123189353647}
episode index:426
target Thresh 15.280221641710703
target distance 8.0
model initialize at round 426
at step 0:
{'currentTarget': array([24.,  7.]), 'previousTarget': array([24.,  7.]), 'currentState': array([23.24860792, 13.89455318,  0.        ]), 'targetState': array([24,  7], dtype=int32), 'currentDistance': 6.935376966970766}
done in step count: 6
reward sum = 0.9045119776784468
running average episode reward sum: 0.6030103248046639
{'currentTarget': array([24.,  7.]), 'previousTarget': array([24.,  7.]), 'currentState': array([23.94891393,  7.80300999,  0.        ]), 'targetState': array([24,  7], dtype=int32), 'currentDistance': 0.8046333486534054}
episode index:427
target Thresh 15.29693306296575
target distance 1.0
model initialize at round 427
at step 0:
{'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([23.43667233,  2.4377093 ,  0.        ]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 0.7119364810753227}
done in step count: 0
reward sum = 0.9974553625359133
running average episode reward sum: 0.6039319253601108
{'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([23.43667233,  2.4377093 ,  0.        ]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 0.7119364810753227}
episode index:428
target Thresh 15.313627781152462
target distance 9.0
model initialize at round 428
at step 0:
{'currentTarget': array([26., 26.]), 'previousTarget': array([26., 26.]), 'currentState': array([18.59258187, 27.83657472,  0.        ]), 'targetState': array([26, 26], dtype=int32), 'currentDistance': 7.631700339756787}
done in step count: 6
reward sum = 0.8805234304337298
running average episode reward sum: 0.6045766608031728
{'currentTarget': array([26., 26.]), 'previousTarget': array([26., 26.]), 'currentState': array([25.19102678, 25.92844973,  0.        ]), 'targetState': array([26, 26], dtype=int32), 'currentDistance': 0.8121312187277246}
episode index:429
target Thresh 15.330305812965566
target distance 10.0
model initialize at round 429
at step 0:
{'currentTarget': array([17.,  2.]), 'previousTarget': array([17.,  2.]), 'currentState': array([17.23099415, 10.96488166,  0.        ]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 8.967857126941338}
done in step count: 9
reward sum = 0.8693511695363099
running average episode reward sum: 0.6051924154746452
{'currentTarget': array([17.,  2.]), 'previousTarget': array([17.,  2.]), 'currentState': array([17.2372225 ,  2.57625872,  0.        ]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 0.6231762393734062}
episode index:430
target Thresh 15.346967175083098
target distance 14.0
model initialize at round 430
at step 0:
{'currentTarget': array([ 8., 23.]), 'previousTarget': array([ 8., 23.]), 'currentState': array([20.31257081, 11.78702402,  0.        ]), 'targetState': array([ 8, 23], dtype=int32), 'currentDistance': 16.653234828747095}
done in step count: 11
reward sum = 0.757948735152537
running average episode reward sum: 0.6055468384901391
{'currentTarget': array([ 8., 23.]), 'previousTarget': array([ 8., 23.]), 'currentState': array([ 8.27659163, 22.08613411,  0.        ]), 'targetState': array([ 8, 23], dtype=int32), 'currentDistance': 0.9548056283141041}
episode index:431
target Thresh 15.36361188416641
target distance 7.0
model initialize at round 431
at step 0:
{'currentTarget': array([19., 27.]), 'previousTarget': array([19., 27.]), 'currentState': array([22.09751827, 21.35535479,  0.        ]), 'targetState': array([19, 27], dtype=int32), 'currentDistance': 6.438683017231415}
done in step count: 4
reward sum = 0.9073359700439969
running average episode reward sum: 0.6062454244428102
{'currentTarget': array([19., 27.]), 'previousTarget': array([19., 27.]), 'currentState': array([19.2837494 , 26.20422268,  0.        ]), 'targetState': array([19, 27], dtype=int32), 'currentDistance': 0.8448522162415745}
episode index:432
target Thresh 15.380239956860226
target distance 12.0
model initialize at round 432
at step 0:
{'currentTarget': array([ 8., 29.]), 'previousTarget': array([ 8., 29.]), 'currentState': array([ 7.96693733, 18.7894485 ,  0.        ]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 10.210605030107946}
done in step count: 7
reward sum = 0.844161647369462
running average episode reward sum: 0.6067948845419479
{'currentTarget': array([ 8., 29.]), 'previousTarget': array([ 8., 29.]), 'currentState': array([ 7.80859471, 28.57461143,  0.        ]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 0.4664669591832158}
episode index:433
target Thresh 15.39685140979261
target distance 5.0
model initialize at round 433
at step 0:
{'currentTarget': array([12.,  3.]), 'previousTarget': array([12.,  3.]), 'currentState': array([16.13063675,  7.12850368,  0.        ]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 5.840094400929009}
done in step count: 5
reward sum = 0.919858303696773
running average episode reward sum: 0.6075162288257148
{'currentTarget': array([12.,  3.]), 'previousTarget': array([12.,  3.]), 'currentState': array([12.84395486,  3.62876469,  0.        ]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 1.0524280696448618}
episode index:434
target Thresh 15.413446259575025
target distance 13.0
model initialize at round 434
at step 0:
{'currentTarget': array([10.,  8.]), 'previousTarget': array([10.,  8.]), 'currentState': array([14.31770396, 19.93423021,  0.        ]), 'targetState': array([10,  8], dtype=int32), 'currentDistance': 12.691273307800836}
done in step count: 13
reward sum = 0.8111658047387602
running average episode reward sum: 0.6079843887703426
{'currentTarget': array([10.,  8.]), 'previousTarget': array([10.,  8.]), 'currentState': array([10.68624651,  8.47729608,  0.        ]), 'targetState': array([10,  8], dtype=int32), 'currentDistance': 0.8359101746902929}
episode index:435
target Thresh 15.430024522802313
target distance 12.0
model initialize at round 435
at step 0:
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([ 9.50660354, 13.77237678,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 11.671962695037767}
done in step count: 10
reward sum = 0.8359798060562795
running average episode reward sum: 0.608507314039347
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.76706111,  3.5614965 ,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.6078970634172958}
episode index:436
target Thresh 15.446586216052744
target distance 11.0
model initialize at round 436
at step 0:
{'currentTarget': array([26., 17.]), 'previousTarget': array([26., 17.]), 'currentState': array([23.53911352, 26.92431164,  0.        ]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 10.224867908458235}
done in step count: 10
reward sum = 0.8502285112716749
running average episode reward sum: 0.6090604517904507
{'currentTarget': array([26., 17.]), 'previousTarget': array([26., 17.]), 'currentState': array([26.09986635, 17.55746007,  0.        ]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 0.5663347210175914}
episode index:437
target Thresh 15.463131355888013
target distance 14.0
model initialize at round 437
at step 0:
{'currentTarget': array([18., 22.]), 'previousTarget': array([18., 22.]), 'currentState': array([20.26431426,  9.7680341 ,  0.        ]), 'targetState': array([18, 22], dtype=int32), 'currentDistance': 12.439779292742939}
done in step count: 8
reward sum = 0.822465595068136
running average episode reward sum: 0.6095476781449659
{'currentTarget': array([18., 22.]), 'previousTarget': array([18., 22.]), 'currentState': array([18.10816728, 21.10560369,  0.        ]), 'targetState': array([18, 22], dtype=int32), 'currentDistance': 0.9009133754689554}
episode index:438
target Thresh 15.479659958853258
target distance 14.0
model initialize at round 438
at step 0:
{'currentTarget': array([24.,  8.]), 'previousTarget': array([24.,  8.]), 'currentState': array([11.69410205, 12.10547518,  0.        ]), 'targetState': array([24,  8], dtype=int32), 'currentDistance': 12.972665525477057}
done in step count: 9
reward sum = 0.8063938092938674
running average episode reward sum: 0.6099960747990637
{'currentTarget': array([24.,  8.]), 'previousTarget': array([24.,  8.]), 'currentState': array([23.24436584,  8.3150412 ,  0.        ]), 'targetState': array([24,  8], dtype=int32), 'currentDistance': 0.8186781658828259}
episode index:439
target Thresh 15.496172041477084
target distance 4.0
model initialize at round 439
at step 0:
{'currentTarget': array([ 7., 29.]), 'previousTarget': array([ 7., 29.]), 'currentState': array([ 9.02676582, 26.17018008,  0.        ]), 'targetState': array([ 7, 29], dtype=int32), 'currentDistance': 3.480755732488139}
done in step count: 2
reward sum = 0.9537287280917706
running average episode reward sum: 0.610777285374729
{'currentTarget': array([ 7., 29.]), 'previousTarget': array([ 7., 29.]), 'currentState': array([ 7.5224933 , 28.07858491,  0.        ]), 'targetState': array([ 7, 29], dtype=int32), 'currentDistance': 1.059247384045618}
episode index:440
target Thresh 15.512667620271579
target distance 8.0
model initialize at round 440
at step 0:
{'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([ 7.32842883, 20.96903062,  0.        ]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 7.001313847701782}
done in step count: 7
reward sum = 0.8958606638656175
running average episode reward sum: 0.6114237329450031
{'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([ 8.24813938, 14.5044502 ,  0.        ]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 0.5621771597031969}
episode index:441
target Thresh 15.52914671173232
target distance 5.0
model initialize at round 441
at step 0:
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([3.4673596 , 7.39697099, 0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 3.8903678835195037}
done in step count: 3
reward sum = 0.9397328470846247
running average episode reward sum: 0.6121665137462239
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.14765674, 10.47000009,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.5501839864925168}
episode index:442
target Thresh 15.545609332338401
target distance 4.0
model initialize at round 442
at step 0:
{'currentTarget': array([13., 13.]), 'previousTarget': array([13., 13.]), 'currentState': array([11.19382501, 10.04085183,  0.        ]), 'targetState': array([13, 13], dtype=int32), 'currentDistance': 3.466817847928336}
done in step count: 2
reward sum = 0.9548892226691836
running average episode reward sum: 0.6129401541726865
{'currentTarget': array([13., 13.]), 'previousTarget': array([13., 13.]), 'currentState': array([12.03308162, 12.06142318,  0.        ]), 'targetState': array([13, 13], dtype=int32), 'currentDistance': 1.3475376039845681}
episode index:443
target Thresh 15.562055498552443
target distance 14.0
model initialize at round 443
at step 0:
{'currentTarget': array([20., 21.]), 'previousTarget': array([20., 21.]), 'currentState': array([19.67467403,  8.75715268,  0.        ]), 'targetState': array([20, 21], dtype=int32), 'currentDistance': 12.247168961380115}
done in step count: 8
reward sum = 0.8189652604847263
running average episode reward sum: 0.6134041746823984
{'currentTarget': array([20., 21.]), 'previousTarget': array([20., 21.]), 'currentState': array([19.48001568, 20.53881228,  0.        ]), 'targetState': array([20, 21], dtype=int32), 'currentDistance': 0.695037986245136}
episode index:444
target Thresh 15.57848522682061
target distance 10.0
model initialize at round 444
at step 0:
{'currentTarget': array([ 2., 13.]), 'previousTarget': array([ 2., 13.]), 'currentState': array([ 8.96425217, 21.97965002,  0.        ]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 11.363754781336691}
done in step count: 11
reward sum = 0.836538503333731
running average episode reward sum: 0.6139056001400418
{'currentTarget': array([ 2., 13.]), 'previousTarget': array([ 2., 13.]), 'currentState': array([ 2.80757742, 13.3817773 ,  0.        ]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 0.8932721887755223}
episode index:445
target Thresh 15.594898533572639
target distance 4.0
model initialize at round 445
at step 0:
{'currentTarget': array([ 5., 15.]), 'previousTarget': array([ 5., 15.]), 'currentState': array([ 7.96527958, 12.70461482,  0.        ]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 3.7498901471780193}
done in step count: 3
reward sum = 0.9459209764542871
running average episode reward sum: 0.6146500292349167
{'currentTarget': array([ 5., 15.]), 'previousTarget': array([ 5., 15.]), 'currentState': array([ 5.538454  , 14.70636505,  0.        ]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 0.613314104289052}
episode index:446
target Thresh 15.611295435221834
target distance 5.0
model initialize at round 446
at step 0:
{'currentTarget': array([ 8., 15.]), 'previousTarget': array([ 8., 15.]), 'currentState': array([ 9.87411831, 18.9553057 ,  0.        ]), 'targetState': array([ 8, 15], dtype=int32), 'currentDistance': 4.376843908225251}
done in step count: 4
reward sum = 0.9399535403566937
running average episode reward sum: 0.6153777775819452
{'currentTarget': array([ 8., 15.]), 'previousTarget': array([ 8., 15.]), 'currentState': array([ 8.89356978, 15.69646186,  0.        ]), 'targetState': array([ 8, 15], dtype=int32), 'currentDistance': 1.132928097165606}
episode index:447
target Thresh 15.627675948165095
target distance 11.0
model initialize at round 447
at step 0:
{'currentTarget': array([ 2., 13.]), 'previousTarget': array([ 2., 13.]), 'currentState': array([4.91881035, 3.62558126, 0.        ]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 9.81830843391575}
done in step count: 7
reward sum = 0.8549728913711037
running average episode reward sum: 0.615912588103796
{'currentTarget': array([ 2., 13.]), 'previousTarget': array([ 2., 13.]), 'currentState': array([ 1.99926721, 12.62399924,  0.        ]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 0.37600147606284834}
episode index:448
target Thresh 15.644040088782944
target distance 8.0
model initialize at round 448
at step 0:
{'currentTarget': array([20., 29.]), 'previousTarget': array([20., 29.]), 'currentState': array([13.26551676, 24.44033027,  0.        ]), 'targetState': array([20, 29], dtype=int32), 'currentDistance': 8.132887101867263}
done in step count: 6
reward sum = 0.8811955804843175
running average episode reward sum: 0.6165034188217927
{'currentTarget': array([20., 29.]), 'previousTarget': array([20., 29.]), 'currentState': array([19.23103461, 28.70555621,  0.        ]), 'targetState': array([20, 29], dtype=int32), 'currentDistance': 0.82341054116412}
episode index:449
target Thresh 15.660387873439518
target distance 7.0
model initialize at round 449
at step 0:
{'currentTarget': array([ 8., 16.]), 'previousTarget': array([ 8., 16.]), 'currentState': array([11.82880087, 21.93583035,  0.        ]), 'targetState': array([ 8, 16], dtype=int32), 'currentDistance': 7.063554212777888}
done in step count: 7
reward sum = 0.8950429183241937
running average episode reward sum: 0.6171223954873536
{'currentTarget': array([ 8., 16.]), 'previousTarget': array([ 8., 16.]), 'currentState': array([ 8.85394336, 16.319662  ,  0.        ]), 'targetState': array([ 8, 16], dtype=int32), 'currentDistance': 0.9118130642735838}
episode index:450
target Thresh 15.6767193184826
target distance 9.0
model initialize at round 450
at step 0:
{'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([21.8870728 , 10.22334558,  0.        ]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 8.74465756922097}
done in step count: 7
reward sum = 0.8700935464014903
running average episode reward sum: 0.6176833071301787
{'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([14.59179378, 13.5717869 ,  0.        ]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 0.7304699373452859}
episode index:451
target Thresh 15.693034440243647
target distance 15.0
model initialize at round 451
at step 0:
{'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([12.2433947 , 18.48617065,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 16.349836219901608}
done in step count: 14
reward sum = 0.7628460187112902
running average episode reward sum: 0.6180044635717299
{'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.86912808, 5.48451117, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.9950551176287863}
episode index:452
target Thresh 15.70933325503777
target distance 13.0
model initialize at round 452
at step 0:
{'currentTarget': array([ 9., 26.]), 'previousTarget': array([ 9., 26.]), 'currentState': array([13.41932642, 14.49958074,  0.        ]), 'targetState': array([ 9, 26], dtype=int32), 'currentDistance': 12.320312056988449}
done in step count: 8
reward sum = 0.8291455161910349
running average episode reward sum: 0.6184705586106246
{'currentTarget': array([ 9., 26.]), 'previousTarget': array([ 9., 26.]), 'currentState': array([ 9.20216507, 25.19122797,  0.        ]), 'targetState': array([ 9, 26], dtype=int32), 'currentDistance': 0.8336563481723023}
episode index:453
target Thresh 15.725615779163792
target distance 6.0
model initialize at round 453
at step 0:
{'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([12.77335835, 12.70062321,  0.        ]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 6.4241412372374525}
done in step count: 4
reward sum = 0.9143741610326663
running average episode reward sum: 0.6191223286600123
{'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([ 8.78767872, 16.31973696,  0.        ]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 1.0407668185555783}
episode index:454
target Thresh 15.74188202890424
target distance 9.0
model initialize at round 454
at step 0:
{'currentTarget': array([27.,  2.]), 'previousTarget': array([27.,  2.]), 'currentState': array([21.14575219,  9.78131831,  0.        ]), 'targetState': array([27,  2], dtype=int32), 'currentDistance': 9.737614286696253}
done in step count: 7
reward sum = 0.8618655807361004
running average episode reward sum: 0.6196558303129268
{'currentTarget': array([27.,  2.]), 'previousTarget': array([27.,  2.]), 'currentState': array([26.59050667,  2.87857801,  0.        ]), 'targetState': array([27,  2], dtype=int32), 'currentDistance': 0.9693214635743662}
episode index:455
target Thresh 15.758132020525355
target distance 3.0
model initialize at round 455
at step 0:
{'currentTarget': array([15., 12.]), 'previousTarget': array([15., 12.]), 'currentState': array([13.40110183, 12.04323275,  0.        ]), 'targetState': array([15, 12], dtype=int32), 'currentDistance': 1.5994825526777747}
done in step count: 2
reward sum = 0.9677660853399532
running average episode reward sum: 0.6204192299950037
{'currentTarget': array([15., 12.]), 'previousTarget': array([15., 12.]), 'currentState': array([14.34929928, 11.92099294,  0.        ]), 'targetState': array([15, 12], dtype=int32), 'currentDistance': 0.6554796264058386}
episode index:456
target Thresh 15.774365770277146
target distance 14.0
model initialize at round 456
at step 0:
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 8.3088243 , 16.03308511,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.072843454101418}
done in step count: 15
reward sum = 0.792552629216943
running average episode reward sum: 0.6207958895119007
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.86882949, 3.53218845, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.018866639136066}
episode index:457
target Thresh 15.790583294393347
target distance 5.0
model initialize at round 457
at step 0:
{'currentTarget': array([ 4., 16.]), 'previousTarget': array([ 4., 16.]), 'currentState': array([ 3.23598187, 19.91665149,  0.        ]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 3.9904739795353548}
done in step count: 4
reward sum = 0.9382358321447619
running average episode reward sum: 0.6214889898233261
{'currentTarget': array([ 4., 16.]), 'previousTarget': array([ 4., 16.]), 'currentState': array([ 4.12669703, 16.48939621,  0.        ]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 0.5055302098655731}
episode index:458
target Thresh 15.806784609091498
target distance 4.0
model initialize at round 458
at step 0:
{'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([24.71469212,  3.81095833,  0.        ]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 2.4249300055769947}
done in step count: 2
reward sum = 0.957979197304539
running average episode reward sum: 0.6222220839572722
{'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([26.15473771,  2.93811399,  0.        ]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 0.8475247582432688}
episode index:459
target Thresh 15.822969730572904
target distance 7.0
model initialize at round 459
at step 0:
{'currentTarget': array([18., 16.]), 'previousTarget': array([18., 16.]), 'currentState': array([12.297755  , 10.36801121,  0.        ]), 'targetState': array([18, 16], dtype=int32), 'currentDistance': 8.01466754004531}
done in step count: 5
reward sum = 0.8912192787596668
running average episode reward sum: 0.6228068604677122
{'currentTarget': array([18., 16.]), 'previousTarget': array([18., 16.]), 'currentState': array([17.11071962, 15.14464572,  0.        ]), 'targetState': array([18, 16], dtype=int32), 'currentDistance': 1.2338762236272345}
episode index:460
target Thresh 15.839138675022696
target distance 11.0
model initialize at round 460
at step 0:
{'currentTarget': array([7., 7.]), 'previousTarget': array([7., 7.]), 'currentState': array([17.58786723, 16.59224248,  0.        ]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 14.286848787119297}
done in step count: 12
reward sum = 0.7973876409664564
running average episode reward sum: 0.6231855606423298
{'currentTarget': array([7., 7.]), 'previousTarget': array([7., 7.]), 'currentState': array([7.89266446, 7.45899393, 0.        ]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 1.0037555813043897}
episode index:461
target Thresh 15.855291458609813
target distance 11.0
model initialize at round 461
at step 0:
{'currentTarget': array([15., 12.]), 'previousTarget': array([15., 12.]), 'currentState': array([24.54130077,  4.21769364,  0.        ]), 'targetState': array([15, 12], dtype=int32), 'currentDistance': 12.312624119409204}
done in step count: 8
reward sum = 0.8230341835029509
running average episode reward sum: 0.6236181334190845
{'currentTarget': array([15., 12.]), 'previousTarget': array([15., 12.]), 'currentState': array([15.47989368, 11.8637637 ,  0.        ]), 'targetState': array([15, 12], dtype=int32), 'currentDistance': 0.4988569699008803}
episode index:462
target Thresh 15.871428097487044
target distance 14.0
model initialize at round 462
at step 0:
{'currentTarget': array([ 8., 23.]), 'previousTarget': array([ 8., 23.]), 'currentState': array([20.83744299, 17.08457953,  0.        ]), 'targetState': array([ 8, 23], dtype=int32), 'currentDistance': 14.13478482309305}
done in step count: 10
reward sum = 0.7930459156732449
running average episode reward sum: 0.6239840681539746
{'currentTarget': array([ 8., 23.]), 'previousTarget': array([ 8., 23.]), 'currentState': array([ 8.8055414 , 22.70430633,  0.        ]), 'targetState': array([ 8, 23], dtype=int32), 'currentDistance': 0.8580977128845549}
episode index:463
target Thresh 15.887548607791029
target distance 6.0
model initialize at round 463
at step 0:
{'currentTarget': array([ 9., 12.]), 'previousTarget': array([ 9., 12.]), 'currentState': array([ 4.81720698, 14.82573944,  0.        ]), 'targetState': array([ 9, 12], dtype=int32), 'currentDistance': 5.047827340514997}
done in step count: 3
reward sum = 0.9268078375568587
running average episode reward sum: 0.6246367055880325
{'currentTarget': array([ 9., 12.]), 'previousTarget': array([ 9., 12.]), 'currentState': array([ 8.0270043 , 12.64561468,  0.        ]), 'targetState': array([ 9, 12], dtype=int32), 'currentDistance': 1.1677067050087384}
episode index:464
target Thresh 15.90365300564228
target distance 7.0
model initialize at round 464
at step 0:
{'currentTarget': array([19.,  9.]), 'previousTarget': array([19.,  9.]), 'currentState': array([24.72791243, 10.10478359,  0.        ]), 'targetState': array([19,  9], dtype=int32), 'currentDistance': 5.833483311948534}
done in step count: 6
reward sum = 0.9039331000137281
running average episode reward sum: 0.6252373429953996
{'currentTarget': array([19.,  9.]), 'previousTarget': array([19.,  9.]), 'currentState': array([19.71766081,  9.39255089,  0.        ]), 'targetState': array([19,  9], dtype=int32), 'currentDistance': 0.8180056530353124}
episode index:465
target Thresh 15.919741307145195
target distance 10.0
model initialize at round 465
at step 0:
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([5.69480717, 6.98939641, 0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 8.540136181191935}
done in step count: 6
reward sum = 0.8665484610202058
running average episode reward sum: 0.6257551780126203
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.30744141,  5.12700055,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.7041069071554781}
episode index:466
target Thresh 15.935813528388078
target distance 15.0
model initialize at round 466
at step 0:
{'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([26.15481067, 26.3710022 ,  0.        ]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 18.069950805434587}
done in step count: 15
reward sum = 0.7305571070821547
running average episode reward sum: 0.6259795932782938
{'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([14.83019066, 13.50462292,  0.        ]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 0.9715250014854597}
episode index:467
target Thresh 15.951869685443146
target distance 11.0
model initialize at round 467
at step 0:
{'currentTarget': array([14., 29.]), 'previousTarget': array([14., 29.]), 'currentState': array([ 4.07630944, 20.52694631,  0.        ]), 'targetState': array([14, 29], dtype=int32), 'currentDistance': 13.048841832827508}
done in step count: 12
reward sum = 0.8075870186215712
running average episode reward sum: 0.6263676433324461
{'currentTarget': array([14., 29.]), 'previousTarget': array([14., 29.]), 'currentState': array([13.30924378, 28.25505275,  0.        ]), 'targetState': array([14, 29], dtype=int32), 'currentDistance': 1.0159185781874056}
episode index:468
target Thresh 15.967909794366566
target distance 6.0
model initialize at round 468
at step 0:
{'currentTarget': array([19., 12.]), 'previousTarget': array([19., 12.]), 'currentState': array([14.7833674 , 11.84805581,  0.        ]), 'targetState': array([19, 12], dtype=int32), 'currentDistance': 4.219369331917109}
done in step count: 3
reward sum = 0.930971676980296
running average episode reward sum: 0.627017118883934
{'currentTarget': array([19., 12.]), 'previousTarget': array([19., 12.]), 'currentState': array([18.1603213 , 11.39487918,  0.        ]), 'targetState': array([19, 12], dtype=int32), 'currentDistance': 1.035003153785328}
episode index:469
target Thresh 15.983933871198445
target distance 11.0
model initialize at round 469
at step 0:
{'currentTarget': array([17.,  7.]), 'previousTarget': array([17.,  7.]), 'currentState': array([ 9.32672858, 16.71397877,  0.        ]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 12.37903379678044}
done in step count: 9
reward sum = 0.8151215736195742
running average episode reward sum: 0.6274173411280525
{'currentTarget': array([17.,  7.]), 'previousTarget': array([17.,  7.]), 'currentState': array([16.75909239,  7.61440533,  0.        ]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 0.6599472636051059}
episode index:470
target Thresh 15.999941931962859
target distance 10.0
model initialize at round 470
at step 0:
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([12.61123323,  8.07578552,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 9.1440578720926}
done in step count: 8
reward sum = 0.8581203521351294
running average episode reward sum: 0.6279071564380463
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.77806988, 5.52189217, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.9368906981701023}
episode index:471
target Thresh 16.01593399266787
target distance 3.0
model initialize at round 471
at step 0:
{'currentTarget': array([17., 15.]), 'previousTarget': array([17., 15.]), 'currentState': array([15.30889249, 15.31128055,  0.        ]), 'targetState': array([17, 15], dtype=int32), 'currentDistance': 1.7195174309788959}
done in step count: 2
reward sum = 0.9653440988949986
running average episode reward sum: 0.6286220652144381
{'currentTarget': array([17., 15.]), 'previousTarget': array([17., 15.]), 'currentState': array([16.41129181, 14.91348008,  0.        ]), 'targetState': array([17, 15], dtype=int32), 'currentDistance': 0.5950319587210452}
episode index:472
target Thresh 16.031910069305546
target distance 14.0
model initialize at round 472
at step 0:
{'currentTarget': array([23.,  9.]), 'previousTarget': array([23.,  9.]), 'currentState': array([10.8291831 , 16.40048158,  0.        ]), 'targetState': array([23,  9], dtype=int32), 'currentDistance': 14.24415359415817}
done in step count: 8
reward sum = 0.7931938679031526
running average episode reward sum: 0.6289699971440125
{'currentTarget': array([23.,  9.]), 'previousTarget': array([23.,  9.]), 'currentState': array([22.18294829,  9.93145555,  0.        ]), 'targetState': array([23,  9], dtype=int32), 'currentDistance': 1.239024996162292}
episode index:473
target Thresh 16.04787017785196
target distance 5.0
model initialize at round 473
at step 0:
{'currentTarget': array([22., 12.]), 'previousTarget': array([22., 12.]), 'currentState': array([18.59240365, 15.80089831,  0.        ]), 'targetState': array([22, 12], dtype=int32), 'currentDistance': 5.104756690806804}
done in step count: 4
reward sum = 0.9128097336312789
running average episode reward sum: 0.6295688151534793
{'currentTarget': array([22., 12.]), 'previousTarget': array([22., 12.]), 'currentState': array([21.67703587, 12.45221764,  0.        ]), 'targetState': array([22, 12], dtype=int32), 'currentDistance': 0.5557037187331275}
episode index:474
target Thresh 16.063814334267217
target distance 13.0
model initialize at round 474
at step 0:
{'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([8.93024427, 2.57187539, 0.        ]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 16.643250080017044}
done in step count: 13
reward sum = 0.7716254005947416
running average episode reward sum: 0.6298678816491452
{'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([19.54991266, 14.48043722,  0.        ]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 0.6874038810607362}
episode index:475
target Thresh 16.079742554495482
target distance 11.0
model initialize at round 475
at step 0:
{'currentTarget': array([20., 26.]), 'previousTarget': array([20., 26.]), 'currentState': array([19.95732075, 16.61856318,  0.        ]), 'targetState': array([20, 26], dtype=int32), 'currentDistance': 9.381533905306105}
done in step count: 7
reward sum = 0.8643171278841066
running average episode reward sum: 0.6303604220824119
{'currentTarget': array([20., 26.]), 'previousTarget': array([20., 26.]), 'currentState': array([19.59230329, 25.38851595,  0.        ]), 'targetState': array([20, 26], dtype=int32), 'currentDistance': 0.7349349327924601}
episode index:476
target Thresh 16.09565485446498
target distance 10.0
model initialize at round 476
at step 0:
{'currentTarget': array([20., 14.]), 'previousTarget': array([20., 14.]), 'currentState': array([11.62806773, 14.62189516,  0.        ]), 'targetState': array([20, 14], dtype=int32), 'currentDistance': 8.394998719338032}
done in step count: 5
reward sum = 0.8769207619140951
running average episode reward sum: 0.6308773200694805
{'currentTarget': array([20., 14.]), 'previousTarget': array([20., 14.]), 'currentState': array([19.02284986, 13.4999623 ,  0.        ]), 'targetState': array([20, 14], dtype=int32), 'currentDistance': 1.0976611962469516}
episode index:477
target Thresh 16.111551250088006
target distance 14.0
model initialize at round 477
at step 0:
{'currentTarget': array([18., 23.]), 'previousTarget': array([18., 23.]), 'currentState': array([ 5.16805172, 17.92225399,  0.        ]), 'targetState': array([18, 23], dtype=int32), 'currentDistance': 13.800087000617857}
done in step count: 11
reward sum = 0.8043051302213727
running average episode reward sum: 0.6312401397559908
{'currentTarget': array([18., 23.]), 'previousTarget': array([18., 23.]), 'currentState': array([17.35701457, 22.31512321,  0.        ]), 'targetState': array([18, 23], dtype=int32), 'currentDistance': 0.939407518246668}
episode index:478
target Thresh 16.12743175726095
target distance 12.0
model initialize at round 478
at step 0:
{'currentTarget': array([14., 18.]), 'previousTarget': array([14., 18.]), 'currentState': array([23.70245123,  6.74100369,  0.        ]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 14.862791047214328}
done in step count: 9
reward sum = 0.7942918841794161
running average episode reward sum: 0.6315805400575011
{'currentTarget': array([14., 18.]), 'previousTarget': array([14., 18.]), 'currentState': array([14.64693856, 17.53863972,  0.        ]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 0.7945960027251685}
episode index:479
target Thresh 16.14329639186434
target distance 15.0
model initialize at round 479
at step 0:
{'currentTarget': array([25., 19.]), 'previousTarget': array([25., 19.]), 'currentState': array([11.09689379,  6.53978989,  0.        ]), 'targetState': array([25, 19], dtype=int32), 'currentDistance': 18.669579493448396}
done in step count: 16
reward sum = 0.7417334114369158
running average episode reward sum: 0.6318100252062082
{'currentTarget': array([25., 19.]), 'previousTarget': array([25., 19.]), 'currentState': array([24.40936837, 18.41062359,  0.        ]), 'targetState': array([25, 19], dtype=int32), 'currentDistance': 0.8343921595416826}
episode index:480
target Thresh 16.159145169762795
target distance 8.0
model initialize at round 480
at step 0:
{'currentTarget': array([24.,  5.]), 'previousTarget': array([24.,  5.]), 'currentState': array([17.64837325,  3.77426462,  0.        ]), 'targetState': array([24,  5], dtype=int32), 'currentDistance': 6.468816711309453}
done in step count: 4
reward sum = 0.9059612736830629
running average episode reward sum: 0.6323799862217526
{'currentTarget': array([24.,  5.]), 'previousTarget': array([24.,  5.]), 'currentState': array([23.02371091,  4.1466914 ,  0.        ]), 'targetState': array([24,  5], dtype=int32), 'currentDistance': 1.296640258498421}
episode index:481
target Thresh 16.1749781068051
target distance 7.0
model initialize at round 481
at step 0:
{'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([17.70792842, 11.24840397,  0.        ]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 6.78638032232073}
done in step count: 4
reward sum = 0.9027181881331992
running average episode reward sum: 0.6329408538605731
{'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([22.0026117 ,  7.89031124,  0.        ]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 1.336950834412846}
episode index:482
target Thresh 16.190795218824192
target distance 10.0
model initialize at round 482
at step 0:
{'currentTarget': array([ 3., 22.]), 'previousTarget': array([ 3., 22.]), 'currentState': array([11.5623939 , 24.04320723,  0.        ]), 'targetState': array([ 3, 22], dtype=int32), 'currentDistance': 8.802799846638187}
done in step count: 8
reward sum = 0.8605776756983604
running average episode reward sum: 0.6334121516283532
{'currentTarget': array([ 3., 22.]), 'previousTarget': array([ 3., 22.]), 'currentState': array([ 3.87810004, 22.26670571,  0.        ]), 'targetState': array([ 3, 22], dtype=int32), 'currentDistance': 0.9177099824949445}
episode index:483
target Thresh 16.206596521637188
target distance 11.0
model initialize at round 483
at step 0:
{'currentTarget': array([23.,  6.]), 'previousTarget': array([23.,  6.]), 'currentState': array([19.51552659, 15.93607664,  0.        ]), 'targetState': array([23,  6], dtype=int32), 'currentDistance': 10.529348220257194}
done in step count: 9
reward sum = 0.853734070040438
running average episode reward sum: 0.6338673622035847
{'currentTarget': array([23.,  6.]), 'previousTarget': array([23.,  6.]), 'currentState': array([22.60951775,  6.62805998,  0.        ]), 'targetState': array([23,  6], dtype=int32), 'currentDistance': 0.7395510311599339}
episode index:484
target Thresh 16.222382031045385
target distance 7.0
model initialize at round 484
at step 0:
{'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([ 8.42491245, 18.40836814,  0.        ]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 6.148020564008549}
done in step count: 4
reward sum = 0.9133470610788882
running average episode reward sum: 0.6344436090053894
{'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([13.20812497, 20.12462726,  0.        ]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 1.1803997205145746}
episode index:485
target Thresh 16.2381517628343
target distance 5.0
model initialize at round 485
at step 0:
{'currentTarget': array([19., 13.]), 'previousTarget': array([19., 13.]), 'currentState': array([15.48970807, 16.85096669,  0.        ]), 'targetState': array([19, 13], dtype=int32), 'currentDistance': 5.210767114798772}
done in step count: 4
reward sum = 0.9178440647982543
running average episode reward sum: 0.6350267375152514
{'currentTarget': array([19., 13.]), 'previousTarget': array([19., 13.]), 'currentState': array([18.33818772, 13.59142148,  0.        ]), 'targetState': array([19, 13], dtype=int32), 'currentDistance': 0.8875668206148094}
episode index:486
target Thresh 16.253905732773667
target distance 5.0
model initialize at round 486
at step 0:
{'currentTarget': array([22., 26.]), 'previousTarget': array([22., 26.]), 'currentState': array([25.97638178, 25.13449712,  0.        ]), 'targetState': array([22, 26], dtype=int32), 'currentDistance': 4.069484891692089}
done in step count: 4
reward sum = 0.9403500337434638
running average episode reward sum: 0.6356536847354325
{'currentTarget': array([22., 26.]), 'previousTarget': array([22., 26.]), 'currentState': array([22.83175242, 26.45051296,  0.        ]), 'targetState': array([22, 26], dtype=int32), 'currentDistance': 0.9459249520568177}
episode index:487
target Thresh 16.26964395661745
target distance 8.0
model initialize at round 487
at step 0:
{'currentTarget': array([23., 22.]), 'previousTarget': array([23., 22.]), 'currentState': array([20.52453619, 15.48313069,  0.        ]), 'targetState': array([23, 22], dtype=int32), 'currentDistance': 6.971191191001167}
done in step count: 5
reward sum = 0.8992529718486433
running average episode reward sum: 0.636193847209025
{'currentTarget': array([23., 22.]), 'previousTarget': array([23., 22.]), 'currentState': array([22.42588347, 21.26930076,  0.        ]), 'targetState': array([23, 22], dtype=int32), 'currentDistance': 0.929263777834073}
episode index:488
target Thresh 16.28536645010388
target distance 14.0
model initialize at round 488
at step 0:
{'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([16.59651232,  5.03048843,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 12.955924617798331}
done in step count: 12
reward sum = 0.7914657452125973
running average episode reward sum: 0.6365113766527951
{'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.6802851 , 2.41196888, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.7953025648548615}
episode index:489
target Thresh 16.301073228955456
target distance 2.0
model initialize at round 489
at step 0:
{'currentTarget': array([ 8., 22.]), 'previousTarget': array([ 8., 22.]), 'currentState': array([ 9.58811659, 23.23079419,  0.        ]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 2.009220901960977}
done in step count: 2
reward sum = 0.9738588852019975
running average episode reward sum: 0.6371998409559568
{'currentTarget': array([ 8., 22.]), 'previousTarget': array([ 8., 22.]), 'currentState': array([ 8.90642345, 22.55388662,  0.        ]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 1.0622588479624615}
episode index:490
target Thresh 16.316764308878945
target distance 9.0
model initialize at round 490
at step 0:
{'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([17.73779964,  4.27768704,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 9.064975620610813}
done in step count: 7
reward sum = 0.8679611386455834
running average episode reward sum: 0.6376698232323104
{'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.686095  ,  9.33674133,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.7642781366930904}
episode index:491
target Thresh 16.33243970556544
target distance 7.0
model initialize at round 491
at step 0:
{'currentTarget': array([9., 7.]), 'previousTarget': array([9., 7.]), 'currentState': array([ 5.56646848, 12.77099371,  0.        ]), 'targetState': array([9, 7], dtype=int32), 'currentDistance': 6.7151699243991025}
done in step count: 5
reward sum = 0.9104222341460017
running average episode reward sum: 0.6382241980512406
{'currentTarget': array([9., 7.]), 'previousTarget': array([9., 7.]), 'currentState': array([8.34198344, 7.86992687, 0.        ]), 'targetState': array([9, 7], dtype=int32), 'currentDistance': 1.0907605398961873}
episode index:492
target Thresh 16.348099434690337
target distance 14.0
model initialize at round 492
at step 0:
{'currentTarget': array([ 6., 15.]), 'previousTarget': array([ 6., 15.]), 'currentState': array([10.40650454, 28.55313954,  0.        ]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 14.251486715440569}
done in step count: 17
reward sum = 0.7752002158902016
running average episode reward sum: 0.6385020398724149
{'currentTarget': array([ 6., 15.]), 'previousTarget': array([ 6., 15.]), 'currentState': array([ 6.76159538, 15.47160925,  0.        ]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 0.8957917209322412}
episode index:493
target Thresh 16.363743511913363
target distance 5.0
model initialize at round 493
at step 0:
{'currentTarget': array([25., 26.]), 'previousTarget': array([25., 26.]), 'currentState': array([21.15922761, 22.03562045,  0.        ]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 5.519767908466255}
done in step count: 4
reward sum = 0.9163858067478305
running average episode reward sum: 0.6390645576191263
{'currentTarget': array([25., 26.]), 'previousTarget': array([25., 26.]), 'currentState': array([24.4133732 , 25.41143543,  0.        ]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 0.8309869184558747}
episode index:494
target Thresh 16.379371952878596
target distance 2.0
model initialize at round 494
at step 0:
{'currentTarget': array([11., 15.]), 'previousTarget': array([11., 15.]), 'currentState': array([11.58375835, 14.05863905,  0.        ]), 'targetState': array([11, 15], dtype=int32), 'currentDistance': 1.1076706438074864}
done in step count: 0
reward sum = 0.9935301305577666
running average episode reward sum: 0.6397806496856691
{'currentTarget': array([11., 15.]), 'previousTarget': array([11., 15.]), 'currentState': array([11.58375835, 14.05863905,  0.        ]), 'targetState': array([11, 15], dtype=int32), 'currentDistance': 1.1076706438074864}
episode index:495
target Thresh 16.394984773214482
target distance 13.0
model initialize at round 495
at step 0:
{'currentTarget': array([26., 25.]), 'previousTarget': array([26., 25.]), 'currentState': array([13.63543844, 13.67604148,  0.        ]), 'targetState': array([26, 25], dtype=int32), 'currentDistance': 16.766467099264062}
done in step count: 13
reward sum = 0.7769838657054844
running average episode reward sum: 0.6400572690728057
{'currentTarget': array([26., 25.]), 'previousTarget': array([26., 25.]), 'currentState': array([25.30924281, 24.31961375,  0.        ]), 'targetState': array([26, 25], dtype=int32), 'currentDistance': 0.9695725510035129}
episode index:496
target Thresh 16.410581988533842
target distance 13.0
model initialize at round 496
at step 0:
{'currentTarget': array([10., 16.]), 'previousTarget': array([10., 16.]), 'currentState': array([21.89388561, 11.39649695,  0.        ]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 12.75369574985045}
done in step count: 11
reward sum = 0.81566755712263
running average episode reward sum: 0.6404106096926243
{'currentTarget': array([10., 16.]), 'previousTarget': array([10., 16.]), 'currentState': array([10.84395927, 16.18965022,  0.        ]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 0.8650054671387664}
episode index:497
target Thresh 16.426163614433886
target distance 13.0
model initialize at round 497
at step 0:
{'currentTarget': array([10., 26.]), 'previousTarget': array([10., 26.]), 'currentState': array([10.06639317, 14.54061925,  0.        ]), 'targetState': array([10, 26], dtype=int32), 'currentDistance': 11.459573078121538}
done in step count: 9
reward sum = 0.8339674312490675
running average episode reward sum: 0.6407992780090027
{'currentTarget': array([10., 26.]), 'previousTarget': array([10., 26.]), 'currentState': array([ 9.94326534, 25.64048535,  0.        ]), 'targetState': array([10, 26], dtype=int32), 'currentDistance': 0.36396374506141144}
episode index:498
target Thresh 16.44172966649625
target distance 5.0
model initialize at round 498
at step 0:
{'currentTarget': array([18., 14.]), 'previousTarget': array([18., 14.]), 'currentState': array([16.77002805, 10.48121715,  0.        ]), 'targetState': array([18, 14], dtype=int32), 'currentDistance': 3.727554662289959}
done in step count: 3
reward sum = 0.938982467339393
running average episode reward sum: 0.6413968395106667
{'currentTarget': array([18., 14.]), 'previousTarget': array([18., 14.]), 'currentState': array([17.52777369, 13.53290641,  0.        ]), 'targetState': array([18, 14], dtype=int32), 'currentDistance': 0.6642093824793234}
episode index:499
target Thresh 16.457280160286984
target distance 5.0
model initialize at round 499
at step 0:
{'currentTarget': array([26.,  4.]), 'previousTarget': array([26.,  4.]), 'currentState': array([22.74875998,  5.37114257,  0.        ]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 3.5285398640497743}
done in step count: 3
reward sum = 0.9362100876292565
running average episode reward sum: 0.6419864660069039
{'currentTarget': array([26.,  4.]), 'previousTarget': array([26.,  4.]), 'currentState': array([25.36342365,  3.97739241,  0.        ]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 0.6369776750032698}
episode index:500
target Thresh 16.47281511135658
target distance 7.0
model initialize at round 500
at step 0:
{'currentTarget': array([ 9., 26.]), 'previousTarget': array([ 9., 26.]), 'currentState': array([14.93231893, 23.24157356,  0.        ]), 'targetState': array([ 9, 26], dtype=int32), 'currentDistance': 6.542272103643173}
done in step count: 6
reward sum = 0.9056574046249193
running average episode reward sum: 0.6425127553055427
{'currentTarget': array([ 9., 26.]), 'previousTarget': array([ 9., 26.]), 'currentState': array([ 9.81737301, 26.14998801,  0.        ]), 'targetState': array([ 9, 26], dtype=int32), 'currentDistance': 0.8310204791397799}
episode index:501
target Thresh 16.48833453524
target distance 5.0
model initialize at round 501
at step 0:
{'currentTarget': array([14., 26.]), 'previousTarget': array([14., 26.]), 'currentState': array([17.87162662, 23.04991055,  0.        ]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 4.8674963215681615}
done in step count: 4
reward sum = 0.9248897105505174
running average episode reward sum: 0.643075259200453
{'currentTarget': array([14., 26.]), 'previousTarget': array([14., 26.]), 'currentState': array([14.87999889, 26.01125884,  0.        ]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 0.8800709132636589}
episode index:502
target Thresh 16.50383844745666
target distance 13.0
model initialize at round 502
at step 0:
{'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([20.61780655, 27.13783956,  0.        ]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 12.034095985990366}
done in step count: 11
reward sum = 0.8121103885300636
running average episode reward sum: 0.643411313135502
{'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([ 9.80494666, 24.33790632,  0.        ]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 0.8729947338829344}
episode index:503
target Thresh 16.51932686351047
target distance 3.0
model initialize at round 503
at step 0:
{'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([ 9.14849067, 23.92165124,  0.        ]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 2.4039234985486906}
done in step count: 2
reward sum = 0.9650961898413389
running average episode reward sum: 0.6440495767797596
{'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([ 7.92908674, 25.07428804,  0.        ]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 0.9320519793886712}
episode index:504
target Thresh 16.534799798889864
target distance 9.0
model initialize at round 504
at step 0:
{'currentTarget': array([22.,  6.]), 'previousTarget': array([22.,  6.]), 'currentState': array([14.62028217,  5.74676988,  0.        ]), 'targetState': array([22,  6], dtype=int32), 'currentDistance': 7.384061260572696}
done in step count: 5
reward sum = 0.8885343697863777
running average episode reward sum: 0.6445337050827431
{'currentTarget': array([22.,  6.]), 'previousTarget': array([22.,  6.]), 'currentState': array([21.15598297,  5.46496423,  0.        ]), 'targetState': array([22,  6], dtype=int32), 'currentDistance': 0.9993137767979401}
episode index:505
target Thresh 16.550257269067764
target distance 14.0
model initialize at round 505
at step 0:
{'currentTarget': array([24., 14.]), 'previousTarget': array([24., 14.]), 'currentState': array([10.19772932,  3.23699856,  0.        ]), 'targetState': array([24, 14], dtype=int32), 'currentDistance': 17.502710532426303}
done in step count: 15
reward sum = 0.7518440121732176
running average episode reward sum: 0.6447457807884555
{'currentTarget': array([24., 14.]), 'previousTarget': array([24., 14.]), 'currentState': array([23.41446199, 13.33237329,  0.        ]), 'targetState': array([24, 14], dtype=int32), 'currentDistance': 0.8880203764393914}
episode index:506
target Thresh 16.565699289501644
target distance 9.0
model initialize at round 506
at step 0:
{'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([15.73977816,  4.23178434,  0.        ]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 7.363974050756509}
done in step count: 5
reward sum = 0.8800282663269949
running average episode reward sum: 0.6452098488072691
{'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([22.30810797,  2.53736218,  0.        ]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 0.8323150446489447}
episode index:507
target Thresh 16.581125875633525
target distance 9.0
model initialize at round 507
at step 0:
{'currentTarget': array([7., 5.]), 'previousTarget': array([7., 5.]), 'currentState': array([ 2.67055315, 12.85543406,  0.        ]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 8.96950134169195}
done in step count: 8
reward sum = 0.8695662576375577
running average episode reward sum: 0.6456514952813444
{'currentTarget': array([7., 5.]), 'previousTarget': array([7., 5.]), 'currentState': array([6.64762732, 5.47753263, 0.        ]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 0.5934677000602013}
episode index:508
target Thresh 16.59653704289
target distance 13.0
model initialize at round 508
at step 0:
{'currentTarget': array([7., 7.]), 'previousTarget': array([7., 7.]), 'currentState': array([19.22517186, 16.08845532,  0.        ]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 15.233346552808054}
done in step count: 12
reward sum = 0.7927428997038559
running average episode reward sum: 0.6459404764295222
{'currentTarget': array([7., 7.]), 'previousTarget': array([7., 7.]), 'currentState': array([7.96799177, 7.38036777, 0.        ]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 1.0400421657388879}
episode index:509
target Thresh 16.611932806682233
target distance 15.0
model initialize at round 509
at step 0:
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([10.47024632, 16.36529821,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 16.67653637132953}
done in step count: 21
reward sum = 0.7318822702200684
running average episode reward sum: 0.6461089897506801
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.75057163, 2.2342217 , 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.7862681290688888}
episode index:510
target Thresh 16.62731318240599
target distance 11.0
model initialize at round 510
at step 0:
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([6.33905387, 7.35021818, 0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 10.327380476091625}
done in step count: 7
reward sum = 0.8566994128149967
running average episode reward sum: 0.6465211040815301
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.08563745, 10.39208591,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.098006563484258}
episode index:511
target Thresh 16.64267818544165
target distance 12.0
model initialize at round 511
at step 0:
{'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([11.64817023, 24.88699222,  0.        ]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 13.64327710555108}
done in step count: 9
reward sum = 0.7934578714848859
running average episode reward sum: 0.6468080899553647
{'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([21.57985234, 16.4618234 ,  0.        ]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 0.6243435835688537}
episode index:512
target Thresh 16.658027831154214
target distance 12.0
model initialize at round 512
at step 0:
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.6085543 , 18.28328753,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 11.290075602448828}
done in step count: 11
reward sum = 0.8394179190228706
running average episode reward sum: 0.6471835477118315
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.55710063,  7.70840615,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.901221609144292}
episode index:513
target Thresh 16.67336213489333
target distance 14.0
model initialize at round 513
at step 0:
{'currentTarget': array([18.,  2.]), 'previousTarget': array([18.,  2.]), 'currentState': array([22.45424587, 15.50575542,  0.        ]), 'targetState': array([18,  2], dtype=int32), 'currentDistance': 14.221312731789032}
done in step count: 16
reward sum = 0.7856785274954196
running average episode reward sum: 0.6474529931977918
{'currentTarget': array([18.,  2.]), 'previousTarget': array([18.,  2.]), 'currentState': array([18.95280777,  2.46772081,  0.        ]), 'targetState': array([18,  2], dtype=int32), 'currentDistance': 1.0614166954991309}
episode index:514
target Thresh 16.68868111199331
target distance 4.0
model initialize at round 514
at step 0:
{'currentTarget': array([24., 13.]), 'previousTarget': array([24., 13.]), 'currentState': array([26.34048593, 10.21586394,  0.        ]), 'targetState': array([24, 13], dtype=int32), 'currentDistance': 3.6372088163772065}
done in step count: 2
reward sum = 0.9548602196979914
running average episode reward sum: 0.6480499004337145
{'currentTarget': array([24., 13.]), 'previousTarget': array([24., 13.]), 'currentState': array([24.98410815, 12.33254123,  0.        ]), 'targetState': array([24, 13], dtype=int32), 'currentDistance': 1.1891047321857002}
episode index:515
target Thresh 16.703984777773123
target distance 10.0
model initialize at round 515
at step 0:
{'currentTarget': array([17.,  9.]), 'previousTarget': array([17.,  9.]), 'currentState': array([ 8.69026518, 14.01756006,  0.        ]), 'targetState': array([17,  9], dtype=int32), 'currentDistance': 9.707090285170535}
done in step count: 6
reward sum = 0.8543252426074116
running average episode reward sum: 0.6484496588487798
{'currentTarget': array([17.,  9.]), 'previousTarget': array([17.,  9.]), 'currentState': array([16.20743471,  9.54117876,  0.        ]), 'targetState': array([17,  9], dtype=int32), 'currentDistance': 0.9597052600943599}
episode index:516
target Thresh 16.719273147536434
target distance 13.0
model initialize at round 516
at step 0:
{'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([ 8.48720765, 24.42361581,  0.        ]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 16.218612330185973}
done in step count: 10
reward sum = 0.7576682096435845
running average episode reward sum: 0.64866091329906
{'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([19.48931128, 13.68201494,  0.        ]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 0.8520254410852798}
episode index:517
target Thresh 16.734546236571624
target distance 12.0
model initialize at round 517
at step 0:
{'currentTarget': array([22., 23.]), 'previousTarget': array([22., 23.]), 'currentState': array([19.50829005, 12.7328018 ,  0.        ]), 'targetState': array([22, 23], dtype=int32), 'currentDistance': 10.56522491211304}
done in step count: 7
reward sum = 0.8453904666432014
running average episode reward sum: 0.6490407000815775
{'currentTarget': array([22., 23.]), 'previousTarget': array([22., 23.]), 'currentState': array([21.58296558, 22.21726805,  0.        ]), 'targetState': array([22, 23], dtype=int32), 'currentDistance': 0.8868974089491446}
episode index:518
target Thresh 16.749804060151774
target distance 13.0
model initialize at round 518
at step 0:
{'currentTarget': array([15., 16.]), 'previousTarget': array([15., 16.]), 'currentState': array([8.38561758, 4.66272247, 0.        ]), 'targetState': array([15, 16], dtype=int32), 'currentDistance': 13.125696803380897}
done in step count: 8
reward sum = 0.8104861593894453
running average episode reward sum: 0.6493517703307257
{'currentTarget': array([15., 16.]), 'previousTarget': array([15., 16.]), 'currentState': array([14.1674757 , 15.33113581,  0.        ]), 'targetState': array([15, 16], dtype=int32), 'currentDistance': 1.0679307163380787}
episode index:519
target Thresh 16.765046633534716
target distance 14.0
model initialize at round 519
at step 0:
{'currentTarget': array([27., 15.]), 'previousTarget': array([27., 15.]), 'currentState': array([16.95865417, 27.38588905,  0.        ]), 'targetState': array([27, 15], dtype=int32), 'currentDistance': 15.944869827453982}
done in step count: 10
reward sum = 0.7674137208558023
running average episode reward sum: 0.6495788125432739
{'currentTarget': array([27., 15.]), 'previousTarget': array([27., 15.]), 'currentState': array([26.66140959, 15.56712198,  0.        ]), 'targetState': array([27, 15], dtype=int32), 'currentDistance': 0.6605079947434982}
episode index:520
target Thresh 16.780273971963023
target distance 13.0
model initialize at round 520
at step 0:
{'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([ 6.12195743, 11.69476259,  0.        ]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 13.779403024307408}
done in step count: 9
reward sum = 0.8029297798629236
running average episode reward sum: 0.6498731522118337
{'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([13.02179598, 22.09504095,  0.        ]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 1.3326042138434218}
episode index:521
target Thresh 16.795486090664035
target distance 8.0
model initialize at round 521
at step 0:
{'currentTarget': array([ 7., 22.]), 'previousTarget': array([ 7., 22.]), 'currentState': array([10.70152417, 14.93739969,  0.        ]), 'targetState': array([ 7, 22], dtype=int32), 'currentDistance': 7.973807394977026}
done in step count: 6
reward sum = 0.8987686623045555
running average episode reward sum: 0.6503499635338504
{'currentTarget': array([ 7., 22.]), 'previousTarget': array([ 7., 22.]), 'currentState': array([ 7.93262714, 21.16632646,  0.        ]), 'targetState': array([ 7, 22], dtype=int32), 'currentDistance': 1.2509216406074177}
episode index:522
target Thresh 16.81068300484987
target distance 16.0
model initialize at round 522
at step 0:
{'currentTarget': array([20.,  9.]), 'previousTarget': array([20.,  9.]), 'currentState': array([14.98296511, 23.86776388,  0.        ]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 15.691432113411263}
done in step count: 13
reward sum = 0.7753827684556386
running average episode reward sum: 0.650589031994504
{'currentTarget': array([20.,  9.]), 'previousTarget': array([20.,  9.]), 'currentState': array([19.92656971,  9.48403805,  0.        ]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 0.4895761906602805}
episode index:523
target Thresh 16.825864729717438
target distance 11.0
model initialize at round 523
at step 0:
{'currentTarget': array([19., 28.]), 'previousTarget': array([19., 28.]), 'currentState': array([ 9.55793786, 27.82501416,  0.        ]), 'targetState': array([19, 28], dtype=int32), 'currentDistance': 9.443683470495522}
done in step count: 7
reward sum = 0.8572040100958207
running average episode reward sum: 0.6509833353878272
{'currentTarget': array([19., 28.]), 'previousTarget': array([19., 28.]), 'currentState': array([18.23793146, 27.41735502,  0.        ]), 'targetState': array([19, 28], dtype=int32), 'currentDistance': 0.9592828741445785}
episode index:524
target Thresh 16.841031280448476
target distance 14.0
model initialize at round 524
at step 0:
{'currentTarget': array([ 2., 14.]), 'previousTarget': array([ 2., 14.]), 'currentState': array([14.61715841, 14.57857147,  0.        ]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 12.630416911619522}
done in step count: 13
reward sum = 0.8067715706787648
running average episode reward sum: 0.6512800748836194
{'currentTarget': array([ 2., 14.]), 'previousTarget': array([ 2., 14.]), 'currentState': array([ 2.7021479 , 14.18990746,  0.        ]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 0.7273764634391117}
episode index:525
target Thresh 16.856182672209535
target distance 12.0
model initialize at round 525
at step 0:
{'currentTarget': array([11., 12.]), 'previousTarget': array([11., 12.]), 'currentState': array([ 6.7197665 , 22.81779385,  0.        ]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 11.633789689191492}
done in step count: 9
reward sum = 0.8414865227413091
running average episode reward sum: 0.6516416841000789
{'currentTarget': array([11., 12.]), 'previousTarget': array([11., 12.]), 'currentState': array([10.5162279 , 12.82571959,  0.        ]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 0.9569996307630587}
episode index:526
target Thresh 16.871318920152
target distance 10.0
model initialize at round 526
at step 0:
{'currentTarget': array([18.,  2.]), 'previousTarget': array([18.,  2.]), 'currentState': array([9.56147206, 6.06265628, 0.        ]), 'targetState': array([18,  2], dtype=int32), 'currentDistance': 9.365571520189922}
done in step count: 6
reward sum = 0.8601801860604262
running average episode reward sum: 0.6520373928324514
{'currentTarget': array([18.,  2.]), 'previousTarget': array([18.,  2.]), 'currentState': array([17.16572529,  2.09962755,  0.        ]), 'targetState': array([18,  2], dtype=int32), 'currentDistance': 0.840202320902996}
episode index:527
target Thresh 16.886440039412122
target distance 12.0
model initialize at round 527
at step 0:
{'currentTarget': array([24.,  3.]), 'previousTarget': array([24.,  3.]), 'currentState': array([22.63206792, 13.83093238,  0.        ]), 'targetState': array([24,  3], dtype=int32), 'currentDistance': 10.916974597950848}
done in step count: 9
reward sum = 0.8484520271785644
running average episode reward sum: 0.6524093902459858
{'currentTarget': array([24.,  3.]), 'previousTarget': array([24.,  3.]), 'currentState': array([23.79026369,  3.77976686,  0.        ]), 'targetState': array([24,  3], dtype=int32), 'currentDistance': 0.8074810668531015}
episode index:528
target Thresh 16.90154604511103
target distance 14.0
model initialize at round 528
at step 0:
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([26.83397442, 19.90384746,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 16.84886586457872}
done in step count: 15
reward sum = 0.7688834225110881
running average episode reward sum: 0.6526295680007401
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.96865568,  7.53885579,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.1084490883941231}
episode index:529
target Thresh 16.91663695235472
target distance 6.0
model initialize at round 529
at step 0:
{'currentTarget': array([ 3., 21.]), 'previousTarget': array([ 3., 21.]), 'currentState': array([ 7.74401259, 24.22681814,  0.        ]), 'targetState': array([ 3, 21], dtype=int32), 'currentDistance': 5.737421967283717}
done in step count: 6
reward sum = 0.9078035087634239
running average episode reward sum: 0.6531110282663302
{'currentTarget': array([ 3., 21.]), 'previousTarget': array([ 3., 21.]), 'currentState': array([ 3.85310745, 21.31373829,  0.        ]), 'targetState': array([ 3, 21], dtype=int32), 'currentDistance': 0.9089686670840946}
episode index:530
target Thresh 16.931712776234107
target distance 7.0
model initialize at round 530
at step 0:
{'currentTarget': array([5., 2.]), 'previousTarget': array([5., 2.]), 'currentState': array([9.49312162, 7.8740406 , 0.        ]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 7.395437440826394}
done in step count: 7
reward sum = 0.8861662031180016
running average episode reward sum: 0.6535499269007025
{'currentTarget': array([5., 2.]), 'previousTarget': array([5., 2.]), 'currentState': array([5.71640038, 2.2413328 , 0.        ]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 0.7559570301656101}
episode index:531
target Thresh 16.946773531825013
target distance 1.0
model initialize at round 531
at step 0:
{'currentTarget': array([26., 19.]), 'previousTarget': array([26., 19.]), 'currentState': array([25.40975207, 19.31017816,  0.        ]), 'targetState': array([26, 19], dtype=int32), 'currentDistance': 0.6667856545826141}
done in step count: 0
reward sum = 0.9967812456806705
running average episode reward sum: 0.6541950985525445
{'currentTarget': array([26., 19.]), 'previousTarget': array([26., 19.]), 'currentState': array([25.40975207, 19.31017816,  0.        ]), 'targetState': array([26, 19], dtype=int32), 'currentDistance': 0.6667856545826141}
episode index:532
target Thresh 16.961819234188198
target distance 14.0
model initialize at round 532
at step 0:
{'currentTarget': array([ 3., 20.]), 'previousTarget': array([ 3., 20.]), 'currentState': array([7.97530366, 6.61473554, 0.        ]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 14.280019295655048}
done in step count: 13
reward sum = 0.8049283701686403
running average episode reward sum: 0.6544779001878467
{'currentTarget': array([ 3., 20.]), 'previousTarget': array([ 3., 20.]), 'currentState': array([ 3.54539399, 19.63693786,  0.        ]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 0.655186021369203}
episode index:533
target Thresh 16.976849898369366
target distance 10.0
model initialize at round 533
at step 0:
{'currentTarget': array([11.,  5.]), 'previousTarget': array([11.,  5.]), 'currentState': array([19.72985578,  6.32287467,  0.        ]), 'targetState': array([11,  5], dtype=int32), 'currentDistance': 8.829517497963018}
done in step count: 9
reward sum = 0.8644576976850676
running average episode reward sum: 0.6548711207824107
{'currentTarget': array([11.,  5.]), 'previousTarget': array([11.,  5.]), 'currentState': array([11.91062638,  5.33894576,  0.        ]), 'targetState': array([11,  5], dtype=int32), 'currentDistance': 0.971660760514847}
episode index:534
target Thresh 16.99186553939918
target distance 13.0
model initialize at round 534
at step 0:
{'currentTarget': array([25.,  8.]), 'previousTarget': array([25.,  8.]), 'currentState': array([13.46861339, 16.59950447,  0.        ]), 'targetState': array([25,  8], dtype=int32), 'currentDistance': 14.3848654627607}
done in step count: 9
reward sum = 0.7901579525888125
running average episode reward sum: 0.6551239933652264
{'currentTarget': array([25.,  8.]), 'previousTarget': array([25.,  8.]), 'currentState': array([24.15854049,  8.40333951,  0.        ]), 'targetState': array([25,  8], dtype=int32), 'currentDistance': 0.9331328244052873}
episode index:535
target Thresh 17.00686617229328
target distance 6.0
model initialize at round 535
at step 0:
{'currentTarget': array([27., 10.]), 'previousTarget': array([27., 10.]), 'currentState': array([22.49408245,  6.99289072,  0.        ]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 5.417194772935418}
done in step count: 4
reward sum = 0.9159634484567056
running average episode reward sum: 0.6556106341396508
{'currentTarget': array([27., 10.]), 'previousTarget': array([27., 10.]), 'currentState': array([26.2313301 ,  9.35188055,  0.        ]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 1.0054413166246106}
episode index:536
target Thresh 17.021851812052304
target distance 17.0
model initialize at round 536
at step 0:
{'currentTarget': array([ 8., 13.]), 'previousTarget': array([ 8., 13.]), 'currentState': array([23.56782866,  4.30592796,  0.        ]), 'targetState': array([ 8, 13], dtype=int32), 'currentDistance': 17.830989250173847}
done in step count: 18
reward sum = 0.7292945313013278
running average episode reward sum: 0.6557478481008457
{'currentTarget': array([ 8., 13.]), 'previousTarget': array([ 8., 13.]), 'currentState': array([ 8.67279899, 12.57814103,  0.        ]), 'targetState': array([ 8, 13], dtype=int32), 'currentDistance': 0.7941180461576266}
episode index:537
target Thresh 17.036822473661893
target distance 16.0
model initialize at round 537
at step 0:
{'currentTarget': array([ 6., 19.]), 'previousTarget': array([ 6., 19.]), 'currentState': array([15.30307674,  4.08647442,  0.        ]), 'targetState': array([ 6, 19], dtype=int32), 'currentDistance': 17.577271749964744}
done in step count: 16
reward sum = 0.7586931907042203
running average episode reward sum: 0.655939196321298
{'currentTarget': array([ 6., 19.]), 'previousTarget': array([ 6., 19.]), 'currentState': array([ 6.56638029, 18.67123026,  0.        ]), 'targetState': array([ 6, 19], dtype=int32), 'currentDistance': 0.6548863867646166}
episode index:538
target Thresh 17.051778172092703
target distance 9.0
model initialize at round 538
at step 0:
{'currentTarget': array([24.,  6.]), 'previousTarget': array([24.,  6.]), 'currentState': array([15.99876457, 13.58826411,  0.        ]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 11.027307944467005}
done in step count: 7
reward sum = 0.8482476486933491
running average episode reward sum: 0.6562959838025079
{'currentTarget': array([24.,  6.]), 'previousTarget': array([24.,  6.]), 'currentState': array([23.01429552,  6.74510729,  0.        ]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 1.2356367602673677}
episode index:539
target Thresh 17.066718922300446
target distance 10.0
model initialize at round 539
at step 0:
{'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([20.75716913,  6.51419187,  0.        ]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 8.887113599692398}
done in step count: 9
reward sum = 0.8651639798538088
running average episode reward sum: 0.656682776387788
{'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([12.81751126,  5.35982331,  0.        ]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 0.8931950932898229}
episode index:540
target Thresh 17.081644739225865
target distance 13.0
model initialize at round 540
at step 0:
{'currentTarget': array([20.,  4.]), 'previousTarget': array([20.,  4.]), 'currentState': array([8.48127091, 2.7835371 , 0.        ]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 11.582784718723746}
done in step count: 8
reward sum = 0.8383409442776232
running average episode reward sum: 0.6570185585835179
{'currentTarget': array([20.,  4.]), 'previousTarget': array([20.,  4.]), 'currentState': array([19.06326103,  3.24964086,  0.        ]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 1.200216116569163}
episode index:541
target Thresh 17.09655563779478
target distance 10.0
model initialize at round 541
at step 0:
{'currentTarget': array([26.,  4.]), 'previousTarget': array([26.,  4.]), 'currentState': array([17.50889528,  3.1363564 ,  0.        ]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 8.534912985545802}
done in step count: 6
reward sum = 0.875440031435546
running average episode reward sum: 0.6574215502308464
{'currentTarget': array([26.,  4.]), 'previousTarget': array([26.,  4.]), 'currentState': array([25.2265718 ,  3.29904405,  0.        ]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 1.0438057417929607}
episode index:542
target Thresh 17.11145163291809
target distance 14.0
model initialize at round 542
at step 0:
{'currentTarget': array([ 6., 12.]), 'previousTarget': array([ 6., 12.]), 'currentState': array([ 8.48723027, 25.43826222,  0.        ]), 'targetState': array([ 6, 12], dtype=int32), 'currentDistance': 13.666499406496769}
done in step count: 15
reward sum = 0.7961839861168246
running average episode reward sum: 0.6576770979949089
{'currentTarget': array([ 6., 12.]), 'previousTarget': array([ 6., 12.]), 'currentState': array([ 6.74168903, 12.35174662,  0.        ]), 'targetState': array([ 6, 12], dtype=int32), 'currentDistance': 0.8208704562710185}
episode index:543
target Thresh 17.126332739491794
target distance 16.0
model initialize at round 543
at step 0:
{'currentTarget': array([7., 6.]), 'previousTarget': array([7., 6.]), 'currentState': array([21.70801091,  8.78888883,  0.        ]), 'targetState': array([7, 6], dtype=int32), 'currentDistance': 14.970086368320413}
done in step count: 14
reward sum = 0.7849555058052278
running average episode reward sum: 0.657911065656325
{'currentTarget': array([7., 6.]), 'previousTarget': array([7., 6.]), 'currentState': array([7.77015424, 6.4186119 , 0.        ]), 'targetState': array([7, 6], dtype=int32), 'currentDistance': 0.8765691488407671}
episode index:544
target Thresh 17.141198972396992
target distance 15.0
model initialize at round 544
at step 0:
{'currentTarget': array([17., 11.]), 'previousTarget': array([17., 11.]), 'currentState': array([ 9.6866858 , 24.75446796,  0.        ]), 'targetState': array([17, 11], dtype=int32), 'currentDistance': 15.577867426489599}
done in step count: 11
reward sum = 0.7938264754220473
running average episode reward sum: 0.6581604517292896
{'currentTarget': array([17., 11.]), 'previousTarget': array([17., 11.]), 'currentState': array([16.30732444, 11.88057548,  0.        ]), 'targetState': array([17, 11], dtype=int32), 'currentDistance': 1.120362711357027}
episode index:545
target Thresh 17.15605034649993
target distance 5.0
model initialize at round 545
at step 0:
{'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([21.96834946, 10.97110248,  0.        ]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 4.996054432450586}
done in step count: 4
reward sum = 0.9274496420209237
running average episode reward sum: 0.6586536553745124
{'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([24.44514591,  7.46302372,  0.        ]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 0.7226714564809165}
episode index:546
target Thresh 17.170886876651977
target distance 17.0
model initialize at round 546
at step 0:
{'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([ 5.56595516, 13.00552522,  0.        ]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 15.563800032687944}
done in step count: 11
reward sum = 0.7754504180057431
running average episode reward sum: 0.6588671777924854
{'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([20.33033916, 10.38175509,  0.        ]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 0.9114123117059815}
episode index:547
target Thresh 17.185708577689667
target distance 8.0
model initialize at round 547
at step 0:
{'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([23.59499836, 27.75323725,  0.        ]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 7.168699059837754}
done in step count: 6
reward sum = 0.8968323400171925
running average episode reward sum: 0.6593014207892457
{'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([25.49110898, 21.52593267,  0.        ]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 0.7318300618376695}
episode index:548
target Thresh 17.200515464434698
target distance 14.0
model initialize at round 548
at step 0:
{'currentTarget': array([ 4., 28.]), 'previousTarget': array([ 4., 28.]), 'currentState': array([16.61557162, 18.772686  ,  0.        ]), 'targetState': array([ 4, 28], dtype=int32), 'currentDistance': 15.629970275720286}
done in step count: 14
reward sum = 0.7797563494568234
running average episode reward sum: 0.6595208286738862
{'currentTarget': array([ 4., 28.]), 'previousTarget': array([ 4., 28.]), 'currentState': array([ 4.98053825, 27.05903833,  0.        ]), 'targetState': array([ 4, 28], dtype=int32), 'currentDistance': 1.3589937930130158}
episode index:549
target Thresh 17.21530755169396
target distance 8.0
model initialize at round 549
at step 0:
{'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([14.81836323, 15.98249698,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 7.215380023531141}
done in step count: 6
reward sum = 0.9003558398071373
running average episode reward sum: 0.6599587105123103
{'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.49084415,  9.53552496,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.7264399285747161}
episode index:550
target Thresh 17.23008485425954
target distance 3.0
model initialize at round 550
at step 0:
{'currentTarget': array([17., 25.]), 'previousTarget': array([17., 25.]), 'currentState': array([15.10200787, 23.81111294,  0.        ]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 2.2396041127348476}
done in step count: 2
reward sum = 0.9699648265991768
running average episode reward sum: 0.6605213350424134
{'currentTarget': array([17., 25.]), 'previousTarget': array([17., 25.]), 'currentState': array([16.17554376, 24.39537486,  0.        ]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 1.022398970123126}
episode index:551
target Thresh 17.244847386908745
target distance 10.0
model initialize at round 551
at step 0:
{'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([ 8.42570949, 19.87454519,  0.        ]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 8.57520826152369}
done in step count: 7
reward sum = 0.8702819382855435
running average episode reward sum: 0.6609013361352452
{'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([16.31247061, 19.53462494,  0.        ]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 0.8302232231749788}
episode index:552
target Thresh 17.25959516440411
target distance 11.0
model initialize at round 552
at step 0:
{'currentTarget': array([7., 3.]), 'previousTarget': array([7., 3.]), 'currentState': array([14.59860685, 12.85245442,  0.        ]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 12.44225398820726}
done in step count: 11
reward sum = 0.8264416684772387
running average episode reward sum: 0.6612006857416503
{'currentTarget': array([7., 3.]), 'previousTarget': array([7., 3.]), 'currentState': array([7.93020853, 3.49883416, 0.        ]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 1.0555204587581388}
episode index:553
target Thresh 17.274328201493404
target distance 17.0
model initialize at round 553
at step 0:
{'currentTarget': array([24., 24.]), 'previousTarget': array([24., 24.]), 'currentState': array([26.88422926,  8.31769562,  0.        ]), 'targetState': array([24, 24], dtype=int32), 'currentDistance': 15.945326875008073}
done in step count: 15
reward sum = 0.7752108233848345
running average episode reward sum: 0.6614064802139304
{'currentTarget': array([24., 24.]), 'previousTarget': array([24., 24.]), 'currentState': array([24.14929119, 23.53637511,  0.        ]), 'targetState': array([24, 24], dtype=int32), 'currentDistance': 0.48706868375648077}
episode index:554
target Thresh 17.289046512909678
target distance 8.0
model initialize at round 554
at step 0:
{'currentTarget': array([17.,  2.]), 'previousTarget': array([17.,  2.]), 'currentState': array([10.64619887,  4.17450178,  0.        ]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 6.7155972774061174}
done in step count: 5
reward sum = 0.89413087843286
running average episode reward sum: 0.6618258034539646
{'currentTarget': array([17.,  2.]), 'previousTarget': array([17.,  2.]), 'currentState': array([16.12685978,  1.82462367,  0.        ]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 0.8905788523440951}
episode index:555
target Thresh 17.303750113371237
target distance 16.0
model initialize at round 555
at step 0:
{'currentTarget': array([ 3., 29.]), 'previousTarget': array([ 3., 29.]), 'currentState': array([17.8535502 , 22.27094054,  0.        ]), 'targetState': array([ 3, 29], dtype=int32), 'currentDistance': 16.306691712371425}
done in step count: 18
reward sum = 0.751456403246847
running average episode reward sum: 0.6619870095686999
{'currentTarget': array([ 3., 29.]), 'previousTarget': array([ 3., 29.]), 'currentState': array([ 3.75294006, 29.232776  ,  0.        ]), 'targetState': array([ 3, 29], dtype=int32), 'currentDistance': 0.7881011348132092}
episode index:556
target Thresh 17.31843901758169
target distance 16.0
model initialize at round 556
at step 0:
{'currentTarget': array([25., 28.]), 'previousTarget': array([25., 28.]), 'currentState': array([16.77151066, 13.49258494,  0.        ]), 'targetState': array([25, 28], dtype=int32), 'currentDistance': 16.678522968821728}
done in step count: 11
reward sum = 0.7608723379759976
running average episode reward sum: 0.6621645415766125
{'currentTarget': array([25., 28.]), 'previousTarget': array([25., 28.]), 'currentState': array([24.45975742, 27.52137923,  0.        ]), 'targetState': array([25, 28], dtype=int32), 'currentDistance': 0.7217616553829743}
episode index:557
target Thresh 17.333113240229927
target distance 6.0
model initialize at round 557
at step 0:
{'currentTarget': array([18., 11.]), 'previousTarget': array([18., 11.]), 'currentState': array([20.56758484, 15.90797102,  0.        ]), 'targetState': array([18, 11], dtype=int32), 'currentDistance': 5.539013585522808}
done in step count: 5
reward sum = 0.9182523317901762
running average episode reward sum: 0.6626234802687516
{'currentTarget': array([18., 11.]), 'previousTarget': array([18., 11.]), 'currentState': array([18.79917258, 11.38996524,  0.        ]), 'targetState': array([18, 11], dtype=int32), 'currentDistance': 0.8892410800481519}
episode index:558
target Thresh 17.34777279599019
target distance 15.0
model initialize at round 558
at step 0:
{'currentTarget': array([11., 29.]), 'previousTarget': array([11., 29.]), 'currentState': array([24.59841859, 20.92342842,  0.        ]), 'targetState': array([11, 29], dtype=int32), 'currentDistance': 15.816067677804988}
done in step count: 15
reward sum = 0.7651805641656847
running average episode reward sum: 0.6628069455351147
{'currentTarget': array([11., 29.]), 'previousTarget': array([11., 29.]), 'currentState': array([11.74913669, 28.70372668,  0.        ]), 'targetState': array([11, 29], dtype=int32), 'currentDistance': 0.8055952181356362}
episode index:559
target Thresh 17.362417699522023
target distance 13.0
model initialize at round 559
at step 0:
{'currentTarget': array([14., 29.]), 'previousTarget': array([14., 29.]), 'currentState': array([18.75355741, 17.06522822,  0.        ]), 'targetState': array([14, 29], dtype=int32), 'currentDistance': 12.846598204832722}
done in step count: 12
reward sum = 0.8195808344619439
running average episode reward sum: 0.6630868989081982
{'currentTarget': array([14., 29.]), 'previousTarget': array([14., 29.]), 'currentState': array([14.40473419, 28.5903241 ,  0.        ]), 'targetState': array([14, 29], dtype=int32), 'currentDistance': 0.5758854989659723}
episode index:560
target Thresh 17.37704796547034
target distance 8.0
model initialize at round 560
at step 0:
{'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([9.47838426, 6.51266962, 0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 6.979848444809439}
done in step count: 5
reward sum = 0.8981063489300345
running average episode reward sum: 0.6635058284091283
{'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.13288701,  8.41727073,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.0447288361566918}
episode index:561
target Thresh 17.3916636084654
target distance 8.0
model initialize at round 561
at step 0:
{'currentTarget': array([13.,  4.]), 'previousTarget': array([13.,  4.]), 'currentState': array([15.71510187, 11.05485493,  0.        ]), 'targetState': array([13,  4], dtype=int32), 'currentDistance': 7.559282785337331}
done in step count: 7
reward sum = 0.8919345689770612
running average episode reward sum: 0.6639122852428789
{'currentTarget': array([13.,  4.]), 'previousTarget': array([13.,  4.]), 'currentState': array([13.7958315 ,  4.48911881,  0.        ]), 'targetState': array([13,  4], dtype=int32), 'currentDistance': 0.9341225794407282}
episode index:562
target Thresh 17.406264643122853
target distance 7.0
model initialize at round 562
at step 0:
{'currentTarget': array([ 9., 22.]), 'previousTarget': array([ 9., 22.]), 'currentState': array([ 9.10030144, 16.27998924,  0.        ]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 5.720890092029134}
done in step count: 5
reward sum = 0.9152973506126025
running average episode reward sum: 0.6643587951280827
{'currentTarget': array([ 9., 22.]), 'previousTarget': array([ 9., 22.]), 'currentState': array([ 8.41746839, 21.50035501,  0.        ]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 0.7674556637553769}
episode index:563
target Thresh 17.420851084043733
target distance 17.0
model initialize at round 563
at step 0:
{'currentTarget': array([19.,  4.]), 'previousTarget': array([19.,  4.]), 'currentState': array([ 3.57647157, 17.21503425,  0.        ]), 'targetState': array([19,  4], dtype=int32), 'currentDistance': 20.310646455825072}
done in step count: 11
reward sum = 0.6993028375827027
running average episode reward sum: 0.6644207526501654
{'currentTarget': array([19.,  4.]), 'previousTarget': array([19.,  4.]), 'currentState': array([18.14908111,  4.8094874 ,  0.        ]), 'targetState': array([19,  4], dtype=int32), 'currentDistance': 1.1744500033811445}
episode index:564
target Thresh 17.43542294581448
target distance 6.0
model initialize at round 564
at step 0:
{'currentTarget': array([27., 19.]), 'previousTarget': array([27., 19.]), 'currentState': array([22.65833437, 21.24554467,  0.        ]), 'targetState': array([27, 19], dtype=int32), 'currentDistance': 4.887998698677912}
done in step count: 4
reward sum = 0.918670160969467
running average episode reward sum: 0.664870751602943
{'currentTarget': array([27., 19.]), 'previousTarget': array([27., 19.]), 'currentState': array([26.30841583, 18.95635715,  0.        ]), 'targetState': array([27, 19], dtype=int32), 'currentDistance': 0.6929598561623053}
episode index:565
target Thresh 17.449980243006962
target distance 13.0
model initialize at round 565
at step 0:
{'currentTarget': array([9., 2.]), 'previousTarget': array([9., 2.]), 'currentState': array([16.71092221, 13.99678648,  0.        ]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 14.261178320233427}
done in step count: 15
reward sum = 0.7925714206066647
running average episode reward sum: 0.6650963711594867
{'currentTarget': array([9., 2.]), 'previousTarget': array([9., 2.]), 'currentState': array([9.70290124, 2.38052548, 0.        ]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 0.7992933150842436}
episode index:566
target Thresh 17.464522990178473
target distance 14.0
model initialize at round 566
at step 0:
{'currentTarget': array([26., 29.]), 'previousTarget': array([26., 29.]), 'currentState': array([14.21257088, 16.1359967 ,  0.        ]), 'targetState': array([26, 29], dtype=int32), 'currentDistance': 17.447809784487035}
done in step count: 15
reward sum = 0.7588615729287741
running average episode reward sum: 0.6652617418857112
{'currentTarget': array([26., 29.]), 'previousTarget': array([26., 29.]), 'currentState': array([25.13859764, 28.36567023,  0.        ]), 'targetState': array([26, 29], dtype=int32), 'currentDistance': 1.0697608528753098}
episode index:567
target Thresh 17.479051201871766
target distance 7.0
model initialize at round 567
at step 0:
{'currentTarget': array([19.,  7.]), 'previousTarget': array([19.,  7.]), 'currentState': array([15.70466644, 12.77124095,  0.        ]), 'targetState': array([19,  7], dtype=int32), 'currentDistance': 6.645784031893847}
done in step count: 5
reward sum = 0.9103360749847001
running average episode reward sum: 0.6656932107820123
{'currentTarget': array([19.,  7.]), 'previousTarget': array([19.,  7.]), 'currentState': array([18.22605789,  7.79884636,  0.        ]), 'targetState': array([19,  7], dtype=int32), 'currentDistance': 1.1122688107943717}
episode index:568
target Thresh 17.493564892615048
target distance 2.0
model initialize at round 568
at step 0:
{'currentTarget': array([13., 20.]), 'previousTarget': array([13., 20.]), 'currentState': array([14.52075467, 20.70047152,  0.        ]), 'targetState': array([13, 20], dtype=int32), 'currentDistance': 1.674322281539303}
done in step count: 2
reward sum = 0.9770749479322914
running average episode reward sum: 0.666240454608287
{'currentTarget': array([13., 20.]), 'previousTarget': array([13., 20.]), 'currentState': array([13.83674994, 20.38559939,  0.        ]), 'targetState': array([13, 20], dtype=int32), 'currentDistance': 0.921323696246017}
episode index:569
target Thresh 17.50806407692201
target distance 17.0
model initialize at round 569
at step 0:
{'currentTarget': array([19., 20.]), 'previousTarget': array([19., 20.]), 'currentState': array([ 3.24123919, 16.9728077 ,  0.        ]), 'targetState': array([19, 20], dtype=int32), 'currentDistance': 16.046882422986965}
done in step count: 12
reward sum = 0.7826770874481794
running average episode reward sum: 0.6664447294027429
{'currentTarget': array([19., 20.]), 'previousTarget': array([19., 20.]), 'currentState': array([18.07400972, 19.25656295,  0.        ]), 'targetState': array([19, 20], dtype=int32), 'currentDistance': 1.1875001705540686}
episode index:570
target Thresh 17.522548769291852
target distance 2.0
model initialize at round 570
at step 0:
{'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([26.29315338,  7.93286771,  0.        ]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 1.2800013609383576}
done in step count: 1
reward sum = 0.9837057022726723
running average episode reward sum: 0.6670003528228304
{'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([26.50770444,  8.44247895,  0.        ]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 0.7437638317482034}
episode index:571
target Thresh 17.537018984209247
target distance 11.0
model initialize at round 571
at step 0:
{'currentTarget': array([ 3., 14.]), 'previousTarget': array([ 3., 14.]), 'currentState': array([12.96541715, 21.51522884,  0.        ]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 12.481514467035852}
done in step count: 11
reward sum = 0.8244125064480133
running average episode reward sum: 0.6672755488956017
{'currentTarget': array([ 3., 14.]), 'previousTarget': array([ 3., 14.]), 'currentState': array([ 3.84361881, 14.43824799,  0.        ]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 0.9506597713004854}
episode index:572
target Thresh 17.551474736144424
target distance 13.0
model initialize at round 572
at step 0:
{'currentTarget': array([ 8., 23.]), 'previousTarget': array([ 8., 23.]), 'currentState': array([15.69597253, 10.80378604,  0.        ]), 'targetState': array([ 8, 23], dtype=int32), 'currentDistance': 14.421360136537611}
done in step count: 16
reward sum = 0.792777960640545
running average episode reward sum: 0.6674945757921897
{'currentTarget': array([ 8., 23.]), 'previousTarget': array([ 8., 23.]), 'currentState': array([ 8.64051323, 22.45547432,  0.        ]), 'targetState': array([ 8, 23], dtype=int32), 'currentDistance': 0.8406934117420641}
episode index:573
target Thresh 17.56591603955313
target distance 6.0
model initialize at round 573
at step 0:
{'currentTarget': array([ 8., 18.]), 'previousTarget': array([ 8., 18.]), 'currentState': array([ 3.58647132, 20.34885818,  0.        ]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 4.999637002325784}
done in step count: 4
reward sum = 0.9217468848502929
running average episode reward sum: 0.6679375240658101
{'currentTarget': array([ 8., 18.]), 'previousTarget': array([ 8., 18.]), 'currentState': array([ 7.1500442 , 18.00654522,  0.        ]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 0.849980998044501}
episode index:574
target Thresh 17.58034290887668
target distance 4.0
model initialize at round 574
at step 0:
{'currentTarget': array([26., 26.]), 'previousTarget': array([26., 26.]), 'currentState': array([25.85253528, 23.09851015,  0.        ]), 'targetState': array([26, 26], dtype=int32), 'currentDistance': 2.905234795399045}
done in step count: 2
reward sum = 0.9628421716014814
running average episode reward sum: 0.6684504017136983
{'currentTarget': array([26., 26.]), 'previousTarget': array([26., 26.]), 'currentState': array([25.54355259, 25.19882727,  0.        ]), 'targetState': array([26, 26], dtype=int32), 'currentDistance': 0.9220748258695616}
episode index:575
target Thresh 17.59475535854193
target distance 1.0
model initialize at round 575
at step 0:
{'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([23.26094908, 11.50168371,  0.        ]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 0.8913559290741039}
done in step count: 0
reward sum = 0.9984010951458856
running average episode reward sum: 0.6690232327786848
{'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([23.26094908, 11.50168371,  0.        ]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 0.8913559290741039}
episode index:576
target Thresh 17.609153402961347
target distance 13.0
model initialize at round 576
at step 0:
{'currentTarget': array([14., 26.]), 'previousTarget': array([14., 26.]), 'currentState': array([17.89666318, 13.81140524,  0.        ]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 12.79632080471265}
done in step count: 12
reward sum = 0.8262853650992557
running average episode reward sum: 0.6692957841345263
{'currentTarget': array([14., 26.]), 'previousTarget': array([14., 26.]), 'currentState': array([14.26394794, 25.2458899 ,  0.        ]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 0.7989684320931083}
episode index:577
target Thresh 17.62353705653296
target distance 14.0
model initialize at round 577
at step 0:
{'currentTarget': array([25., 21.]), 'previousTarget': array([25., 21.]), 'currentState': array([12.68188596,  7.99039012,  0.        ]), 'targetState': array([25, 21], dtype=int32), 'currentDistance': 17.91607888847523}
done in step count: 14
reward sum = 0.760316033167583
running average episode reward sum: 0.6694532586138222
{'currentTarget': array([25., 21.]), 'previousTarget': array([25., 21.]), 'currentState': array([24.03963518, 20.24313313,  0.        ]), 'targetState': array([25, 21], dtype=int32), 'currentDistance': 1.222762465548443}
episode index:578
target Thresh 17.63790633364043
target distance 13.0
model initialize at round 578
at step 0:
{'currentTarget': array([10., 25.]), 'previousTarget': array([10., 25.]), 'currentState': array([22.18323296, 19.70558149,  0.        ]), 'targetState': array([10, 25], dtype=int32), 'currentDistance': 13.283901265509433}
done in step count: 13
reward sum = 0.8099679509633742
running average episode reward sum: 0.6696959437474138
{'currentTarget': array([10., 25.]), 'previousTarget': array([10., 25.]), 'currentState': array([10.87281919, 24.86104399,  0.        ]), 'targetState': array([10, 25], dtype=int32), 'currentDistance': 0.8838111238119052}
episode index:579
target Thresh 17.652261248653044
target distance 16.0
model initialize at round 579
at step 0:
{'currentTarget': array([27., 26.]), 'previousTarget': array([27., 26.]), 'currentState': array([12.63146472, 25.98782506,  0.        ]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 14.36854043834709}
done in step count: 10
reward sum = 0.7952052086902841
running average episode reward sum: 0.669912339031798
{'currentTarget': array([27., 26.]), 'previousTarget': array([27., 26.]), 'currentState': array([26.08992079, 25.37705533,  0.        ]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 1.1028618377103998}
episode index:580
target Thresh 17.66660181592571
target distance 6.0
model initialize at round 580
at step 0:
{'currentTarget': array([ 3., 16.]), 'previousTarget': array([ 3., 16.]), 'currentState': array([ 7.32930732, 10.64239043,  0.        ]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 6.8881697246962785}
done in step count: 6
reward sum = 0.9044508205417496
running average episode reward sum: 0.670316019722865
{'currentTarget': array([ 3., 16.]), 'previousTarget': array([ 3., 16.]), 'currentState': array([ 3.48163706, 15.70381427,  0.        ]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 0.5654204152295004}
episode index:581
target Thresh 17.680928049798993
target distance 13.0
model initialize at round 581
at step 0:
{'currentTarget': array([26., 24.]), 'previousTarget': array([26., 24.]), 'currentState': array([13.53272623, 15.90946954,  0.        ]), 'targetState': array([26, 24], dtype=int32), 'currentDistance': 14.86235507398428}
done in step count: 12
reward sum = 0.7961702669955943
running average episode reward sum: 0.6705322641339866
{'currentTarget': array([26., 24.]), 'previousTarget': array([26., 24.]), 'currentState': array([25.19393143, 23.53029308,  0.        ]), 'targetState': array([26, 24], dtype=int32), 'currentDistance': 0.9329368316809105}
episode index:582
target Thresh 17.695239964599132
target distance 5.0
model initialize at round 582
at step 0:
{'currentTarget': array([11.,  6.]), 'previousTarget': array([11.,  6.]), 'currentState': array([7.91972798, 9.96890199, 0.        ]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 5.023968424596362}
done in step count: 4
reward sum = 0.9273282521241877
running average episode reward sum: 0.6709727375267657
{'currentTarget': array([11.,  6.]), 'previousTarget': array([11.,  6.]), 'currentState': array([10.34826511,  6.33839154,  0.        ]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 0.7343481469638955}
episode index:583
target Thresh 17.70953757463805
target distance 10.0
model initialize at round 583
at step 0:
{'currentTarget': array([17., 26.]), 'previousTarget': array([17., 26.]), 'currentState': array([13.47022548, 17.37872577,  0.        ]), 'targetState': array([17, 26], dtype=int32), 'currentDistance': 9.31588307879872}
done in step count: 7
reward sum = 0.8622799904754258
running average episode reward sum: 0.671300318439349
{'currentTarget': array([17., 26.]), 'previousTarget': array([17., 26.]), 'currentState': array([16.3994245 , 25.53283656,  0.        ]), 'targetState': array([17, 26], dtype=int32), 'currentDistance': 0.7608762119625192}
episode index:584
target Thresh 17.723820894213343
target distance 2.0
model initialize at round 584
at step 0:
{'currentTarget': array([17., 23.]), 'previousTarget': array([17., 23.]), 'currentState': array([15.7177788 , 23.36357224,  0.        ]), 'targetState': array([17, 23], dtype=int32), 'currentDistance': 1.3327700380856862}
done in step count: 1
reward sum = 0.9830945438172969
running average episode reward sum: 0.6718333000211917
{'currentTarget': array([17., 23.]), 'previousTarget': array([17., 23.]), 'currentState': array([16.18345621, 22.86493   ,  0.        ]), 'targetState': array([17, 23], dtype=int32), 'currentDistance': 0.8276398136339145}
episode index:585
target Thresh 17.738089937608347
target distance 10.0
model initialize at round 585
at step 0:
{'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([12.83073258, 20.3840245 ,  0.        ]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 10.896678704886316}
done in step count: 9
reward sum = 0.8425742763284769
running average episode reward sum: 0.6721246668749584
{'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([ 4.62893847, 14.37504613,  0.        ]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 0.7322726247987993}
episode index:586
target Thresh 17.7523447190921
target distance 1.0
model initialize at round 586
at step 0:
{'currentTarget': array([21.,  9.]), 'previousTarget': array([21.,  9.]), 'currentState': array([20.34400716,  9.29762727,  0.        ]), 'targetState': array([21,  9], dtype=int32), 'currentDistance': 0.7203531026852182}
done in step count: 0
reward sum = 0.996941658380728
running average episode reward sum: 0.6726780177974554
{'currentTarget': array([21.,  9.]), 'previousTarget': array([21.,  9.]), 'currentState': array([20.34400716,  9.29762727,  0.        ]), 'targetState': array([21,  9], dtype=int32), 'currentDistance': 0.7203531026852182}
episode index:587
target Thresh 17.76658525291938
target distance 13.0
model initialize at round 587
at step 0:
{'currentTarget': array([6., 7.]), 'previousTarget': array([6., 7.]), 'currentState': array([17.80790198, 11.48042253,  0.        ]), 'targetState': array([6, 7], dtype=int32), 'currentDistance': 12.629360045170158}
done in step count: 11
reward sum = 0.8192878526444587
running average episode reward sum: 0.6729273542512768
{'currentTarget': array([6., 7.]), 'previousTarget': array([6., 7.]), 'currentState': array([6.834454  , 7.23632234, 0.        ]), 'targetState': array([6, 7], dtype=int32), 'currentDistance': 0.8672725800325636}
episode index:588
target Thresh 17.780811553330732
target distance 7.0
model initialize at round 588
at step 0:
{'currentTarget': array([21., 29.]), 'previousTarget': array([21., 29.]), 'currentState': array([15.31502914, 25.61364937,  0.        ]), 'targetState': array([21, 29], dtype=int32), 'currentDistance': 6.617119028037287}
done in step count: 5
reward sum = 0.903887698598843
running average episode reward sum: 0.6733194770769942
{'currentTarget': array([21., 29.]), 'previousTarget': array([21., 29.]), 'currentState': array([20.10854483, 28.61328024,  0.        ]), 'targetState': array([21, 29], dtype=int32), 'currentDistance': 0.9717224406467524}
episode index:589
target Thresh 17.79502363455245
target distance 10.0
model initialize at round 589
at step 0:
{'currentTarget': array([ 8., 29.]), 'previousTarget': array([ 8., 29.]), 'currentState': array([10.63772348, 20.2567904 ,  0.        ]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 9.132431179680905}
done in step count: 7
reward sum = 0.8763366353628336
running average episode reward sum: 0.6736635739554448
{'currentTarget': array([ 8., 29.]), 'previousTarget': array([ 8., 29.]), 'currentState': array([ 7.87923032, 28.01995867,  0.        ]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 0.9874544619309413}
episode index:590
target Thresh 17.80922151079662
target distance 14.0
model initialize at round 590
at step 0:
{'currentTarget': array([26., 15.]), 'previousTarget': array([26., 15.]), 'currentState': array([15.51859683, 27.2491554 ,  0.        ]), 'targetState': array([26, 15], dtype=int32), 'currentDistance': 16.121464589831564}
done in step count: 10
reward sum = 0.7665722112302309
running average episode reward sum: 0.6738207797714767
{'currentTarget': array([26., 15.]), 'previousTarget': array([26., 15.]), 'currentState': array([25.22262836, 15.22703797,  0.        ]), 'targetState': array([26, 15], dtype=int32), 'currentDistance': 0.8098474625065454}
episode index:591
target Thresh 17.823405196261117
target distance 8.0
model initialize at round 591
at step 0:
{'currentTarget': array([ 6., 14.]), 'previousTarget': array([ 6., 14.]), 'currentState': array([10.46218836, 21.03817284,  0.        ]), 'targetState': array([ 6, 14], dtype=int32), 'currentDistance': 8.333486780956013}
done in step count: 7
reward sum = 0.8825315418560238
running average episode reward sum: 0.6741733317344573
{'currentTarget': array([ 6., 14.]), 'previousTarget': array([ 6., 14.]), 'currentState': array([ 6.73275483, 14.40948755,  0.        ]), 'targetState': array([ 6, 14], dtype=int32), 'currentDistance': 0.8394103202472395}
episode index:592
target Thresh 17.837574705129626
target distance 7.0
model initialize at round 592
at step 0:
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([15.04040384,  5.27192575,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 8.32450077956621}
done in step count: 7
reward sum = 0.8892591212124344
running average episode reward sum: 0.6745360396425146
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.78413141, 10.39189705,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.9922959534972269}
episode index:593
target Thresh 17.851730051571664
target distance 12.0
model initialize at round 593
at step 0:
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([13.06744254,  2.46381387,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 11.077157018051492}
done in step count: 12
reward sum = 0.8341753102581237
running average episode reward sum: 0.6748047926233489
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.64802387, 2.3350401 , 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.7295113479384933}
episode index:594
target Thresh 17.865871249742575
target distance 6.0
model initialize at round 594
at step 0:
{'currentTarget': array([19., 12.]), 'previousTarget': array([19., 12.]), 'currentState': array([24.10709989, 11.9443397 ,  0.        ]), 'targetState': array([19, 12], dtype=int32), 'currentDistance': 5.107403191667449}
done in step count: 6
reward sum = 0.920090524683982
running average episode reward sum: 0.6752170375511819
{'currentTarget': array([19., 12.]), 'previousTarget': array([19., 12.]), 'currentState': array([19.65685984, 12.3849286 ,  0.        ]), 'targetState': array([19, 12], dtype=int32), 'currentDistance': 0.7613375593862517}
episode index:595
target Thresh 17.879998313783556
target distance 15.0
model initialize at round 595
at step 0:
{'currentTarget': array([ 6., 13.]), 'previousTarget': array([ 6., 13.]), 'currentState': array([20.06741655,  7.68470424,  0.        ]), 'targetState': array([ 6, 13], dtype=int32), 'currentDistance': 15.038104179973892}
done in step count: 15
reward sum = 0.7805815136452188
running average episode reward sum: 0.6753938235848967
{'currentTarget': array([ 6., 13.]), 'previousTarget': array([ 6., 13.]), 'currentState': array([ 6.98742115, 12.21389886,  0.        ]), 'targetState': array([ 6, 13], dtype=int32), 'currentDistance': 1.2621234249199813}
episode index:596
target Thresh 17.894111257821677
target distance 12.0
model initialize at round 596
at step 0:
{'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([25.04563707,  8.85165679,  0.        ]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 12.186530928377689}
done in step count: 11
reward sum = 0.8302669435617752
running average episode reward sum: 0.6756532425463321
{'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([14.96777028, 13.42280181,  0.        ]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 1.1268261015383059}
episode index:597
target Thresh 17.908210095969878
target distance 13.0
model initialize at round 597
at step 0:
{'currentTarget': array([ 3., 27.]), 'previousTarget': array([ 3., 27.]), 'currentState': array([14.77122402, 18.05381832,  0.        ]), 'targetState': array([ 3, 27], dtype=int32), 'currentDistance': 14.784988390603191}
done in step count: 16
reward sum = 0.7820408121515461
running average episode reward sum: 0.6758311481811233
{'currentTarget': array([ 3., 27.]), 'previousTarget': array([ 3., 27.]), 'currentState': array([ 3.92620206, 26.15054903,  0.        ]), 'targetState': array([ 3, 27], dtype=int32), 'currentDistance': 1.2567486626598015}
episode index:598
target Thresh 17.922294842327
target distance 4.0
model initialize at round 598
at step 0:
{'currentTarget': array([ 7., 17.]), 'previousTarget': array([ 7., 17.]), 'currentState': array([ 4.3094939, 19.1819551,  0.       ]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 3.464065693974802}
done in step count: 3
reward sum = 0.943212305534841
running average episode reward sum: 0.6762775274087589
{'currentTarget': array([ 7., 17.]), 'previousTarget': array([ 7., 17.]), 'currentState': array([ 6.48560774, 17.15494013,  0.        ]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 0.5372204735083339}
episode index:599
target Thresh 17.93636551097779
target distance 5.0
model initialize at round 599
at step 0:
{'currentTarget': array([13., 12.]), 'previousTarget': array([13., 12.]), 'currentState': array([15.60177654,  7.79552621,  0.        ]), 'targetState': array([13, 12], dtype=int32), 'currentDistance': 4.944374687039032}
done in step count: 4
reward sum = 0.9377504937088004
running average episode reward sum: 0.6767133156859256
{'currentTarget': array([13., 12.]), 'previousTarget': array([13., 12.]), 'currentState': array([13.545396  , 11.12729567,  0.        ]), 'targetState': array([13, 12], dtype=int32), 'currentDistance': 1.0291110919266704}
episode index:600
target Thresh 17.95042211599292
target distance 8.0
model initialize at round 600
at step 0:
{'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([14.66533482, 11.32707453,  0.        ]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 7.957214121446954}
done in step count: 5
reward sum = 0.8882625409481184
running average episode reward sum: 0.6770653110690574
{'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([18.30972111, 17.24845576,  0.        ]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 1.0204428858317072}
episode index:601
target Thresh 17.964464671429
target distance 1.0
model initialize at round 601
at step 0:
{'currentTarget': array([9., 5.]), 'previousTarget': array([9., 5.]), 'currentState': array([8.68416089, 4.74057698, 0.        ]), 'targetState': array([9, 5], dtype=int32), 'currentDistance': 0.40872319062603935}
done in step count: 0
reward sum = 0.9967589569867389
running average episode reward sum: 0.677596363637027
{'currentTarget': array([9., 5.]), 'previousTarget': array([9., 5.]), 'currentState': array([8.68416089, 4.74057698, 0.        ]), 'targetState': array([9, 5], dtype=int32), 'currentDistance': 0.40872319062603935}
episode index:602
target Thresh 17.978493191328575
target distance 15.0
model initialize at round 602
at step 0:
{'currentTarget': array([24., 17.]), 'previousTarget': array([24., 17.]), 'currentState': array([9.59341425, 7.02543283, 0.        ]), 'targetState': array([24, 17], dtype=int32), 'currentDistance': 17.52260549645307}
done in step count: 13
reward sum = 0.7684014983982087
running average episode reward sum: 0.6777469525835629
{'currentTarget': array([24., 17.]), 'previousTarget': array([24., 17.]), 'currentState': array([23.2448045 , 16.09318423,  0.        ]), 'targetState': array([24, 17], dtype=int32), 'currentDistance': 1.180099604570226}
episode index:603
target Thresh 17.992507689720178
target distance 17.0
model initialize at round 603
at step 0:
{'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([25.29180676,  6.55350375,  0.        ]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 18.025868055792735}
done in step count: 16
reward sum = 0.7493521652231356
running average episode reward sum: 0.6778655042601184
{'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([16.3649897 , 21.14854932,  0.        ]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 0.9263831512614287}
episode index:604
target Thresh 18.006508180618297
target distance 11.0
model initialize at round 604
at step 0:
{'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([10.21712866, 11.50464964,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 10.229783981365669}
done in step count: 7
reward sum = 0.8601641408551897
running average episode reward sum: 0.6781668243206062
{'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.07615188,  2.76357591,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.1985589371581553}
episode index:605
target Thresh 18.02049467802344
target distance 16.0
model initialize at round 605
at step 0:
{'currentTarget': array([15., 12.]), 'previousTarget': array([15., 12.]), 'currentState': array([11.16593218, 26.34603941,  0.        ]), 'targetState': array([15, 12], dtype=int32), 'currentDistance': 14.84954285288173}
done in step count: 11
reward sum = 0.780539527210749
running average episode reward sum: 0.6783357561735601
{'currentTarget': array([15., 12.]), 'previousTarget': array([15., 12.]), 'currentState': array([14.41903452, 12.18439847,  0.        ]), 'targetState': array([15, 12], dtype=int32), 'currentDistance': 0.6095274296658557}
episode index:606
target Thresh 18.03446719592209
target distance 9.0
model initialize at round 606
at step 0:
{'currentTarget': array([11., 22.]), 'previousTarget': array([11., 22.]), 'currentState': array([18.73700905, 25.54479313,  0.        ]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 8.510397601942952}
done in step count: 6
reward sum = 0.8852476159438365
running average episode reward sum: 0.6786766323840548
{'currentTarget': array([11., 22.]), 'previousTarget': array([11., 22.]), 'currentState': array([11.93569398, 22.40464681,  0.        ]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 1.0194421350937934}
episode index:607
target Thresh 18.04842574828677
target distance 1.0
model initialize at round 607
at step 0:
{'currentTarget': array([27.,  4.]), 'previousTarget': array([27.,  4.]), 'currentState': array([26.81881852,  3.65170354,  0.        ]), 'targetState': array([27,  4], dtype=int32), 'currentDistance': 0.3926030483210865}
done in step count: 0
reward sum = 0.9977122788885743
running average episode reward sum: 0.6792013620658057
{'currentTarget': array([27.,  4.]), 'previousTarget': array([27.,  4.]), 'currentState': array([26.81881852,  3.65170354,  0.        ]), 'targetState': array([27,  4], dtype=int32), 'currentDistance': 0.3926030483210865}
episode index:608
target Thresh 18.06237034907604
target distance 10.0
model initialize at round 608
at step 0:
{'currentTarget': array([11., 26.]), 'previousTarget': array([11., 26.]), 'currentState': array([10.2158083 , 17.16769719,  0.        ]), 'targetState': array([11, 26], dtype=int32), 'currentDistance': 8.867047396096092}
done in step count: 7
reward sum = 0.8802360498551147
running average episode reward sum: 0.6795314682854926
{'currentTarget': array([11., 26.]), 'previousTarget': array([11., 26.]), 'currentState': array([10.53316767, 25.15403444,  0.        ]), 'targetState': array([11, 26], dtype=int32), 'currentDistance': 0.9662246965022715}
episode index:609
target Thresh 18.076301012234495
target distance 16.0
model initialize at round 609
at step 0:
{'currentTarget': array([7., 8.]), 'previousTarget': array([7., 8.]), 'currentState': array([22.79775873, 18.91695056,  0.        ]), 'targetState': array([7, 8], dtype=int32), 'currentDistance': 19.202838081651695}
done in step count: 22
reward sum = 0.726539125101466
running average episode reward sum: 0.6796085300179778
{'currentTarget': array([7., 8.]), 'previousTarget': array([7., 8.]), 'currentState': array([7.97853802, 8.46371558, 0.        ]), 'targetState': array([7, 8], dtype=int32), 'currentDistance': 1.0828521604593004}
episode index:610
target Thresh 18.090217751692805
target distance 9.0
model initialize at round 610
at step 0:
{'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 4.02193809, 12.61794162,  0.        ]), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 7.680471312923766}
done in step count: 6
reward sum = 0.8864380496323034
running average episode reward sum: 0.6799470398700471
{'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.67897154, 5.44376618, 0.        ]), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 0.5477113192222879}
episode index:611
target Thresh 18.104120581367706
target distance 12.0
model initialize at round 611
at step 0:
{'currentTarget': array([ 4., 23.]), 'previousTarget': array([ 4., 23.]), 'currentState': array([11.91921558, 11.87811023,  0.        ]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 13.65321967507447}
done in step count: 12
reward sum = 0.8150139066846093
running average episode reward sum: 0.6801677373648422
{'currentTarget': array([ 4., 23.]), 'previousTarget': array([ 4., 23.]), 'currentState': array([ 4.37999912, 22.51258117,  0.        ]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 0.6180424338948326}
episode index:612
target Thresh 18.11800951516203
target distance 7.0
model initialize at round 612
at step 0:
{'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([5.60240555, 9.2663773 , 0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 5.6691686809531}
done in step count: 4
reward sum = 0.916008053221625
running average episode reward sum: 0.68055246871208
{'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.20692009, 10.22674532,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.107654523929553}
episode index:613
target Thresh 18.13188456696471
target distance 9.0
model initialize at round 613
at step 0:
{'currentTarget': array([17., 28.]), 'previousTarget': array([17., 28.]), 'currentState': array([21.92841007, 19.75105894,  0.        ]), 'targetState': array([17, 28], dtype=int32), 'currentDistance': 9.609071467902826}
done in step count: 9
reward sum = 0.8625476331440755
running average episode reward sum: 0.6808488777746727
{'currentTarget': array([17., 28.]), 'previousTarget': array([17., 28.]), 'currentState': array([17.05030456, 27.71271622,  0.        ]), 'targetState': array([17, 28], dtype=int32), 'currentDistance': 0.2916547927851404}
episode index:614
target Thresh 18.145745750650804
target distance 9.0
model initialize at round 614
at step 0:
{'currentTarget': array([17., 11.]), 'previousTarget': array([17., 11.]), 'currentState': array([22.57624125, 19.41668689,  0.        ]), 'targetState': array([17, 11], dtype=int32), 'currentDistance': 10.096290644506317}
done in step count: 8
reward sum = 0.865411022579345
running average episode reward sum: 0.6811489788231356
{'currentTarget': array([17., 11.]), 'previousTarget': array([17., 11.]), 'currentState': array([17.64693105, 11.45071542,  0.        ]), 'targetState': array([17, 11], dtype=int32), 'currentDistance': 0.7884568336364602}
episode index:615
target Thresh 18.159593080081493
target distance 3.0
model initialize at round 615
at step 0:
{'currentTarget': array([18., 10.]), 'previousTarget': array([18., 10.]), 'currentState': array([19.8933351 , 10.16472447,  0.        ]), 'targetState': array([18, 10], dtype=int32), 'currentDistance': 1.9004872971802913}
done in step count: 1
reward sum = 0.9790444595026184
running average episode reward sum: 0.6816325753826802
{'currentTarget': array([18., 10.]), 'previousTarget': array([18., 10.]), 'currentState': array([18.92622811, 10.28073504,  0.        ]), 'targetState': array([18, 10], dtype=int32), 'currentDistance': 0.9678381404292414}
episode index:616
target Thresh 18.17342656910411
target distance 11.0
model initialize at round 616
at step 0:
{'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([15.20644219, 27.50853002,  0.        ]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 10.237344577377495}
done in step count: 8
reward sum = 0.8514957612511709
running average episode reward sum: 0.6819078803840877
{'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([18.54737534, 18.46188939,  0.        ]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 0.6466922693697041}
episode index:617
target Thresh 18.187246231552145
target distance 9.0
model initialize at round 617
at step 0:
{'currentTarget': array([ 9., 27.]), 'previousTarget': array([ 9., 27.]), 'currentState': array([11.66029319, 19.17130995,  0.        ]), 'targetState': array([ 9, 27], dtype=int32), 'currentDistance': 8.268346134290876}
done in step count: 8
reward sum = 0.8782694621481455
running average episode reward sum: 0.6822256175714081
{'currentTarget': array([ 9., 27.]), 'previousTarget': array([ 9., 27.]), 'currentState': array([ 8.99464945, 26.47671843,  0.        ]), 'targetState': array([ 9, 27], dtype=int32), 'currentDistance': 0.5233089281497907}
episode index:618
target Thresh 18.20105208124526
target distance 5.0
model initialize at round 618
at step 0:
{'currentTarget': array([24., 17.]), 'previousTarget': array([24., 17.]), 'currentState': array([23.59742802, 20.57356381,  0.        ]), 'targetState': array([24, 17], dtype=int32), 'currentDistance': 3.596167756084949}
done in step count: 3
reward sum = 0.9416076456273561
running average episode reward sum: 0.6826446515424193
{'currentTarget': array([24., 17.]), 'previousTarget': array([24., 17.]), 'currentState': array([23.55862805, 17.44779664,  0.        ]), 'targetState': array([24, 17], dtype=int32), 'currentDistance': 0.6287535547714896}
episode index:619
target Thresh 18.214844131989302
target distance 7.0
model initialize at round 619
at step 0:
{'currentTarget': array([24.,  4.]), 'previousTarget': array([24.,  4.]), 'currentState': array([24.65641674,  9.74719071,  0.        ]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 5.784555647889616}
done in step count: 4
reward sum = 0.9185632294158106
running average episode reward sum: 0.683025165377699
{'currentTarget': array([24.,  4.]), 'previousTarget': array([24.,  4.]), 'currentState': array([23.39087614,  4.80544347,  0.        ]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 1.0098371439354124}
episode index:620
target Thresh 18.22862239757633
target distance 14.0
model initialize at round 620
at step 0:
{'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([19.75275521,  7.15989995,  0.        ]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 13.13186317848326}
done in step count: 12
reward sum = 0.81810832389725
running average episode reward sum: 0.6832426905927064
{'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([17.17315617, 19.23948413,  0.        ]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 0.7799791320309721}
episode index:621
target Thresh 18.242386891784612
target distance 5.0
model initialize at round 621
at step 0:
{'currentTarget': array([ 7., 27.]), 'previousTarget': array([ 7., 27.]), 'currentState': array([10.35580641, 22.8021813 ,  0.        ]), 'targetState': array([ 7, 27], dtype=int32), 'currentDistance': 5.374301672977683}
done in step count: 4
reward sum = 0.9296713532463943
running average episode reward sum: 0.6836388781532429
{'currentTarget': array([ 7., 27.]), 'previousTarget': array([ 7., 27.]), 'currentState': array([ 7.5157463 , 26.40048581,  0.        ]), 'targetState': array([ 7, 27], dtype=int32), 'currentDistance': 0.7908296282675265}
episode index:622
target Thresh 18.256137628378635
target distance 10.0
model initialize at round 622
at step 0:
{'currentTarget': array([18., 25.]), 'previousTarget': array([18., 25.]), 'currentState': array([ 9.51885617, 24.27209693,  0.        ]), 'targetState': array([18, 25], dtype=int32), 'currentDistance': 8.512323042453145}
done in step count: 6
reward sum = 0.8737509570051604
running average episode reward sum: 0.6839440339780453
{'currentTarget': array([18., 25.]), 'previousTarget': array([18., 25.]), 'currentState': array([17.2533927 , 24.58385289,  0.        ]), 'targetState': array([18, 25], dtype=int32), 'currentDistance': 0.8547519410317929}
episode index:623
target Thresh 18.26987462110914
target distance 11.0
model initialize at round 623
at step 0:
{'currentTarget': array([ 8., 12.]), 'previousTarget': array([ 8., 12.]), 'currentState': array([17.8010745 ,  3.33370993,  0.        ]), 'targetState': array([ 8, 12], dtype=int32), 'currentDistance': 13.083028896495861}
done in step count: 11
reward sum = 0.819594591092679
running average episode reward sum: 0.68416142269137
{'currentTarget': array([ 8., 12.]), 'previousTarget': array([ 8., 12.]), 'currentState': array([ 8.4032765 , 11.42788604,  0.        ]), 'targetState': array([ 8, 12], dtype=int32), 'currentDistance': 0.699961657729212}
episode index:624
target Thresh 18.283597883713124
target distance 8.0
model initialize at round 624
at step 0:
{'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([20.8812263 ,  2.96512528,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 6.958609134841874}
done in step count: 6
reward sum = 0.9023919207757312
running average episode reward sum: 0.6845105914883051
{'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.66474611,  4.09111612,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.6709616470702219}
episode index:625
target Thresh 18.29730742991385
target distance 8.0
model initialize at round 625
at step 0:
{'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([21.32233483, 20.35398901,  0.        ]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 10.37775224173852}
done in step count: 9
reward sum = 0.8529679166793287
running average episode reward sum: 0.6847796926467572
{'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([14.49187988, 13.20974217,  0.        ]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 0.5347313307973989}
episode index:626
target Thresh 18.311003273420862
target distance 13.0
model initialize at round 626
at step 0:
{'currentTarget': array([15., 27.]), 'previousTarget': array([15., 27.]), 'currentState': array([ 6.97278695, 14.88220328,  0.        ]), 'targetState': array([15, 27], dtype=int32), 'currentDistance': 14.535375696755649}
done in step count: 12
reward sum = 0.8024110282615587
running average episode reward sum: 0.6849673024324268
{'currentTarget': array([15., 27.]), 'previousTarget': array([15., 27.]), 'currentState': array([14.06138839, 26.25024217,  0.        ]), 'targetState': array([15, 27], dtype=int32), 'currentDistance': 1.2013028576518436}
episode index:627
target Thresh 18.324685427930007
target distance 10.0
model initialize at round 627
at step 0:
{'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([14.71333742, 21.82079981,  0.        ]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 9.51423986601147}
done in step count: 7
reward sum = 0.8641789982043652
running average episode reward sum: 0.6852526713747387
{'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([ 6.45868254, 18.14512596,  0.        ]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 0.48109376709240614}
episode index:628
target Thresh 18.33835390712344
target distance 8.0
model initialize at round 628
at step 0:
{'currentTarget': array([ 7., 12.]), 'previousTarget': array([ 7., 12.]), 'currentState': array([13.75703049,  9.26755637,  0.        ]), 'targetState': array([ 7, 12], dtype=int32), 'currentDistance': 7.288601319731555}
done in step count: 5
reward sum = 0.9029660233750314
running average episode reward sum: 0.685598797530542
{'currentTarget': array([ 7., 12.]), 'previousTarget': array([ 7., 12.]), 'currentState': array([ 7.88973153, 11.76654896,  0.        ]), 'targetState': array([ 7, 12], dtype=int32), 'currentDistance': 0.9198486704062379}
episode index:629
target Thresh 18.35200872466964
target distance 2.0
model initialize at round 629
at step 0:
{'currentTarget': array([21.,  9.]), 'previousTarget': array([21.,  9.]), 'currentState': array([19.81259072,  8.26285169,  0.        ]), 'targetState': array([21,  9], dtype=int32), 'currentDistance': 1.3976152690301764}
done in step count: 1
reward sum = 0.9847823645495677
running average episode reward sum: 0.6860736920813658
{'currentTarget': array([21.,  9.]), 'previousTarget': array([21.,  9.]), 'currentState': array([20.25936499,  8.60592344,  0.        ]), 'targetState': array([21,  9], dtype=int32), 'currentDistance': 0.8389496686185652}
episode index:630
target Thresh 18.36564989422343
target distance 4.0
model initialize at round 630
at step 0:
{'currentTarget': array([ 6., 13.]), 'previousTarget': array([ 6., 13.]), 'currentState': array([ 4.24055915, 15.86699343,  0.        ]), 'targetState': array([ 6, 13], dtype=int32), 'currentDistance': 3.363819767979159}
done in step count: 3
reward sum = 0.9509023600836993
running average episode reward sum: 0.6864933888610842
{'currentTarget': array([ 6., 13.]), 'previousTarget': array([ 6., 13.]), 'currentState': array([ 5.55071874, 13.45136631,  0.        ]), 'targetState': array([ 6, 13], dtype=int32), 'currentDistance': 0.6368557077763478}
episode index:631
target Thresh 18.37927742942598
target distance 2.0
model initialize at round 631
at step 0:
{'currentTarget': array([ 4., 16.]), 'previousTarget': array([ 4., 16.]), 'currentState': array([ 5.22363663, 16.9617002 ,  0.        ]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 1.5563270453502376}
done in step count: 1
reward sum = 0.9781164440814463
running average episode reward sum: 0.6869548177459266
{'currentTarget': array([ 4., 16.]), 'previousTarget': array([ 4., 16.]), 'currentState': array([ 4.5740701 , 16.43160868,  0.        ]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 0.7182217799008697}
episode index:632
target Thresh 18.39289134390482
target distance 12.0
model initialize at round 632
at step 0:
{'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([ 6.46590662, 18.01303255,  0.        ]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 10.719849018778747}
done in step count: 8
reward sum = 0.845485824251893
running average episode reward sum: 0.687205261674056
{'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([16.16390011, 19.54720676,  0.        ]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 0.9508337117544164}
episode index:633
target Thresh 18.406491651273875
target distance 16.0
model initialize at round 633
at step 0:
{'currentTarget': array([22., 22.]), 'previousTarget': array([22., 22.]), 'currentState': array([ 6.16110291, 10.4823378 ,  0.        ]), 'targetState': array([22, 22], dtype=int32), 'currentDistance': 19.583850582833662}
done in step count: 19
reward sum = 0.718930886746176
running average episode reward sum: 0.6872553020921509
{'currentTarget': array([22., 22.]), 'previousTarget': array([22., 22.]), 'currentState': array([21.2861999, 21.8655493,  0.       ]), 'targetState': array([22, 22], dtype=int32), 'currentDistance': 0.7263522410356889}
episode index:634
target Thresh 18.420078365133442
target distance 12.0
model initialize at round 634
at step 0:
{'currentTarget': array([19., 17.]), 'previousTarget': array([19., 17.]), 'currentState': array([7.60467672, 8.18752646, 0.        ]), 'targetState': array([19, 17], dtype=int32), 'currentDistance': 14.405314380149647}
done in step count: 12
reward sum = 0.799753613600127
running average episode reward sum: 0.687432464787439
{'currentTarget': array([19., 17.]), 'previousTarget': array([19., 17.]), 'currentState': array([18.112261  , 16.50827283,  0.        ]), 'targetState': array([19, 17], dtype=int32), 'currentDistance': 1.014828138116575}
episode index:635
target Thresh 18.433651499070244
target distance 16.0
model initialize at round 635
at step 0:
{'currentTarget': array([25.,  2.]), 'previousTarget': array([25.,  2.]), 'currentState': array([24.41626945, 16.61789942,  0.        ]), 'targetState': array([25,  2], dtype=int32), 'currentDistance': 14.629549710929858}
done in step count: 12
reward sum = 0.7911345461434571
running average episode reward sum: 0.6875955183744767
{'currentTarget': array([25.,  2.]), 'previousTarget': array([25.,  2.]), 'currentState': array([24.6287764 ,  2.38663381,  0.        ]), 'targetState': array([25,  2], dtype=int32), 'currentDistance': 0.535996891108824}
episode index:636
target Thresh 18.447211066657417
target distance 5.0
model initialize at round 636
at step 0:
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([19.73336029, 10.05498427,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 4.261565371446026}
done in step count: 3
reward sum = 0.9360959752536515
running average episode reward sum: 0.6879856289818224
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.74018884,  8.15689859,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.7566351045215223}
episode index:637
target Thresh 18.460757081454528
target distance 12.0
model initialize at round 637
at step 0:
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([3.87891036, 2.21290788, 0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 13.576355858524375}
done in step count: 11
reward sum = 0.8164306005191059
running average episode reward sum: 0.6881869533886206
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.26977944,  9.58786556,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.8384967837682533}
episode index:638
target Thresh 18.47428955700759
target distance 4.0
model initialize at round 638
at step 0:
{'currentTarget': array([22.,  4.]), 'previousTarget': array([22.,  4.]), 'currentState': array([19.39758086,  6.31144452,  0.        ]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 3.480712734918616}
done in step count: 2
reward sum = 0.954991185665477
running average episode reward sum: 0.6886044873984435
{'currentTarget': array([22.,  4.]), 'previousTarget': array([22.,  4.]), 'currentState': array([21.14713454,  4.85748667,  0.        ]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 1.2094059987182075}
episode index:639
target Thresh 18.487808506849085
target distance 15.0
model initialize at round 639
at step 0:
{'currentTarget': array([20., 12.]), 'previousTarget': array([20., 12.]), 'currentState': array([17.28420204, 25.51633859,  0.        ]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 13.78647769942966}
done in step count: 10
reward sum = 0.8090024045615329
running average episode reward sum: 0.6887926091440109
{'currentTarget': array([20., 12.]), 'previousTarget': array([20., 12.]), 'currentState': array([19.38958592, 12.78770912,  0.        ]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 0.9965395159781861}
episode index:640
target Thresh 18.50131394449796
target distance 9.0
model initialize at round 640
at step 0:
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([11.69077063, 10.8925687 ,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 8.216745493630858}
done in step count: 6
reward sum = 0.8835860613900935
running average episode reward sum: 0.6890964990851124
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.59952021, 8.06185771, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.6027029566526382}
episode index:641
target Thresh 18.51480588345965
target distance 15.0
model initialize at round 641
at step 0:
{'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([23.27193537, 22.52746701,  0.        ]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 13.799807983368334}
done in step count: 10
reward sum = 0.8098011275644147
running average episode reward sum: 0.6892845125251114
{'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([25.44382927,  9.77151024,  0.        ]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 0.9510804048633354}
episode index:642
target Thresh 18.528284337226104
target distance 11.0
model initialize at round 642
at step 0:
{'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([ 4.76891761, 11.90957165,  0.        ]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 10.119682733057461}
done in step count: 9
reward sum = 0.8623255146990516
running average episode reward sum: 0.6895536276140287
{'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([ 3.69350296, 21.23502982,  0.        ]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 0.8240872633861169}
episode index:643
target Thresh 18.541749319275766
target distance 8.0
model initialize at round 643
at step 0:
{'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([10.5320375 , 20.60036922,  0.        ]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 7.455979995928763}
done in step count: 6
reward sum = 0.8914398753100865
running average episode reward sum: 0.6898671155762897
{'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([13.58426526, 14.52155042,  0.        ]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 0.6669709213223267}
episode index:644
target Thresh 18.555200843073635
target distance 15.0
model initialize at round 644
at step 0:
{'currentTarget': array([ 8., 13.]), 'previousTarget': array([ 8., 13.]), 'currentState': array([11.42802438, 27.34011108,  0.        ]), 'targetState': array([ 8, 13], dtype=int32), 'currentDistance': 14.744156023840304}
done in step count: 19
reward sum = 0.7603688214365092
running average episode reward sum: 0.6899764205466156
{'currentTarget': array([ 8., 13.]), 'previousTarget': array([ 8., 13.]), 'currentState': array([ 8.33125602, 13.53889206,  0.        ]), 'targetState': array([ 8, 13], dtype=int32), 'currentDistance': 0.6325624130273001}
episode index:645
target Thresh 18.568638922071223
target distance 11.0
model initialize at round 645
at step 0:
{'currentTarget': array([23., 28.]), 'previousTarget': array([23., 28.]), 'currentState': array([13.4181813 , 25.09859532,  0.        ]), 'targetState': array([23, 28], dtype=int32), 'currentDistance': 10.011463364904744}
done in step count: 7
reward sum = 0.8616006802430118
running average episode reward sum: 0.6902420927752477
{'currentTarget': array([23., 28.]), 'previousTarget': array([23., 28.]), 'currentState': array([22.10604185, 27.49773733,  0.        ]), 'targetState': array([23, 28], dtype=int32), 'currentDistance': 1.025392101993673}
episode index:646
target Thresh 18.582063569706612
target distance 5.0
model initialize at round 646
at step 0:
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([12.60179567, 12.41891479,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 3.682541492064021}
done in step count: 3
reward sum = 0.9388566765307543
running average episode reward sum: 0.6906263502462764
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.5079338 , 11.24897474,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.5514685585488998}
episode index:647
target Thresh 18.595474799404453
target distance 12.0
model initialize at round 647
at step 0:
{'currentTarget': array([19.,  5.]), 'previousTarget': array([19.,  5.]), 'currentState': array([27.0217781, 16.2388677,  0.       ]), 'targetState': array([19,  5], dtype=int32), 'currentDistance': 13.808007499764441}
done in step count: 13
reward sum = 0.8034319567769389
running average episode reward sum: 0.690800432972404
{'currentTarget': array([19.,  5.]), 'previousTarget': array([19.,  5.]), 'currentState': array([19.43160422,  5.22857979,  0.        ]), 'targetState': array([19,  5], dtype=int32), 'currentDistance': 0.4883962744448338}
episode index:648
target Thresh 18.60887262457598
target distance 14.0
model initialize at round 648
at step 0:
{'currentTarget': array([18., 12.]), 'previousTarget': array([18., 12.]), 'currentState': array([ 5.43242335, 20.50359151,  0.        ]), 'targetState': array([18, 12], dtype=int32), 'currentDistance': 15.174157350663156}
done in step count: 10
reward sum = 0.7858543311421883
running average episode reward sum: 0.6909468950651155
{'currentTarget': array([18., 12.]), 'previousTarget': array([18., 12.]), 'currentState': array([17.45163238, 12.60083029,  0.        ]), 'targetState': array([18, 12], dtype=int32), 'currentDistance': 0.8134519528514027}
episode index:649
target Thresh 18.622257058619013
target distance 12.0
model initialize at round 649
at step 0:
{'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([11.34135723, 14.84908141,  0.        ]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 10.81784487512256}
done in step count: 9
reward sum = 0.8382150368567071
running average episode reward sum: 0.6911734614371026
{'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([21.30707613, 12.82844684,  0.        ]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 0.7138445089916655}
episode index:650
target Thresh 18.63562811491799
target distance 3.0
model initialize at round 650
at step 0:
{'currentTarget': array([26., 26.]), 'previousTarget': array([26., 26.]), 'currentState': array([25.66323701, 27.68196881,  0.        ]), 'targetState': array([26, 26], dtype=int32), 'currentDistance': 1.7153508055652846}
done in step count: 1
reward sum = 0.9760176109739559
running average episode reward sum: 0.6916110100539026
{'currentTarget': array([26., 26.]), 'previousTarget': array([26., 26.]), 'currentState': array([25.61082245, 26.70592099,  0.        ]), 'targetState': array([26, 26], dtype=int32), 'currentDistance': 0.8060915657692016}
episode index:651
target Thresh 18.648985806843967
target distance 18.0
model initialize at round 651
at step 0:
{'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([7.75187923, 5.21322012, 0.        ]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 17.200946965638924}
done in step count: 15
reward sum = 0.7619925209965022
running average episode reward sum: 0.6917189571565753
{'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([ 3.69717893, 21.58558822,  0.        ]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 0.5132618498188523}
episode index:652
target Thresh 18.662330147754638
target distance 18.0
model initialize at round 652
at step 0:
{'currentTarget': array([9., 2.]), 'previousTarget': array([9., 2.]), 'currentState': array([26.14762348,  8.63418072,  0.        ]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 18.386227043008162}
done in step count: 16
reward sum = 0.7418904631325208
running average episode reward sum: 0.691795789478131
{'currentTarget': array([9., 2.]), 'previousTarget': array([9., 2.]), 'currentState': array([9.81670338, 2.35564264, 0.        ]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 0.8907783639147068}
episode index:653
target Thresh 18.67566115099435
target distance 11.0
model initialize at round 653
at step 0:
{'currentTarget': array([15., 13.]), 'previousTarget': array([15., 13.]), 'currentState': array([16.84593484,  3.18405724,  0.        ]), 'targetState': array([15, 13], dtype=int32), 'currentDistance': 9.988003192731579}
done in step count: 8
reward sum = 0.8637474469208346
running average episode reward sum: 0.6920587125017438
{'currentTarget': array([15., 13.]), 'previousTarget': array([15., 13.]), 'currentState': array([15.02372415, 12.23291504,  0.        ]), 'targetState': array([15, 13], dtype=int32), 'currentDistance': 0.7674517346996445}
episode index:654
target Thresh 18.688978829894097
target distance 5.0
model initialize at round 654
at step 0:
{'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([10.70147708,  7.62105   ,  0.        ]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 3.633334424092949}
done in step count: 3
reward sum = 0.9437443586409733
running average episode reward sum: 0.6924429653966129
{'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([10.62344564,  4.55258656,  0.        ]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 0.6686890826071233}
episode index:655
target Thresh 18.702283197771564
target distance 7.0
model initialize at round 655
at step 0:
{'currentTarget': array([ 9., 19.]), 'previousTarget': array([ 9., 19.]), 'currentState': array([ 5.64681977, 24.72205102,  0.        ]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 6.632170502849718}
done in step count: 5
reward sum = 0.9089059658119987
running average episode reward sum: 0.6927729394826119
{'currentTarget': array([ 9., 19.]), 'previousTarget': array([ 9., 19.]), 'currentState': array([ 8.44991109, 19.77905571,  0.        ]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 0.9536905251921959}
episode index:656
target Thresh 18.715574267931125
target distance 16.0
model initialize at round 656
at step 0:
{'currentTarget': array([20., 21.]), 'previousTarget': array([20., 21.]), 'currentState': array([ 4.52438956, 12.62935096,  0.        ]), 'targetState': array([20, 21], dtype=int32), 'currentDistance': 17.59438216257531}
done in step count: 15
reward sum = 0.7688701813395207
running average episode reward sum: 0.6928887648126834
{'currentTarget': array([20., 21.]), 'previousTarget': array([20., 21.]), 'currentState': array([19.12048495, 20.46926802,  0.        ]), 'targetState': array([20, 21], dtype=int32), 'currentDistance': 1.0272405544947298}
episode index:657
target Thresh 18.72885205366385
target distance 13.0
model initialize at round 657
at step 0:
{'currentTarget': array([ 5., 12.]), 'previousTarget': array([ 5., 12.]), 'currentState': array([13.03671969, 24.13881218,  0.        ]), 'targetState': array([ 5, 12], dtype=int32), 'currentDistance': 14.558146334552577}
done in step count: 15
reward sum = 0.7951054614901091
running average episode reward sum: 0.6930441093365093
{'currentTarget': array([ 5., 12.]), 'previousTarget': array([ 5., 12.]), 'currentState': array([ 5.84881311, 12.66396934,  0.        ]), 'targetState': array([ 5, 12], dtype=int32), 'currentDistance': 1.0776543831767877}
episode index:658
target Thresh 18.742116568247518
target distance 10.0
model initialize at round 658
at step 0:
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([11.84991479, 12.20731857,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 8.931887253860962}
done in step count: 8
reward sum = 0.8707380254371769
running average episode reward sum: 0.6933137510908349
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.52996486, 10.85384668,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.5497486240431935}
episode index:659
target Thresh 18.755367824946646
target distance 15.0
model initialize at round 659
at step 0:
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([15.96466184, 16.49043667,  0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 17.46599673093241}
done in step count: 11
reward sum = 0.7548759605182012
running average episode reward sum: 0.6934070271657248
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.9361285 , 6.17596489, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.9525230739000257}
episode index:660
target Thresh 18.7686058370125
target distance 6.0
model initialize at round 660
at step 0:
{'currentTarget': array([21.,  9.]), 'previousTarget': array([21.,  9.]), 'currentState': array([16.33494782, 14.16366935,  0.        ]), 'targetState': array([21,  9], dtype=int32), 'currentDistance': 6.958893082478601}
done in step count: 6
reward sum = 0.8926522033449424
running average episode reward sum: 0.6937084570843015
{'currentTarget': array([21.,  9.]), 'previousTarget': array([21.,  9.]), 'currentState': array([20.71220315,  9.39914519,  0.        ]), 'targetState': array([21,  9], dtype=int32), 'currentDistance': 0.4920812018324587}
episode index:661
target Thresh 18.781830617683084
target distance 15.0
model initialize at round 661
at step 0:
{'currentTarget': array([24., 13.]), 'previousTarget': array([24., 13.]), 'currentState': array([10.112041  , 24.62367225,  0.        ]), 'targetState': array([24, 13], dtype=int32), 'currentDistance': 18.110360619976284}
done in step count: 12
reward sum = 0.738678546054762
running average episode reward sum: 0.6937763877322931
{'currentTarget': array([24., 13.]), 'previousTarget': array([24., 13.]), 'currentState': array([23.70622385, 13.43687969,  0.        ]), 'targetState': array([24, 13], dtype=int32), 'currentDistance': 0.5264677543750931}
episode index:662
target Thresh 18.795042180183188
target distance 8.0
model initialize at round 662
at step 0:
{'currentTarget': array([17., 16.]), 'previousTarget': array([17., 16.]), 'currentState': array([23.80145347, 12.53725743,  0.        ]), 'targetState': array([17, 16], dtype=int32), 'currentDistance': 7.632192044650258}
done in step count: 6
reward sum = 0.890258974392364
running average episode reward sum: 0.6940727415583264
{'currentTarget': array([17., 16.]), 'previousTarget': array([17., 16.]), 'currentState': array([17.50640088, 15.84446347,  0.        ]), 'targetState': array([17, 16], dtype=int32), 'currentDistance': 0.5297484944907446}
episode index:663
target Thresh 18.808240537724373
target distance 15.0
model initialize at round 663
at step 0:
{'currentTarget': array([ 4., 28.]), 'previousTarget': array([ 4., 28.]), 'currentState': array([14.11884395, 14.02922177,  0.        ]), 'targetState': array([ 4, 28], dtype=int32), 'currentDistance': 17.250323102917445}
done in step count: 17
reward sum = 0.7555652596304854
running average episode reward sum: 0.6941653507722905
{'currentTarget': array([ 4., 28.]), 'previousTarget': array([ 4., 28.]), 'currentState': array([ 4.44659154, 27.62032098,  0.        ]), 'targetState': array([ 4, 28], dtype=int32), 'currentDistance': 0.5861741800907705}
episode index:664
target Thresh 18.821425703504993
target distance 17.0
model initialize at round 664
at step 0:
{'currentTarget': array([ 4., 20.]), 'previousTarget': array([ 4., 20.]), 'currentState': array([2.17985502, 4.22228789, 0.        ]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 15.882352693302293}
done in step count: 13
reward sum = 0.7879984644394766
running average episode reward sum: 0.6943064531988576
{'currentTarget': array([ 4., 20.]), 'previousTarget': array([ 4., 20.]), 'currentState': array([ 3.49782151, 19.19857347,  0.        ]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 0.9457630352175971}
episode index:665
target Thresh 18.834597690710222
target distance 13.0
model initialize at round 665
at step 0:
{'currentTarget': array([11.,  3.]), 'previousTarget': array([11.,  3.]), 'currentState': array([21.41362196, 14.94345248,  0.        ]), 'targetState': array([11,  3], dtype=int32), 'currentDistance': 15.845806367936644}
done in step count: 14
reward sum = 0.7827551356282432
running average episode reward sum: 0.6944392590283311
{'currentTarget': array([11.,  3.]), 'previousTarget': array([11.,  3.]), 'currentState': array([11.60308021,  3.33877909,  0.        ]), 'targetState': array([11,  3], dtype=int32), 'currentDistance': 0.6917203311031276}
episode index:666
target Thresh 18.847756512512042
target distance 14.0
model initialize at round 666
at step 0:
{'currentTarget': array([ 2., 29.]), 'previousTarget': array([ 2., 29.]), 'currentState': array([ 3.75074121, 16.20934677,  0.        ]), 'targetState': array([ 2, 29], dtype=int32), 'currentDistance': 12.909914980945414}
done in step count: 11
reward sum = 0.8182291665640679
running average episode reward sum: 0.6946248510936022
{'currentTarget': array([ 2., 29.]), 'previousTarget': array([ 2., 29.]), 'currentState': array([ 1.53009173, 28.62851912,  0.        ]), 'targetState': array([ 2, 29], dtype=int32), 'currentDistance': 0.5990090362045427}
episode index:667
target Thresh 18.860902182069278
target distance 5.0
model initialize at round 667
at step 0:
{'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([25.8761009 , 11.52838957,  0.        ]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 3.9119501160929158}
done in step count: 3
reward sum = 0.947995254100765
running average episode reward sum: 0.695004148104092
{'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([22.9467507 , 10.95731764,  0.        ]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 0.9477123363441513}
episode index:668
target Thresh 18.874034712527603
target distance 4.0
model initialize at round 668
at step 0:
{'currentTarget': array([ 5., 24.]), 'previousTarget': array([ 5., 24.]), 'currentState': array([ 4.81422083, 21.13196754,  0.        ]), 'targetState': array([ 5, 24], dtype=int32), 'currentDistance': 2.8740431564595146}
done in step count: 2
reward sum = 0.9625111461317984
running average episode reward sum: 0.695404009087691
{'currentTarget': array([ 5., 24.]), 'previousTarget': array([ 5., 24.]), 'currentState': array([ 4.55234319, 23.22095048,  0.        ]), 'targetState': array([ 5, 24], dtype=int32), 'currentDistance': 0.8985069658848079}
episode index:669
target Thresh 18.887154117019545
target distance 5.0
model initialize at round 669
at step 0:
{'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([15.71793109, 17.32091439,  0.        ]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 3.8960713068450694}
done in step count: 3
reward sum = 0.9427343531278389
running average episode reward sum: 0.6957731588549151
{'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([16.51302723, 20.32416761,  0.        ]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 0.8330017411145773}
episode index:670
target Thresh 18.900260408664515
target distance 10.0
model initialize at round 670
at step 0:
{'currentTarget': array([19., 24.]), 'previousTarget': array([19., 24.]), 'currentState': array([10.43217659, 23.33941612,  0.        ]), 'targetState': array([19, 24], dtype=int32), 'currentDistance': 8.593251366782109}
done in step count: 7
reward sum = 0.87203269260177
running average episode reward sum: 0.6960358407233902
{'currentTarget': array([19., 24.]), 'previousTarget': array([19., 24.]), 'currentState': array([18.18107411, 23.82544413,  0.        ]), 'targetState': array([19, 24], dtype=int32), 'currentDistance': 0.8373227334712454}
episode index:671
target Thresh 18.913353600568797
target distance 7.0
model initialize at round 671
at step 0:
{'currentTarget': array([13., 17.]), 'previousTarget': array([13., 17.]), 'currentState': array([18.26600003, 23.26353991,  0.        ]), 'targetState': array([13, 17], dtype=int32), 'currentDistance': 8.183073294895149}
done in step count: 6
reward sum = 0.89687469907428
running average episode reward sum: 0.6963347080721266
{'currentTarget': array([13., 17.]), 'previousTarget': array([13., 17.]), 'currentState': array([13.92910641, 17.86866808,  0.        ]), 'targetState': array([13, 17], dtype=int32), 'currentDistance': 1.2719366970314583}
episode index:672
target Thresh 18.926433705825588
target distance 12.0
model initialize at round 672
at step 0:
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([17.24685651, 16.34277165,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 11.411096347313064}
done in step count: 11
reward sum = 0.8381536658258596
running average episode reward sum: 0.696545434606679
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.85188538,  5.45048666,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.47421110429082386}
episode index:673
target Thresh 18.939500737515
target distance 4.0
model initialize at round 673
at step 0:
{'currentTarget': array([22., 27.]), 'previousTarget': array([22., 27.]), 'currentState': array([24.74240804, 28.23084337,  0.        ]), 'targetState': array([22, 27], dtype=int32), 'currentDistance': 3.0059569577855147}
done in step count: 2
reward sum = 0.9570413278972729
running average episode reward sum: 0.6969319270299588
{'currentTarget': array([22., 27.]), 'previousTarget': array([22., 27.]), 'currentState': array([22.83131373, 27.14313278,  0.        ]), 'targetState': array([22, 27], dtype=int32), 'currentDistance': 0.8435457948871558}
episode index:674
target Thresh 18.952554708704064
target distance 12.0
model initialize at round 674
at step 0:
{'currentTarget': array([20.,  9.]), 'previousTarget': array([20.,  9.]), 'currentState': array([18.37144542, 19.77074409,  0.        ]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 10.893168417856069}
done in step count: 9
reward sum = 0.8516955245594924
running average episode reward sum: 0.6971612064337062
{'currentTarget': array([20.,  9.]), 'previousTarget': array([20.,  9.]), 'currentState': array([19.51637285,  9.9396317 ,  0.        ]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 1.056788982945282}
episode index:675
target Thresh 18.965595632446743
target distance 2.0
model initialize at round 675
at step 0:
{'currentTarget': array([ 5., 23.]), 'previousTarget': array([ 5., 23.]), 'currentState': array([ 3.7407276 , 21.82831562,  0.        ]), 'targetState': array([ 5, 23], dtype=int32), 'currentDistance': 1.7200614132262078}
done in step count: 1
reward sum = 0.9807442087108992
running average episode reward sum: 0.6975807079163648
{'currentTarget': array([ 5., 23.]), 'previousTarget': array([ 5., 23.]), 'currentState': array([ 4.19092062, 22.47633278,  0.        ]), 'targetState': array([ 5, 23], dtype=int32), 'currentDistance': 0.963761793545375}
episode index:676
target Thresh 18.97862352178397
target distance 18.0
model initialize at round 676
at step 0:
{'currentTarget': array([ 5., 15.]), 'previousTarget': array([ 5., 15.]), 'currentState': array([21.1911881,  5.2576865,  0.       ]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 18.896223019612332}
done in step count: 12
reward sum = 0.7262002742056141
running average episode reward sum: 0.6976229820172353
{'currentTarget': array([ 5., 15.]), 'previousTarget': array([ 5., 15.]), 'currentState': array([ 5.47398645, 14.05201951,  0.        ]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 1.059872712874198}
episode index:677
target Thresh 18.991638389743635
target distance 5.0
model initialize at round 677
at step 0:
{'currentTarget': array([ 9., 18.]), 'previousTarget': array([ 9., 18.]), 'currentState': array([12.65345716, 19.24655813,  0.        ]), 'targetState': array([ 9, 18], dtype=int32), 'currentDistance': 3.860266367196442}
done in step count: 3
reward sum = 0.9402274406409944
running average episode reward sum: 0.697980805702521
{'currentTarget': array([ 9., 18.]), 'previousTarget': array([ 9., 18.]), 'currentState': array([ 9.68927574, 17.95346743,  0.        ]), 'targetState': array([ 9, 18], dtype=int32), 'currentDistance': 0.6908446483253979}
episode index:678
target Thresh 19.004640249340603
target distance 11.0
model initialize at round 678
at step 0:
{'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([25.28077233, 10.31218314,  0.        ]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 9.714478057179779}
done in step count: 8
reward sum = 0.8598801708146465
running average episode reward sum: 0.6982192436481942
{'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([25.543681  , 19.49989676,  0.        ]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 0.6770009412740249}
episode index:679
target Thresh 19.01762911357674
target distance 17.0
model initialize at round 679
at step 0:
{'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([22.91869569,  8.35941303,  0.        ]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 20.93700194627067}
done in step count: 17
reward sum = 0.7162510711634825
running average episode reward sum: 0.698245761041599
{'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([ 9.41451234, 23.43990517,  0.        ]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 0.6967974592940714}
episode index:680
target Thresh 19.030604995440907
target distance 3.0
model initialize at round 680
at step 0:
{'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([10.19218659, 23.61751714,  0.        ]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 2.425809353670636}
done in step count: 2
reward sum = 0.9638364790088904
running average episode reward sum: 0.6986357620958829
{'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([11.57118136, 22.43117973,  0.        ]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 0.6081129747211668}
episode index:681
target Thresh 19.043567907908987
target distance 18.0
model initialize at round 681
at step 0:
{'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([26.66144758, 25.38492703,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 21.333095978536882}
done in step count: 15
reward sum = 0.7085047066498462
running average episode reward sum: 0.6986502326890704
{'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.62021646,  9.42556024,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.7521768213011906}
episode index:682
target Thresh 19.056517863943895
target distance 11.0
model initialize at round 682
at step 0:
{'currentTarget': array([ 5., 23.]), 'previousTarget': array([ 5., 23.]), 'currentState': array([11.8204695 , 13.29296756,  0.        ]), 'targetState': array([ 5, 23], dtype=int32), 'currentDistance': 11.863611718766174}
done in step count: 9
reward sum = 0.8372552821213217
running average episode reward sum: 0.6988531683397765
{'currentTarget': array([ 5., 23.]), 'previousTarget': array([ 5., 23.]), 'currentState': array([ 5.42114529, 22.32610524,  0.        ]), 'targetState': array([ 5, 23], dtype=int32), 'currentDistance': 0.7946681741415192}
episode index:683
target Thresh 19.069454876495595
target distance 11.0
model initialize at round 683
at step 0:
{'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([ 9.47782174, 10.78183216,  0.        ]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 12.122283750316653}
done in step count: 10
reward sum = 0.833529605976935
running average episode reward sum: 0.6990500637164391
{'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([15.54651068, 20.54133499,  0.        ]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 0.6450008964508084}
episode index:684
target Thresh 19.08237895850109
target distance 6.0
model initialize at round 684
at step 0:
{'currentTarget': array([13., 29.]), 'previousTarget': array([13., 29.]), 'currentState': array([ 8.16260791, 25.89254647,  0.        ]), 'targetState': array([13, 29], dtype=int32), 'currentDistance': 5.749489512972729}
done in step count: 4
reward sum = 0.9202265788544641
running average episode reward sum: 0.6993729491399981
{'currentTarget': array([13., 29.]), 'previousTarget': array([13., 29.]), 'currentState': array([12.16534972, 28.34077248,  0.        ]), 'targetState': array([13, 29], dtype=int32), 'currentDistance': 1.0635892111823204}
episode index:685
target Thresh 19.095290122884464
target distance 9.0
model initialize at round 685
at step 0:
{'currentTarget': array([19., 29.]), 'previousTarget': array([19., 29.]), 'currentState': array([11.21656871, 25.46401995,  0.        ]), 'targetState': array([19, 29], dtype=int32), 'currentDistance': 8.548974064471917}
done in step count: 7
reward sum = 0.8765331859192437
running average episode reward sum: 0.6996312002140204
{'currentTarget': array([19., 29.]), 'previousTarget': array([19., 29.]), 'currentState': array([18.25563377, 28.67527696,  0.        ]), 'targetState': array([19, 29], dtype=int32), 'currentDistance': 0.8121121423632505}
episode index:686
target Thresh 19.10818838255689
target distance 12.0
model initialize at round 686
at step 0:
{'currentTarget': array([21., 13.]), 'previousTarget': array([21., 13.]), 'currentState': array([20.13510494, 23.91624093,  0.        ]), 'targetState': array([21, 13], dtype=int32), 'currentDistance': 10.950450197134431}
done in step count: 10
reward sum = 0.8445829559877908
running average episode reward sum: 0.6998421925805033
{'currentTarget': array([21., 13.]), 'previousTarget': array([21., 13.]), 'currentState': array([20.46442897, 13.54611665,  0.        ]), 'targetState': array([21, 13], dtype=int32), 'currentDistance': 0.7649050405570663}
episode index:687
target Thresh 19.121073750416624
target distance 11.0
model initialize at round 687
at step 0:
{'currentTarget': array([ 2., 21.]), 'previousTarget': array([ 2., 21.]), 'currentState': array([11.79644608, 25.54735565,  0.        ]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 10.80040736825733}
done in step count: 8
reward sum = 0.8494195956830537
running average episode reward sum: 0.7000596015966407
{'currentTarget': array([ 2., 21.]), 'previousTarget': array([ 2., 21.]), 'currentState': array([ 2.80850369, 20.83437943,  0.        ]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 0.8252929081229164}
episode index:688
target Thresh 19.13394623934903
target distance 18.0
model initialize at round 688
at step 0:
{'currentTarget': array([25.,  6.]), 'previousTarget': array([25.,  6.]), 'currentState': array([16.47838581, 22.48668039,  0.        ]), 'targetState': array([25,  6], dtype=int32), 'currentDistance': 18.558786024438234}
done in step count: 14
reward sum = 0.7465310179827618
running average episode reward sum: 0.7001270492256482
{'currentTarget': array([25.,  6.]), 'previousTarget': array([25.,  6.]), 'currentState': array([24.4678342 ,  6.69533825,  0.        ]), 'targetState': array([25,  6], dtype=int32), 'currentDistance': 0.8756116232540625}
episode index:689
target Thresh 19.146805862226607
target distance 19.0
model initialize at round 689
at step 0:
{'currentTarget': array([17.,  6.]), 'previousTarget': array([17.,  6.]), 'currentState': array([14.70393473, 23.65024674,  0.        ]), 'targetState': array([17,  6], dtype=int32), 'currentDistance': 17.79896417462244}
done in step count: 16
reward sum = 0.7494285291607593
running average episode reward sum: 0.7001985006458439
{'currentTarget': array([17.,  6.]), 'previousTarget': array([17.,  6.]), 'currentState': array([16.58730724,  6.9450742 ,  0.        ]), 'targetState': array([17,  6], dtype=int32), 'currentDistance': 1.0312519380435496}
episode index:690
target Thresh 19.159652631908976
target distance 5.0
model initialize at round 690
at step 0:
{'currentTarget': array([26., 19.]), 'previousTarget': array([26., 19.]), 'currentState': array([22.47164869, 21.38269383,  0.        ]), 'targetState': array([26, 19], dtype=int32), 'currentDistance': 4.257521910567209}
done in step count: 3
reward sum = 0.9384347778267681
running average episode reward sum: 0.7005432709456716
{'currentTarget': array([26., 19.]), 'previousTarget': array([26., 19.]), 'currentState': array([25.32717454, 19.69604298,  0.        ]), 'targetState': array([26, 19], dtype=int32), 'currentDistance': 0.9680753743250994}
episode index:691
target Thresh 19.172486561242906
target distance 9.0
model initialize at round 691
at step 0:
{'currentTarget': array([17.,  7.]), 'previousTarget': array([17.,  7.]), 'currentState': array([8.98659086, 2.55065012, 0.        ]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 9.165775491751317}
done in step count: 7
reward sum = 0.8758705910929369
running average episode reward sum: 0.7007966341250752
{'currentTarget': array([17.,  7.]), 'previousTarget': array([17.,  7.]), 'currentState': array([16.12907675,  6.60415456,  0.        ]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 0.956661342238516}
episode index:692
target Thresh 19.18530766306233
target distance 12.0
model initialize at round 692
at step 0:
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([5.31934142, 7.91460252, 0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 10.719746519602417}
done in step count: 9
reward sum = 0.8440209435942219
running average episode reward sum: 0.7010033070103121
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.14628077,  6.82159704,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.8721606202654077}
episode index:693
target Thresh 19.198115950188345
target distance 15.0
model initialize at round 693
at step 0:
{'currentTarget': array([27., 27.]), 'previousTarget': array([27., 27.]), 'currentState': array([13.29911304, 28.47056931,  0.        ]), 'targetState': array([27, 27], dtype=int32), 'currentDistance': 13.779581914022396}
done in step count: 12
reward sum = 0.7983153996008711
running average episode reward sum: 0.7011435261639009
{'currentTarget': array([27., 27.]), 'previousTarget': array([27., 27.]), 'currentState': array([26.29890102, 26.82959743,  0.        ]), 'targetState': array([27, 27], dtype=int32), 'currentDistance': 0.7215100928436764}
episode index:694
target Thresh 19.210911435429246
target distance 9.0
model initialize at round 694
at step 0:
{'currentTarget': array([27.,  5.]), 'previousTarget': array([27.,  5.]), 'currentState': array([21.81428295, 12.93337536,  0.        ]), 'targetState': array([27,  5], dtype=int32), 'currentDistance': 9.477874543589781}
done in step count: 8
reward sum = 0.8632068478202353
running average episode reward sum: 0.7013767107993776
{'currentTarget': array([27.,  5.]), 'previousTarget': array([27.,  5.]), 'currentState': array([26.64334062,  5.40264809,  0.        ]), 'targetState': array([27,  5], dtype=int32), 'currentDistance': 0.537895343560577}
episode index:695
target Thresh 19.223694131580515
target distance 10.0
model initialize at round 695
at step 0:
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([24.66832495,  9.27188116,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 9.663789425727257}
done in step count: 7
reward sum = 0.8647776696889277
running average episode reward sum: 0.701611482292035
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.97518772,  5.05980921,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.977020075732282}
episode index:696
target Thresh 19.236464051424853
target distance 8.0
model initialize at round 696
at step 0:
{'currentTarget': array([11., 18.]), 'previousTarget': array([11., 18.]), 'currentState': array([ 4.35586429, 24.08302134,  0.        ]), 'targetState': array([11, 18], dtype=int32), 'currentDistance': 9.008201154328152}
done in step count: 7
reward sum = 0.863501217361675
running average episode reward sum: 0.7018437487698967
{'currentTarget': array([11., 18.]), 'previousTarget': array([11., 18.]), 'currentState': array([10.44178507, 18.29011732,  0.        ]), 'targetState': array([11, 18], dtype=int32), 'currentDistance': 0.6291041029932531}
episode index:697
target Thresh 19.24922120773218
target distance 15.0
model initialize at round 697
at step 0:
{'currentTarget': array([23., 12.]), 'previousTarget': array([23., 12.]), 'currentState': array([ 9.12280464, 19.23056597,  0.        ]), 'targetState': array([23, 12], dtype=int32), 'currentDistance': 15.647927504577796}
done in step count: 12
reward sum = 0.7800088762090562
running average episode reward sum: 0.7019557331931621
{'currentTarget': array([23., 12.]), 'previousTarget': array([23., 12.]), 'currentState': array([22.44705307, 11.89517531,  0.        ]), 'targetState': array([23, 12], dtype=int32), 'currentDistance': 0.5627952719259425}
episode index:698
target Thresh 19.26196561325965
target distance 18.0
model initialize at round 698
at step 0:
{'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([19.13295522,  6.23927641,  0.        ]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 19.585674276030133}
done in step count: 20
reward sum = 0.7201890857367053
running average episode reward sum: 0.7019818181038109
{'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([ 9.59232445, 22.19557834,  0.        ]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 0.9989707038517873}
episode index:699
target Thresh 19.274697280751674
target distance 8.0
model initialize at round 699
at step 0:
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([5.77732736, 5.10524082, 0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 9.322831428894155}
done in step count: 7
reward sum = 0.8698640693312448
running average episode reward sum: 0.7022216498912787
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.141812  , 10.61066324,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.9423745356520246}
episode index:700
target Thresh 19.287416222939918
target distance 19.0
model initialize at round 700
at step 0:
{'currentTarget': array([18., 27.]), 'previousTarget': array([18., 27.]), 'currentState': array([17.21668164,  8.95740747,  0.        ]), 'targetState': array([18, 27], dtype=int32), 'currentDistance': 18.059588386420344}
done in step count: 18
reward sum = 0.7484684421761758
running average episode reward sum: 0.7022876224908292
{'currentTarget': array([18., 27.]), 'previousTarget': array([18., 27.]), 'currentState': array([17.74534988, 26.0294888 ,  0.        ]), 'targetState': array([18, 27], dtype=int32), 'currentDistance': 1.0033636778032302}
episode index:701
target Thresh 19.30012245254332
target distance 8.0
model initialize at round 701
at step 0:
{'currentTarget': array([20., 12.]), 'previousTarget': array([20., 12.]), 'currentState': array([13.31849527, 11.49323672,  0.        ]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 6.700695070694114}
done in step count: 5
reward sum = 0.901557964159165
running average episode reward sum: 0.7025714833763966
{'currentTarget': array([20., 12.]), 'previousTarget': array([20., 12.]), 'currentState': array([19.31351203, 11.63775507,  0.        ]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 0.7762004437044459}
episode index:702
target Thresh 19.31281598226812
target distance 8.0
model initialize at round 702
at step 0:
{'currentTarget': array([16., 16.]), 'previousTarget': array([16., 16.]), 'currentState': array([23.27992815,  8.54993904,  0.        ]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 10.41636991465062}
done in step count: 9
reward sum = 0.8616807485291188
running average episode reward sum: 0.7027978123453195
{'currentTarget': array([16., 16.]), 'previousTarget': array([16., 16.]), 'currentState': array([16.59937286, 15.18460184,  0.        ]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 1.0119891223131505}
episode index:703
target Thresh 19.325496824807846
target distance 11.0
model initialize at round 703
at step 0:
{'currentTarget': array([19.,  8.]), 'previousTarget': array([19.,  8.]), 'currentState': array([15.44239753, 17.85071468,  0.        ]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 10.473448098712664}
done in step count: 9
reward sum = 0.8585230269262468
running average episode reward sum: 0.7030190129342129
{'currentTarget': array([19.,  8.]), 'previousTarget': array([19.,  8.]), 'currentState': array([18.39446791,  8.93655699,  0.        ]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 1.1152614593007262}
episode index:704
target Thresh 19.33816499284334
target distance 17.0
model initialize at round 704
at step 0:
{'currentTarget': array([ 3., 18.]), 'previousTarget': array([ 3., 18.]), 'currentState': array([18.86029553, 24.53870928,  0.        ]), 'targetState': array([ 3, 18], dtype=int32), 'currentDistance': 17.15528179646222}
done in step count: 12
reward sum = 0.7667786718978078
running average episode reward sum: 0.7031094521667853
{'currentTarget': array([ 3., 18.]), 'previousTarget': array([ 3., 18.]), 'currentState': array([ 3.62789816, 17.7478175 ,  0.        ]), 'targetState': array([ 3, 18], dtype=int32), 'currentDistance': 0.6766476983650584}
episode index:705
target Thresh 19.350820499042772
target distance 14.0
model initialize at round 705
at step 0:
{'currentTarget': array([ 2., 13.]), 'previousTarget': array([ 2., 13.]), 'currentState': array([14.30628943,  3.3096849 ,  0.        ]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 15.663555354204842}
done in step count: 10
reward sum = 0.7836670586011775
running average episode reward sum: 0.703223556425191
{'currentTarget': array([ 2., 13.]), 'previousTarget': array([ 2., 13.]), 'currentState': array([ 2.74542087, 12.28293982,  0.        ]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 1.0343246964888908}
episode index:706
target Thresh 19.36346335606165
target distance 15.0
model initialize at round 706
at step 0:
{'currentTarget': array([27., 28.]), 'previousTarget': array([27., 28.]), 'currentState': array([13.37406898, 20.09175353,  0.        ]), 'targetState': array([27, 28], dtype=int32), 'currentDistance': 15.75456627633046}
done in step count: 14
reward sum = 0.7734727380949976
running average episode reward sum: 0.7033229187755018
{'currentTarget': array([27., 28.]), 'previousTarget': array([27., 28.]), 'currentState': array([26.51548225, 27.92294318,  0.        ]), 'targetState': array([27, 28], dtype=int32), 'currentDistance': 0.4906069773740573}
episode index:707
target Thresh 19.37609357654283
target distance 12.0
model initialize at round 707
at step 0:
{'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([25.26157737, 11.51700866,  0.        ]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 12.137921846867837}
done in step count: 7
reward sum = 0.8299283411344901
running average episode reward sum: 0.703501739993523
{'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([15.82005709, 17.06750733,  0.        ]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 1.2417875114197967}
episode index:708
target Thresh 19.388711173116533
target distance 12.0
model initialize at round 708
at step 0:
{'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([ 6.19397676, 23.27534789,  0.        ]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 11.291503091079324}
done in step count: 10
reward sum = 0.82529159391803
running average episode reward sum: 0.7036735169384094
{'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([16.17161116, 19.8552195 ,  0.        ]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 0.8409455772220527}
episode index:709
target Thresh 19.401316158400356
target distance 9.0
model initialize at round 709
at step 0:
{'currentTarget': array([12., 14.]), 'previousTarget': array([12., 14.]), 'currentState': array([ 4.14993596, 15.38070977,  0.        ]), 'targetState': array([12, 14], dtype=int32), 'currentDistance': 7.970562394313798}
done in step count: 6
reward sum = 0.8815675053530385
running average episode reward sum: 0.7039240718516694
{'currentTarget': array([12., 14.]), 'previousTarget': array([12., 14.]), 'currentState': array([11.52204818, 13.82938527,  0.        ]), 'targetState': array([12, 14], dtype=int32), 'currentDistance': 0.5074912159418463}
episode index:710
target Thresh 19.413908544999295
target distance 12.0
model initialize at round 710
at step 0:
{'currentTarget': array([25.,  6.]), 'previousTarget': array([25.,  6.]), 'currentState': array([14.26863289, 13.42370737,  0.        ]), 'targetState': array([25,  6], dtype=int32), 'currentDistance': 13.04889539889583}
done in step count: 9
reward sum = 0.8250326563489633
running average episode reward sum: 0.7040944074135502
{'currentTarget': array([25.,  6.]), 'previousTarget': array([25.,  6.]), 'currentState': array([24.04154864,  6.94440693,  0.        ]), 'targetState': array([25,  6], dtype=int32), 'currentDistance': 1.3455606465134868}
episode index:711
target Thresh 19.426488345505724
target distance 12.0
model initialize at round 711
at step 0:
{'currentTarget': array([27., 17.]), 'previousTarget': array([27., 17.]), 'currentState': array([16.20241129,  6.30598855,  0.        ]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 15.197032695662724}
done in step count: 14
reward sum = 0.7854863311924245
running average episode reward sum: 0.7042087219132397
{'currentTarget': array([27., 17.]), 'previousTarget': array([27., 17.]), 'currentState': array([26.02648327, 16.37104979,  0.        ]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 1.159013888202296}
episode index:712
target Thresh 19.439055572499452
target distance 7.0
model initialize at round 712
at step 0:
{'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([ 6.21396291, 19.82372022,  0.        ]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 6.0914403584628465}
done in step count: 5
reward sum = 0.9174675295119997
running average episode reward sum: 0.7045078226251594
{'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([ 7.23902776, 14.81101346,  0.        ]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 1.1121248046896404}
episode index:713
target Thresh 19.45161023854771
target distance 2.0
model initialize at round 713
at step 0:
{'currentTarget': array([19., 27.]), 'previousTarget': array([19., 27.]), 'currentState': array([17.79060829, 27.62713695,  0.        ]), 'targetState': array([19, 27], dtype=int32), 'currentDistance': 1.3623248748040129}
done in step count: 1
reward sum = 0.9840099008216873
running average episode reward sum: 0.7048992821184319
{'currentTarget': array([19., 27.]), 'previousTarget': array([19., 27.]), 'currentState': array([18.29117036, 27.19383305,  0.        ]), 'targetState': array([19, 27], dtype=int32), 'currentDistance': 0.7348542120201109}
episode index:714
target Thresh 19.464152356205155
target distance 7.0
model initialize at round 714
at step 0:
{'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([8.54889011, 6.10841185, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 5.935956706248132}
done in step count: 4
reward sum = 0.9120060777135394
running average episode reward sum: 0.7051889419724111
{'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.9173069 , 3.86699024, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.9268999652097805}
episode index:715
target Thresh 19.47668193801391
target distance 13.0
model initialize at round 715
at step 0:
{'currentTarget': array([10., 16.]), 'previousTarget': array([10., 16.]), 'currentState': array([21.66077387, 26.76859677,  0.        ]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 15.872502124266891}
done in step count: 12
reward sum = 0.7796252112133708
running average episode reward sum: 0.7052929032423008
{'currentTarget': array([10., 16.]), 'previousTarget': array([10., 16.]), 'currentState': array([10.84363386, 16.56850386,  0.        ]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 1.0173075864083259}
episode index:716
target Thresh 19.489198996503564
target distance 4.0
model initialize at round 716
at step 0:
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.77109207, 4.02960706, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 3.0688462353350694}
done in step count: 3
reward sum = 0.9519833715186654
running average episode reward sum: 0.7056369624728117
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.81720509, 6.60783356, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.43267596854963686}
episode index:717
target Thresh 19.501703544191173
target distance 12.0
model initialize at round 717
at step 0:
{'currentTarget': array([14., 17.]), 'previousTarget': array([14., 17.]), 'currentState': array([10.25677955,  5.73272181,  0.        ]), 'targetState': array([14, 17], dtype=int32), 'currentDistance': 11.872794836795812}
done in step count: 10
reward sum = 0.8408279870676891
running average episode reward sum: 0.705825250807902
{'currentTarget': array([14., 17.]), 'previousTarget': array([14., 17.]), 'currentState': array([13.74400358, 16.21182561,  0.        ]), 'targetState': array([14, 17], dtype=int32), 'currentDistance': 0.8287056412038749}
episode index:718
target Thresh 19.51419559358128
target distance 19.0
model initialize at round 718
at step 0:
{'currentTarget': array([6., 3.]), 'previousTarget': array([6., 3.]), 'currentState': array([23.62523711,  9.4796266 ,  0.        ]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 18.778566077072725}
done in step count: 13
reward sum = 0.7439690246136101
running average episode reward sum: 0.705878301953668
{'currentTarget': array([6., 3.]), 'previousTarget': array([6., 3.]), 'currentState': array([6.72818184, 2.87841222, 0.        ]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 0.7382630829404263}
episode index:719
target Thresh 19.526675157165947
target distance 9.0
model initialize at round 719
at step 0:
{'currentTarget': array([15., 28.]), 'previousTarget': array([15., 28.]), 'currentState': array([ 6.42592594, 20.99205589,  0.        ]), 'targetState': array([15, 28], dtype=int32), 'currentDistance': 11.07366364801006}
done in step count: 8
reward sum = 0.8542948311081394
running average episode reward sum: 0.706084436021938
{'currentTarget': array([15., 28.]), 'previousTarget': array([15., 28.]), 'currentState': array([14.01196399, 27.38537735,  0.        ]), 'targetState': array([15, 28], dtype=int32), 'currentDistance': 1.1636048107142014}
episode index:720
target Thresh 19.539142247424724
target distance 12.0
model initialize at round 720
at step 0:
{'currentTarget': array([23., 26.]), 'previousTarget': array([23., 26.]), 'currentState': array([13.61320925, 15.24669588,  0.        ]), 'targetState': array([23, 26], dtype=int32), 'currentDistance': 14.27394094881185}
done in step count: 12
reward sum = 0.8002863077702246
running average episode reward sum: 0.7062150904903823
{'currentTarget': array([23., 26.]), 'previousTarget': array([23., 26.]), 'currentState': array([22.11329636, 25.7119852 ,  0.        ]), 'targetState': array([23, 26], dtype=int32), 'currentDistance': 0.9323067468551459}
episode index:721
target Thresh 19.551596876824714
target distance 9.0
model initialize at round 721
at step 0:
{'currentTarget': array([24., 16.]), 'previousTarget': array([24., 16.]), 'currentState': array([20.39619866,  8.07536745,  0.        ]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 8.705583561049297}
done in step count: 6
reward sum = 0.8819594726275858
running average episode reward sum: 0.7064585037620406
{'currentTarget': array([24., 16.]), 'previousTarget': array([24., 16.]), 'currentState': array([23.58478118, 15.28134978,  0.        ]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 0.8299788002863647}
episode index:722
target Thresh 19.564039057820544
target distance 3.0
model initialize at round 722
at step 0:
{'currentTarget': array([12., 27.]), 'previousTarget': array([12., 27.]), 'currentState': array([11.80404595, 25.21578765,  0.        ]), 'targetState': array([12, 27], dtype=int32), 'currentDistance': 1.7949405852064448}
done in step count: 1
reward sum = 0.9766756740986045
running average episode reward sum: 0.7068322481193525
{'currentTarget': array([12., 27.]), 'previousTarget': array([12., 27.]), 'currentState': array([11.67241652, 26.28471255,  0.        ]), 'targetState': array([12, 27], dtype=int32), 'currentDistance': 0.7867318896165586}
episode index:723
target Thresh 19.576468802854393
target distance 13.0
model initialize at round 723
at step 0:
{'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([10.15824242,  9.93919641,  0.        ]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 12.657886261068032}
done in step count: 12
reward sum = 0.8202396892252493
running average episode reward sum: 0.7069888882313773
{'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([13.44350757, 21.61079502,  0.        ]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 0.6790908212944962}
episode index:724
target Thresh 19.588886124356012
target distance 19.0
model initialize at round 724
at step 0:
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([19.6671052 , 17.68316746,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 20.645984427985223}
done in step count: 15
reward sum = 0.7110317010975225
running average episode reward sum: 0.7069944645249858
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.67811739, 6.83750987, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.6973135900209594}
episode index:725
target Thresh 19.601291034742722
target distance 1.0
model initialize at round 725
at step 0:
{'currentTarget': array([11., 21.]), 'previousTarget': array([11., 21.]), 'currentState': array([10.91985521, 21.12570864,  0.        ]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 0.14908336155800186}
done in step count: 0
reward sum = 0.9961459571266142
running average episode reward sum: 0.7073927448178254
{'currentTarget': array([11., 21.]), 'previousTarget': array([11., 21.]), 'currentState': array([10.91985521, 21.12570864,  0.        ]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 0.14908336155800186}
episode index:726
target Thresh 19.61368354641943
target distance 11.0
model initialize at round 726
at step 0:
{'currentTarget': array([11., 16.]), 'previousTarget': array([11., 16.]), 'currentState': array([20.84955907, 15.26341286,  0.        ]), 'targetState': array([11, 16], dtype=int32), 'currentDistance': 9.877063048183166}
done in step count: 8
reward sum = 0.8658956727513036
running average episode reward sum: 0.7076107681024658
{'currentTarget': array([11., 16.]), 'previousTarget': array([11., 16.]), 'currentState': array([11.78519195, 15.63410285,  0.        ]), 'targetState': array([11, 16], dtype=int32), 'currentDistance': 0.8662604272960802}
episode index:727
target Thresh 19.626063671778653
target distance 5.0
model initialize at round 727
at step 0:
{'currentTarget': array([17., 17.]), 'previousTarget': array([17., 17.]), 'currentState': array([20.71241605, 13.72290957,  0.        ]), 'targetState': array([17, 17], dtype=int32), 'currentDistance': 4.951904142582317}
done in step count: 3
reward sum = 0.9349937718380636
running average episode reward sum: 0.7079231073933114
{'currentTarget': array([17., 17.]), 'previousTarget': array([17., 17.]), 'currentState': array([17.80371684, 16.2216751 ,  0.        ]), 'targetState': array([17, 17], dtype=int32), 'currentDistance': 1.1188165217884531}
episode index:728
target Thresh 19.638431423200515
target distance 5.0
model initialize at round 728
at step 0:
{'currentTarget': array([12.,  4.]), 'previousTarget': array([12.,  4.]), 'currentState': array([9.42801806, 7.91006899, 0.        ]), 'targetState': array([12,  4], dtype=int32), 'currentDistance': 4.680142153808041}
done in step count: 4
reward sum = 0.9346965105858838
running average episode reward sum: 0.7082341820204617
{'currentTarget': array([12.,  4.]), 'previousTarget': array([12.,  4.]), 'currentState': array([11.42023748,  4.52549851,  0.        ]), 'targetState': array([12,  4], dtype=int32), 'currentDistance': 0.7824789205704548}
episode index:729
target Thresh 19.650786813052775
target distance 19.0
model initialize at round 729
at step 0:
{'currentTarget': array([23.,  4.]), 'previousTarget': array([23.,  4.]), 'currentState': array([ 5.42926717, 10.54983586,  0.        ]), 'targetState': array([23,  4], dtype=int32), 'currentDistance': 18.751826632475918}
done in step count: 13
reward sum = 0.7441903073955849
running average episode reward sum: 0.708283436986729
{'currentTarget': array([23.,  4.]), 'previousTarget': array([23.,  4.]), 'currentState': array([22.20663041,  4.61195064,  0.        ]), 'targetState': array([23,  4], dtype=int32), 'currentDistance': 1.0019575285856621}
episode index:730
target Thresh 19.66312985369082
target distance 9.0
model initialize at round 730
at step 0:
{'currentTarget': array([15., 20.]), 'previousTarget': array([15., 20.]), 'currentState': array([22.67714882, 13.45893928,  0.        ]), 'targetState': array([15, 20], dtype=int32), 'currentDistance': 10.085836072328371}
done in step count: 8
reward sum = 0.8607997257163555
running average episode reward sum: 0.7084920776005862
{'currentTarget': array([15., 20.]), 'previousTarget': array([15., 20.]), 'currentState': array([15.54115522, 19.27782175,  0.        ]), 'targetState': array([15, 20], dtype=int32), 'currentDistance': 0.9024358130665696}
episode index:731
target Thresh 19.675460557457683
target distance 17.0
model initialize at round 731
at step 0:
{'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([18.48084629, 15.61937082,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 17.71863863353375}
done in step count: 12
reward sum = 0.7415023316260296
running average episode reward sum: 0.7085371735760309
{'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.58172274, 6.91639414, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.5876999924316525}
episode index:732
target Thresh 19.687778936684083
target distance 6.0
model initialize at round 732
at step 0:
{'currentTarget': array([10., 20.]), 'previousTarget': array([10., 20.]), 'currentState': array([ 5.40259826, 18.3190676 ,  0.        ]), 'targetState': array([10, 20], dtype=int32), 'currentDistance': 4.89506245968795}
done in step count: 4
reward sum = 0.9276573029171487
running average episode reward sum: 0.7088361096324308
{'currentTarget': array([10., 20.]), 'previousTarget': array([10., 20.]), 'currentState': array([ 9.17500439, 19.58914433,  0.        ]), 'targetState': array([10, 20], dtype=int32), 'currentDistance': 0.9216399133711448}
episode index:733
target Thresh 19.70008500368839
target distance 14.0
model initialize at round 733
at step 0:
{'currentTarget': array([ 7., 16.]), 'previousTarget': array([ 7., 16.]), 'currentState': array([19.5670166 , 21.55601081,  0.        ]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 13.74042075093771}
done in step count: 10
reward sum = 0.8063948652904362
running average episode reward sum: 0.7089690234684772
{'currentTarget': array([ 7., 16.]), 'previousTarget': array([ 7., 16.]), 'currentState': array([ 7.69153804, 15.93076119,  0.        ]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 0.694995588550382}
episode index:734
target Thresh 19.712378770776677
target distance 10.0
model initialize at round 734
at step 0:
{'currentTarget': array([15., 29.]), 'previousTarget': array([15., 29.]), 'currentState': array([13.42610198, 20.12329984,  0.        ]), 'targetState': array([15, 29], dtype=int32), 'currentDistance': 9.015151732001343}
done in step count: 8
reward sum = 0.870052116709384
running average episode reward sum: 0.7091881841395533
{'currentTarget': array([15., 29.]), 'previousTarget': array([15., 29.]), 'currentState': array([14.7373289 , 28.50362474,  0.        ]), 'targetState': array([15, 29], dtype=int32), 'currentDistance': 0.5615910498229036}
episode index:735
target Thresh 19.72466025024271
target distance 8.0
model initialize at round 735
at step 0:
{'currentTarget': array([15., 20.]), 'previousTarget': array([15., 20.]), 'currentState': array([ 8.2345227, 21.2790131,  0.       ]), 'targetState': array([15, 20], dtype=int32), 'currentDistance': 6.885314633018135}
done in step count: 5
reward sum = 0.9024147688374237
running average episode reward sum: 0.7094507202600667
{'currentTarget': array([15., 20.]), 'previousTarget': array([15., 20.]), 'currentState': array([14.05970269, 20.03672922,  0.        ]), 'targetState': array([15, 20], dtype=int32), 'currentDistance': 0.9410143773115537}
episode index:736
target Thresh 19.73692945436797
target distance 2.0
model initialize at round 736
at step 0:
{'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([18.18625945, 23.17057425,  0.        ]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 1.6665699952062256}
done in step count: 1
reward sum = 0.9795011618722386
running average episode reward sum: 0.7098171387697169
{'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([17.66757601, 22.47187084,  0.        ]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 0.817508302745654}
episode index:737
target Thresh 19.749186395421667
target distance 7.0
model initialize at round 737
at step 0:
{'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([10.7112481, 19.9707067,  0.       ]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 6.557454137955211}
done in step count: 5
reward sum = 0.9140018874121979
running average episode reward sum: 0.7100938118708583
{'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([ 8.58065692, 14.74155247,  0.        ]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 0.9418399667065369}
episode index:738
target Thresh 19.761431085660732
target distance 16.0
model initialize at round 738
at step 0:
{'currentTarget': array([5., 6.]), 'previousTarget': array([5., 6.]), 'currentState': array([19.64072585, 15.91930556,  0.        ]), 'targetState': array([5, 6], dtype=int32), 'currentDistance': 17.684554738457305}
done in step count: 13
reward sum = 0.7528551639453885
running average episode reward sum: 0.7101516756761013
{'currentTarget': array([5., 6.]), 'previousTarget': array([5., 6.]), 'currentState': array([5.5696767 , 6.26393986, 0.        ]), 'targetState': array([5, 6], dtype=int32), 'currentDistance': 0.6278501316096631}
episode index:739
target Thresh 19.773663537329867
target distance 13.0
model initialize at round 739
at step 0:
{'currentTarget': array([17., 28.]), 'previousTarget': array([17., 28.]), 'currentState': array([10.38345832, 15.32954434,  0.        ]), 'targetState': array([17, 28], dtype=int32), 'currentDistance': 14.294022197940855}
done in step count: 14
reward sum = 0.7984658042389476
running average episode reward sum: 0.7102710190930781
{'currentTarget': array([17., 28.]), 'previousTarget': array([17., 28.]), 'currentState': array([16.49899586, 27.04148495,  0.        ]), 'targetState': array([17, 28], dtype=int32), 'currentDistance': 1.0815527033620769}
episode index:740
target Thresh 19.78588376266152
target distance 16.0
model initialize at round 740
at step 0:
{'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([ 2.09744538, 23.59100676,  0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 14.87690497021079}
done in step count: 11
reward sum = 0.8004377965527626
running average episode reward sum: 0.7103927016537523
{'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([4.07238127, 9.92513025, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 1.31009255114283}
episode index:741
target Thresh 19.798091773875917
target distance 13.0
model initialize at round 741
at step 0:
{'currentTarget': array([10., 13.]), 'previousTarget': array([10., 13.]), 'currentState': array([21.58866858, 20.10022599,  0.        ]), 'targetState': array([10, 13], dtype=int32), 'currentDistance': 13.590822219706354}
done in step count: 9
reward sum = 0.8115885977850931
running average episode reward sum: 0.7105290842630938
{'currentTarget': array([10., 13.]), 'previousTarget': array([10., 13.]), 'currentState': array([10.89399099, 13.33536717,  0.        ]), 'targetState': array([10, 13], dtype=int32), 'currentDistance': 0.9548251343338845}
episode index:742
target Thresh 19.810287583181072
target distance 9.0
model initialize at round 742
at step 0:
{'currentTarget': array([22.,  2.]), 'previousTarget': array([22.,  2.]), 'currentState': array([15.26038584, 10.03317273,  0.        ]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 10.48590783224389}
done in step count: 9
reward sum = 0.8554914908735624
running average episode reward sum: 0.7107241884442653
{'currentTarget': array([22.,  2.]), 'previousTarget': array([22.,  2.]), 'currentState': array([21.50054631,  2.37723571,  0.        ]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 0.625907959245385}
episode index:743
target Thresh 19.82247120277279
target distance 14.0
model initialize at round 743
at step 0:
{'currentTarget': array([20.,  6.]), 'previousTarget': array([20.,  6.]), 'currentState': array([7.51449049, 6.65240633, 0.        ]), 'targetState': array([20,  6], dtype=int32), 'currentDistance': 12.502543016129401}
done in step count: 10
reward sum = 0.8121260414987677
running average episode reward sum: 0.7108604812575107
{'currentTarget': array([20.,  6.]), 'previousTarget': array([20.,  6.]), 'currentState': array([19.2774325 ,  5.76538434,  0.        ]), 'targetState': array([20,  6], dtype=int32), 'currentDistance': 0.7597027699281127}
episode index:744
target Thresh 19.8346426448347
target distance 16.0
model initialize at round 744
at step 0:
{'currentTarget': array([12., 25.]), 'previousTarget': array([12., 25.]), 'currentState': array([8.95252766, 9.85373062, 0.        ]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 15.449807890185854}
done in step count: 13
reward sum = 0.7907958830920212
running average episode reward sum: 0.7109677770988994
{'currentTarget': array([12., 25.]), 'previousTarget': array([12., 25.]), 'currentState': array([11.62230805, 24.29692388,  0.        ]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 0.7981022779021709}
episode index:745
target Thresh 19.846801921538244
target distance 9.0
model initialize at round 745
at step 0:
{'currentTarget': array([20., 11.]), 'previousTarget': array([20., 11.]), 'currentState': array([12.15116644, 17.36812186,  0.        ]), 'targetState': array([20, 11], dtype=int32), 'currentDistance': 10.107282736814977}
done in step count: 8
reward sum = 0.8560108226293804
running average episode reward sum: 0.7111622047738732
{'currentTarget': array([20., 11.]), 'previousTarget': array([20., 11.]), 'currentState': array([19.37831026, 11.39168894,  0.        ]), 'targetState': array([20, 11], dtype=int32), 'currentDistance': 0.7347913696375704}
episode index:746
target Thresh 19.858949045042692
target distance 5.0
model initialize at round 746
at step 0:
{'currentTarget': array([23.,  5.]), 'previousTarget': array([23.,  5.]), 'currentState': array([21.88815172,  8.78065252,  0.        ]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 3.9407537473615517}
done in step count: 3
reward sum = 0.9466703123250294
running average episode reward sum: 0.711477476671532
{'currentTarget': array([23.,  5.]), 'previousTarget': array([23.,  5.]), 'currentState': array([22.28550594,  5.7567808 ,  0.        ]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 1.0407780513791105}
episode index:747
target Thresh 19.871084027495172
target distance 5.0
model initialize at round 747
at step 0:
{'currentTarget': array([13., 29.]), 'previousTarget': array([13., 29.]), 'currentState': array([ 9.53963649, 28.64453906,  0.        ]), 'targetState': array([13, 29], dtype=int32), 'currentDistance': 3.478572707820138}
done in step count: 3
reward sum = 0.9433540680005293
running average episode reward sum: 0.7117874721144853
{'currentTarget': array([13., 29.]), 'previousTarget': array([13., 29.]), 'currentState': array([12.23711425, 28.84771481,  0.        ]), 'targetState': array([13, 29], dtype=int32), 'currentDistance': 0.7779366588374187}
episode index:748
target Thresh 19.883206881030674
target distance 16.0
model initialize at round 748
at step 0:
{'currentTarget': array([24.,  4.]), 'previousTarget': array([24.,  4.]), 'currentState': array([10.77349252, 19.31403476,  0.        ]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 20.23512196352916}
done in step count: 18
reward sum = 0.7351863806829938
running average episode reward sum: 0.7118187123128411
{'currentTarget': array([24.,  4.]), 'previousTarget': array([24.,  4.]), 'currentState': array([23.13987359,  4.86572391,  0.        ]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 1.220366878857352}
episode index:749
target Thresh 19.895317617772044
target distance 14.0
model initialize at round 749
at step 0:
{'currentTarget': array([11.,  5.]), 'previousTarget': array([11.,  5.]), 'currentState': array([20.42026109, 18.23202592,  0.        ]), 'targetState': array([11,  5], dtype=int32), 'currentDistance': 16.24277774561175}
done in step count: 14
reward sum = 0.7886347279742577
running average episode reward sum: 0.7119211336670563
{'currentTarget': array([11.,  5.]), 'previousTarget': array([11.,  5.]), 'currentState': array([11.89178842,  5.83954531,  0.        ]), 'targetState': array([11,  5], dtype=int32), 'currentDistance': 1.2247950515425339}
episode index:750
target Thresh 19.90741624983002
target distance 9.0
model initialize at round 750
at step 0:
{'currentTarget': array([15., 16.]), 'previousTarget': array([15., 16.]), 'currentState': array([22.66710007, 21.17628127,  0.        ]), 'targetState': array([15, 16], dtype=int32), 'currentDistance': 9.250854627202257}
done in step count: 6
reward sum = 0.8735953878245104
running average episode reward sum: 0.7121364123010875
{'currentTarget': array([15., 16.]), 'previousTarget': array([15., 16.]), 'currentState': array([15.95998001, 16.45799142,  0.        ]), 'targetState': array([15, 16], dtype=int32), 'currentDistance': 1.063634224442959}
episode index:751
target Thresh 19.919502789303245
target distance 4.0
model initialize at round 751
at step 0:
{'currentTarget': array([22., 24.]), 'previousTarget': array([22., 24.]), 'currentState': array([22.63575333, 21.11147249,  0.        ]), 'targetState': array([22, 24], dtype=int32), 'currentDistance': 2.957663519047565}
done in step count: 2
reward sum = 0.9607465729904285
running average episode reward sum: 0.7124670109190254
{'currentTarget': array([22., 24.]), 'previousTarget': array([22., 24.]), 'currentState': array([21.86832759, 23.22811103,  0.        ]), 'targetState': array([22, 24], dtype=int32), 'currentDistance': 0.7830390847095268}
episode index:752
target Thresh 19.93157724827825
target distance 12.0
model initialize at round 752
at step 0:
{'currentTarget': array([15., 26.]), 'previousTarget': array([15., 26.]), 'currentState': array([17.97251728, 15.31467509,  0.        ]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 11.091078729146407}
done in step count: 9
reward sum = 0.845602206023884
running average episode reward sum: 0.7126438172870264
{'currentTarget': array([15., 26.]), 'previousTarget': array([15., 26.]), 'currentState': array([15.20401189, 25.35145164,  0.        ]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 0.6798792787847638}
episode index:753
target Thresh 19.943639638829495
target distance 4.0
model initialize at round 753
at step 0:
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([16.91138482,  8.12565613,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 3.604798961078584}
done in step count: 3
reward sum = 0.9448366018189771
running average episode reward sum: 0.7129517652771219
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.61226654,  6.16083372,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.6330385462023483}
episode index:754
target Thresh 19.955689973019375
target distance 8.0
model initialize at round 754
at step 0:
{'currentTarget': array([24., 21.]), 'previousTarget': array([24., 21.]), 'currentState': array([17.7100386 , 14.05420721,  0.        ]), 'targetState': array([24, 21], dtype=int32), 'currentDistance': 9.370573724390882}
done in step count: 6
reward sum = 0.8772200854342186
running average episode reward sum: 0.7131693392111049
{'currentTarget': array([24., 21.]), 'previousTarget': array([24., 21.]), 'currentState': array([23.01785275, 20.2009781 ,  0.        ]), 'targetState': array([24, 21], dtype=int32), 'currentDistance': 1.2661157965221894}
episode index:755
target Thresh 19.967728262898227
target distance 5.0
model initialize at round 755
at step 0:
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([19.24656904,  8.97310257,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 5.130862943331399}
done in step count: 3
reward sum = 0.9380560557125557
running average episode reward sum: 0.7134668084128264
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.99255908,  5.88044941,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.3267873564100077}
episode index:756
target Thresh 19.979754520504336
target distance 16.0
model initialize at round 756
at step 0:
{'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([10.89899879, 19.23863184,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 17.164209342966913}
done in step count: 16
reward sum = 0.7678442191924809
running average episode reward sum: 0.7135386411879646
{'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.73444323, 4.51179117, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.895174319877507}
episode index:757
target Thresh 19.991768757863966
target distance 9.0
model initialize at round 757
at step 0:
{'currentTarget': array([13.,  6.]), 'previousTarget': array([13.,  6.]), 'currentState': array([5.3016355 , 6.51210043, 0.        ]), 'targetState': array([13,  6], dtype=int32), 'currentDistance': 7.7153783293847455}
done in step count: 6
reward sum = 0.8848884835504962
running average episode reward sum: 0.7137646963889707
{'currentTarget': array([13.,  6.]), 'previousTarget': array([13.,  6.]), 'currentState': array([12.33070004,  5.84784421,  0.        ]), 'targetState': array([13,  6], dtype=int32), 'currentDistance': 0.6863773177693351}
episode index:758
target Thresh 20.003770986991356
target distance 12.0
model initialize at round 758
at step 0:
{'currentTarget': array([14., 29.]), 'previousTarget': array([14., 29.]), 'currentState': array([14.86535874, 18.32067156,  0.        ]), 'targetState': array([14, 29], dtype=int32), 'currentDistance': 10.714331603653443}
done in step count: 9
reward sum = 0.845602922056144
running average episode reward sum: 0.7139383962910355
{'currentTarget': array([14., 29.]), 'previousTarget': array([14., 29.]), 'currentState': array([13.7868045 , 28.57961798,  0.        ]), 'targetState': array([14, 29], dtype=int32), 'currentDistance': 0.47135269803997365}
episode index:759
target Thresh 20.01576121988873
target distance 10.0
model initialize at round 759
at step 0:
{'currentTarget': array([ 6., 15.]), 'previousTarget': array([ 6., 15.]), 'currentState': array([14.58000171,  7.35940132,  0.        ]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 11.488915424599767}
done in step count: 9
reward sum = 0.8394073709195822
running average episode reward sum: 0.7141034870471258
{'currentTarget': array([ 6., 15.]), 'previousTarget': array([ 6., 15.]), 'currentState': array([ 6.48074526, 14.45326337,  0.        ]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 0.7280363598835394}
episode index:760
target Thresh 20.027739468546322
target distance 4.0
model initialize at round 760
at step 0:
{'currentTarget': array([ 8., 16.]), 'previousTarget': array([ 8., 16.]), 'currentState': array([10.93399179, 15.804159  ,  0.        ]), 'targetState': array([ 8, 16], dtype=int32), 'currentDistance': 2.940520620536028}
done in step count: 2
reward sum = 0.9638063215682655
running average episode reward sum: 0.714431611665419
{'currentTarget': array([ 8., 16.]), 'previousTarget': array([ 8., 16.]), 'currentState': array([ 8.88491279, 15.68518743,  0.        ]), 'targetState': array([ 8, 16], dtype=int32), 'currentDistance': 0.9392430975509346}
episode index:761
target Thresh 20.03970574494239
target distance 6.0
model initialize at round 761
at step 0:
{'currentTarget': array([19., 25.]), 'previousTarget': array([19., 25.]), 'currentState': array([23.92600167, 24.14366591,  0.        ]), 'targetState': array([19, 25], dtype=int32), 'currentDistance': 4.999880049261226}
done in step count: 4
reward sum = 0.9312825173709058
running average episode reward sum: 0.7147161929065025
{'currentTarget': array([19., 25.]), 'previousTarget': array([19., 25.]), 'currentState': array([19.59796983, 24.64596879,  0.        ]), 'targetState': array([19, 25], dtype=int32), 'currentDistance': 0.6949143917584472}
episode index:762
target Thresh 20.0516600610432
target distance 12.0
model initialize at round 762
at step 0:
{'currentTarget': array([ 7., 16.]), 'previousTarget': array([ 7., 16.]), 'currentState': array([12.04380193,  5.37314129,  0.        ]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 11.763080546277227}
done in step count: 10
reward sum = 0.8347534113009086
running average episode reward sum: 0.714873515604267
{'currentTarget': array([ 7., 16.]), 'previousTarget': array([ 7., 16.]), 'currentState': array([ 7.58248175, 15.3573603 ,  0.        ]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 0.8673354414389604}
episode index:763
target Thresh 20.063602428803083
target distance 10.0
model initialize at round 763
at step 0:
{'currentTarget': array([15., 17.]), 'previousTarget': array([15., 17.]), 'currentState': array([ 6.31944335, 12.2627961 ,  0.        ]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 9.889042652965031}
done in step count: 7
reward sum = 0.8669942392804715
running average episode reward sum: 0.7150726264991311
{'currentTarget': array([15., 17.]), 'previousTarget': array([15., 17.]), 'currentState': array([14.00600082, 16.41988403,  0.        ]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 1.1508991786381109}
episode index:764
target Thresh 20.075532860164394
target distance 20.0
model initialize at round 764
at step 0:
{'currentTarget': array([24.,  5.]), 'previousTarget': array([24.,  5.]), 'currentState': array([17.23254184, 23.45010591,  0.        ]), 'targetState': array([24,  5], dtype=int32), 'currentDistance': 19.652096524677056}
done in step count: 15
reward sum = 0.7278350073421225
running average episode reward sum: 0.7150893093499062
{'currentTarget': array([24.,  5.]), 'previousTarget': array([24.,  5.]), 'currentState': array([23.39395742,  5.3912273 ,  0.        ]), 'targetState': array([24,  5], dtype=int32), 'currentDistance': 0.7213504082380905}
episode index:765
target Thresh 20.087451367057575
target distance 19.0
model initialize at round 765
at step 0:
{'currentTarget': array([26.,  3.]), 'previousTarget': array([26.,  3.]), 'currentState': array([10.42180541, 21.69766495,  0.        ]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 24.336861364595123}
done in step count: 27
reward sum = 0.6654475519188003
running average episode reward sum: 0.7150245028780641
{'currentTarget': array([26.,  3.]), 'previousTarget': array([26.,  3.]), 'currentState': array([25.30608127,  3.70647576,  0.        ]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 0.9902682547345414}
episode index:766
target Thresh 20.099357961401132
target distance 15.0
model initialize at round 766
at step 0:
{'currentTarget': array([25., 10.]), 'previousTarget': array([25., 10.]), 'currentState': array([15.63109344, 24.04890716,  0.        ]), 'targetState': array([25, 10], dtype=int32), 'currentDistance': 16.886331824760294}
done in step count: 14
reward sum = 0.7741971851768094
running average episode reward sum: 0.7151016510948812
{'currentTarget': array([25., 10.]), 'previousTarget': array([25., 10.]), 'currentState': array([24.35844272, 10.5497753 ,  0.        ]), 'targetState': array([25, 10], dtype=int32), 'currentDistance': 0.8448956277838936}
episode index:767
target Thresh 20.111252655101655
target distance 7.0
model initialize at round 767
at step 0:
{'currentTarget': array([ 2., 13.]), 'previousTarget': array([ 2., 13.]), 'currentState': array([ 7.81539202, 10.39321697,  0.        ]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 6.372919432546277}
done in step count: 5
reward sum = 0.9106033248928924
running average episode reward sum: 0.7153562105659724
{'currentTarget': array([ 2., 13.]), 'previousTarget': array([ 2., 13.]), 'currentState': array([ 2.63189632, 12.63258895,  0.        ]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 0.7309472163325421}
episode index:768
target Thresh 20.123135460053845
target distance 12.0
model initialize at round 768
at step 0:
{'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([ 5.13640004, 15.88166618,  0.        ]), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 10.882521026880767}
done in step count: 9
reward sum = 0.8500981281971027
running average episode reward sum: 0.7155314276240103
{'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([4.26514672, 5.69695055, 0.        ]), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.0127928770767796}
episode index:769
target Thresh 20.135006388140503
target distance 13.0
model initialize at round 769
at step 0:
{'currentTarget': array([ 5., 19.]), 'previousTarget': array([ 5., 19.]), 'currentState': array([16.70369029, 20.45943278,  0.        ]), 'targetState': array([ 5, 19], dtype=int32), 'currentDistance': 11.794333828423989}
done in step count: 10
reward sum = 0.8318610246349701
running average episode reward sum: 0.7156825050227258
{'currentTarget': array([ 5., 19.]), 'previousTarget': array([ 5., 19.]), 'currentState': array([ 5.57346344, 18.85170015,  0.        ]), 'targetState': array([ 5, 19], dtype=int32), 'currentDistance': 0.5923285925145351}
episode index:770
target Thresh 20.14686545123256
target distance 4.0
model initialize at round 770
at step 0:
{'currentTarget': array([ 9., 29.]), 'previousTarget': array([ 9., 29.]), 'currentState': array([12.01353556, 25.90748894,  0.        ]), 'targetState': array([ 9, 29], dtype=int32), 'currentDistance': 4.317988097410005}
done in step count: 3
reward sum = 0.9399158578678092
running average episode reward sum: 0.7159733394622136
{'currentTarget': array([ 9., 29.]), 'previousTarget': array([ 9., 29.]), 'currentState': array([ 9.46525043, 28.45363939,  0.        ]), 'targetState': array([ 9, 29], dtype=int32), 'currentDistance': 0.7176126275270177}
episode index:771
target Thresh 20.158712661189085
target distance 17.0
model initialize at round 771
at step 0:
{'currentTarget': array([21., 18.]), 'previousTarget': array([21., 18.]), 'currentState': array([4.6959427 , 6.05619621, 0.        ]), 'targetState': array([21, 18], dtype=int32), 'currentDistance': 20.210807340443566}
done in step count: 16
reward sum = 0.7323431246119559
running average episode reward sum: 0.7159945438471226
{'currentTarget': array([21., 18.]), 'previousTarget': array([21., 18.]), 'currentState': array([20.20681882, 17.5208433 ,  0.        ]), 'targetState': array([21, 18], dtype=int32), 'currentDistance': 0.9266755266087131}
episode index:772
target Thresh 20.17054802985728
target distance 16.0
model initialize at round 772
at step 0:
{'currentTarget': array([ 9., 18.]), 'previousTarget': array([ 9., 18.]), 'currentState': array([23.92906928, 20.76536268,  0.        ]), 'targetState': array([ 9, 18], dtype=int32), 'currentDistance': 15.18302803584953}
done in step count: 12
reward sum = 0.793625949805164
running average episode reward sum: 0.7160949725741058
{'currentTarget': array([ 9., 18.]), 'previousTarget': array([ 9., 18.]), 'currentState': array([ 9.94643015, 17.99824952,  0.        ]), 'targetState': array([ 9, 18], dtype=int32), 'currentDistance': 0.9464317654961852}
episode index:773
target Thresh 20.18237156907252
target distance 17.0
model initialize at round 773
at step 0:
{'currentTarget': array([22., 21.]), 'previousTarget': array([22., 21.]), 'currentState': array([10.68669802,  4.91557205,  0.        ]), 'targetState': array([22, 21], dtype=int32), 'currentDistance': 19.664679612686623}
done in step count: 18
reward sum = 0.7245484423296131
running average episode reward sum: 0.7161058943696554
{'currentTarget': array([22., 21.]), 'previousTarget': array([22., 21.]), 'currentState': array([21.32249671, 20.76139978,  0.        ]), 'targetState': array([22, 21], dtype=int32), 'currentDistance': 0.7182901725516541}
episode index:774
target Thresh 20.194183290658344
target distance 7.0
model initialize at round 774
at step 0:
{'currentTarget': array([9., 7.]), 'previousTarget': array([9., 7.]), 'currentState': array([14.68925261,  2.89276332,  0.        ]), 'targetState': array([9, 7], dtype=int32), 'currentDistance': 7.016907330883604}
done in step count: 5
reward sum = 0.895399441425528
running average episode reward sum: 0.7163372408819857
{'currentTarget': array([9., 7.]), 'previousTarget': array([9., 7.]), 'currentState': array([9.55880982, 6.65892383, 0.        ]), 'targetState': array([9, 7], dtype=int32), 'currentDistance': 0.6546765323603939}
episode index:775
target Thresh 20.205983206426474
target distance 12.0
model initialize at round 775
at step 0:
{'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([ 6.43115443, 10.13633883,  0.        ]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 12.207832526708971}
done in step count: 9
reward sum = 0.840568172103651
running average episode reward sum: 0.7164973322881991
{'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([11.19762617, 20.01660705,  0.        ]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 1.2691987469204222}
episode index:776
target Thresh 20.21777132817683
target distance 16.0
model initialize at round 776
at step 0:
{'currentTarget': array([11., 27.]), 'previousTarget': array([11., 27.]), 'currentState': array([25.8043834, 28.8775627,  0.       ]), 'targetState': array([11, 27], dtype=int32), 'currentDistance': 14.922969190721464}
done in step count: 13
reward sum = 0.7859074342060213
running average episode reward sum: 0.7165866631786982
{'currentTarget': array([11., 27.]), 'previousTarget': array([11., 27.]), 'currentState': array([11.59852338, 26.68342681,  0.        ]), 'targetState': array([11, 27], dtype=int32), 'currentDistance': 0.677088487564933}
episode index:777
target Thresh 20.22954766769753
target distance 14.0
model initialize at round 777
at step 0:
{'currentTarget': array([23., 25.]), 'previousTarget': array([23., 25.]), 'currentState': array([10.44513345, 26.49338073,  0.        ]), 'targetState': array([23, 25], dtype=int32), 'currentDistance': 12.643372182228031}
done in step count: 10
reward sum = 0.81368503358495
running average episode reward sum: 0.7167114682820482
{'currentTarget': array([23., 25.]), 'previousTarget': array([23., 25.]), 'currentState': array([22.43231478, 24.93925136,  0.        ]), 'targetState': array([23, 25], dtype=int32), 'currentDistance': 0.570926354343807}
episode index:778
target Thresh 20.24131223676492
target distance 4.0
model initialize at round 778
at step 0:
{'currentTarget': array([22., 24.]), 'previousTarget': array([22., 24.]), 'currentState': array([19.46962357, 22.36986646,  0.        ]), 'targetState': array([22, 24], dtype=int32), 'currentDistance': 3.0100066870017805}
done in step count: 2
reward sum = 0.9596356386475406
running average episode reward sum: 0.7170233093223121
{'currentTarget': array([22., 24.]), 'previousTarget': array([22., 24.]), 'currentState': array([21.11006612, 23.31319749,  0.        ]), 'targetState': array([22, 24], dtype=int32), 'currentDistance': 1.1241352236315507}
episode index:779
target Thresh 20.253065047143565
target distance 17.0
model initialize at round 779
at step 0:
{'currentTarget': array([ 9., 25.]), 'previousTarget': array([ 9., 25.]), 'currentState': array([7.01345789, 8.84204429, 0.        ]), 'targetState': array([ 9, 25], dtype=int32), 'currentDistance': 16.279615541392285}
done in step count: 14
reward sum = 0.7817181556668663
running average episode reward sum: 0.7171062514330102
{'currentTarget': array([ 9., 25.]), 'previousTarget': array([ 9., 25.]), 'currentState': array([ 8.50122262, 24.40439045,  0.        ]), 'targetState': array([ 9, 25], dtype=int32), 'currentDistance': 0.7768716798028056}
episode index:780
target Thresh 20.264806110586278
target distance 16.0
model initialize at round 780
at step 0:
{'currentTarget': array([12.,  2.]), 'previousTarget': array([12.,  2.]), 'currentState': array([16.72599828, 17.10055876,  0.        ]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 15.82282953718383}
done in step count: 15
reward sum = 0.7839792859223369
running average episode reward sum: 0.7171918763171196
{'currentTarget': array([12.,  2.]), 'previousTarget': array([12.,  2.]), 'currentState': array([12.52506612,  2.50809288,  0.        ]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 0.7306523166766595}
episode index:781
target Thresh 20.276535438834124
target distance 14.0
model initialize at round 781
at step 0:
{'currentTarget': array([17., 12.]), 'previousTarget': array([17., 12.]), 'currentState': array([23.74212885, 24.95003021,  0.        ]), 'targetState': array([17, 12], dtype=int32), 'currentDistance': 14.599985746416074}
done in step count: 12
reward sum = 0.8030253245686577
running average episode reward sum: 0.7173016377598964
{'currentTarget': array([17., 12.]), 'previousTarget': array([17., 12.]), 'currentState': array([17.63568792, 12.48665905,  0.        ]), 'targetState': array([17, 12], dtype=int32), 'currentDistance': 0.8005848857887848}
episode index:782
target Thresh 20.28825304361643
target distance 6.0
model initialize at round 782
at step 0:
{'currentTarget': array([18., 13.]), 'previousTarget': array([18., 13.]), 'currentState': array([13.86620879,  8.3615526 ,  0.        ]), 'targetState': array([18, 13], dtype=int32), 'currentDistance': 6.213165382666583}
done in step count: 4
reward sum = 0.911780991572367
running average episode reward sum: 0.7175500149678307
{'currentTarget': array([18., 13.]), 'previousTarget': array([18., 13.]), 'currentState': array([17.12831694, 12.44414055,  0.        ]), 'targetState': array([18, 13], dtype=int32), 'currentDistance': 1.0338331989932887}
episode index:783
target Thresh 20.299958936650803
target distance 4.0
model initialize at round 783
at step 0:
{'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([13.41517186,  7.78621757,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 3.4032586076935245}
done in step count: 3
reward sum = 0.9437948853026998
running average episode reward sum: 0.717838592608564
{'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.44708982,  9.69138432,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.6332087386131525}
episode index:784
target Thresh 20.31165312964314
target distance 5.0
model initialize at round 784
at step 0:
{'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([12.19960046, 10.08015764,  0.        ]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 5.459688711256846}
done in step count: 4
reward sum = 0.9177034280563435
running average episode reward sum: 0.7180931974944847
{'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([15.31169817, 13.57765782,  0.        ]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 0.8075471065204414}
episode index:785
target Thresh 20.323335634287638
target distance 13.0
model initialize at round 785
at step 0:
{'currentTarget': array([12., 24.]), 'previousTarget': array([12., 24.]), 'currentState': array([23.819417  , 24.58581084,  0.        ]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 11.833925492243655}
done in step count: 10
reward sum = 0.8313588048261796
running average episode reward sum: 0.7182373013206066
{'currentTarget': array([12., 24.]), 'previousTarget': array([12., 24.]), 'currentState': array([12.50753748, 23.68801178,  0.        ]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 0.5957608126215759}
episode index:786
target Thresh 20.33500646226679
target distance 16.0
model initialize at round 786
at step 0:
{'currentTarget': array([ 4., 29.]), 'previousTarget': array([ 4., 29.]), 'currentState': array([ 1.80220607, 13.76359051,  0.        ]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 15.394105110953543}
done in step count: 13
reward sum = 0.7917752396001444
running average episode reward sum: 0.7183307421570482
{'currentTarget': array([ 4., 29.]), 'previousTarget': array([ 4., 29.]), 'currentState': array([ 3.70690836, 28.50451684,  0.        ]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 0.5756789658943688}
episode index:787
target Thresh 20.346665625251433
target distance 17.0
model initialize at round 787
at step 0:
{'currentTarget': array([ 4., 26.]), 'previousTarget': array([ 4., 26.]), 'currentState': array([13.8181411 , 10.35544777,  0.        ]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 18.470189745036013}
done in step count: 15
reward sum = 0.7443032569102975
running average episode reward sum: 0.7183637022011512
{'currentTarget': array([ 4., 26.]), 'previousTarget': array([ 4., 26.]), 'currentState': array([ 4.49839918, 25.53150833,  0.        ]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 0.6840220711928405}
episode index:788
target Thresh 20.358313134900733
target distance 3.0
model initialize at round 788
at step 0:
{'currentTarget': array([13., 27.]), 'previousTarget': array([13., 27.]), 'currentState': array([14.94453275, 27.54442701,  0.        ]), 'targetState': array([13, 27], dtype=int32), 'currentDistance': 2.019308889241859}
done in step count: 2
reward sum = 0.9679766908606301
running average episode reward sum: 0.7186800684732164
{'currentTarget': array([13., 27.]), 'previousTarget': array([13., 27.]), 'currentState': array([13.68993664, 26.78239268,  0.        ]), 'targetState': array([13, 27], dtype=int32), 'currentDistance': 0.7234400526121944}
episode index:789
target Thresh 20.369949002862192
target distance 2.0
model initialize at round 789
at step 0:
{'currentTarget': array([14., 27.]), 'previousTarget': array([14., 27.]), 'currentState': array([14.66615954, 27.97515154,  0.        ]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 1.1809695410996712}
done in step count: 0
reward sum = 0.9941911803544573
running average episode reward sum: 0.7190288167161042
{'currentTarget': array([14., 27.]), 'previousTarget': array([14., 27.]), 'currentState': array([14.66615954, 27.97515154,  0.        ]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 1.1809695410996712}
episode index:790
target Thresh 20.38157324077169
target distance 17.0
model initialize at round 790
at step 0:
{'currentTarget': array([10., 28.]), 'previousTarget': array([10., 28.]), 'currentState': array([17.0596366, 12.4461031,  0.       ]), 'targetState': array([10, 28], dtype=int32), 'currentDistance': 17.081047329748696}
done in step count: 12
reward sum = 0.7708324177321932
running average episode reward sum: 0.7190943079942534
{'currentTarget': array([10., 28.]), 'previousTarget': array([10., 28.]), 'currentState': array([10.95642376, 27.02960813,  0.        ]), 'targetState': array([10, 28], dtype=int32), 'currentDistance': 1.3625001981356295}
episode index:791
target Thresh 20.39318586025346
target distance 4.0
model initialize at round 791
at step 0:
{'currentTarget': array([15., 26.]), 'previousTarget': array([15., 26.]), 'currentState': array([12.51096535, 25.68933493,  0.        ]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 2.508347322189861}
done in step count: 2
reward sum = 0.9609206997707267
running average episode reward sum: 0.7193996443475067
{'currentTarget': array([15., 26.]), 'previousTarget': array([15., 26.]), 'currentState': array([14.19507611, 25.81606645,  0.        ]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 0.8256718617500052}
episode index:792
target Thresh 20.40478687292012
target distance 14.0
model initialize at round 792
at step 0:
{'currentTarget': array([23., 16.]), 'previousTarget': array([23., 16.]), 'currentState': array([14.58402836,  2.87671429,  0.        ]), 'targetState': array([23, 16], dtype=int32), 'currentDistance': 15.590035485480675}
done in step count: 13
reward sum = 0.7938059566171303
running average episode reward sum: 0.7194934732406587
{'currentTarget': array([23., 16.]), 'previousTarget': array([23., 16.]), 'currentState': array([22.15082142, 15.24760431,  0.        ]), 'targetState': array([23, 16], dtype=int32), 'currentDistance': 1.134549927687972}
episode index:793
target Thresh 20.41637629037269
target distance 10.0
model initialize at round 793
at step 0:
{'currentTarget': array([17., 12.]), 'previousTarget': array([17., 12.]), 'currentState': array([17.8313158 ,  3.38611507,  0.        ]), 'targetState': array([17, 12], dtype=int32), 'currentDistance': 8.653906601985783}
done in step count: 7
reward sum = 0.8722855652159045
running average episode reward sum: 0.7196859066058668
{'currentTarget': array([17., 12.]), 'previousTarget': array([17., 12.]), 'currentState': array([16.70526043, 11.66942656,  0.        ]), 'targetState': array([17, 12], dtype=int32), 'currentDistance': 0.44288848615828097}
episode index:794
target Thresh 20.427954124200582
target distance 15.0
model initialize at round 794
at step 0:
{'currentTarget': array([16., 25.]), 'previousTarget': array([16., 25.]), 'currentState': array([18.7428773 , 11.28775847,  0.        ]), 'targetState': array([16, 25], dtype=int32), 'currentDistance': 13.98388156758937}
done in step count: 11
reward sum = 0.8076745091971047
running average episode reward sum: 0.719796584093403
{'currentTarget': array([16., 25.]), 'previousTarget': array([16., 25.]), 'currentState': array([16.18270709, 24.46096033,  0.        ]), 'targetState': array([16, 25], dtype=int32), 'currentDistance': 0.5691622335294587}
episode index:795
target Thresh 20.439520385981638
target distance 9.0
model initialize at round 795
at step 0:
{'currentTarget': array([24.,  4.]), 'previousTarget': array([24.,  4.]), 'currentState': array([20.13583471, 11.62006783,  0.        ]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 8.54384030781341}
done in step count: 7
reward sum = 0.8779110925842426
running average episode reward sum: 0.7199952204106025
{'currentTarget': array([24.,  4.]), 'previousTarget': array([24.,  4.]), 'currentState': array([23.33431362,  4.57533544,  0.        ]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 0.8798575075712665}
episode index:796
target Thresh 20.451075087282113
target distance 8.0
model initialize at round 796
at step 0:
{'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([22.72224879, 21.14463043,  0.        ]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 7.8972520694981725}
done in step count: 6
reward sum = 0.8835357859626016
running average episode reward sum: 0.7202004155995009
{'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([16.6449393 , 16.96814579,  0.        ]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 0.645725480299535}
episode index:797
target Thresh 20.462618239656717
target distance 16.0
model initialize at round 797
at step 0:
{'currentTarget': array([ 2., 19.]), 'previousTarget': array([ 2., 19.]), 'currentState': array([16.75768912, 23.39107183,  0.        ]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 15.397106867782904}
done in step count: 11
reward sum = 0.7905287006937973
running average episode reward sum: 0.7202885462825764
{'currentTarget': array([ 2., 19.]), 'previousTarget': array([ 2., 19.]), 'currentState': array([ 2.75818849, 18.82070388,  0.        ]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 0.7791000453268155}
episode index:798
target Thresh 20.474149854648594
target distance 3.0
model initialize at round 798
at step 0:
{'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([23.66829991, 11.67440975,  0.        ]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 2.130824751027643}
done in step count: 1
reward sum = 0.9718846642539926
running average episode reward sum: 0.7206034350409888
{'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([22.71560001, 12.38241088,  0.        ]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 0.9452511285236809}
episode index:799
target Thresh 20.485669943789368
target distance 15.0
model initialize at round 799
at step 0:
{'currentTarget': array([ 5., 12.]), 'previousTarget': array([ 5., 12.]), 'currentState': array([13.48851055, 26.24652594,  0.        ]), 'targetState': array([ 5, 12], dtype=int32), 'currentDistance': 16.583676094104586}
done in step count: 14
reward sum = 0.7792828952851129
running average episode reward sum: 0.7206767843662939
{'currentTarget': array([ 5., 12.]), 'previousTarget': array([ 5., 12.]), 'currentState': array([ 5.63487571, 12.37788773,  0.        ]), 'targetState': array([ 5, 12], dtype=int32), 'currentDistance': 0.7388276568650026}
episode index:800
target Thresh 20.497178518599128
target distance 16.0
model initialize at round 800
at step 0:
{'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([21.39310107, 22.67816663,  0.        ]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 14.765862647975917}
done in step count: 12
reward sum = 0.7936442736067897
running average episode reward sum: 0.7207678798584793
{'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([22.55250792,  8.43612885,  0.        ]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 0.6248660198580771}
episode index:801
target Thresh 20.508675590586442
target distance 19.0
model initialize at round 801
at step 0:
{'currentTarget': array([21., 23.]), 'previousTarget': array([21., 23.]), 'currentState': array([ 2.93864191, 13.81710261,  0.        ]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 20.261743771531346}
done in step count: 15
reward sum = 0.7296250132520519
running average episode reward sum: 0.7207789236657033
{'currentTarget': array([21., 23.]), 'previousTarget': array([21., 23.]), 'currentState': array([20.3067525 , 22.61188793,  0.        ]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 0.7944954812170264}
episode index:802
target Thresh 20.520161171248397
target distance 7.0
model initialize at round 802
at step 0:
{'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([9.91851974, 7.09765965, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 7.811210516876087}
done in step count: 6
reward sum = 0.887605807314983
running average episode reward sum: 0.7209866781907958
{'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.64361155, 2.13873523, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.6583944813539755}
episode index:803
target Thresh 20.53163527207056
target distance 5.0
model initialize at round 803
at step 0:
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([12.62116146,  7.6769982 ,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 4.914789039380795}
done in step count: 3
reward sum = 0.9326295748539858
running average episode reward sum: 0.7212499156244565
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.62467545, 10.25762373,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.9702277796341464}
episode index:804
target Thresh 20.543097904527045
target distance 18.0
model initialize at round 804
at step 0:
{'currentTarget': array([18., 23.]), 'previousTarget': array([18., 23.]), 'currentState': array([16.76479118,  5.74829918,  0.        ]), 'targetState': array([18, 23], dtype=int32), 'currentDistance': 17.29586430254182}
done in step count: 15
reward sum = 0.7656182988791274
running average episode reward sum: 0.7213050316284995
{'currentTarget': array([18., 23.]), 'previousTarget': array([18., 23.]), 'currentState': array([17.94565955, 22.51900232,  0.        ]), 'targetState': array([18, 23], dtype=int32), 'currentDistance': 0.48405749102547735}
episode index:805
target Thresh 20.554549080080477
target distance 18.0
model initialize at round 805
at step 0:
{'currentTarget': array([20., 22.]), 'previousTarget': array([20., 22.]), 'currentState': array([ 3.47964799, 28.52203086,  0.        ]), 'targetState': array([20, 22], dtype=int32), 'currentDistance': 17.7611631641059}
done in step count: 11
reward sum = 0.7608760189629263
running average episode reward sum: 0.7213541271462841
{'currentTarget': array([20., 22.]), 'previousTarget': array([20., 22.]), 'currentState': array([19.00720918, 22.82932532,  0.        ]), 'targetState': array([20, 22], dtype=int32), 'currentDistance': 1.2936050758885866}
episode index:806
target Thresh 20.565988810182034
target distance 15.0
model initialize at round 806
at step 0:
{'currentTarget': array([ 2., 19.]), 'previousTarget': array([ 2., 19.]), 'currentState': array([7.84796272, 5.37845755, 0.        ]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 14.823801361991473}
done in step count: 12
reward sum = 0.7968757457335697
running average episode reward sum: 0.7214477103167765
{'currentTarget': array([ 2., 19.]), 'previousTarget': array([ 2., 19.]), 'currentState': array([ 2.59218308, 18.31413466,  0.        ]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 0.9061413092586281}
episode index:807
target Thresh 20.577417106271447
target distance 16.0
model initialize at round 807
at step 0:
{'currentTarget': array([ 6., 27.]), 'previousTarget': array([ 6., 27.]), 'currentState': array([20.88420463, 22.39008796,  0.        ]), 'targetState': array([ 6, 27], dtype=int32), 'currentDistance': 15.581746896539892}
done in step count: 14
reward sum = 0.7377017612122958
running average episode reward sum: 0.7214678267163996
{'currentTarget': array([ 6., 27.]), 'previousTarget': array([ 6., 27.]), 'currentState': array([ 6.53079426, 26.60829737,  0.        ]), 'targetState': array([ 6, 27], dtype=int32), 'currentDistance': 0.6596768144845419}
episode index:808
target Thresh 20.588833979777018
target distance 20.0
model initialize at round 808
at step 0:
{'currentTarget': array([ 3., 16.]), 'previousTarget': array([ 3., 16.]), 'currentState': array([21.2398237 ,  1.89153617,  0.        ]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 23.059486556471455}
done in step count: 20
reward sum = 0.6793995043567183
running average episode reward sum: 0.7214158263179328
{'currentTarget': array([ 3., 16.]), 'previousTarget': array([ 3., 16.]), 'currentState': array([ 3.67907262, 15.30158008,  0.        ]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 0.9741303865330805}
episode index:809
target Thresh 20.600239442115615
target distance 13.0
model initialize at round 809
at step 0:
{'currentTarget': array([21., 15.]), 'previousTarget': array([21., 15.]), 'currentState': array([14.51330727, 26.6066854 ,  0.        ]), 'targetState': array([21, 15], dtype=int32), 'currentDistance': 13.296327632255046}
done in step count: 10
reward sum = 0.8170751346469187
running average episode reward sum: 0.72153392422945
{'currentTarget': array([21., 15.]), 'previousTarget': array([21., 15.]), 'currentState': array([20.18929717, 15.70377058,  0.        ]), 'targetState': array([21, 15], dtype=int32), 'currentDistance': 1.073560480734765}
episode index:810
target Thresh 20.611633504692705
target distance 9.0
model initialize at round 810
at step 0:
{'currentTarget': array([23., 21.]), 'previousTarget': array([23., 21.]), 'currentState': array([15.11329556, 17.84187442,  0.        ]), 'targetState': array([23, 21], dtype=int32), 'currentDistance': 8.495520241631072}
done in step count: 7
reward sum = 0.8753089247254381
running average episode reward sum: 0.7217235358206905
{'currentTarget': array([23., 21.]), 'previousTarget': array([23., 21.]), 'currentState': array([22.39002126, 20.59431244,  0.        ]), 'targetState': array([23, 21], dtype=int32), 'currentDistance': 0.7325683930684295}
episode index:811
target Thresh 20.623016178902354
target distance 1.0
model initialize at round 811
at step 0:
{'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([24.55738124,  9.49328724,  0.        ]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 0.7532806085481951}
done in step count: 0
reward sum = 0.9978037817321782
running average episode reward sum: 0.7220635361235371
{'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([24.55738124,  9.49328724,  0.        ]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 0.7532806085481951}
episode index:812
target Thresh 20.63438747612723
target distance 11.0
model initialize at round 812
at step 0:
{'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([22.02841172, 19.87145567,  0.        ]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 9.92488124355741}
done in step count: 8
reward sum = 0.8619403629864829
running average episode reward sum: 0.7222355863410808
{'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([20.45696995, 10.54509443,  0.        ]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 0.7694215820381061}
episode index:813
target Thresh 20.645747407738636
target distance 4.0
model initialize at round 813
at step 0:
{'currentTarget': array([9., 8.]), 'previousTarget': array([9., 8.]), 'currentState': array([6.35104144, 7.93903652, 0.        ]), 'targetState': array([9, 8], dtype=int32), 'currentDistance': 2.6496599816573103}
done in step count: 3
reward sum = 0.9530854845395871
running average episode reward sum: 0.7225191857246169
{'currentTarget': array([9., 8.]), 'previousTarget': array([9., 8.]), 'currentState': array([8.47052461, 8.16497242, 0.        ]), 'targetState': array([9, 8], dtype=int32), 'currentDistance': 0.5545810032799502}
episode index:814
target Thresh 20.657095985096504
target distance 8.0
model initialize at round 814
at step 0:
{'currentTarget': array([23., 28.]), 'previousTarget': array([23., 28.]), 'currentState': array([15.69760054, 21.07406068,  0.        ]), 'targetState': array([23, 28], dtype=int32), 'currentDistance': 10.064475808886156}
done in step count: 7
reward sum = 0.8644844795315467
running average episode reward sum: 0.7226933762691654
{'currentTarget': array([23., 28.]), 'previousTarget': array([23., 28.]), 'currentState': array([22.26098889, 27.30254602,  0.        ]), 'targetState': array([23, 28], dtype=int32), 'currentDistance': 1.0161591741602163}
episode index:815
target Thresh 20.66843321954941
target distance 11.0
model initialize at round 815
at step 0:
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([22.3354817 , 16.83549452,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 11.699370958805154}
done in step count: 8
reward sum = 0.843883763984165
running average episode reward sum: 0.722841893901169
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.68495214,  7.67540389,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.9619406699807652}
episode index:816
target Thresh 20.679759122434593
target distance 12.0
model initialize at round 816
at step 0:
{'currentTarget': array([14., 18.]), 'previousTarget': array([14., 18.]), 'currentState': array([ 3.29844594, 13.68774414,  0.        ]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 11.537712507054302}
done in step count: 8
reward sum = 0.8374909501071143
running average episode reward sum: 0.7229822232233306
{'currentTarget': array([14., 18.]), 'previousTarget': array([14., 18.]), 'currentState': array([13.17963785, 17.88624218,  0.        ]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 0.8282118687990135}
episode index:817
target Thresh 20.691073705077955
target distance 13.0
model initialize at round 817
at step 0:
{'currentTarget': array([12.,  2.]), 'previousTarget': array([12.,  2.]), 'currentState': array([14.16326317, 14.39336228,  0.        ]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 12.58074466182951}
done in step count: 12
reward sum = 0.8218974878257851
running average episode reward sum: 0.7231031465296907
{'currentTarget': array([12.,  2.]), 'previousTarget': array([12.,  2.]), 'currentState': array([12.12715567,  2.35395914,  0.        ]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 0.37610588764524755}
episode index:818
target Thresh 20.702376978794078
target distance 8.0
model initialize at round 818
at step 0:
{'currentTarget': array([23., 20.]), 'previousTarget': array([23., 20.]), 'currentState': array([16.13082147, 15.49981582,  0.        ]), 'targetState': array([23, 20], dtype=int32), 'currentDistance': 8.212019932083189}
done in step count: 6
reward sum = 0.8905558214785901
running average episode reward sum: 0.7233076064502633
{'currentTarget': array([23., 20.]), 'previousTarget': array([23., 20.]), 'currentState': array([22.02334583, 19.4427366 ,  0.        ]), 'targetState': array([23, 20], dtype=int32), 'currentDistance': 1.1244535887082483}
episode index:819
target Thresh 20.71366895488624
target distance 14.0
model initialize at round 819
at step 0:
{'currentTarget': array([13.,  7.]), 'previousTarget': array([13.,  7.]), 'currentState': array([16.08697586, 20.24018562,  0.        ]), 'targetState': array([13,  7], dtype=int32), 'currentDistance': 13.595290918373314}
done in step count: 14
reward sum = 0.8030379462402497
running average episode reward sum: 0.7234048385719584
{'currentTarget': array([13.,  7.]), 'previousTarget': array([13.,  7.]), 'currentState': array([13.20611783,  7.19313878,  0.        ]), 'targetState': array([13,  7], dtype=int32), 'currentDistance': 0.2824661863645609}
episode index:820
target Thresh 20.724949644646415
target distance 9.0
model initialize at round 820
at step 0:
{'currentTarget': array([ 5., 26.]), 'previousTarget': array([ 5., 26.]), 'currentState': array([ 2.52714115, 18.18997455,  0.        ]), 'targetState': array([ 5, 26], dtype=int32), 'currentDistance': 8.192162624029173}
done in step count: 6
reward sum = 0.8872515447608191
running average episode reward sum: 0.7236044082506293
{'currentTarget': array([ 5., 26.]), 'previousTarget': array([ 5., 26.]), 'currentState': array([ 4.53882222, 25.35029095,  0.        ]), 'targetState': array([ 5, 26], dtype=int32), 'currentDistance': 0.7967476326931654}
episode index:821
target Thresh 20.7362190593553
target distance 17.0
model initialize at round 821
at step 0:
{'currentTarget': array([19.,  4.]), 'previousTarget': array([19.,  4.]), 'currentState': array([24.89125863, 20.15207666,  0.        ]), 'targetState': array([19,  4], dtype=int32), 'currentDistance': 17.192920309878623}
done in step count: 16
reward sum = 0.7634805133195174
running average episode reward sum: 0.7236529193273555
{'currentTarget': array([19.,  4.]), 'previousTarget': array([19.,  4.]), 'currentState': array([19.37230714,  4.20976728,  0.        ]), 'targetState': array([19,  4], dtype=int32), 'currentDistance': 0.42733466581306456}
episode index:822
target Thresh 20.747477210282298
target distance 12.0
model initialize at round 822
at step 0:
{'currentTarget': array([16., 13.]), 'previousTarget': array([16., 13.]), 'currentState': array([4.89857745, 4.06331038, 0.        ]), 'targetState': array([16, 13], dtype=int32), 'currentDistance': 14.251526370009739}
done in step count: 10
reward sum = 0.8085296811922351
running average episode reward sum: 0.7237560502652229
{'currentTarget': array([16., 13.]), 'previousTarget': array([16., 13.]), 'currentState': array([15.04928821, 12.33195817,  0.        ]), 'targetState': array([16, 13], dtype=int32), 'currentDistance': 1.1619521426473476}
episode index:823
target Thresh 20.758724108685577
target distance 10.0
model initialize at round 823
at step 0:
{'currentTarget': array([18.,  4.]), 'previousTarget': array([18.,  4.]), 'currentState': array([ 8.51029563, 13.16208029,  0.        ]), 'targetState': array([18,  4], dtype=int32), 'currentDistance': 13.19083789006592}
done in step count: 11
reward sum = 0.8221056551790719
running average episode reward sum: 0.7238754065818659
{'currentTarget': array([18.,  4.]), 'previousTarget': array([18.,  4.]), 'currentState': array([17.43226445,  4.60780352,  0.        ]), 'targetState': array([18,  4], dtype=int32), 'currentDistance': 0.8317143627038048}
episode index:824
target Thresh 20.769959765812022
target distance 17.0
model initialize at round 824
at step 0:
{'currentTarget': array([20., 24.]), 'previousTarget': array([20., 24.]), 'currentState': array([20.60330245,  8.28417003,  0.        ]), 'targetState': array([20, 24], dtype=int32), 'currentDistance': 15.72740555343647}
done in step count: 13
reward sum = 0.7804679638694184
running average episode reward sum: 0.7239440036210023
{'currentTarget': array([20., 24.]), 'previousTarget': array([20., 24.]), 'currentState': array([19.60283519, 23.54454833,  0.        ]), 'targetState': array([20, 24], dtype=int32), 'currentDistance': 0.6042980266018103}
episode index:825
target Thresh 20.7811841928973
target distance 18.0
model initialize at round 825
at step 0:
{'currentTarget': array([22., 28.]), 'previousTarget': array([22., 28.]), 'currentState': array([ 5.57831025, 29.03054948,  0.        ]), 'targetState': array([22, 28], dtype=int32), 'currentDistance': 16.453994239418755}
done in step count: 12
reward sum = 0.7679014188222597
running average episode reward sum: 0.7239972208306891
{'currentTarget': array([22., 28.]), 'previousTarget': array([22., 28.]), 'currentState': array([21.28315097, 27.78707476,  0.        ]), 'targetState': array([22, 28], dtype=int32), 'currentDistance': 0.7478032428753654}
episode index:826
target Thresh 20.792397401165832
target distance 18.0
model initialize at round 826
at step 0:
{'currentTarget': array([12.,  2.]), 'previousTarget': array([12.,  2.]), 'currentState': array([13.1319152 , 18.96448112,  0.        ]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 17.00220137292409}
done in step count: 15
reward sum = 0.7675664262264661
running average episode reward sum: 0.7240499042713128
{'currentTarget': array([12.,  2.]), 'previousTarget': array([12.,  2.]), 'currentState': array([11.64412541,  2.44359767,  0.        ]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 0.5687052138502993}
episode index:827
target Thresh 20.803599401830837
target distance 17.0
model initialize at round 827
at step 0:
{'currentTarget': array([22.,  4.]), 'previousTarget': array([22.,  4.]), 'currentState': array([20.17573714, 19.59083867,  0.        ]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 15.697203108508708}
done in step count: 12
reward sum = 0.7853644830876705
running average episode reward sum: 0.724123955695004
{'currentTarget': array([22.,  4.]), 'previousTarget': array([22.,  4.]), 'currentState': array([21.13978475,  4.81752634,  0.        ]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 1.186726421893411}
episode index:828
target Thresh 20.814790206094305
target distance 18.0
model initialize at round 828
at step 0:
{'currentTarget': array([17., 10.]), 'previousTarget': array([17., 10.]), 'currentState': array([14.09399068, 26.63059127,  0.        ]), 'targetState': array([17, 10], dtype=int32), 'currentDistance': 16.88257848465065}
done in step count: 13
reward sum = 0.7753333072540786
running average episode reward sum: 0.7241857281335553
{'currentTarget': array([17., 10.]), 'previousTarget': array([17., 10.]), 'currentState': array([16.24693123, 10.86382723,  0.        ]), 'targetState': array([17, 10], dtype=int32), 'currentDistance': 1.1459974037244631}
episode index:829
target Thresh 20.825969825147048
target distance 18.0
model initialize at round 829
at step 0:
{'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([10.44428957,  8.07139146,  0.        ]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 17.315038532401065}
done in step count: 13
reward sum = 0.7379147013128657
running average episode reward sum: 0.7242022690650967
{'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([26.33366954,  2.69317641,  0.        ]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 0.7335782114278244}
episode index:830
target Thresh 20.837138270168687
target distance 9.0
model initialize at round 830
at step 0:
{'currentTarget': array([ 6., 29.]), 'previousTarget': array([ 6., 29.]), 'currentState': array([13.61667395, 21.7020973 ,  0.        ]), 'targetState': array([ 6, 29], dtype=int32), 'currentDistance': 10.548606821381096}
done in step count: 7
reward sum = 0.8559306236090441
running average episode reward sum: 0.7243607869406009
{'currentTarget': array([ 6., 29.]), 'previousTarget': array([ 6., 29.]), 'currentState': array([ 6.60332382, 28.30573517,  0.        ]), 'targetState': array([ 6, 29], dtype=int32), 'currentDistance': 0.9197843668331038}
episode index:831
target Thresh 20.848295552327667
target distance 6.0
model initialize at round 831
at step 0:
{'currentTarget': array([18., 13.]), 'previousTarget': array([18., 13.]), 'currentState': array([13.9316842 ,  8.14411497,  0.        ]), 'targetState': array([18, 13], dtype=int32), 'currentDistance': 6.334888546329584}
done in step count: 4
reward sum = 0.9176670800684038
running average episode reward sum: 0.7245931262352256
{'currentTarget': array([18., 13.]), 'previousTarget': array([18., 13.]), 'currentState': array([17.01482522, 12.08412123,  0.        ]), 'targetState': array([18, 13], dtype=int32), 'currentDistance': 1.345140611195449}
episode index:832
target Thresh 20.859441682781267
target distance 13.0
model initialize at round 832
at step 0:
{'currentTarget': array([23.,  5.]), 'previousTarget': array([23.,  5.]), 'currentState': array([11.14483118, 13.45569432,  0.        ]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 14.561723597620489}
done in step count: 11
reward sum = 0.8018078535058508
running average episode reward sum: 0.7246858209858507
{'currentTarget': array([23.,  5.]), 'previousTarget': array([23.,  5.]), 'currentState': array([22.3423779 ,  5.49815792,  0.        ]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 0.8250019010339353}
episode index:833
target Thresh 20.870576672675625
target distance 12.0
model initialize at round 833
at step 0:
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([19.75490859, 21.86462402,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 11.859562002209836}
done in step count: 9
reward sum = 0.8416467694744411
running average episode reward sum: 0.7248260619312806
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.16489324, 11.83398521,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.8501300548251544}
episode index:834
target Thresh 20.881700533145725
target distance 16.0
model initialize at round 834
at step 0:
{'currentTarget': array([ 2., 20.]), 'previousTarget': array([ 2., 20.]), 'currentState': array([13.0854513 ,  4.97605759,  0.        ]), 'targetState': array([ 2, 20], dtype=int32), 'currentDistance': 18.670995582341604}
done in step count: 16
reward sum = 0.7520280626576303
running average episode reward sum: 0.7248586391776595
{'currentTarget': array([ 2., 20.]), 'previousTarget': array([ 2., 20.]), 'currentState': array([ 2.99158485, 19.05801475,  0.        ]), 'targetState': array([ 2, 20], dtype=int32), 'currentDistance': 1.3676902887730675}
episode index:835
target Thresh 20.892813275315433
target distance 16.0
model initialize at round 835
at step 0:
{'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([ 6.80348951, 22.34291708,  0.        ]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 20.896248507452825}
done in step count: 18
reward sum = 0.7243916634147713
running average episode reward sum: 0.724858080594211
{'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([21.11685473,  8.66317451,  0.        ]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 1.1044211169051514}
episode index:836
target Thresh 20.903914910297487
target distance 10.0
model initialize at round 836
at step 0:
{'currentTarget': array([11.,  8.]), 'previousTarget': array([11.,  8.]), 'currentState': array([ 7.28122097, 16.65071487,  0.        ]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 9.416166167340315}
done in step count: 8
reward sum = 0.8659222225053174
running average episode reward sum: 0.7250266160086806
{'currentTarget': array([11.,  8.]), 'previousTarget': array([11.,  8.]), 'currentState': array([10.33920461,  8.5348736 ,  0.        ]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 0.8501413537349207}
episode index:837
target Thresh 20.91500544919353
target distance 15.0
model initialize at round 837
at step 0:
{'currentTarget': array([ 4., 28.]), 'previousTarget': array([ 4., 28.]), 'currentState': array([17.28662837, 20.16102287,  0.        ]), 'targetState': array([ 4, 28], dtype=int32), 'currentDistance': 15.426731855478867}
done in step count: 10
reward sum = 0.781177583430517
running average episode reward sum: 0.7250936219363916
{'currentTarget': array([ 4., 28.]), 'previousTarget': array([ 4., 28.]), 'currentState': array([ 4.75779569, 27.32271864,  0.        ]), 'targetState': array([ 4, 28], dtype=int32), 'currentDistance': 1.016348541172683}
episode index:838
target Thresh 20.9260849030941
target distance 8.0
model initialize at round 838
at step 0:
{'currentTarget': array([11., 27.]), 'previousTarget': array([11., 27.]), 'currentState': array([11.71798047, 20.48283863,  0.        ]), 'targetState': array([11, 27], dtype=int32), 'currentDistance': 6.5565912084853455}
done in step count: 5
reward sum = 0.900453798185174
running average episode reward sum: 0.7253026328735176
{'currentTarget': array([11., 27.]), 'previousTarget': array([11., 27.]), 'currentState': array([10.93571837, 26.60013187,  0.        ]), 'targetState': array([11, 27], dtype=int32), 'currentDistance': 0.40500203769042503}
episode index:839
target Thresh 20.93715328307865
target distance 12.0
model initialize at round 839
at step 0:
{'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([13.18017361, 19.85589385,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 10.91985535751757}
done in step count: 9
reward sum = 0.8514308334446908
running average episode reward sum: 0.7254527854932452
{'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([11.51265554,  9.85226095,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.9817603294603922}
episode index:840
target Thresh 20.94821060021556
target distance 10.0
model initialize at round 840
at step 0:
{'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([22.88200617, 11.61576778,  0.        ]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 8.989223126731572}
done in step count: 7
reward sum = 0.8789942553099476
running average episode reward sum: 0.7256353556119334
{'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([14.98284411, 12.47423627,  0.        ]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 1.1146344918455862}
episode index:841
target Thresh 20.959256865562146
target distance 6.0
model initialize at round 841
at step 0:
{'currentTarget': array([19., 28.]), 'previousTarget': array([19., 28.]), 'currentState': array([23.63906401, 23.0397495 ,  0.        ]), 'targetState': array([19, 28], dtype=int32), 'currentDistance': 6.791538847510415}
done in step count: 5
reward sum = 0.9100800175251733
running average episode reward sum: 0.7258544110298826
{'currentTarget': array([19., 28.]), 'previousTarget': array([19., 28.]), 'currentState': array([19.57430559, 27.42217344,  0.        ]), 'targetState': array([19, 28], dtype=int32), 'currentDistance': 0.8146842628753559}
episode index:842
target Thresh 20.970292090164683
target distance 12.0
model initialize at round 842
at step 0:
{'currentTarget': array([11., 16.]), 'previousTarget': array([11., 16.]), 'currentState': array([21.62005818, 17.75462566,  0.        ]), 'targetState': array([11, 16], dtype=int32), 'currentDistance': 10.764030236908354}
done in step count: 8
reward sum = 0.853602130949821
running average episode reward sum: 0.7260059504366679
{'currentTarget': array([11., 16.]), 'previousTarget': array([11., 16.]), 'currentState': array([11.97272032, 15.71383874,  0.        ]), 'targetState': array([11, 16], dtype=int32), 'currentDistance': 1.01393939498436}
episode index:843
target Thresh 20.98131628505839
target distance 20.0
model initialize at round 843
at step 0:
{'currentTarget': array([19.,  8.]), 'previousTarget': array([19.,  8.]), 'currentState': array([25.13069509, 27.05872941,  0.        ]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 20.02050421475397}
done in step count: 20
reward sum = 0.7197120906070328
running average episode reward sum: 0.7259984932567749
{'currentTarget': array([19.,  8.]), 'previousTarget': array([19.,  8.]), 'currentState': array([19.22778163,  8.12067747,  0.        ]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 0.2577741714964814}
episode index:844
target Thresh 20.99232946126746
target distance 13.0
model initialize at round 844
at step 0:
{'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([18.1304844 , 15.89910817,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 12.088330697995703}
done in step count: 11
reward sum = 0.8333535138358924
running average episode reward sum: 0.726125540618407
{'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.0555689 ,  4.79692525,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.7988602829347798}
episode index:845
target Thresh 21.003331629805075
target distance 9.0
model initialize at round 845
at step 0:
{'currentTarget': array([20., 24.]), 'previousTarget': array([20., 24.]), 'currentState': array([22.66082653, 16.51930845,  0.        ]), 'targetState': array([20, 24], dtype=int32), 'currentDistance': 7.939820145494812}
done in step count: 5
reward sum = 0.8910354347172116
running average episode reward sum: 0.7263204695712424
{'currentTarget': array([20., 24.]), 'previousTarget': array([20., 24.]), 'currentState': array([20.6042363, 23.1936456,  0.       ]), 'targetState': array([20, 24], dtype=int32), 'currentDistance': 1.0076253946913307}
episode index:846
target Thresh 21.01432280167341
target distance 12.0
model initialize at round 846
at step 0:
{'currentTarget': array([ 3., 13.]), 'previousTarget': array([ 3., 13.]), 'currentState': array([12.34036994, 24.10363418,  0.        ]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 14.50976232008734}
done in step count: 11
reward sum = 0.8061240069895909
running average episode reward sum: 0.7264146886236843
{'currentTarget': array([ 3., 13.]), 'previousTarget': array([ 3., 13.]), 'currentState': array([ 3.91295841, 13.24029195,  0.        ]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 0.9440515278561076}
episode index:847
target Thresh 21.025302987863622
target distance 13.0
model initialize at round 847
at step 0:
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([13.83125627, 17.34054029,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 14.475608355546854}
done in step count: 12
reward sum = 0.7988642704352432
running average episode reward sum: 0.7265001244512923
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.73745322, 8.89664555, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.7446605926283817}
episode index:848
target Thresh 21.036272199355913
target distance 21.0
model initialize at round 848
at step 0:
{'currentTarget': array([27.,  6.]), 'previousTarget': array([27.,  6.]), 'currentState': array([23.39623916, 25.4199785 ,  0.        ]), 'targetState': array([27,  6], dtype=int32), 'currentDistance': 19.751522905812028}
done in step count: 15
reward sum = 0.7252149118085682
running average episode reward sum: 0.7264986106554824
{'currentTarget': array([27.,  6.]), 'previousTarget': array([27.,  6.]), 'currentState': array([26.53659091,  6.47642905,  0.        ]), 'targetState': array([27,  6], dtype=int32), 'currentDistance': 0.6646296839117943}
episode index:849
target Thresh 21.04723044711949
target distance 9.0
model initialize at round 849
at step 0:
{'currentTarget': array([21., 16.]), 'previousTarget': array([21., 16.]), 'currentState': array([25.8244139 ,  8.20052886,  0.        ]), 'targetState': array([21, 16], dtype=int32), 'currentDistance': 9.17097156785373}
done in step count: 8
reward sum = 0.8680643787545486
running average episode reward sum: 0.7266651586179518
{'currentTarget': array([21., 16.]), 'previousTarget': array([21., 16.]), 'currentState': array([21.41526628, 15.55549467,  0.        ]), 'targetState': array([21, 16], dtype=int32), 'currentDistance': 0.6083017929903377}
episode index:850
target Thresh 21.058177742112598
target distance 13.0
model initialize at round 850
at step 0:
{'currentTarget': array([11.,  6.]), 'previousTarget': array([11.,  6.]), 'currentState': array([17.46960765, 17.92614782,  0.        ]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 13.567933702343009}
done in step count: 11
reward sum = 0.8215608650267298
running average episode reward sum: 0.7267766694362935
{'currentTarget': array([11.,  6.]), 'previousTarget': array([11.,  6.]), 'currentState': array([11.57726398,  6.96972871,  0.        ]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 1.128542188630125}
episode index:851
target Thresh 21.069114095282536
target distance 20.0
model initialize at round 851
at step 0:
{'currentTarget': array([6., 7.]), 'previousTarget': array([6., 7.]), 'currentState': array([24.57519197, 16.63538799,  0.        ]), 'targetState': array([6, 7], dtype=int32), 'currentDistance': 20.92554559749364}
done in step count: 15
reward sum = 0.710754420224024
running average episode reward sum: 0.7267578639794716
{'currentTarget': array([6., 7.]), 'previousTarget': array([6., 7.]), 'currentState': array([6.588292  , 6.66434053, 0.        ]), 'targetState': array([6, 7], dtype=int32), 'currentDistance': 0.6773143753023103}
episode index:852
target Thresh 21.08003951756566
target distance 19.0
model initialize at round 852
at step 0:
{'currentTarget': array([12., 27.]), 'previousTarget': array([12., 27.]), 'currentState': array([22.83352129,  8.9475894 ,  0.        ]), 'targetState': array([12, 27], dtype=int32), 'currentDistance': 21.053615179812066}
done in step count: 21
reward sum = 0.7013708328716598
running average episode reward sum: 0.7267281019265903
{'currentTarget': array([12., 27.]), 'previousTarget': array([12., 27.]), 'currentState': array([12.47708803, 26.52119958,  0.        ]), 'targetState': array([12, 27], dtype=int32), 'currentDistance': 0.6759162851024427}
episode index:853
target Thresh 21.09095401988739
target distance 11.0
model initialize at round 853
at step 0:
{'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([21.23355573, 19.12648159,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 12.444675889935864}
done in step count: 11
reward sum = 0.8268000826424997
running average episode reward sum: 0.7268452822318783
{'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.69221553,  9.17554325,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.7141272820577051}
episode index:854
target Thresh 21.101857613162235
target distance 11.0
model initialize at round 854
at step 0:
{'currentTarget': array([20., 10.]), 'previousTarget': array([20., 10.]), 'currentState': array([10.31312895, 14.31363243,  0.        ]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 10.603909444324826}
done in step count: 8
reward sum = 0.8426945046279029
running average episode reward sum: 0.7269807783984233
{'currentTarget': array([20., 10.]), 'previousTarget': array([20., 10.]), 'currentState': array([19.40042314, 10.16837728,  0.        ]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 0.6227706803838059}
episode index:855
target Thresh 21.112750308293784
target distance 7.0
model initialize at round 855
at step 0:
{'currentTarget': array([18.,  6.]), 'previousTarget': array([18.,  6.]), 'currentState': array([23.70462942,  2.3996231 ,  0.        ]), 'targetState': array([18,  6], dtype=int32), 'currentDistance': 6.745777248363941}
done in step count: 4
reward sum = 0.9133859962850169
running average episode reward sum: 0.7271985415034311
{'currentTarget': array([18.,  6.]), 'previousTarget': array([18.,  6.]), 'currentState': array([18.92513955,  5.1674397 ,  0.        ]), 'targetState': array([18,  6], dtype=int32), 'currentDistance': 1.2446042884453183}
episode index:856
target Thresh 21.123632116174733
target distance 1.0
model initialize at round 856
at step 0:
{'currentTarget': array([22., 17.]), 'previousTarget': array([22., 17.]), 'currentState': array([21.4420945 , 17.31943035,  0.        ]), 'targetState': array([22, 17], dtype=int32), 'currentDistance': 0.6428796861325451}
done in step count: 0
reward sum = 0.9967068868619786
running average episode reward sum: 0.7275130203194854
{'currentTarget': array([22., 17.]), 'previousTarget': array([22., 17.]), 'currentState': array([21.4420945 , 17.31943035,  0.        ]), 'targetState': array([22, 17], dtype=int32), 'currentDistance': 0.6428796861325451}
episode index:857
target Thresh 21.134503047686888
target distance 10.0
model initialize at round 857
at step 0:
{'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 2.59644175, 19.70698047,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 10.247436296799512}
done in step count: 8
reward sum = 0.8576249763090935
running average episode reward sum: 0.7276646659558369
{'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.17374545, 11.5430671 ,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.9887458967864415}
episode index:858
target Thresh 21.145363113701187
target distance 9.0
model initialize at round 858
at step 0:
{'currentTarget': array([5., 8.]), 'previousTarget': array([5., 8.]), 'currentState': array([12.86198795,  8.8235521 ,  0.        ]), 'targetState': array([5, 8], dtype=int32), 'currentDistance': 7.9050042735931685}
done in step count: 7
reward sum = 0.8894348234379138
running average episode reward sum: 0.7278529897712992
{'currentTarget': array([5., 8.]), 'previousTarget': array([5., 8.]), 'currentState': array([5.79411501, 7.64148451, 0.        ]), 'targetState': array([5, 8], dtype=int32), 'currentDistance': 0.8712932935311395}
episode index:859
target Thresh 21.156212325077696
target distance 11.0
model initialize at round 859
at step 0:
{'currentTarget': array([22., 29.]), 'previousTarget': array([22., 29.]), 'currentState': array([15.59640598, 19.14989936,  0.        ]), 'targetState': array([22, 29], dtype=int32), 'currentDistance': 11.748638171454452}
done in step count: 8
reward sum = 0.8435211397700929
running average episode reward sum: 0.727987487620135
{'currentTarget': array([22., 29.]), 'previousTarget': array([22., 29.]), 'currentState': array([21.35654858, 28.1449092 ,  0.        ]), 'targetState': array([22, 29], dtype=int32), 'currentDistance': 1.070144851915367}
episode index:860
target Thresh 21.16705069266563
target distance 11.0
model initialize at round 860
at step 0:
{'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([11.25483775, 15.2859748 ,  0.        ]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 10.646021193057345}
done in step count: 7
reward sum = 0.8548767621544718
running average episode reward sum: 0.7281348619227301
{'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([20.01276356, 11.71305904,  0.        ]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 1.217821407427305}
episode index:861
target Thresh 21.177878227303353
target distance 3.0
model initialize at round 861
at step 0:
{'currentTarget': array([12.,  7.]), 'previousTarget': array([12.,  7.]), 'currentState': array([14.0848406 ,  7.55936053,  0.        ]), 'targetState': array([12,  7], dtype=int32), 'currentDistance': 2.1585746492801348}
done in step count: 2
reward sum = 0.9683784348863436
running average episode reward sum: 0.7284135667637551
{'currentTarget': array([12.,  7.]), 'previousTarget': array([12.,  7.]), 'currentState': array([12.72036409,  6.67500713,  0.        ]), 'targetState': array([12,  7], dtype=int32), 'currentDistance': 0.790281462593916}
episode index:862
target Thresh 21.188694939818397
target distance 11.0
model initialize at round 862
at step 0:
{'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([16.91222012,  9.91296616,  0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 9.954175753358749}
done in step count: 9
reward sum = 0.8599600505791577
running average episode reward sum: 0.7285659960613395
{'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([7.57544374, 8.59176873, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.7055411226589541}
episode index:863
target Thresh 21.199500841027483
target distance 11.0
model initialize at round 863
at step 0:
{'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([ 6.49038672, 12.4304978 ,  0.        ]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 9.638261348423725}
done in step count: 7
reward sum = 0.8632421840011537
running average episode reward sum: 0.7287218712788625
{'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([15.10247034, 13.53630931,  0.        ]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 1.0102319289100523}
episode index:864
target Thresh 21.210295941736508
target distance 10.0
model initialize at round 864
at step 0:
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([19.39346784, 18.93478143,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 10.436465619142641}
done in step count: 8
reward sum = 0.8573795551945417
running average episode reward sum: 0.7288706084857015
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.62298909, 10.40358698,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.7422922992551501}
episode index:865
target Thresh 21.221080252740578
target distance 10.0
model initialize at round 865
at step 0:
{'currentTarget': array([19.,  6.]), 'previousTarget': array([19.,  6.]), 'currentState': array([10.49965239,  4.3330605 ,  0.        ]), 'targetState': array([19,  6], dtype=int32), 'currentDistance': 8.66225125763372}
done in step count: 7
reward sum = 0.8710326544891136
running average episode reward sum: 0.7290347678921719
{'currentTarget': array([19.,  6.]), 'previousTarget': array([19.,  6.]), 'currentState': array([18.24016929,  5.89470396,  0.        ]), 'targetState': array([19,  6], dtype=int32), 'currentDistance': 0.7670918907547176}
episode index:866
target Thresh 21.231853784824
target distance 6.0
model initialize at round 866
at step 0:
{'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([15.56870973, 27.08635193,  0.        ]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 5.322764900632965}
done in step count: 5
reward sum = 0.9208939015814019
running average episode reward sum: 0.7292560587038089
{'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([13.92125046, 22.34905726,  0.        ]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 0.3578302105916354}
episode index:867
target Thresh 21.24261654876031
target distance 12.0
model initialize at round 867
at step 0:
{'currentTarget': array([9., 5.]), 'previousTarget': array([9., 5.]), 'currentState': array([10.88235199, 15.91185069,  0.        ]), 'targetState': array([9, 5], dtype=int32), 'currentDistance': 11.0730183116654}
done in step count: 9
reward sum = 0.8458973153029199
running average episode reward sum: 0.729390438031688
{'currentTarget': array([9., 5.]), 'previousTarget': array([9., 5.]), 'currentState': array([8.26018177, 5.57970655, 0.        ]), 'targetState': array([9, 5], dtype=int32), 'currentDistance': 0.93988866231221}
episode index:868
target Thresh 21.253368555312274
target distance 3.0
model initialize at round 868
at step 0:
{'currentTarget': array([10., 20.]), 'previousTarget': array([10., 20.]), 'currentState': array([ 8.27938262, 22.01643997,  0.        ]), 'targetState': array([10, 20], dtype=int32), 'currentDistance': 2.650764861533798}
done in step count: 2
reward sum = 0.967472954452842
running average episode reward sum: 0.7296644110080069
{'currentTarget': array([10., 20.]), 'previousTarget': array([10., 20.]), 'currentState': array([ 9.17781547, 20.55392557,  0.        ]), 'targetState': array([10, 20], dtype=int32), 'currentDistance': 0.991373263431489}
episode index:869
target Thresh 21.264109815231897
target distance 13.0
model initialize at round 869
at step 0:
{'currentTarget': array([18., 20.]), 'previousTarget': array([18., 20.]), 'currentState': array([23.49518359,  8.42916489,  0.        ]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 12.809421059518755}
done in step count: 9
reward sum = 0.8251644234934843
running average episode reward sum: 0.7297741811373005
{'currentTarget': array([18., 20.]), 'previousTarget': array([18., 20.]), 'currentState': array([18.79445755, 19.33374488,  0.        ]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 1.0368503684805679}
episode index:870
target Thresh 21.274840339260443
target distance 19.0
model initialize at round 870
at step 0:
{'currentTarget': array([ 2., 12.]), 'previousTarget': array([ 2., 12.]), 'currentState': array([19.53810787, 21.58853623,  0.        ]), 'targetState': array([ 2, 12], dtype=int32), 'currentDistance': 19.98812784670512}
done in step count: 14
reward sum = 0.7242333871232665
running average episode reward sum: 0.7297678197205221
{'currentTarget': array([ 2., 12.]), 'previousTarget': array([ 2., 12.]), 'currentState': array([ 2.80137724, 11.70699492,  0.        ]), 'targetState': array([ 2, 12], dtype=int32), 'currentDistance': 0.8532628270747764}
episode index:871
target Thresh 21.28556013812843
target distance 11.0
model initialize at round 871
at step 0:
{'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([ 6.42768741, 22.26352084,  0.        ]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 9.655343250864707}
done in step count: 7
reward sum = 0.859000173451674
running average episode reward sum: 0.7299160219610396
{'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([15.01826268, 20.74831987,  0.        ]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 1.0134846046774988}
episode index:872
target Thresh 21.296269222555665
target distance 7.0
model initialize at round 872
at step 0:
{'currentTarget': array([26., 29.]), 'previousTarget': array([26., 29.]), 'currentState': array([20.46848273, 28.00960291,  0.        ]), 'targetState': array([26, 29], dtype=int32), 'currentDistance': 5.6194812632626245}
done in step count: 5
reward sum = 0.9108596773781598
running average episode reward sum: 0.7301232884620901
{'currentTarget': array([26., 29.]), 'previousTarget': array([26., 29.]), 'currentState': array([25.39448842, 28.75249163,  0.        ]), 'targetState': array([26, 29], dtype=int32), 'currentDistance': 0.6541442203739336}
episode index:873
target Thresh 21.30696760325123
target distance 17.0
model initialize at round 873
at step 0:
{'currentTarget': array([16., 25.]), 'previousTarget': array([16., 25.]), 'currentState': array([6.57917327, 8.7796973 , 0.        ]), 'targetState': array([16, 25], dtype=int32), 'currentDistance': 18.757670325690142}
done in step count: 14
reward sum = 0.7594880684328044
running average episode reward sum: 0.7301568866085096
{'currentTarget': array([16., 25.]), 'previousTarget': array([16., 25.]), 'currentState': array([15.12541401, 24.05595881,  0.        ]), 'targetState': array([16, 25], dtype=int32), 'currentDistance': 1.2869010919039867}
episode index:874
target Thresh 21.317655290913507
target distance 8.0
model initialize at round 874
at step 0:
{'currentTarget': array([ 3., 18.]), 'previousTarget': array([ 3., 18.]), 'currentState': array([ 9.98157287, 25.22194785,  0.        ]), 'targetState': array([ 3, 18], dtype=int32), 'currentDistance': 10.044843971916643}
done in step count: 8
reward sum = 0.8619176192782388
running average episode reward sum: 0.7303074703029894
{'currentTarget': array([ 3., 18.]), 'previousTarget': array([ 3., 18.]), 'currentState': array([ 3.80017024, 18.34106809,  0.        ]), 'targetState': array([ 3, 18], dtype=int32), 'currentDistance': 0.8698274879426516}
episode index:875
target Thresh 21.328332296230187
target distance 6.0
model initialize at round 875
at step 0:
{'currentTarget': array([ 8., 21.]), 'previousTarget': array([ 8., 21.]), 'currentState': array([ 3.22449207, 17.01432943,  0.        ]), 'targetState': array([ 8, 21], dtype=int32), 'currentDistance': 6.220212683293832}
done in step count: 5
reward sum = 0.9041841417715021
running average episode reward sum: 0.7305059596539807
{'currentTarget': array([ 8., 21.]), 'previousTarget': array([ 8., 21.]), 'currentState': array([ 7.31217369, 20.71667966,  0.        ]), 'targetState': array([ 8, 21], dtype=int32), 'currentDistance': 0.7438920891997023}
episode index:876
target Thresh 21.33899862987827
target distance 13.0
model initialize at round 876
at step 0:
{'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([22.59518981, 22.12156087,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 13.799592986433039}
done in step count: 13
reward sum = 0.8066664129317348
running average episode reward sum: 0.7305928016759622
{'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.20392191, 10.20934528,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.2922491980818094}
episode index:877
target Thresh 21.349654302524094
target distance 8.0
model initialize at round 877
at step 0:
{'currentTarget': array([ 2., 28.]), 'previousTarget': array([ 2., 28.]), 'currentState': array([ 8.60507369, 24.13005307,  0.        ]), 'targetState': array([ 2, 28], dtype=int32), 'currentDistance': 7.65529148186423}
done in step count: 5
reward sum = 0.897243979901925
running average episode reward sum: 0.7307826093960372
{'currentTarget': array([ 2., 28.]), 'previousTarget': array([ 2., 28.]), 'currentState': array([ 2.83966196, 27.15912458,  0.        ]), 'targetState': array([ 2, 28], dtype=int32), 'currentDistance': 1.1883196821472217}
episode index:878
target Thresh 21.360299324823337
target distance 14.0
model initialize at round 878
at step 0:
{'currentTarget': array([26., 13.]), 'previousTarget': array([26., 13.]), 'currentState': array([13.46174455, 14.20836896,  0.        ]), 'targetState': array([26, 13], dtype=int32), 'currentDistance': 12.596348889411596}
done in step count: 10
reward sum = 0.8083534504647292
running average episode reward sum: 0.7308708583619857
{'currentTarget': array([26., 13.]), 'previousTarget': array([26., 13.]), 'currentState': array([25.33449486, 12.82221937,  0.        ]), 'targetState': array([26, 13], dtype=int32), 'currentDistance': 0.6888418147973774}
episode index:879
target Thresh 21.370933707421017
target distance 19.0
model initialize at round 879
at step 0:
{'currentTarget': array([ 2., 17.]), 'previousTarget': array([ 2., 17.]), 'currentState': array([19.72962475, 27.49323511,  0.        ]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 20.602125541136203}
done in step count: 15
reward sum = 0.7191829427280074
running average episode reward sum: 0.7308575766396744
{'currentTarget': array([ 2., 17.]), 'previousTarget': array([ 2., 17.]), 'currentState': array([ 2.75826776, 16.82816407,  0.        ]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 0.7774944262330898}
episode index:880
target Thresh 21.381557460951516
target distance 20.0
model initialize at round 880
at step 0:
{'currentTarget': array([25., 26.]), 'previousTarget': array([25., 26.]), 'currentState': array([21.96854813,  7.0432179 ,  0.        ]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 19.197637566797482}
done in step count: 14
reward sum = 0.7469933636182914
running average episode reward sum: 0.7308758919483903
{'currentTarget': array([25., 26.]), 'previousTarget': array([25., 26.]), 'currentState': array([24.59202939, 25.28863037,  0.        ]), 'targetState': array([25, 26], dtype=int32), 'currentDistance': 0.8200529118289768}
episode index:881
target Thresh 21.392170596038593
target distance 11.0
model initialize at round 881
at step 0:
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([ 7.57520658, 20.57525444,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 11.005175203985377}
done in step count: 9
reward sum = 0.8415648241472087
running average episode reward sum: 0.7310013896039445
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.44422677, 11.46332955,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.7235731868001241}
episode index:882
target Thresh 21.402773123295383
target distance 3.0
model initialize at round 882
at step 0:
{'currentTarget': array([19., 11.]), 'previousTarget': array([19., 11.]), 'currentState': array([18.11051272, 13.00099254,  0.        ]), 'targetState': array([19, 11], dtype=int32), 'currentDistance': 2.189785093371986}
done in step count: 2
reward sum = 0.9677782496985395
running average episode reward sum: 0.7312695400683777
{'currentTarget': array([19., 11.]), 'previousTarget': array([19., 11.]), 'currentState': array([18.57575959, 11.36273187,  0.        ]), 'targetState': array([19, 11], dtype=int32), 'currentDistance': 0.5581705278523227}
episode index:883
target Thresh 21.413365053324412
target distance 14.0
model initialize at round 883
at step 0:
{'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([4.33200085, 5.50460565, 0.        ]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 12.757038864399501}
done in step count: 10
reward sum = 0.8160562103174989
running average episode reward sum: 0.7313654525912839
{'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([16.21803677,  3.66781425,  0.        ]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 0.8495962959422183}
episode index:884
target Thresh 21.42394639671761
target distance 20.0
model initialize at round 884
at step 0:
{'currentTarget': array([ 3., 27.]), 'previousTarget': array([ 3., 27.]), 'currentState': array([5.72321424, 8.30612946, 0.        ]), 'targetState': array([ 3, 27], dtype=int32), 'currentDistance': 18.89118026283412}
done in step count: 14
reward sum = 0.7430888233758467
running average episode reward sum: 0.7313786993379332
{'currentTarget': array([ 3., 27.]), 'previousTarget': array([ 3., 27.]), 'currentState': array([ 2.70456995, 26.59064204,  0.        ]), 'targetState': array([ 3, 27], dtype=int32), 'currentDistance': 0.504829533141839}
episode index:885
target Thresh 21.43451716405633
target distance 16.0
model initialize at round 885
at step 0:
{'currentTarget': array([22., 20.]), 'previousTarget': array([22., 20.]), 'currentState': array([ 7.20093751, 26.65362158,  0.        ]), 'targetState': array([22, 20], dtype=int32), 'currentDistance': 16.225995525146153}
done in step count: 12
reward sum = 0.7757914906866856
running average episode reward sum: 0.7314288266419385
{'currentTarget': array([22., 20.]), 'previousTarget': array([22., 20.]), 'currentState': array([21.39856791, 20.37077159,  0.        ]), 'targetState': array([22, 20], dtype=int32), 'currentDistance': 0.7065352946323251}
episode index:886
target Thresh 21.445077365911327
target distance 4.0
model initialize at round 886
at step 0:
{'currentTarget': array([13., 17.]), 'previousTarget': array([13., 17.]), 'currentState': array([12.66233164, 19.86008918,  0.        ]), 'targetState': array([13, 17], dtype=int32), 'currentDistance': 2.8799531341213975}
done in step count: 2
reward sum = 0.9628600667712685
running average episode reward sum: 0.7316897412305848
{'currentTarget': array([13., 17.]), 'previousTarget': array([13., 17.]), 'currentState': array([12.4797655 , 17.85268748,  0.        ]), 'targetState': array([13, 17], dtype=int32), 'currentDistance': 0.9988592830790994}
episode index:887
target Thresh 21.45562701284281
target distance 21.0
model initialize at round 887
at step 0:
{'currentTarget': array([17.,  7.]), 'previousTarget': array([17.,  7.]), 'currentState': array([17.32082564, 26.58694232,  0.        ]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 19.589569631415774}
done in step count: 17
reward sum = 0.7283677676015128
running average episode reward sum: 0.7316860002692909
{'currentTarget': array([17.,  7.]), 'previousTarget': array([17.,  7.]), 'currentState': array([16.5013103 ,  7.45254844,  0.        ]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 0.6734177845961763}
episode index:888
target Thresh 21.466166115400434
target distance 18.0
model initialize at round 888
at step 0:
{'currentTarget': array([15., 29.]), 'previousTarget': array([15., 29.]), 'currentState': array([ 7.43112934, 11.73219317,  0.        ]), 'targetState': array([15, 29], dtype=int32), 'currentDistance': 18.853778289054812}
done in step count: 15
reward sum = 0.7563097497834698
running average episode reward sum: 0.7317136985252123
{'currentTarget': array([15., 29.]), 'previousTarget': array([15., 29.]), 'currentState': array([14.41166958, 28.05924523,  0.        ]), 'targetState': array([15, 29], dtype=int32), 'currentDistance': 1.1095729900236946}
episode index:889
target Thresh 21.476694684123288
target distance 11.0
model initialize at round 889
at step 0:
{'currentTarget': array([ 3., 17.]), 'previousTarget': array([ 3., 17.]), 'currentState': array([12.82808757, 26.16805106,  0.        ]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 13.440404218706293}
done in step count: 10
reward sum = 0.8156333928747013
running average episode reward sum: 0.7318079903166161
{'currentTarget': array([ 3., 17.]), 'previousTarget': array([ 3., 17.]), 'currentState': array([ 3.79837745, 17.31427127,  0.        ]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 0.8580052397412175}
episode index:890
target Thresh 21.487212729539955
target distance 17.0
model initialize at round 890
at step 0:
{'currentTarget': array([9., 8.]), 'previousTarget': array([9., 8.]), 'currentState': array([24.71344185, 10.86233507,  0.        ]), 'targetState': array([9, 8], dtype=int32), 'currentDistance': 15.972013548475806}
done in step count: 13
reward sum = 0.7835670444797632
running average episode reward sum: 0.7318660812864963
{'currentTarget': array([9., 8.]), 'previousTarget': array([9., 8.]), 'currentState': array([9.78720099, 7.54993068, 0.        ]), 'targetState': array([9, 8], dtype=int32), 'currentDistance': 0.9067787989014299}
episode index:891
target Thresh 21.49772026216847
target distance 12.0
model initialize at round 891
at step 0:
{'currentTarget': array([ 3., 15.]), 'previousTarget': array([ 3., 15.]), 'currentState': array([ 1.88178558, 25.58994222,  0.        ]), 'targetState': array([ 3, 15], dtype=int32), 'currentDistance': 10.64881588008682}
done in step count: 8
reward sum = 0.8541817789497756
running average episode reward sum: 0.7320032065080919
{'currentTarget': array([ 3., 15.]), 'previousTarget': array([ 3., 15.]), 'currentState': array([ 2.0061695 , 15.93661487,  0.        ]), 'targetState': array([ 3, 15], dtype=int32), 'currentDistance': 1.3656304317023118}
episode index:892
target Thresh 21.508217292516377
target distance 12.0
model initialize at round 892
at step 0:
{'currentTarget': array([19., 21.]), 'previousTarget': array([19., 21.]), 'currentState': array([23.85509881, 10.30328345,  0.        ]), 'targetState': array([19, 21], dtype=int32), 'currentDistance': 11.746988098173194}
done in step count: 9
reward sum = 0.8436870670585683
running average episode reward sum: 0.7321282724213622
{'currentTarget': array([19., 21.]), 'previousTarget': array([19., 21.]), 'currentState': array([19.86069649, 20.19187695,  0.        ]), 'targetState': array([19, 21], dtype=int32), 'currentDistance': 1.1806190423455807}
episode index:893
target Thresh 21.5187038310807
target distance 17.0
model initialize at round 893
at step 0:
{'currentTarget': array([25., 27.]), 'previousTarget': array([25., 27.]), 'currentState': array([ 8.83617628, 17.3535648 ,  0.        ]), 'targetState': array([25, 27], dtype=int32), 'currentDistance': 18.823466988421394}
done in step count: 15
reward sum = 0.7480704261635946
running average episode reward sum: 0.7321461048080986
{'currentTarget': array([25., 27.]), 'previousTarget': array([25., 27.]), 'currentState': array([24.31653324, 26.8431327 ,  0.        ]), 'targetState': array([25, 27], dtype=int32), 'currentDistance': 0.7012375942387784}
episode index:894
target Thresh 21.529179888347983
target distance 7.0
model initialize at round 894
at step 0:
{'currentTarget': array([11., 18.]), 'previousTarget': array([11., 18.]), 'currentState': array([ 5.5969013 , 23.79551673,  0.        ]), 'targetState': array([11, 18], dtype=int32), 'currentDistance': 7.923477124876459}
done in step count: 6
reward sum = 0.8950717458896105
running average episode reward sum: 0.732328144630536
{'currentTarget': array([11., 18.]), 'previousTarget': array([11., 18.]), 'currentState': array([10.01911628, 18.78402096,  0.        ]), 'targetState': array([11, 18], dtype=int32), 'currentDistance': 1.255715626074295}
episode index:895
target Thresh 21.539645474794277
target distance 2.0
model initialize at round 895
at step 0:
{'currentTarget': array([ 3., 24.]), 'previousTarget': array([ 3., 24.]), 'currentState': array([ 4.20127094, 23.67352185,  0.        ]), 'targetState': array([ 3, 24], dtype=int32), 'currentDistance': 1.2448453120695973}
done in step count: 1
reward sum = 0.984370615018376
running average episode reward sum: 0.7326094420305225
{'currentTarget': array([ 3., 24.]), 'previousTarget': array([ 3., 24.]), 'currentState': array([ 3.58903629, 23.57182959,  0.        ]), 'targetState': array([ 3, 24], dtype=int32), 'currentDistance': 0.728212639590482}
episode index:896
target Thresh 21.55010060088518
target distance 20.0
model initialize at round 896
at step 0:
{'currentTarget': array([8., 8.]), 'previousTarget': array([8., 8.]), 'currentState': array([10.29616052, 26.79146969,  0.        ]), 'targetState': array([8, 8], dtype=int32), 'currentDistance': 18.93123573249862}
done in step count: 16
reward sum = 0.7401967892474585
running average episode reward sum: 0.7326179006115893
{'currentTarget': array([8., 8.]), 'previousTarget': array([8., 8.]), 'currentState': array([7.69960032, 8.66556185, 0.        ]), 'targetState': array([8, 8], dtype=int32), 'currentDistance': 0.7302140456148227}
episode index:897
target Thresh 21.56054527707581
target distance 10.0
model initialize at round 897
at step 0:
{'currentTarget': array([10., 24.]), 'previousTarget': array([10., 24.]), 'currentState': array([18.61187816, 28.32219195,  0.        ]), 'targetState': array([10, 24], dtype=int32), 'currentDistance': 9.635651958714005}
done in step count: 8
reward sum = 0.85569111680606
running average episode reward sum: 0.7327549531908704
{'currentTarget': array([10., 24.]), 'previousTarget': array([10., 24.]), 'currentState': array([10.65332356, 23.72074053,  0.        ]), 'targetState': array([10, 24], dtype=int32), 'currentDistance': 0.7105051221523477}
episode index:898
target Thresh 21.570979513810848
target distance 11.0
model initialize at round 898
at step 0:
{'currentTarget': array([21., 21.]), 'previousTarget': array([21., 21.]), 'currentState': array([11.42199016, 18.45581257,  0.        ]), 'targetState': array([21, 21], dtype=int32), 'currentDistance': 9.910154500810506}
done in step count: 8
reward sum = 0.8538109448044746
running average episode reward sum: 0.7328896094663025
{'currentTarget': array([21., 21.]), 'previousTarget': array([21., 21.]), 'currentState': array([20.37738669, 20.78797777,  0.        ]), 'targetState': array([21, 21], dtype=int32), 'currentDistance': 0.6577239256778028}
episode index:899
target Thresh 21.581403321524526
target distance 15.0
model initialize at round 899
at step 0:
{'currentTarget': array([18., 23.]), 'previousTarget': array([18., 23.]), 'currentState': array([5.56106532, 9.11410284, 0.        ]), 'targetState': array([18, 23], dtype=int32), 'currentDistance': 18.642565166686882}
done in step count: 15
reward sum = 0.7477078411443266
running average episode reward sum: 0.7329060741681671
{'currentTarget': array([18., 23.]), 'previousTarget': array([18., 23.]), 'currentState': array([17.2020542 , 22.66204172,  0.        ]), 'targetState': array([18, 23], dtype=int32), 'currentDistance': 0.8665640727373016}
episode index:900
target Thresh 21.591816710640664
target distance 21.0
model initialize at round 900
at step 0:
{'currentTarget': array([17.,  6.]), 'previousTarget': array([17.,  6.]), 'currentState': array([22.33559531, 26.21871608,  0.        ]), 'targetState': array([17,  6], dtype=int32), 'currentDistance': 20.91088370761424}
done in step count: 20
reward sum = 0.7124360733981245
running average episode reward sum: 0.7328833549664245
{'currentTarget': array([17.,  6.]), 'previousTarget': array([17.,  6.]), 'currentState': array([17.78516726,  6.67379671,  0.        ]), 'targetState': array([17,  6], dtype=int32), 'currentDistance': 1.0346446913069227}
episode index:901
target Thresh 21.602219691572643
target distance 2.0
model initialize at round 901
at step 0:
{'currentTarget': array([17.,  7.]), 'previousTarget': array([17.,  7.]), 'currentState': array([18.29479599,  5.84212047,  0.        ]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 1.737003645618351}
done in step count: 1
reward sum = 0.9803573521134146
running average episode reward sum: 0.7331577163823303
{'currentTarget': array([17.,  7.]), 'previousTarget': array([17.,  7.]), 'currentState': array([17.70769203,  6.46232384,  0.        ]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 0.8887764924180914}
episode index:902
target Thresh 21.612612274723446
target distance 8.0
model initialize at round 902
at step 0:
{'currentTarget': array([18., 16.]), 'previousTarget': array([18., 16.]), 'currentState': array([24.86468863, 18.58789599,  0.        ]), 'targetState': array([18, 16], dtype=int32), 'currentDistance': 7.336290323166939}
done in step count: 7
reward sum = 0.8946060780881329
running average episode reward sum: 0.7333365074805649
{'currentTarget': array([18., 16.]), 'previousTarget': array([18., 16.]), 'currentState': array([18.94884163, 16.17673624,  0.        ]), 'targetState': array([18, 16], dtype=int32), 'currentDistance': 0.9651611981853135}
episode index:903
target Thresh 21.622994470485658
target distance 18.0
model initialize at round 903
at step 0:
{'currentTarget': array([19.,  9.]), 'previousTarget': array([19.,  9.]), 'currentState': array([22.17769971, 26.50696564,  0.        ]), 'targetState': array([19,  9], dtype=int32), 'currentDistance': 17.793021702353276}
done in step count: 18
reward sum = 0.7496348058298521
running average episode reward sum: 0.7333545365716592
{'currentTarget': array([19.,  9.]), 'previousTarget': array([19.,  9.]), 'currentState': array([19.27052391,  9.46317548,  0.        ]), 'targetState': array([19,  9], dtype=int32), 'currentDistance': 0.5363904424904594}
episode index:904
target Thresh 21.633366289241472
target distance 11.0
model initialize at round 904
at step 0:
{'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([19.66418636, 24.9394803 ,  0.        ]), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 10.979430910308913}
done in step count: 9
reward sum = 0.8478707786213443
running average episode reward sum: 0.733481073855692
{'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.00417869, 15.51458251,  0.        ]), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.5145994810848885}
episode index:905
target Thresh 21.643727741362717
target distance 9.0
model initialize at round 905
at step 0:
{'currentTarget': array([ 4., 20.]), 'previousTarget': array([ 4., 20.]), 'currentState': array([11.66783452, 16.04706966,  0.        ]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 8.62678065640826}
done in step count: 6
reward sum = 0.8790900237824165
running average episode reward sum: 0.7336417901359643
{'currentTarget': array([ 4., 20.]), 'previousTarget': array([ 4., 20.]), 'currentState': array([ 4.56519985, 19.31096043,  0.        ]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 0.8911938036604391}
episode index:906
target Thresh 21.65407883721084
target distance 11.0
model initialize at round 906
at step 0:
{'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([12.91020131, 12.05180037,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 13.432437533344915}
done in step count: 10
reward sum = 0.817709723369874
running average episode reward sum: 0.7337344780447116
{'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.78940836, 2.53548092, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.9538895975854617}
episode index:907
target Thresh 21.664419587136937
target distance 4.0
model initialize at round 907
at step 0:
{'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([9.15803766, 6.21098638, 0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 3.981877311477446}
done in step count: 3
reward sum = 0.9419304445586922
running average episode reward sum: 0.7339637687567314
{'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([11.02908409,  8.37091866,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 1.1569014832200966}
episode index:908
target Thresh 21.674750001481762
target distance 17.0
model initialize at round 908
at step 0:
{'currentTarget': array([25., 28.]), 'previousTarget': array([25., 28.]), 'currentState': array([ 9.60510981, 26.38904557,  0.        ]), 'targetState': array([25, 28], dtype=int32), 'currentDistance': 15.478947578050025}
done in step count: 11
reward sum = 0.7810090022851018
running average episode reward sum: 0.7340155236891059
{'currentTarget': array([25., 28.]), 'previousTarget': array([25., 28.]), 'currentState': array([24.09734803, 27.76773076,  0.        ]), 'targetState': array([25, 28], dtype=int32), 'currentDistance': 0.932056635851743}
episode index:909
target Thresh 21.685070090575724
target distance 21.0
model initialize at round 909
at step 0:
{'currentTarget': array([17., 29.]), 'previousTarget': array([17., 29.]), 'currentState': array([10.30738407,  8.82532942,  0.        ]), 'targetState': array([17, 29], dtype=int32), 'currentDistance': 21.255786055306512}
done in step count: 18
reward sum = 0.7190626264016574
running average episode reward sum: 0.7339990919338449
{'currentTarget': array([17., 29.]), 'previousTarget': array([17., 29.]), 'currentState': array([16.43991434, 28.38403839,  0.        ]), 'targetState': array([17, 29], dtype=int32), 'currentDistance': 0.8325290706316131}
episode index:910
target Thresh 21.695379864738918
target distance 12.0
model initialize at round 910
at step 0:
{'currentTarget': array([15., 17.]), 'previousTarget': array([15., 17.]), 'currentState': array([3.73200154, 9.08260667, 0.        ]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 13.771452585568216}
done in step count: 11
reward sum = 0.8047466032670004
running average episode reward sum: 0.7340767511120372
{'currentTarget': array([15., 17.]), 'previousTarget': array([15., 17.]), 'currentState': array([14.39387843, 16.80719054,  0.        ]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 0.6360494055588688}
episode index:911
target Thresh 21.705679334281122
target distance 2.0
model initialize at round 911
at step 0:
{'currentTarget': array([ 4., 24.]), 'previousTarget': array([ 4., 24.]), 'currentState': array([ 5.17809772, 23.60308874,  0.        ]), 'targetState': array([ 4, 24], dtype=int32), 'currentDistance': 1.2431624192014163}
done in step count: 1
reward sum = 0.9840334204223008
running average episode reward sum: 0.7343508264073335
{'currentTarget': array([ 4., 24.]), 'previousTarget': array([ 4., 24.]), 'currentState': array([ 4.58455032, 23.49541123,  0.        ]), 'targetState': array([ 4, 24], dtype=int32), 'currentDistance': 0.7722104004591925}
episode index:912
target Thresh 21.7159685095018
target distance 19.0
model initialize at round 912
at step 0:
{'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([ 5.17248619, 26.52679667,  0.        ]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 25.02813249056569}
done in step count: 20
reward sum = 0.6687643257764211
running average episode reward sum: 0.7342789901525352
{'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([21.26394263,  8.40546104,  0.        ]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 0.8403446380779382}
episode index:913
target Thresh 21.726247400690127
target distance 5.0
model initialize at round 913
at step 0:
{'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([18.89223063,  4.01317985,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 4.015379595313496}
done in step count: 3
reward sum = 0.9461952219482026
running average episode reward sum: 0.7345108459860097
{'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.69413429,  4.32377063,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.9690761470767808}
episode index:914
target Thresh 21.736516018125002
target distance 11.0
model initialize at round 914
at step 0:
{'currentTarget': array([10.,  3.]), 'previousTarget': array([10.,  3.]), 'currentState': array([19.81152916,  8.16039258,  0.        ]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 11.08583583016082}
done in step count: 9
reward sum = 0.8354788256286876
running average episode reward sum: 0.7346211935047449
{'currentTarget': array([10.,  3.]), 'previousTarget': array([10.,  3.]), 'currentState': array([10.51807266,  2.63079374,  0.        ]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 0.6361702223298413}
episode index:915
target Thresh 21.746774372075038
target distance 13.0
model initialize at round 915
at step 0:
{'currentTarget': array([ 2., 28.]), 'previousTarget': array([ 2., 28.]), 'currentState': array([13.82292235, 17.29203594,  0.        ]), 'targetState': array([ 2, 28], dtype=int32), 'currentDistance': 15.951237796415521}
done in step count: 13
reward sum = 0.7882141254146849
running average episode reward sum: 0.7346797010723322
{'currentTarget': array([ 2., 28.]), 'previousTarget': array([ 2., 28.]), 'currentState': array([ 2.65719974, 27.45207351,  0.        ]), 'targetState': array([ 2, 28], dtype=int32), 'currentDistance': 0.8556488370527389}
episode index:916
target Thresh 21.757022472798592
target distance 19.0
model initialize at round 916
at step 0:
{'currentTarget': array([22.,  5.]), 'previousTarget': array([22.,  5.]), 'currentState': array([ 4.60629511, 12.37161955,  0.        ]), 'targetState': array([22,  5], dtype=int32), 'currentDistance': 18.89131400054704}
done in step count: 14
reward sum = 0.7240760937062753
running average episode reward sum: 0.7346681377055206
{'currentTarget': array([22.,  5.]), 'previousTarget': array([22.,  5.]), 'currentState': array([21.40311617,  5.21119381,  0.        ]), 'targetState': array([22,  5], dtype=int32), 'currentDistance': 0.6331454294854062}
episode index:917
target Thresh 21.767260330543767
target distance 16.0
model initialize at round 917
at step 0:
{'currentTarget': array([13., 29.]), 'previousTarget': array([13., 29.]), 'currentState': array([12.59661907, 14.05934381,  0.        ]), 'targetState': array([13, 29], dtype=int32), 'currentDistance': 14.946100609118542}
done in step count: 11
reward sum = 0.8015691462227639
running average episode reward sum: 0.7347410146211167
{'currentTarget': array([13., 29.]), 'previousTarget': array([13., 29.]), 'currentState': array([12.61118398, 28.12373102,  0.        ]), 'targetState': array([13, 29], dtype=int32), 'currentDistance': 0.9586580354087774}
episode index:918
target Thresh 21.777487955548416
target distance 11.0
model initialize at round 918
at step 0:
{'currentTarget': array([19., 27.]), 'previousTarget': array([19., 27.]), 'currentState': array([ 9.43169117, 26.44736272,  0.        ]), 'targetState': array([19, 27], dtype=int32), 'currentDistance': 9.584254891927234}
done in step count: 9
reward sum = 0.8472636777627549
running average episode reward sum: 0.734863454950977
{'currentTarget': array([19., 27.]), 'previousTarget': array([19., 27.]), 'currentState': array([18.40187213, 26.80142292,  0.        ]), 'targetState': array([19, 27], dtype=int32), 'currentDistance': 0.6302299629152514}
episode index:919
target Thresh 21.787705358040174
target distance 6.0
model initialize at round 919
at step 0:
{'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([15.47872245,  3.8668264 ,  0.        ]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 4.661119292972206}
done in step count: 4
reward sum = 0.9278094333582363
running average episode reward sum: 0.7350731788405501
{'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([19.31496984,  4.93536938,  0.        ]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 0.688072263852479}
episode index:920
target Thresh 21.79791254823644
target distance 10.0
model initialize at round 920
at step 0:
{'currentTarget': array([10., 15.]), 'previousTarget': array([10., 15.]), 'currentState': array([ 6.46455169, 23.67686069,  0.        ]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 9.369488044065495}
done in step count: 8
reward sum = 0.8643829948206709
running average episode reward sum: 0.7352135803779878
{'currentTarget': array([10., 15.]), 'previousTarget': array([10., 15.]), 'currentState': array([ 9.34012413, 15.34225428,  0.        ]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 0.7433533186664412}
episode index:921
target Thresh 21.808109536344396
target distance 11.0
model initialize at round 921
at step 0:
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([ 3.91026729, 11.18544251,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 12.992466091136532}
done in step count: 9
reward sum = 0.8221740663084633
running average episode reward sum: 0.7353078976078473
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.19042665,  3.32973215,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.874146614390984}
episode index:922
target Thresh 21.818296332561047
target distance 2.0
model initialize at round 922
at step 0:
{'currentTarget': array([10., 23.]), 'previousTarget': array([10., 23.]), 'currentState': array([10.44406837, 22.12639761,  0.        ]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 0.9799887017470063}
done in step count: 0
reward sum = 0.9921108430969181
running average episode reward sum: 0.7355861239843252
{'currentTarget': array([10., 23.]), 'previousTarget': array([10., 23.]), 'currentState': array([10.44406837, 22.12639761,  0.        ]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 0.9799887017470063}
episode index:923
target Thresh 21.828472947073177
target distance 8.0
model initialize at round 923
at step 0:
{'currentTarget': array([ 5., 13.]), 'previousTarget': array([ 5., 13.]), 'currentState': array([ 3.78159817, 19.72060752,  0.        ]), 'targetState': array([ 5, 13], dtype=int32), 'currentDistance': 6.830158743136478}
done in step count: 6
reward sum = 0.9005146747584832
running average episode reward sum: 0.7357646180868946
{'currentTarget': array([ 5., 13.]), 'previousTarget': array([ 5., 13.]), 'currentState': array([ 4.33897942, 13.62599051,  0.        ]), 'targetState': array([ 5, 13], dtype=int32), 'currentDistance': 0.9103913014469179}
episode index:924
target Thresh 21.83863939005741
target distance 5.0
model initialize at round 924
at step 0:
{'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([7.08692718, 6.40028071, 0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 5.316965099322875}
done in step count: 4
reward sum = 0.9156484550174948
running average episode reward sum: 0.7359590870997925
{'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([10.39930177,  9.62447488,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.7084190004142756}
episode index:925
target Thresh 21.848795671680183
target distance 13.0
model initialize at round 925
at step 0:
{'currentTarget': array([19.,  2.]), 'previousTarget': array([19.,  2.]), 'currentState': array([16.0899452 , 13.56149864,  0.        ]), 'targetState': array([19,  2], dtype=int32), 'currentDistance': 11.922108444428407}
done in step count: 10
reward sum = 0.8252253464274069
running average episode reward sum: 0.7360554869478785
{'currentTarget': array([19.,  2.]), 'previousTarget': array([19.,  2.]), 'currentState': array([18.2459942 ,  2.29669544,  0.        ]), 'targetState': array([19,  2], dtype=int32), 'currentDistance': 0.8102795432272343}
episode index:926
target Thresh 21.85894180209778
target distance 11.0
model initialize at round 926
at step 0:
{'currentTarget': array([ 8., 28.]), 'previousTarget': array([ 8., 28.]), 'currentState': array([16.41569197, 18.13854456,  0.        ]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 12.964265300018054}
done in step count: 10
reward sum = 0.8219426080698353
running average episode reward sum: 0.7361481375639756
{'currentTarget': array([ 8., 28.]), 'previousTarget': array([ 8., 28.]), 'currentState': array([ 8.60076976, 27.56526971,  0.        ]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 0.7415623565342349}
episode index:927
target Thresh 21.86907779145634
target distance 6.0
model initialize at round 927
at step 0:
{'currentTarget': array([ 7., 18.]), 'previousTarget': array([ 7., 18.]), 'currentState': array([11.76007247, 16.19478691,  0.        ]), 'targetState': array([ 7, 18], dtype=int32), 'currentDistance': 5.090882460907741}
done in step count: 4
reward sum = 0.9261072629919002
running average episode reward sum: 0.7363528348974109
{'currentTarget': array([ 7., 18.]), 'previousTarget': array([ 7., 18.]), 'currentState': array([ 7.52473229, 17.4573213 ,  0.        ]), 'targetState': array([ 7, 18], dtype=int32), 'currentDistance': 0.7548802210868025}
episode index:928
target Thresh 21.879203649891842
target distance 15.0
model initialize at round 928
at step 0:
{'currentTarget': array([20., 20.]), 'previousTarget': array([20., 20.]), 'currentState': array([22.88341637,  6.39245355,  0.        ]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 13.909687642573695}
done in step count: 11
reward sum = 0.8052893102035372
running average episode reward sum: 0.7364270399300331
{'currentTarget': array([20., 20.]), 'previousTarget': array([20., 20.]), 'currentState': array([19.92270827, 19.5495919 ,  0.        ]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 0.45699175987445456}
episode index:929
target Thresh 21.889319387530158
target distance 15.0
model initialize at round 929
at step 0:
{'currentTarget': array([24.,  8.]), 'previousTarget': array([24.,  8.]), 'currentState': array([15.57663441, 21.61217213,  0.        ]), 'targetState': array([24,  8], dtype=int32), 'currentDistance': 16.007633114280942}
done in step count: 12
reward sum = 0.7859651030019144
running average episode reward sum: 0.736480306664519
{'currentTarget': array([24.,  8.]), 'previousTarget': array([24.,  8.]), 'currentState': array([23.04994005,  8.88890934,  0.        ]), 'targetState': array([24,  8], dtype=int32), 'currentDistance': 1.3010663794876725}
episode index:930
target Thresh 21.89942501448701
target distance 11.0
model initialize at round 930
at step 0:
{'currentTarget': array([19., 25.]), 'previousTarget': array([19., 25.]), 'currentState': array([26.77273069, 15.14164948,  0.        ]), 'targetState': array([19, 25], dtype=int32), 'currentDistance': 12.553980138312587}
done in step count: 11
reward sum = 0.8278378706956985
running average episode reward sum: 0.7365784350899016
{'currentTarget': array([19., 25.]), 'previousTarget': array([19., 25.]), 'currentState': array([19.67141347, 24.30385065,  0.        ]), 'targetState': array([19, 25], dtype=int32), 'currentDistance': 0.9671711116362655}
episode index:931
target Thresh 21.909520540868037
target distance 4.0
model initialize at round 931
at step 0:
{'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([ 6.84975421, 19.80837798,  0.        ]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 3.0348067900214386}
done in step count: 2
reward sum = 0.9624113955792106
running average episode reward sum: 0.7368207451333451
{'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([ 7.14632927, 17.78393179,  0.        ]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 1.1590093923053204}
episode index:932
target Thresh 21.919605976768768
target distance 17.0
model initialize at round 932
at step 0:
{'currentTarget': array([ 7., 20.]), 'previousTarget': array([ 7., 20.]), 'currentState': array([23.40291458, 18.74091369,  0.        ]), 'targetState': array([ 7, 20], dtype=int32), 'currentDistance': 16.451167289217054}
done in step count: 16
reward sum = 0.7683332982413122
running average episode reward sum: 0.7368545206457866
{'currentTarget': array([ 7., 20.]), 'previousTarget': array([ 7., 20.]), 'currentState': array([ 7.60507458, 19.66228002,  0.        ]), 'targetState': array([ 7, 20], dtype=int32), 'currentDistance': 0.692943025231174}
episode index:933
target Thresh 21.929681332274633
target distance 4.0
model initialize at round 933
at step 0:
{'currentTarget': array([16., 28.]), 'previousTarget': array([16., 28.]), 'currentState': array([18.6959734 , 25.73511207,  0.        ]), 'targetState': array([16, 28], dtype=int32), 'currentDistance': 3.521077943517561}
done in step count: 2
reward sum = 0.9522869728446803
running average episode reward sum: 0.7370851763761923
{'currentTarget': array([16., 28.]), 'previousTarget': array([16., 28.]), 'currentState': array([16.70970297, 27.39066881,  0.        ]), 'targetState': array([16, 28], dtype=int32), 'currentDistance': 0.9353944640940204}
episode index:934
target Thresh 21.93974661746099
target distance 15.0
model initialize at round 934
at step 0:
{'currentTarget': array([17., 19.]), 'previousTarget': array([17., 19.]), 'currentState': array([21.72795776,  5.28405011,  0.        ]), 'targetState': array([17, 19], dtype=int32), 'currentDistance': 14.507958712492307}
done in step count: 11
reward sum = 0.8057226669283803
running average episode reward sum: 0.7371585854569969
{'currentTarget': array([17., 19.]), 'previousTarget': array([17., 19.]), 'currentState': array([17.50030109, 18.33557439,  0.        ]), 'targetState': array([17, 19], dtype=int32), 'currentDistance': 0.8317226561370796}
episode index:935
target Thresh 21.94980184239313
target distance 21.0
model initialize at round 935
at step 0:
{'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([3.3225708 , 3.21592474, 0.        ]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 22.481484119250183}
done in step count: 17
reward sum = 0.7098944855247509
running average episode reward sum: 0.7371294571451034
{'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([13.11750355, 22.09074676,  0.        ]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 1.2670996139366133}
episode index:936
target Thresh 21.959847017126272
target distance 20.0
model initialize at round 936
at step 0:
{'currentTarget': array([24., 22.]), 'previousTarget': array([24., 22.]), 'currentState': array([4.16201891, 9.24752641, 0.        ]), 'targetState': array([24, 22], dtype=int32), 'currentDistance': 23.583279595068863}
done in step count: 18
reward sum = 0.6758264212786731
running average episode reward sum: 0.7370640323469535
{'currentTarget': array([24., 22.]), 'previousTarget': array([24., 22.]), 'currentState': array([23.27053796, 22.04867429,  0.        ]), 'targetState': array([24, 22], dtype=int32), 'currentDistance': 0.7310841657677664}
episode index:937
target Thresh 21.969882151705598
target distance 10.0
model initialize at round 937
at step 0:
{'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([15.22015513, 18.11792386,  0.        ]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 13.370934934673137}
done in step count: 10
reward sum = 0.8217844543001002
running average episode reward sum: 0.7371543526262212
{'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([24.20987521,  9.48266417,  0.        ]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 0.9258843777486525}
episode index:938
target Thresh 21.979907256166232
target distance 12.0
model initialize at round 938
at step 0:
{'currentTarget': array([19., 15.]), 'previousTarget': array([19., 15.]), 'currentState': array([ 8.41160548, 22.60181594,  0.        ]), 'targetState': array([19, 15], dtype=int32), 'currentDistance': 13.03463479109842}
done in step count: 10
reward sum = 0.8155860962575008
running average episode reward sum: 0.7372378795097476
{'currentTarget': array([19., 15.]), 'previousTarget': array([19., 15.]), 'currentState': array([18.32998049, 15.36291069,  0.        ]), 'targetState': array([19, 15], dtype=int32), 'currentDistance': 0.761991015509303}
episode index:939
target Thresh 21.98992234053329
target distance 16.0
model initialize at round 939
at step 0:
{'currentTarget': array([27.,  6.]), 'previousTarget': array([27.,  6.]), 'currentState': array([27.06819313, 20.50924301,  0.        ]), 'targetState': array([27,  6], dtype=int32), 'currentDistance': 14.509403263712654}
done in step count: 11
reward sum = 0.7997881292173386
running average episode reward sum: 0.7373044223285854
{'currentTarget': array([27.,  6.]), 'previousTarget': array([27.,  6.]), 'currentState': array([26.14961818,  6.86527604,  0.        ]), 'targetState': array([27,  6], dtype=int32), 'currentDistance': 1.2131990173545335}
episode index:940
target Thresh 21.999927414821858
target distance 1.0
model initialize at round 940
at step 0:
{'currentTarget': array([ 6., 29.]), 'previousTarget': array([ 6., 29.]), 'currentState': array([ 5.46614331, 28.62238067,  0.        ]), 'targetState': array([ 6, 29], dtype=int32), 'currentDistance': 0.6539107896633631}
done in step count: 0
reward sum = 0.9969767635782381
running average episode reward sum: 0.7375803759324638
{'currentTarget': array([ 6., 29.]), 'previousTarget': array([ 6., 29.]), 'currentState': array([ 5.46614331, 28.62238067,  0.        ]), 'targetState': array([ 6, 29], dtype=int32), 'currentDistance': 0.6539107896633631}
episode index:941
target Thresh 22.009922489037006
target distance 11.0
model initialize at round 941
at step 0:
{'currentTarget': array([11., 13.]), 'previousTarget': array([11., 13.]), 'currentState': array([18.73122704,  3.06938243,  0.        ]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 12.585270632272376}
done in step count: 10
reward sum = 0.8345048213149873
running average episode reward sum: 0.7376832681250144
{'currentTarget': array([11., 13.]), 'previousTarget': array([11., 13.]), 'currentState': array([11.96323326, 12.09995401,  0.        ]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 1.3182947710647388}
episode index:942
target Thresh 22.01990757317381
target distance 12.0
model initialize at round 942
at step 0:
{'currentTarget': array([17., 11.]), 'previousTarget': array([17., 11.]), 'currentState': array([ 6.49712133, 16.75767365,  0.        ]), 'targetState': array([17, 11], dtype=int32), 'currentDistance': 11.977531721615817}
done in step count: 8
reward sum = 0.8337745428658664
running average episode reward sum: 0.7377851676740502
{'currentTarget': array([17., 11.]), 'previousTarget': array([17., 11.]), 'currentState': array([16.20115238, 11.63542548,  0.        ]), 'targetState': array([17, 11], dtype=int32), 'currentDistance': 1.0207463208412042}
episode index:943
target Thresh 22.02988267721736
target distance 7.0
model initialize at round 943
at step 0:
{'currentTarget': array([12., 23.]), 'previousTarget': array([12., 23.]), 'currentState': array([ 8.14032173, 17.25717914,  0.        ]), 'targetState': array([12, 23], dtype=int32), 'currentDistance': 6.919328562358061}
done in step count: 5
reward sum = 0.8938111389174335
running average episode reward sum: 0.7379504494232487
{'currentTarget': array([12., 23.]), 'previousTarget': array([12., 23.]), 'currentState': array([11.50563923, 22.5996775 ,  0.        ]), 'targetState': array([12, 23], dtype=int32), 'currentDistance': 0.6361215898978666}
episode index:944
target Thresh 22.03984781114275
target distance 3.0
model initialize at round 944
at step 0:
{'currentTarget': array([ 2., 25.]), 'previousTarget': array([ 2., 25.]), 'currentState': array([ 1.79939754, 26.89865446,  0.        ]), 'targetState': array([ 2, 25], dtype=int32), 'currentDistance': 1.909222382973663}
done in step count: 1
reward sum = 0.9787355307035832
running average episode reward sum: 0.7382052484510586
{'currentTarget': array([ 2., 25.]), 'previousTarget': array([ 2., 25.]), 'currentState': array([ 1.78634278, 25.89385712,  0.        ]), 'targetState': array([ 2, 25], dtype=int32), 'currentDistance': 0.9190375180697018}
episode index:945
target Thresh 22.04980298491512
target distance 18.0
model initialize at round 945
at step 0:
{'currentTarget': array([27.,  8.]), 'previousTarget': array([27.,  8.]), 'currentState': array([17.78979075, 24.61827278,  0.        ]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 18.999866964617688}
done in step count: 15
reward sum = 0.7378343227015935
running average episode reward sum: 0.7382048563519577
{'currentTarget': array([27.,  8.]), 'previousTarget': array([27.,  8.]), 'currentState': array([26.27917087,  8.49421418,  0.        ]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 0.8739807128356991}
episode index:946
target Thresh 22.05974820848965
target distance 17.0
model initialize at round 946
at step 0:
{'currentTarget': array([20., 12.]), 'previousTarget': array([20., 12.]), 'currentState': array([ 4.48589015, 22.51680869,  0.        ]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 18.74275511842265}
done in step count: 15
reward sum = 0.7394984110004241
running average episode reward sum: 0.7382062223019561
{'currentTarget': array([20., 12.]), 'previousTarget': array([20., 12.]), 'currentState': array([19.27637112, 12.41253561,  0.        ]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 0.8329612125724025}
episode index:947
target Thresh 22.06968349181156
target distance 18.0
model initialize at round 947
at step 0:
{'currentTarget': array([ 7., 24.]), 'previousTarget': array([ 7., 24.]), 'currentState': array([12.66463956,  7.34157181,  0.        ]), 'targetState': array([ 7, 24], dtype=int32), 'currentDistance': 17.59520875705058}
done in step count: 13
reward sum = 0.7615951285956564
running average episode reward sum: 0.7382308941440381
{'currentTarget': array([ 7., 24.]), 'previousTarget': array([ 7., 24.]), 'currentState': array([ 7.62560573, 23.50583601,  0.        ]), 'targetState': array([ 7, 24], dtype=int32), 'currentDistance': 0.7972330783388639}
episode index:948
target Thresh 22.079608844816136
target distance 14.0
model initialize at round 948
at step 0:
{'currentTarget': array([ 7., 15.]), 'previousTarget': array([ 7., 15.]), 'currentState': array([19.64081788, 18.00743981,  0.        ]), 'targetState': array([ 7, 15], dtype=int32), 'currentDistance': 12.99365117581436}
done in step count: 10
reward sum = 0.8188264841498594
running average episode reward sum: 0.7383158210038967
{'currentTarget': array([ 7., 15.]), 'previousTarget': array([ 7., 15.]), 'currentState': array([ 7.88009521, 14.62219309,  0.        ]), 'targetState': array([ 7, 15], dtype=int32), 'currentDistance': 0.9577607459977762}
episode index:949
target Thresh 22.089524277428723
target distance 5.0
model initialize at round 949
at step 0:
{'currentTarget': array([14., 26.]), 'previousTarget': array([14., 26.]), 'currentState': array([17.80211687, 27.41040075,  0.        ]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 4.055283340347972}
done in step count: 4
reward sum = 0.9348575283956025
running average episode reward sum: 0.7385227070116774
{'currentTarget': array([14., 26.]), 'previousTarget': array([14., 26.]), 'currentState': array([14.7821407, 25.5528914,  0.       ]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 0.9009162973663672}
episode index:950
target Thresh 22.099429799564767
target distance 8.0
model initialize at round 950
at step 0:
{'currentTarget': array([23., 13.]), 'previousTarget': array([23., 13.]), 'currentState': array([16.53215659, 12.59692234,  0.        ]), 'targetState': array([23, 13], dtype=int32), 'currentDistance': 6.480391193535341}
done in step count: 5
reward sum = 0.9035794677048258
running average episode reward sum: 0.738696268274236
{'currentTarget': array([23., 13.]), 'previousTarget': array([23., 13.]), 'currentState': array([22.04958433, 12.75127781,  0.        ]), 'targetState': array([23, 13], dtype=int32), 'currentDistance': 0.9824218413334082}
episode index:951
target Thresh 22.109325421129785
target distance 7.0
model initialize at round 951
at step 0:
{'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([25.89098215, 12.04950241,  0.        ]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 5.96716987963107}
done in step count: 5
reward sum = 0.9149444750692386
running average episode reward sum: 0.7388814029452391
{'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([20.53668797, 12.58637666,  0.        ]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 0.6775826454414116}
episode index:952
target Thresh 22.1192111520194
target distance 11.0
model initialize at round 952
at step 0:
{'currentTarget': array([21., 28.]), 'previousTarget': array([21., 28.]), 'currentState': array([11.54940009, 25.60548711,  0.        ]), 'targetState': array([21, 28], dtype=int32), 'currentDistance': 9.749232309551498}
done in step count: 8
reward sum = 0.8535014985195762
running average episode reward sum: 0.7390016758681922
{'currentTarget': array([21., 28.]), 'previousTarget': array([21., 28.]), 'currentState': array([20.17844081, 27.95918152,  0.        ]), 'targetState': array([21, 28], dtype=int32), 'currentDistance': 0.8225725818619496}
episode index:953
target Thresh 22.12908700211934
target distance 22.0
model initialize at round 953
at step 0:
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([19.58884633, 26.44518006,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 20.75777458145221}
done in step count: 15
reward sum = 0.7147068337740351
running average episode reward sum: 0.7389762095766889
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.09021579,  6.81770176,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.2232511857607753}
episode index:954
target Thresh 22.138952981305465
target distance 17.0
model initialize at round 954
at step 0:
{'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([20.65563846, 13.75190718,  0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 16.360917986713527}
done in step count: 14
reward sum = 0.7677071768105365
running average episode reward sum: 0.7390062943591327
{'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([5.65336961, 8.61459517, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 0.7585701818159765}
episode index:955
target Thresh 22.148809099443746
target distance 9.0
model initialize at round 955
at step 0:
{'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([13.1289869 ,  3.11458135,  0.        ]), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 8.344772699797176}
done in step count: 7
reward sum = 0.888997915641078
running average episode reward sum: 0.7391631893604736
{'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([5.85484058, 4.34749173, 0.        ]), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.0754159445988487}
episode index:956
target Thresh 22.158655366390306
target distance 11.0
model initialize at round 956
at step 0:
{'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([ 3.63557652, 18.62038994,  0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 9.716663728287665}
done in step count: 8
reward sum = 0.8588807076682847
running average episode reward sum: 0.7392882860358214
{'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([4.1187852 , 9.68583077, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 1.1166482795289707}
episode index:957
target Thresh 22.168491791991414
target distance 7.0
model initialize at round 957
at step 0:
{'currentTarget': array([19., 12.]), 'previousTarget': array([19., 12.]), 'currentState': array([25.09183991,  6.60948718,  0.        ]), 'targetState': array([19, 12], dtype=int32), 'currentDistance': 8.13438024621221}
done in step count: 7
reward sum = 0.8889535776246037
running average episode reward sum: 0.7394445128537638
{'currentTarget': array([19., 12.]), 'previousTarget': array([19., 12.]), 'currentState': array([19.63500857, 11.47367427,  0.        ]), 'targetState': array([19, 12], dtype=int32), 'currentDistance': 0.8247755240781487}
episode index:958
target Thresh 22.178318386083497
target distance 18.0
model initialize at round 958
at step 0:
{'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([26.36514843, 19.16288538,  0.        ]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 18.026260223519362}
done in step count: 16
reward sum = 0.753379474085108
running average episode reward sum: 0.7394590435745473
{'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([ 9.60708052, 23.43541445,  0.        ]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 0.8290377528067701}
episode index:959
target Thresh 22.188135158493143
target distance 14.0
model initialize at round 959
at step 0:
{'currentTarget': array([6., 7.]), 'previousTarget': array([6., 7.]), 'currentState': array([ 3.70601755, 19.41751313,  0.        ]), 'targetState': array([6, 7], dtype=int32), 'currentDistance': 12.627627958140229}
done in step count: 10
reward sum = 0.8136598625395296
running average episode reward sum: 0.7395363360943025
{'currentTarget': array([6., 7.]), 'previousTarget': array([6., 7.]), 'currentState': array([5.15134674, 7.59047359, 0.        ]), 'targetState': array([6, 7], dtype=int32), 'currentDistance': 1.0338623795941162}
episode index:960
target Thresh 22.19794211903713
target distance 17.0
model initialize at round 960
at step 0:
{'currentTarget': array([21., 21.]), 'previousTarget': array([21., 21.]), 'currentState': array([ 4.60659951, 12.74055296,  0.        ]), 'targetState': array([21, 21], dtype=int32), 'currentDistance': 18.356525951585716}
done in step count: 13
reward sum = 0.7586946726588237
running average episode reward sum: 0.7395562719283968
{'currentTarget': array([21., 21.]), 'previousTarget': array([21., 21.]), 'currentState': array([20.04161561, 20.53522018,  0.        ]), 'targetState': array([21, 21], dtype=int32), 'currentDistance': 1.0651389264857707}
episode index:961
target Thresh 22.20773927752242
target distance 16.0
model initialize at round 961
at step 0:
{'currentTarget': array([24., 18.]), 'previousTarget': array([24., 18.]), 'currentState': array([ 9.50765455, 19.57527256,  0.        ]), 'targetState': array([24, 18], dtype=int32), 'currentDistance': 14.5777076506464}
done in step count: 12
reward sum = 0.7781988305982993
running average episode reward sum: 0.7395964409083031
{'currentTarget': array([24., 18.]), 'previousTarget': array([24., 18.]), 'currentState': array([23.34886962, 17.99009264,  0.        ]), 'targetState': array([24, 18], dtype=int32), 'currentDistance': 0.6512057473284489}
episode index:962
target Thresh 22.217526643746176
target distance 16.0
model initialize at round 962
at step 0:
{'currentTarget': array([27., 23.]), 'previousTarget': array([27., 23.]), 'currentState': array([11.48149827,  8.30317748,  0.        ]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 21.373359310364982}
done in step count: 17
reward sum = 0.7044538091570998
running average episode reward sum: 0.739559948040441
{'currentTarget': array([27., 23.]), 'previousTarget': array([27., 23.]), 'currentState': array([26.3407065 , 22.73172101,  0.        ]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 0.711787564498412}
episode index:963
target Thresh 22.227304227495754
target distance 8.0
model initialize at round 963
at step 0:
{'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([ 9.45357895, 18.50801808,  0.        ]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 7.004684324418802}
done in step count: 6
reward sum = 0.8926883657480583
running average episode reward sum: 0.7397187949467767
{'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([15.3106795 , 20.81328985,  0.        ]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 0.7141592524852287}
episode index:964
target Thresh 22.237072038548746
target distance 22.0
model initialize at round 964
at step 0:
{'currentTarget': array([17., 25.]), 'previousTarget': array([17., 25.]), 'currentState': array([25.21062446,  4.08759928,  0.        ]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 22.466482990060985}
done in step count: 21
reward sum = 0.6923935214055809
running average episode reward sum: 0.7396697532125373
{'currentTarget': array([17., 25.]), 'previousTarget': array([17., 25.]), 'currentState': array([17.66919214, 24.43012762,  0.        ]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 0.8789611155706414}
episode index:965
target Thresh 22.246830086672965
target distance 19.0
model initialize at round 965
at step 0:
{'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([26.85086486, 27.12466443,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 21.1245054073223}
done in step count: 20
reward sum = 0.7236534171494623
running average episode reward sum: 0.7396531731545009
{'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.93109897,  9.99645191,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.3637674674782776}
episode index:966
target Thresh 22.256578381626458
target distance 2.0
model initialize at round 966
at step 0:
{'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 5.85848159, 10.67209464,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 1.3246793947417168}
done in step count: 1
reward sum = 0.983312511256705
running average episode reward sum: 0.7399051476509871
{'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 6.42641181, 10.25326294,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.6270131815406215}
episode index:967
target Thresh 22.26631693315752
target distance 10.0
model initialize at round 967
at step 0:
{'currentTarget': array([25.,  2.]), 'previousTarget': array([25.,  2.]), 'currentState': array([21.93626022, 10.62718749,  0.        ]), 'targetState': array([25,  2], dtype=int32), 'currentDistance': 9.155045901580676}
done in step count: 8
reward sum = 0.8631496501722506
running average episode reward sum: 0.7400324663519388
{'currentTarget': array([25.,  2.]), 'previousTarget': array([25.,  2.]), 'currentState': array([24.46060861,  2.32516134,  0.        ]), 'targetState': array([25,  2], dtype=int32), 'currentDistance': 0.6298197891376922}
episode index:968
target Thresh 22.276045751004702
target distance 4.0
model initialize at round 968
at step 0:
{'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([14.83212101,  3.61638904,  0.        ]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 3.701690244761194}
done in step count: 3
reward sum = 0.9460516644316244
running average episode reward sum: 0.7402450764634761
{'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([12.54826385,  5.53958735,  0.        ]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 0.7159420760875866}
episode index:969
target Thresh 22.285764844896832
target distance 10.0
model initialize at round 969
at step 0:
{'currentTarget': array([17., 12.]), 'previousTarget': array([17., 12.]), 'currentState': array([22.31538826, 21.03488833,  0.        ]), 'targetState': array([17, 12], dtype=int32), 'currentDistance': 10.48248822857694}
done in step count: 8
reward sum = 0.8572951687441109
running average episode reward sum: 0.7403657466617036
{'currentTarget': array([17., 12.]), 'previousTarget': array([17., 12.]), 'currentState': array([17.21352157, 12.6909501 ,  0.        ]), 'targetState': array([17, 12], dtype=int32), 'currentDistance': 0.723189806015313}
episode index:970
target Thresh 22.29547422455299
target distance 16.0
model initialize at round 970
at step 0:
{'currentTarget': array([25., 19.]), 'previousTarget': array([25., 19.]), 'currentState': array([10.2909143 , 19.65537298,  0.        ]), 'targetState': array([25, 19], dtype=int32), 'currentDistance': 14.723678750721913}
done in step count: 12
reward sum = 0.7939808272687271
running average episode reward sum: 0.7404209630166027
{'currentTarget': array([25., 19.]), 'previousTarget': array([25., 19.]), 'currentState': array([24.12352294, 18.59783034,  0.        ]), 'targetState': array([25, 19], dtype=int32), 'currentDistance': 0.9643404372424561}
episode index:971
target Thresh 22.305173899682565
target distance 14.0
model initialize at round 971
at step 0:
{'currentTarget': array([21., 27.]), 'previousTarget': array([21., 27.]), 'currentState': array([16.10058419, 13.91221726,  0.        ]), 'targetState': array([21, 27], dtype=int32), 'currentDistance': 13.974774859632685}
done in step count: 10
reward sum = 0.8184004740189269
running average episode reward sum: 0.7405011888509673
{'currentTarget': array([21., 27.]), 'previousTarget': array([21., 27.]), 'currentState': array([20.51187444, 26.10027969,  0.        ]), 'targetState': array([21, 27], dtype=int32), 'currentDistance': 1.0236030460967722}
episode index:972
target Thresh 22.314863879985232
target distance 15.0
model initialize at round 972
at step 0:
{'currentTarget': array([20.,  2.]), 'previousTarget': array([20.,  2.]), 'currentState': array([6.22526431, 1.62677366, 0.        ]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 13.779791043793061}
done in step count: 11
reward sum = 0.8073334192229197
running average episode reward sum: 0.7405698756242169
{'currentTarget': array([20.,  2.]), 'previousTarget': array([20.,  2.]), 'currentState': array([19.05920804,  1.64155667,  0.        ]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 1.0067627049350467}
episode index:973
target Thresh 22.324544175150976
target distance 17.0
model initialize at round 973
at step 0:
{'currentTarget': array([27., 25.]), 'previousTarget': array([27., 25.]), 'currentState': array([10.95496178, 15.90969378,  0.        ]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 18.441174550548464}
done in step count: 14
reward sum = 0.7439714913371658
running average episode reward sum: 0.7405733680428134
{'currentTarget': array([27., 25.]), 'previousTarget': array([27., 25.]), 'currentState': array([26.44610208, 24.89510253,  0.        ]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 0.5637431878241541}
episode index:974
target Thresh 22.33421479486008
target distance 7.0
model initialize at round 974
at step 0:
{'currentTarget': array([5., 7.]), 'previousTarget': array([5., 7.]), 'currentState': array([ 5.51718402, 12.91564965,  0.        ]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 5.938214388223181}
done in step count: 5
reward sum = 0.9144717396409359
running average episode reward sum: 0.7407517253470166
{'currentTarget': array([5., 7.]), 'previousTarget': array([5., 7.]), 'currentState': array([4.47786646, 7.64395708, 0.        ]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 0.8290380912576689}
episode index:975
target Thresh 22.343875748783184
target distance 14.0
model initialize at round 975
at step 0:
{'currentTarget': array([25., 19.]), 'previousTarget': array([25., 19.]), 'currentState': array([21.09297277,  6.10049486,  0.        ]), 'targetState': array([25, 19], dtype=int32), 'currentDistance': 13.47820813717066}
done in step count: 10
reward sum = 0.8150442859610936
running average episode reward sum: 0.7408278447738753
{'currentTarget': array([25., 19.]), 'previousTarget': array([25., 19.]), 'currentState': array([24.72507371, 18.64895177,  0.        ]), 'targetState': array([25, 19], dtype=int32), 'currentDistance': 0.44589160730222677}
episode index:976
target Thresh 22.353527046581227
target distance 7.0
model initialize at round 976
at step 0:
{'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([15.47679186, 23.90125799,  0.        ]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 5.920487848325658}
done in step count: 5
reward sum = 0.9137606439121293
running average episode reward sum: 0.7410048486624509
{'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([14.36416636, 18.67382711,  0.        ]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 0.9264595992622983}
episode index:977
target Thresh 22.36316869790551
target distance 8.0
model initialize at round 977
at step 0:
{'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([24.40734661, 14.06598639,  0.        ]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 10.146394848474541}
done in step count: 8
reward sum = 0.8643297808253486
running average episode reward sum: 0.7411309477750918
{'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([17.7682758 , 20.40537471,  0.        ]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 0.9715075611301569}
episode index:978
target Thresh 22.37280071239769
target distance 8.0
model initialize at round 978
at step 0:
{'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([15.43283707, 10.91698539,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 7.063831041823823}
done in step count: 6
reward sum = 0.8966684264601015
running average episode reward sum: 0.7412898216041878
{'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.50238562,  4.60831708,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.7859196738929718}
episode index:979
target Thresh 22.38242309968978
target distance 7.0
model initialize at round 979
at step 0:
{'currentTarget': array([11., 17.]), 'previousTarget': array([11., 17.]), 'currentState': array([ 5.80201292, 19.52910152,  0.        ]), 'targetState': array([11, 17], dtype=int32), 'currentDistance': 5.780607596290837}
done in step count: 4
reward sum = 0.9139843492982997
running average episode reward sum: 0.7414660405099981
{'currentTarget': array([11., 17.]), 'previousTarget': array([11., 17.]), 'currentState': array([10.03962374, 17.66268489,  0.        ]), 'targetState': array([11, 17], dtype=int32), 'currentDistance': 1.1668221056382793}
episode index:980
target Thresh 22.392035869404168
target distance 22.0
model initialize at round 980
at step 0:
{'currentTarget': array([10.,  2.]), 'previousTarget': array([10.,  2.]), 'currentState': array([21.83450238, 22.91709363,  0.        ]), 'targetState': array([10,  2], dtype=int32), 'currentDistance': 24.032899383913914}
done in step count: 19
reward sum = 0.6918981333357868
running average episode reward sum: 0.741415512572002
{'currentTarget': array([10.,  2.]), 'previousTarget': array([10.,  2.]), 'currentState': array([10.61356322,  2.94356972,  0.        ]), 'targetState': array([10,  2], dtype=int32), 'currentDistance': 1.1255148363208851}
episode index:981
target Thresh 22.40163903115363
target distance 13.0
model initialize at round 981
at step 0:
{'currentTarget': array([21., 17.]), 'previousTarget': array([21., 17.]), 'currentState': array([18.14182381,  5.29356265,  0.        ]), 'targetState': array([21, 17], dtype=int32), 'currentDistance': 12.050304831316508}
done in step count: 8
reward sum = 0.8342313490695752
running average episode reward sum: 0.7415100297171116
{'currentTarget': array([21., 17.]), 'previousTarget': array([21., 17.]), 'currentState': array([20.62691619, 16.44649494,  0.        ]), 'targetState': array([21, 17], dtype=int32), 'currentDistance': 0.6675023462646361}
episode index:982
target Thresh 22.411232594541318
target distance 7.0
model initialize at round 982
at step 0:
{'currentTarget': array([17., 18.]), 'previousTarget': array([17., 18.]), 'currentState': array([22.9304558 , 12.94352883,  0.        ]), 'targetState': array([17, 18], dtype=int32), 'currentDistance': 7.793472058366768}
done in step count: 6
reward sum = 0.8895417122294709
running average episode reward sum: 0.7416606214592402
{'currentTarget': array([17., 18.]), 'previousTarget': array([17., 18.]), 'currentState': array([17.61454651, 17.51341093,  0.        ]), 'targetState': array([17, 18], dtype=int32), 'currentDistance': 0.7838598963253296}
episode index:983
target Thresh 22.420816569160802
target distance 17.0
model initialize at round 983
at step 0:
{'currentTarget': array([9., 8.]), 'previousTarget': array([9., 8.]), 'currentState': array([22.443766  , 24.21845186,  0.        ]), 'targetState': array([9, 8], dtype=int32), 'currentDistance': 21.065920935533313}
done in step count: 18
reward sum = 0.7286487378029664
running average episode reward sum: 0.7416473980002398
{'currentTarget': array([9., 8.]), 'previousTarget': array([9., 8.]), 'currentState': array([9.7011584 , 8.94433123, 0.        ]), 'targetState': array([9, 8], dtype=int32), 'currentDistance': 1.1761737021165757}
episode index:984
target Thresh 22.430390964596057
target distance 16.0
model initialize at round 984
at step 0:
{'currentTarget': array([24., 29.]), 'previousTarget': array([24., 29.]), 'currentState': array([24.96324832, 14.12571192,  0.        ]), 'targetState': array([24, 29], dtype=int32), 'currentDistance': 14.905445088389406}
done in step count: 10
reward sum = 0.804576221787189
running average episode reward sum: 0.741711285130988
{'currentTarget': array([24., 29.]), 'previousTarget': array([24., 29.]), 'currentState': array([23.54570741, 28.03748834,  0.        ]), 'targetState': array([24, 29], dtype=int32), 'currentDistance': 1.064335685726586}
episode index:985
target Thresh 22.43995579042148
target distance 14.0
model initialize at round 985
at step 0:
{'currentTarget': array([ 7., 23.]), 'previousTarget': array([ 7., 23.]), 'currentState': array([13.60750759, 10.18058658,  0.        ]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 14.422084351055949}
done in step count: 12
reward sum = 0.8088365437021346
running average episode reward sum: 0.7417793634865368
{'currentTarget': array([ 7., 23.]), 'previousTarget': array([ 7., 23.]), 'currentState': array([ 7.96669346, 22.10409075,  0.        ]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 1.3180097235119066}
episode index:986
target Thresh 22.449511056201896
target distance 13.0
model initialize at round 986
at step 0:
{'currentTarget': array([21.,  5.]), 'previousTarget': array([21.,  5.]), 'currentState': array([21.48912758, 16.68640375,  0.        ]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 11.696635346374661}
done in step count: 10
reward sum = 0.8286907253354925
running average episode reward sum: 0.7418674195775692
{'currentTarget': array([21.,  5.]), 'previousTarget': array([21.,  5.]), 'currentState': array([20.28444203,  5.73452008,  0.        ]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 1.0254476816515565}
episode index:987
target Thresh 22.45905677149257
target distance 9.0
model initialize at round 987
at step 0:
{'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([13.78511882, 12.28484297,  0.        ]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 8.35817617911804}
done in step count: 6
reward sum = 0.8791311265685452
running average episode reward sum: 0.7420063504550904
{'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([16.50452125, 19.52501786,  0.        ]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 0.6863725130188351}
episode index:988
target Thresh 22.468592945839227
target distance 21.0
model initialize at round 988
at step 0:
{'currentTarget': array([7., 4.]), 'previousTarget': array([7., 4.]), 'currentState': array([22.38861555, 24.24077487,  0.        ]), 'targetState': array([7, 4], dtype=int32), 'currentDistance': 25.42633390478611}
done in step count: 22
reward sum = 0.6743994894173917
running average episode reward sum: 0.7419379916471656
{'currentTarget': array([7., 4.]), 'previousTarget': array([7., 4.]), 'currentState': array([7.45847902, 4.96334743, 0.        ]), 'targetState': array([7, 4], dtype=int32), 'currentDistance': 1.0668839156925645}
episode index:989
target Thresh 22.47811958877803
target distance 22.0
model initialize at round 989
at step 0:
{'currentTarget': array([17., 29.]), 'previousTarget': array([17., 29.]), 'currentState': array([13.92942334,  7.97718441,  0.        ]), 'targetState': array([17, 29], dtype=int32), 'currentDistance': 21.245875273889023}
done in step count: 18
reward sum = 0.720322913455085
running average episode reward sum: 0.7419161582348504
{'currentTarget': array([17., 29.]), 'previousTarget': array([17., 29.]), 'currentState': array([16.26973654, 28.14898723,  0.        ]), 'targetState': array([17, 29], dtype=int32), 'currentDistance': 1.1213863952383436}
episode index:990
target Thresh 22.487636709835627
target distance 20.0
model initialize at round 990
at step 0:
{'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([ 2.86708856, 28.26698521,  0.        ]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 23.86661197640618}
done in step count: 23
reward sum = 0.6787286870297199
running average episode reward sum: 0.7418523969117372
{'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([21.34450287, 14.35198497,  0.        ]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 0.744022789723245}
episode index:991
target Thresh 22.497144318529145
target distance 13.0
model initialize at round 991
at step 0:
{'currentTarget': array([ 9., 13.]), 'previousTarget': array([ 9., 13.]), 'currentState': array([ 7.38487864, 24.54816103,  0.        ]), 'targetState': array([ 9, 13], dtype=int32), 'currentDistance': 11.660559171537026}
done in step count: 10
reward sum = 0.8277575358125245
running average episode reward sum: 0.7419389948340163
{'currentTarget': array([ 9., 13.]), 'previousTarget': array([ 9., 13.]), 'currentState': array([ 8.24803822, 13.697662  ,  0.        ]), 'targetState': array([ 9, 13], dtype=int32), 'currentDistance': 1.025757659467159}
episode index:992
target Thresh 22.50664242436619
target distance 10.0
model initialize at round 992
at step 0:
{'currentTarget': array([20.,  9.]), 'previousTarget': array([20.,  9.]), 'currentState': array([18.50620443, 17.70630336,  0.        ]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 8.833523837004282}
done in step count: 8
reward sum = 0.8668945768270179
running average episode reward sum: 0.7420648312710687
{'currentTarget': array([20.,  9.]), 'previousTarget': array([20.,  9.]), 'currentState': array([19.38812817,  9.49331307,  0.        ]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 0.7859675071198148}
episode index:993
target Thresh 22.516131036844868
target distance 10.0
model initialize at round 993
at step 0:
{'currentTarget': array([27., 18.]), 'previousTarget': array([27., 18.]), 'currentState': array([18.56784266, 26.84311795,  0.        ]), 'targetState': array([27, 18], dtype=int32), 'currentDistance': 12.218920266793894}
done in step count: 10
reward sum = 0.8324200621096911
running average episode reward sum: 0.7421557319057152
{'currentTarget': array([27., 18.]), 'previousTarget': array([27., 18.]), 'currentState': array([26.28673279, 18.55902791,  0.        ]), 'targetState': array([27, 18], dtype=int32), 'currentDistance': 0.9062352416499657}
episode index:994
target Thresh 22.52561016545379
target distance 7.0
model initialize at round 994
at step 0:
{'currentTarget': array([17., 15.]), 'previousTarget': array([17., 15.]), 'currentState': array([11.57627249, 15.6073437 ,  0.        ]), 'targetState': array([17, 15], dtype=int32), 'currentDistance': 5.4576264531862275}
done in step count: 4
reward sum = 0.9207602435335444
running average episode reward sum: 0.7423352339274517
{'currentTarget': array([17., 15.]), 'previousTarget': array([17., 15.]), 'currentState': array([16.0643568 , 15.39702542,  0.        ]), 'targetState': array([17, 15], dtype=int32), 'currentDistance': 1.0163942997624498}
episode index:995
target Thresh 22.53507981967209
target distance 4.0
model initialize at round 995
at step 0:
{'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([21.7931212, 11.3765924,  0.       ]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 2.631552067542007}
done in step count: 2
reward sum = 0.9586700280765242
running average episode reward sum: 0.7425524375360351
{'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([21.79682527, 13.49770474,  0.        ]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 0.5418306864783807}
episode index:996
target Thresh 22.54454000896942
target distance 11.0
model initialize at round 996
at step 0:
{'currentTarget': array([ 4., 12.]), 'previousTarget': array([ 4., 12.]), 'currentState': array([ 5.70275563, 21.91405439,  0.        ]), 'targetState': array([ 4, 12], dtype=int32), 'currentDistance': 10.05921722889755}
done in step count: 9
reward sum = 0.8582682547172215
running average episode reward sum: 0.7426685015452439
{'currentTarget': array([ 4., 12.]), 'previousTarget': array([ 4., 12.]), 'currentState': array([ 3.40218619, 12.8561886 ,  0.        ]), 'targetState': array([ 4, 12], dtype=int32), 'currentDistance': 1.0442414754570504}
episode index:997
target Thresh 22.553990742805972
target distance 11.0
model initialize at round 997
at step 0:
{'currentTarget': array([25., 13.]), 'previousTarget': array([25., 13.]), 'currentState': array([15.81325459,  2.93139684,  0.        ]), 'targetState': array([25, 13], dtype=int32), 'currentDistance': 13.629859158962077}
done in step count: 10
reward sum = 0.8151864946169027
running average episode reward sum: 0.742741164864955
{'currentTarget': array([25., 13.]), 'previousTarget': array([25., 13.]), 'currentState': array([24.11419034, 12.61513156,  0.        ]), 'targetState': array([25, 13], dtype=int32), 'currentDistance': 0.9658066432266836}
episode index:998
target Thresh 22.563432030632477
target distance 12.0
model initialize at round 998
at step 0:
{'currentTarget': array([ 2., 26.]), 'previousTarget': array([ 2., 26.]), 'currentState': array([13.48138422, 15.14611673,  0.        ]), 'targetState': array([ 2, 26], dtype=int32), 'currentDistance': 15.799650803642754}
done in step count: 13
reward sum = 0.7887713396187309
running average episode reward sum: 0.7427872411159598
{'currentTarget': array([ 2., 26.]), 'previousTarget': array([ 2., 26.]), 'currentState': array([ 2.6805737 , 25.36541069,  0.        ]), 'targetState': array([ 2, 26], dtype=int32), 'currentDistance': 0.9305289687531026}
episode index:999
target Thresh 22.57286388189023
target distance 12.0
model initialize at round 999
at step 0:
{'currentTarget': array([5., 8.]), 'previousTarget': array([5., 8.]), 'currentState': array([ 3.55323377, 18.59217501,  0.        ]), 'targetState': array([5, 8], dtype=int32), 'currentDistance': 10.69052402331495}
done in step count: 9
reward sum = 0.8472590431800978
running average episode reward sum: 0.7428917129180239
{'currentTarget': array([5., 8.]), 'previousTarget': array([5., 8.]), 'currentState': array([4.20698355, 8.76125193, 0.        ]), 'targetState': array([5, 8], dtype=int32), 'currentDistance': 1.0992632024423536}

Process finished with exit code 0
