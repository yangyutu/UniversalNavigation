/home/yangyutu/anaconda3/bin/python /home/yangyutu/Dropbox/UniversalNavigationProject/activeParticleModel/Navigation/FreeSpace/TwoDim/DDPGHER_MLP.py
episode index:0
target Thresh 6.399999999999999
target distance 3.0
model initialize at round 0
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([15.00741222,  7.00323634,  0.        ]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 3.154222476848454}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([28.80932929, 23.0072438 ,  0.        ]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 18.494836203813307}
episode index:1
target Thresh 6.6547242560212965
target distance 6.0
model initialize at round 1
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 24.]), 'previousTarget': array([10., 24.]), 'currentState': array([16.19724227, 24.30125952,  0.        ]), 'targetState': array([10, 24], dtype=int32), 'currentDistance': 6.204560347885491}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 20, 'currentTarget': array([28.78885761, 51.17415118]), 'previousTarget': array([28.43849175, 50.6731753 ]), 'currentState': array([40.16322683, 67.6247963 ,  0.        ]), 'targetState': array([10, 24], dtype=int32), 'currentDistance': 20.0}
episode index:2
target Thresh 6.9069139633470655
target distance 4.0
model initialize at round 2
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([18.20956184, 11.49149844,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 5.469092451430901}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 20, 'currentTarget': array([22.06621935, 41.49480066]), 'previousTarget': array([22.06601112, 41.01052354]), 'currentState': array([26.74875361, 60.93892246,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 20.0}
episode index:3
target Thresh 7.15659434115819
target distance 7.0
model initialize at round 3
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([12.97482228, 18.49899444,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 8.999098529051555}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 20, 'currentTarget': array([-1.15816818, 48.56172866]), 'previousTarget': array([-0.85503864, 48.04064818]), 'currentState': array([-5.89571515, 67.99251992,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 20.0}
episode index:4
target Thresh 7.403790357700526
target distance 5.0
model initialize at round 4
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'previousTarget': array([26., 11.]), 'currentState': array([24.72441131, 16.49922961,  0.        ]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 5.6452327509445634}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 20, 'currentTarget': array([-7.36283774, 50.68660234]), 'previousTarget': array([-6.87963877, 50.17618495]), 'currentState': array([-20.23257962,  65.99574158,   0.        ]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 20.0}
episode index:5
target Thresh 7.648526732781718
target distance 7.0
model initialize at round 5
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 17.]), 'previousTarget': array([21., 17.]), 'currentState': array([25.09039581, 10.49904236,  0.        ]), 'targetState': array([21, 17], dtype=int32), 'currentDistance': 7.680741375629177}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 20, 'currentTarget': array([11.06527492, 41.43759196]), 'previousTarget': array([11.44109573, 40.89571679]), 'currentState': array([ 3.53320927, 59.9650847 ,  0.        ]), 'targetState': array([21, 17], dtype=int32), 'currentDistance': 20.0}
episode index:6
target Thresh 7.890827940243231
target distance 2.0
model initialize at round 6
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  3.]), 'previousTarget': array([10.,  3.]), 'currentState': array([7.75917178, 2.49951321, 0.        ]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 2.296039660385173}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 20, 'currentTarget': array([-26.14062126,  38.06907136]), 'previousTarget': array([-25.63871488,  37.57130093]), 'currentState': array([-40.49394009,  51.99682203,   0.        ]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 20.0}
episode index:7
target Thresh 8.130718210407721
target distance 7.0
model initialize at round 7
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  8.]), 'previousTarget': array([11.,  8.]), 'currentState': array([3.69251704, 4.49993542, 0.        ]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 8.102453903276507}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 20, 'currentTarget': array([-29.54294663,  41.30451737]), 'previousTarget': array([-29.03085981,  40.81939447]), 'currentState': array([-44.99721321,  53.99962053,   0.        ]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 20.0}
episode index:8
target Thresh 8.368221532502123
target distance 3.0
model initialize at round 8
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([5.71987036, 7.49906489, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 3.105622372437196}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 20, 'currentTarget': array([-27.99446563,  41.86174104]), 'previousTarget': array([-27.50619974,  41.35169256]), 'currentState': array([-41.07241166,  56.99340743,   0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 20.0}
episode index:9
target Thresh 8.603361657056556
target distance 6.0
model initialize at round 9
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  7.]), 'previousTarget': array([19.,  7.]), 'currentState': array([24.67311528, 12.49977049,  0.        ]), 'targetState': array([19,  7], dtype=int32), 'currentDistance': 7.901374088422487}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 20, 'currentTarget': array([-0.1583354 , 44.21632508]), 'previousTarget': array([ 0.27415638, 43.68501475]), 'currentState': array([-9.31228961, 61.9984825 ,  0.        ]), 'targetState': array([19,  7], dtype=int32), 'currentDistance': 20.0}
episode index:10
target Thresh 8.836162098279434
target distance 4.0
model initialize at round 10
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 16.]), 'previousTarget': array([23., 16.]), 'currentState': array([25.69293067, 20.49964264,  0.        ]), 'targetState': array([23, 16], dtype=int32), 'currentDistance': 5.243916425413341}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 20, 'currentTarget': array([-5.18887266, 53.94347096]), 'previousTarget': array([-4.71497584, 53.42521809]), 'currentState': array([-17.11597821,  69.99788323,   0.        ]), 'targetState': array([23, 16], dtype=int32), 'currentDistance': 20.0}
episode index:11
target Thresh 9.066646136408878
target distance 9.0
model initialize at round 11
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 29.]), 'previousTarget': array([20., 29.]), 'currentState': array([12.81946692, 20.49994579,  0.        ]), 'targetState': array([20, 29], dtype=int32), 'currentDistance': 11.127038101475957}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 20, 'currentTarget': array([-16.51024133,  57.6521344 ]), 'previousTarget': array([-15.99610568,  57.17270966]), 'currentState': array([-32.24382463,  69.99937525,   0.        ]), 'targetState': array([20, 29], dtype=int32), 'currentDistance': 20.0}
episode index:12
target Thresh 9.294836820040768
target distance 5.0
model initialize at round 12
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([ 4.03666314, 12.49991927,  0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 6.07323205575076}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 20, 'currentTarget': array([-23.95005   ,  46.90510457]), 'previousTarget': array([-23.46213608,  46.39603186]), 'currentState': array([-37.07115013,  61.9993661 ,   0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 20.0}
episode index:13
target Thresh 9.52075696843363
target distance 4.0
model initialize at round 13
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 17.]), 'previousTarget': array([19., 17.]), 'currentState': array([15.99598113, 13.49960136,  0.        ]), 'targetState': array([19, 17], dtype=int32), 'currentDistance': 4.612691185325586}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 20, 'currentTarget': array([-1.85084537, 46.63690495]), 'previousTarget': array([-1.3888527 , 46.11233235]), 'currentState': array([-13.35895843,  62.99426824,   0.        ]), 'targetState': array([19, 17], dtype=int32), 'currentDistance': 20.0}
episode index:14
target Thresh 9.744429173790568
target distance 9.0
model initialize at round 14
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 5.]), 'previousTarget': array([8., 5.]), 'currentState': array([ 7.87942222, 14.49994227,  0.        ]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 9.500707457345941}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 20, 'currentTarget': array([-16.884216  ,  46.81298929]), 'previousTarget': array([-16.43117947,  46.28735457]), 'currentState': array([-27.11253709,  63.99965411,   0.        ]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:15
target Thresh 9.965875803518518
target distance 5.0
model initialize at round 15
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 24.]), 'previousTarget': array([16., 24.]), 'currentState': array([13.9076537 , 19.49931619,  0.        ]), 'targetState': array([16, 24], dtype=int32), 'currentDistance': 4.96327188944493}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 20, 'currentTarget': array([-4.30817405, 52.66978917]), 'previousTarget': array([-3.84544421, 52.14491699]), 'currentState': array([-15.86867504,  68.99016958,   0.        ]), 'targetState': array([16, 24], dtype=int32), 'currentDistance': 20.0}
episode index:16
target Thresh 10.185119002464987
target distance 10.0
model initialize at round 16
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 28.]), 'previousTarget': array([17., 28.]), 'currentState': array([14.64202368, 18.49948972,  0.        ]), 'targetState': array([17, 28], dtype=int32), 'currentDistance': 9.788756192986838}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 20, 'currentTarget': array([-4.20047186, 52.78805514]), 'previousTarget': array([-3.71749465, 52.27451676]), 'currentState': array([-17.19987911,  67.98724627,   0.        ]), 'targetState': array([17, 28], dtype=int32), 'currentDistance': 20.0}
episode index:17
target Thresh 10.402180695132575
target distance 9.0
model initialize at round 17
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 6.]), 'previousTarget': array([7., 6.]), 'currentState': array([15.50011408,  5.49995258,  0.        ]), 'targetState': array([7, 6], dtype=int32), 'currentDistance': 8.514809853818765}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 20, 'currentTarget': array([-2.55173043, 35.94061225]), 'previousTarget': array([-2.26195587, 35.41766671]), 'currentState': array([-8.6303483 , 54.99449387,  0.        ]), 'targetState': array([7, 6], dtype=int32), 'currentDistance': 20.0}
episode index:18
target Thresh 10.617082587871437
target distance 10.0
model initialize at round 18
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 26.]), 'previousTarget': array([15., 26.]), 'currentState': array([ 8.76770996, 16.49973047,  0.        ]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 11.362066730237867}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 20, 'currentTarget': array([-17.52800698,  53.17456925]), 'previousTarget': array([-17.01804148,  52.69087166]), 'currentState': array([-32.87666467,  65.99715507,   0.        ]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 20.0}
episode index:19
target Thresh 10.829846171049923
target distance 9.0
model initialize at round 19
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([18.88779915, 12.49960181,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 9.928837721944515}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 20, 'currentTarget': array([ 2.06702409, 43.10384855]), 'previousTarget': array([ 2.3869423 , 42.58336298]), 'currentState': array([-4.49658476, 61.9961496 ,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 20.0}
episode index:20
target Thresh 11.040492721203663
target distance 3.0
model initialize at round 20
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([11.93849923, 11.4983438 ,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 3.951519790807743}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 20, 'currentTarget': array([-16.23159894,  45.73858836]), 'previousTarget': array([-15.75033904,  45.22908946]), 'currentState': array([-29.18548205,  60.97659716,   0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 20.0}
episode index:21
target Thresh 11.249043303163209
target distance 11.0
model initialize at round 21
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 24.]), 'previousTarget': array([12., 24.]), 'currentState': array([22.50000447, 19.49964824,  0.        ]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 11.423802336016296}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.030714956775467414
{'scaleFactor': 20, 'currentTarget': array([12., 24.]), 'previousTarget': array([12., 24.]), 'currentState': array([12.70948046, 24.89123074,  0.        ]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 1.1391464973328889}
episode index:22
target Thresh 11.455518772160548
target distance 4.0
model initialize at round 22
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 21.]), 'previousTarget': array([27., 21.]), 'currentState': array([22.54412925, 22.53322959,  0.        ]), 'targetState': array([27, 21], dtype=int32), 'currentDistance': 4.7122793946262025}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.02937952387218622
{'scaleFactor': 20, 'currentTarget': array([-8.84333951, -4.06268393]), 'previousTarget': array([-8.3599046 , -3.60249275]), 'currentState': array([-25.23391661, -15.5234423 ,   0.        ]), 'targetState': array([27, 21], dtype=int32), 'currentDistance': 20.0}
episode index:23
target Thresh 11.659939775914648
target distance 3.0
model initialize at round 23
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 13.]), 'previousTarget': array([13., 13.]), 'currentState': array([13.49978176, 15.50033915,  0.        ]), 'targetState': array([13, 13], dtype=int32), 'currentDistance': 2.549799535726175}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.028155377044178462
{'scaleFactor': 20, 'currentTarget': array([ 48.42674501, -20.30063654]), 'previousTarget': array([ 47.92233583, -19.80532875]), 'currentState': array([ 62.99940181, -33.9987267 ,   0.        ]), 'targetState': array([13, 13], dtype=int32), 'currentDistance': 20.0}
episode index:24
target Thresh 11.86232675669623
target distance 9.0
model initialize at round 24
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 13.]), 'previousTarget': array([12., 13.]), 'currentState': array([ 2.75998706, 10.50092867,  0.        ]), 'targetState': array([12, 13], dtype=int32), 'currentDistance': 9.572000664028241}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.027029161962411324
{'scaleFactor': 20, 'currentTarget': array([ 37.55526543, -22.5693992 ]), 'previousTarget': array([ 37.08471211, -22.0482834 ]), 'currentState': array([ 49.22490288, -38.81192246,   0.        ]), 'targetState': array([12, 13], dtype=int32), 'currentDistance': 20.0}
episode index:25
target Thresh 12.062699953372032
target distance 9.0
model initialize at round 25
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  9.]), 'previousTarget': array([21.,  9.]), 'currentState': array([24.49999997, 17.50000006,  0.        ]), 'targetState': array([21,  9], dtype=int32), 'currentDistance': 9.19238819919299}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.025989578810010888
{'scaleFactor': 20, 'currentTarget': array([ 58.18086625, -19.76255665]), 'previousTarget': array([ 57.66435477, -19.28393057]), 'currentState': array([ 73.99999994, -31.99999958,   0.        ]), 'targetState': array([21,  9], dtype=int32), 'currentDistance': 20.0}
episode index:26
target Thresh 12.261079403428703
target distance 12.0
model initialize at round 26
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 29.]), 'previousTarget': array([25., 29.]), 'currentState': array([13.37449443, 26.50018847,  0.        ]), 'targetState': array([25, 29], dtype=int32), 'currentDistance': 11.89123363512225}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.02502700181704752
{'scaleFactor': 20, 'currentTarget': array([51.00269113, -6.76150097]), 'previousTarget': array([50.53081289, -6.2410906 ]), 'currentState': array([ 62.76444662, -22.93744323,   0.        ]), 'targetState': array([25, 29], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:27
target Thresh 12.457484944976557
target distance 6.0
model initialize at round 27
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  6.]), 'previousTarget': array([24.,  6.]), 'currentState': array([18.49989384, 10.5000391 ,  0.        ]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 7.106442121991639}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.056432540145324554
{'scaleFactor': 20, 'currentTarget': array([24.,  6.]), 'previousTarget': array([24.,  6.]), 'currentState': array([23.49972758,  5.50046542,  0.        ]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 0.7069705022085654}
episode index:28
target Thresh 12.651936218733425
target distance 10.0
model initialize at round 28
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  4.]), 'previousTarget': array([18.,  4.]), 'currentState': array([ 9.406977  , 13.50000283,  0.        ]), 'targetState': array([18,  4], dtype=int32), 'currentDistance': 12.809765731996743}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.08326292708686811
{'scaleFactor': 20, 'currentTarget': array([18.,  4.]), 'previousTarget': array([18.,  4.]), 'currentState': array([18.36275959,  4.50030991,  0.        ]), 'targetState': array([18,  4], dtype=int32), 'currentDistance': 0.6179842478916605}
episode index:29
target Thresh 12.84445266998873
target distance 9.0
model initialize at round 29
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 2.]), 'previousTarget': array([5., 2.]), 'currentState': array([14.5,  6.5,  0. ]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 10.511898020814394}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08048749618397251
{'scaleFactor': 20, 'currentTarget': array([ 48.09755494, -30.87101648]), 'previousTarget': array([ 47.58200097, -30.39143663]), 'currentState': array([ 64., -43.,   0.]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 20.0}
episode index:30
target Thresh 13.035053550548021
target distance 4.0
model initialize at round 30
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([4.49992988, 3.50010741, 0.        ]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 3.535618517045596}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07789112533932824
{'scaleFactor': 20, 'currentTarget': array([ 40.31102341, -31.41783224]), 'previousTarget': array([ 39.81591689, -30.91323985]), 'currentState': array([ 53.99983287, -45.9992072 ,   0.        ]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 20.0}
episode index:31
target Thresh 13.223757920658194
target distance 11.0
model initialize at round 31
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'previousTarget': array([26., 11.]), 'currentState': array([21.37299144, 21.5000011 ,  0.        ]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 11.474285657326053}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07545702767247423
{'scaleFactor': 20, 'currentTarget': array([ 55.7386203 , -14.87263765]), 'previousTarget': array([ 55.22766129, -14.38524483]), 'currentState': array([ 70.82747233, -27.99995819,   0.        ]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 20.0}
episode index:32
target Thresh 13.41058465091351
target distance 11.0
model initialize at round 32
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 27.]), 'previousTarget': array([ 2., 27.]), 'currentState': array([13.5, 20.5,  0. ]), 'targetState': array([ 2, 27], dtype=int32), 'currentDistance': 13.209844813623016}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07317045107633864
{'scaleFactor': 20, 'currentTarget': array([ 48.26694684, -15.47457415]), 'previousTarget': array([ 47.76197415, -14.97999282]), 'currentState': array([ 63., -29.,   0.]), 'targetState': array([ 2, 27], dtype=int32), 'currentDistance': 20.0}
episode index:33
target Thresh 13.595552424142689
target distance 11.0
model initialize at round 33
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  4.]), 'previousTarget': array([23.,  4.]), 'currentState': array([11.50000307, 14.50000012,  0.        ]), 'targetState': array([23,  4], dtype=int32), 'currentDistance': 15.572409315892754}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0710183789858581
{'scaleFactor': 20, 'currentTarget': array([ 24.11127868, -15.0236115 ]), 'previousTarget': array([ 23.85490782, -14.51088385]), 'currentState': array([ 25.27760553, -34.98957458,   0.        ]), 'targetState': array([23,  4], dtype=int32), 'currentDistance': 20.0}
episode index:34
target Thresh 13.77867973727719
target distance 10.0
model initialize at round 34
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 18.]), 'previousTarget': array([23., 18.]), 'currentState': array([27.49999994, 27.50000003,  0.        ]), 'targetState': array([23, 18], dtype=int32), 'currentDistance': 10.511898022231833}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.068989282443405
{'scaleFactor': 20, 'currentTarget': array([ 60.92885594, -10.09544893]), 'previousTarget': array([60.41022903, -9.62063648]), 'currentState': array([ 76.99999979, -21.99999997,   0.        ]), 'targetState': array([23, 18], dtype=int32), 'currentDistance': 20.0}
episode index:35
target Thresh 13.959984903200937
target distance 10.0
model initialize at round 35
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 20.]), 'previousTarget': array([11., 20.]), 'currentState': array([21.5, 13.5,  0. ]), 'targetState': array([11, 20], dtype=int32), 'currentDistance': 12.349089035228577}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.06707291348664375
{'scaleFactor': 20, 'currentTarget': array([ 56.37889464, -22.35363499]), 'previousTarget': array([ 55.87480833, -21.85801449]), 'currentState': array([ 71., -36.,   0.]), 'targetState': array([11, 20], dtype=int32), 'currentDistance': 20.0}
episode index:36
target Thresh 14.139486052581606
target distance 13.0
model initialize at round 36
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 27.]), 'previousTarget': array([ 6., 27.]), 'currentState': array([13.5      , 13.5000003,  0.       ]), 'targetState': array([ 6, 27], dtype=int32), 'currentDistance': 15.44344495096138}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0652601320410588
{'scaleFactor': 20, 'currentTarget': array([ 49.58173337, -21.16928327]), 'previousTarget': array([ 49.08795027, -20.66366083]), 'currentState': array([ 63.        , -35.99999872,   0.        ]), 'targetState': array([ 6, 27], dtype=int32), 'currentDistance': 20.0}
episode index:37
target Thresh 14.317201135683721
target distance 6.0
model initialize at round 37
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 4.]), 'previousTarget': array([7., 4.]), 'currentState': array([13.5,  7.5,  0. ]), 'targetState': array([7, 4], dtype=int32), 'currentDistance': 7.382411530116743}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.06354276014524145
{'scaleFactor': 20, 'currentTarget': array([ 47.54547491, -29.30521153]), 'previousTarget': array([ 47.03327788, -28.82007466]), 'currentState': array([ 63., -42.,   0.]), 'targetState': array([7, 4], dtype=int32), 'currentDistance': 20.0}
episode index:38
target Thresh 14.49314792416369
target distance 3.0
model initialize at round 38
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([13.5       , 21.50000322,  0.        ]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 3.5355307196309287}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.061913458603055777
{'scaleFactor': 20, 'currentTarget': array([ 49.27558883, -13.45211021]), 'previousTarget': array([ 48.77973997, -12.94819516]), 'currentState': array([ 63.        , -27.99998078,   0.        ]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 20.0}
episode index:39
target Thresh 14.667344012846986
target distance 11.0
model initialize at round 39
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 28.]), 'previousTarget': array([ 2., 28.]), 'currentState': array([13.5, 22.5,  0. ]), 'targetState': array([ 2, 28], dtype=int32), 'currentDistance': 12.747548783982069}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.06036562213797938
{'scaleFactor': 20, 'currentTarget': array([ 48.14623112, -13.60725756]), 'previousTarget': array([ 47.6402251 , -13.11392179]), 'currentState': array([ 63., -27.,   0.]), 'targetState': array([ 2, 28], dtype=int32), 'currentDistance': 20.0}
episode index:40
target Thresh 14.839806821487631
target distance 4.0
model initialize at round 40
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 15.]), 'previousTarget': array([ 2., 15.]), 'currentState': array([ 5.5       , 18.50000003,  0.        ]), 'targetState': array([ 2, 15], dtype=int32), 'currentDistance': 4.949747489379213}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05889328989071159
{'scaleFactor': 20, 'currentTarget': array([ 39.89562539, -17.8905427 ]), 'previousTarget': array([ 39.38621833, -17.40138913]), 'currentState': array([ 55.        , -30.99999988,   0.        ]), 'targetState': array([ 2, 15], dtype=int32), 'currentDistance': 20.0}
episode index:41
target Thresh 15.010553596510224
target distance 14.0
model initialize at round 41
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 28.]), 'previousTarget': array([25., 28.]), 'currentState': array([11.49999917, 27.50000703,  0.        ]), 'targetState': array([25, 28], dtype=int32), 'currentDistance': 13.5092566596856}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0574910687028375
{'scaleFactor': 20, 'currentTarget': array([49.31374677, -5.7684144 ]), 'previousTarget': array([48.84403854, -5.24664675]), 'currentState': array([ 60.99999589, -21.99898991,   0.        ]), 'targetState': array([25, 28], dtype=int32), 'currentDistance': 20.0}
episode index:42
target Thresh 15.179601412734545
target distance 1.0
model initialize at round 42
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 12.]), 'previousTarget': array([19., 12.]), 'currentState': array([20.5       , 12.50000033,  0.        ]), 'targetState': array([19, 12], dtype=int32), 'currentDistance': 1.581138933751838}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0561540671050971
{'scaleFactor': 20, 'currentTarget': array([ 55.5779055 , -23.14347631]), 'previousTarget': array([ 55.07510762, -22.64638897]), 'currentState': array([ 70.        , -36.99999788,   0.        ]), 'targetState': array([19, 12], dtype=int32), 'currentDistance': 20.0}
episode index:43
target Thresh 15.346967175083098
target distance 6.0
model initialize at round 43
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  2.]), 'previousTarget': array([11.,  2.]), 'currentState': array([17.5,  4.5,  0. ]), 'targetState': array([11,  2], dtype=int32), 'currentDistance': 6.964194138592138}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.054877838307253984
{'scaleFactor': 20, 'currentTarget': array([ 51.68051492, -32.14257502]), 'previousTarget': array([ 51.16959038, -31.65560275]), 'currentState': array([ 67., -45.,   0.]), 'targetState': array([11,  2], dtype=int32), 'currentDistance': 20.0}
episode index:44
target Thresh 15.512667620271579
target distance 13.0
model initialize at round 44
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 2.]), 'previousTarget': array([6., 2.]), 'currentState': array([19.5,  3.5,  0. ]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 13.583077707206204}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.053658330789315006
{'scaleFactor': 20, 'currentTarget': array([ 53.0913656 , -33.87913569]), 'previousTarget': array([ 52.57675612, -33.39833465]), 'currentState': array([ 69., -46.,   0.]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 20.0}
episode index:45
target Thresh 15.6767193184826
target distance 9.0
model initialize at round 45
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 24.]), 'previousTarget': array([ 2., 24.]), 'currentState': array([11.5, 27.5,  0. ]), 'targetState': array([ 2, 24], dtype=int32), 'currentDistance': 10.124228365658341}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.052491845337373375
{'scaleFactor': 20, 'currentTarget': array([45.22736135, -9.70268851]), 'previousTarget': array([44.71295565, -9.22118773]), 'currentState': array([ 61., -22.,   0.]), 'targetState': array([ 2, 24], dtype=int32), 'currentDistance': 20.0}
episode index:46
target Thresh 15.839138675022696
target distance 15.0
model initialize at round 46
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([6.49999973, 1.5000172 , 0.        ]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 16.807727849049684}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05137499756423777
{'scaleFactor': 20, 'currentTarget': array([ 45.66657445, -30.87536247]), 'previousTarget': array([ 45.20995776, -30.34925731]), 'currentState': array([ 55.99999893, -47.99904019,   0.        ]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 20.0}
episode index:47
target Thresh 15.999941931962859
target distance 4.0
model initialize at round 47
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 23.]), 'previousTarget': array([23., 23.]), 'currentState': array([27.5       , 19.50000018,  0.        ]), 'targetState': array([23, 23], dtype=int32), 'currentDistance': 5.700877015714582}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05030468511498282
{'scaleFactor': 20, 'currentTarget': array([ 62.72631813, -15.99064503]), 'previousTarget': array([ 62.22508312, -15.49190345]), 'currentState': array([ 77.        , -29.99999925,   0.        ]), 'targetState': array([23, 23], dtype=int32), 'currentDistance': 20.0}
episode index:48
target Thresh 16.159145169762795
target distance 11.0
model initialize at round 48
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  3.]), 'previousTarget': array([13.,  3.]), 'currentState': array([ 2.49999988, 13.50000009,  0.        ]), 'targetState': array([13,  3], dtype=int32), 'currentDistance': 14.849242552431472}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.06597003720645726
{'scaleFactor': 20, 'currentTarget': array([13.,  3.]), 'previousTarget': array([13.,  3.]), 'currentState': array([12.49999934,  3.50001234,  0.        ]), 'targetState': array([13,  3], dtype=int32), 'currentDistance': 0.7071159692472139}
episode index:49
target Thresh 16.316764308878945
target distance 14.0
model initialize at round 49
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  4.]), 'previousTarget': array([22.,  4.]), 'currentState': array([ 8.49999845, 13.50000015,  0.        ]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 16.507575372345975}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.06465063646232812
{'scaleFactor': 20, 'currentTarget': array([ 44.62068799, -21.13405855]), 'previousTarget': array([ 44.13109411, -20.62469962]), 'currentState': array([ 57.99999416, -35.9999311 ,   0.        ]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 20.0}
episode index:50
target Thresh 16.47281511135658
target distance 13.0
model initialize at round 50
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 24.]), 'previousTarget': array([ 2., 24.]), 'currentState': array([15.5, 17.5,  0. ]), 'targetState': array([ 2, 24], dtype=int32), 'currentDistance': 14.983324063771803}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0633829769238511
{'scaleFactor': 20, 'currentTarget': array([ 50.05181363, -18.71272322]), 'previousTarget': array([ 49.54521474, -18.22015069]), 'currentState': array([ 65., -32.,   0.]), 'targetState': array([ 2, 24], dtype=int32), 'currentDistance': 20.0}
episode index:51
target Thresh 16.62731318240599
target distance 3.0
model initialize at round 51
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 23.]), 'previousTarget': array([20., 23.]), 'currentState': array([17.5       , 20.50000504,  0.        ]), 'targetState': array([20, 23], dtype=int32), 'currentDistance': 3.53553034452588}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.06216407352146935
{'scaleFactor': 20, 'currentTarget': array([ 53.5892014 , -14.16249444]), 'previousTarget': array([ 53.09683007, -13.65560285]), 'currentState': array([ 67.        , -28.99996325,   0.        ]), 'targetState': array([20, 23], dtype=int32), 'currentDistance': 20.0}
episode index:52
target Thresh 16.780273971963023
target distance 12.0
model initialize at round 52
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([14.5, 21.5,  0. ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 13.209844813622878}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.06099116647389445
{'scaleFactor': 20, 'currentTarget': array([ 47.45048451, -16.76997163]), 'previousTarget': array([ 46.92822063, -16.30285177]), 'currentState': array([ 64., -28.,   0.]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:53
target Thresh 16.931712776234107
target distance 11.0
model initialize at round 53
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 28.]), 'previousTarget': array([18., 28.]), 'currentState': array([ 7.49999985, 23.50001183,  0.        ]), 'targetState': array([18, 28], dtype=int32), 'currentDistance': 11.423655135099507}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.059861700428081595
{'scaleFactor': 20, 'currentTarget': array([45.29013884, -9.78610327]), 'previousTarget': array([44.81785201, -9.26612407]), 'currentState': array([ 56.99999937, -25.99965212,   0.        ]), 'targetState': array([18, 28], dtype=int32), 'currentDistance': 20.0}
episode index:54
target Thresh 17.081644739225865
target distance 10.0
model initialize at round 54
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 28.]), 'previousTarget': array([17., 28.]), 'currentState': array([27.5, 17.5,  0. ]), 'targetState': array([17, 28], dtype=int32), 'currentDistance': 14.84924240491761}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.058773305874843744
{'scaleFactor': 20, 'currentTarget': array([ 62.85786438, -17.85786438]), 'previousTarget': array([ 62.35786438, -17.35786438]), 'currentState': array([ 77., -32.,   0.]), 'targetState': array([17, 28], dtype=int32), 'currentDistance': 20.0}
episode index:55
target Thresh 17.23008485425954
target distance 8.0
model initialize at round 55
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  9.]), 'previousTarget': array([23.,  9.]), 'currentState': array([15.49999994,  6.50000888,  0.        ]), 'targetState': array([23,  9], dtype=int32), 'currentDistance': 7.905691398523447}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.057723782555650104
{'scaleFactor': 20, 'currentTarget': array([ 52.43323578, -27.44104458]), 'previousTarget': array([ 51.95084009, -26.92684212]), 'currentState': array([ 64.99999973, -42.99985057,   0.        ]), 'targetState': array([23,  9], dtype=int32), 'currentDistance': 20.0}
episode index:56
target Thresh 17.37704796547034
target distance 16.0
model initialize at round 56
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([15.5       ,  5.50000092,  0.        ]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 16.56804060570308}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.056711084616077295
{'scaleFactor': 20, 'currentTarget': array([ 52.77104953, -28.17429645]), 'previousTarget': array([ 52.28826233, -27.66101058]), 'currentState': array([ 65.        , -43.99999613,   0.        ]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 20.0}
episode index:57
target Thresh 17.522548769291852
target distance 14.0
model initialize at round 57
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  9.]), 'previousTarget': array([20.,  9.]), 'currentState': array([ 6.49999878, 18.50000012,  0.        ]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 16.507575087097074}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.055733307295110446
{'scaleFactor': 20, 'currentTarget': array([ 42.62069206, -16.13407326]), 'previousTarget': array([ 42.13109822, -15.62471429]), 'currentState': array([ 55.99999523, -30.9999485 ,   0.        ]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 20.0}
episode index:58
target Thresh 17.66660181592571
target distance 17.0
model initialize at round 58
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  8.]), 'previousTarget': array([19.,  8.]), 'currentState': array([17.5, 24.5,  0. ]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 16.568041525780803}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05478867496807468
{'scaleFactor': 20, 'currentTarget': array([ 50.51916152, -13.66942354]), 'previousTarget': array([ 49.99385477, -13.20632169]), 'currentState': array([ 67., -25.,   0.]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:59
target Thresh 17.80922151079662
target distance 9.0
model initialize at round 59
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 20.]), 'previousTarget': array([18., 20.]), 'currentState': array([ 9.49999991, 25.5000006 ,  0.        ]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 10.12422876452474}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.053875530385273436
{'scaleFactor': 20, 'currentTarget': array([45.36548176, -9.36781035]), 'previousTarget': array([44.87162312, -8.86209013]), 'currentState': array([ 58.99999964, -23.99996406,   0.        ]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:60
target Thresh 17.95042211599292
target distance 5.0
model initialize at round 60
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 29.]), 'previousTarget': array([17., 29.]), 'currentState': array([15.5       , 23.50000402,  0.        ]), 'targetState': array([17, 29], dtype=int32), 'currentDistance': 5.700873243948378}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05299232496912141
{'scaleFactor': 20, 'currentTarget': array([ 51.84931234, -10.93148912]), 'previousTarget': array([ 51.35930852, -10.42277101]), 'currentState': array([ 65.        , -25.99997988,   0.        ]), 'targetState': array([17, 29], dtype=int32), 'currentDistance': 20.0}
episode index:61
target Thresh 18.090217751692805
target distance 14.0
model initialize at round 61
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([13.49999943, 22.50000003,  0.        ]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 19.091883513505252}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.06455766078905743
{'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([26.49999782,  9.50001222,  0.        ]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 0.7071169596854594}
episode index:62
target Thresh 18.22862239757633
target distance 8.0
model initialize at round 62
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  3.]), 'previousTarget': array([13.,  3.]), 'currentState': array([17.5, 10.5,  0. ]), 'targetState': array([13,  3], dtype=int32), 'currentDistance': 8.746427842267924}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.06353293601462795
{'scaleFactor': 20, 'currentTarget': array([ 51.21295565, -26.72118773]), 'previousTarget': array([ 50.69706371, -26.24164755]), 'currentState': array([ 67., -39.,   0.]), 'targetState': array([13,  3], dtype=int32), 'currentDistance': 20.0}
episode index:63
target Thresh 18.36564989422343
target distance 15.0
model initialize at round 63
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 28.]), 'previousTarget': array([25., 28.]), 'currentState': array([16.5       , 12.50000581,  0.        ]), 'targetState': array([25, 28], dtype=int32), 'currentDistance': 17.677664434109925}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.06254023388939939
{'scaleFactor': 20, 'currentTarget': array([ 55.32992814, -20.08399939]), 'previousTarget': array([ 54.86463207, -19.56215889]), 'currentState': array([ 66.        , -36.99995536,   0.        ]), 'targetState': array([25, 28], dtype=int32), 'currentDistance': 20.0}
episode index:64
target Thresh 18.50131394449796
target distance 18.0
model initialize at round 64
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  3.]), 'previousTarget': array([22.,  3.]), 'currentState': array([4.49998492, 9.50000009, 0.        ]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 18.66816887007529}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.06157807644494709
{'scaleFactor': 20, 'currentTarget': array([ 42.05972814, -23.95520325]), 'previousTarget': array([ 41.59087212, -23.43207318]), 'currentState': array([ 53.99995962, -39.9998557 ,   0.        ]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 20.0}
episode index:65
target Thresh 18.63562811491799
target distance 7.0
model initialize at round 65
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([17.5       ,  2.50000015,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 8.746427714491594}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.06064507528669032
{'scaleFactor': 20, 'currentTarget': array([ 53.24510699, -32.48094595]), 'previousTarget': array([ 52.74867211, -31.9775693 ]), 'currentState': array([ 67.        , -46.99999958,   0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 20.0}
episode index:66
target Thresh 18.7686058370125
target distance 10.0
model initialize at round 66
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 29.]), 'previousTarget': array([ 5., 29.]), 'currentState': array([15.5, 22.5,  0. ]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 12.349089035228578}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05973992490927703
{'scaleFactor': 20, 'currentTarget': array([ 50.37889464, -13.35363499]), 'previousTarget': array([ 49.87480833, -12.85801449]), 'currentState': array([ 65., -27.,   0.]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 20.0}
episode index:67
target Thresh 18.900260408664515
target distance 7.0
model initialize at round 67
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([17.5       ,  9.50000033,  0.        ]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 8.514693433220685}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.07189646824467191
{'scaleFactor': 20, 'currentTarget': array([23.,  3.]), 'previousTarget': array([23.,  3.]), 'currentState': array([23.5       ,  3.50000843,  0.        ]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 0.7071127449914956}
episode index:68
target Thresh 19.030604995440907
target distance 6.0
model initialize at round 68
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([ 5.5       , 10.50000441,  0.        ]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 6.964190021843947}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07085449044402449
{'scaleFactor': 20, 'currentTarget': array([ 42.14257213, -23.68049587]), 'previousTarget': array([ 41.65559983, -23.16957134]), 'currentState': array([ 55.        , -38.99997851,   0.        ]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 20.0}
episode index:69
target Thresh 19.159652631908976
target distance 16.0
model initialize at round 69
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 24.]), 'previousTarget': array([10., 24.]), 'currentState': array([16.5,  7.5,  0. ]), 'targetState': array([10, 24], dtype=int32), 'currentDistance': 17.73414785096828}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.06984228343768129
{'scaleFactor': 20, 'currentTarget': array([ 53.06045104, -26.74981729]), 'previousTarget': array([ 52.57071515, -26.24111428]), 'currentState': array([ 66., -42.,   0.]), 'targetState': array([10, 24], dtype=int32), 'currentDistance': 20.0}
episode index:70
target Thresh 19.287416222939918
target distance 16.0
model initialize at round 70
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 26.]), 'previousTarget': array([12., 26.]), 'currentState': array([14.5       ,  9.50000012,  0.        ]), 'targetState': array([12, 26], dtype=int32), 'currentDistance': 16.68831915041456}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.06885858930475619
{'scaleFactor': 20, 'currentTarget': array([ 51.62255542, -24.29016618]), 'previousTarget': array([ 51.13826919, -23.77779836]), 'currentState': array([ 64.        , -39.99999958,   0.        ]), 'targetState': array([12, 26], dtype=int32), 'currentDistance': 20.0}
episode index:71
target Thresh 19.413908544999295
target distance 15.0
model initialize at round 71
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  7.]), 'previousTarget': array([20.,  7.]), 'currentState': array([12.5, 21.5,  0. ]), 'targetState': array([20,  7], dtype=int32), 'currentDistance': 16.324827717314403}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.06790222000885679
{'scaleFactor': 20, 'currentTarget': array([ 46.63557428, -15.19631145]), 'previousTarget': array([ 46.12039686, -14.71454633]), 'currentState': array([ 62.        , -27.99999928,   0.        ]), 'targetState': array([20,  7], dtype=int32), 'currentDistance': 20.0}
episode index:72
target Thresh 19.539142247424724
target distance 8.0
model initialize at round 72
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 14.]), 'previousTarget': array([11., 14.]), 'currentState': array([ 3.49999997, 21.50000027,  0.        ]), 'targetState': array([11, 14], dtype=int32), 'currentDistance': 10.606601928532541}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.07887268018365297
{'scaleFactor': 20, 'currentTarget': array([11., 14.]), 'previousTarget': array([11., 14.]), 'currentState': array([10.49999988, 14.50001279,  0.        ]), 'targetState': array([11, 14], dtype=int32), 'currentDistance': 0.7071159060354917}
episode index:73
target Thresh 19.66312985369082
target distance 5.0
model initialize at round 73
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([10.5       ,  3.50000316,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 4.527692220211428}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07780683315414415
{'scaleFactor': 20, 'currentTarget': array([ 46.62070028, -31.13408723]), 'previousTarget': array([ 46.12900264, -30.6266193 ]), 'currentState': array([ 60.        , -45.99996558,   0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 20.0}
episode index:74
target Thresh 19.78588376266152
target distance 7.0
model initialize at round 74
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([11.5,  8.5,  0. ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 7.516648189186529}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0767694087120889
{'scaleFactor': 20, 'currentTarget': array([ 45.8336656 , -27.96227394]), 'previousTarget': array([ 45.32435182, -27.47311617]), 'currentState': array([ 61., -41.,   0.]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 20.0}
episode index:75
target Thresh 19.90741624983002
target distance 19.0
model initialize at round 75
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.89370359, 14.62789732]), 'previousTarget': array([23.30852571, 14.97927459]), 'currentState': array([ 6.49998093, 24.50000003,  0.        ]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07575928491324561
{'scaleFactor': 20, 'currentTarget': array([43.55501881, -9.3434412 ]), 'previousTarget': array([43.08062854, -8.82311891]), 'currentState': array([ 55.99989137, -24.9999142 ,   0.        ]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 20.0}
episode index:76
target Thresh 20.027739468546322
target distance 13.0
model initialize at round 76
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 21.]), 'previousTarget': array([21., 21.]), 'currentState': array([24.5       ,  7.50000009,  0.        ]), 'targetState': array([21, 21], dtype=int32), 'currentDistance': 13.946325594435715}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07477539809619048
{'scaleFactor': 20, 'currentTarget': array([ 61.12476739, -26.69547804]), 'previousTarget': array([ 60.63615824, -26.18590249]), 'currentState': array([ 74.        , -41.99999976,   0.        ]), 'targetState': array([21, 21], dtype=int32), 'currentDistance': 20.0}
episode index:77
target Thresh 20.14686545123256
target distance 15.0
model initialize at round 77
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 11.]), 'previousTarget': array([17., 11.]), 'currentState': array([25.5, 25.5,  0. ]), 'targetState': array([17, 11], dtype=int32), 'currentDistance': 16.80773631397158}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07381673914623932
{'scaleFactor': 20, 'currentTarget': array([ 57.87624678, -13.66670064]), 'previousTarget': array([ 57.35014149, -13.21008489]), 'currentState': array([ 75., -24.,   0.]), 'targetState': array([17, 11], dtype=int32), 'currentDistance': 20.0}
episode index:78
target Thresh 20.264806110586278
target distance 2.0
model initialize at round 78
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 28.]), 'previousTarget': array([23., 28.]), 'currentState': array([24.5       , 25.50000086,  0.        ]), 'targetState': array([23, 28], dtype=int32), 'currentDistance': 2.915475206319649}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07288235004312237
{'scaleFactor': 20, 'currentTarget': array([59.99582615, -9.72123178]), 'previousTarget': array([59.49718518, -9.21989901]), 'currentState': array([ 74.        , -23.99999624,   0.        ]), 'targetState': array([23, 28], dtype=int32), 'currentDistance': 20.0}
episode index:79
target Thresh 20.38157324077169
target distance 15.0
model initialize at round 79
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([23.5, 13.5,  0. ]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 15.8902485820708}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07197132066758334
{'scaleFactor': 20, 'currentTarget': array([ 57.49962624, -23.36123371]), 'previousTarget': array([ 56.98875993, -22.87457203]), 'currentState': array([ 73., -36.,   0.]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 20.0}
episode index:80
target Thresh 20.497178518599128
target distance 15.0
model initialize at round 80
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  6.]), 'previousTarget': array([22.,  6.]), 'currentState': array([7.49999902, 9.50000063, 0.        ]), 'targetState': array([22,  6], dtype=int32), 'currentDistance': 14.916434993045973}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07108278584452675
{'scaleFactor': 20, 'currentTarget': array([ 44.88953994, -24.08332267]), 'previousTarget': array([ 44.41608011, -23.56316391]), 'currentState': array([ 56.99999511, -39.99988246,   0.        ]), 'targetState': array([22,  6], dtype=int32), 'currentDistance': 20.0}
episode index:81
target Thresh 20.611633504692705
target distance 5.0
model initialize at round 81
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  7.]), 'previousTarget': array([12.,  7.]), 'currentState': array([11.5       ,  1.50000244,  0.        ]), 'targetState': array([12,  7], dtype=int32), 'currentDistance': 5.522678074839339}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07021592260252033
{'scaleFactor': 20, 'currentTarget': array([ 47.69587774, -33.06679388]), 'previousTarget': array([ 47.20421615, -32.55936928]), 'currentState': array([ 61.        , -47.99998948,   0.        ]), 'targetState': array([12,  7], dtype=int32), 'currentDistance': 20.0}
episode index:82
target Thresh 20.724949644646415
target distance 13.0
model initialize at round 82
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 29.]), 'previousTarget': array([17., 29.]), 'currentState': array([ 4.49999988, 22.50000268,  0.        ]), 'targetState': array([17, 29], dtype=int32), 'currentDistance': 14.089001671925601}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.06936994763140562
{'scaleFactor': 20, 'currentTarget': array([ 42.97484592, -10.31323814]), 'previousTarget': array([42.51042619, -9.78978414]), 'currentState': array([ 53.99999946, -26.99993911,   0.        ]), 'targetState': array([17, 29], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:83
target Thresh 20.837138270168687
target distance 11.0
model initialize at round 83
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  3.]), 'previousTarget': array([13.,  3.]), 'currentState': array([15.5, 13.5,  0. ]), 'targetState': array([13,  3], dtype=int32), 'currentDistance': 10.793516572461392}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.06854411492150794
{'scaleFactor': 20, 'currentTarget': array([ 49., -24.]), 'previousTarget': array([ 48.48135685, -23.52489784]), 'currentState': array([ 65., -36.,   0.]), 'targetState': array([13,  3], dtype=int32), 'currentDistance': 20.0}
episode index:84
target Thresh 20.94821060021556
target distance 8.0
model initialize at round 84
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([18.5, 22.5,  0. ]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 10.700467279516461}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0677377135694902
{'scaleFactor': 20, 'currentTarget': array([ 53.61197079, -13.10810973]), 'previousTarget': array([ 53.10981484, -12.61034302]), 'currentState': array([ 68., -27.,   0.]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 20.0}
episode index:85
target Thresh 21.058177742112598
target distance 19.0
model initialize at round 85
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'previousTarget': array([25.91410718,  9.23313766]), 'currentState': array([19.5, 27.5,  0. ]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 19.60867155112748}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.06695006573728683
{'scaleFactor': 20, 'currentTarget': array([ 52.77647243, -10.30396849]), 'previousTarget': array([52.2512035 , -9.83909899]), 'currentState': array([ 69., -22.,   0.]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 20.0}
episode index:86
target Thresh 21.16705069266563
target distance 5.0
model initialize at round 86
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 22.]), 'previousTarget': array([19., 22.]), 'currentState': array([18.5       , 26.50000021,  0.        ]), 'targetState': array([19, 22], dtype=int32), 'currentDistance': 4.527692776409088}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.06618052475180078
{'scaleFactor': 20, 'currentTarget': array([53.26940183, -9.47189779]), 'previousTarget': array([52.76322474, -8.97862704]), 'currentState': array([ 68.        , -22.99999735,   0.        ]), 'targetState': array([19, 22], dtype=int32), 'currentDistance': 20.0}
episode index:87
target Thresh 21.274840339260443
target distance 9.0
model initialize at round 87
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 27.]), 'previousTarget': array([19., 27.]), 'currentState': array([10.49999997, 21.5000045 ,  0.        ]), 'targetState': array([19, 27], dtype=int32), 'currentDistance': 10.124225945967433}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.06542847333416668
{'scaleFactor': 20, 'currentTarget': array([ 48.04669056, -11.96503558]), 'previousTarget': array([ 47.57080109, -11.44709043]), 'currentState': array([ 59.99999985, -27.9999474 ,   0.        ]), 'targetState': array([19, 27], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:88
target Thresh 21.381557460951516
target distance 11.0
model initialize at round 88
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([17.5       , 11.50000009,  0.        ]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 12.020815194638029}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.06469332194838953
{'scaleFactor': 20, 'currentTarget': array([ 53.88255429, -22.9025623 ]), 'previousTarget': array([ 53.39188627, -22.39445924]), 'currentState': array([ 67.        , -37.99999976,   0.        ]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 20.0}
episode index:89
target Thresh 21.487212729539955
target distance 8.0
model initialize at round 89
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  3.]), 'previousTarget': array([12.,  3.]), 'currentState': array([4.49999994, 5.5000014 , 0.        ]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 7.905694649910088}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.06397450726007409
{'scaleFactor': 20, 'currentTarget': array([ 40.67337417, -29.08684531]), 'previousTarget': array([ 40.18286714, -28.57836765]), 'currentState': array([ 53.99999979, -43.999962  ,   0.        ]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 20.0}
episode index:90
target Thresh 21.591816710640664
target distance 14.0
model initialize at round 90
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([27.5, 12.5,  0. ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 14.916433890176357}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.06327149069677657
{'scaleFactor': 20, 'currentTarget': array([ 60.75969257, -25.32727904]), 'previousTarget': array([ 60.24264733, -24.85103077]), 'currentState': array([ 77., -37.,   0.]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 20.0}
episode index:91
target Thresh 21.695379864738918
target distance 13.0
model initialize at round 91
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 18.]), 'previousTarget': array([ 3., 18.]), 'currentState': array([16.5, 15.5,  0. ]), 'targetState': array([ 3, 18], dtype=int32), 'currentDistance': 13.729530217746076}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0625837571022464
{'scaleFactor': 20, 'currentTarget': array([ 50.5755395 , -21.26869927]), 'previousTarget': array([ 50.06496027, -20.78152727]), 'currentState': array([ 66., -34.,   0.]), 'targetState': array([ 3, 18], dtype=int32), 'currentDistance': 20.0}
episode index:92
target Thresh 21.79791254823644
target distance 13.0
model initialize at round 92
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  3.]), 'previousTarget': array([10.,  3.]), 'currentState': array([23.5,  2.5,  0. ]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 13.509256086106378}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.06191081347749106
{'scaleFactor': 20, 'currentTarget': array([ 57.33422194, -34.56684281]), 'previousTarget': array([ 56.82162718, -34.08272873]), 'currentState': array([ 73., -47.,   0.]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 20.0}
episode index:93
target Thresh 21.89942501448701
target distance 9.0
model initialize at round 93
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 16.]), 'previousTarget': array([13., 16.]), 'currentState': array([ 7.5       , 24.50000012,  0.        ]), 'targetState': array([13, 16], dtype=int32), 'currentDistance': 10.12422846574287}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.061252187802198606
{'scaleFactor': 20, 'currentTarget': array([ 42.36783989, -11.36548353]), 'previousTarget': array([ 41.8621196 , -10.87162497]), 'currentState': array([ 57.        , -24.99999455,   0.        ]), 'targetState': array([13, 16], dtype=int32), 'currentDistance': 20.0}
episode index:94
target Thresh 21.999927414821858
target distance 5.0
model initialize at round 94
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 13.]), 'previousTarget': array([ 2., 13.]), 'currentState': array([ 6.5, 17.5,  0. ]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 6.36396103067887}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.060607427930596515
{'scaleFactor': 20, 'currentTarget': array([ 40.63557441, -19.19631201]), 'previousTarget': array([ 40.12380199, -18.71045212]), 'currentState': array([ 56., -32.,   0.]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 20.0}
episode index:95
target Thresh 22.099429799564767
target distance 16.0
model initialize at round 95
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 29.]), 'previousTarget': array([25., 29.]), 'currentState': array([21.5       , 12.50000015,  0.        ]), 'targetState': array([25, 29], dtype=int32), 'currentDistance': 16.867127647664706}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05997610055631947
{'scaleFactor': 20, 'currentTarget': array([ 59.56414671, -20.5920362 ]), 'previousTarget': array([ 59.08973407, -20.07423219]), 'currentState': array([ 71.        , -36.99999949,   0.        ]), 'targetState': array([25, 29], dtype=int32), 'currentDistance': 20.0}
episode index:96
target Thresh 22.19794211903713
target distance 6.0
model initialize at round 96
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  5.]), 'previousTarget': array([23.,  5.]), 'currentState': array([20.5       , 10.50000027,  0.        ]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 6.04152323097654}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.059357790241305865
{'scaleFactor': 20, 'currentTarget': array([ 55.3995715, -25.3315114]), 'previousTarget': array([ 54.8945712, -24.8368546]), 'currentState': array([ 70.       , -38.9999966,   0.       ]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 20.0}
episode index:97
target Thresh 22.29547422455299
target distance 6.0
model initialize at round 97
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([2.49999997, 1.50000158, 0.        ]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 5.700876738648066}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05875209850414968
{'scaleFactor': 20, 'currentTarget': array([ 38.93533272, -32.85685292]), 'previousTarget': array([ 38.4471422 , -32.34667236]), 'currentState': array([ 51.99999994, -47.99998569,   0.        ]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 20.0}
episode index:98
target Thresh 22.392035869404168
target distance 11.0
model initialize at round 98
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 23.]), 'previousTarget': array([11., 23.]), 'currentState': array([ 7.5       , 11.50000045,  0.        ]), 'targetState': array([11, 23], dtype=int32), 'currentDistance': 12.02081485250482}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.058158642963703724
{'scaleFactor': 20, 'currentTarget': array([ 44.95815625, -22.0314669 ]), 'previousTarget': array([ 44.47887422, -21.51586457]), 'currentState': array([ 57.        , -37.99999842,   0.        ]), 'targetState': array([11, 23], dtype=int32), 'currentDistance': 20.0}
episode index:99
target Thresh 22.487636709835627
target distance 14.0
model initialize at round 99
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 10.]), 'previousTarget': array([18., 10.]), 'currentState': array([4.49999991, 1.50000042, 0.        ]), 'targetState': array([18, 10], dtype=int32), 'currentDistance': 15.953055986895908}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05757705653406669
{'scaleFactor': 20, 'currentTarget': array([ 43.45274173, -31.00719121]), 'previousTarget': array([ 42.99329205, -30.48208897]), 'currentState': array([ 53.99999967, -47.9999941 ,   0.        ]), 'targetState': array([18, 10], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:100
target Thresh 22.582286306011078
target distance 12.0
model initialize at round 100
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([15.5, 12.5,  0. ]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 15.116216457831085}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05700698666739276
{'scaleFactor': 20, 'currentTarget': array([ 51.33685742, -22.39457173]), 'previousTarget': array([ 50.84094454, -21.89074937]), 'currentState': array([ 65., -37.,   0.]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 20.0}
episode index:101
target Thresh 22.675994122969
target distance 7.0
model initialize at round 101
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([ 9.49999997, 13.5000011 ,  0.        ]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 7.382411033573621}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05644809464124185
{'scaleFactor': 20, 'currentTarget': array([ 46.3991816 , -20.46875294]), 'previousTarget': array([ 45.91603415, -19.95509526]), 'currentState': array([ 58.99999991, -35.99999171,   0.        ]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 20.0}
episode index:102
target Thresh 22.768769531569195
target distance 3.0
model initialize at round 102
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 14.]), 'previousTarget': array([11., 14.]), 'currentState': array([14.5       , 10.50000003,  0.        ]), 'targetState': array([11, 14], dtype=int32), 'currentDistance': 4.949747447232506}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05590005488744339
{'scaleFactor': 20, 'currentTarget': array([ 49.85786436, -24.8578643 ]), 'previousTarget': array([ 49.35786436, -24.3578643 ]), 'currentState': array([ 64.        , -38.99999991,   0.        ]), 'targetState': array([11, 14], dtype=int32), 'currentDistance': 20.0}
episode index:103
target Thresh 22.860621809429826
target distance 9.0
model initialize at round 103
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([15.49999997,  4.50000075,  0.        ]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 10.124227985925664}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05536255435967951
{'scaleFactor': 20, 'currentTarget': array([ 53.04669701, -28.96507678]), 'previousTarget': array([ 52.5708076, -28.4471316]), 'currentState': array([ 64.99999988, -44.99999338,   0.        ]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 20.0}
episode index:104
target Thresh 22.951560141855225
target distance 22.0
model initialize at round 104
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.40138923, 19.61378166]), 'previousTarget': array([12.92760259, 20.13646016]), 'currentState': array([26.5,  4.5,  0. ]), 'targetState': array([ 7, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05483529193720637
{'scaleFactor': 20, 'currentTarget': array([ 62.16189274, -30.5602359 ]), 'previousTarget': array([ 61.66408639, -30.05813397]), 'currentState': array([ 76., -45.,   0.]), 'targetState': array([ 7, 27], dtype=int32), 'currentDistance': 20.0}
episode index:105
target Thresh 23.041593622754423
target distance 8.0
model initialize at round 105
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 24.]), 'previousTarget': array([14., 24.]), 'currentState': array([ 6.49999997, 20.50000075,  0.        ]), 'targetState': array([14, 24], dtype=int32), 'currentDistance': 8.276472390555687}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05431797786232707
{'scaleFactor': 20, 'currentTarget': array([ 43.57836625, -13.32507696]), 'previousTarget': array([ 43.09742568, -12.8099921 ]), 'currentState': array([ 55.99999988, -28.9999938 ,   0.        ]), 'targetState': array([14, 24], dtype=int32), 'currentDistance': 20.0}
episode index:106
target Thresh 23.13073125555053
target distance 11.0
model initialize at round 106
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 18.]), 'previousTarget': array([18., 18.]), 'currentState': array([ 7.49999991, 27.50000003,  0.        ]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 14.159802344799491}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.06145432328041028
{'scaleFactor': 20, 'currentTarget': array([18., 18.]), 'previousTarget': array([18., 18.]), 'currentState': array([17.49999952, 17.50000304,  0.        ]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 0.7071049688749995}
episode index:107
target Thresh 23.21898195408111
target distance 18.0
model initialize at round 107
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 22.]), 'previousTarget': array([ 6., 22.]), 'currentState': array([24.5, 25.5,  0. ]), 'targetState': array([ 6, 22], dtype=int32), 'currentDistance': 18.828170383762796}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.060885301768554626
{'scaleFactor': 20, 'currentTarget': array([ 57.43433174, -12.79381265]), 'previousTarget': array([ 56.91590918, -12.32109434]), 'currentState': array([ 74., -24.,   0.]), 'targetState': array([ 6, 22], dtype=int32), 'currentDistance': 20.0}
episode index:108
target Thresh 23.306354543489558
target distance 18.0
model initialize at round 108
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 25.]), 'previousTarget': array([19., 25.]), 'currentState': array([18.5,  6.5,  0. ]), 'targetState': array([19, 25], dtype=int32), 'currentDistance': 18.50675552332174}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0603267210183844
{'scaleFactor': 20, 'currentTarget': array([ 56.30762027, -26.77384037]), 'previousTarget': array([ 55.82975823, -26.25791094]), 'currentState': array([ 68., -43.,   0.]), 'targetState': array([19, 25], dtype=int32), 'currentDistance': 20.0}
episode index:109
target Thresh 23.392857761107628
target distance 8.0
model initialize at round 109
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  6.]), 'previousTarget': array([11.,  6.]), 'currentState': array([19.5,  7.5,  0. ]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 8.631338250816102}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05977829628185363
{'scaleFactor': 20, 'currentTarget': array([ 53.59211358, -29.24864572]), 'previousTarget': array([ 53.08076497, -28.76237106]), 'currentState': array([ 69., -42.,   0.]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 20.0}
episode index:110
target Thresh 23.478500257329163
target distance 12.0
model initialize at round 110
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([ 9.5       , 16.50000003,  0.        ]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 12.509995973418349}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0592397530721072
{'scaleFactor': 20, 'currentTarget': array([ 46.59891014, -17.30882505]), 'previousTarget': array([ 46.1153953 , -16.79581051]), 'currentState': array([ 59.        , -32.99999997,   0.        ]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 20.0}
episode index:111
target Thresh 23.56329059647516
target distance 6.0
model initialize at round 111
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'previousTarget': array([26., 11.]), 'currentState': array([20.49999997,  8.50000039,  0.        ]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 6.041522853608503}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05871082670539196
{'scaleFactor': 20, 'currentTarget': array([ 57.08115478, -25.73227207]), 'previousTarget': array([ 56.59445908, -25.22102452]), 'currentState': array([ 69.99999997, -40.99999747,   0.        ]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 20.0}
episode index:112
target Thresh 23.64723725765019
target distance 15.0
model initialize at round 112
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([27.5, 22.5,  0. ]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 15.572411502397511}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05819126186729115
{'scaleFactor': 20, 'currentTarget': array([ 60.91133466, -15.11913944]), 'previousTarget': array([ 60.39574603, -14.64027808]), 'currentState': array([ 77., -27.,   0.]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 20.0}
episode index:113
target Thresh 23.730348635590325
target distance 16.0
model initialize at round 113
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 26.]), 'previousTarget': array([19., 26.]), 'currentState': array([21.5,  9.5,  0. ]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 16.688319268278725}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05768081220178859
{'scaleFactor': 20, 'currentTarget': array([ 58.62255547, -24.29016656]), 'previousTarget': array([ 58.13826924, -23.77779874]), 'currentState': array([ 71., -40.,   0.]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 20.0}
episode index:114
target Thresh 23.812633041502618
target distance 14.0
model initialize at round 114
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([ 6.5, 22.5,  0. ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 13.583077707206048}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05717923992177304
{'scaleFactor': 20, 'currentTarget': array([ 40., -15.]), 'previousTarget': array([ 39.47978669, -14.52699847]), 'currentState': array([ 56., -27.,   0.]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 20.0}
episode index:115
target Thresh 23.894098703896233
target distance 7.0
model initialize at round 115
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 15.]), 'previousTarget': array([19., 15.]), 'currentState': array([12.49999997, 15.50000021,  0.        ]), 'targetState': array([19, 15], dtype=int32), 'currentDistance': 6.519202450917278}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.056686315439688786
{'scaleFactor': 20, 'currentTarget': array([ 48.80820183, -18.9674844 ]), 'previousTarget': array([ 48.31893247, -18.45807449]), 'currentState': array([ 61.99999994, -33.99999791,   0.        ]), 'targetState': array([19, 15], dtype=int32), 'currentDistance': 20.0}
episode index:116
target Thresh 23.974753769405304
target distance 16.0
model initialize at round 116
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([22.5, 20.5,  0. ]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 17.10263137648717}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05620181701712735
{'scaleFactor': 20, 'currentTarget': array([ 56.52085402, -16.33524419]), 'previousTarget': array([ 56.01032297, -15.8481264 ]), 'currentState': array([ 72., -29.,   0.]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 20.0}
episode index:117
target Thresh 24.054606303603585
target distance 7.0
model initialize at round 117
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 21.]), 'previousTarget': array([25., 21.]), 'currentState': array([24.5, 27.5,  0. ]), 'targetState': array([25, 21], dtype=int32), 'currentDistance': 6.519202405202565}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.055725530432236436
{'scaleFactor': 20, 'currentTarget': array([58.96748622, -8.80820219]), 'previousTarget': array([58.4580763 , -8.31893284]), 'currentState': array([ 74., -22.,   0.]), 'targetState': array([25, 21], dtype=int32), 'currentDistance': 20.0}
episode index:118
target Thresh 24.13366429181104
target distance 7.0
model initialize at round 118
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 16.]), 'previousTarget': array([18., 16.]), 'currentState': array([15.5       ,  8.50000006,  0.        ]), 'targetState': array([18, 16], dtype=int32), 'currentDistance': 7.905694093875023}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05525724866389831
{'scaleFactor': 20, 'currentTarget': array([ 52.27636832, -25.56921251]), 'previousTarget': array([ 51.79064567, -25.05745105]), 'currentState': array([ 65.        , -40.99999982,   0.        ]), 'targetState': array([18, 16], dtype=int32), 'currentDistance': 20.0}
episode index:119
target Thresh 24.211935639892374
target distance 9.0
model initialize at round 119
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  3.]), 'previousTarget': array([17.,  3.]), 'currentState': array([26.5,  5.5,  0. ]), 'targetState': array([17,  3], dtype=int32), 'currentDistance': 9.823441352194328}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05479677159169916
{'scaleFactor': 20, 'currentTarget': array([ 60.35679005, -31.53845987]), 'previousTarget': array([ 59.8435349 , -31.05511749]), 'currentState': array([ 76., -44.,   0.]), 'targetState': array([17,  3], dtype=int32), 'currentDistance': 20.0}
episode index:120
target Thresh 24.289428175047625
target distance 8.0
model initialize at round 120
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 4.5, 17.5,  0. ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 7.6485292703890995}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05434390571077603
{'scaleFactor': 20, 'currentTarget': array([ 38.56139529, -19.28585494]), 'previousTarget': array([ 38.04815518, -18.8019493 ]), 'currentState': array([ 54., -32.,   0.]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 20.0}
episode index:121
target Thresh 24.366149646594884
target distance 9.0
model initialize at round 121
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([24.5,  6.5,  0. ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 9.823441352194331}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0538984638606877
{'scaleFactor': 20, 'currentTarget': array([ 58.35679005, -30.53845987]), 'previousTarget': array([ 57.8435349 , -30.05511749]), 'currentState': array([ 74., -43.,   0.]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 20.0}
episode index:122
target Thresh 24.442107726745235
target distance 16.0
model initialize at round 122
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.63561484,  7.41836814]), 'previousTarget': array([18.17009216,  7.94846611]), 'currentState': array([ 5.49999988, 22.5       ,  0.        ]), 'targetState': array([19,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05346026496751138
{'scaleFactor': 20, 'currentTarget': array([ 40.45973639, -13.26752892]), 'previousTarget': array([ 39.95405602, -12.77354592]), 'currentState': array([ 54.9999994 , -26.99999961,   0.        ]), 'targetState': array([19,  7], dtype=int32), 'currentDistance': 20.0}
episode index:123
target Thresh 24.51731001137
target distance 9.0
model initialize at round 123
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([20.5, 28.5,  0. ]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 10.70046727951624}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.053029133798418546
{'scaleFactor': 20, 'currentTarget': array([55.52524904, -7.19849303]), 'previousTarget': array([55.02129278, -6.70264347]), 'currentState': array([ 70.        , -20.99999964,   0.        ]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 20.0}
episode index:124
target Thresh 24.591764020760305
target distance 23.0
model initialize at round 124
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.94433475,  7.50493644]), 'previousTarget': array([14.86874449,  8.01887683]), 'currentState': array([14.5, 27.5,  0. ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.052604900728031194
{'scaleFactor': 20, 'currentTarget': array([ 46.48323487, -12.34790493]), 'previousTarget': array([ 45.94900795, -11.91028269]), 'currentState': array([ 64., -22.,   0.]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 20.0}
episode index:125
target Thresh 24.665477200379133
target distance 15.0
model initialize at round 125
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([19.5,  6.5,  0. ]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 15.890248582070715}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05218740151590397
{'scaleFactor': 20, 'currentTarget': array([ 56.36123371, -27.49962624]), 'previousTarget': array([ 55.87457203, -26.98875993]), 'currentState': array([ 69., -43.,   0.]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 20.0}
episode index:126
target Thresh 24.738456921605877
target distance 10.0
model initialize at round 126
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 18.]), 'previousTarget': array([ 9., 18.]), 'currentState': array([19.5, 11.5,  0. ]), 'targetState': array([ 9, 18], dtype=int32), 'currentDistance': 12.349089035228577}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.051776477094518894
{'scaleFactor': 20, 'currentTarget': array([ 54.37889464, -24.35363499]), 'previousTarget': array([ 53.87480833, -23.85801449]), 'currentState': array([ 69., -38.,   0.]), 'targetState': array([ 9, 18], dtype=int32), 'currentDistance': 20.0}
episode index:127
target Thresh 24.81071048247348
target distance 12.0
model initialize at round 127
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([24.5, 22.5,  0. ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 15.572411502397435}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.051371973367217966
{'scaleFactor': 20, 'currentTarget': array([ 57.10361849, -16.29895838]), 'previousTarget': array([ 56.58009366, -15.83619348]), 'currentState': array([ 74., -27.,   0.]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 20.0}
episode index:128
target Thresh 24.88224510839823
target distance 4.0
model initialize at round 128
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([ 5.5, 11.5,  0. ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 4.301162633521245}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.050973741015534105
{'scaleFactor': 20, 'currentTarget': array([ 40.02006875, -24.74852236]), 'previousTarget': array([ 39.5117392 , -24.25794434]), 'currentState': array([ 55., -38.,   0.]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 20.0}
episode index:129
target Thresh 24.95306795290234
target distance 5.0
model initialize at round 129
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([15.5       , 13.50000006,  0.        ]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 4.527692575650858}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05058163531541461
{'scaleFactor': 20, 'currentTarget': array([ 51.47189995, -21.26940192]), 'previousTarget': array([ 50.9786292 , -20.76322484]), 'currentState': array([ 65.        , -35.99999964,   0.        ]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 20.0}
episode index:130
target Thresh 25.023186098329276
target distance 20.0
model initialize at round 130
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4.35378787, 27.90922026]), 'previousTarget': array([ 3.7615699 , 28.20729355]), 'currentState': array([22.5, 19.5,  0. ]), 'targetState': array([ 2, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05019551596186183
{'scaleFactor': 20, 'currentTarget': array([ 56.70744274, -17.11055888]), 'previousTarget': array([ 56.19892183, -16.62067521]), 'currentState': array([ 72., -30.,   0.]), 'targetState': array([ 2, 29], dtype=int32), 'currentDistance': 20.0}
episode index:131
target Thresh 25.092606556552017
target distance 12.0
model initialize at round 131
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  7.]), 'previousTarget': array([18.,  7.]), 'currentState': array([15.5, 18.5,  0. ]), 'targetState': array([18,  7], dtype=int32), 'currentDistance': 11.768602295939724}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.049815246901544694
{'scaleFactor': 20, 'currentTarget': array([ 49.44739946, -18.42555701]), 'previousTarget': array([ 48.93173921, -17.94495098]), 'currentState': array([ 65., -31.,   0.]), 'targetState': array([18,  7], dtype=int32), 'currentDistance': 20.0}
episode index:132
target Thresh 25.16133626967423
target distance 18.0
model initialize at round 132
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.97063166, 13.75600292]), 'previousTarget': array([11.85786438, 13.85786438]), 'currentState': array([26.5, 27.5,  0. ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04944069617296165
{'scaleFactor': 20, 'currentTarget': array([ 57.90362596, -13.48405927]), 'previousTarget': array([ 57.3763372 , -13.04229069]), 'currentState': array([ 76., -22.,   0.]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 20.0}
episode index:133
target Thresh 25.229382110724508
target distance 16.0
model initialize at round 133
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 16.]), 'previousTarget': array([27., 16.]), 'currentState': array([11.49999994,  5.5       ,  0.        ]), 'targetState': array([27, 16], dtype=int32), 'currentDistance': 18.721645276196824}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04907173575376045
{'scaleFactor': 20, 'currentTarget': array([ 51.13974837, -26.59955625]), 'previousTarget': array([ 50.68782267, -26.07240175]), 'currentState': array([ 60.99999976, -44.        ,   0.        ]), 'targetState': array([27, 16], dtype=int32), 'currentDistance': 20.0}
episode index:134
target Thresh 25.296750884343655
target distance 3.0
model initialize at round 134
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 23.]), 'previousTarget': array([27., 23.]), 'currentState': array([27.5, 19.5,  0. ]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 3.53553390593283}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0487082414148437
{'scaleFactor': 20, 'currentTarget': array([ 63.27559146, -15.45212695]), 'previousTarget': array([ 62.77974263, -14.94821188]), 'currentState': array([ 77., -30.,   0.]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 20.0}
episode index:135
target Thresh 25.363449327465176
target distance 16.0
model initialize at round 135
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 12.]), 'previousTarget': array([14., 12.]), 'currentState': array([18.5, 27.5,  0. ]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 16.14001239156892}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.048350092580911025
{'scaleFactor': 20, 'currentTarget': array([ 51.0753432 , -11.34373461]), 'previousTarget': array([ 50.54893996, -10.88578484]), 'currentState': array([ 68., -22.,   0.]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 20.0}
episode index:136
target Thresh 25.42948410998897
target distance 17.0
model initialize at round 136
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2.78589041, 15.48355773]), 'previousTarget': array([ 2.20859686, 15.86502556]), 'currentState': array([19.5,  4.5,  0. ]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04799717219710876
{'scaleFactor': 20, 'currentTarget': array([ 54.21119033, -31.53556134]), 'previousTarget': array([ 53.70623266, -31.04100866]), 'currentState': array([ 69., -45.,   0.]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:137
target Thresh 25.49486183544831
target distance 18.0
model initialize at round 137
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 9.5, 28.5,  0. ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 17.67766952966362}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04764936660147753
{'scaleFactor': 20, 'currentTarget': array([ 41.96683367, -10.51805149]), 'previousTarget': array([ 41.43845926, -10.06430033]), 'currentState': array([ 59., -21.,   0.]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 20.0}
episode index:138
target Thresh 25.559589041670232
target distance 12.0
model initialize at round 138
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 13.]), 'previousTarget': array([ 9., 13.]), 'currentState': array([11.5, 24.5,  0. ]), 'targetState': array([ 9, 13], dtype=int32), 'currentDistance': 11.768602295939758}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04730656540290575
{'scaleFactor': 20, 'currentTarget': array([ 44.85217755, -13.19966821]), 'previousTarget': array([ 44.33206891, -12.72723464]), 'currentState': array([ 61., -25.,   0.]), 'targetState': array([ 9, 13], dtype=int32), 'currentDistance': 20.0}
episode index:139
target Thresh 25.623672201429294
target distance 9.0
model initialize at round 139
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 12.]), 'previousTarget': array([18., 12.]), 'currentState': array([ 9.49999997, 18.50000003,  0.        ]), 'targetState': array([18, 12], dtype=int32), 'currentDistance': 10.7004673212934}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04696866136431357
{'scaleFactor': 20, 'currentTarget': array([ 45.19849312, -16.52524865]), 'previousTarget': array([ 44.70264356, -16.02129239]), 'currentState': array([ 58.99999985, -30.99999949,   0.        ]), 'targetState': array([18, 12], dtype=int32), 'currentDistance': 20.0}
episode index:140
target Thresh 25.68711772309487
target distance 12.0
model initialize at round 140
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 15.]), 'previousTarget': array([21., 15.]), 'currentState': array([22.5,  2.5,  0. ]), 'targetState': array([21, 15], dtype=int32), 'currentDistance': 12.589678312014176}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0466355502908078
{'scaleFactor': 20, 'currentTarget': array([ 59.29458922, -31.5542065 ]), 'previousTarget': array([ 58.80789154, -31.04327386]), 'currentState': array([ 72., -47.,   0.]), 'targetState': array([21, 15], dtype=int32), 'currentDistance': 20.0}
episode index:141
target Thresh 25.74993195127201
target distance 19.0
model initialize at round 141
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.8435349 , 25.94488251]), 'previousTarget': array([11.30234469, 26.39288577]), 'currentState': array([27.5, 13.5,  0. ]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.046307130922562674
{'scaleFactor': 20, 'currentTarget': array([ 62.44219562, -22.28612631]), 'previousTarget': array([ 61.93912078, -21.7893911 ]), 'currentState': array([ 77., -36.,   0.]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 20.0}
episode index:142
target Thresh 25.812121167435865
target distance 10.0
model initialize at round 142
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 26.]), 'previousTarget': array([13., 26.]), 'currentState': array([ 3.49999991, 25.50000009,  0.        ]), 'targetState': array([13, 26], dtype=int32), 'currentDistance': 9.513148879804481}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0459833048321951
{'scaleFactor': 20, 'currentTarget': array([40.5060986 , -8.38262281]), 'previousTarget': array([40.02536502, -7.86722915]), 'currentState': array([ 52.99999964, -23.99999893,   0.        ]), 'targetState': array([13, 26], dtype=int32), 'currentDistance': 20.0}
episode index:143
target Thresh 25.873691590559883
target distance 18.0
model initialize at round 143
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.87523613,  4.22533058]), 'previousTarget': array([11.90599608,  4.35899411]), 'currentState': array([23.5, 20.5,  0. ]), 'targetState': array([11,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04566397632641597
{'scaleFactor': 20, 'currentTarget': array([ 55.22758686, -19.8271416 ]), 'previousTarget': array([ 54.6991327, -19.3824826]), 'currentState': array([ 73., -29.,   0.]), 'targetState': array([11,  3], dtype=int32), 'currentDistance': 20.0}
episode index:144
target Thresh 25.93464937773768
target distance 8.0
model initialize at round 144
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 14.]), 'previousTarget': array([ 3., 14.]), 'currentState': array([3.5, 5.5, 0. ]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 8.514693182963196}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04534905235175103
{'scaleFactor': 20, 'currentTarget': array([ 39.94120756, -28.85180077]), 'previousTarget': array([ 39.4516397, -28.3428138]), 'currentState': array([ 53., -44.,   0.]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 20.0}
episode index:145
target Thresh 25.99500062479878
target distance 18.0
model initialize at round 145
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  6.]), 'previousTarget': array([10.,  6.]), 'currentState': array([18.5, 23.5,  0. ]), 'targetState': array([10,  6], dtype=int32), 'currentDistance': 19.455076458343683}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0450384424041363
{'scaleFactor': 20, 'currentTarget': array([ 50.48844632, -16.33845314]), 'previousTarget': array([ 49.95960477, -15.89091392]), 'currentState': array([ 68., -26.,   0.]), 'targetState': array([10,  6], dtype=int32), 'currentDistance': 20.0}
episode index:146
target Thresh 26.054751366918175
target distance 2.0
model initialize at round 146
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([14.5, 11.5,  0. ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 2.9154759474226775}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0447320584422034
{'scaleFactor': 20, 'currentTarget': array([ 49.30393111, -24.43439795]), 'previousTarget': array([ 48.7984601 , -23.94032727]), 'currentState': array([ 64., -38.,   0.]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 20.0}
episode index:147
target Thresh 26.11390757921987
target distance 10.0
model initialize at round 147
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  4.]), 'previousTarget': array([25.,  4.]), 'currentState': array([26.5, 13.5,  0. ]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 9.617692030835606}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.044429814804080404
{'scaleFactor': 20, 'currentTarget': array([ 60.26293165, -23.6572013 ]), 'previousTarget': array([ 59.74660918, -23.17804084]), 'currentState': array([ 76., -36.,   0.]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 20.0}
episode index:148
target Thresh 26.172475177374395
target distance 18.0
model initialize at round 148
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7.62788961, 21.8937183 ]), 'previousTarget': array([ 7.28714138, 22.48314552]), 'currentState': array([17.5,  4.5,  0. ]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04413162812754295
{'scaleFactor': 20, 'currentTarget': array([ 53.76756726, -30.00324289]), 'previousTarget': array([ 53.2749202 , -29.49675821]), 'currentState': array([ 67., -45.,   0.]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 20.0}
episode index:149
target Thresh 26.230460018190367
target distance 12.0
model initialize at round 149
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 29.]), 'previousTarget': array([ 5., 29.]), 'currentState': array([17.5, 27.5,  0. ]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 12.589678312014263}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04383741727335933
{'scaleFactor': 20, 'currentTarget': array([51.5542065 , -9.29458922]), 'previousTarget': array([51.04327386, -8.80789154]), 'currentState': array([ 67., -22.,   0.]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 20.0}
episode index:150
target Thresh 26.287867900200197
target distance 14.0
model initialize at round 150
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 26.]), 'previousTarget': array([16., 26.]), 'currentState': array([17.5, 11.5,  0. ]), 'targetState': array([16, 26], dtype=int32), 'currentDistance': 14.57737973711334}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04354710325168146
{'scaleFactor': 20, 'currentTarget': array([ 54.5359259 , -22.35880897]), 'previousTarget': array([ 54.05124442, -21.84661428]), 'currentState': array([ 67., -38.,   0.]), 'targetState': array([16, 26], dtype=int32), 'currentDistance': 20.0}
episode index:151
target Thresh 26.34470456423992
target distance 11.0
model initialize at round 151
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 18.]), 'previousTarget': array([17., 18.]), 'currentState': array([23.5, 28.5,  0. ]), 'targetState': array([17, 18], dtype=int32), 'currentDistance': 12.349089035228445}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.043260609151341445
{'scaleFactor': 20, 'currentTarget': array([56.58787169, -9.57012492]), 'previousTarget': array([56.06682273, -9.10040856]), 'currentState': array([ 73., -21.,   0.]), 'targetState': array([17, 18], dtype=int32), 'currentDistance': 20.0}
episode index:152
target Thresh 26.400975694023302
target distance 15.0
model initialize at round 152
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 18.]), 'previousTarget': array([ 5., 18.]), 'currentState': array([5.5, 2.5, 0. ]), 'targetState': array([ 5, 18], dtype=int32), 'currentDistance': 15.508062419270818}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04297786007192091
{'scaleFactor': 20, 'currentTarget': array([ 42.80578478, -31.14752022]), 'previousTarget': array([ 42.3236243 , -30.63381348]), 'currentState': array([ 55., -47.,   0.]), 'targetState': array([ 5, 18], dtype=int32), 'currentDistance': 20.0}
episode index:153
target Thresh 26.45668691671022
target distance 24.0
model initialize at round 153
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.87523613, 18.77466942]), 'previousTarget': array([14.43965318, 19.32048962]), 'currentState': array([26.5,  2.5,  0. ]), 'targetState': array([ 9, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04269878305846688
{'scaleFactor': 20, 'currentTarget': array([ 62.57648826, -32.17403181]), 'previousTarget': array([ 62.08173352, -31.66928442]), 'currentState': array([ 76., -47.,   0.]), 'targetState': array([ 9, 27], dtype=int32), 'currentDistance': 20.0}
episode index:154
target Thresh 26.511843803469365
target distance 1.0
model initialize at round 154
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 7.5, 10.5,  0. ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 1.5811388300841602}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.042423307038734835
{'scaleFactor': 20, 'currentTarget': array([ 42.57790579, -25.14347812]), 'previousTarget': array([ 42.07510792, -24.64639078]), 'currentState': array([ 57., -39.,   0.]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 20.0}
episode index:155
target Thresh 26.56645187003538
target distance 21.0
model initialize at round 155
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5.52864968, 16.72329403]), 'previousTarget': array([ 5.28336929, 16.71986011]), 'currentState': array([23.5, 25.5,  0. ]), 'targetState': array([ 2, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.042151362762845514
{'scaleFactor': 20, 'currentTarget': array([ 55.47047164, -14.37110414]), 'previousTarget': array([ 54.9468483 , -13.91423631]), 'currentState': array([ 73., -24.,   0.]), 'targetState': array([ 2, 15], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:156
target Thresh 26.620516577260425
target distance 12.0
model initialize at round 156
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'previousTarget': array([12.,  6.]), 'currentState': array([24.5,  2.5,  0. ]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 12.980754985747247}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.041882882745247776
{'scaleFactor': 20, 'currentTarget': array([ 58.79757311, -34.00437702]), 'previousTarget': array([ 58.28870996, -33.5147524 ]), 'currentState': array([ 74., -47.,   0.]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 20.0}
episode index:157
target Thresh 26.674043331660275
target distance 5.0
model initialize at round 157
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 8.]), 'previousTarget': array([8., 8.]), 'currentState': array([5.5       , 2.50000003, 0.        ]), 'targetState': array([8, 8], dtype=int32), 'currentDistance': 6.041522959666266}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04161780120888545
{'scaleFactor': 20, 'currentTarget': array([ 42.00695344, -31.795371  ]), 'previousTarget': array([ 41.51869154, -31.2853481 ]), 'currentState': array([ 55.        , -46.99999994,   0.        ]), 'targetState': array([8, 8], dtype=int32), 'currentDistance': 20.0}
episode index:158
target Thresh 26.727037485954984
target distance 7.0
model initialize at round 158
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([23.5,  2.5,  0. ]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 8.746427842267988}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04135605403147107
{'scaleFactor': 20, 'currentTarget': array([ 59.24510704, -32.48094632]), 'previousTarget': array([ 58.74867216, -31.97756967]), 'currentState': array([ 73., -47.,   0.]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 20.0}
episode index:159
target Thresh 26.779504339604138
target distance 16.0
model initialize at round 159
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 24.]), 'previousTarget': array([22., 24.]), 'currentState': array([ 6.49999964, 20.50000003,  0.        ]), 'targetState': array([22, 24], dtype=int32), 'currentDistance': 15.89024892435135}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04109757869377438
{'scaleFactor': 20, 'currentTarget': array([ 45.20090475, -12.16611765]), 'previousTarget': array([ 44.74171711, -11.64000586]), 'currentState': array([ 55.99999833, -28.99999949,   0.        ]), 'targetState': array([22, 24], dtype=int32), 'currentDistance': 20.0}
episode index:160
target Thresh 26.831449139336822
target distance 14.0
model initialize at round 160
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 15.]), 'previousTarget': array([ 5., 15.]), 'currentState': array([ 8.5, 28.5,  0. ]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 13.946325680981298}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.040842314229837896
{'scaleFactor': 20, 'currentTarget': array([41.45566617, -9.76233929]), 'previousTarget': array([40.93217825, -9.29699672]), 'currentState': array([ 58., -21.,   0.]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:161
target Thresh 26.8828770796763
target distance 14.0
model initialize at round 161
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 26.]), 'previousTarget': array([13., 26.]), 'currentState': array([13.5, 11.5,  0. ]), 'targetState': array([13, 26], dtype=int32), 'currentDistance': 14.508618128546992}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04059020117903643
{'scaleFactor': 20, 'currentTarget': array([ 50.68711916, -22.23951253]), 'previousTarget': array([ 50.20399781, -21.72634062]), 'currentState': array([ 63., -38.,   0.]), 'targetState': array([13, 26], dtype=int32), 'currentDistance': 20.0}
episode index:162
target Thresh 26.933793303459463
target distance 6.0
model initialize at round 162
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([10.5,  3.5,  0. ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 8.514693182963258}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04034118153990124
{'scaleFactor': 20, 'currentTarget': array([ 45.73103679, -31.9858397 ]), 'previousTarget': array([ 45.22988917, -31.48700828]), 'currentState': array([ 60., -46.,   0.]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:163
target Thresh 26.98420290235112
target distance 16.0
model initialize at round 163
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.10542787, 10.83685776]), 'previousTarget': array([23.59074408, 11.32117742]), 'currentState': array([ 9.49999928, 24.5       ,  0.        ]), 'targetState': array([25, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.0446055668560499
{'scaleFactor': 20, 'currentTarget': array([25., 10.]), 'previousTarget': array([25., 10.]), 'currentState': array([24.49999538,  9.50000039,  0.        ]), 'targetState': array([25, 10], dtype=int32), 'currentDistance': 0.7071097736221131}
episode index:164
target Thresh 27.034110917353168
target distance 20.0
model initialize at round 164
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.9598119 , 21.08023371]), 'previousTarget': array([12.50609905, 21.61737619]), 'currentState': array([25.5,  5.5,  0. ]), 'targetState': array([ 9, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.044335230087225355
{'scaleFactor': 20, 'currentTarget': array([ 61.27974263, -29.44821188]), 'previousTarget': array([ 60.78291051, -28.94522566]), 'currentState': array([ 75., -44.,   0.]), 'targetState': array([ 9, 26], dtype=int32), 'currentDistance': 20.0}
episode index:165
target Thresh 27.083522339308693
target distance 11.0
model initialize at round 165
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 25.]), 'previousTarget': array([ 5., 25.]), 'currentState': array([16.5, 16.5,  0. ]), 'targetState': array([ 5, 25], dtype=int32), 'currentDistance': 14.300349646075205}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04406815038790472
{'scaleFactor': 20, 'currentTarget': array([ 51.50594505, -19.21876743]), 'previousTarget': array([ 51.00300298, -18.72186234]), 'currentState': array([ 66., -33.,   0.]), 'targetState': array([ 5, 25], dtype=int32), 'currentDistance': 20.0}
episode index:166
target Thresh 27.132442109401072
target distance 9.0
model initialize at round 166
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 12.]), 'previousTarget': array([15., 12.]), 'currentState': array([6.49999997, 7.50000006, 0.        ]), 'targetState': array([15, 12], dtype=int32), 'currentDistance': 9.617692029286363}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.043804269247857394
{'scaleFactor': 20, 'currentTarget': array([ 43.90581504, -26.07107326]), 'previousTarget': array([ 43.42858547, -25.55381023]), 'currentState': array([ 55.99999988, -41.99999955,   0.        ]), 'targetState': array([15, 12], dtype=int32), 'currentDistance': 20.0}
episode index:167
target Thresh 27.180875119648082
target distance 18.0
model initialize at round 167
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([14.5, 21.5,  0. ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 17.56416807025021}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04354352954995348
{'scaleFactor': 20, 'currentTarget': array([ 47.35899411, -16.90599608]), 'previousTarget': array([ 46.83205074, -16.44651786]), 'currentState': array([ 64., -28.,   0.]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:168
target Thresh 27.228826213391105
target distance 9.0
model initialize at round 168
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 16.]), 'previousTarget': array([11., 16.]), 'currentState': array([20.5, 22.5,  0. ]), 'targetState': array([11, 16], dtype=int32), 'currentDistance': 11.510864433221359}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04328587552894784
{'scaleFactor': 20, 'currentTarget': array([ 53.83713287, -15.22028328]), 'previousTarget': array([ 53.31930001, -14.74479061]), 'currentState': array([ 70., -27.,   0.]), 'targetState': array([11, 16], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:169
target Thresh 27.276300185779476
target distance 13.0
model initialize at round 169
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([23.5, 23.5,  0. ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 15.116216457830955}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04303125273171873
{'scaleFactor': 20, 'currentTarget': array([ 56.13875119, -15.2436861 ]), 'previousTarget': array([ 55.61468916, -14.78149834]), 'currentState': array([ 73., -26.,   0.]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 20.0}
episode index:170
target Thresh 27.323301784249992
target distance 1.0
model initialize at round 170
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 27.]), 'previousTarget': array([26., 27.]), 'currentState': array([25.5, 25.5,  0. ]), 'targetState': array([26, 27], dtype=int32), 'currentDistance': 1.5811388300842424}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.042779607978901665
{'scaleFactor': 20, 'currentTarget': array([61.14347812, -9.57790579]), 'previousTarget': array([60.64639078, -9.07510792]), 'currentState': array([ 75., -24.,   0.]), 'targetState': array([26, 27], dtype=int32), 'currentDistance': 20.0}
episode index:171
target Thresh 27.369835709001674
target distance 8.0
model initialize at round 171
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 18.]), 'previousTarget': array([12., 18.]), 'currentState': array([20.5, 12.5,  0. ]), 'targetState': array([12, 18], dtype=int32), 'currentDistance': 10.124228365658404}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04253088932786154
{'scaleFactor': 20, 'currentTarget': array([ 55.48752298, -23.23816834]), 'previousTarget': array([ 54.98426357, -22.74160634]), 'currentState': array([ 70., -37.,   0.]), 'targetState': array([12, 18], dtype=int32), 'currentDistance': 20.0}
episode index:172
target Thresh 27.415906613465772
target distance 17.0
model initialize at round 172
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 12.]), 'previousTarget': array([22., 12.]), 'currentState': array([ 5.49999887, 12.50000003,  0.        ]), 'targetState': array([22, 12], dtype=int32), 'currentDistance': 16.507575152090475}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04228504603694904
{'scaleFactor': 20, 'currentTarget': array([ 43.82798862, -20.41125981]), 'previousTarget': array([ 43.36649637, -19.88539088]), 'currentState': array([ 54.99999526, -36.99999899,   0.        ]), 'targetState': array([22, 12], dtype=int32), 'currentDistance': 20.0}
episode index:173
target Thresh 27.461519104771128
target distance 16.0
model initialize at round 173
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.55815704,  7.42284624]), 'previousTarget': array([11.47772  ,  7.3881475]), 'currentState': array([27.5, 19.5,  0. ]), 'targetState': array([11,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04204202853098957
{'scaleFactor': 20, 'currentTarget': array([ 59.55438895, -20.21988471]), 'previousTarget': array([ 59.02945056, -19.76450298]), 'currentState': array([ 77., -30.,   0.]), 'targetState': array([11,  7], dtype=int32), 'currentDistance': 20.0}
episode index:174
target Thresh 27.50667774420488
target distance 11.0
model initialize at round 174
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 27.]), 'previousTarget': array([ 9., 27.]), 'currentState': array([20.5, 21.5,  0. ]), 'targetState': array([ 9, 27], dtype=int32), 'currentDistance': 12.747548783982065}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04180178836795534
{'scaleFactor': 20, 'currentTarget': array([ 55.14623112, -14.60725756]), 'previousTarget': array([ 54.6402251 , -14.11392179]), 'currentState': array([ 70., -28.,   0.]), 'targetState': array([ 9, 27], dtype=int32), 'currentDistance': 20.0}
episode index:175
target Thresh 27.551387047668605
target distance 11.0
model initialize at round 175
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'previousTarget': array([7., 3.]), 'currentState': array([ 8.5, 13.5,  0. ]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 10.606601717798146}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.041564278206773775
{'scaleFactor': 20, 'currentTarget': array([ 42.11284334, -23.85099785]), 'previousTarget': array([ 41.59498124, -23.37439164]), 'currentState': array([ 58., -36.,   0.]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 20.0}
episode index:176
target Thresh 27.595651486129906
target distance 19.0
model initialize at round 176
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3.890578  , 11.01800354]), 'previousTarget': array([ 3.69147429, 10.97927459]), 'currentState': array([21.5, 20.5,  0. ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04132945177622703
{'scaleFactor': 20, 'currentTarget': array([ 53.58874323, -19.15885487]), 'previousTarget': array([ 53.06509161, -18.70081791]), 'currentState': array([ 71., -29.,   0.]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 20.0}
episode index:177
target Thresh 27.639475486069518
target distance 17.0
model initialize at round 177
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.07696097, 13.80649112]), 'previousTarget': array([23.56399985, 14.29270602]), 'currentState': array([ 9.4999992, 27.5      ,  0.       ]), 'targetState': array([26, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.04517018708061819
{'scaleFactor': 20, 'currentTarget': array([26., 12.]), 'previousTarget': array([26., 12.]), 'currentState': array([25.49999312, 11.50000036,  0.        ]), 'targetState': array([26, 12], dtype=int32), 'currentDistance': 0.7071113962857893}
episode index:178
target Thresh 27.682863429923955
target distance 6.0
model initialize at round 178
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 17.]), 'previousTarget': array([ 5., 17.]), 'currentState': array([ 6.5, 22.5,  0. ]), 'targetState': array([ 5, 17], dtype=int32), 'currentDistance': 5.700877125495613}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04491783966675999
{'scaleFactor': 20, 'currentTarget': array([ 40.85686542, -13.93533487]), 'previousTarget': array([ 40.34668485, -13.44714438]), 'currentState': array([ 56., -27.,   0.]), 'targetState': array([ 5, 17], dtype=int32), 'currentDistance': 20.0}
episode index:179
target Thresh 27.725819656523758
target distance 19.0
model initialize at round 179
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.94131758,  5.51429349]), 'previousTarget': array([15.97927459,  5.69147429]), 'currentState': array([26.5, 22.5,  0. ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04466829611305577
{'scaleFactor': 20, 'currentTarget': array([ 58.1702963 , -17.93900304]), 'previousTarget': array([ 57.64107249, -17.49673902]), 'currentState': array([ 76., -27.,   0.]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 20.0}
episode index:180
target Thresh 27.768348461527385
target distance 17.0
model initialize at round 180
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 25.]), 'previousTarget': array([22., 25.]), 'currentState': array([21.5,  7.5,  0. ]), 'targetState': array([22, 25], dtype=int32), 'currentDistance': 17.50714140001165}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0444215099466853
{'scaleFactor': 20, 'currentTarget': array([ 59.19362795, -25.85659332]), 'previousTarget': array([ 58.71490438, -25.34105445]), 'currentState': array([ 71., -42.,   0.]), 'targetState': array([22, 25], dtype=int32), 'currentDistance': 20.0}
episode index:181
target Thresh 27.810454097850776
target distance 16.0
model initialize at round 181
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([ 2.5, 24.5,  0. ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 16.446884203398454}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.044177435716209
{'scaleFactor': 20, 'currentTarget': array([ 36.17429997, -12.77104998]), 'previousTarget': array([ 35.65429346, -12.29698462]), 'currentState': array([ 52., -25.,   0.]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 20.0}
episode index:182
target Thresh 27.852140776092654
target distance 8.0
model initialize at round 182
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([12.5, 12.5,  0. ]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 8.6313382508161}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04393602896366141
{'scaleFactor': 20, 'currentTarget': array([ 49.24864572, -21.59211358]), 'previousTarget': array([ 48.76237106, -21.08076497]), 'currentState': array([ 62., -37.,   0.]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 20.0}
episode index:183
target Thresh 27.893412664955576
target distance 7.0
model initialize at round 183
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([13.49999997,  9.50000003,  0.        ]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 7.905694191887886}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04369724619755456
{'scaleFactor': 20, 'currentTarget': array([ 49.18284446, -25.54018586]), 'previousTarget': array([ 48.68662034, -25.03657875]), 'currentState': array([ 62.99999994, -39.99999964,   0.        ]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 20.0}
episode index:184
target Thresh 27.93427389166283
target distance 4.0
model initialize at round 184
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  9.]), 'previousTarget': array([23.,  9.]), 'currentState': array([27.5,  7.5,  0. ]), 'targetState': array([23,  9], dtype=int32), 'currentDistance': 4.743416490252665}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04346104486675696
{'scaleFactor': 20, 'currentTarget': array([ 62.45973695, -28.26752934]), 'previousTarget': array([ 61.95596795, -27.77152115]), 'currentState': array([ 77., -42.,   0.]), 'targetState': array([23,  9], dtype=int32), 'currentDistance': 20.0}
episode index:185
target Thresh 27.974728542371132
target distance 11.0
model initialize at round 185
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([10.49999979, 15.50000003,  0.        ]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 11.423659862284069}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04322738333521526
{'scaleFactor': 20, 'currentTarget': array([ 46.90138829, -18.88621744]), 'previousTarget': array([ 46.41433297, -18.37500842]), 'currentState': array([ 59.99999902, -33.99999914,   0.        ]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 20.0}
episode index:186
target Thresh 28.01478066257927
target distance 8.0
model initialize at round 186
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 19.]), 'previousTarget': array([24., 19.]), 'currentState': array([22.5, 26.5,  0. ]), 'targetState': array([24, 19], dtype=int32), 'currentDistance': 7.648529270389084}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04299622085748683
{'scaleFactor': 20, 'currentTarget': array([56.94846611, -9.82990784]), 'previousTarget': array([56.4386492 , -9.34113561]), 'currentState': array([ 72., -23.,   0.]), 'targetState': array([24, 19], dtype=int32), 'currentDistance': 20.0}
episode index:187
target Thresh 28.054434257532634
target distance 13.0
model initialize at round 187
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 23.]), 'previousTarget': array([11., 23.]), 'currentState': array([24.5, 10.5,  0. ]), 'targetState': array([11, 23], dtype=int32), 'currentDistance': 18.39836949297421}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0427675175550534
{'scaleFactor': 20, 'currentTarget': array([ 59.74518344, -24.97145037]), 'previousTarget': array([ 59.24427846, -24.47237   ]), 'currentState': array([ 74., -39.,   0.]), 'targetState': array([11, 23], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:188
target Thresh 28.093693292623772
target distance 2.0
model initialize at round 188
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.5,  5.5,  0. ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.5811388300841693}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.04772694867910073
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.5,  4.5,  0. ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.7071067811867265}
episode index:189
target Thresh 28.132561693788904
target distance 10.0
model initialize at round 189
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 24.]), 'previousTarget': array([ 5., 24.]), 'currentState': array([15.5, 18.5,  0. ]), 'targetState': array([ 5, 24], dtype=int32), 'currentDistance': 11.853269591129807}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.047475754212368625
{'scaleFactor': 20, 'currentTarget': array([ 50.2569172 , -17.48550743]), 'previousTarget': array([ 49.75177381, -16.99112054]), 'currentState': array([ 65., -31.,   0.]), 'targetState': array([ 5, 24], dtype=int32), 'currentDistance': 20.0}
episode index:190
target Thresh 28.171043347900543
target distance 24.0
model initialize at round 190
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.09192171, 24.49583637]), 'previousTarget': array([20., 25.]), 'currentState': array([20.5,  4.5,  0. ]), 'targetState': array([20, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.047227190054188684
{'scaleFactor': 20, 'currentTarget': array([ 58.80285177, -28.42822063]), 'previousTarget': array([ 58.32799241, -27.91126146]), 'currentState': array([ 70., -45.,   0.]), 'targetState': array([20, 29], dtype=int32), 'currentDistance': 20.0}
episode index:191
target Thresh 28.20914210315616
target distance 22.0
model initialize at round 191
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6.50493644, 5.05566525]), 'previousTarget': array([6.0206292 , 5.09184678]), 'currentState': array([26.5,  5.5,  0. ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.046981215105989786
{'scaleFactor': 20, 'currentTarget': array([ 59.4657424 , -32.74751913]), 'previousTarget': array([ 58.94856411, -32.27280223]), 'currentState': array([ 76., -44.,   0.]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 20.0}
episode index:192
target Thresh 28.246861769463035
target distance 9.0
model initialize at round 192
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([23.5, 17.5,  0. ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 12.02081528017131}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.046737789120984655
{'scaleFactor': 20, 'currentTarget': array([ 56.66845003, -20.45528364]), 'previousTarget': array([ 56.14883001, -19.98308896]), 'currentState': array([ 73., -32.,   0.]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 20.0}
episode index:193
target Thresh 28.28420611881923
target distance 17.0
model initialize at round 193
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 15.]), 'previousTarget': array([25., 15.]), 'currentState': array([ 8.49951935, 21.50000003,  0.        ]), 'targetState': array([25, 15], dtype=int32), 'currentDistance': 17.734595065286598}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.046496872682216694
{'scaleFactor': 20, 'currentTarget': array([ 45.81176028, -12.13105504]), 'previousTarget': array([ 45.33919047, -11.61005142]), 'currentState': array([ 57.98454776, -27.99999467,   0.        ]), 'targetState': array([25, 15], dtype=int32), 'currentDistance': 20.0}
episode index:194
target Thresh 28.321178885690806
target distance 21.0
model initialize at round 194
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.43779978,  9.86463735]), 'previousTarget': array([22.64100589,  9.90599608]), 'currentState': array([ 5.5, 20.5,  0. ]), 'targetState': array([27,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04625842718128225
{'scaleFactor': 20, 'currentTarget': array([-26.16198639, -19.95537334]), 'previousTarget': array([-25.63686573, -19.5050884 ]), 'currentState': array([-44.        , -28.99999994,   0.        ]), 'targetState': array([27,  7], dtype=int32), 'currentDistance': 20.0}
episode index:195
target Thresh 28.35778376738525
target distance 15.0
model initialize at round 195
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 13.]), 'previousTarget': array([ 7., 13.]), 'currentState': array([12.45637763, 27.5       ,  0.        ]), 'targetState': array([ 7, 13], dtype=int32), 'currentDistance': 15.492645248364706}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.046022414797704275
{'scaleFactor': 20, 'currentTarget': array([ 45.08564966, -11.25621847]), 'previousTarget': array([ 44.56017498, -10.79632233]), 'currentState': array([ 61.9548867, -22.       ,   0.       ]), 'targetState': array([ 7, 13], dtype=int32), 'currentDistance': 20.0}
episode index:196
target Thresh 28.394024424421247
target distance 8.0
model initialize at round 196
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 29.]), 'previousTarget': array([20., 29.]), 'currentState': array([21.49999997, 20.5       ,  0.        ]), 'targetState': array([20, 29], dtype=int32), 'currentDistance': 8.631338245636918}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0457887984789342
{'scaleFactor': 20, 'currentTarget': array([ 57.79328066, -13.98059372]), 'previousTarget': array([ 57.30217696, -13.47277578]), 'currentState': array([ 70.99999997, -29.        ,   0.        ]), 'targetState': array([20, 29], dtype=int32), 'currentDistance': 20.0}
episode index:197
target Thresh 28.429904480894695
target distance 15.0
model initialize at round 197
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 17.]), 'previousTarget': array([14., 17.]), 'currentState': array([14.5,  1.5,  0. ]), 'targetState': array([14, 17], dtype=int32), 'currentDistance': 15.50806241927082}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04555754192095979
{'scaleFactor': 20, 'currentTarget': array([ 51.80578478, -32.14752022]), 'previousTarget': array([ 51.3236243 , -31.63381348]), 'currentState': array([ 64., -48.,   0.]), 'targetState': array([14, 17], dtype=int32), 'currentDistance': 20.0}
episode index:198
target Thresh 28.465427524841143
target distance 5.0
model initialize at round 198
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 15.]), 'previousTarget': array([ 9., 15.]), 'currentState': array([7.49999991, 9.5       , 0.        ]), 'targetState': array([ 9, 15], dtype=int32), 'currentDistance': 5.700877149020294}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04532860954949768
{'scaleFactor': 20, 'currentTarget': array([-15.00129682, -23.08031392]), 'previousTarget': array([-14.54246478, -22.55443647]), 'currentState': array([-25.66545281, -40.        ,   0.        ]), 'targetState': array([ 9, 15], dtype=int32), 'currentDistance': 20.0}
episode index:199
target Thresh 28.50059710859459
target distance 13.0
model initialize at round 199
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 20.]), 'previousTarget': array([15., 20.]), 'currentState': array([26.5,  6.5,  0. ]), 'targetState': array([15, 20], dtype=int32), 'currentDistance': 17.734147850968146}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04510196650175019
{'scaleFactor': 20, 'currentTarget': array([-12.6701366 , -25.87417383]), 'previousTarget': array([-12.21008489, -25.35014149]), 'currentState': array([-23., -43.,   0.]), 'targetState': array([15, 20], dtype=int32), 'currentDistance': 20.0}
episode index:200
target Thresh 28.535416749142716
target distance 21.0
model initialize at round 200
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9.96499055, 21.49459386]), 'previousTarget': array([10., 22.]), 'currentState': array([9.5, 1.5, 0. ]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.044877578608706656
{'scaleFactor': 20, 'currentTarget': array([-28.48442761, -31.64788721]), 'previousTarget': array([-28.00739157, -31.13173951]), 'currentState': array([-40., -48.,   0.]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 20.0}
episode index:201
target Thresh 28.56988992847859
target distance 23.0
model initialize at round 201
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.05566525,  7.50493644]), 'previousTarget': array([26.13125551,  8.01887683]), 'currentState': array([26.5, 27.5,  0. ]), 'targetState': array([26,  5], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04465541237797049
{'scaleFactor': 20, 'currentTarget': array([14.85399301, -7.22221867]), 'previousTarget': array([15.34057221, -6.70999078]), 'currentState': array([  1.37745062, -21.99999946,   0.        ]), 'targetState': array([26,  5], dtype=int32), 'currentDistance': 20.0}
episode index:202
target Thresh 28.604020093948883
target distance 8.0
model initialize at round 202
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  9.]), 'previousTarget': array([21.,  9.]), 'currentState': array([13.49134761, 12.50000006,  0.        ]), 'targetState': array([21,  9], dtype=int32), 'currentDistance': 8.284314160919541}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.044435434977093784
{'scaleFactor': 20, 'currentTarget': array([-11.4888517 , -22.97176063]), 'previousTarget': array([-10.98762524, -22.47300702]), 'currentState': array([-25.74397426, -36.99999931,   0.        ]), 'targetState': array([21,  9], dtype=int32), 'currentDistance': 20.0}
episode index:203
target Thresh 28.637810658598575
target distance 17.0
model initialize at round 203
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 27.]), 'previousTarget': array([27., 27.]), 'currentState': array([20.5,  9.5,  0. ]), 'targetState': array([27, 27], dtype=int32), 'currentDistance': 18.66815470259457}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04421761421740215
{'scaleFactor': 20, 'currentTarget': array([-16.17379731, -24.65436464]), 'previousTarget': array([-15.68495969, -24.14504179]), 'currentState': array([-29., -40.,   0.]), 'targetState': array([27, 27], dtype=int32), 'currentDistance': 20.0}
episode index:204
target Thresh 28.671265001512296
target distance 18.0
model initialize at round 204
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 10.]), 'previousTarget': array([20., 10.]), 'currentState': array([ 1.5, 10.5,  0. ]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 18.506755523321743}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04400191853829287
{'scaleFactor': 20, 'currentTarget': array([-31.77384037, -27.30762027]), 'previousTarget': array([-31.25791094, -26.82975823]), 'currentState': array([-48., -39.,   0.]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 20.0}
episode index:205
target Thresh 28.70438646815221
target distance 10.0
model initialize at round 205
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  6.]), 'previousTarget': array([13.,  6.]), 'currentState': array([22.5, 12.5,  0. ]), 'targetState': array([13,  6], dtype=int32), 'currentDistance': 11.51086443322123}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04378831699199048
{'scaleFactor': 20, 'currentTarget': array([-12.913245  , -22.25924041]), 'previousTarget': array([-12.42107144, -21.75206752]), 'currentState': array([-26.43027177, -36.99999979,   0.        ]), 'targetState': array([13,  6], dtype=int32), 'currentDistance': 20.0}
episode index:206
target Thresh 28.73717837069259
target distance 7.0
model initialize at round 206
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([16.5, 14.5,  0. ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 6.9641941385919575}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.04785883174911192
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.51241995,  8.5000006 ,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.6983801954236536}
episode index:207
target Thresh 28.769643988351007
target distance 8.0
model initialize at round 207
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([12.5, 18.5,  0. ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 9.300537618869024}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.047628741211856576
{'scaleFactor': 20, 'currentTarget': array([-18.71078075, -16.41236022]), 'previousTarget': array([-18.21658794, -15.90691568]), 'currentState': array([-32.39291475, -30.99999914,   0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:208
target Thresh 28.80178656771629
target distance 14.0
model initialize at round 208
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 20.]), 'previousTarget': array([12., 20.]), 'currentState': array([25.5, 27.5,  0. ]), 'targetState': array([12, 20], dtype=int32), 'currentDistance': 15.443445211480391}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04740085249792425
{'scaleFactor': 20, 'currentTarget': array([-10.98395771,  -6.81481947]), 'previousTarget': array([-10.49904453,  -6.301901  ]), 'currentState': array([-23.99972862, -22.        ,   0.        ]), 'targetState': array([12, 20], dtype=int32), 'currentDistance': 20.0}
episode index:209
target Thresh 28.833609323073155
target distance 18.0
model initialize at round 209
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 21.]), 'previousTarget': array([27., 21.]), 'currentState': array([26.5,  2.5,  0. ]), 'targetState': array([27, 21], dtype=int32), 'currentDistance': 18.506755523321747}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.047175134152696044
{'scaleFactor': 20, 'currentTarget': array([-11.15219099, -30.88697975]), 'previousTarget': array([-10.67272673, -30.37190008]), 'currentState': array([-23., -47.,   0.]), 'targetState': array([27, 21], dtype=int32), 'currentDistance': 20.0}
episode index:210
target Thresh 28.865115436723663
target distance 8.0
model initialize at round 210
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 16.]), 'previousTarget': array([ 2., 16.]), 'currentState': array([ 9.5, 23.5,  0. ]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 10.606601717798101}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04695155531784914
{'scaleFactor': 20, 'currentTarget': array([-23.50019615, -11.36748703]), 'previousTarget': array([-23.00663877, -10.86148664]), 'currentState': array([-37.13432869, -25.99999979,   0.        ]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 20.0}
episode index:211
target Thresh 28.89630805930543
target distance 3.0
model initialize at round 211
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 14.]), 'previousTarget': array([21., 14.]), 'currentState': array([17.59153372, 13.50000006,  0.        ]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 3.4449444580325927}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04673008571729325
{'scaleFactor': 20, 'currentTarget': array([-17.36499758, -22.26187062]), 'previousTarget': array([-16.8612017 , -21.76588776]), 'currentState': array([-31.89991465, -35.99999952,   0.        ]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 20.0}
episode index:212
target Thresh 28.92719031010671
target distance 25.0
model initialize at round 212
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.99814274, 10.39526112]), 'previousTarget': array([22.05477229, 10.69369935]), 'currentState': array([13.5, 28.5,  0. ]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04651069564350314
{'scaleFactor': 20, 'currentTarget': array([-17.49389723, -13.41553165]), 'previousTarget': array([-16.96233262, -12.99301073]), 'currentState': array([-36., -21.,   0.]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 20.0}
episode index:213
target Thresh 28.957765277378314
target distance 21.0
model initialize at round 213
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.98765985,  4.50594619]), 'previousTarget': array([21.,  5.]), 'currentState': array([20.5, 24.5,  0. ]), 'targetState': array([21,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.046293355944234435
{'scaleFactor': 20, 'currentTarget': array([-11.69937621, -14.9656382 ]), 'previousTarget': array([-11.16755086, -14.5207111 ]), 'currentState': array([-29., -25.,   0.]), 'targetState': array([21,  4], dtype=int32), 'currentDistance': 20.0}
episode index:214
target Thresh 28.988036018642454
target distance 10.0
model initialize at round 214
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 27.]), 'previousTarget': array([24., 27.]), 'currentState': array([18.5, 16.5,  0. ]), 'targetState': array([24, 27], dtype=int32), 'currentDistance': 11.853269591129806}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.046078038009610085
{'scaleFactor': 20, 'currentTarget': array([-17.48550743, -18.2569172 ]), 'previousTarget': array([-16.99112054, -17.75177381]), 'currentState': array([-31., -33.,   0.]), 'targetState': array([24, 27], dtype=int32), 'currentDistance': 20.0}
episode index:215
target Thresh 29.01800556099848
target distance 8.0
model initialize at round 215
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([ 6.5, 13.5,  0. ]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 8.860022573334755}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04586471375956559
{'scaleFactor': 20, 'currentTarget': array([-30.40833003, -20.46134345]), 'previousTarget': array([-29.92381723, -19.94880625]), 'currentState': array([-43., -36.,   0.]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 20.0}
episode index:216
target Thresh 29.0476769014256
target distance 10.0
model initialize at round 216
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 19.]), 'previousTarget': array([18., 19.]), 'currentState': array([10.5, 28.5,  0. ]), 'targetState': array([18, 19], dtype=int32), 'currentDistance': 12.103718436910192}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04565335563164133
{'scaleFactor': 20, 'currentTarget': array([-22.62886615,  -9.51148502]), 'previousTarget': array([-22.10854972,  -9.04049051]), 'currentState': array([-39., -21.,   0.]), 'targetState': array([18, 19], dtype=int32), 'currentDistance': 20.0}
episode index:217
target Thresh 29.077053007082586
target distance 16.0
model initialize at round 217
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([7.5, 7.5, 0. ]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 16.568041525780952}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04544393656911087
{'scaleFactor': 20, 'currentTarget': array([-29.77104998, -26.17429997]), 'previousTarget': array([-29.28826278, -25.66101411]), 'currentState': array([-42., -42.,   0.]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 20.0}
episode index:218
target Thresh 29.106136815604483
target distance 18.0
model initialize at round 218
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.85000343, 22.67353688]), 'previousTarget': array([11., 23.]), 'currentState': array([2.5, 4.5, 0. ]), 'targetState': array([11, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04523643000943456
{'scaleFactor': 20, 'currentTarget': array([-34.02106788, -29.78332096]), 'previousTarget': array([-33.53067089, -29.27513539]), 'currentState': array([-47., -45.,   0.]), 'targetState': array([11, 23], dtype=int32), 'currentDistance': 20.0}
episode index:219
target Thresh 29.13493123539638
target distance 3.0
model initialize at round 219
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 29.]), 'previousTarget': array([22., 29.]), 'currentState': array([24.5001967, 27.5      ,  0.       ]), 'targetState': array([22, 29], dtype=int32), 'currentDistance': 2.9156446140311028}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04503080987302804
{'scaleFactor': 20, 'currentTarget': array([-11.44578223,  -7.29277836]), 'previousTarget': array([-10.95196281,  -6.78708502]), 'currentState': array([-24.99929217, -22.        ,   0.        ]), 'targetState': array([22, 29], dtype=int32), 'currentDistance': 20.0}
episode index:220
target Thresh 29.16343914592425
target distance 12.0
model initialize at round 220
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 2.5, 17.5,  0. ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 14.577379737113207}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0448270505523356
{'scaleFactor': 20, 'currentTarget': array([-30.44162619, -20.7830371 ]), 'previousTarget': array([-29.92146413, -20.31285791]), 'currentState': array([-47., -32.,   0.]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 20.0}
episode index:221
target Thresh 29.19166339800291
target distance 21.0
model initialize at round 221
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.36618652,  8.3236243 ]), 'previousTarget': array([19.54489741,  8.41603543]), 'currentState': array([ 3.5, 20.5,  0. ]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04462512690119896
{'scaleFactor': 20, 'currentTarget': array([-27.86330678, -20.57026935]), 'previousTarget': array([-27.33704394, -20.12700607]), 'currentState': array([-46., -29.,   0.]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 20.0}
episode index:222
target Thresh 29.219606814081075
target distance 20.0
model initialize at round 222
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5.52633404, 14.82455532]), 'previousTarget': array([ 5.8434743 , 14.74695771]), 'currentState': array([24.5,  8.5,  0. ]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04442501422451196
{'scaleFactor': 20, 'currentTarget': array([-15.55557175, -23.3704006 ]), 'previousTarget': array([-15.11301814, -22.83974599]), 'currentState': array([-25., -41.,   0.]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 20.0}
episode index:223
target Thresh 29.24727218852365
target distance 14.0
model initialize at round 223
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 29.]), 'previousTarget': array([11., 29.]), 'currentState': array([20.5, 14.5,  0. ]), 'targetState': array([11, 29], dtype=int32), 'currentDistance': 17.33493582335974}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.044226688268152535
{'scaleFactor': 20, 'currentTarget': array([-18.4000212 , -18.04003392]), 'previousTarget': array([-17.93610454, -17.51753514]), 'currentState': array([-29., -35.,   0.]), 'targetState': array([11, 29], dtype=int32), 'currentDistance': 20.0}
episode index:224
target Thresh 29.27466228789113
target distance 7.0
model initialize at round 224
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 27.]), 'previousTarget': array([16., 27.]), 'currentState': array([ 8.5, 23.5,  0. ]), 'targetState': array([16, 27], dtype=int32), 'currentDistance': 8.27647267862353}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04403012520918297
{'scaleFactor': 20, 'currentTarget': array([-26.35328548, -12.38112509]), 'previousTarget': array([-25.84874893, -11.88600565]), 'currentState': array([-41., -26.,   0.]), 'targetState': array([16, 27], dtype=int32), 'currentDistance': 20.0}
episode index:225
target Thresh 29.301779851216274
target distance 6.0
model initialize at round 225
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 13.]), 'previousTarget': array([ 7., 13.]), 'currentState': array([11.5,  6.5,  0. ]), 'targetState': array([ 7, 13], dtype=int32), 'currentDistance': 7.905694150420898}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04383530164631048
{'scaleFactor': 20, 'currentTarget': array([-25.47217783, -27.4098213 ]), 'previousTarget': array([-24.98896347, -26.8963477 ]), 'currentState': array([-38., -43.,   0.]), 'targetState': array([ 7, 13], dtype=int32), 'currentDistance': 20.0}
episode index:226
target Thresh 29.328627590278014
target distance 9.0
model initialize at round 226
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([26.5, 21.5,  0. ]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 10.70046727951624}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04364219459059986
{'scaleFactor': 20, 'currentTarget': array([ -4.19138262, -13.28415786]), 'previousTarget': array([ -3.69921977, -12.77694857]), 'currentState': array([-17.73553228, -28.        ,   0.        ]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 20.0}
episode index:227
target Thresh 29.355208189872634
target distance 7.0
model initialize at round 227
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 3.]), 'previousTarget': array([9., 3.]), 'currentState': array([15.5,  2.5,  0. ]), 'targetState': array([9, 3], dtype=int32), 'currentDistance': 6.519202405202568}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04345078145643056
{'scaleFactor': 20, 'currentTarget': array([-20.9592144 , -31.83629603]), 'previousTarget': array([-20.47155409, -31.32569263]), 'currentState': array([-33.99999973, -47.        ,   0.        ]), 'targetState': array([9, 3], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:228
target Thresh 29.38152430808224
target distance 14.0
model initialize at round 228
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 25.]), 'previousTarget': array([27., 25.]), 'currentState': array([16.5, 10.5,  0. ]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 17.902513789968268}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04326104005269069
{'scaleFactor': 20, 'currentTarget': array([-19.32117742, -24.40925592]), 'previousTarget': array([-18.82500046, -23.90567276]), 'currentState': array([-33., -39.,   0.]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 20.0}
episode index:229
target Thresh 29.407578576540587
target distance 15.0
model initialize at round 229
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 25.]), 'previousTarget': array([ 8., 25.]), 'currentState': array([22.5, 23.5,  0. ]), 'targetState': array([ 8, 25], dtype=int32), 'currentDistance': 14.577379737113182}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.043072948574200735
{'scaleFactor': 20, 'currentTarget': array([-15.68315006,  -9.50973295]), 'previousTarget': array([-15.21804765,  -8.98583786]), 'currentState': array([-27., -26.,   0.]), 'targetState': array([ 8, 25], dtype=int32), 'currentDistance': 20.0}
episode index:230
target Thresh 29.433373600696225
target distance 19.0
model initialize at round 230
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  8.]), 'previousTarget': array([11.91410718,  8.23313766]), 'currentState': array([ 4.5, 26.5,  0. ]), 'targetState': array([12,  8], dtype=int32), 'currentDistance': 19.962464777677056}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04288648559336004
{'scaleFactor': 20, 'currentTarget': array([-27.43032195, -13.44456106]), 'previousTarget': array([-26.9005899 , -12.99943349]), 'currentState': array([-45., -23.,   0.]), 'targetState': array([12,  8], dtype=int32), 'currentDistance': 20.0}
episode index:231
target Thresh 29.45891196007307
target distance 3.0
model initialize at round 231
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 13.]), 'previousTarget': array([24., 13.]), 'currentState': array([23.50015756,  9.5       ,  0.        ]), 'targetState': array([24, 13], dtype=int32), 'currentDistance': 3.5355116263348494}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.042701630052009346
{'scaleFactor': 20, 'currentTarget': array([-12.27543164, -25.45210133]), 'previousTarget': array([-11.77958308, -24.94818601]), 'currentState': array([-25.99981302, -40.        ,   0.        ]), 'targetState': array([24, 13], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:232
target Thresh 29.484196208528346
target distance 6.0
model initialize at round 232
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 26.]), 'previousTarget': array([20., 26.]), 'currentState': array([20.5, 19.5,  0. ]), 'targetState': array([20, 26], dtype=int32), 'currentDistance': 6.519202405202724}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04251836125350287
{'scaleFactor': 20, 'currentTarget': array([-15.82990784, -14.94846611]), 'previousTarget': array([-15.33951584, -14.44006452]), 'currentState': array([-29., -30.,   0.]), 'targetState': array([20, 26], dtype=int32), 'currentDistance': 20.0}
episode index:233
target Thresh 29.50922887450796
target distance 9.0
model initialize at round 233
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([16.5,  1.5,  0. ]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 12.103718436910263}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.042336658854983625
{'scaleFactor': 20, 'currentTarget': array([-18.6161722 , -34.10375958]), 'previousTarget': array([-18.11408959, -33.60591559]), 'currentState': array([-33., -48.,   0.]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 20.0}
episode index:234
target Thresh 29.534012461299376
target distance 14.0
model initialize at round 234
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 14.]), 'previousTarget': array([ 3., 14.]), 'currentState': array([16.5, 11.5,  0. ]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 13.72953021774592}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04215650285985604
{'scaleFactor': 20, 'currentTarget': array([-21.61580042, -21.55615617]), 'previousTarget': array([-21.14907711, -21.0331682 ]), 'currentState': array([-33., -38.,   0.]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:235
target Thresh 29.55854944728193
target distance 27.0
model initialize at round 235
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.5976663 , 11.23624794]), 'previousTarget': array([14.71285862, 11.51685448]), 'currentState': array([ 4.5, 28.5,  0. ]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04197787361044986
{'scaleFactor': 20, 'currentTarget': array([-26.14555454, -14.32842699]), 'previousTarget': array([-25.61599637, -13.91255687]), 'currentState': array([-45., -21.,   0.]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 20.0}
episode index:236
target Thresh 29.58284228617466
target distance 17.0
model initialize at round 236
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9.29770785, 13.47670203]), 'previousTarget': array([ 9.76756726, 14.00324289]), 'currentState': array([22.5, 28.5,  0. ]), 'targetState': array([ 8, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04180075178086991
{'scaleFactor': 20, 'currentTarget': array([-8.27252566, -6.11973679]), 'previousTarget': array([-7.7854596 , -5.60813153]), 'currentState': array([-21.6358249, -21.       ,   0.       ]), 'targetState': array([ 8, 12], dtype=int32), 'currentDistance': 20.0}
episode index:237
target Thresh 29.606893407281703
target distance 23.0
model initialize at round 237
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.33402667,  7.6885793 ]), 'previousTarget': array([17.39893894,  7.91602889]), 'currentState': array([ 6.5, 24.5,  0. ]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.041625118370025915
{'scaleFactor': 20, 'currentTarget': array([-24.57271678, -17.22598989]), 'previousTarget': array([-24.0427642 , -16.79737404]), 'currentState': array([-43., -25.,   0.]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 20.0}
episode index:238
target Thresh 29.63070521573521
target distance 3.0
model initialize at round 238
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([9.50121415, 1.5       , 0.        ]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 4.300457038627589}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.041450954694837526
{'scaleFactor': 20, 'currentTarget': array([-25.99204689, -33.72364692]), 'previousTarget': array([-25.49335594, -33.22236271]), 'currentState': array([-39.99867898, -48.        ,   0.        ]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 20.0}
episode index:239
target Thresh 29.654280092735878
target distance 19.0
model initialize at round 239
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  2.]), 'previousTarget': array([12.,  2.]), 'currentState': array([14.5, 20.5,  0. ]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 18.66815470259438}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04127824238360903
{'scaleFactor': 20, 'currentTarget': array([-18.30454642, -17.98810508]), 'previousTarget': array([-17.77646458, -17.53079935]), 'currentState': array([-35., -29.,   0.]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 20.0}
episode index:240
target Thresh 29.67762039579104
target distance 4.0
model initialize at round 240
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([ 6.50045371, 12.5       ,  0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 3.8077078504245248}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.04501352000608784
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([7.44618437, 9.5       , 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.7461311918285929}
episode index:241
target Thresh 29.70072845895046
target distance 14.0
model initialize at round 241
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([24.5, 18.5,  0. ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 17.102631376486954}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.04804165157382775
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.24803159,  6.        ,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.0303007657243206}
episode index:242
target Thresh 29.723606593039708
target distance 19.0
model initialize at round 242
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6.32646312, 19.14999657]), 'previousTarget': array([ 6.92524322, 19.43827311]), 'currentState': array([24.5, 27.5,  0. ]), 'targetState': array([ 6, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.047843949303976605
{'scaleFactor': 20, 'currentTarget': array([-12.9378345 ,  -6.04681337]), 'previousTarget': array([-12.46846522,  -5.52369972]), 'currentState': array([-25., -22.,   0.]), 'targetState': array([ 6, 19], dtype=int32), 'currentDistance': 20.0}
episode index:243
target Thresh 29.746257085891262
target distance 12.0
model initialize at round 243
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 12.]), 'previousTarget': array([22., 12.]), 'currentState': array([22.5, 23.5,  0. ]), 'targetState': array([22, 12], dtype=int32), 'currentDistance': 11.510864433221256}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04764786754453408
{'scaleFactor': 20, 'currentTarget': array([-11.1913188, -13.742768 ]), 'previousTarget': array([-10.67360727, -13.26563843]), 'currentState': array([-26.99512416, -26.        ,   0.        ]), 'targetState': array([22, 12], dtype=int32), 'currentDistance': 20.0}
episode index:244
target Thresh 29.768682202573277
target distance 15.0
model initialize at round 244
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 27.]), 'previousTarget': array([24., 27.]), 'currentState': array([12.5, 11.5,  0. ]), 'targetState': array([24, 27], dtype=int32), 'currentDistance': 19.300259065618896}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.047453386452515574
{'scaleFactor': 20, 'currentTarget': array([-23.31371592, -23.41625467]), 'previousTarget': array([-22.81741641, -22.91278278]), 'currentState': array([-37., -38.,   0.]), 'targetState': array([24, 27], dtype=int32), 'currentDistance': 20.0}
episode index:245
target Thresh 29.790884185616115
target distance 3.0
model initialize at round 245
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 12.]), 'previousTarget': array([18., 12.]), 'currentState': array([20.5000371, 11.5      ,  0.       ]), 'targetState': array([18, 12], dtype=int32), 'currentDistance': 2.5495461401655435}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.047260486507586644
{'scaleFactor': 20, 'currentTarget': array([-15.29718571, -23.42665263]), 'previousTarget': array([-14.80188596, -22.92223634]), 'currentState': array([-28.99454117, -38.        ,   0.        ]), 'targetState': array([18, 12], dtype=int32), 'currentDistance': 20.0}
episode index:246
target Thresh 29.812865255236577
target distance 16.0
model initialize at round 246
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 10.]), 'previousTarget': array([25., 10.]), 'currentState': array([8.5, 2.5, 0. ]), 'targetState': array([25, 10], dtype=int32), 'currentDistance': 18.124568960391926}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04706914850553164
{'scaleFactor': 20, 'currentTarget': array([-25.86353984, -33.92760259]), 'previousTarget': array([-25.35574433, -33.43663442]), 'currentState': array([-41., -47.,   0.]), 'targetState': array([25, 10], dtype=int32), 'currentDistance': 20.0}
episode index:247
target Thresh 29.834627609559945
target distance 17.0
model initialize at round 247
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([ 6.5, 23.5,  0. ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 18.124568960391787}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.046879353551880304
{'scaleFactor': 20, 'currentTarget': array([-25.69146209, -15.97927285]), 'previousTarget': array([-25.16351941, -15.52769342]), 'currentState': array([-42.9999868, -26.       ,   0.       ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 20.0}
episode index:248
target Thresh 29.856173424839785
target distance 9.0
model initialize at round 248
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([ 7.64564404, 11.5       ,  0.        ]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 11.98974459472229}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04669108305568801
{'scaleFactor': 20, 'currentTarget': array([ -9.21162324, -23.51823498]), 'previousTarget': array([ -8.71596873, -23.01409706]), 'currentState': array([-23.00576989, -38.        ,   0.        ]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 20.0}
episode index:249
target Thresh 29.87750485567558
target distance 2.0
model initialize at round 249
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([10.49999899,  5.5       ,  0.        ]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 1.5811397913650944}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.05042471872346526
{'scaleFactor': 20, 'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([11.49999151,  4.5       ,  0.        ]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 0.7071127871379983}
episode index:250
target Thresh 29.898624035228192
target distance 20.0
model initialize at round 250
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5.16608071, 10.50829159]), 'previousTarget': array([ 5.7615699 , 10.79270645]), 'currentState': array([23.5, 18.5,  0. ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.050223823429746274
{'scaleFactor': 20, 'currentTarget': array([-14.18980011, -14.85939349]), 'previousTarget': array([-13.72470222, -14.33391321]), 'currentState': array([-26., -31.,   0.]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 20.0}
episode index:251
target Thresh 29.919533075433172
target distance 14.0
model initialize at round 251
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  2.]), 'previousTarget': array([17.,  2.]), 'currentState': array([ 5.50000072, 15.5       ,  0.        ]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 17.73414738714881}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0500245225431203
{'scaleFactor': 20, 'currentTarget': array([-26.77101098, -23.83428304]), 'previousTarget': array([-26.24549222, -23.37764397]), 'currentState': array([-43.99477941, -34.        ,   0.        ]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 20.0}
episode index:252
target Thresh 29.940234067211968
target distance 5.0
model initialize at round 252
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 28.]), 'previousTarget': array([ 5., 28.]), 'currentState': array([ 6.5, 22.5,  0. ]), 'targetState': array([ 5, 28], dtype=int32), 'currentDistance': 5.700877125495767}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04982679715757437
{'scaleFactor': 20, 'currentTarget': array([-29.84931507, -11.93150685]), 'previousTarget': array([-29.35931127, -11.42278873]), 'currentState': array([-43., -27.,   0.]), 'targetState': array([ 5, 28], dtype=int32), 'currentDistance': 20.0}
episode index:253
target Thresh 29.96072908068101
target distance 22.0
model initialize at round 253
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.33978252, 17.54483829]), 'previousTarget': array([10.52085402, 17.66475581]), 'currentState': array([25.5,  4.5,  0. ]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.049630628664828015
{'scaleFactor': 20, 'currentTarget': array([-16.38500389, -26.50643803]), 'previousTarget': array([-15.95406225, -25.9781528 ]), 'currentState': array([-24., -45.,   0.]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:254
target Thresh 29.98102016535872
target distance 9.0
model initialize at round 254
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 25.]), 'previousTarget': array([12., 25.]), 'currentState': array([17.5, 15.5,  0. ]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 10.977249200050105}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04943599874849535
{'scaleFactor': 20, 'currentTarget': array([-20.04352241, -17.9674505 ]), 'previousTarget': array([-19.56594467, -17.45075318]), 'currentState': array([-32., -34.,   0.]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 20.0}
episode index:255
target Thresh 30.001109350370477
target distance 10.0
model initialize at round 255
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 16.]), 'previousTarget': array([16., 16.]), 'currentState': array([7.5, 5.5, 0. ]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 13.509256086106328}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.049242889378384044
{'scaleFactor': 20, 'currentTarget': array([-28.09955806, -29.62023248]), 'previousTarget': array([-27.60164072, -29.11821953]), 'currentState': array([-42., -44.,   0.]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 20.0}
episode index:256
target Thresh 30.020998644651527
target distance 15.0
model initialize at round 256
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([19.5, 12.5,  0. ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 14.577379737113162}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0490512828049273
{'scaleFactor': 20, 'currentTarget': array([-18.21655685, -20.8398494 ]), 'previousTarget': array([-17.74669654, -20.31791553]), 'currentState': array([-30., -37.,   0.]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 20.0}
episode index:257
target Thresh 30.040690037147865
target distance 15.0
model initialize at round 257
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 4.]), 'previousTarget': array([8., 4.]), 'currentState': array([10.49999982, 18.5       ,  0.        ]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 14.713938939180347}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04886116155374541
{'scaleFactor': 20, 'currentTarget': array([ -5.08063597, -14.63134762]), 'previousTarget': array([ -4.62812099, -14.09811211]), 'currentState': array([-16.57268623, -31.        ,   0.        ]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 20.0}
episode index:258
target Thresh 30.060185497015155
target distance 22.0
model initialize at round 258
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.05566525, 26.49506356]), 'previousTarget': array([22.09184678, 26.9793708 ]), 'currentState': array([22.5,  6.5,  0. ]), 'targetState': array([22, 29], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.048672508420333264
{'scaleFactor': 20, 'currentTarget': array([-15.74751913, -26.4657424 ]), 'previousTarget': array([-15.27280223, -25.94856411]), 'currentState': array([-27., -43.,   0.]), 'targetState': array([22, 29], dtype=int32), 'currentDistance': 20.0}
episode index:259
target Thresh 30.07948697381563
target distance 20.0
model initialize at round 259
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.28501949, 20.94286082]), 'previousTarget': array([21.638375  , 21.52431817]), 'currentState': array([11.5,  3.5,  0. ]), 'targetState': array([23, 24], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04848530646487045
{'scaleFactor': 20, 'currentTarget': array([-24.86043438, -30.92180994]), 'previousTarget': array([-24.36836698, -30.41490092]), 'currentState': array([-38., -46.,   0.]), 'targetState': array([23, 24], dtype=int32), 'currentDistance': 20.0}
episode index:260
target Thresh 30.098596397713052
target distance 9.0
model initialize at round 260
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 23.]), 'previousTarget': array([22., 23.]), 'currentState': array([25.5, 13.5,  0. ]), 'targetState': array([22, 23], dtype=int32), 'currentDistance': 10.12422836565834}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04829953900715064
{'scaleFactor': 20, 'currentTarget': array([-11.70268851, -20.22736135]), 'previousTarget': array([-11.22118773, -19.71295565]), 'currentState': array([-24., -36.,   0.]), 'targetState': array([22, 23], dtype=int32), 'currentDistance': 20.0}
episode index:261
target Thresh 30.11751567966574
target distance 10.0
model initialize at round 261
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 12.]), 'previousTarget': array([ 6., 12.]), 'currentState': array([10.02852987, 21.5       ,  0.        ]), 'targetState': array([ 6, 12], dtype=int32), 'currentDistance': 10.31886878192571}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.048115189621627155
{'scaleFactor': 20, 'currentTarget': array([ -9.59009036, -11.36369346]), 'previousTarget': array([ -9.13871215, -10.83135168]), 'currentState': array([-20.69114007, -28.        ,   0.        ]), 'targetState': array([ 6, 12], dtype=int32), 'currentDistance': 20.0}
episode index:262
target Thresh 30.13624671161765
target distance 6.0
model initialize at round 262
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  8.]), 'previousTarget': array([17.,  8.]), 'currentState': array([11.49999985,  7.5       ,  0.        ]), 'targetState': array([17,  8], dtype=int32), 'currentDistance': 5.522680656993325}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.047932242132571544
{'scaleFactor': 20, 'currentTarget': array([-13.28928242, -26.89607945]), 'previousTarget': array([-12.80078053, -26.38610693]), 'currentState': array([-26.39926275, -42.        ,   0.        ]), 'targetState': array([17,  8], dtype=int32), 'currentDistance': 20.0}
episode index:263
target Thresh 30.154791366687586
target distance 22.0
model initialize at round 263
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.95770931, 22.6236628 ]), 'previousTarget': array([12.27605889, 23.20732955]), 'currentState': array([3.5, 4.5, 0. ]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0477506806093421
{'scaleFactor': 20, 'currentTarget': array([-33.19631201, -29.63557441]), 'previousTarget': array([-32.70688931, -29.12676615]), 'currentState': array([-46., -45.,   0.]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 20.0}
episode index:264
target Thresh 30.173151499356518
target distance 12.0
model initialize at round 264
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 25.]), 'previousTarget': array([11., 25.]), 'currentState': array([12.5, 12.5,  0. ]), 'targetState': array([11, 25], dtype=int32), 'currentDistance': 12.589678312014241}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04757048936175968
{'scaleFactor': 20, 'currentTarget': array([-24.75653614, -21.18552585]), 'previousTarget': array([-24.27471281, -20.67147028]), 'currentState': array([-37., -37.,   0.]), 'targetState': array([11, 25], dtype=int32), 'currentDistance': 20.0}
episode index:265
target Thresh 30.191328945653
target distance 17.0
model initialize at round 265
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([14.5, 28.5,  0. ]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 17.7341478509681}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04739165293558765
{'scaleFactor': 20, 'currentTarget': array([48.08224422, -8.81296924]), 'previousTarget': array([47.56671024, -8.34133769]), 'currentState': array([ 63.94024792, -21.        ,   0.        ]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 20.0}
episode index:266
target Thresh 30.209325523336823
target distance 23.0
model initialize at round 266
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.78991511,  8.35014149]), 'previousTarget': array([22.39893894,  8.91602889]), 'currentState': array([12.5, 25.5,  0. ]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04721415610811354
{'scaleFactor': 20, 'currentTarget': array([ 43.06347112, -11.20438233]), 'previousTarget': array([ 43.35106878, -10.96181642]), 'currentState': array([ 58.43461839, -24.        ,   0.        ]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 20.0}
episode index:267
target Thresh 30.227143032080743
target distance 15.0
model initialize at round 267
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 12.]), 'previousTarget': array([17., 12.]), 'currentState': array([ 5.5, 26.5,  0. ]), 'targetState': array([17, 12], dtype=int32), 'currentDistance': 18.50675552332169}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.047037983883829536
{'scaleFactor': 20, 'currentTarget': array([34.45053224, -7.94723323]), 'previousTarget': array([34.72343465, -7.64893202]), 'currentState': array([ 47.61921526, -23.        ,   0.        ]), 'targetState': array([17, 12], dtype=int32), 'currentDistance': 20.0}
episode index:268
target Thresh 30.244783253650485
target distance 21.0
model initialize at round 268
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.36614761, 23.18998284]), 'previousTarget': array([22.79898987, 23.17157288]), 'currentState': array([ 2.5, 25.5,  0. ]), 'targetState': array([24, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04686312149020935
{'scaleFactor': 20, 'currentTarget': array([-30.32294577, -12.96025987]), 'previousTarget': array([-29.80454642, -12.48810508]), 'currentState': array([-47., -24.,   0.]), 'targetState': array([24, 23], dtype=int32), 'currentDistance': 20.0}
episode index:269
target Thresh 30.26224795208291
target distance 2.0
model initialize at round 269
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 23.]), 'previousTarget': array([22., 23.]), 'currentState': array([24.5, 21.5,  0. ]), 'targetState': array([22, 23], dtype=int32), 'currentDistance': 2.9154759474227605}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04668955437357895
{'scaleFactor': 20, 'currentTarget': array([  1.59319149, -10.86920214]), 'previousTarget': array([  2.04360066, -10.33942043]), 'currentState': array([ -8.72842489, -28.        ,   0.        ]), 'targetState': array([22, 23], dtype=int32), 'currentDistance': 20.0}
episode index:270
target Thresh 30.279538873862407
target distance 8.0
model initialize at round 270
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 25.]), 'previousTarget': array([11., 25.]), 'currentState': array([19.5, 27.5,  0. ]), 'targetState': array([11, 25], dtype=int32), 'currentDistance': 8.860022573334728}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.046517268195078655
{'scaleFactor': 20, 'currentTarget': array([11.39422777, -2.00213122]), 'previousTarget': array([11.67616905, -1.5065042 ]), 'currentState': array([ 11.68619418, -22.        ,   0.        ]), 'targetState': array([11, 25], dtype=int32), 'currentDistance': 20.0}
episode index:271
target Thresh 30.29665774809557
target distance 23.0
model initialize at round 271
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.99578351, 23.04220131]), 'previousTarget': array([21.28798697, 23.62485559]), 'currentState': array([13.5,  4.5,  0. ]), 'targetState': array([23, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0463462488267144
{'scaleFactor': 20, 'currentTarget': array([-23.42829872, -29.44518316]), 'previousTarget': array([-22.94075514, -28.93512389]), 'currentState': array([-36., -45.,   0.]), 'targetState': array([23, 28], dtype=int32), 'currentDistance': 20.0}
episode index:272
target Thresh 30.313606286684085
target distance 18.0
model initialize at round 272
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.8937183 , 20.37211039]), 'previousTarget': array([22.48314552, 20.71285862]), 'currentState': array([ 4.5, 10.5,  0. ]), 'targetState': array([23, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04617648234749566
{'scaleFactor': 20, 'currentTarget': array([-30.00324289, -25.76756726]), 'previousTarget': array([-29.49675821, -25.2749202 ]), 'currentState': array([-45., -39.,   0.]), 'targetState': array([23, 21], dtype=int32), 'currentDistance': 20.0}
episode index:273
target Thresh 30.330386184495936
target distance 19.0
model initialize at round 273
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  2.]), 'previousTarget': array([22.,  2.]), 'currentState': array([26.5, 20.5,  0. ]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 19.03943276465971}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.046007955039658086
{'scaleFactor': 20, 'currentTarget': array([ 58.65493806, -19.04264963]), 'previousTarget': array([ 58.12515324, -18.59471353]), 'currentState': array([ 76., -29.,   0.]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 20.0}
episode index:274
target Thresh 30.346999119534885
target distance 16.0
model initialize at round 274
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 25.]), 'previousTarget': array([12., 25.]), 'currentState': array([22.5,  8.5,  0. ]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 19.55760721560797}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04584065338496842
{'scaleFactor': 20, 'currentTarget': array([-16.82541376, -23.78146944]), 'previousTarget': array([-16.36535001, -23.25793314]), 'currentState': array([-27., -41.,   0.]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 20.0}
episode index:275
target Thresh 30.363446753108285
target distance 4.0
model initialize at round 275
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 7.]), 'previousTarget': array([9., 7.]), 'currentState': array([13.5,  4.5,  0. ]), 'targetState': array([9, 7], dtype=int32), 'currentDistance': 5.147815070493568}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04567456406110984
{'scaleFactor': 20, 'currentTarget': array([ -8.92567693, -27.27720012]), 'previousTarget': array([ -8.49031526, -26.74354654]), 'currentState': array([-18.19402976, -45.        ,   0.        ]), 'targetState': array([9, 7], dtype=int32), 'currentDistance': 20.0}
episode index:276
target Thresh 30.3797307299932
target distance 12.0
model initialize at round 276
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([ 3.5, 23.5,  0. ]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 12.349089035228443}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04550967393814554
{'scaleFactor': 20, 'currentTarget': array([ -7.86673431, -10.24458501]), 'previousTarget': array([-7.39073607, -9.72584714]), 'currentState': array([-20.18610518, -26.        ,   0.        ]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 20.0}
episode index:277
target Thresh 30.39585267860088
target distance 9.0
model initialize at round 277
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 24.]), 'previousTarget': array([11., 24.]), 'currentState': array([ 1.5, 15.5,  0. ]), 'targetState': array([11, 24], dtype=int32), 'currentDistance': 12.747548783982012}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04534597007505869
{'scaleFactor': 20, 'currentTarget': array([-33.73751236, -19.97924944]), 'previousTarget': array([-33.23647941, -19.48030027]), 'currentState': array([-48., -34.,   0.]), 'targetState': array([11, 24], dtype=int32), 'currentDistance': 20.0}
episode index:278
target Thresh 30.411814211139628
target distance 23.0
model initialize at round 278
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9.07456437, 21.4954746 ]), 'previousTarget': array([ 9.13125551, 21.98112317]), 'currentState': array([9.5, 1.5, 0. ]), 'targetState': array([ 9, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04518343971636672
{'scaleFactor': 20, 'currentTarget': array([-28.85355625, -31.3940736 ]), 'previousTarget': array([-28.37955384, -30.87665265]), 'currentState': array([-40., -48.,   0.]), 'targetState': array([ 9, 25], dtype=int32), 'currentDistance': 20.0}
episode index:279
target Thresh 30.427616923775997
target distance 7.0
model initialize at round 279
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 26.]), 'previousTarget': array([17., 26.]), 'currentState': array([24.5, 25.5,  0. ]), 'targetState': array([17, 26], dtype=int32), 'currentDistance': 7.516648189186541}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.045022070288808266
{'scaleFactor': 20, 'currentTarget': array([23.52701415, 17.30830336]), 'previousTarget': array([23.97663062, 16.77034377]), 'currentState': array([35.53671733,  1.31558535,  0.        ]), 'targetState': array([17, 26], dtype=int32), 'currentDistance': 20.0}
episode index:280
target Thresh 30.44326239679442
target distance 14.0
model initialize at round 280
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 19.]), 'previousTarget': array([17., 19.]), 'currentState': array([16.5,  5.5,  0. ]), 'targetState': array([17, 19], dtype=int32), 'currentDistance': 13.509256086106305}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04486184939810076
{'scaleFactor': 20, 'currentTarget': array([-6.56892993, 23.79542242]), 'previousTarget': array([-6.0971696 , 23.76356222]), 'currentState': array([-26.16738025,  27.78299625,   0.        ]), 'targetState': array([17, 19], dtype=int32), 'currentDistance': 20.0}
episode index:281
target Thresh 30.458752194755238
target distance 14.0
model initialize at round 281
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([15.5, 24.5,  0. ]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 14.983324063771589}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04470276482576707
{'scaleFactor': 20, 'currentTarget': array([30.62619552, 13.53059685]), 'previousTarget': array([30.06029176, 13.26033699]), 'currentState': array([49.81742723, 19.16057247,  0.        ]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:282
target Thresh 30.474087866651153
target distance 6.0
model initialize at round 282
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 12.]), 'previousTarget': array([11., 12.]), 'currentState': array([ 5.5       , 17.49898702,  0.        ]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 7.777458340345817}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04454480452602937
{'scaleFactor': 20, 'currentTarget': array([42.50347076, 51.3657545 ]), 'previousTarget': array([42.02094213, 50.85178887]), 'currentState': array([55.        , 66.98102769,  0.        ]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 20.0}
episode index:283
target Thresh 30.489270946062135
target distance 11.0
model initialize at round 283
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([15.5       , 19.49980131,  0.        ]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 13.209671840040556}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.044387956622768704
{'scaleFactor': 20, 'currentTarget': array([53.47671017, 52.65214074]), 'previousTarget': array([53.0033863 , 52.13336877]), 'currentState': array([65.        , 68.99881598,  0.        ]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:284
target Thresh 30.50430295130878
target distance 19.0
model initialize at round 284
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  2.]), 'previousTarget': array([22.,  2.]), 'currentState': array([3.5, 6.5, 0. ]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 19.03943276465977}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04423220940654846
{'scaleFactor': 20, 'currentTarget': array([43.04250544, 38.65398121]), 'previousTarget': array([42.59456832, 38.12419685]), 'currentState': array([53.        , 55.99896038,  0.        ]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:285
target Thresh 30.519185385604136
target distance 24.0
model initialize at round 285
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9.73855458, 25.14310384]), 'previousTarget': array([ 9.92091492, 24.57960839]), 'currentState': array([13.5,  5.5,  0. ]), 'targetState': array([ 9, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04407755133170039
{'scaleFactor': 20, 'currentTarget': array([10.85763384, 35.72044388]), 'previousTarget': array([10.61312034, 35.15164041]), 'currentState': array([16.1861245 , 54.99755952,  0.        ]), 'targetState': array([ 9, 29], dtype=int32), 'currentDistance': 20.0}
episode index:286
target Thresh 30.533919737204037
target distance 1.0
model initialize at round 286
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 12.]), 'previousTarget': array([ 8., 12.]), 'currentState': array([ 7.5       , 11.49999994,  0.        ]), 'targetState': array([ 8, 12], dtype=int32), 'currentDistance': 0.7071068233333766}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.04740829157096276
{'scaleFactor': 20, 'currentTarget': array([ 8., 12.]), 'previousTarget': array([ 8., 12.]), 'currentState': array([ 7.5       , 11.49999994,  0.        ]), 'targetState': array([ 8, 12], dtype=int32), 'currentDistance': 0.7071068233333766}
episode index:287
target Thresh 30.54850747955592
target distance 6.0
model initialize at round 287
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 18.]), 'previousTarget': array([25., 18.]), 'currentState': array([19.45481467, 14.5       ,  0.        ]), 'targetState': array([25, 18], dtype=int32), 'currentDistance': 6.557368398729153}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.047243679447452475
{'scaleFactor': 20, 'currentTarget': array([55.13532575, 49.53842529]), 'previousTarget': array([54.63902266, 49.03489388]), 'currentState': array([68.95214763, 63.99855784,  0.        ]), 'targetState': array([25, 18], dtype=int32), 'currentDistance': 20.0}
episode index:288
target Thresh 30.562950071446178
target distance 20.0
model initialize at round 288
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8.99719013, 23.98782391]), 'previousTarget': array([ 8.9223227 , 23.61161351]), 'currentState': array([4.5, 4.5, 0. ]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04708020650818793
{'scaleFactor': 20, 'currentTarget': array([12.04320839, 34.75399116]), 'previousTarget': array([11.76077846, 34.19374803]), 'currentState': array([17.48903805, 53.99828744,  0.        ]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 20.0}
episode index:289
target Thresh 30.577248957146033
target distance 6.0
model initialize at round 289
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 17.]), 'previousTarget': array([24., 17.]), 'currentState': array([26.5       , 23.49990541,  0.        ]), 'targetState': array([24, 17], dtype=int32), 'currentDistance': 6.9641058511140335}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.046917860968504525
{'scaleFactor': 20, 'currentTarget': array([62.39093386, 58.34383285]), 'previousTarget': array([61.89600026, 57.83913006]), 'currentState': array([76.        , 72.99966168,  0.        ]), 'targetState': array([24, 17], dtype=int32), 'currentDistance': 20.0}
episode index:290
target Thresh 30.591405566555974
target distance 23.0
model initialize at round 290
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4.69786843, 24.36938123]), 'previousTarget': array([ 4.58678368, 23.83200822]), 'currentState': array([2.41583216, 4.5       , 0.        ]), 'targetState': array([ 5, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04675663120572616
{'scaleFactor': 20, 'currentTarget': array([34.56091478, 44.02046944]), 'previousTarget': array([34.02668288, 43.58016015]), 'currentState': array([51.89322478, 54.        ,  0.        ]), 'targetState': array([ 5, 27], dtype=int32), 'currentDistance': 20.0}
episode index:291
target Thresh 30.60542131534874
target distance 22.0
model initialize at round 291
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.32728486,  8.37187469]), 'previousTarget': array([17.22895002,  8.17429997]), 'currentState': array([ 5.5       , 24.49996611,  0.        ]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04659650575639148
{'scaleFactor': 20, 'currentTarget': array([46.66681041, 55.81775004]), 'previousTarget': array([46.22384557, 55.29171794]), 'currentState': array([55.        , 73.99900278,  0.        ]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 20.0}
episode index:292
target Thresh 30.619297605110887
target distance 16.0
model initialize at round 292
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  6.]), 'previousTarget': array([25.,  6.]), 'currentState': array([9.49999991, 7.5       , 0.        ]), 'targetState': array([25,  6], dtype=int32), 'currentDistance': 15.572411591388654}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0464374733135369
{'scaleFactor': 20, 'currentTarget': array([47.90549858, 40.3560228 ]), 'previousTarget': array([47.44360295, 39.83068146]), 'currentState': array([58.99999991, 56.99669707,  0.        ]), 'targetState': array([25,  6], dtype=int32), 'currentDistance': 20.0}
episode index:293
target Thresh 30.633035823482956
target distance 11.0
model initialize at round 293
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 19.]), 'previousTarget': array([ 5., 19.]), 'currentState': array([15.5,  8.5,  0. ]), 'targetState': array([ 5, 19], dtype=int32), 'currentDistance': 14.849242404917497}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04627952272403507
{'scaleFactor': 20, 'currentTarget': array([48.23112754, 47.10023215]), 'previousTarget': array([47.70859674, 46.6349737 ]), 'currentState': array([65.        , 57.99999896,  0.        ]), 'targetState': array([ 5, 19], dtype=int32), 'currentDistance': 20.0}
episode index:294
target Thresh 30.64663734429823
target distance 9.0
model initialize at round 294
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 26.]), 'previousTarget': array([10., 26.]), 'currentState': array([12.5, 17.5,  0. ]), 'targetState': array([10, 26], dtype=int32), 'currentDistance': 8.86002257333462}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0461226429859875
{'scaleFactor': 20, 'currentTarget': array([45.46023727, 50.10727727]), 'previousTarget': array([45.00924095, 50.02371084]), 'currentState': array([62.       , 61.3516647,  0.       ]), 'targetState': array([10, 26], dtype=int32), 'currentDistance': 20.0}
episode index:295
target Thresh 30.660103527720132
target distance 15.0
model initialize at round 295
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  2.]), 'previousTarget': array([26.,  2.]), 'currentState': array([11.5       ,  7.49946013,  0.        ]), 'targetState': array([26,  2], dtype=int32), 'currentDistance': 15.50787096063433}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04596682324616997
{'scaleFactor': 20, 'currentTarget': array([43.60678331, 11.99422198]), 'previousTarget': array([43.06128214, 11.5804588 ]), 'currentState': array([61.        , 21.86721612,  0.        ]), 'targetState': array([26,  2], dtype=int32), 'currentDistance': 20.0}
episode index:296
target Thresh 30.67343572037822
target distance 21.0
model initialize at round 296
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.35786469, 18.64213502]), 'previousTarget': array([10.20689655, 18.48275862]), 'currentState': array([24.5       ,  4.49999908,  0.        ]), 'targetState': array([ 4, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.045812052797529666
{'scaleFactor': 20, 'currentTarget': array([55.51992576, 46.32066053]), 'previousTarget': array([54.99257667, 45.88822577]), 'currentState': array([74.       , 53.9683305,  0.       ]), 'targetState': array([ 4, 25], dtype=int32), 'currentDistance': 20.0}
episode index:297
target Thresh 30.68663525550287
target distance 17.0
model initialize at round 297
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 22.]), 'previousTarget': array([ 6., 22.]), 'currentState': array([23.5       , 22.49952927,  0.        ]), 'targetState': array([ 6, 22], dtype=int32), 'currentDistance': 17.507127962458757}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04565832107673259
{'scaleFactor': 20, 'currentTarget': array([56.89184746, 59.45269058]), 'previousTarget': array([56.37821286, 58.98537359]), 'currentState': array([73.        , 71.30711684,  0.        ]), 'targetState': array([ 6, 22], dtype=int32), 'currentDistance': 20.0}
episode index:298
target Thresh 30.699703453058596
target distance 9.0
model initialize at round 298
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 28.]), 'previousTarget': array([16., 28.]), 'currentState': array([ 7.4998619 , 21.49999937,  0.        ]), 'targetState': array([16, 28], dtype=int32), 'currentDistance': 10.700577363977228}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04550561766176024
{'scaleFactor': 20, 'currentTarget': array([39.79273537, 42.09533546]), 'previousTarget': array([39.25591192, 41.66371327]), 'currentState': array([56.99985471, 52.28920833,  0.        ]), 'targetState': array([16, 28], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:299
target Thresh 30.71264161987605
target distance 11.0
model initialize at round 299
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 26.]), 'previousTarget': array([14., 26.]), 'currentState': array([25.5       , 20.47315675,  0.        ]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 12.759153432255573}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04535393226955437
{'scaleFactor': 20, 'currentTarget': array([58.73380922, 58.00199495]), 'previousTarget': array([58.21704874, 57.53603293]), 'currentState': array([75.        , 69.63861978,  0.        ]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 20.0}
episode index:300
target Thresh 30.725451049782684
target distance 8.0
model initialize at round 300
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([11.5       , 21.50009248,  0.        ]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 7.6486199511839486}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04520325475370868
{'scaleFactor': 20, 'currentTarget': array([44.93594672, 39.91070646]), 'previousTarget': array([44.41778857, 39.44510606]), 'currentState': array([61.        , 51.82482382,  0.        ]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 20.0}
episode index:301
target Thresh 30.738133023732168
target distance 24.0
model initialize at round 301
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.59746503, 13.80560614]), 'previousTarget': array([10.04003392, 13.4000212 ]), 'currentState': array([27.5       , 24.49692559,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04505357510220633
{'scaleFactor': 20, 'currentTarget': array([61.56338595, 57.24405924]), 'previousTarget': array([61.06070117, 56.8093756 ]), 'currentState': array([77.        , 69.96062115,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 20.0}
episode index:302
target Thresh 30.75068880993247
target distance 1.0
model initialize at round 302
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  8.]), 'previousTarget': array([17.,  8.]), 'currentState': array([16.5       ,  7.50019908,  0.        ]), 'targetState': array([17,  8], dtype=int32), 'currentDistance': 0.706966024727541}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.04820521346820565
{'scaleFactor': 20, 'currentTarget': array([17.,  8.]), 'previousTarget': array([17.,  8.]), 'currentState': array([16.5       ,  7.50019908,  0.        ]), 'targetState': array([17,  8], dtype=int32), 'currentDistance': 0.706966024727541}
episode index:303
target Thresh 30.763119663972663
target distance 21.0
model initialize at round 303
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.16030045, 25.97484227]), 'previousTarget': array([15.87838597, 25.3829006 ]), 'currentState': array([8.5, 7.5, 0. ]), 'targetState': array([17, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.048046643687060235
{'scaleFactor': 20, 'currentTarget': array([40.80878179, 42.15499512]), 'previousTarget': array([40.27319574, 41.72680432]), 'currentState': array([58.        , 52.37566127,  0.        ]), 'targetState': array([17, 28], dtype=int32), 'currentDistance': 20.0}
episode index:304
target Thresh 30.77542682894852
target distance 1.0
model initialize at round 304
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 12.]), 'previousTarget': array([25., 12.]), 'currentState': array([25.5       , 12.50001439,  0.        ]), 'targetState': array([25, 12], dtype=int32), 'currentDistance': 0.7071169597237666}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.05116780223234856
{'scaleFactor': 20, 'currentTarget': array([25., 12.]), 'previousTarget': array([25., 12.]), 'currentState': array([25.5       , 12.50001439,  0.        ]), 'targetState': array([25, 12], dtype=int32), 'currentDistance': 0.7071169597237666}
episode index:305
target Thresh 30.78761153558679
target distance 11.0
model initialize at round 305
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 23.]), 'previousTarget': array([20., 23.]), 'currentState': array([16.5       , 12.49999991,  0.        ]), 'targetState': array([20, 23], dtype=int32), 'currentDistance': 11.067971895408188}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05100058719237357
{'scaleFactor': 20, 'currentTarget': array([49.34863481, 42.52616838]), 'previousTarget': array([48.82158276, 42.0739581 ]), 'currentState': array([66.        , 53.60461761,  0.        ]), 'targetState': array([20, 23], dtype=int32), 'currentDistance': 20.0}
episode index:306
target Thresh 30.799675002368296
target distance 7.0
model initialize at round 306
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 18.]), 'previousTarget': array([ 8., 18.]), 'currentState': array([ 3.5       , 24.50001624,  0.        ]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 7.905707504690333}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.050834461501193196
{'scaleFactor': 20, 'currentTarget': array([36.22870729, 36.33974268]), 'previousTarget': array([35.70134681, 35.89817479]), 'currentState': array([53.        , 47.23578512,  0.        ]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 20.0}
episode index:307
target Thresh 30.81161843564976
target distance 20.0
model initialize at round 307
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6.78522425, 5.13405133]), 'previousTarget': array([6.22127294, 5.03319094]), 'currentState': array([26.5       ,  8.49970388,  0.        ]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05066941454826725
{'scaleFactor': 20, 'currentTarget': array([59.92619794, 44.92668375]), 'previousTarget': array([59.41425147, 44.46463518]), 'currentState': array([76.        , 56.82764536,  0.        ]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 20.0}
episode index:308
target Thresh 30.823443029784467
target distance 10.0
model initialize at round 308
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([26.5       , 28.50456882,  0.        ]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 10.516027213899195}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05050543586040877
{'scaleFactor': 20, 'currentTarget': array([60.42313635, 49.9429551 ]), 'previousTarget': array([59.9126873 , 49.47982677]), 'currentState': array([76.       , 62.4873291,  0.       ]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 20.0}
episode index:309
target Thresh 30.835149967241687
target distance 12.0
model initialize at round 309
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 28.]), 'previousTarget': array([ 7., 28.]), 'currentState': array([ 9.5       , 16.49999928,  0.        ]), 'targetState': array([ 7, 28], dtype=int32), 'currentDistance': 11.768602994870985}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.050342515099568746
{'scaleFactor': 20, 'currentTarget': array([42.28931032, 51.20585062]), 'previousTarget': array([41.76465852, 50.74910478]), 'currentState': array([59.        , 62.19461082,  0.        ]), 'targetState': array([ 7, 28], dtype=int32), 'currentDistance': 20.0}
episode index:310
target Thresh 30.84674041872492
target distance 13.0
model initialize at round 310
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([14.5      , 18.9968551,  0.       ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 17.354199562933324}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05018064206066338
{'scaleFactor': 20, 'currentTarget': array([49.36258586, 49.1681215 ]), 'previousTarget': array([48.86139521, 48.69398404]), 'currentState': array([64.        , 62.79699186,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 20.0}
episode index:311
target Thresh 30.85821554328897
target distance 11.0
model initialize at round 311
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  2.]), 'previousTarget': array([23.,  2.]), 'currentState': array([15.5       , 12.50001672,  0.        ]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 12.903501505461133}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.050019806669443306
{'scaleFactor': 20, 'currentTarget': array([47.22151918, 14.48113295]), 'previousTarget': array([46.6793718 , 14.06360755]), 'currentState': array([65.        , 23.64222566,  0.        ]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 20.0}
episode index:312
target Thresh 30.869576488455863
target distance 14.0
model initialize at round 312
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 25.]), 'previousTarget': array([ 9., 25.]), 'currentState': array([23.5       , 16.49457005,  0.        ]), 'targetState': array([ 9, 25], dtype=int32), 'currentDistance': 16.81048299993552}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04985999898040355
{'scaleFactor': 20, 'currentTarget': array([56.15018584, 55.14907776]), 'previousTarget': array([55.62858528, 54.68380959]), 'currentState': array([73.        , 65.92329526,  0.        ]), 'targetState': array([ 9, 25], dtype=int32), 'currentDistance': 20.0}
episode index:313
target Thresh 30.880824390329575
target distance 13.0
model initialize at round 313
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 28.]), 'previousTarget': array([17., 28.]), 'currentState': array([ 5.5, 15.5,  0. ]), 'targetState': array([17, 28], dtype=int32), 'currentDistance': 16.985287751462973}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.049701209174733474
{'scaleFactor': 20, 'currentTarget': array([36.79278872, 36.99706344]), 'previousTarget': array([36.24311397, 36.60728825]), 'currentState': array([55.        , 45.27338251,  0.        ]), 'targetState': array([17, 28], dtype=int32), 'currentDistance': 20.0}
episode index:314
target Thresh 30.891960373709672
target distance 17.0
model initialize at round 314
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.90796309, 18.93585307]), 'previousTarget': array([25.33935727, 18.53366395]), 'currentState': array([9.49999964, 7.5       , 0.        ]), 'targetState': array([26, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04954342755830574
{'scaleFactor': 20, 'currentTarget': array([40.47423324, 24.88814222]), 'previousTarget': array([39.91561199, 24.53391073]), 'currentState': array([58.99999896, 32.42445461,  0.        ]), 'targetState': array([26, 19], dtype=int32), 'currentDistance': 20.0}
episode index:315
target Thresh 30.90298555220377
target distance 18.0
model initialize at round 315
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 27.]), 'previousTarget': array([ 7., 27.]), 'currentState': array([10.5       ,  9.49999997,  0.        ]), 'targetState': array([ 7, 27], dtype=int32), 'currentDistance': 17.846568326798288}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.049386644559703506
{'scaleFactor': 20, 'currentTarget': array([42.85812702, 48.5527153 ]), 'previousTarget': array([42.329409  , 48.10079057]), 'currentState': array([60.        , 58.85592794,  0.        ]), 'targetState': array([ 7, 27], dtype=int32), 'currentDistance': 20.0}
episode index:316
target Thresh 30.91390102833891
target distance 10.0
model initialize at round 316
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 22.]), 'previousTarget': array([20., 22.]), 'currentState': array([26.5       , 12.49295539,  0.        ]), 'targetState': array([20, 22], dtype=int32), 'currentDistance': 11.516679090798949}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04923085072828488
{'scaleFactor': 20, 'currentTarget': array([59.18371287, 47.22681442]), 'previousTarget': array([58.65916809, 46.76509764]), 'currentState': array([76.        , 58.05328601,  0.        ]), 'targetState': array([20, 22], dtype=int32), 'currentDistance': 20.0}
episode index:317
target Thresh 30.924707893671794
target distance 4.0
model initialize at round 317
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 27.]), 'previousTarget': array([ 7., 27.]), 'currentState': array([11.5       , 24.50874949,  0.        ]), 'targetState': array([ 7, 27], dtype=int32), 'currentDistance': 5.143571631503606}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04907603673228399
{'scaleFactor': 20, 'currentTarget': array([44.75938475, 54.13804762]), 'previousTarget': array([44.23918419, 53.66644009]), 'currentState': array([61.        , 65.81034029,  0.        ]), 'targetState': array([ 7, 27], dtype=int32), 'currentDistance': 20.0}
episode index:318
target Thresh 30.93540722889797
target distance 23.0
model initialize at round 318
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9.95154291, 9.00121871]), 'previousTarget': array([9.57871024, 8.58189596]), 'currentState': array([21.5       , 25.33012372,  0.        ]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.048922193356947676
{'scaleFactor': 20, 'currentTarget': array([57.3874417 , 58.38994832]), 'previousTarget': array([56.89216966, 57.89298815]), 'currentState': array([71.        , 73.04253364,  0.        ]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 20.0}
episode index:319
target Thresh 30.946000103959864
target distance 1.0
model initialize at round 319
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 13.]), 'previousTarget': array([ 3., 13.]), 'currentState': array([ 2.5       , 13.50037095,  0.        ]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 0.7073691307303634}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.051894311502707215
{'scaleFactor': 20, 'currentTarget': array([ 3., 13.]), 'previousTarget': array([ 3., 13.]), 'currentState': array([ 2.5       , 13.50037095,  0.        ]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 0.7073691307303634}
episode index:320
target Thresh 30.956487578153826
target distance 10.0
model initialize at round 320
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  8.]), 'previousTarget': array([24.,  8.]), 'currentState': array([14.5       , 12.50934586,  0.        ]), 'targetState': array([24,  8], dtype=int32), 'currentDistance': 10.51590224748624}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05173264698089192
{'scaleFactor': 20, 'currentTarget': array([47.6381978 , 24.61686447]), 'previousTarget': array([47.10930439, 24.15832806]), 'currentState': array([64.        , 36.11866558,  0.        ]), 'targetState': array([24,  8], dtype=int32), 'currentDistance': 20.0}
episode index:321
target Thresh 30.966870700236
target distance 13.0
model initialize at round 321
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 27.]), 'previousTarget': array([16., 27.]), 'currentState': array([ 3.5       , 27.49785191,  0.        ]), 'targetState': array([16, 27], dtype=int32), 'currentDistance': 12.509910332312751}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05157198658654133
{'scaleFactor': 20, 'currentTarget': array([35.93173186, 39.17359503]), 'previousTarget': array([35.3912859 , 38.74012476]), 'currentState': array([53.        , 49.59828796,  0.        ]), 'targetState': array([16, 27], dtype=int32), 'currentDistance': 20.0}
episode index:322
target Thresh 30.97715050852726
target distance 21.0
model initialize at round 322
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.34786235, 23.89929041]), 'previousTarget': array([18.83071558, 23.41826648]), 'currentState': array([ 4.5, 10.5,  0. ]), 'targetState': array([25, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05141232099339414
{'scaleFactor': 20, 'currentTarget': array([38.22817368, 39.31489634]), 'previousTarget': array([37.6986203 , 38.85291443]), 'currentState': array([54.        , 51.61324967,  0.        ]), 'targetState': array([25, 29], dtype=int32), 'currentDistance': 20.0}
episode index:323
target Thresh 30.987328031016993
target distance 16.0
model initialize at round 323
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([10.5       , 20.17167376,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 17.42908581062108}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.051253640990328114
{'scaleFactor': 20, 'currentTarget': array([46.63920846, 51.49558836]), 'previousTarget': array([46.14632869, 50.99176937]), 'currentState': array([60.        , 66.37810326,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 20.0}
episode index:324
target Thresh 30.99740428546593
target distance 9.0
model initialize at round 324
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 16.]), 'previousTarget': array([ 9., 16.]), 'currentState': array([18.5       , 11.49997491,  0.        ]), 'targetState': array([ 9, 16], dtype=int32), 'currentDistance': 10.511908763047288}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05109593747958864
{'scaleFactor': 20, 'currentTarget': array([52.09754725, 48.87096738]), 'previousTarget': array([51.58199321, 48.3913877 ]), 'currentState': array([68.        , 60.99994081,  0.        ]), 'targetState': array([ 9, 16], dtype=int32), 'currentDistance': 20.0}
episode index:325
target Thresh 31.007380279507917
target distance 3.0
model initialize at round 325
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 21.]), 'previousTarget': array([10., 21.]), 'currentState': array([ 9.5       , 18.49999994,  0.        ]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 2.5495098152435047}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.050939201475050024
{'scaleFactor': 20, 'currentTarget': array([44.5663404 , 54.15533463]), 'previousTarget': array([44.06330633, 53.65850271]), 'currentState': array([59.      , 67.999809,  0.      ]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 20.0}
episode index:326
target Thresh 31.01725701075067
target distance 14.0
model initialize at round 326
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 4.]), 'previousTarget': array([8., 4.]), 'currentState': array([22.5, 10.5,  0. ]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 15.89024858207081}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05078342410050859
{'scaleFactor': 20, 'currentTarget': array([56.94846015, 46.82986357]), 'previousTarget': array([56.44111726, 46.33826103]), 'currentState': array([72.        , 59.99994892,  0.        ]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 20.0}
episode index:327
target Thresh 31.02703546687554
target distance 6.0
model initialize at round 327
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 11.]), 'previousTarget': array([27., 11.]), 'currentState': array([21.5       ,  7.49999994,  0.        ]), 'targetState': array([27, 11], dtype=int32), 'currentDistance': 6.519202437202838}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.05341371014740838
{'scaleFactor': 20, 'currentTarget': array([27., 11.]), 'previousTarget': array([27., 11.]), 'currentState': array([26.        , 11.99967578,  0.        ]), 'targetState': array([27, 11], dtype=int32), 'currentDistance': 1.413984323175706}
episode index:328
target Thresh 31.03671662573629
target distance 15.0
model initialize at round 328
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([ 8.5, 10.5,  0. ]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 14.713938969562095}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05325135844483267
{'scaleFactor': 20, 'currentTarget': array([41.40824504, 48.8324726 ]), 'previousTarget': array([40.88385449, 48.36879573]), 'currentState': array([58.        , 59.99999997,  0.        ]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 20.0}
episode index:329
target Thresh 31.046301455456874
target distance 5.0
model initialize at round 329
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([21.5       , 20.49998459,  0.        ]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 7.778163698099406}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05308999069196954
{'scaleFactor': 20, 'currentTarget': array([56.85545109, 55.84150869]), 'previousTarget': array([56.35545455, 55.34170258]), 'currentState': array([71.        , 69.98123062,  0.        ]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 20.0}
episode index:330
target Thresh 31.055790914528256
target distance 4.0
model initialize at round 330
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([12.5       ,  8.30923253,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 4.816743708909608}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05292959797084577
{'scaleFactor': 20, 'currentTarget': array([48.62507127, 41.2714939 ]), 'previousTarget': array([48.13358805, 40.7662479 ]), 'currentState': array([62.        , 56.14130497,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 20.0}
episode index:331
target Thresh 31.065185951904247
target distance 24.0
model initialize at round 331
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.46425177, 17.29095276]), 'previousTarget': array([20.88854382, 16.94427191]), 'currentState': array([3.4999994 , 8.49972776, 0.        ]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.052770171470933575
{'scaleFactor': 20, 'currentTarget': array([37.09102872, 27.68796266]), 'previousTarget': array([36.66157473, 27.44971055]), 'currentState': array([52.99999765, 39.80838789,  0.        ]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 20.0}
episode index:332
target Thresh 31.07448750709641
target distance 13.0
model initialize at round 332
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([ 8.5       , 16.50010684,  0.        ]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 14.089052095462192}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.05499492856454535
{'scaleFactor': 20, 'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([20.        ,  9.63183123,  0.        ]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 1.0656210589921102}
episode index:333
target Thresh 31.083696510268023
target distance 4.0
model initialize at round 333
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 7.]), 'previousTarget': array([9., 7.]), 'currentState': array([11.5       , 11.06468976,  0.        ]), 'targetState': array([9, 7], dtype=int32), 'currentDistance': 4.771970543553931}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05483027308980121
{'scaleFactor': 20, 'currentTarget': array([45.24974166, 35.3686123 ]), 'previousTarget': array([44.75895319, 35.02762212]), 'currentState': array([61.        , 47.69457524,  0.        ]), 'targetState': array([9, 7], dtype=int32), 'currentDistance': 20.0}
episode index:334
target Thresh 31.092813882327075
target distance 14.0
model initialize at round 334
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 13.]), 'previousTarget': array([26., 13.]), 'currentState': array([12.5       ,  7.76201414,  0.        ]), 'targetState': array([26, 13], dtype=int32), 'currentDistance': 14.480555787483196}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.05698845543699329
{'scaleFactor': 20, 'currentTarget': array([26., 13.]), 'previousTarget': array([26., 13.]), 'currentState': array([25.        , 12.91830997,  0.        ]), 'targetState': array([26, 13], dtype=int32), 'currentDistance': 1.003331082305881}
episode index:335
target Thresh 31.10184053501837
target distance 15.0
model initialize at round 335
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 10.]), 'previousTarget': array([18., 10.]), 'currentState': array([ 3.5       , 12.50012493,  0.        ]), 'targetState': array([18, 10], dtype=int32), 'currentDistance': 14.713960196775199}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.059087724064701946
{'scaleFactor': 20, 'currentTarget': array([18., 10.]), 'previousTarget': array([18., 10.]), 'currentState': array([17.        , 10.01908594,  0.        ]), 'targetState': array([18, 10], dtype=int32), 'currentDistance': 1.0001821199315049}
episode index:336
target Thresh 31.11077737101469
target distance 10.0
model initialize at round 336
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 19.]), 'previousTarget': array([ 7., 19.]), 'currentState': array([ 3.5       , 29.04885446,  0.        ]), 'targetState': array([ 7, 19], dtype=int32), 'currentDistance': 10.640933982249132}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.058912389571928354
{'scaleFactor': 20, 'currentTarget': array([39.75195009, 56.04101751]), 'previousTarget': array([39.26183055, 55.53522284]), 'currentState': array([53.        , 71.02398028,  0.        ]), 'targetState': array([ 7, 19], dtype=int32), 'currentDistance': 20.0}
episode index:337
target Thresh 31.1196252840071
target distance 11.0
model initialize at round 337
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 29.]), 'previousTarget': array([ 8., 29.]), 'currentState': array([19.5, 20.5,  0. ]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 14.300349646075107}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05873809256136052
{'scaleFactor': 20, 'currentTarget': array([52.40085204, 58.84247674]), 'previousTarget': array([51.88003706, 58.37356211]), 'currentState': array([69.        , 69.99901235,  0.        ]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 20.0}
episode index:338
target Thresh 31.128385158794256
target distance 21.0
model initialize at round 338
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.28401803, 10.00932291]), 'previousTarget': array([16.79310345, 10.51724138]), 'currentState': array([ 3.5       , 24.50072888,  0.        ]), 'targetState': array([23,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.058564823851739986
{'scaleFactor': 20, 'currentTarget': array([38.68640787, 19.30829988]), 'previousTarget': array([38.18362231, 18.81172258]), 'currentState': array([53.        , 33.27687462,  0.        ]), 'targetState': array([23,  4], dtype=int32), 'currentDistance': 20.0}
episode index:339
target Thresh 31.137057871370946
target distance 15.0
model initialize at round 339
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 6.]), 'previousTarget': array([5., 6.]), 'currentState': array([ 6.5       , 21.39573634,  0.        ]), 'targetState': array([5, 6], dtype=int32), 'currentDistance': 15.468635924005364}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0583925743698231
{'scaleFactor': 20, 'currentTarget': array([43.46019661, 53.78629627]), 'previousTarget': array([42.97502741, 53.27550495]), 'currentState': array([56.        , 69.36683962,  0.        ]), 'targetState': array([5, 6], dtype=int32), 'currentDistance': 20.0}
episode index:340
target Thresh 31.145644289015653
target distance 3.0
model initialize at round 340
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([11.5       , 21.49959141,  0.        ]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 3.5358228343833127}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.058221335148797224
{'scaleFactor': 20, 'currentTarget': array([46.1261494 , 57.37352453]), 'previousTarget': array([45.61894002, 56.88188901]), 'currentState': array([61.        , 70.74396089,  0.        ]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 20.0}
episode index:341
target Thresh 31.154145270377295
target distance 7.0
model initialize at round 341
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 21.]), 'previousTarget': array([24., 21.]), 'currentState': array([22.5, 14.5,  0. ]), 'targetState': array([24, 21], dtype=int32), 'currentDistance': 6.670832032063071}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05805109732672472
{'scaleFactor': 20, 'currentTarget': array([57.06584783, 50.4544057 ]), 'previousTarget': array([56.55734127, 49.96410796]), 'currentState': array([72.        , 63.75745419,  0.        ]), 'targetState': array([24, 21], dtype=int32), 'currentDistance': 20.0}
episode index:342
target Thresh 31.162561665561093
target distance 19.0
model initialize at round 342
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 24.]), 'previousTarget': array([16.76114  , 23.4327075]), 'currentState': array([9.5, 5.5, 0. ]), 'targetState': array([17, 24], dtype=int32), 'currentDistance': 19.962464777677106}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05788185214501415
{'scaleFactor': 20, 'currentTarget': array([42.90788331, 43.1203452 ]), 'previousTarget': array([42.38362368, 42.65335159]), 'currentState': array([59.        , 54.99653062,  0.        ]), 'targetState': array([17, 24], dtype=int32), 'currentDistance': 20.0}
episode index:343
target Thresh 31.17089431621358
target distance 5.0
model initialize at round 343
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 15.]), 'previousTarget': array([ 8., 15.]), 'currentState': array([13.5      , 16.4204751,  0.       ]), 'targetState': array([ 8, 15], dtype=int32), 'currentDistance': 5.680470886904212}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.057713590946918185
{'scaleFactor': 20, 'currentTarget': array([48.30536355, 52.21283369]), 'previousTarget': array([47.80024165, 51.71863887]), 'currentState': array([63.        , 65.77998739,  0.        ]), 'targetState': array([ 8, 15], dtype=int32), 'currentDistance': 20.0}
episode index:344
target Thresh 31.179144055606763
target distance 3.0
model initialize at round 344
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 29.]), 'previousTarget': array([24., 29.]), 'currentState': array([25.5       , 26.49963894,  0.        ]), 'targetState': array([24, 29], dtype=int32), 'currentDistance': 2.915785555564725}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05754630517605755
{'scaleFactor': 20, 'currentTarget': array([60.14728129, 61.59694984]), 'previousTarget': array([59.64014097, 61.10517575]), 'currentState': array([75.        , 74.99085692,  0.        ]), 'targetState': array([24, 29], dtype=int32), 'currentDistance': 20.0}
episode index:345
target Thresh 31.18731170872146
target distance 11.0
model initialize at round 345
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 12.]), 'previousTarget': array([18., 12.]), 'currentState': array([13.5       , 23.47128579,  0.        ]), 'targetState': array([18, 12], dtype=int32), 'currentDistance': 12.322353577160424}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05737998637497068
{'scaleFactor': 20, 'currentTarget': array([50.90638933, 55.34336613]), 'previousTarget': array([50.42767764, 54.83149358]), 'currentState': array([63.        , 71.27272835,  0.        ]), 'targetState': array([18, 12], dtype=int32), 'currentDistance': 20.0}
episode index:346
target Thresh 31.195398092329786
target distance 16.0
model initialize at round 346
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 16.]), 'previousTarget': array([ 3., 16.]), 'currentState': array([19.5       , 15.49999925,  0.        ]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 16.507574041786427}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05721462618368834
{'scaleFactor': 20, 'currentTarget': array([52.94045211, 53.0682702 ]), 'previousTarget': array([52.42537294, 52.58899128]), 'currentState': array([69.        , 64.98845986,  0.        ]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 20.0}
episode index:347
target Thresh 31.203404015076842
target distance 2.0
model initialize at round 347
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 23.]), 'previousTarget': array([24., 23.]), 'currentState': array([25.5       , 24.97311509,  0.        ]), 'targetState': array([24, 23], dtype=int32), 'currentDistance': 2.4785445647584154}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05705021633833292
{'scaleFactor': 20, 'currentTarget': array([60.55019287, 57.976396  ]), 'previousTarget': array([60.04737263, 57.48114299]), 'currentState': array([75.        , 71.80401596,  0.        ]), 'targetState': array([24, 23], dtype=int32), 'currentDistance': 20.0}
episode index:348
target Thresh 31.211330277561572
target distance 9.0
model initialize at round 348
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([6.5       , 2.64200163, 0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 8.524210584651176}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.05935109925610643
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.        ,  2.63104283,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 1.1824614407283416}
episode index:349
target Thresh 31.219177672416834
target distance 18.0
model initialize at round 349
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 24.]), 'previousTarget': array([ 4., 24.]), 'currentState': array([22.5      , 23.4999994,  0.       ]), 'targetState': array([ 4, 24], dtype=int32), 'currentDistance': 18.506755539425324}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.059181524686803266
{'scaleFactor': 20, 'currentTarget': array([55.77273747, 61.29940624]), 'previousTarget': array([55.25683609, 60.82182791]), 'currentState': array([72.        , 72.99025527,  0.        ]), 'targetState': array([ 4, 24], dtype=int32), 'currentDistance': 20.0}
episode index:350
target Thresh 31.226946984388647
target distance 14.0
model initialize at round 350
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 27.]), 'previousTarget': array([24., 27.]), 'currentState': array([15.5, 13.5,  0. ]), 'targetState': array([24, 27], dtype=int32), 'currentDistance': 15.953056133543706}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05901291635436223
{'scaleFactor': 20, 'currentTarget': array([49.77209102, 48.94356455]), 'previousTarget': array([49.25836532, 48.46004389]), 'currentState': array([65.        , 61.90931898,  0.        ]), 'targetState': array([24, 27], dtype=int32), 'currentDistance': 20.0}
episode index:351
target Thresh 31.234638990414687
target distance 16.0
model initialize at round 351
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 18.]), 'previousTarget': array([24., 18.]), 'currentState': array([ 8.5      , 14.5017705,  0.       ]), 'targetState': array([24, 18], dtype=int32), 'currentDistance': 15.88985870484932}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.060967914019070285
{'scaleFactor': 20, 'currentTarget': array([24., 18.]), 'previousTarget': array([24., 18.]), 'currentState': array([23.        , 18.41790202,  0.        ]), 'targetState': array([24, 18], dtype=int32), 'currentDistance': 1.0838090708449042}
episode index:352
target Thresh 31.242254459701964
target distance 7.0
model initialize at round 352
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 23.]), 'previousTarget': array([12., 23.]), 'currentState': array([ 5.5       , 26.61207488,  0.        ]), 'targetState': array([12, 23], dtype=int32), 'currentDistance': 7.436200975745199}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.06333156937385682
{'scaleFactor': 20, 'currentTarget': array([12., 23.]), 'previousTarget': array([12., 23.]), 'currentState': array([11.        , 23.58207701,  0.        ]), 'targetState': array([12, 23], dtype=int32), 'currentDistance': 1.1570711504815967}
episode index:353
target Thresh 31.249794153803755
target distance 11.0
model initialize at round 353
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 29.]), 'previousTarget': array([25., 29.]), 'currentState': array([15.5, 18.5,  0. ]), 'targetState': array([25, 29], dtype=int32), 'currentDistance': 14.159802258506181}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.06546313821064603
{'scaleFactor': 20, 'currentTarget': array([25., 29.]), 'previousTarget': array([25., 29.]), 'currentState': array([25.5      , 28.4326584,  0.       ]), 'targetState': array([25, 29], dtype=int32), 'currentDistance': 0.7562251559062066}
episode index:354
target Thresh 31.257258826695754
target distance 13.0
model initialize at round 354
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  4.]), 'previousTarget': array([18.,  4.]), 'currentState': array([5.5       , 9.50008419, 0.        ]), 'targetState': array([18,  4], dtype=int32), 'currentDistance': 13.656534191157583}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.06751426819778128
{'scaleFactor': 20, 'currentTarget': array([18.,  4.]), 'previousTarget': array([18.,  4.]), 'currentState': array([17.        ,  4.54653334,  0.        ]), 'targetState': array([18,  4], dtype=int32), 'currentDistance': 1.1396046219665297}
episode index:355
target Thresh 31.26464922485147
target distance 20.0
model initialize at round 355
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 27.]), 'previousTarget': array([10.96680906, 26.77872706]), 'currentState': array([8.5, 7.5, 0. ]), 'targetState': array([11, 27], dtype=int32), 'currentDistance': 19.659603251337494}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.06732462137700099
{'scaleFactor': 20, 'currentTarget': array([41.1414683 , 46.23889008]), 'previousTarget': array([40.61174829, 45.78562384]), 'currentState': array([58.        , 56.99946204,  0.        ]), 'targetState': array([11, 27], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:356
target Thresh 31.271966087316876
target distance 6.0
model initialize at round 356
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 23.]), 'previousTarget': array([13., 23.]), 'currentState': array([ 9.5       , 29.36268958,  0.        ]), 'targetState': array([13, 23], dtype=int32), 'currentDistance': 7.2618054744412355}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.06713603700339595
{'scaleFactor': 20, 'currentTarget': array([46.1179188 , 62.33083515]), 'previousTarget': array([45.63110961, 61.82056196]), 'currentState': array([59.        , 77.62959275,  0.        ]), 'targetState': array([13, 23], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:357
target Thresh 31.27921014578432
target distance 13.0
model initialize at round 357
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 21.]), 'previousTarget': array([24., 21.]), 'currentState': array([11.5       , 16.17384784,  0.        ]), 'targetState': array([24, 21], dtype=int32), 'currentDistance': 13.399318813986172}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.0691653058487598
{'scaleFactor': 20, 'currentTarget': array([24., 21.]), 'previousTarget': array([24., 21.]), 'currentState': array([23.        , 21.14158261,  0.        ]), 'targetState': array([24, 21], dtype=int32), 'currentDistance': 1.0099730862489296}
episode index:358
target Thresh 31.286382124665675
target distance 15.0
model initialize at round 358
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 20.]), 'previousTarget': array([ 2., 20.]), 'currentState': array([2.5, 5.5, 0. ]), 'targetState': array([ 2, 20], dtype=int32), 'currentDistance': 14.508618128546914}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.06897264482968246
{'scaleFactor': 20, 'currentTarget': array([35.61507698, 43.52931101]), 'previousTarget': array([35.09175973, 43.0627631 ]), 'currentState': array([52.       , 54.9981513,  0.       ]), 'targetState': array([ 2, 20], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:359
target Thresh 31.29348274116482
target distance 8.0
model initialize at round 359
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 10.]), 'previousTarget': array([26., 10.]), 'currentState': array([19.5,  2.5,  0. ]), 'targetState': array([26, 10], dtype=int32), 'currentDistance': 9.924716620639556}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.07119423696284717
{'scaleFactor': 20, 'currentTarget': array([26., 10.]), 'previousTarget': array([26., 10.]), 'currentState': array([26.5       ,  9.42864928,  0.        ]), 'targetState': array([26, 10], dtype=int32), 'currentDistance': 0.7592375446794293}
episode index:360
target Thresh 31.30051270534931
target distance 9.0
model initialize at round 360
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 15.]), 'previousTarget': array([ 8., 15.]), 'currentState': array([ 4.5       , 24.47530341,  0.        ]), 'targetState': array([ 8, 15], dtype=int32), 'currentDistance': 10.101058100004861}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0709970230100415
{'scaleFactor': 20, 'currentTarget': array([41.4975976, 56.8252922]), 'previousTarget': array([41.01445416, 56.31334802]), 'currentState': array([54.        , 72.43586344,  0.        ]), 'targetState': array([ 8, 15], dtype=int32), 'currentDistance': 20.0}
episode index:361
target Thresh 31.30747272022143
target distance 5.0
model initialize at round 361
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 24.]), 'previousTarget': array([21., 24.]), 'currentState': array([19.5      , 29.2058156,  0.       ]), 'targetState': array([21, 24], dtype=int32), 'currentDistance': 5.417611654983832}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0708008986370856
{'scaleFactor': 20, 'currentTarget': array([55.49960242, 61.70820923]), 'previousTarget': array([55.00628775, 61.20283677]), 'currentState': array([69.        , 76.46420005,  0.        ]), 'targetState': array([21, 24], dtype=int32), 'currentDistance': 20.0}
episode index:362
target Thresh 31.31436348178846
target distance 5.0
model initialize at round 362
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 18.]), 'previousTarget': array([ 5., 18.]), 'currentState': array([ 7.5       , 23.33567283,  0.        ]), 'targetState': array([ 5, 18], dtype=int32), 'currentDistance': 5.892317413512247}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07060585483918728
{'scaleFactor': 20, 'currentTarget': array([43.16231619, 57.82388508]), 'previousTarget': array([42.66528305, 57.32137907]), 'currentState': array([57.        , 72.26405498,  0.        ]), 'targetState': array([ 5, 18], dtype=int32), 'currentDistance': 20.0}
episode index:363
target Thresh 31.32118567913231
target distance 17.0
model initialize at round 363
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 8.]), 'previousTarget': array([6., 8.]), 'currentState': array([ 5.5       , 25.49458846,  0.        ]), 'targetState': array([6, 8], dtype=int32), 'currentDistance': 17.50173207263108}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07041188271050819
{'scaleFactor': 20, 'currentTarget': array([43.06773604, 57.86133534]), 'previousTarget': array([42.58823046, 57.34774654]), 'currentState': array([55.        , 73.91191405,  0.        ]), 'targetState': array([6, 8], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:364
target Thresh 31.32793999447839
target distance 9.0
model initialize at round 364
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  6.]), 'previousTarget': array([25.,  6.]), 'currentState': array([16.5       ,  8.50173405,  0.        ]), 'targetState': array([25,  6], dtype=int32), 'currentDistance': 8.860512019491978}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.07257529770209936
{'scaleFactor': 20, 'currentTarget': array([25.,  6.]), 'previousTarget': array([25.,  6.]), 'currentState': array([24.        ,  6.34024687,  0.        ]), 'targetState': array([25,  6], dtype=int32), 'currentDistance': 1.0562991696650919}
episode index:365
target Thresh 31.334627103263863
target distance 7.0
model initialize at round 365
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 16.]), 'previousTarget': array([ 7., 16.]), 'currentState': array([12.5       , 23.47917613,  0.        ]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 9.283753316491566}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07237700453897887
{'scaleFactor': 20, 'currentTarget': array([48.09810478, 58.50668281]), 'previousTarget': array([47.60033883, 58.00491419]), 'currentState': array([62.        , 72.88504535,  0.        ]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 20.0}
episode index:366
target Thresh 31.341247674205185
target distance 10.0
model initialize at round 366
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 18.]), 'previousTarget': array([17., 18.]), 'currentState': array([17.5       , 28.46906078,  0.        ]), 'targetState': array([17, 18], dtype=int32), 'currentDistance': 10.480993921685}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07217979199255113
{'scaleFactor': 20, 'currentTarget': array([54.04600914, 61.5776145 ]), 'previousTarget': array([53.55747067, 61.06852455]), 'currentState': array([67.       , 76.8155317,  0.       ]), 'targetState': array([17, 18], dtype=int32), 'currentDistance': 20.0}
episode index:367
target Thresh 31.347802369364967
target distance 9.0
model initialize at round 367
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 14.]), 'previousTarget': array([18., 14.]), 'currentState': array([16.5       , 23.46920639,  0.        ]), 'targetState': array([18, 14], dtype=int32), 'currentDistance': 9.587276448960075}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07198365125344094
{'scaleFactor': 20, 'currentTarget': array([53.09749601, 55.56896247]), 'previousTarget': array([52.60996294, 55.05942074]), 'currentState': array([66.        , 70.85050005,  0.        ]), 'targetState': array([18, 14], dtype=int32), 'currentDistance': 20.0}
episode index:368
target Thresh 31.35429184421818
target distance 17.0
model initialize at round 368
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  8.]), 'previousTarget': array([11.,  8.]), 'currentState': array([ 4.5       , 25.49917832,  0.        ]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 18.667384441348315}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07178857360776766
{'scaleFactor': 20, 'currentTarget': array([42.82206312, 55.21450564]), 'previousTarget': array([42.35177762, 54.69694052]), 'currentState': array([54.        , 71.79924944,  0.        ]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 20.0}
episode index:369
target Thresh 31.36071674771773
target distance 1.0
model initialize at round 369
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  6.]), 'previousTarget': array([18.,  6.]), 'currentState': array([18.5       ,  5.47708121,  0.        ]), 'targetState': array([18,  6], dtype=int32), 'currentDistance': 0.7234943410563502}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.07429725313855748
{'scaleFactor': 20, 'currentTarget': array([18.,  6.]), 'previousTarget': array([18.,  6.]), 'currentState': array([18.5       ,  5.47708121,  0.        ]), 'targetState': array([18,  6], dtype=int32), 'currentDistance': 0.7234943410563502}
episode index:370
target Thresh 31.367077722359312
target distance 5.0
model initialize at round 370
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 22.]), 'previousTarget': array([18., 22.]), 'currentState': array([18.5       , 17.49999997,  0.        ]), 'targetState': array([18, 22], dtype=int32), 'currentDistance': 4.527692598688704}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07409699100071769
{'scaleFactor': 20, 'currentTarget': array([53.01941271, 52.9756209 ]), 'previousTarget': array([52.51074767, 52.48547704]), 'currentState': array([68.        , 66.22635689,  0.        ]), 'targetState': array([18, 22], dtype=int32), 'currentDistance': 20.0}
episode index:371
target Thresh 31.373375404245692
target distance 18.0
model initialize at round 371
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([7.5       , 7.50000504, 0.        ]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 17.564167640120267}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.07582718869318424
{'scaleFactor': 20, 'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([24.        ,  9.35615906,  0.        ]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 1.0615315694244956}
episode index:372
target Thresh 31.379610423150307
target distance 6.0
model initialize at round 372
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 26.]), 'previousTarget': array([14., 26.]), 'currentState': array([19.5, 20.5,  0. ]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 7.7781745930520225}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07562389864306847
{'scaleFactor': 20, 'currentTarget': array([53.38176824, 57.50099182]), 'previousTarget': array([52.86778441, 57.01853342]), 'currentState': array([69.        , 69.99382323,  0.        ]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 20.0}
episode index:373
target Thresh 31.385783402580245
target distance 14.0
model initialize at round 373
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 18.]), 'previousTarget': array([27., 18.]), 'currentState': array([13.5       ,  5.49999985,  0.        ]), 'targetState': array([27, 18], dtype=int32), 'currentDistance': 18.398369594213754}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.0775014319605981
{'scaleFactor': 20, 'currentTarget': array([27., 18.]), 'previousTarget': array([27., 18.]), 'currentState': array([26.        , 17.86593643,  0.        ]), 'targetState': array([27, 18], dtype=int32), 'currentDistance': 1.0089465006880771}
episode index:374
target Thresh 31.39189495983859
target distance 10.0
model initialize at round 374
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 18.]), 'previousTarget': array([14., 18.]), 'currentState': array([7.5, 8.5, 0. ]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 11.510864433221274}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07729476147536983
{'scaleFactor': 20, 'currentTarget': array([41.91400069, 42.29587437]), 'previousTarget': array([41.40263533, 41.80911403]), 'currentState': array([57.        , 55.42647317,  0.        ]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 20.0}
episode index:375
target Thresh 31.397945706086166
target distance 7.0
model initialize at round 375
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([19.5, 13.5,  0. ]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 6.964194138592013}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0770891903012332
{'scaleFactor': 20, 'currentTarget': array([53.57139769, 50.16591379]), 'previousTarget': array([53.05852349, 49.68158688]), 'currentState': array([69.       , 62.8921949,  0.       ]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 20.0}
episode index:376
target Thresh 31.403936246402637
target distance 7.0
model initialize at round 376
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 14.]), 'previousTarget': array([ 6., 14.]), 'currentState': array([ 7.5       , 21.42697889,  0.        ]), 'targetState': array([ 6, 14], dtype=int32), 'currentDistance': 7.576939710396369}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0768847096903546
{'scaleFactor': 20, 'currentTarget': array([43.59448152, 55.62362689]), 'previousTarget': array([43.10157378, 55.11736594]), 'currentState': array([57.        , 70.46586642,  0.        ]), 'targetState': array([ 6, 14], dtype=int32), 'currentDistance': 20.0}
episode index:377
target Thresh 31.409867179847026
target distance 14.0
model initialize at round 377
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  8.]), 'previousTarget': array([19.,  8.]), 'currentState': array([ 5.5       , 11.50004953,  0.        ]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 13.946338111585877}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.07873903945148897
{'scaleFactor': 20, 'currentTarget': array([19.,  8.]), 'previousTarget': array([19.,  8.]), 'currentState': array([18.        ,  8.06548184,  0.        ]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 1.0021416422260347}
episode index:378
target Thresh 31.41573909951762
target distance 6.0
model initialize at round 378
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([15.5      , 13.3902666,  0.       ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 7.843751704070842}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07853128472998108
{'scaleFactor': 20, 'currentTarget': array([50.55897311, 48.82007409]), 'previousTarget': array([50.05627521, 48.3230568 ]), 'currentState': array([65.        , 62.65686354,  0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 20.0}
episode index:379
target Thresh 31.42155259261128
target distance 6.0
model initialize at round 379
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 27.]), 'previousTarget': array([ 7., 27.]), 'currentState': array([13.5       , 23.49999681,  0.        ]), 'targetState': array([ 7, 27], dtype=int32), 'currentDistance': 7.382413041949849}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07832462345437587
{'scaleFactor': 20, 'currentTarget': array([47.52770138, 60.19561256]), 'previousTarget': array([47.01535759, 59.71084878]), 'currentState': array([63.        , 72.86873275,  0.        ]), 'targetState': array([ 7, 27], dtype=int32), 'currentDistance': 20.0}
episode index:380
target Thresh 31.427308240482162
target distance 19.0
model initialize at round 380
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6.6150874 , 6.14170336]), 'previousTarget': array([6.09517373, 5.66410281]), 'currentState': array([21.5       , 19.49982363,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07811904701486307
{'scaleFactor': 20, 'currentTarget': array([56.64939055, 55.04970831]), 'previousTarget': array([56.14793573, 54.55194899]), 'currentState': array([71.       , 68.9802506,  0.       ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 20.0}
episode index:381
target Thresh 31.433006618699846
target distance 22.0
model initialize at round 381
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.0758141 , 20.59726429]), 'previousTarget': array([22.6773982 , 20.57770876]), 'currentState': array([ 3.49999982, 16.50000969,  0.        ]), 'targetState': array([25, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.07963093228596066
{'scaleFactor': 20, 'currentTarget': array([25., 21.]), 'previousTarget': array([25., 21.]), 'currentState': array([24.49830386, 21.14010273,  0.        ]), 'targetState': array([25, 21], dtype=int32), 'currentDistance': 0.5208913429801113}
episode index:382
target Thresh 31.4386482971069
target distance 20.0
model initialize at round 382
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.88419958, 21.94384383]), 'previousTarget': array([16.46924689, 21.38463841]), 'currentState': array([5.5, 5.5, 0. ]), 'targetState': array([19, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07942301862463962
{'scaleFactor': 20, 'currentTarget': array([39.34160155, 41.16380868]), 'previousTarget': array([38.81957684, 40.6917796 ]), 'currentState': array([55.        , 53.60625851,  0.        ]), 'targetState': array([19, 25], dtype=int32), 'currentDistance': 20.0}
episode index:383
target Thresh 31.444233839875878
target distance 25.0
model initialize at round 383
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.3508552 , 11.61247794]), 'previousTarget': array([12.89833465, 11.07675612]), 'currentState': array([25.5       , 27.49952552,  0.        ]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07921618784697128
{'scaleFactor': 20, 'currentTarget': array([61.45685651, 62.26264877]), 'previousTarget': array([60.96113212, 61.75879476]), 'currentState': array([75.        , 76.97941691,  0.        ]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 20.0}
episode index:384
target Thresh 31.4497638055657
target distance 15.0
model initialize at round 384
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([26.5       , 14.49960324,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 16.446751527761382}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07901043151490122
{'scaleFactor': 20, 'currentTarget': array([60.72829105, 51.05147635]), 'previousTarget': array([60.21935735, 50.56272321]), 'currentState': array([76.        , 63.96561223,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 20.0}
episode index:385
target Thresh 31.455238747177546
target distance 20.0
model initialize at round 385
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.50829159, 25.83391929]), 'previousTarget': array([19.57218647, 25.56953382]), 'currentState': array([27.5,  7.5,  0. ]), 'targetState': array([19, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07880574127781599
{'scaleFactor': 20, 'currentTarget': array([59.22755759, 47.76244782]), 'previousTarget': array([58.69749078, 47.32383599]), 'currentState': array([77.        , 56.93524951,  0.        ]), 'targetState': array([19, 27], dtype=int32), 'currentDistance': 20.0}
episode index:386
target Thresh 31.46065921221014
target distance 18.0
model initialize at round 386
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 17.]), 'previousTarget': array([ 4., 17.]), 'currentState': array([22.5,  9.5,  0. ]), 'targetState': array([ 4, 17], dtype=int32), 'currentDistance': 19.962464777677177}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07860210887141336
{'scaleFactor': 20, 'currentTarget': array([54.54905433, 45.30186204]), 'previousTarget': array([54.02956211, 44.87982021]), 'currentState': array([72.        , 55.07245544,  0.        ]), 'targetState': array([ 4, 17], dtype=int32), 'currentDistance': 20.0}
episode index:387
target Thresh 31.466025742714503
target distance 22.0
model initialize at round 387
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.93304501, 25.13569284]), 'previousTarget': array([15.57704261, 24.55791146]), 'currentState': array([6.5, 7.5, 0. ]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07839952611659014
{'scaleFactor': 20, 'currentTarget': array([37.57360162, 37.26023734]), 'previousTarget': array([37.0342861 , 36.91882168]), 'currentState': array([56.        , 45.03634451,  0.        ]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 20.0}
episode index:388
target Thresh 31.47133887534816
target distance 7.0
model initialize at round 388
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 14.]), 'previousTarget': array([21., 14.]), 'currentState': array([15.5       , 21.42135662,  0.        ]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 9.237236278104678}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07819798491834698
{'scaleFactor': 20, 'currentTarget': array([52.73032164, 54.8448586 ]), 'previousTarget': array([52.24997208, 54.32994885]), 'currentState': array([65.        , 70.63900286,  0.        ]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 20.0}
episode index:389
target Thresh 31.476599141428796
target distance 13.0
model initialize at round 389
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 24.]), 'previousTarget': array([14., 24.]), 'currentState': array([20.5, 11.5,  0. ]), 'targetState': array([14, 24], dtype=int32), 'currentDistance': 14.089002803605336}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0779974772647102
{'scaleFactor': 20, 'currentTarget': array([51.46986535, 39.21747498]), 'previousTarget': array([50.97358147, 39.03718164]), 'currentState': array([70.        , 46.74303873,  0.        ]), 'targetState': array([14, 24], dtype=int32), 'currentDistance': 20.0}
episode index:390
target Thresh 31.481807066987407
target distance 18.0
model initialize at round 390
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3.32646312, 20.85000343]), 'previousTarget': array([ 3.11145618, 20.94427191]), 'currentState': array([21.5, 12.5,  0. ]), 'targetState': array([ 3, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07779799522567002
{'scaleFactor': 20, 'currentTarget': array([52.69739452, 42.89405771]), 'previousTarget': array([52.19950435, 42.69008291]), 'currentState': array([71.        , 50.95722288,  0.        ]), 'targetState': array([ 3, 21], dtype=int32), 'currentDistance': 20.0}
episode index:391
target Thresh 31.48696317282089
target distance 21.0
model initialize at round 391
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4.01234015, 22.49405381]), 'previousTarget': array([ 4., 22.]), 'currentState': array([4.5, 2.5, 0. ]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07759953095213515
{'scaleFactor': 20, 'currentTarget': array([36.38343211, 40.40574234]), 'previousTarget': array([35.85926981, 40.01911941]), 'currentState': array([54.        , 49.87445587,  0.        ]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 20.0}
episode index:392
target Thresh 31.49206797454412
target distance 9.0
model initialize at round 392
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 29.]), 'previousTarget': array([17., 29.]), 'currentState': array([ 8.5       , 27.52749181,  0.        ]), 'targetState': array([17, 29], dtype=int32), 'currentDistance': 8.626603061210682}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.07959052032538999
{'scaleFactor': 20, 'currentTarget': array([17., 29.]), 'previousTarget': array([17., 29.]), 'currentState': array([16.        , 28.74108377,  0.        ]), 'targetState': array([17, 29], dtype=int32), 'currentDistance': 1.0329751270771685}
episode index:393
target Thresh 31.497121982641527
target distance 16.0
model initialize at round 393
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  3.]), 'previousTarget': array([21.,  3.]), 'currentState': array([20.5       , 19.49954158,  0.        ]), 'targetState': array([21,  3], dtype=int32), 'currentDistance': 16.50711581023436}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07938851392862505
{'scaleFactor': 20, 'currentTarget': array([58.07451773, 52.91448436]), 'previousTarget': array([57.59492207, 52.3997584 ]), 'currentState': array([70.        , 68.97010246,  0.        ]), 'targetState': array([21,  3], dtype=int32), 'currentDistance': 20.0}
episode index:394
target Thresh 31.502125702518136
target distance 20.0
model initialize at round 394
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.98199646, 26.109422  ]), 'previousTarget': array([22.638375  , 25.52431817]), 'currentState': array([13.5,  8.5,  0. ]), 'targetState': array([24, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07918753034905891
{'scaleFactor': 20, 'currentTarget': array([44.26629032, 35.57637851]), 'previousTarget': array([43.76353802, 35.37957314]), 'currentState': array([63.        , 42.57981493,  0.        ]), 'targetState': array([24, 28], dtype=int32), 'currentDistance': 20.0}
episode index:395
target Thresh 31.507079634550095
target distance 20.0
model initialize at round 395
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.98804832, 12.99724255]), 'previousTarget': array([21.61161351, 12.9223227 ]), 'currentState': array([2.5       , 8.50102496, 0.        ]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.08072859488320037
{'scaleFactor': 20, 'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([21.        , 12.94016016,  0.        ]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 1.001788803150795}
episode index:396
target Thresh 31.511984274134743
target distance 4.0
model initialize at round 396
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  9.]), 'previousTarget': array([18.,  9.]), 'currentState': array([22.5      ,  9.1086177,  0.       ]), 'targetState': array([18,  9], dtype=int32), 'currentDistance': 4.501310676335537}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08052524829659281
{'scaleFactor': 20, 'currentTarget': array([55.49641121, 34.66790027]), 'previousTarget': array([54.97458513, 34.20577156]), 'currentState': array([72.        , 45.96531401,  0.        ]), 'targetState': array([18,  9], dtype=int32), 'currentDistance': 20.0}
episode index:397
target Thresh 31.516840111740123
target distance 6.0
model initialize at round 397
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 19.]), 'previousTarget': array([17., 19.]), 'currentState': array([19.5       , 25.41020739,  0.        ]), 'targetState': array([17, 19], dtype=int32), 'currentDistance': 6.880462106056239}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08032292355212901
{'scaleFactor': 20, 'currentTarget': array([55.34722872, 60.05023616]), 'previousTarget': array([54.85189262, 59.54591103]), 'currentState': array([69.        , 74.66535971,  0.        ]), 'targetState': array([17, 19], dtype=int32), 'currentDistance': 20.0}
episode index:398
target Thresh 31.521647632954043
target distance 6.0
model initialize at round 398
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 28.]), 'previousTarget': array([14., 28.]), 'currentState': array([13.5, 22.5,  0. ]), 'targetState': array([14, 28], dtype=int32), 'currentDistance': 5.522680508593542}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08012161296678533
{'scaleFactor': 20, 'currentTarget': array([47.179004  , 53.65870575]), 'previousTarget': array([46.66113138, 53.18198306]), 'currentState': array([63.        , 65.89374092,  0.        ]), 'targetState': array([14, 28], dtype=int32), 'currentDistance': 20.0}
episode index:399
target Thresh 31.526407318532627
target distance 9.0
model initialize at round 399
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  7.]), 'previousTarget': array([24.,  7.]), 'currentState': array([22.5       , 16.49832794,  0.        ]), 'targetState': array([24,  7], dtype=int32), 'currentDistance': 9.616040436121855}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07992130893436836
{'scaleFactor': 20, 'currentTarget': array([59.37727342, 50.47897036]), 'previousTarget': array([58.89215942, 49.96688023]), 'currentState': array([72.        , 65.99240884,  0.        ]), 'targetState': array([24,  7], dtype=int32), 'currentDistance': 20.0}
episode index:400
target Thresh 31.531119644448406
target distance 6.0
model initialize at round 400
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 12.]), 'previousTarget': array([19., 12.]), 'currentState': array([21.5,  6.5,  0. ]), 'targetState': array([19, 12], dtype=int32), 'currentDistance': 6.041522986797323}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07972200392455697
{'scaleFactor': 20, 'currentTarget': array([55.45242403, 41.49626188]), 'previousTarget': array([54.93846586, 41.01441442]), 'currentState': array([71.        , 54.07691693,  0.        ]), 'targetState': array([19, 12], dtype=int32), 'currentDistance': 20.0}
episode index:401
target Thresh 31.53578508193789
target distance 22.0
model initialize at round 401
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.66406015, 19.30256217]), 'previousTarget': array([23.29527642, 19.26234812]), 'currentState': array([ 4.5       , 13.58077934,  0.        ]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.08117115824560032
{'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([25.        , 19.52649838,  0.        ]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 1.1064374300754711}
episode index:402
target Thresh 31.540404097548727
target distance 15.0
model initialize at round 402
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 2.]), 'previousTarget': array([8., 2.]), 'currentState': array([23.5       , 12.46182212,  0.        ]), 'targetState': array([8, 2], dtype=int32), 'currentDistance': 18.700259947909565}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0809697409794822
{'scaleFactor': 20, 'currentTarget': array([58.26661951, 48.14416308]), 'previousTarget': array([57.76206003, 47.65007517]), 'currentState': array([73.        , 61.66923237,  0.        ]), 'targetState': array([8, 2], dtype=int32), 'currentDistance': 20.0}
episode index:403
target Thresh 31.54497715318632
target distance 18.0
model initialize at round 403
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 10.]), 'previousTarget': array([25., 10.]), 'currentState': array([23.5       , 28.49998567,  0.        ]), 'targetState': array([25, 10], dtype=int32), 'currentDistance': 18.56069690524226}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08076932082854289
{'scaleFactor': 20, 'currentTarget': array([61.46482906, 61.64839593]), 'previousTarget': array([60.98863865, 61.13183022]), 'currentState': array([73.        , 77.98668933,  0.        ]), 'targetState': array([25, 10], dtype=int32), 'currentDistance': 20.0}
episode index:404
target Thresh 31.54950470616004
target distance 5.0
model initialize at round 404
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 19.]), 'previousTarget': array([ 4., 19.]), 'currentState': array([ 9.5       , 21.69720745,  0.        ]), 'targetState': array([ 4, 19], dtype=int32), 'currentDistance': 6.125759384191637}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08056989040674402
{'scaleFactor': 20, 'currentTarget': array([42.63339411, 46.13387869]), 'previousTarget': array([42.11494816, 45.67841655]), 'currentState': array([59.        , 57.62884331,  0.        ]), 'targetState': array([ 4, 19], dtype=int32), 'currentDistance': 20.0}
episode index:405
target Thresh 31.553987209228968
target distance 21.0
model initialize at round 405
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.89929041, 17.34786235]), 'previousTarget': array([15.41826648, 16.83071558]), 'currentState': array([2.5, 2.5, 0. ]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08037144240081608
{'scaleFactor': 20, 'currentTarget': array([35.02130493, 31.72885334]), 'previousTarget': array([34.4809451 , 31.32098143]), 'currentState': array([52.        , 42.29880669,  0.        ]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 20.0}
episode index:406
target Thresh 31.558425110647136
target distance 11.0
model initialize at round 406
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 13.]), 'previousTarget': array([11., 13.]), 'currentState': array([22.5       , 21.33133453,  0.        ]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 14.200744173195767}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08017396956936444
{'scaleFactor': 20, 'currentTarget': array([57.40803169, 56.49972611]), 'previousTarget': array([56.90431455, 56.00414233]), 'currentState': array([72.        , 70.17724273,  0.        ]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 20.0}
episode index:407
target Thresh 31.562818854208388
target distance 17.0
model initialize at round 407
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 19.]), 'previousTarget': array([19., 19.]), 'currentState': array([11.5,  2.5,  0. ]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 18.12456896039183}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07997746474198855
{'scaleFactor': 20, 'currentTarget': array([45.13270079, 39.05153404]), 'previousTarget': array([44.61142444, 38.5804666 ]), 'currentState': array([61.        , 51.22645974,  0.        ]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 20.0}
episode index:408
target Thresh 31.567168879290744
target distance 2.0
model initialize at round 408
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([6.5       , 3.70469007, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 3.02588304982389}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07978192081841401
{'scaleFactor': 20, 'currentTarget': array([40.97377371, 34.4775709 ]), 'previousTarget': array([40.46548924, 33.99055656]), 'currentState': array([56.        , 47.67653008,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 20.0}
episode index:409
target Thresh 31.571475620900337
target distance 5.0
model initialize at round 409
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 15.]), 'previousTarget': array([10., 15.]), 'currentState': array([ 6.5       , 10.49999997,  0.        ]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 5.700877149020179}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.08183792758331523
{'scaleFactor': 20, 'currentTarget': array([10., 15.]), 'previousTarget': array([10., 15.]), 'currentState': array([10.5       , 14.49610537,  0.        ]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 0.709866040330694}
episode index:410
target Thresh 31.57573950971491
target distance 15.0
model initialize at round 410
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 25.]), 'previousTarget': array([23., 25.]), 'currentState': array([24.5, 10.5,  0. ]), 'targetState': array([23, 25], dtype=int32), 'currentDistance': 14.57737973711318}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08163880853810036
{'scaleFactor': 20, 'currentTarget': array([56.38249788, 42.93848393]), 'previousTarget': array([55.87074879, 42.61088933]), 'currentState': array([74.        , 52.40545911,  0.        ]), 'targetState': array([23, 25], dtype=int32), 'currentDistance': 20.0}
episode index:411
target Thresh 31.57996097212691
target distance 11.0
model initialize at round 411
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 15.]), 'previousTarget': array([ 4., 15.]), 'currentState': array([ 7.5       , 26.49408832,  0.        ]), 'targetState': array([ 4, 15], dtype=int32), 'currentDistance': 12.015159855457332}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08144065609019235
{'scaleFactor': 20, 'currentTarget': array([43.86985651, 60.81004223]), 'previousTarget': array([43.37924992, 60.3032545 ]), 'currentState': array([57.        , 75.89643782,  0.        ]), 'targetState': array([ 4, 15], dtype=int32), 'currentDistance': 20.0}
episode index:412
target Thresh 31.584140430286084
target distance 17.0
model initialize at round 412
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 22.]), 'previousTarget': array([ 8., 22.]), 'currentState': array([25.5       , 21.49999958,  0.        ]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 17.507141411927744}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08124346321830327
{'scaleFactor': 20, 'currentTarget': array([57.55412676, 49.77859779]), 'previousTarget': array([57.03145098, 49.33607212]), 'currentState': array([75.        , 59.55824538,  0.        ]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 20.0}
episode index:413
target Thresh 31.588278302141735
target distance 12.0
model initialize at round 413
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([16.5       ,  5.49828935,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 12.589474610420467}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08104722296898369
{'scaleFactor': 20, 'currentTarget': array([48.78222548, 30.46671362]), 'previousTarget': array([48.261698  , 30.03853906]), 'currentState': array([66.        , 40.64257921,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:414
target Thresh 31.592375001484502
target distance 3.0
model initialize at round 414
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 24.]), 'previousTarget': array([26., 24.]), 'currentState': array([24.5       , 26.55177918,  0.        ]), 'targetState': array([26, 24], dtype=int32), 'currentDistance': 2.9599961127317203}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08085192845580542
{'scaleFactor': 20, 'currentTarget': array([57.19524568, 44.13076746]), 'previousTarget': array([56.67016433, 43.69141867]), 'currentState': array([74.        , 54.97513153,  0.        ]), 'targetState': array([26, 24], dtype=int32), 'currentDistance': 20.0}
episode index:415
target Thresh 31.596430937987726
target distance 14.0
model initialize at round 415
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  5.]), 'previousTarget': array([11.,  5.]), 'currentState': array([25.5       , 14.43771231,  0.        ]), 'targetState': array([11,  5], dtype=int32), 'currentDistance': 17.300878985741132}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08065757285855589
{'scaleFactor': 20, 'currentTarget': array([58.35256693, 36.52875033]), 'previousTarget': array([57.86126468, 36.2546964 ]), 'currentState': array([75.        , 47.61310742,  0.        ]), 'targetState': array([11,  5], dtype=int32), 'currentDistance': 20.0}
episode index:416
target Thresh 31.60044651724844
target distance 21.0
model initialize at round 416
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6.54849798, 19.89196526]), 'previousTarget': array([ 6.09009055, 19.89618185]), 'currentState': array([26.5, 18.5,  0. ]), 'targetState': array([ 5, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08046414942244424
{'scaleFactor': 20, 'currentTarget': array([58.91092291, 52.77906212]), 'previousTarget': array([58.40233847, 52.40938993]), 'currentState': array([76.        , 63.16960803,  0.        ]), 'targetState': array([ 5, 20], dtype=int32), 'currentDistance': 20.0}
episode index:417
target Thresh 31.604422140827918
target distance 11.0
model initialize at round 417
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 17.]), 'previousTarget': array([27., 17.]), 'currentState': array([17.5       , 27.52001736,  0.        ]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 14.174652206097107}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08027165145731878
{'scaleFactor': 20, 'currentTarget': array([49.78638584, 30.47953576]), 'previousTarget': array([49.26537014, 30.10925809]), 'currentState': array([67.        , 40.66243748,  0.        ]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 20.0}
episode index:418
target Thresh 31.608358206291825
target distance 8.0
model initialize at round 418
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 14.]), 'previousTarget': array([19., 14.]), 'currentState': array([27.5, 10.5,  0. ]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 9.192388155425164}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08008007233689558
{'scaleFactor': 20, 'currentTarget': array([61.07413851, 45.96198832]), 'previousTarget': array([60.5612508 , 45.50255341]), 'currentState': array([77.        , 58.06020868,  0.        ]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:419
target Thresh 31.612255107249997
target distance 15.0
model initialize at round 419
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3.41836808, 15.6356149 ]), 'previousTarget': array([ 3.37889464, 15.64636501]), 'currentState': array([18.5,  2.5,  0. ]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07988940549799821
{'scaleFactor': 20, 'currentTarget': array([50.13805158, 39.74397166]), 'previousTarget': array([49.63623551, 39.47939828]), 'currentState': array([68.        , 48.74123783,  0.        ]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 20.0}
episode index:420
target Thresh 31.616113233395772
target distance 15.0
model initialize at round 420
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 20.]), 'previousTarget': array([23., 20.]), 'currentState': array([ 8.5, 22.5,  0. ]), 'targetState': array([23, 20], dtype=int32), 'currentDistance': 14.713938969562069}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.08151043473516949
{'scaleFactor': 20, 'currentTarget': array([23., 20.]), 'previousTarget': array([23., 20.]), 'currentState': array([22.        , 19.23624041,  0.        ]), 'targetState': array([23, 20], dtype=int32), 'currentDistance': 1.2583039035867651}
episode index:421
target Thresh 31.619932970544976
target distance 8.0
model initialize at round 421
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 2.]), 'previousTarget': array([8., 2.]), 'currentState': array([12.5       , 10.47210318,  0.        ]), 'targetState': array([8, 2], dtype=int32), 'currentDistance': 9.593046036956581}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08131728204622359
{'scaleFactor': 20, 'currentTarget': array([48.33120414, 45.079225  ]), 'previousTarget': array([47.83593313, 44.57783736]), 'currentState': array([62.        , 59.67936265,  0.        ]), 'targetState': array([8, 2], dtype=int32), 'currentDistance': 20.0}
episode index:422
target Thresh 31.623714700674515
target distance 13.0
model initialize at round 422
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  9.]), 'previousTarget': array([18.,  9.]), 'currentState': array([26.5       , 22.49725592,  0.        ]), 'targetState': array([18,  9], dtype=int32), 'currentDistance': 15.950734071095152}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08112504260876206
{'scaleFactor': 20, 'currentTarget': array([62.44277181, 57.20139934]), 'previousTarget': array([61.94787179, 56.6978255 ]), 'currentState': array([76.        , 71.90519354,  0.        ]), 'targetState': array([18,  9], dtype=int32), 'currentDistance': 20.0}
episode index:423
target Thresh 31.627458801960547
target distance 2.0
model initialize at round 423
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 14.]), 'previousTarget': array([21., 14.]), 'currentState': array([20.5       , 12.49760506,  0.        ]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 1.5834110549844982}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.08324526656487348
{'scaleFactor': 20, 'currentTarget': array([21., 14.]), 'previousTarget': array([21., 14.]), 'currentState': array([21.5       , 13.46492571,  0.        ]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 0.732328136620638}
episode index:424
target Thresh 31.631165648816324
target distance 14.0
model initialize at round 424
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([20.5       , 18.49991086,  0.        ]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 14.508615056896423}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08304939534942672
{'scaleFactor': 20, 'currentTarget': array([52.7265919 , 45.27066287]), 'previousTarget': array([52.21707968, 44.91482259]), 'currentState': array([70.        , 55.35180232,  0.        ]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 20.0}
episode index:425
target Thresh 31.63483561192962
target distance 5.0
model initialize at round 425
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 22.]), 'previousTarget': array([ 9., 22.]), 'currentState': array([ 5.5       , 17.49999899,  0.        ]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 5.700877925329758}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.08502051107496308
{'scaleFactor': 20, 'currentTarget': array([ 9., 22.]), 'previousTarget': array([ 9., 22.]), 'currentState': array([ 9.5       , 21.44862482,  0.        ]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 0.7443215633398189}
episode index:426
target Thresh 31.6384690582998
target distance 19.0
model initialize at round 426
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  9.]), 'previousTarget': array([19.,  9.]), 'currentState': array([17.5       , 28.49997401,  0.        ]), 'targetState': array([19,  9], dtype=int32), 'currentDistance': 19.55758130452998}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0848213998078086
{'scaleFactor': 20, 'currentTarget': array([55.57690712, 61.56698752]), 'previousTarget': array([55.10155301, 61.05023805]), 'currentState': array([67.        , 77.98383707,  0.        ]), 'targetState': array([19,  9], dtype=int32), 'currentDistance': 20.0}
episode index:427
target Thresh 31.64206635127454
target distance 15.0
model initialize at round 427
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  6.]), 'previousTarget': array([11.,  6.]), 'currentState': array([26.5       , 13.30893081,  0.        ]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 17.13681620511031}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08462321896713615
{'scaleFactor': 20, 'currentTarget': array([58.98230285, 35.62517113]), 'previousTarget': array([58.46597993, 35.20450366]), 'currentState': array([76.        , 46.13221561,  0.        ]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 20.0}
episode index:428
target Thresh 31.645627850586123
target distance 18.0
model initialize at round 428
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.70104502,  8.57697726]), 'previousTarget': array([13.71285862,  8.51685448]), 'currentState': array([ 4.5       , 26.33481413,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08442596204646684
{'scaleFactor': 20, 'currentTarget': array([43.83280552, 58.53580087]), 'previousTarget': array([43.3717916 , 58.01294765]), 'currentState': array([54.        , 75.75869715,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 20.0}
episode index:429
target Thresh 31.649153912387458
target distance 12.0
model initialize at round 429
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 20.]), 'previousTarget': array([18., 20.]), 'currentState': array([ 6.5       , 15.87971892,  0.        ]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 12.215838741612332}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.08609387978482168
{'scaleFactor': 20, 'currentTarget': array([18., 20.]), 'previousTarget': array([18., 20.]), 'currentState': array([17.49999982, 19.24767169,  0.        ]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 0.9033261153585955}
episode index:430
target Thresh 31.652644889287657
target distance 12.0
model initialize at round 430
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 16.]), 'previousTarget': array([ 5., 16.]), 'currentState': array([13.5       , 28.49892405,  0.        ]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 15.115326735686134}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08589412600341838
{'scaleFactor': 20, 'currentTarget': array([49.33008602, 63.34330818]), 'previousTarget': array([48.83419932, 62.84015594]), 'currentState': array([63.        , 77.94239897,  0.        ]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 20.0}
episode index:431
target Thresh 31.65610113038732
target distance 22.0
model initialize at round 431
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.95783658,  6.37598428]), 'previousTarget': array([23.94427191,  6.11145618]), 'currentState': array([15.5       , 24.49958768,  0.        ]), 'targetState': array([26,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08569529700804009
{'scaleFactor': 20, 'currentTarget': array([55.47423439, 56.41321285]), 'previousTarget': array([55.01790178, 55.889643  ]), 'currentState': array([65.        , 73.99899659,  0.        ]), 'targetState': array([26,  2], dtype=int32), 'currentDistance': 20.0}
episode index:432
target Thresh 31.65952298131344
target distance 16.0
model initialize at round 432
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 24.]), 'previousTarget': array([19., 24.]), 'currentState': array([22.5,  8.5,  0. ]), 'targetState': array([19, 24], dtype=int32), 'currentDistance': 15.890248582070644}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08549738639139334
{'scaleFactor': 20, 'currentTarget': array([54.32737558, 42.71787324]), 'previousTarget': array([53.81293091, 42.37647586]), 'currentState': array([72.        , 52.08154485,  0.        ]), 'targetState': array([19, 24], dtype=int32), 'currentDistance': 20.0}
episode index:433
target Thresh 31.66291078425396
target distance 17.0
model initialize at round 433
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 26.]), 'previousTarget': array([ 3., 26.]), 'currentState': array([20.5       , 24.49999994,  0.        ]), 'targetState': array([ 3, 26], dtype=int32), 'currentDistance': 17.564168075340675}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08530038780523806
{'scaleFactor': 20, 'currentTarget': array([52.86719933, 56.03270235]), 'previousTarget': array([52.35345114, 55.6336578 ]), 'currentState': array([70.        , 66.35099393,  0.        ]), 'targetState': array([ 3, 26], dtype=int32), 'currentDistance': 20.0}
episode index:434
target Thresh 31.666264877991996
target distance 16.0
model initialize at round 434
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 20.]), 'previousTarget': array([16., 20.]), 'currentState': array([10.5,  4.5,  0. ]), 'targetState': array([16, 20], dtype=int32), 'currentDistance': 16.446884203398504}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08510429495970878
{'scaleFactor': 20, 'currentTarget': array([43.48829874, 38.78792899]), 'previousTarget': array([42.97447037, 38.3882376 ]), 'currentState': array([60.        , 50.07348266,  0.        ]), 'targetState': array([16, 20], dtype=int32), 'currentDistance': 20.0}
episode index:435
target Thresh 31.669585597939715
target distance 12.0
model initialize at round 435
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 14.]), 'previousTarget': array([ 6., 14.]), 'currentState': array([12.5       , 26.49757364,  0.        ]), 'targetState': array([ 6, 14], dtype=int32), 'currentDistance': 14.086850144204602}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08490910162264523
{'scaleFactor': 20, 'currentTarget': array([48.5276142 , 60.66025224]), 'previousTarget': array([48.03477829, 60.16443606]), 'currentState': array([62.        , 75.44182256,  0.        ]), 'targetState': array([ 6, 14], dtype=int32), 'currentDistance': 20.0}
episode index:436
target Thresh 31.672873276171885
target distance 8.0
model initialize at round 436
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([ 6.49999997, 17.5       ,  0.        ]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 9.924716640158012}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.08670277830719061
{'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([13.49999997, 24.42610362,  0.        ]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 0.7611550585785922}
episode index:437
target Thresh 31.676128241459065
target distance 25.0
model initialize at round 437
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.88498466, 10.00637061]), 'previousTarget': array([14.57218647,  9.43046618]), 'currentState': array([22.5       , 28.49992466,  0.        ]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08650482675854405
{'scaleFactor': 20, 'currentTarget': array([59.31621176, 60.68598483]), 'previousTarget': array([58.83120962, 60.20771095]), 'currentState': array([72.        , 76.14953926,  0.        ]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 20.0}
episode index:438
target Thresh 31.679350819300495
target distance 2.0
model initialize at round 438
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 19.]), 'previousTarget': array([17., 19.]), 'currentState': array([19.5       , 18.49266058,  0.        ]), 'targetState': array([17, 19], dtype=int32), 'currentDistance': 2.550959287207543}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0863077770392763
{'scaleFactor': 20, 'currentTarget': array([52.49546995, 43.29382703]), 'previousTarget': array([51.99349391, 42.94127527]), 'currentState': array([69.        , 54.58986562,  0.        ]), 'targetState': array([17, 19], dtype=int32), 'currentDistance': 20.0}
episode index:439
target Thresh 31.68254133195665
target distance 19.0
model initialize at round 439
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 21.]), 'previousTarget': array([ 7., 21.]), 'currentState': array([12.5,  2.5,  0. ]), 'targetState': array([ 7, 21], dtype=int32), 'currentDistance': 19.30025906561881}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08611162300055067
{'scaleFactor': 20, 'currentTarget': array([44.29798656, 40.61185783]), 'previousTarget': array([43.79478925, 40.33113298]), 'currentState': array([62.        , 49.91985011,  0.        ]), 'targetState': array([ 7, 21], dtype=int32), 'currentDistance': 20.0}
episode index:440
target Thresh 31.685700098481448
target distance 22.0
model initialize at round 440
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5.51143812, 22.16126467]), 'previousTarget': array([ 4.9414844 , 21.93592685]), 'currentState': array([24.5       , 28.44095558,  0.        ]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08591635854930225
{'scaleFactor': 20, 'currentTarget': array([56.00165206, 47.16682645]), 'previousTarget': array([55.50341607, 46.93876465]), 'currentState': array([74.        , 55.88803458,  0.        ]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 20.0}
episode index:441
target Thresh 31.688827434754177
target distance 19.0
model initialize at round 441
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 23.]), 'previousTarget': array([24.91410718, 22.76686234]), 'currentState': array([18.5,  4.5,  0. ]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 19.608671551127554}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.085721977647607
{'scaleFactor': 20, 'currentTarget': array([50.67302307, 37.80017928]), 'previousTarget': array([50.19180382, 37.5858346 ]), 'currentState': array([68.        , 47.78896651,  0.        ]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 20.0}
episode index:442
target Thresh 31.691923653511072
target distance 6.0
model initialize at round 442
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  6.]), 'previousTarget': array([19.,  6.]), 'currentState': array([25.5       ,  7.15883288,  0.        ]), 'targetState': array([19,  6], dtype=int32), 'currentDistance': 6.602491472076551}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08552847431205936
{'scaleFactor': 20, 'currentTarget': array([57.79264697, 28.98040887]), 'previousTarget': array([57.30417557, 28.74955007]), 'currentState': array([75.        , 39.17388725,  0.        ]), 'targetState': array([19,  6], dtype=int32), 'currentDistance': 20.0}
episode index:443
target Thresh 31.694989064376585
target distance 11.0
model initialize at round 443
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  7.]), 'previousTarget': array([24.,  7.]), 'currentState': array([25.5       , 18.43846816,  0.        ]), 'targetState': array([24,  7], dtype=int32), 'currentDistance': 11.536401250268128}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08533584261315832
{'scaleFactor': 20, 'currentTarget': array([58.96839593, 33.08242865]), 'previousTarget': array([58.45436732, 32.63611812]), 'currentState': array([75.        , 45.04017387,  0.        ]), 'targetState': array([24,  7], dtype=int32), 'currentDistance': 20.0}
episode index:444
target Thresh 31.69802397389436
target distance 6.0
model initialize at round 444
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'previousTarget': array([11., 19.]), 'currentState': array([11.5       , 25.32459879,  0.        ]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 6.344332103897271}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0851440766747018
{'scaleFactor': 20, 'currentTarget': array([47.24936513, 57.28569878]), 'previousTarget': array([46.75624242, 56.80070437]), 'currentState': array([61.        , 71.80878526,  0.        ]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 20.0}
episode index:445
target Thresh 31.701028685557876
target distance 13.0
model initialize at round 445
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 20.]), 'previousTarget': array([ 8., 20.]), 'currentState': array([21.5, 15.5,  0. ]), 'targetState': array([ 8, 20], dtype=int32), 'currentDistance': 14.230249470757755}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08495317067318901
{'scaleFactor': 20, 'currentTarget': array([53.67817565, 46.36429005]), 'previousTarget': array([53.16581668, 45.99410989]), 'currentState': array([71.        , 56.36200985,  0.        ]), 'targetState': array([ 8, 20], dtype=int32), 'currentDistance': 20.0}
episode index:446
target Thresh 31.704003499840805
target distance 17.0
model initialize at round 446
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.57633937,  8.55929138]), 'previousTarget': array([23.14900215,  9.11284334]), 'currentState': array([11.5       , 24.50175127,  0.        ]), 'targetState': array([24,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08476311883723109
{'scaleFactor': 20, 'currentTarget': array([48.88616222, 40.69302689]), 'previousTarget': array([48.41823591, 40.21271074]), 'currentState': array([61.        , 56.60701238,  0.        ]), 'targetState': array([24,  8], dtype=int32), 'currentDistance': 20.0}
episode index:447
target Thresh 31.706948714227053
target distance 21.0
model initialize at round 447
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.85678383, 11.5406247 ]), 'previousTarget': array([12.52709229, 11.99469707]), 'currentState': array([ 2.5       , 28.65018427,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08457391544696942
{'scaleFactor': 20, 'currentTarget': array([42.09456429, 55.52558201]), 'previousTarget': array([41.64523432, 55.05557843]), 'currentState': array([52.        , 72.90034369,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 20.0}
episode index:448
target Thresh 31.709864623240517
target distance 9.0
model initialize at round 448
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 16.]), 'previousTarget': array([ 5., 16.]), 'currentState': array([ 2.5       , 25.24379218,  0.        ]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 9.575891279543534}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08438555483350177
{'scaleFactor': 20, 'currentTarget': array([39.0174914 , 55.86359403]), 'previousTarget': array([38.53377682, 55.38200894]), 'currentState': array([52.        , 71.07722181,  0.        ]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 20.0}
episode index:449
target Thresh 31.71275151847452
target distance 15.0
model initialize at round 449
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  2.]), 'previousTarget': array([25.,  2.]), 'currentState': array([10.5, 10.5,  0. ]), 'targetState': array([25,  2], dtype=int32), 'currentDistance': 16.8077363139715}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.08589212629908756
{'scaleFactor': 20, 'currentTarget': array([25.,  2.]), 'previousTarget': array([25.,  2.]), 'currentState': array([24.        ,  1.19587844,  0.        ]), 'targetState': array([25,  2], dtype=int32), 'currentDistance': 1.2832036024815974}
episode index:450
target Thresh 31.715609688620997
target distance 7.0
model initialize at round 450
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 22.]), 'previousTarget': array([11., 22.]), 'currentState': array([18.5       , 18.49999991,  0.        ]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 8.276472716432401}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0857016781254754
{'scaleFactor': 20, 'currentTarget': array([50.82424613, 45.75816631]), 'previousTarget': array([50.31190469, 45.38829693]), 'currentState': array([68.        , 56.00479887,  0.        ]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 20.0}
episode index:451
target Thresh 31.718439419499344
target distance 2.0
model initialize at round 451
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([6.5       , 6.56006768, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 2.94683070042563}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08551207264289691
{'scaleFactor': 20, 'currentTarget': array([39.29592802, 28.24064815]), 'previousTarget': array([38.78938059, 27.87742112]), 'currentState': array([56.        , 39.23946533,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 20.0}
episode index:452
target Thresh 31.721240994085004
target distance 7.0
model initialize at round 452
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 21.]), 'previousTarget': array([10., 21.]), 'currentState': array([ 7.5       , 28.09056041,  0.        ]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 7.518380609088932}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08532330427061678
{'scaleFactor': 20, 'currentTarget': array([43.60144514, 58.23774246]), 'previousTarget': array([43.11430669, 57.76183323]), 'currentState': array([57.        , 73.08626857,  0.        ]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 20.0}
episode index:453
target Thresh 31.72401469253778
target distance 8.0
model initialize at round 453
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 23.]), 'previousTarget': array([12., 23.]), 'currentState': array([ 4.49999952, 19.49741569,  0.        ]), 'targetState': array([12, 23], dtype=int32), 'currentDistance': 8.277566308396853}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.08704890450959996
{'scaleFactor': 20, 'currentTarget': array([12., 23.]), 'previousTarget': array([12., 23.]), 'currentState': array([11.4999992 , 22.42200495,  0.        ]), 'targetState': array([12, 23], dtype=int32), 'currentDistance': 0.7642506650234525}
episode index:454
target Thresh 31.726760792229815
target distance 19.0
model initialize at round 454
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.93293136,  6.23761265]), 'previousTarget': array([27.,  6.]), 'currentState': array([21.5       , 25.48555428,  0.        ]), 'targetState': array([27,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0868575882359525
{'scaleFactor': 20, 'currentTarget': array([59.91012996, 55.39177269]), 'previousTarget': array([59.44091585, 54.88315972]), 'currentState': array([71.        , 72.03553376,  0.        ]), 'targetState': array([27,  6], dtype=int32), 'currentDistance': 20.0}
episode index:455
target Thresh 31.72947956777337
target distance 21.0
model initialize at round 455
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.35786438, 20.64213562]), 'previousTarget': array([12.20689655, 20.48275862]), 'currentState': array([26.5,  6.5,  0. ]), 'targetState': array([ 6, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08666711106876838
{'scaleFactor': 20, 'currentTarget': array([57.44493244, 47.69400576]), 'previousTarget': array([56.92211576, 47.3023759 ]), 'currentState': array([76.        , 55.15788329,  0.        ]), 'targetState': array([ 6, 27], dtype=int32), 'currentDistance': 20.0}
episode index:456
target Thresh 31.732171291048274
target distance 16.0
model initialize at round 456
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  9.]), 'previousTarget': array([24.,  9.]), 'currentState': array([14.5       , 24.67961478,  0.        ]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 18.333039020368922}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08647746749969011
{'scaleFactor': 20, 'currentTarget': array([52.5727337 , 50.04142341]), 'previousTarget': array([52.10401258, 49.53255324]), 'currentState': array([64.        , 66.45536824,  0.        ]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 20.0}
episode index:457
target Thresh 31.73483623122909
target distance 10.0
model initialize at round 457
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  2.]), 'previousTarget': array([13.,  2.]), 'currentState': array([23.5       ,  6.07422877,  0.        ]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 11.262741232874768}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08628865206846809
{'scaleFactor': 20, 'currentTarget': array([57.3133485 , 37.04810125]), 'previousTarget': array([56.80290093, 36.58442901]), 'currentState': array([73.        , 49.45491248,  0.        ]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 20.0}
episode index:458
target Thresh 31.737474654812058
target distance 24.0
model initialize at round 458
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3.11846905, 7.98135858]), 'previousTarget': array([2.92091492, 7.42039161]), 'currentState': array([ 7.5       , 27.49551207,  0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08610065936243656
{'scaleFactor': 20, 'currentTarget': array([44.91409187, 59.58200716]), 'previousTarget': array([44.43143182, 59.07219036]), 'currentState': array([57.        , 75.51721424,  0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 20.0}
episode index:459
target Thresh 31.740086825641733
target distance 17.0
model initialize at round 459
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'previousTarget': array([10., 12.]), 'currentState': array([27.5,  8.5,  0. ]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 17.846568297574812}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08591348401599648
{'scaleFactor': 20, 'currentTarget': array([60.43969977, 46.15619094]), 'previousTarget': array([59.92213257, 45.6916339 ]), 'currentState': array([77.        , 57.37030956,  0.        ]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 20.0}
episode index:460
target Thresh 31.74267300493738
target distance 20.0
model initialize at round 460
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.62584485,  3.70993024]), 'previousTarget': array([13.39299151,  3.12283287]), 'currentState': array([20.5       , 22.49146348,  0.        ]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08572712071010495
{'scaleFactor': 20, 'currentTarget': array([57.33653469, 56.19831657]), 'previousTarget': array([56.84890868, 55.68971206]), 'currentState': array([70.        , 71.67851833,  0.        ]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 20.0}
episode index:461
target Thresh 31.74523345131908
target distance 18.0
model initialize at round 461
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 29.]), 'previousTarget': array([20.88854382, 28.94427191]), 'currentState': array([ 3.49654663, 20.5       ,  0.        ]), 'targetState': array([21, 29], dtype=int32), 'currentDistance': 19.458182854845532}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.08707955817019626
{'scaleFactor': 20, 'currentTarget': array([21., 29.]), 'previousTarget': array([21., 29.]), 'currentState': array([20.49202469, 28.44309624,  0.        ]), 'targetState': array([21, 29], dtype=int32), 'currentDistance': 0.7537776293713149}
episode index:462
target Thresh 31.747768420833605
target distance 17.0
model initialize at round 462
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.7012056 ,  4.47815403]), 'previousTarget': array([25.23243274,  5.00324289]), 'currentState': array([12.5       , 19.50240678,  0.        ]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0868914813706926
{'scaleFactor': 20, 'currentTarget': array([49.70700849, 32.140661  ]), 'previousTarget': array([49.23154377, 31.62212722]), 'currentState': array([62.        , 47.91666682,  0.        ]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 20.0}
episode index:463
target Thresh 31.75027816698002
target distance 14.0
model initialize at round 463
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 21.]), 'previousTarget': array([ 6., 21.]), 'currentState': array([8.5, 7.5, 0. ]), 'targetState': array([ 6, 21], dtype=int32), 'currentDistance': 13.729530217745998}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08670421524704887
{'scaleFactor': 20, 'currentTarget': array([41.51597912, 45.4025453 ]), 'previousTarget': array([40.99309558, 44.93919078]), 'currentState': array([58.        , 56.72849143,  0.        ]), 'targetState': array([ 6, 21], dtype=int32), 'currentDistance': 20.0}
episode index:464
target Thresh 31.752762940735032
target distance 13.0
model initialize at round 464
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 12.]), 'previousTarget': array([11., 12.]), 'currentState': array([11.5      , 25.4899556,  0.       ]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 13.499218577382518}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08651775456909823
{'scaleFactor': 20, 'currentTarget': array([48.55456426, 59.24263753]), 'previousTarget': array([48.07039299, 58.73043761]), 'currentState': array([61.        , 74.89866287,  0.        ]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 20.0}
episode index:465
target Thresh 31.755222990578087
target distance 11.0
model initialize at round 465
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([26.5       , 19.47683662,  0.        ]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 12.327521196193748}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08633209415156796
{'scaleFactor': 20, 'currentTarget': array([62.72281007, 53.87544761]), 'previousTarget': array([62.23067724, 53.36907745]), 'currentState': array([76.        , 68.83259398,  0.        ]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 20.0}
episode index:466
target Thresh 31.757658562516216
target distance 17.0
model initialize at round 466
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 19.]), 'previousTarget': array([20., 19.]), 'currentState': array([27.5,  2.5,  0. ]), 'targetState': array([20, 19], dtype=int32), 'currentDistance': 18.124568960391894}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08614722885359888
{'scaleFactor': 20, 'currentTarget': array([59.66913976, 41.8482172 ]), 'previousTarget': array([59.14158221, 41.40033654]), 'currentState': array([77.        , 51.83026525,  0.        ]), 'targetState': array([20, 19], dtype=int32), 'currentDistance': 20.0}
episode index:467
target Thresh 31.760069900108643
target distance 16.0
model initialize at round 467
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  9.]), 'previousTarget': array([21.,  9.]), 'currentState': array([ 5.5       , 15.50000003,  0.        ]), 'targetState': array([21,  9], dtype=int32), 'currentDistance': 16.807736325496947}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.08754370993166444
{'scaleFactor': 20, 'currentTarget': array([21.,  9.]), 'previousTarget': array([21.,  9.]), 'currentState': array([20.49999997,  8.45007692,  0.        ]), 'targetState': array([21,  9], dtype=int32), 'currentDistance': 0.7432465416603186}
episode index:468
target Thresh 31.76245724449114
target distance 18.0
model initialize at round 468
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 12.]), 'previousTarget': array([ 7., 12.]), 'currentState': array([25.5       , 13.49996671,  0.        ]), 'targetState': array([ 7, 12], dtype=int32), 'currentDistance': 18.560708503005173}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08735704956933679
{'scaleFactor': 20, 'currentTarget': array([58.97082506, 50.78072986]), 'previousTarget': array([58.45660444, 50.30178364]), 'currentState': array([75.        , 62.74173111,  0.        ]), 'targetState': array([ 7, 12], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:469
target Thresh 31.764820834400137
target distance 4.0
model initialize at round 469
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([15.5       ,  3.48151898,  0.        ]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 4.5297706966377485}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0871711835064233
{'scaleFactor': 20, 'currentTarget': array([49.65707949, 36.32429397]), 'previousTarget': array([49.14568018, 35.83909235]), 'currentState': array([65.        , 49.15374409,  0.        ]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 20.0}
episode index:470
target Thresh 31.767160906196587
target distance 3.0
model initialize at round 470
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 8.]), 'previousTarget': array([9., 8.]), 'currentState': array([11.5       ,  5.49988446,  0.        ]), 'targetState': array([9, 8], dtype=int32), 'currentDistance': 3.535615608542632}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08698610668369205
{'scaleFactor': 20, 'currentTarget': array([45.60739964, 38.36973933]), 'previousTarget': array([45.09493356, 37.88511944]), 'currentState': array([61.        , 51.13954176,  0.        ]), 'targetState': array([9, 8], dtype=int32), 'currentDistance': 20.0}
episode index:471
target Thresh 31.769477693889623
target distance 17.0
model initialize at round 471
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'previousTarget': array([ 5., 22.]), 'currentState': array([22.5       , 22.49999821,  0.        ]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 17.507141348942916}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08680181408478592
{'scaleFactor': 20, 'currentTarget': array([55.966857  , 60.00524459]), 'previousTarget': array([55.45219769, 59.52522019]), 'currentState': array([72.        , 71.96092632,  0.        ]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 20.0}
episode index:472
target Thresh 31.771771429159948
target distance 26.0
model initialize at round 472
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.18315006, 19.99026705]), 'previousTarget': array([15.05501274, 19.73939228]), 'currentState': array([26.5,  3.5,  0. ]), 'targetState': array([ 9, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08661830073576945
{'scaleFactor': 20, 'currentTarget': array([57.17138148, 46.25423831]), 'previousTarget': array([56.642673  , 45.83501205]), 'currentState': array([76.       , 52.9983561,  0.       ]), 'targetState': array([ 9, 29], dtype=int32), 'currentDistance': 20.0}
episode index:473
target Thresh 31.774042341382994
target distance 11.0
model initialize at round 473
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([16.5       , 22.49858335,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 13.208611546216073}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08643556170468134
{'scaleFactor': 20, 'currentTarget': array([52.47405169, 57.26308441]), 'previousTarget': array([51.97946705, 56.75812392]), 'currentState': array([66.        , 71.99565792,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 20.0}
episode index:474
target Thresh 31.776290657651884
target distance 9.0
model initialize at round 474
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 4.]), 'previousTarget': array([7., 4.]), 'currentState': array([16.5       ,  8.44921342,  0.        ]), 'targetState': array([7, 4], dtype=int32), 'currentDistance': 10.490257385576271}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08625359210109254
{'scaleFactor': 20, 'currentTarget': array([51.22765342, 44.36591516]), 'previousTarget': array([50.72216523, 43.8719457 ]), 'currentState': array([66.        , 57.84841406,  0.        ]), 'targetState': array([7, 4], dtype=int32), 'currentDistance': 20.0}
episode index:475
target Thresh 31.77851660280011
target distance 9.0
model initialize at round 475
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 25.]), 'previousTarget': array([ 9., 25.]), 'currentState': array([ 2.5, 16.5,  0. ]), 'targetState': array([ 9, 25], dtype=int32), 'currentDistance': 10.700467279516287}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08607238707567007
{'scaleFactor': 20, 'currentTarget': array([37.01494623, 49.76315073]), 'previousTarget': array([36.50479057, 49.2746546 ]), 'currentState': array([52.        , 63.00883545,  0.        ]), 'targetState': array([ 9, 25], dtype=int32), 'currentDistance': 20.0}
episode index:476
target Thresh 31.780720399424048
target distance 15.0
model initialize at round 476
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([17.5,  3.5,  0. ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 17.21917535772264}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08589194181974624
{'scaleFactor': 20, 'currentTarget': array([50.20161855, 42.14535858]), 'previousTarget': array([49.68061997, 41.67792888]), 'currentState': array([67.        , 52.99959189,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 20.0}
episode index:477
target Thresh 31.782902267905197
target distance 19.0
model initialize at round 477
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 25.]), 'previousTarget': array([12.08589282, 24.76686234]), 'currentState': array([19.5,  6.5,  0. ]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 19.962464777677162}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0857122515648932
{'scaleFactor': 20, 'currentTarget': array([51.43031777, 46.44453642]), 'previousTarget': array([50.90058581, 45.99940963]), 'currentState': array([69.        , 55.99996766,  0.        ]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 20.0}
episode index:478
target Thresh 31.78506242643222
target distance 24.0
model initialize at round 478
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.37130092,  8.16874412]), 'previousTarget': array([14.1492875,  7.59715  ]), 'currentState': array([19.5       , 27.49997392,  0.        ]), 'targetState': array([13,  3], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08553331158250303
{'scaleFactor': 20, 'currentTarget': array([56.93113189, 61.05174162]), 'previousTarget': array([56.44793188, 60.53904217]), 'currentState': array([69.       , 76.9998582,  0.       ]), 'targetState': array([13,  3], dtype=int32), 'currentDistance': 20.0}
episode index:479
target Thresh 31.78720109102277
target distance 12.0
model initialize at round 479
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([13.5       , 27.50001094,  0.        ]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 15.572418877188575}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.08704205024216712
{'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([24.        , 17.32649937,  0.        ]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 1.0519514439286097}
episode index:480
target Thresh 31.789318475545087
target distance 13.0
model initialize at round 480
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 23.]), 'previousTarget': array([ 2., 23.]), 'currentState': array([15.5, 14.5,  0. ]), 'targetState': array([ 2, 23], dtype=int32), 'currentDistance': 15.953056133543843}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08686108963875304
{'scaleFactor': 20, 'currentTarget': array([48.2372081 , 53.09087391]), 'previousTarget': array([47.71580423, 52.62383375]), 'currentState': array([65.        , 63.99998972,  0.        ]), 'targetState': array([ 2, 23], dtype=int32), 'currentDistance': 20.0}
episode index:481
target Thresh 31.791414791739392
target distance 20.0
model initialize at round 481
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.47979738, 16.74061153]), 'previousTarget': array([24.88854382, 17.05572809]), 'currentState': array([ 7.5, 25.5,  0. ]), 'targetState': array([27, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.08811127220354625
{'scaleFactor': 20, 'currentTarget': array([27., 16.]), 'previousTarget': array([27., 16.]), 'currentState': array([26.        , 15.42683352,  0.        ]), 'targetState': array([27, 16], dtype=int32), 'currentDistance': 1.1526143381404121}
episode index:482
target Thresh 31.79349024923905
target distance 24.0
model initialize at round 482
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.38968015, 17.61921925]), 'previousTarget': array([22.93091516, 17.6609096 ]), 'currentState': array([ 3.5       , 15.52145296,  0.        ]), 'targetState': array([27, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.08924600142478917
{'scaleFactor': 20, 'currentTarget': array([27., 18.]), 'previousTarget': array([27., 18.]), 'currentState': array([26.        , 17.44437391,  0.        ]), 'targetState': array([27, 18], dtype=int32), 'currentDistance': 1.1439931592293213}
episode index:483
target Thresh 31.79554505559154
target distance 12.0
model initialize at round 483
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 26.]), 'previousTarget': array([22., 26.]), 'currentState': array([10.5       , 25.50058928,  0.        ]), 'targetState': array([22, 26], dtype=int32), 'currentDistance': 11.510838851531625}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.0907178704084963
{'scaleFactor': 20, 'currentTarget': array([22., 26.]), 'previousTarget': array([22., 26.]), 'currentState': array([21.4999997 , 25.43447871,  0.        ]), 'targetState': array([22, 26], dtype=int32), 'currentDistance': 0.7548606641397183}
episode index:484
target Thresh 31.79757941627921
target distance 19.0
model initialize at round 484
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9.27268191, 5.34029958]), 'previousTarget': array([8.69147429, 4.97927459]), 'currentState': array([26.5       , 15.49999994,  0.        ]), 'targetState': array([7, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09053082325301487
{'scaleFactor': 20, 'currentTarget': array([61.01592427, 51.75320664]), 'previousTarget': array([60.5096307, 51.260329 ]), 'currentState': array([76.        , 64.99999774,  0.        ]), 'targetState': array([7, 4], dtype=int32), 'currentDistance': 20.0}
episode index:485
target Thresh 31.79959353473982
target distance 22.0
model initialize at round 485
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.4561672 ,  7.12611937]), 'previousTarget': array([15.26234812,  7.70472358]), 'currentState': array([10.5      , 26.5023008,  0.       ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.090344545838914
{'scaleFactor': 20, 'currentTarget': array([48.62329645, 52.16846213]), 'previousTarget': array([48.150523  , 51.64966461]), 'currentState': array([60.      , 68.617493,  0.      ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 20.0}
episode index:486
target Thresh 31.8015876123869
target distance 19.0
model initialize at round 486
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9.3639063 , 24.29134036]), 'previousTarget': array([ 9.43827311, 24.07475678]), 'currentState': array([18.5,  6.5,  0. ]), 'targetState': array([ 9, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0901590334244604
{'scaleFactor': 20, 'currentTarget': array([50.29513049, 46.69744144]), 'previousTarget': array([49.76560114, 46.25386042]), 'currentState': array([68., 56.,  0.]), 'targetState': array([ 9, 25], dtype=int32), 'currentDistance': 20.0}
episode index:487
target Thresh 31.803561848629876
target distance 6.0
model initialize at round 487
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 12.]), 'previousTarget': array([18., 12.]), 'currentState': array([24.5       ,  9.49999988,  0.        ]), 'targetState': array([18, 12], dtype=int32), 'currentDistance': 6.964194181385783}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08997428130678732
{'scaleFactor': 20, 'currentTarget': array([58.68051418, 46.1425704 ]), 'previousTarget': array([58.16958963, 45.65559813]), 'currentState': array([74.        , 58.99999449,  0.        ]), 'targetState': array([18, 12], dtype=int32), 'currentDistance': 20.0}
episode index:488
target Thresh 31.805516440894014
target distance 17.0
model initialize at round 488
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 29.]), 'previousTarget': array([23., 29.]), 'currentState': array([ 6.49966061, 21.60886371,  0.        ]), 'targetState': array([23, 29], dtype=int32), 'currentDistance': 18.0801022006083}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.0912728621956443
{'scaleFactor': 20, 'currentTarget': array([23., 29.]), 'previousTarget': array([23., 29.]), 'currentState': array([22.49855763, 29.09202129,  0.        ]), 'targetState': array([23, 29], dtype=int32), 'currentDistance': 0.5098160162293985}
episode index:489
target Thresh 31.807451584640173
target distance 15.0
model initialize at round 489
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([25.5, 10.5,  0. ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 15.572411502397522}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09108659104830626
{'scaleFactor': 20, 'currentTarget': array([59.26523684, 47.65426275]), 'previousTarget': array([58.75247632, 47.17054348]), 'currentState': array([75., 60.,  0.]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 20.0}
episode index:490
target Thresh 31.809367473384338
target distance 16.0
model initialize at round 490
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 18.]), 'previousTarget': array([ 2., 18.]), 'currentState': array([18.5, 23.5,  0. ]), 'targetState': array([ 2, 18], dtype=int32), 'currentDistance': 17.39252713092619}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09090107864291255
{'scaleFactor': 20, 'currentTarget': array([52.63557437, 60.19631175]), 'previousTarget': array([52.12595911, 59.70785888]), 'currentState': array([68.       , 72.9999997,  0.       ]), 'targetState': array([ 2, 18], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:491
target Thresh 31.811264298716978
target distance 10.0
model initialize at round 491
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 20.]), 'previousTarget': array([16., 20.]), 'currentState': array([18.5, 10.5,  0. ]), 'targetState': array([16, 20], dtype=int32), 'currentDistance': 9.823441352194195}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09071632035298793
{'scaleFactor': 20, 'currentTarget': array([52.14752022, 47.80578478]), 'previousTarget': array([51.63035296, 47.32813479]), 'currentState': array([68., 60.,  0.]), 'targetState': array([16, 20], dtype=int32), 'currentDistance': 20.0}
episode index:492
target Thresh 31.813142250322212
target distance 20.0
model initialize at round 492
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.82455532, 28.47366596]), 'previousTarget': array([15.60700849, 27.87716713]), 'currentState': array([9.5, 9.5, 0. ]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09053231158959445
{'scaleFactor': 20, 'currentTarget': array([42.59746091, 47.55636808]), 'previousTarget': array([42.07008556, 47.09570645]), 'currentState': array([59., 59.,  0.]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:493
target Thresh 31.815001515996762
target distance 22.0
model initialize at round 493
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.96499055, 27.49459386]), 'previousTarget': array([15.90815322, 26.9793708 ]), 'currentState': array([15.5,  7.5,  0. ]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09034904780095154
{'scaleFactor': 20, 'currentTarget': array([47.63513716, 47.07722123]), 'previousTarget': array([47.10212131, 46.63522342]), 'currentState': array([65., 57.,  0.]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 20.0}
episode index:494
target Thresh 31.816842281668748
target distance 3.0
model initialize at round 494
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 25.]), 'previousTarget': array([19., 25.]), 'currentState': array([20.5       , 22.49999982,  0.        ]), 'targetState': array([19, 25], dtype=int32), 'currentDistance': 2.9154761007544274}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09016652447206074
{'scaleFactor': 20, 'currentTarget': array([55.2909464 , 58.43502332]), 'previousTarget': array([54.78523433, 57.94122592]), 'currentState': array([70.        , 71.98654509,  0.        ]), 'targetState': array([19, 25], dtype=int32), 'currentDistance': 20.0}
episode index:495
target Thresh 31.818664731416266
target distance 21.0
model initialize at round 495
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9.27647243, 19.19603151]), 'previousTarget': array([ 9.0913656 , 19.12086431]), 'currentState': array([25.5,  7.5,  0. ]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0899847371243348
{'scaleFactor': 20, 'currentTarget': array([56.96161731, 48.36190125]), 'previousTarget': array([56.43569141, 47.91625053]), 'currentState': array([75., 57.,  0.]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 20.0}
episode index:496
target Thresh 31.820469047485812
target distance 10.0
model initialize at round 496
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 24.]), 'previousTarget': array([21., 24.]), 'currentState': array([26.5, 14.5,  0. ]), 'targetState': array([21, 24], dtype=int32), 'currentDistance': 10.977249200050045}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08980368131523152
{'scaleFactor': 20, 'currentTarget': array([59.82527828, 52.23656585]), 'previousTarget': array([59.30602624, 51.76308309]), 'currentState': array([76.        , 63.99999976,  0.        ]), 'targetState': array([21, 24], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:497
target Thresh 31.8222554103105
target distance 2.0
model initialize at round 497
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  8.]), 'previousTarget': array([21.,  8.]), 'currentState': array([21.5       ,  9.51036143,  0.        ]), 'targetState': array([21,  8], dtype=int32), 'currentDistance': 1.5909719225624683}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08962335263789169
{'scaleFactor': 20, 'currentTarget': array([56.44165965, 41.38445637]), 'previousTarget': array([55.93739887, 40.88898111]), 'currentState': array([71.        , 55.09776108,  0.        ]), 'targetState': array([21,  8], dtype=int32), 'currentDistance': 20.0}
episode index:498
target Thresh 31.824023998528094
target distance 9.0
model initialize at round 498
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 23.]), 'previousTarget': array([15., 23.]), 'currentState': array([ 6.5       , 22.50019717,  0.        ]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 8.514681606890058}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.09116731055773818
{'scaleFactor': 20, 'currentTarget': array([15., 23.]), 'previousTarget': array([15., 23.]), 'currentState': array([14.        , 22.74330989,  0.        ]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 1.0324193983284553}
episode index:499
target Thresh 31.825774988998898
target distance 7.0
model initialize at round 499
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  8.]), 'previousTarget': array([19.,  8.]), 'currentState': array([26.5       , 10.49982181,  0.        ]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 7.905637804208111}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0909849759366227
{'scaleFactor': 20, 'currentTarget': array([61.22460502, 46.52026478]), 'previousTarget': array([60.71889267, 46.02652913]), 'currentState': array([76.        , 59.99942288,  0.        ]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 20.0}
episode index:500
target Thresh 31.82750855682341
target distance 8.0
model initialize at round 500
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  6.]), 'previousTarget': array([18.,  6.]), 'currentState': array([26.5       ,  9.49944234,  0.        ]), 'targetState': array([18,  6], dtype=int32), 'currentDistance': 9.192175840550032}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09080336919822625
{'scaleFactor': 20, 'currentTarget': array([61.23567327, 45.507701  ]), 'previousTarget': array([60.73015988, 45.01373711]), 'currentState': array([76.        , 58.99898174,  0.        ]), 'targetState': array([18,  6], dtype=int32), 'currentDistance': 20.0}
episode index:501
target Thresh 31.829224875359866
target distance 15.0
model initialize at round 501
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([19.5,  9.5,  0. ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 16.44688420339863}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09062248599265209
{'scaleFactor': 20, 'currentTarget': array([53.73227429, 46.08115517]), 'previousTarget': array([53.22329409, 45.59177563]), 'currentState': array([69., 59.,  0.]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 20.0}
episode index:502
target Thresh 31.83092411624154
target distance 5.0
model initialize at round 502
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 21.]), 'previousTarget': array([10., 21.]), 'currentState': array([ 8.5, 16.5,  0. ]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 4.743416490252466}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09044232200459514
{'scaleFactor': 20, 'currentTarget': array([43.40714004, 52.30948511]), 'previousTarget': array([42.90232687, 51.81462256]), 'currentState': array([58.       , 65.9860504,  0.       ]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 20.0}
episode index:503
target Thresh 31.832606449393943
target distance 10.0
model initialize at round 503
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 24.]), 'previousTarget': array([23., 24.]), 'currentState': array([13.5, 14.5,  0. ]), 'targetState': array([23, 24], dtype=int32), 'currentDistance': 13.43502884254429}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.09191865422571714
{'scaleFactor': 20, 'currentTarget': array([23., 24.]), 'previousTarget': array([23., 24.]), 'currentState': array([22.5       , 23.49989381,  0.        ]), 'targetState': array([23, 24], dtype=int32), 'currentDistance': 0.7071818697812043}
episode index:504
target Thresh 31.83427204305179
target distance 13.0
model initialize at round 504
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 20.]), 'previousTarget': array([ 2., 20.]), 'currentState': array([8.5, 7.5, 0. ]), 'targetState': array([ 2, 20], dtype=int32), 'currentDistance': 14.08900280360541}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09173663708863652
{'scaleFactor': 20, 'currentTarget': array([41.31329359, 45.97485469]), 'previousTarget': array([40.78983957, 45.51043503]), 'currentState': array([58., 57.,  0.]), 'targetState': array([ 2, 20], dtype=int32), 'currentDistance': 20.0}
episode index:505
target Thresh 31.835921063775835
target distance 17.0
model initialize at round 505
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 15.]), 'previousTarget': array([ 8., 15.]), 'currentState': array([25.5,  6.5,  0. ]), 'targetState': array([ 8, 15], dtype=int32), 'currentDistance': 19.45507645834379}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09155533938688032
{'scaleFactor': 20, 'currentTarget': array([57.94066034, 45.5607026 ]), 'previousTarget': array([57.41850944, 45.09698696]), 'currentState': array([75., 56.,  0.]), 'targetState': array([ 8, 15], dtype=int32), 'currentDistance': 20.0}
episode index:506
target Thresh 31.837553676469525
target distance 7.0
model initialize at round 506
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 20.]), 'previousTarget': array([16., 20.]), 'currentState': array([23.5, 18.5,  0. ]), 'targetState': array([16, 20], dtype=int32), 'currentDistance': 7.648529270389242}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0913747568634348
{'scaleFactor': 20, 'currentTarget': array([57.70177602, 55.11728498]), 'previousTarget': array([57.1912417 , 54.62980488]), 'currentState': array([73.        , 67.99999988,  0.        ]), 'targetState': array([16, 20], dtype=int32), 'currentDistance': 20.0}
episode index:507
target Thresh 31.83917004439549
target distance 10.0
model initialize at round 507
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 20.]), 'previousTarget': array([13., 20.]), 'currentState': array([ 7.5, 10.5,  0. ]), 'targetState': array([13, 20], dtype=int32), 'currentDistance': 10.977249200050007}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09119488529480599
{'scaleFactor': 20, 'currentTarget': array([42.20039901, 46.54264783]), 'previousTarget': array([41.69269102, 46.05113251]), 'currentState': array([57.        , 59.99522418,  0.        ]), 'targetState': array([13, 20], dtype=int32), 'currentDistance': 20.0}
episode index:508
target Thresh 31.84077032919187
target distance 7.0
model initialize at round 508
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 18.]), 'previousTarget': array([25., 18.]), 'currentState': array([18.5       , 23.50000396,  0.        ]), 'targetState': array([25, 18], dtype=int32), 'currentDistance': 8.514695743290684}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.09277473474267221
{'scaleFactor': 20, 'currentTarget': array([25., 18.]), 'previousTarget': array([25., 18.]), 'currentState': array([24.        , 18.00794572,  0.        ]), 'targetState': array([25, 18], dtype=int32), 'currentDistance': 1.0000315667052218}
episode index:509
target Thresh 31.842354690888477
target distance 11.0
model initialize at round 509
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([14.5, 20.5,  0. ]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 11.067971810589226}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.09421276197618773
{'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([24.        , 16.81593826,  0.        ]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 1.0167982706975969}
episode index:510
target Thresh 31.8439232879228
target distance 14.0
model initialize at round 510
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([ 2.5, 15.5,  0. ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 14.983324063771667}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.09555054788112503
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.        ,  8.80807056,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.0182518885384217}
episode index:511
target Thresh 31.84547627715585
target distance 7.0
model initialize at round 511
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 12.]), 'previousTarget': array([11., 12.]), 'currentState': array([15.5,  5.5,  0. ]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 7.905694150420999}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09536392571729471
{'scaleFactor': 20, 'currentTarget': array([49.35439711, 42.54146436]), 'previousTarget': array([48.83987962, 42.05971746]), 'currentState': array([65., 55.,  0.]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 20.0}
episode index:512
target Thresh 31.84701381388785
target distance 2.0
model initialize at round 512
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 22.]), 'previousTarget': array([ 7., 22.]), 'currentState': array([ 5.5       , 21.57639027,  0.        ]), 'targetState': array([ 7, 22], dtype=int32), 'currentDistance': 1.5586677665189304}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.09708855744104268
{'scaleFactor': 20, 'currentTarget': array([ 7., 22.]), 'previousTarget': array([ 7., 22.]), 'currentState': array([ 6.5       , 21.52664545,  0.        ]), 'targetState': array([ 7, 22], dtype=int32), 'currentDistance': 0.6885234406442238}
episode index:513
target Thresh 31.848536051873744
target distance 11.0
model initialize at round 513
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  3.]), 'previousTarget': array([26.,  3.]), 'currentState': array([25.5       , 14.49946609,  0.        ]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 11.510331028564357}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09689966919699396
{'scaleFactor': 20, 'currentTarget': array([62.47484273, 48.40680796]), 'previousTarget': array([61.99026812, 47.89442944]), 'currentState': array([75.        , 63.99912772,  0.        ]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 20.0}
episode index:514
target Thresh 31.85004314333861
target distance 7.0
model initialize at round 514
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 10.]), 'previousTarget': array([20., 10.]), 'currentState': array([27.5,  7.5,  0. ]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 7.905694150421027}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09671151449952406
{'scaleFactor': 20, 'currentTarget': array([61.56921267, 44.27636835]), 'previousTarget': array([61.05745121, 43.79064569]), 'currentState': array([77., 57.,  0.]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 20.0}
episode index:515
target Thresh 31.85153523899284
target distance 9.0
model initialize at round 515
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([13.5       , 20.50000092,  0.        ]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 11.335784741508773}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.09817420104331351
{'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([21.5       , 12.60665193,  0.        ]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 0.7861466571843977}
episode index:516
target Thresh 31.853012488047252
target distance 10.0
model initialize at round 516
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 27.]), 'previousTarget': array([23., 27.]), 'currentState': array([24.5, 17.5,  0. ]), 'targetState': array([23, 27], dtype=int32), 'currentDistance': 9.617692030835604}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.097984308971663
{'scaleFactor': 20, 'currentTarget': array([58.26293165, 54.6572013 ]), 'previousTarget': array([57.74660918, 54.17804084]), 'currentState': array([74., 67.,  0.]), 'targetState': array([23, 27], dtype=int32), 'currentDistance': 20.0}
episode index:517
target Thresh 31.854475038227978
target distance 18.0
model initialize at round 517
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([20.5       , 22.49999988,  0.        ]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 18.82817026663123}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0977951500740343
{'scaleFactor': 20, 'currentTarget': array([57.705134  , 56.22545479]), 'previousTarget': array([57.22118771, 55.71295546]), 'currentState': array([70.        , 71.99999979,  0.        ]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 20.0}
episode index:518
target Thresh 31.855923035791257
target distance 14.0
model initialize at round 518
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 13.]), 'previousTarget': array([13., 13.]), 'currentState': array([27.5,  9.5,  0. ]), 'targetState': array([13, 13], dtype=int32), 'currentDistance': 14.916433890176357}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09760672011242731
{'scaleFactor': 20, 'currentTarget': array([60.75969257, 47.32727904]), 'previousTarget': array([60.24264733, 46.85103077]), 'currentState': array([77., 59.,  0.]), 'targetState': array([13, 13], dtype=int32), 'currentDistance': 20.0}
episode index:519
target Thresh 31.85735662553805
target distance 13.0
model initialize at round 519
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  7.]), 'previousTarget': array([21.,  7.]), 'currentState': array([10.5, 19.5,  0. ]), 'targetState': array([21,  7], dtype=int32), 'currentDistance': 16.324827717314392}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09741901488144186
{'scaleFactor': 20, 'currentTarget': array([45.56697338, 30.56630066]), 'previousTarget': array([45.06315873, 30.07027837]), 'currentState': array([60.        , 44.41143491,  0.        ]), 'targetState': array([21,  7], dtype=int32), 'currentDistance': 20.0}
episode index:520
target Thresh 31.858775950828523
target distance 9.0
model initialize at round 520
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 19.]), 'previousTarget': array([17., 19.]), 'currentState': array([18.5, 10.5,  0. ]), 'targetState': array([17, 19], dtype=int32), 'currentDistance': 8.63133825081597}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09723203020796502
{'scaleFactor': 20, 'currentTarget': array([52.41248658, 47.46886176]), 'previousTarget': array([51.89770621, 46.98726934]), 'currentState': array([68., 60.,  0.]), 'targetState': array([17, 19], dtype=int32), 'currentDistance': 20.0}
episode index:521
target Thresh 31.860181153596397
target distance 12.0
model initialize at round 521
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 8.]), 'previousTarget': array([6., 8.]), 'currentState': array([14.5       , 20.49999961,  0.        ]), 'targetState': array([6, 8], dtype=int32), 'currentDistance': 15.116216137454819}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09704576195086163
{'scaleFactor': 20, 'currentTarget': array([50.33685736, 55.39457125]), 'previousTarget': array([49.84094447, 54.8907489 ]), 'currentState': array([64.        , 69.99999946,  0.        ]), 'targetState': array([6, 8], dtype=int32), 'currentDistance': 20.0}
episode index:522
target Thresh 31.86157237436311
target distance 18.0
model initialize at round 522
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 29.]), 'previousTarget': array([20., 29.]), 'currentState': array([17.5, 11.5,  0. ]), 'targetState': array([20, 29], dtype=int32), 'currentDistance': 17.6776695296636}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09686020600066877
{'scaleFactor': 20, 'currentTarget': array([50.46801505, 49.74418046]), 'previousTarget': array([49.94162619, 49.2830371 ]), 'currentState': array([67., 61.,  0.]), 'targetState': array([20, 29], dtype=int32), 'currentDistance': 20.0}
episode index:523
target Thresh 31.862949752251907
target distance 6.0
model initialize at round 523
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 24.]), 'previousTarget': array([ 8., 24.]), 'currentState': array([14.5       , 27.49897453,  0.        ]), 'targetState': array([ 8, 24], dtype=int32), 'currentDistance': 7.3819254110841035}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09667535827929345
{'scaleFactor': 20, 'currentTarget': array([49.47386907, 63.25055098]), 'previousTarget': array([48.97036631, 62.7542531 ]), 'currentState': array([64.        , 76.99796966,  0.        ]), 'targetState': array([ 8, 24], dtype=int32), 'currentDistance': 20.0}
episode index:524
target Thresh 31.86431342500172
target distance 5.0
model initialize at round 524
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 16.]), 'previousTarget': array([16., 16.]), 'currentState': array([19.5, 11.5,  0. ]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 5.700877125495675}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09649121473971385
{'scaleFactor': 20, 'currentTarget': array([53.75411842, 48.05538356]), 'previousTarget': array([53.24330275, 47.56813281]), 'currentState': array([69., 61.,  0.]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 20.0}
episode index:525
target Thresh 31.865663528980956
target distance 20.0
model initialize at round 525
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.01234013,  2.50594538]), 'previousTarget': array([10.,  2.]), 'currentState': array([10.5      , 22.4999992,  0.       ]), 'targetState': array([10,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09630777136568397
{'scaleFactor': 20, 'currentTarget': array([48.37523587, 55.72532846]), 'previousTarget': array([47.89741921, 55.20950603]), 'currentState': array([60.        , 71.99999771,  0.        ]), 'targetState': array([10,  2], dtype=int32), 'currentDistance': 20.0}
episode index:526
target Thresh 31.867000199201147
target distance 20.0
model initialize at round 526
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.31742867, 10.18314323]), 'previousTarget': array([16.1492875,  9.59715  ]), 'currentState': array([21.5       , 29.49999997,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0961250241714417
{'scaleFactor': 20, 'currentTarget': array([58.64356894, 63.27363303]), 'previousTarget': array([58.15857586, 62.76185345]), 'currentState': array([71.        , 78.99999979,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 20.0}
episode index:527
target Thresh 31.86832356933042
target distance 9.0
model initialize at round 527
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 16.]), 'previousTarget': array([25., 16.]), 'currentState': array([27.5       , 25.48833886,  0.        ]), 'targetState': array([25, 16], dtype=int32), 'currentDistance': 9.81216460737039}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09594296920142002
{'scaleFactor': 20, 'currentTarget': array([63.77361171, 59.97913724]), 'previousTarget': array([63.28216028, 59.47160483]), 'currentState': array([77.        , 74.98122552,  0.        ]), 'targetState': array([25, 16], dtype=int32), 'currentDistance': 20.0}
episode index:528
target Thresh 31.8696337717069
target distance 24.0
model initialize at round 528
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.10774054, 23.15744778]), 'previousTarget': array([11.05572809, 22.88854382]), 'currentState': array([20.5,  5.5,  0. ]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09576160252996176
{'scaleFactor': 20, 'currentTarget': array([51.55611236, 47.26546647]), 'previousTarget': array([51.02515773, 46.83969955]), 'currentState': array([70., 55.,  0.]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 20.0}
episode index:529
target Thresh 31.87093093735191
target distance 11.0
model initialize at round 529
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 24.]), 'previousTarget': array([23., 24.]), 'currentState': array([12.49999836, 21.49998698,  0.        ]), 'targetState': array([23, 24], dtype=int32), 'currentDistance': 10.793521183558692}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.0971241408980132
{'scaleFactor': 20, 'currentTarget': array([23., 24.]), 'previousTarget': array([23., 24.]), 'currentState': array([22.49999681, 24.52667056,  0.        ]), 'targetState': array([23, 24], dtype=int32), 'currentDistance': 0.7262128224340547}
episode index:530
target Thresh 31.8722151959831
target distance 20.0
model initialize at round 530
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7.50594619, 11.01234015]), 'previousTarget': array([ 7., 11.]), 'currentState': array([27.5, 11.5,  0. ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09694123291138795
{'scaleFactor': 20, 'currentTarget': array([60.72533058, 49.37523613]), 'previousTarget': array([60.20950814, 48.89741947]), 'currentState': array([77., 61.,  0.]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 20.0}
episode index:531
target Thresh 31.873486676027397
target distance 10.0
model initialize at round 531
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([ 9.5       , 14.49999937,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 12.349088503091052}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09675901254877256
{'scaleFactor': 20, 'currentTarget': array([45.35363489, 49.37889392]), 'previousTarget': array([44.85801439, 48.87480762]), 'currentState': array([59.       , 63.9999992,  0.       ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 20.0}
episode index:532
target Thresh 31.874745504633868
target distance 4.0
model initialize at round 532
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 14.]), 'previousTarget': array([18., 14.]), 'currentState': array([22.5       , 16.49999583,  0.        ]), 'targetState': array([18, 14], dtype=int32), 'currentDistance': 5.147813044234733}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09657747593986304
{'scaleFactor': 20, 'currentTarget': array([57.593595  , 52.12716156]), 'previousTarget': array([57.09110325, 51.62974961]), 'currentState': array([72.        , 65.99999455,  0.        ]), 'targetState': array([18, 14], dtype=int32), 'currentDistance': 20.0}
episode index:533
target Thresh 31.875991807686425
target distance 9.0
model initialize at round 533
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  5.]), 'previousTarget': array([17.,  5.]), 'currentState': array([14.5       , 13.80665831,  0.        ]), 'targetState': array([17,  5], dtype=int32), 'currentDistance': 9.154628918545246}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09639661924334644
{'scaleFactor': 20, 'currentTarget': array([51.38277627, 47.28735163]), 'previousTarget': array([50.89803097, 46.77496086]), 'currentState': array([64.        , 62.80526596,  0.        ]), 'targetState': array([17,  5], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:534
target Thresh 31.87722570981641
target distance 4.0
model initialize at round 534
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 25.]), 'previousTarget': array([ 5., 25.]), 'currentState': array([ 9.5       , 27.49996653,  0.        ]), 'targetState': array([ 5, 25], dtype=int32), 'currentDistance': 5.147798817075159}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0962164386466299
{'scaleFactor': 20, 'currentTarget': array([44.5935897 , 63.12712726]), 'previousTarget': array([44.09109789, 62.62971536]), 'currentState': array([59.        , 76.99995473,  0.        ]), 'targetState': array([ 5, 25], dtype=int32), 'currentDistance': 20.0}
episode index:535
target Thresh 31.878447334415068
target distance 16.0
model initialize at round 535
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 21.]), 'previousTarget': array([10., 21.]), 'currentState': array([26.5, 27.5,  0. ]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 17.734147850968302}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09603693036557276
{'scaleFactor': 20, 'currentTarget': array([60.74981729, 64.06045104]), 'previousTarget': array([60.24111428, 63.57071515]), 'currentState': array([76., 77.,  0.]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 20.0}
episode index:536
target Thresh 31.87965680364587
target distance 21.0
model initialize at round 536
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.543335  ,  8.12394591]), 'previousTarget': array([21.36758945,  7.54387571]), 'currentState': array([26.5, 27.5,  0. ]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0958580906442216
{'scaleFactor': 20, 'currentTarget': array([63.75204516, 61.18900375]), 'previousTarget': array([63.26785634, 60.6767683 ]), 'currentState': array([76., 77.,  0.]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 20.0}
episode index:537
target Thresh 31.88085423845675
target distance 13.0
model initialize at round 537
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([14.5, 23.5,  0. ]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 12.980754985747073}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.09715503524087482
{'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([26.        , 20.09698432,  0.        ]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 1.004691971802028}
episode index:538
target Thresh 31.882039758592185
target distance 16.0
model initialize at round 538
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([22.5,  4.5,  0. ]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 16.446884203398557}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09697478471167098
{'scaleFactor': 20, 'currentTarget': array([54.98810861, 43.48355805]), 'previousTarget': array([54.4614688, 43.0267744]), 'currentState': array([72., 54.,  0.]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 20.0}
episode index:539
target Thresh 31.883213482605182
target distance 11.0
model initialize at round 539
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 19.]), 'previousTarget': array([12., 19.]), 'currentState': array([23.5, 25.5,  0. ]), 'targetState': array([12, 19], dtype=int32), 'currentDistance': 13.209844813623016}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09679520177701974
{'scaleFactor': 20, 'currentTarget': array([58.26694684, 61.47457415]), 'previousTarget': array([57.76197415, 60.97999282]), 'currentState': array([73., 75.,  0.]), 'targetState': array([12, 19], dtype=int32), 'currentDistance': 20.0}
episode index:540
target Thresh 31.884375527869114
target distance 26.0
model initialize at round 540
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.78039959, 23.31423159]), 'previousTarget': array([23.70751784, 22.86817872]), 'currentState': array([26.5,  3.5,  0. ]), 'targetState': array([23, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09661628273491803
{'scaleFactor': 20, 'currentTarget': array([57.7809164 , 44.74984893]), 'previousTarget': array([57.24534079, 44.32886683]), 'currentState': array([76., 53.,  0.]), 'targetState': array([23, 29], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:541
target Thresh 31.885526010589484
target distance 9.0
model initialize at round 541
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([15.5, 25.5,  0. ]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 12.103718436910317}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09643802391068387
{'scaleFactor': 20, 'currentTarget': array([50.6161722 , 61.10375958]), 'previousTarget': array([50.11408959, 60.60591559]), 'currentState': array([65., 75.,  0.]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 20.0}
episode index:542
target Thresh 31.886665045815512
target distance 19.0
model initialize at round 542
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.85000342,  4.32646315]), 'previousTarget': array([23.56172689,  4.92524322]), 'currentState': array([15.5       , 22.50000003,  0.        ]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0962604216567047
{'scaleFactor': 20, 'currentTarget': array([53.82120737, 48.24090327]), 'previousTarget': array([53.35207056, 47.72014121]), 'currentState': array([65.        , 64.82507028,  0.        ]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 20.0}
episode index:543
target Thresh 31.88779274745168
target distance 12.0
model initialize at round 543
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([ 9.5, 13.5,  0. ]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 13.72953021774587}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.09757194269818367
{'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([20.        ,  6.20920843,  0.        ]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 1.0216497272271896}
episode index:544
target Thresh 31.888909228269085
target distance 14.0
model initialize at round 544
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 17.]), 'previousTarget': array([ 5., 17.]), 'currentState': array([19.5, 22.5,  0. ]), 'targetState': array([ 5, 17], dtype=int32), 'currentDistance': 15.508062419270924}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09739291161066406
{'scaleFactor': 20, 'currentTarget': array([53.83161034, 58.96466513]), 'previousTarget': array([53.32330654, 58.47433396]), 'currentState': array([69., 72.,  0.]), 'targetState': array([ 5, 17], dtype=int32), 'currentDistance': 20.0}
episode index:545
target Thresh 31.89001459991674
target distance 12.0
model initialize at round 545
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 24.]), 'previousTarget': array([ 5., 24.]), 'currentState': array([17.5, 29.5,  0. ]), 'targetState': array([ 5, 24], dtype=int32), 'currentDistance': 13.656500283747771}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09721453631467383
{'scaleFactor': 20, 'currentTarget': array([52.03850723, 65.72770803]), 'previousTarget': array([51.53168841, 65.23539868]), 'currentState': array([67., 79.,  0.]), 'targetState': array([ 5, 24], dtype=int32), 'currentDistance': 20.0}
episode index:546
target Thresh 31.891108972932734
target distance 15.0
model initialize at round 546
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 14.]), 'previousTarget': array([ 5., 14.]), 'currentState': array([17.5, 29.5,  0. ]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 19.91230775173999}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09703681321355012
{'scaleFactor': 20, 'currentTarget': array([53.19578142, 64.52783536]), 'previousTarget': array([52.69849333, 64.0252491 ]), 'currentState': array([67., 79.,  0.]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 20.0}
episode index:547
target Thresh 31.892192456755275
target distance 7.0
model initialize at round 547
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 24.]), 'previousTarget': array([12., 24.]), 'currentState': array([19.5, 22.5,  0. ]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 7.648529270389238}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09685973873688306
{'scaleFactor': 20, 'currentTarget': array([53.70177604, 59.11728508]), 'previousTarget': array([53.19124172, 58.62980498]), 'currentState': array([69., 72.,  0.]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 20.0}
episode index:548
target Thresh 31.893265159733655
target distance 24.0
model initialize at round 548
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.32842712,  9.70101013]), 'previousTarget': array([21.28797975,  9.27212152]), 'currentState': array([18.5, 29.5,  0. ]), 'targetState': array([22,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09668330934027672
{'scaleFactor': 20, 'currentTarget': array([57.44131758, 62.01429349]), 'previousTarget': array([56.97290771, 61.49469707]), 'currentState': array([68., 79.,  0.]), 'targetState': array([22,  5], dtype=int32), 'currentDistance': 20.0}
episode index:549
target Thresh 31.894327189139062
target distance 22.0
model initialize at round 549
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.28779003, 21.64422855]), 'previousTarget': array([16.06407315, 21.0585156 ]), 'currentState': array([10.5,  2.5,  0. ]), 'targetState': array([17, 24], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09650752150511258
{'scaleFactor': 20, 'currentTarget': array([43.24003831, 41.08653657]), 'previousTarget': array([42.70859686, 40.63497444]), 'currentState': array([60., 52.,  0.]), 'targetState': array([17, 24], dtype=int32), 'currentDistance': 20.0}
episode index:550
target Thresh 31.89537865117532
target distance 9.0
model initialize at round 550
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 12.]), 'previousTarget': array([14., 12.]), 'currentState': array([11.5       , 21.23474851,  0.        ]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 9.56716154819406}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09633237173831563
{'scaleFactor': 20, 'currentTarget': array([48.49230027, 55.03733971]), 'previousTarget': array([48.00854532, 54.52433397]), 'currentState': array([61.        , 70.64366686,  0.        ]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 20.0}
episode index:551
target Thresh 31.89641965098951
target distance 17.0
model initialize at round 551
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  5.]), 'previousTarget': array([21.,  5.]), 'currentState': array([11.5       , 21.50000003,  0.        ]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 19.039432790487044}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09615785657212303
{'scaleFactor': 20, 'currentTarget': array([49.62816846, 46.41831169]), 'previousTarget': array([49.15818845, 45.89760242]), 'currentState': array([61.        , 62.87071113,  0.        ]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 20.0}
episode index:552
target Thresh 31.897450292682485
target distance 11.0
model initialize at round 552
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([13.5, 19.5,  0. ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 14.916433890176409}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09598397256385519
{'scaleFactor': 20, 'currentTarget': array([49.09549768, 54.62415862]), 'previousTarget': array([48.59751068, 54.1222119 ]), 'currentState': array([63., 69.,  0.]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 20.0}
episode index:553
target Thresh 31.898470679319267
target distance 12.0
model initialize at round 553
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 29.]), 'previousTarget': array([11., 29.]), 'currentState': array([ 7.5, 17.5,  0. ]), 'targetState': array([11, 29], dtype=int32), 'currentDistance': 12.020815280171231}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09581071629568938
{'scaleFactor': 20, 'currentTarget': array([41.58076497, 54.26237106]), 'previousTarget': array([41.06629214, 53.7799111 ]), 'currentState': array([57., 67.,  0.]), 'targetState': array([11, 29], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:554
target Thresh 31.899480912939374
target distance 14.0
model initialize at round 554
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([16.5, 11.5,  0. ]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 13.946325680981296}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09563808437443588
{'scaleFactor': 20, 'currentTarget': array([49.45566617, 49.76233929]), 'previousTarget': array([48.93217825, 49.29699672]), 'currentState': array([66., 61.,  0.]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 20.0}
episode index:555
target Thresh 31.90048109456701
target distance 10.0
model initialize at round 555
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 18.]), 'previousTarget': array([13., 18.]), 'currentState': array([22.5,  8.5,  0. ]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 13.435028842544405}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0954660734313164
{'scaleFactor': 20, 'currentTarget': array([55.44583004, 46.77683392]), 'previousTarget': array([54.92466907, 46.30810988]), 'currentState': array([72., 58.,  0.]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 20.0}
episode index:556
target Thresh 31.90147132422117
target distance 5.0
model initialize at round 556
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'previousTarget': array([16., 23.]), 'currentState': array([12.5, 18.5,  0. ]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 5.700877125495577}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.0969513133253857
{'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'previousTarget': array([16., 23.]), 'currentState': array([16.5      , 22.4999989,  0.       ]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 0.7071075609038155}
episode index:557
target Thresh 31.902451700925642
target distance 9.0
model initialize at round 557
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 25.]), 'previousTarget': array([14., 25.]), 'currentState': array([23.5, 25.5,  0. ]), 'targetState': array([14, 25], dtype=int32), 'currentDistance': 9.513148795220308}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09677756545204272
{'scaleFactor': 20, 'currentTarget': array([57.74208799, 62.06956609]), 'previousTarget': array([57.23227429, 61.58115517]), 'currentState': array([73., 75.,  0.]), 'targetState': array([14, 25], dtype=int32), 'currentDistance': 20.0}
episode index:558
target Thresh 31.90342232271892
target distance 20.0
model initialize at round 558
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4.98765985, 2.50594619]), 'previousTarget': array([4.99875234, 2.02495322]), 'currentState': array([ 4.5, 22.5,  0. ]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09660443921688701
{'scaleFactor': 20, 'currentTarget': array([42.53075311, 55.61536159]), 'previousTarget': array([42.05452247, 55.0987487 ]), 'currentState': array([54., 72.,  0.]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 20.0}
episode index:559
target Thresh 31.904383286663986
target distance 10.0
model initialize at round 559
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 23.]), 'previousTarget': array([17., 23.]), 'currentState': array([27.5, 16.5,  0. ]), 'targetState': array([17, 23], dtype=int32), 'currentDistance': 12.349089035228495}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09643193128971399
{'scaleFactor': 20, 'currentTarget': array([60.74365745, 54.34962117]), 'previousTarget': array([60.22533058, 53.87523613]), 'currentState': array([77., 66.,  0.]), 'targetState': array([17, 23], dtype=int32), 'currentDistance': 20.0}
episode index:560
target Thresh 31.905334688858037
target distance 12.0
model initialize at round 560
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'previousTarget': array([13., 22.]), 'currentState': array([25.5, 15.5,  0. ]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 14.089002803605403}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09626003836406388
{'scaleFactor': 20, 'currentTarget': array([58.56570444, 53.60202082]), 'previousTarget': array([58.04653075, 53.12971637]), 'currentState': array([75., 65.,  0.]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 20.0}
episode index:561
target Thresh 31.906276624442086
target distance 22.0
model initialize at round 561
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.89196526, 24.45150202]), 'previousTarget': array([16.81071492, 23.91786413]), 'currentState': array([15.5,  4.5,  0. ]), 'targetState': array([17, 26], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09608875715701039
{'scaleFactor': 20, 'currentTarget': array([47.72442198, 43.92257949]), 'previousTarget': array([47.19147429, 43.47927459]), 'currentState': array([65., 54.,  0.]), 'targetState': array([17, 26], dtype=int32), 'currentDistance': 20.0}
episode index:562
target Thresh 31.907209187610476
target distance 4.0
model initialize at round 562
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([17.5,  5.5,  0. ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 4.301162633521366}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09591808440895175
{'scaleFactor': 20, 'currentTarget': array([52.02006875, 41.74852236]), 'previousTarget': array([51.5117392 , 41.25794434]), 'currentState': array([67., 55.,  0.]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 20.0}
episode index:563
target Thresh 31.908132471620302
target distance 19.0
model initialize at round 563
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'previousTarget': array([25.4327075,  9.23886  ]), 'currentState': array([ 7.5, 16.5,  0. ]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 19.962464777677102}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.09698279209271171
{'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([25.49997601,  8.47099835,  0.        ]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 0.7279194547783899}
episode index:564
target Thresh 31.909046568800733
target distance 5.0
model initialize at round 564
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  6.]), 'previousTarget': array([24.,  6.]), 'currentState': array([20.5       , 10.50105095,  0.        ]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 5.701706731002858}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.09844431758357049
{'scaleFactor': 20, 'currentTarget': array([24.,  6.]), 'previousTarget': array([24.,  6.]), 'currentState': array([24.5       ,  6.52822688,  0.        ]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 0.7273401123957149}
episode index:565
target Thresh 31.90995157056225
target distance 20.0
model initialize at round 565
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 21.]), 'previousTarget': array([22.9007438 , 20.99007438]), 'currentState': array([ 3.49998891, 18.5       ,  0.        ]), 'targetState': array([23, 21], dtype=int32), 'currentDistance': 19.659614247797847}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.09947631454015497
{'scaleFactor': 20, 'currentTarget': array([23., 21.]), 'previousTarget': array([23., 21.]), 'currentState': array([22.47449473, 21.26126639,  0.        ]), 'targetState': array([23, 21], dtype=int32), 'currentDistance': 0.5868695928911615}
episode index:566
target Thresh 31.91084756740578
target distance 11.0
model initialize at round 566
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 24.]), 'previousTarget': array([ 7., 24.]), 'currentState': array([18.5, 18.5,  0. ]), 'targetState': array([ 7, 24], dtype=int32), 'currentDistance': 12.747548783982}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09930087130463441
{'scaleFactor': 20, 'currentTarget': array([51.77940648, 56.29989975]), 'previousTarget': array([51.26168033, 55.82451396]), 'currentState': array([68., 68.,  0.]), 'targetState': array([ 7, 24], dtype=int32), 'currentDistance': 20.0}
episode index:567
target Thresh 31.91173464893176
target distance 17.0
model initialize at round 567
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([ 2.5       , 21.50000021,  0.        ]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 19.03943294545138}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0991260458269854
{'scaleFactor': 20, 'currentTarget': array([40.64740767, 46.54989973]), 'previousTarget': array([40.17759251, 46.02912896]), 'currentState': array([52.        , 63.01558062,  0.        ]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 20.0}
episode index:568
target Thresh 31.912612903849077
target distance 10.0
model initialize at round 568
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 11.]), 'previousTarget': array([20., 11.]), 'currentState': array([21.5       , 21.49427038,  0.        ]), 'targetState': array([20, 11], dtype=int32), 'currentDistance': 10.600929718616307}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09895183485013657
{'scaleFactor': 20, 'currentTarget': array([58.04591068, 55.75322333]), 'previousTarget': array([57.55706279, 55.24374964]), 'currentState': array([71.        , 70.99105683,  0.        ]), 'targetState': array([20, 11], dtype=int32), 'currentDistance': 20.0}
episode index:569
target Thresh 31.913482419983957
target distance 9.0
model initialize at round 569
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([19.5, 16.5,  0. ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 10.977249200050183}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09877823513987317
{'scaleFactor': 20, 'currentTarget': array([54.37065177, 52.36247199]), 'previousTarget': array([53.86642313, 51.86700958]), 'currentState': array([69., 66.,  0.]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 20.0}
episode index:570
target Thresh 31.914343284288737
target distance 12.0
model initialize at round 570
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 24.]), 'previousTarget': array([17., 24.]), 'currentState': array([16.5, 12.5,  0. ]), 'targetState': array([17, 24], dtype=int32), 'currentDistance': 11.510864433221254}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09860524348463698
{'scaleFactor': 20, 'currentTarget': array([50.19560398, 49.74352961]), 'previousTarget': array([49.67788791, 49.26640818]), 'currentState': array([66., 62.,  0.]), 'targetState': array([17, 24], dtype=int32), 'currentDistance': 20.0}
episode index:571
target Thresh 31.91519558285056
target distance 8.0
model initialize at round 571
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([3.5       , 7.60233542, 0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 7.6291196260875624}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.09995164308128791
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.49999994,  9.03807253,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.501447482237429}
episode index:572
target Thresh 31.916039400900004
target distance 11.0
model initialize at round 572
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 29.]), 'previousTarget': array([25., 29.]), 'currentState': array([22.5, 18.5,  0. ]), 'targetState': array([25, 29], dtype=int32), 'currentDistance': 10.793516572461353}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09977720740400817
{'scaleFactor': 20, 'currentTarget': array([56.60876612, 55.22855061]), 'previousTarget': array([56.09491953, 54.74525595]), 'currentState': array([72., 68.,  0.]), 'targetState': array([25, 29], dtype=int32), 'currentDistance': 20.0}
episode index:573
target Thresh 31.91687482281957
target distance 13.0
model initialize at round 573
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 17.]), 'previousTarget': array([ 9., 17.]), 'currentState': array([22.5,  9.5,  0. ]), 'targetState': array([ 9, 17], dtype=int32), 'currentDistance': 15.443445211480533}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09960337951654474
{'scaleFactor': 20, 'currentTarget': array([55.35899411, 47.90599608]), 'previousTarget': array([54.8385161 , 47.43677469]), 'currentState': array([72., 59.,  0.]), 'targetState': array([ 9, 17], dtype=int32), 'currentDistance': 20.0}
episode index:574
target Thresh 31.917701932152145
target distance 18.0
model initialize at round 574
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  9.]), 'previousTarget': array([20.,  9.]), 'currentState': array([2.49999845, 3.5       , 0.        ]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 18.343937806268023}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.10066590099090257
{'scaleFactor': 20, 'currentTarget': array([20.,  9.]), 'previousTarget': array([20.,  9.]), 'currentState': array([19.49999094,  9.14097539,  0.        ]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 0.5195027637730038}
episode index:575
target Thresh 31.91852081160935
target distance 3.0
model initialize at round 575
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 20.]), 'previousTarget': array([16., 20.]), 'currentState': array([13.5       , 22.50223836,  0.        ]), 'targetState': array([16, 20], dtype=int32), 'currentDistance': 3.537117021729108}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.10215883520793226
{'scaleFactor': 20, 'currentTarget': array([16., 20.]), 'previousTarget': array([16., 20.]), 'currentState': array([15.5       , 20.51687008,  0.        ]), 'targetState': array([16, 20], dtype=int32), 'currentDistance': 0.7191346751973148}
episode index:576
target Thresh 31.91933154307982
target distance 11.0
model initialize at round 576
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([16.5       , 25.48505199,  0.        ]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 11.49593054966784}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.10198178350046616
{'scaleFactor': 20, 'currentTarget': array([53.31711044, 59.50088309]), 'previousTarget': array([52.83087631, 58.98960338]), 'currentState': array([66.        , 74.96517462,  0.        ]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 20.0}
episode index:577
target Thresh 31.920134207637375
target distance 9.0
model initialize at round 577
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 27.]), 'previousTarget': array([13., 27.]), 'currentState': array([ 4.5, 20.5,  0. ]), 'targetState': array([13, 27], dtype=int32), 'currentDistance': 10.700467279516303}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.10327845475928002
{'scaleFactor': 20, 'currentTarget': array([13., 27.]), 'previousTarget': array([13., 27.]), 'currentState': array([12.49999982, 27.24695235,  0.        ]), 'targetState': array([13, 27], dtype=int32), 'currentDistance': 0.5576608656460187}
episode index:578
target Thresh 31.920928885549138
target distance 15.0
model initialize at round 578
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 19.]), 'previousTarget': array([13., 19.]), 'currentState': array([4.5, 4.5, 0. ]), 'targetState': array([13, 19], dtype=int32), 'currentDistance': 16.807736313971617}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1031000809168633
{'scaleFactor': 20, 'currentTarget': array([38.7887098 , 41.01475172]), 'previousTarget': array([38.27513523, 40.53067021]), 'currentState': array([54.        , 53.99999914,  0.        ]), 'targetState': array([13, 19], dtype=int32), 'currentDistance': 20.0}
episode index:579
target Thresh 31.921715656283563
target distance 15.0
model initialize at round 579
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'previousTarget': array([25.,  3.]), 'currentState': array([10.5,  5.5,  0. ]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 14.713938969562083}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.1042235623070129
{'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'previousTarget': array([25.,  3.]), 'currentState': array([24.4999983 ,  3.06029162,  0.        ]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 0.5036236469062947}
episode index:580
target Thresh 31.92249459851838
target distance 14.0
model initialize at round 580
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([26.5,  3.5,  0. ]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 14.57737973711333}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.10404417579701805
{'scaleFactor': 20, 'currentTarget': array([60., 41.]), 'previousTarget': array([59.48488033, 40.52018607]), 'currentState': array([76., 53.,  0.]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 20.0}
episode index:581
target Thresh 31.92326579014846
target distance 14.0
model initialize at round 581
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 19.]), 'previousTarget': array([ 3., 19.]), 'currentState': array([12.5,  5.5,  0. ]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 16.507574019219227}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.10386540573551115
{'scaleFactor': 20, 'currentTarget': array([44.92721439, 44.58270709]), 'previousTarget': array([44.4019348 , 44.12425104]), 'currentState': array([62., 55.,  0.]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 20.0}
episode index:582
target Thresh 31.92402930829361
target distance 23.0
model initialize at round 582
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.27944074, 23.92499123]), 'previousTarget': array([17.97452223, 23.34140113]), 'currentState': array([10.5,  5.5,  0. ]), 'targetState': array([20, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.10368724895037304
{'scaleFactor': 20, 'currentTarget': array([43.42302652, 43.8105429 ]), 'previousTarget': array([42.89140873, 43.35752738]), 'currentState': array([60., 55.,  0.]), 'targetState': array([20, 28], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:583
target Thresh 31.924785229306277
target distance 17.0
model initialize at round 583
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 29.]), 'previousTarget': array([ 6., 29.]), 'currentState': array([23.5, 21.5,  0. ]), 'targetState': array([ 6, 29], dtype=int32), 'currentDistance': 19.039432764659814}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.10350970229121145
{'scaleFactor': 20, 'currentTarget': array([56.05425823, 60.3772962 ]), 'previousTarget': array([55.53287081, 59.9114908 ]), 'currentState': array([73., 71.,  0.]), 'targetState': array([ 6, 29], dtype=int32), 'currentDistance': 20.0}
episode index:584
target Thresh 31.9255336287792
target distance 14.0
model initialize at round 584
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  3.]), 'previousTarget': array([20.,  3.]), 'currentState': array([6.5, 5.5, 0. ]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 13.729530217745985}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.10464907569892759
{'scaleFactor': 20, 'currentTarget': array([20.,  3.]), 'previousTarget': array([20.,  3.]), 'currentState': array([19.49999997,  2.96766381,  0.        ]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 0.5010445676138165}
episode index:585
target Thresh 31.926274581552942
target distance 19.0
model initialize at round 585
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 28.]), 'previousTarget': array([18., 28.]), 'currentState': array([12.5,  9.5,  0. ]), 'targetState': array([18, 28], dtype=int32), 'currentDistance': 19.300259065618686}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.10447049365848574
{'scaleFactor': 20, 'currentTarget': array([45.65035134, 47.48092935]), 'previousTarget': array([45.12420936, 47.01812381]), 'currentState': array([62., 59.,  0.]), 'targetState': array([18, 28], dtype=int32), 'currentDistance': 20.0}
episode index:586
target Thresh 31.9270081617234
target distance 3.0
model initialize at round 586
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  7.]), 'previousTarget': array([18.,  7.]), 'currentState': array([21.5       ,  8.49860618,  0.        ]), 'targetState': array([18,  7], dtype=int32), 'currentDistance': 3.8073377139808207}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.10429252007474044
{'scaleFactor': 20, 'currentTarget': array([56.58827043, 44.13053163]), 'previousTarget': array([56.08567972, 43.63322525]), 'currentState': array([71.        , 57.99783313,  0.        ]), 'targetState': array([18,  7], dtype=int32), 'currentDistance': 20.0}
episode index:587
target Thresh 31.927734442649207
target distance 16.0
model initialize at round 587
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.64213562,  7.35786438]), 'previousTarget': array([19.14213562,  7.85786438]), 'currentState': array([ 5.5, 21.5,  0. ]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.10537314567561382
{'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([20.5       ,  6.52015668,  0.        ]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 0.721500501278305}
episode index:588
target Thresh 31.928453496959058
target distance 16.0
model initialize at round 588
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3.83685742, 22.10542827]), 'previousTarget': array([ 3.82990784, 22.05153389]), 'currentState': array([17.5,  7.5,  0. ]), 'targetState': array([ 3, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.10519424390027322
{'scaleFactor': 20, 'currentTarget': array([48.80870431, 43.92909059]), 'previousTarget': array([48.30307838, 43.66099697]), 'currentState': array([67.        , 52.24033365,  0.        ]), 'targetState': array([ 3, 23], dtype=int32), 'currentDistance': 20.0}
episode index:589
target Thresh 31.92916539655898
target distance 15.0
model initialize at round 589
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 24.]), 'previousTarget': array([26., 24.]), 'currentState': array([11.5, 15.5,  0. ]), 'targetState': array([26, 24], dtype=int32), 'currentDistance': 16.8077363139715}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.10629513380417722
{'scaleFactor': 20, 'currentTarget': array([26., 24.]), 'previousTarget': array([26., 24.]), 'currentState': array([25.49999967, 24.0298561 ,  0.        ]), 'targetState': array([26, 24], dtype=int32), 'currentDistance': 0.5008909207204288}
episode index:590
target Thresh 31.92987021263953
target distance 12.0
model initialize at round 590
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 24.]), 'previousTarget': array([ 5., 24.]), 'currentState': array([ 8.5, 12.5,  0. ]), 'targetState': array([ 5, 24], dtype=int32), 'currentDistance': 12.020815280171254}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1061152774018013
{'scaleFactor': 20, 'currentTarget': array([39.62974518, 38.90703633]), 'previousTarget': array([39.11164554, 38.59131111]), 'currentState': array([58.        , 46.81486396,  0.        ]), 'targetState': array([ 5, 24], dtype=int32), 'currentDistance': 20.0}
episode index:591
target Thresh 31.9305680156829
target distance 4.0
model initialize at round 591
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 11.]), 'previousTarget': array([24., 11.]), 'currentState': array([20.5       , 14.50081137,  0.        ]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 4.950321225527371}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.10752636671261075
{'scaleFactor': 20, 'currentTarget': array([24., 11.]), 'previousTarget': array([24., 11.]), 'currentState': array([23.5       , 11.53077886,  0.        ]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 0.7291955793920362}
episode index:592
target Thresh 31.931258875469982
target distance 24.0
model initialize at round 592
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.16827891, 23.77622241]), 'previousTarget': array([21.1492875, 23.40285  ]), 'currentState': array([26.5,  4.5,  0. ]), 'targetState': array([20, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1073450406304647
{'scaleFactor': 20, 'currentTarget': array([57.0957872 , 40.81230953]), 'previousTarget': array([56.57148316, 40.47821867]), 'currentState': array([76.        , 47.34153143,  0.        ]), 'targetState': array([20, 28], dtype=int32), 'currentDistance': 20.0}
episode index:593
target Thresh 31.931942861087325
target distance 6.0
model initialize at round 593
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([17.5       ,  6.50372708,  0.        ]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 5.6998976004661195}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.10870223289789428
{'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([22.        ,  7.99025654,  0.        ]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 1.0000474663925547}
episode index:594
target Thresh 31.93262004093406
target distance 22.0
model initialize at round 594
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.66667816,  9.67690622]), 'previousTarget': array([13.56757793,  9.49734288]), 'currentState': array([18.5       , 29.08409561,  0.        ]), 'targetState': array([13,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.10851954006949446
{'scaleFactor': 20, 'currentTarget': array([52.50888782, 39.26314685]), 'previousTarget': array([52.02670697, 38.96105462]), 'currentState': array([68.        , 51.91326319,  0.        ]), 'targetState': array([13,  7], dtype=int32), 'currentDistance': 20.0}
episode index:595
target Thresh 31.933290482728744
target distance 11.0
model initialize at round 595
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([17.5       , 20.77227068,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 12.095115360832803}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.10833746030427718
{'scaleFactor': 20, 'currentTarget': array([50.16662476, 34.48677614]), 'previousTarget': array([49.65953722, 34.12655054]), 'currentState': array([67.        , 45.28665939,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 20.0}
episode index:596
target Thresh 31.933954253516106
target distance 15.0
model initialize at round 596
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 27.]), 'previousTarget': array([22., 27.]), 'currentState': array([ 7.5, 26.5,  0. ]), 'targetState': array([22, 27], dtype=int32), 'currentDistance': 14.508618128546916}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.1094329464919536
{'scaleFactor': 20, 'currentTarget': array([22., 27.]), 'previousTarget': array([22., 27.]), 'currentState': array([21.        , 27.01920212,  0.        ]), 'targetState': array([22, 27], dtype=int32), 'currentDistance': 1.0001843437690532}
episode index:597
target Thresh 31.93461141967378
target distance 18.0
model initialize at round 597
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 11.]), 'previousTarget': array([22.94427191, 11.11145618]), 'currentState': array([14.5, 28.5,  0. ]), 'targetState': array([23, 11], dtype=int32), 'currentDistance': 19.455076458343616}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.10924994825367275
{'scaleFactor': 20, 'currentTarget': array([46.92494088, 25.59110321]), 'previousTarget': array([46.40128179, 25.19876264]), 'currentState': array([64.        , 36.00466917,  0.        ]), 'targetState': array([23, 11], dtype=int32), 'currentDistance': 20.0}
episode index:598
target Thresh 31.935262046918933
target distance 2.0
model initialize at round 598
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 7.]), 'previousTarget': array([7., 7.]), 'currentState': array([9.5       , 5.49991137, 0.        ]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 2.915521549256055}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.10906756102787363
{'scaleFactor': 20, 'currentTarget': array([41.97238453, 28.54699838]), 'previousTarget': array([41.46559857, 28.20399992]), 'currentState': array([59.       , 39.0379617,  0.       ]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 20.0}
episode index:599
target Thresh 31.935906200314825
target distance 10.0
model initialize at round 599
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 15.]), 'previousTarget': array([12., 15.]), 'currentState': array([22.5       , 22.99229485,  0.        ]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 13.1957105500965}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.10888578175949384
{'scaleFactor': 20, 'currentTarget': array([56.33408231, 50.18496812]), 'previousTarget': array([55.83280839, 49.77982091]), 'currentState': array([72.        , 62.61794937,  0.        ]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 20.0}
episode index:600
target Thresh 31.936543944277343
target distance 18.0
model initialize at round 600
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.33564511, 14.71606909]), 'previousTarget': array([17.78704435, 14.27881227]), 'currentState': array([2.5, 2.5, 0. ]), 'targetState': array([20, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.10989883458950846
{'scaleFactor': 20, 'currentTarget': array([20., 16.]), 'previousTarget': array([20., 16.]), 'currentState': array([19.       , 15.9969677,  0.       ]), 'targetState': array([20, 16], dtype=int32), 'currentDistance': 1.0000045973970761}
episode index:601
target Thresh 31.937175342581405
target distance 16.0
model initialize at round 601
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8.55815704, 28.57715376]), 'previousTarget': array([ 8.47772  , 28.6118525]), 'currentState': array([24.5, 16.5,  0. ]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1097162783858714
{'scaleFactor': 20, 'currentTarget': array([56.12472836, 53.15156214]), 'previousTarget': array([55.60842538, 52.78404391]), 'currentState': array([74.        , 62.12232932,  0.        ]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 20.0}
episode index:602
target Thresh 31.93780045836737
target distance 20.0
model initialize at round 602
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.99719013, 26.98782391]), 'previousTarget': array([19.8507125, 26.40285  ]), 'currentState': array([15.5,  7.5,  0. ]), 'targetState': array([20, 27], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.10953432767544707
{'scaleFactor': 20, 'currentTarget': array([46.23800187, 36.68758216]), 'previousTarget': array([45.71449534, 36.39480537]), 'currentState': array([65.        , 43.61487789,  0.        ]), 'targetState': array([20, 27], dtype=int32), 'currentDistance': 20.0}
episode index:603
target Thresh 31.938419354147342
target distance 4.0
model initialize at round 603
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([19.5      ,  9.5799768,  0.       ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 5.006619005656711}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.10935297945081884
{'scaleFactor': 20, 'currentTarget': array([51.77860129, 27.12829237]), 'previousTarget': array([51.26751562, 26.77461681]), 'currentState': array([69.        , 37.29802326,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:604
target Thresh 31.93903209181141
target distance 8.0
model initialize at round 604
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([ 3.5       , 10.50000006,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 7.5166481852216185}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.11062267869635296
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.        , 10.83564773,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.01341584219613}
episode index:605
target Thresh 31.93963873263385
target distance 15.0
model initialize at round 605
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 2.]), 'previousTarget': array([8., 2.]), 'currentState': array([17.5     , 17.251443,  0.      ]), 'targetState': array([8, 2], dtype=int32), 'currentDistance': 17.96820841210952}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.11044013302193655
{'scaleFactor': 20, 'currentTarget': array([52.38467221, 43.460861  ]), 'previousTarget': array([51.89516501, 43.06679294]), 'currentState': array([67.        , 57.11341365,  0.        ]), 'targetState': array([8, 2], dtype=int32), 'currentDistance': 20.0}
episode index:606
target Thresh 31.940239337279255
target distance 11.0
model initialize at round 606
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 15.]), 'previousTarget': array([ 6., 15.]), 'currentState': array([17.5       , 12.49998924,  0.        ]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 11.768604581398318}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1102581888159696
{'scaleFactor': 20, 'currentTarget': array([49.66108353, 40.10051681]), 'previousTarget': array([49.15612968, 39.78168155]), 'currentState': array([67.        , 50.06856453,  0.        ]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 20.0}
episode index:607
target Thresh 31.940833965808586
target distance 23.0
model initialize at round 607
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.31715543, 11.04018618]), 'previousTarget': array([17.82445292, 11.54716286]), 'currentState': array([ 4.5, 25.5,  0. ]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.1111337700877932
{'scaleFactor': 20, 'currentTarget': array([26.,  3.]), 'previousTarget': array([26.,  3.]), 'currentState': array([26.5       ,  3.63779768,  0.        ]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 0.8104232752391026}
episode index:608
target Thresh 31.941422677685193
target distance 13.0
model initialize at round 608
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 29.]), 'previousTarget': array([ 7., 29.]), 'currentState': array([20.5, 20.5,  0. ]), 'targetState': array([ 7, 29], dtype=int32), 'currentDistance': 15.953056133543843}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1109512844226244
{'scaleFactor': 20, 'currentTarget': array([52.62633361, 55.01847822]), 'previousTarget': array([52.10252357, 54.57602632]), 'currentState': array([70.        , 64.92583489,  0.        ]), 'targetState': array([ 7, 29], dtype=int32), 'currentDistance': 20.0}
episode index:609
target Thresh 31.942005531780755
target distance 21.0
model initialize at round 609
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.82831279,  9.95070559]), 'previousTarget': array([21.27466942, 10.37523613]), 'currentState': array([ 5.5, 21.5,  0. ]), 'targetState': array([26,  7], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.11187714961055498
{'scaleFactor': 20, 'currentTarget': array([26.,  7.]), 'previousTarget': array([26.,  7.]), 'currentState': array([25.        ,  6.72770391,  0.        ]), 'targetState': array([26,  7], dtype=int32), 'currentDistance': 1.0364097461775068}
episode index:610
target Thresh 31.942582586381164
target distance 12.0
model initialize at round 610
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 25.]), 'previousTarget': array([18., 25.]), 'currentState': array([ 6.5, 22.5,  0. ]), 'targetState': array([18, 25], dtype=int32), 'currentDistance': 11.768602295939841}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.11301929481286382
{'scaleFactor': 20, 'currentTarget': array([18., 25.]), 'previousTarget': array([18., 25.]), 'currentState': array([17.        , 24.73511003,  0.        ]), 'targetState': array([18, 25], dtype=int32), 'currentDistance': 1.034488616854069}
episode index:611
target Thresh 31.943153899192364
target distance 13.0
model initialize at round 611
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 14.]), 'previousTarget': array([27., 14.]), 'currentState': array([25.5       , 26.63041377,  0.        ]), 'targetState': array([27, 14], dtype=int32), 'currentDistance': 12.71917261532661}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.11283462276251602
{'scaleFactor': 20, 'currentTarget': array([61.63118881, 52.53369668]), 'previousTarget': array([61.14354905, 52.05463661]), 'currentState': array([75.        , 67.40900802,  0.        ]), 'targetState': array([27, 14], dtype=int32), 'currentDistance': 20.0}
episode index:612
target Thresh 31.94371952734611
target distance 22.0
model initialize at round 612
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.5747971 ,  5.87179226]), 'previousTarget': array([11.42229124,  5.3226018 ]), 'currentState': array([15.5       , 25.48282951,  0.        ]), 'targetState': array([11,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.11265055323109266
{'scaleFactor': 20, 'currentTarget': array([51.79299806, 49.39027659]), 'previousTarget': array([51.3300406 , 49.09229531]), 'currentState': array([65.        , 64.40943435,  0.        ]), 'targetState': array([11,  3], dtype=int32), 'currentDistance': 20.0}
episode index:613
target Thresh 31.944279527405694
target distance 9.0
model initialize at round 613
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([13.5, 20.5,  0. ]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 11.335784048754519}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.11386782978062066
{'scaleFactor': 20, 'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([21.        , 13.13133663,  0.        ]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 1.0085877801247836}
episode index:614
target Thresh 31.944833955371582
target distance 21.0
model initialize at round 614
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.81685674, 11.68257132]), 'previousTarget': array([21.45612429, 11.63241055]), 'currentState': array([2.5, 6.5, 0. ]), 'targetState': array([23, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.11478142525912416
{'scaleFactor': 20, 'currentTarget': array([23., 12.]), 'previousTarget': array([23., 12.]), 'currentState': array([22.      , 11.952097,  0.      ]), 'targetState': array([23, 12], dtype=int32), 'currentDistance': 1.00114669122534}
episode index:615
target Thresh 31.945382866687034
target distance 8.0
model initialize at round 615
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 15.]), 'previousTarget': array([11., 15.]), 'currentState': array([7.5, 7.5, 0. ]), 'targetState': array([11, 15], dtype=int32), 'currentDistance': 8.27647267862343}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.11459509177656065
{'scaleFactor': 20, 'currentTarget': array([41.92030441, 41.93860365]), 'previousTarget': array([41.41013554, 41.4527363 ]), 'currentState': array([57.        , 55.07644141,  0.        ]), 'targetState': array([11, 15], dtype=int32), 'currentDistance': 20.0}
episode index:616
target Thresh 31.945926316243643
target distance 5.0
model initialize at round 616
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 27.]), 'previousTarget': array([12., 27.]), 'currentState': array([17.5       , 29.15533458,  0.        ]), 'targetState': array([12, 27], dtype=int32), 'currentDistance': 5.907238537477536}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.11440936229231988
{'scaleFactor': 20, 'currentTarget': array([52.21137045, 63.61133606]), 'previousTarget': array([51.70548192, 63.11898458]), 'currentState': array([67.        , 77.07597256,  0.        ]), 'targetState': array([12, 27], dtype=int32), 'currentDistance': 20.0}
episode index:617
target Thresh 31.946464358386812
target distance 18.0
model initialize at round 617
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 29.]), 'previousTarget': array([ 5., 29.]), 'currentState': array([ 2.5, 11.5,  0. ]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 17.67766952966361}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.11422423387437114
{'scaleFactor': 20, 'currentTarget': array([35.46248452, 49.71850833]), 'previousTarget': array([34.93623409, 49.25838932]), 'currentState': array([52.        , 60.96620062,  0.        ]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 20.0}
episode index:618
target Thresh 31.946997046921208
target distance 16.0
model initialize at round 618
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 19.]), 'previousTarget': array([25.52228  , 18.6118525]), 'currentState': array([10.5,  6.5,  0. ]), 'targetState': array([26, 19], dtype=int32), 'currentDistance': 19.912307751739817}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.1152467667668707
{'scaleFactor': 20, 'currentTarget': array([26., 19.]), 'previousTarget': array([26., 19.]), 'currentState': array([25.        , 18.99678002,  0.        ]), 'targetState': array([26, 19], dtype=int32), 'currentDistance': 1.00000518411017}
episode index:619
target Thresh 31.94752443511613
target distance 14.0
model initialize at round 619
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 11.]), 'previousTarget': array([24., 11.]), 'currentState': array([17.5       , 24.50057748,  0.        ]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 14.983844375950541}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.11506088488498865
{'scaleFactor': 20, 'currentTarget': array([55.19910388, 53.69037857]), 'previousTarget': array([54.72364243, 53.17410593]), 'currentState': array([67.       , 69.8377886,  0.       ]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 20.0}
episode index:620
target Thresh 31.94804657571083
target distance 6.0
model initialize at round 620
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 12.]), 'previousTarget': array([23., 12.]), 'currentState': array([27.5,  6.5,  0. ]), 'targetState': array([23, 12], dtype=int32), 'currentDistance': 7.106335201776003}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.11487560165651041
{'scaleFactor': 20, 'currentTarget': array([61.47933289, 43.2726278 ]), 'previousTarget': array([60.96612627, 42.78954658]), 'currentState': array([77.        , 55.88646513,  0.        ]), 'targetState': array([23, 12], dtype=int32), 'currentDistance': 20.0}
episode index:621
target Thresh 31.948563520919805
target distance 3.0
model initialize at round 621
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  5.]), 'previousTarget': array([10.,  5.]), 'currentState': array([7.5       , 2.49999964, 0.        ]), 'targetState': array([10,  5], dtype=int32), 'currentDistance': 3.535534158813835}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.11623528076960284
{'scaleFactor': 20, 'currentTarget': array([10.,  5.]), 'previousTarget': array([10.,  5.]), 'currentState': array([9.5       , 4.48880014, 0.        ]), 'targetState': array([10,  5], dtype=int32), 'currentDistance': 0.7150701354889459}
episode index:622
target Thresh 31.949075322438013
target distance 18.0
model initialize at round 622
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 22.]), 'previousTarget': array([22., 22.]), 'currentState': array([ 4.5       , 16.50000039,  0.        ]), 'targetState': array([22, 22], dtype=int32), 'currentDistance': 18.343936211682305}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.1172007627147532
{'scaleFactor': 20, 'currentTarget': array([22., 22.]), 'previousTarget': array([22., 22.]), 'currentState': array([21.        , 21.92253994,  0.        ]), 'targetState': array([22, 22], dtype=int32), 'currentDistance': 1.0029955436523146}
episode index:623
target Thresh 31.949582031446024
target distance 7.0
model initialize at round 623
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 26.]), 'previousTarget': array([26., 26.]), 'currentState': array([21.5, 19.5,  0. ]), 'targetState': array([26, 26], dtype=int32), 'currentDistance': 7.90569415042084}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.11701294097963341
{'scaleFactor': 20, 'currentTarget': array([56.1314998 , 53.10768175]), 'previousTarget': array([55.62323956, 52.6173357 ]), 'currentState': array([71.        , 66.48406772,  0.        ]), 'targetState': array([26, 26], dtype=int32), 'currentDistance': 20.0}
episode index:624
target Thresh 31.950083698615163
target distance 11.0
model initialize at round 624
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([25.5       , 15.48777035,  0.        ]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 14.290516701172146}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.116825720274066
{'scaleFactor': 20, 'currentTarget': array([61.20828879, 50.42763888]), 'previousTarget': array([60.71133021, 49.92503627]), 'currentState': array([75.        , 64.91172332,  0.        ]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 20.0}
episode index:625
target Thresh 31.95058037411257
target distance 21.0
model initialize at round 625
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.49405381,  5.98765985]), 'previousTarget': array([26.,  6.]), 'currentState': array([6.5, 5.5, 0. ]), 'targetState': array([27,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.11771853709321331
{'scaleFactor': 20, 'currentTarget': array([27.,  6.]), 'previousTarget': array([27.,  6.]), 'currentState': array([26.        ,  6.08803661,  0.        ]), 'targetState': array([27,  6], dtype=int32), 'currentDistance': 1.0038677427777025}
episode index:626
target Thresh 31.951072107606205
target distance 18.0
model initialize at round 626
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 20.]), 'previousTarget': array([16., 20.]), 'currentState': array([22.5,  2.5,  0. ]), 'targetState': array([16, 20], dtype=int32), 'currentDistance': 18.668154702594503}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.11753078823022572
{'scaleFactor': 20, 'currentTarget': array([54.6346521 , 42.07443873]), 'previousTarget': array([54.10581151, 41.62522321]), 'currentState': array([72.        , 51.99636859,  0.        ]), 'targetState': array([16, 20], dtype=int32), 'currentDistance': 20.0}
episode index:627
target Thresh 31.951558948269824
target distance 1.0
model initialize at round 627
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 7.]), 'previousTarget': array([5., 7.]), 'currentState': array([6.5       , 6.49759153, 0.        ]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 1.5819021067797114}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.11734363729355339
{'scaleFactor': 20, 'currentTarget': array([41.25126368, 40.20242158]), 'previousTarget': array([40.74519434, 39.70938611]), 'currentState': array([56.        , 53.71074408,  0.        ]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 20.0}
episode index:628
target Thresh 31.952040944787907
target distance 18.0
model initialize at round 628
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'previousTarget': array([19.,  3.]), 'currentState': array([23.5       , 21.49728194,  0.        ]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 19.036791723479517}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.11715708143140148
{'scaleFactor': 20, 'currentTarget': array([60.55755956, 55.29883475]), 'previousTarget': array([60.07221264, 54.78742827]), 'currentState': array([73.        , 70.95724067,  0.        ]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 20.0}
episode index:629
target Thresh 31.9525181453605
target distance 13.0
model initialize at round 629
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 10.]), 'previousTarget': array([27., 10.]), 'currentState': array([25.5       , 23.41897699,  0.        ]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 13.502553222325151}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.11697111781008179
{'scaleFactor': 20, 'currentTarget': array([62.85841871, 56.9371898 ]), 'previousTarget': array([62.37747592, 56.42288882]), 'currentState': array([75.        , 72.83001849,  0.        ]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 20.0}
episode index:630
target Thresh 31.95299059770806
target distance 20.0
model initialize at round 630
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 25.]), 'previousTarget': array([10.00124766, 24.97504678]), 'currentState': array([11.5,  5.5,  0. ]), 'targetState': array([10, 25], dtype=int32), 'currentDistance': 19.55760721560796}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1167857436138693
{'scaleFactor': 20, 'currentTarget': array([43.76119081, 44.85896434]), 'previousTarget': array([43.23050358, 44.41133417]), 'currentState': array([61.        , 54.99915457,  0.        ]), 'targetState': array([10, 25], dtype=int32), 'currentDistance': 20.0}
episode index:631
target Thresh 31.953458349076218
target distance 5.0
model initialize at round 631
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([13.5,  4.5,  0. ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 4.743416490252598}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.11660095604486001
{'scaleFactor': 20, 'currentTarget': array([47.99419179, 40.71580199]), 'previousTarget': array([47.48544182, 40.22581706]), 'currentState': array([63.        , 53.93796974,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 20.0}
episode index:632
target Thresh 31.953921446240496
target distance 4.0
model initialize at round 632
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 16.]), 'previousTarget': array([20., 16.]), 'currentState': array([19.5      , 19.5034301,  0.       ]), 'targetState': array([20, 16], dtype=int32), 'currentDistance': 3.538929563249369}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.11641675232283022
{'scaleFactor': 20, 'currentTarget': array([54.46491838, 48.57480785]), 'previousTarget': array([53.96082437, 48.07920003]), 'currentState': array([69.        , 62.31276266,  0.        ]), 'targetState': array([20, 16], dtype=int32), 'currentDistance': 20.0}
episode index:633
target Thresh 31.954379935511003
target distance 5.0
model initialize at round 633
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 23.]), 'previousTarget': array([10., 23.]), 'currentState': array([ 5.5       , 20.63197428,  0.        ]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 5.085031544754431}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.11770326430324686
{'scaleFactor': 20, 'currentTarget': array([10., 23.]), 'previousTarget': array([10., 23.]), 'currentState': array([ 9.        , 23.18272316,  0.        ]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 1.0165568133980016}
episode index:634
target Thresh 31.95483386273704
target distance 19.0
model initialize at round 634
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.83763326, 10.7588817 ]), 'previousTarget': array([15.39288577, 11.30234469]), 'currentState': array([ 3.5, 26.5,  0. ]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.11751790483190316
{'scaleFactor': 20, 'currentTarget': array([38.34782902, 26.90471276]), 'previousTarget': array([37.8403226 , 26.41282054]), 'currentState': array([53.        , 40.51771705,  0.        ]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 20.0}
episode index:635
target Thresh 31.955283273311714
target distance 18.0
model initialize at round 635
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.48355773, 19.21410959]), 'previousTarget': array([21.09400392, 18.64100589]), 'currentState': array([10.5,  2.5,  0. ]), 'targetState': array([22, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1173331282519788
{'scaleFactor': 20, 'currentTarget': array([44.70089233, 39.1138795 ]), 'previousTarget': array([44.18500873, 38.63277779]), 'currentState': array([60.        , 51.99554494,  0.        ]), 'targetState': array([22, 20], dtype=int32), 'currentDistance': 20.0}
episode index:636
target Thresh 31.955728212176457
target distance 15.0
model initialize at round 636
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([16.5,  3.5,  0. ]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 14.577379737113263}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.11714893181830222
{'scaleFactor': 20, 'currentTarget': array([49.50972146, 41.68309063]), 'previousTarget': array([48.98582685, 41.21799149]), 'currentState': array([66.        , 52.99992383,  0.        ]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 20.0}
episode index:637
target Thresh 31.956168723825524
target distance 16.0
model initialize at round 637
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8.83685742, 22.10542827]), 'previousTarget': array([ 8.82990784, 22.05153389]), 'currentState': array([22.5,  7.5,  0. ]), 'targetState': array([ 8, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.11696531280291303
{'scaleFactor': 20, 'currentTarget': array([54.33768515, 47.61689235]), 'previousTarget': array([53.81070763, 47.16785082]), 'currentState': array([72.        , 56.99999601,  0.        ]), 'targetState': array([ 8, 23], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:638
target Thresh 31.956604852310445
target distance 15.0
model initialize at round 638
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 21.]), 'previousTarget': array([13., 21.]), 'currentState': array([14.5,  6.5,  0. ]), 'targetState': array([13, 21], dtype=int32), 'currentDistance': 14.577379737113263}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.11678226849492725
{'scaleFactor': 20, 'currentTarget': array([47.50973152, 44.68314267]), 'previousTarget': array([46.98583645, 44.21804045]), 'currentState': array([64.        , 55.99999052,  0.        ]), 'targetState': array([13, 21], dtype=int32), 'currentDistance': 20.0}
episode index:639
target Thresh 31.957036641244436
target distance 9.0
model initialize at round 639
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 17.]), 'previousTarget': array([17., 17.]), 'currentState': array([14.5       , 25.52700442,  0.        ]), 'targetState': array([17, 17], dtype=int32), 'currentDistance': 8.885932949984083}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.11659979620040392
{'scaleFactor': 20, 'currentTarget': array([50.86593502, 55.89117947]), 'previousTarget': array([50.37630848, 55.38215269]), 'currentState': array([64.        , 70.97416117,  0.        ]), 'targetState': array([17, 17], dtype=int32), 'currentDistance': 20.0}
episode index:640
target Thresh 31.95746413380675
target distance 20.0
model initialize at round 640
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8.7965827 , 16.50018861]), 'previousTarget': array([ 8.23112767, 16.10023299]), 'currentState': array([25.5, 27.5,  0. ]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.11641789324221297
{'scaleFactor': 20, 'currentTarget': array([60.1341168 , 63.62070308]), 'previousTarget': array([59.62879818, 63.12661505]), 'currentState': array([75.        , 76.99999741,  0.        ]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 20.0}
episode index:641
target Thresh 31.957887372746992
target distance 21.0
model initialize at round 641
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.0401881 , 11.91976629]), 'previousTarget': array([15.58396457, 12.45510259]), 'currentState': array([ 3.5, 27.5,  0. ]), 'targetState': array([20,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.11623655695990423
{'scaleFactor': 20, 'currentTarget': array([39.2387707, 27.2898448]), 'previousTarget': array([38.74454444, 26.78437243]), 'currentState': array([53.        , 41.80289301,  0.        ]), 'targetState': array([20,  7], dtype=int32), 'currentDistance': 20.0}
episode index:642
target Thresh 31.95830640038942
target distance 2.0
model initialize at round 642
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 16.]), 'previousTarget': array([20., 16.]), 'currentState': array([20.5       , 17.54565856,  0.        ]), 'targetState': array([20, 16], dtype=int32), 'currentDistance': 1.6245185070645622}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.11605578470957778
{'scaleFactor': 20, 'currentTarget': array([55.78407861, 51.41163986]), 'previousTarget': array([55.28333137, 50.91239503]), 'currentState': array([70.        , 65.47960272,  0.        ]), 'targetState': array([20, 16], dtype=int32), 'currentDistance': 20.0}
episode index:643
target Thresh 31.95872125863714
target distance 13.0
model initialize at round 643
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 14.]), 'previousTarget': array([ 5., 14.]), 'currentState': array([18.5,  8.5,  0. ]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 14.577379737113297}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.11587557386375545
{'scaleFactor': 20, 'currentTarget': array([51.60314357, 46.54822725]), 'previousTarget': array([51.08457307, 46.07486286]), 'currentState': array([68., 58.,  0.]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 20.0}
episode index:644
target Thresh 31.959131988976328
target distance 8.0
model initialize at round 644
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 29.]), 'previousTarget': array([21., 29.]), 'currentState': array([13.5       , 28.50000021,  0.        ]), 'targetState': array([21, 29], dtype=int32), 'currentDistance': 7.516648175309425}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.11705641952132942
{'scaleFactor': 20, 'currentTarget': array([21., 29.]), 'previousTarget': array([21., 29.]), 'currentState': array([20.        , 29.27535614,  0.        ]), 'targetState': array([21, 29], dtype=int32), 'currentDistance': 1.0372179143144844}
episode index:645
target Thresh 31.959538632480353
target distance 16.0
model initialize at round 645
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 23.]), 'previousTarget': array([ 5., 23.]), 'currentState': array([21.5, 28.5,  0. ]), 'targetState': array([ 5, 23], dtype=int32), 'currentDistance': 17.392527130926187}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1168752176335255
{'scaleFactor': 20, 'currentTarget': array([55.63557441, 65.19631201]), 'previousTarget': array([55.12595915, 64.70785914]), 'currentState': array([71., 78.,  0.]), 'targetState': array([ 5, 23], dtype=int32), 'currentDistance': 20.0}
episode index:646
target Thresh 31.95994122981391
target distance 13.0
model initialize at round 646
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 24.]), 'previousTarget': array([15., 24.]), 'currentState': array([15.5, 11.5,  0. ]), 'targetState': array([15, 24], dtype=int32), 'currentDistance': 12.509996003196727}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.11669457587520475
{'scaleFactor': 20, 'currentTarget': array([48.92316193, 49.10313944]), 'previousTarget': array([48.40297093, 48.6304732 ]), 'currentState': array([65.        , 60.99999943,  0.        ]), 'targetState': array([15, 24], dtype=int32), 'currentDistance': 20.0}
episode index:647
target Thresh 31.960339821237064
target distance 2.0
model initialize at round 647
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'previousTarget': array([ 2., 17.]), 'currentState': array([ 4.5       , 17.49077231,  0.        ]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 2.5477161257170393}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.11651449165317512
{'scaleFactor': 20, 'currentTarget': array([39.57022616, 53.05685011]), 'previousTarget': array([39.0674067 , 52.55978854]), 'currentState': array([54.        , 66.90537447,  0.        ]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 20.0}
episode index:648
target Thresh 31.960734446609294
target distance 6.0
model initialize at round 648
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  8.]), 'previousTarget': array([17.,  8.]), 'currentState': array([11.5,  2.5,  0. ]), 'targetState': array([17,  8], dtype=int32), 'currentDistance': 7.77817459305197}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.11772846327621922
{'scaleFactor': 20, 'currentTarget': array([17.,  8.]), 'previousTarget': array([17.,  8.]), 'currentState': array([16.5       ,  7.49999955,  0.        ]), 'targetState': array([17,  8], dtype=int32), 'currentDistance': 0.7071070972874067}
episode index:649
target Thresh 31.961125145393463
target distance 8.0
model initialize at round 649
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 29.]), 'previousTarget': array([21., 29.]), 'currentState': array([13.5, 23.5,  0. ]), 'targetState': array([21, 29], dtype=int32), 'currentDistance': 9.300537618869024}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.1188973749065619
{'scaleFactor': 20, 'currentTarget': array([21., 29.]), 'previousTarget': array([21., 29.]), 'currentState': array([20.        , 29.06703419,  0.        ]), 'targetState': array([21, 29], dtype=int32), 'currentDistance': 1.0022442728431684}
episode index:650
target Thresh 31.961511956659773
target distance 16.0
model initialize at round 650
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([18.5       , 22.50599903,  0.        ]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 16.14577362276285}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.11871473684986979
{'scaleFactor': 20, 'currentTarget': array([56.13197636, 51.94118906]), 'previousTarget': array([55.65464634, 51.42450078]), 'currentState': array([68.        , 68.03932606,  0.        ]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 20.0}
episode index:651
target Thresh 31.96189491908968
target distance 13.0
model initialize at round 651
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  3.]), 'previousTarget': array([13.,  3.]), 'currentState': array([26.5       ,  3.49999997,  0.        ]), 'targetState': array([13,  3], dtype=int32), 'currentDistance': 13.509256085003342}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1185326590326154
{'scaleFactor': 20, 'currentTarget': array([60.33422175, 40.56684147]), 'previousTarget': array([59.82162699, 40.08272739]), 'currentState': array([76.        , 52.99999842,  0.        ]), 'targetState': array([13,  3], dtype=int32), 'currentDistance': 20.0}
episode index:652
target Thresh 31.96227407097974
target distance 12.0
model initialize at round 652
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([16.5, 13.5,  0. ]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 12.020815280171254}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.11835113888095748
{'scaleFactor': 20, 'currentTarget': array([49.7460779 , 51.34624453]), 'previousTarget': array([49.22533058, 50.87523613]), 'currentState': array([66., 63.,  0.]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 20.0}
episode index:653
target Thresh 31.962649450245458
target distance 5.0
model initialize at round 653
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 27.]), 'previousTarget': array([ 8., 27.]), 'currentState': array([ 3.5       , 26.49992913,  0.        ]), 'targetState': array([ 8, 27], dtype=int32), 'currentDistance': 4.527700395890293}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.119581098446014
{'scaleFactor': 20, 'currentTarget': array([ 8., 27.]), 'previousTarget': array([ 8., 27.]), 'currentState': array([ 7.5       , 26.80661403,  0.        ]), 'targetState': array([ 8, 27], dtype=int32), 'currentDistance': 0.5360952665103799}
episode index:654
target Thresh 31.963021094425073
target distance 23.0
model initialize at round 654
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.37767469,  6.70863052]), 'previousTarget': array([21.92481176,  6.73259233]), 'currentState': array([2.5, 4.5, 0. ]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.1203895282626894
{'scaleFactor': 20, 'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([24.       ,  7.0301066,  0.       ]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 1.0004531011523066}
episode index:655
target Thresh 31.963389040683317
target distance 6.0
model initialize at round 655
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  3.]), 'previousTarget': array([10.,  3.]), 'currentState': array([11.5       ,  9.46909681,  0.        ]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 6.640723871108313}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12020600764033773
{'scaleFactor': 20, 'currentTarget': array([47.52509173, 44.15749684]), 'previousTarget': array([47.03151971, 43.65163873]), 'currentState': array([61.       , 58.9367677,  0.       ]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 20.0}
episode index:656
target Thresh 31.963753325815123
target distance 12.0
model initialize at round 656
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 12.]), 'previousTarget': array([18., 12.]), 'currentState': array([13.5       , 23.67213491,  0.        ]), 'targetState': array([18, 12], dtype=int32), 'currentDistance': 12.509545685799912}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12002304568045898
{'scaleFactor': 20, 'currentTarget': array([51.02726203, 56.19292311]), 'previousTarget': array([50.54904175, 55.67666925]), 'currentState': array([63.        , 72.21333339,  0.        ]), 'targetState': array([18, 12], dtype=int32), 'currentDistance': 20.0}
episode index:657
target Thresh 31.964113986249302
target distance 20.0
model initialize at round 657
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 25.]), 'previousTarget': array([20., 25.]), 'currentState': array([20.5,  5.5,  0. ]), 'targetState': array([20, 25], dtype=int32), 'currentDistance': 19.506409203131167}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1198406398359598
{'scaleFactor': 20, 'currentTarget': array([52.85014149, 44.71008489]), 'previousTarget': array([52.31959283, 44.26117148]), 'currentState': array([70., 55.,  0.]), 'targetState': array([20, 25], dtype=int32), 'currentDistance': 20.0}
episode index:658
target Thresh 31.9644710580522
target distance 13.0
model initialize at round 658
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([11.5, 12.5,  0. ]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 12.747548783981868}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.120863058111844
{'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([23.        ,  9.96067722,  0.        ]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 1.0007728418135893}
episode index:659
target Thresh 31.964824576931296
target distance 12.0
model initialize at round 659
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  6.]), 'previousTarget': array([13.,  6.]), 'currentState': array([24.5       , 18.49999982,  0.        ]), 'targetState': array([13,  6], dtype=int32), 'currentDistance': 16.98528761986832}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12067993226622
{'scaleFactor': 20, 'currentTarget': array([59.97330461, 53.74335851]), 'previousTarget': array([59.47425464, 53.24242386]), 'currentState': array([74.        , 67.99999964,  0.        ]), 'targetState': array([13,  6], dtype=int32), 'currentDistance': 20.0}
episode index:660
target Thresh 31.965174578238774
target distance 11.0
model initialize at round 660
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 16.]), 'previousTarget': array([ 2., 16.]), 'currentState': array([13.5, 10.5,  0. ]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 12.747548783982}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12049736050787474
{'scaleFactor': 20, 'currentTarget': array([46.77940648, 48.29989975]), 'previousTarget': array([46.26168033, 47.82451396]), 'currentState': array([63., 60.,  0.]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 20.0}
episode index:661
target Thresh 31.965521096975053
target distance 23.0
model initialize at round 661
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.62809992,  6.67272673]), 'previousTarget': array([19.08352283,  7.11217878]), 'currentState': array([ 3.5, 18.5,  0. ]), 'targetState': array([26,  2], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.12129585789134985
{'scaleFactor': 20, 'currentTarget': array([26.,  2.]), 'previousTarget': array([26.,  2.]), 'currentState': array([25.        ,  1.86966707,  0.        ]), 'targetState': array([26,  2], dtype=int32), 'currentDistance': 1.0084575713070123}
episode index:662
target Thresh 31.965864167792297
target distance 17.0
model initialize at round 662
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6.80649077, 24.07696144]), 'previousTarget': array([ 6.76756726, 23.99675711]), 'currentState': array([20.5,  9.5,  0. ]), 'targetState': array([ 5, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12111290787944735
{'scaleFactor': 20, 'currentTarget': array([52.16666461, 49.9461528 ]), 'previousTarget': array([51.63923302, 49.50038873]), 'currentState': array([70., 59.,  0.]), 'targetState': array([ 5, 26], dtype=int32), 'currentDistance': 20.0}
episode index:663
target Thresh 31.966203824997876
target distance 12.0
model initialize at round 663
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 18.]), 'previousTarget': array([17., 18.]), 'currentState': array([14.5,  6.5,  0. ]), 'targetState': array([17, 18], dtype=int32), 'currentDistance': 11.768602295939807}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12093050892179759
{'scaleFactor': 20, 'currentTarget': array([48.4473992, 43.4255557]), 'previousTarget': array([47.93173894, 42.94494967]), 'currentState': array([64.        , 55.99999836,  0.        ]), 'targetState': array([17, 18], dtype=int32), 'currentDistance': 20.0}
episode index:664
target Thresh 31.96654010255779
target distance 11.0
model initialize at round 664
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([ 3.5       , 20.50011328,  0.        ]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 10.79349033536859}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.12199101736527697
{'scaleFactor': 20, 'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([13.        , 22.48095511,  0.        ]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 1.126679899944017}
episode index:665
target Thresh 31.966873034100075
target distance 16.0
model initialize at round 665
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8.09203666, 25.93585323]), 'previousTarget': array([ 8., 26.]), 'currentState': array([24.5, 14.5,  0. ]), 'targetState': array([ 8, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1218078476695333
{'scaleFactor': 20, 'currentTarget': array([56.66755086, 54.0207111 ]), 'previousTarget': array([56.14329649, 53.56295601]), 'currentState': array([74., 64.,  0.]), 'targetState': array([ 8, 26], dtype=int32), 'currentDistance': 20.0}
episode index:666
target Thresh 31.967202652918164
target distance 22.0
model initialize at round 666
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.17544467,  7.52633401]), 'previousTarget': array([20.93592685,  6.9414844 ]), 'currentState': array([27.5       , 26.49999997,  0.        ]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12162522720825965
{'scaleFactor': 20, 'currentTarget': array([64.58594111, 60.31908348]), 'previousTarget': array([64.09999961, 59.80796408]), 'currentState': array([77.        , 75.99999997,  0.        ]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 20.0}
episode index:667
target Thresh 31.967528991974216
target distance 23.0
model initialize at round 667
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.80485487,  7.59788126]), 'previousTarget': array([10.5704125 ,  8.11006407]), 'currentState': array([ 4.5       , 26.57810268,  0.        ]), 'targetState': array([12,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1214431535148341
{'scaleFactor': 20, 'currentTarget': array([43.804415  , 57.67324207]), 'previousTarget': array([43.34134734, 57.15141108]), 'currentState': array([54.      , 74.879347,  0.      ]), 'targetState': array([12,  4], dtype=int32), 'currentDistance': 20.0}
episode index:668
target Thresh 31.967852083902404
target distance 18.0
model initialize at round 668
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([22.5,  6.5,  0. ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 18.828170383762814}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12126162413738294
{'scaleFactor': 20, 'currentTarget': array([55.43433174, 44.79381265]), 'previousTarget': array([54.91590918, 44.32109434]), 'currentState': array([72., 56.,  0.]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:669
target Thresh 31.968171961012192
target distance 13.0
model initialize at round 669
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 5.]), 'previousTarget': array([7., 5.]), 'currentState': array([13.5       , 18.49993637,  0.        ]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 14.983266734900567}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12108063663867043
{'scaleFactor': 20, 'currentTarget': array([49.71270898, 53.05170541]), 'previousTarget': array([49.22013633, 52.54510661]), 'currentState': array([63.        , 67.99987912,  0.        ]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 20.0}
episode index:670
target Thresh 31.96848865529156
target distance 9.0
model initialize at round 670
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([12.5       , 15.49999622,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 12.10371609162212}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12090018859598983
{'scaleFactor': 20, 'currentTarget': array([47.61617131, 51.10375317]), 'previousTarget': array([47.11408869, 50.60590919]), 'currentState': array([62.        , 64.99999267,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 20.0}
episode index:671
target Thresh 31.968802198410195
target distance 24.0
model initialize at round 671
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.94231065, 21.97376358]), 'previousTarget': array([10.92091492, 21.57960839]), 'currentState': array([15.5,  2.5,  0. ]), 'targetState': array([10, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12072027760105533
{'scaleFactor': 20, 'currentTarget': array([46.91855467, 43.45240766]), 'previousTarget': array([46.38483801, 43.02409852]), 'currentState': array([65., 52.,  0.]), 'targetState': array([10, 26], dtype=int32), 'currentDistance': 20.0}
episode index:672
target Thresh 31.969112621722672
target distance 15.0
model initialize at round 672
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([10.5, 10.5,  0. ]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 14.916433890176238}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12054090125989478
{'scaleFactor': 20, 'currentTarget': array([43.3107078 , 48.97876931]), 'previousTarget': array([42.78589041, 48.51644227]), 'currentState': array([60., 60.,  0.]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 20.0}
episode index:673
target Thresh 31.969419956271587
target distance 22.0
model initialize at round 673
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.28779003, 22.64422855]), 'previousTarget': array([22.06407315, 22.0585156 ]), 'currentState': array([16.5,  3.5,  0. ]), 'targetState': array([23, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1203620571927436
{'scaleFactor': 20, 'currentTarget': array([49.24003831, 42.08653657]), 'previousTarget': array([48.70859686, 41.63497444]), 'currentState': array([66., 53.,  0.]), 'targetState': array([23, 25], dtype=int32), 'currentDistance': 20.0}
episode index:674
target Thresh 31.969724232790643
target distance 7.0
model initialize at round 674
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 12.]), 'previousTarget': array([17., 12.]), 'currentState': array([10.5, 13.5,  0. ]), 'targetState': array([17, 12], dtype=int32), 'currentDistance': 6.670832032063071}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.12151017007728578
{'scaleFactor': 20, 'currentTarget': array([17., 12.]), 'previousTarget': array([17., 12.]), 'currentState': array([16.        , 11.75512612,  0.        ]), 'targetState': array([17, 12], dtype=int32), 'currentDistance': 1.029545151130751}
episode index:675
target Thresh 31.970025481707747
target distance 13.0
model initialize at round 675
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 20.]), 'previousTarget': array([19., 20.]), 'currentState': array([ 6.5, 20.5,  0. ]), 'targetState': array([19, 20], dtype=int32), 'currentDistance': 12.509996003196804}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.12250440693167389
{'scaleFactor': 20, 'currentTarget': array([19., 20.]), 'previousTarget': array([19., 20.]), 'currentState': array([18.        , 19.73268669,  0.        ]), 'targetState': array([19, 20], dtype=int32), 'currentDistance': 1.035111783421096}
episode index:676
target Thresh 31.970323733148042
target distance 17.0
model initialize at round 676
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4.35014149, 23.78991511]), 'previousTarget': array([ 4.20859686, 23.86502556]), 'currentState': array([21.5, 13.5,  0. ]), 'targetState': array([ 4, 24], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12232345507505399
{'scaleFactor': 20, 'currentTarget': array([53.7150725 , 52.93862429]), 'previousTarget': array([53.19147429, 52.47927459]), 'currentState': array([71., 63.,  0.]), 'targetState': array([ 4, 24], dtype=int32), 'currentDistance': 20.0}
episode index:677
target Thresh 31.970619016936922
target distance 11.0
model initialize at round 677
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 20.]), 'previousTarget': array([18., 20.]), 'currentState': array([ 7.5, 13.5,  0. ]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 12.34908903522843}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.12336157479298987
{'scaleFactor': 20, 'currentTarget': array([18., 20.]), 'previousTarget': array([18., 20.]), 'currentState': array([17.        , 19.60837086,  0.        ]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 1.073952225078436}
episode index:678
target Thresh 31.970911362603015
target distance 11.0
model initialize at round 678
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 28.]), 'previousTarget': array([ 2., 28.]), 'currentState': array([13.5, 22.5,  0. ]), 'targetState': array([ 2, 28], dtype=int32), 'currentDistance': 12.747548783982}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12317989353409003
{'scaleFactor': 20, 'currentTarget': array([46.77940648, 60.29989975]), 'previousTarget': array([46.26168033, 59.82451396]), 'currentState': array([63., 72.,  0.]), 'targetState': array([ 2, 28], dtype=int32), 'currentDistance': 20.0}
episode index:679
target Thresh 31.971200799381123
target distance 14.0
model initialize at round 679
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 14.]), 'previousTarget': array([17., 14.]), 'currentState': array([27.5       , 28.49999997,  0.        ]), 'targetState': array([17, 14], dtype=int32), 'currentDistance': 17.902513765830093}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12299874663183402
{'scaleFactor': 20, 'currentTarget': array([63.32117742, 63.40925586]), 'previousTarget': array([62.82500045, 62.90567271]), 'currentState': array([77.        , 77.99999994,  0.        ]), 'targetState': array([17, 14], dtype=int32), 'currentDistance': 20.0}
episode index:680
target Thresh 31.971487356215174
target distance 11.0
model initialize at round 680
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([26.5       ,  6.49999988,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 12.02081524546221}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1228181317322278
{'scaleFactor': 20, 'currentTarget': array([60.90256248, 42.88255411]), 'previousTarget': array([60.39445942, 42.3918861 ]), 'currentState': array([76.        , 55.99999976,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:681
target Thresh 31.971771061761082
target distance 22.0
model initialize at round 681
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.93304501, 24.13569284]), 'previousTarget': array([13.57704261, 23.55791146]), 'currentState': array([4.5, 6.5, 0. ]), 'targetState': array([16, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12263804649508378
{'scaleFactor': 20, 'currentTarget': array([37.89888325, 44.13601924]), 'previousTarget': array([37.37190008, 43.67272673]), 'currentState': array([54., 56.,  0.]), 'targetState': array([16, 28], dtype=int32), 'currentDistance': 20.0}
episode index:682
target Thresh 31.972051944389644
target distance 22.0
model initialize at round 682
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.75093819,  5.07201987]), 'previousTarget': array([12.56757793,  4.49734288]), 'currentState': array([17.5, 24.5,  0. ]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12245848859391967
{'scaleFactor': 20, 'currentTarget': array([54.85919294, 58.10657985]), 'previousTarget': array([54.37578466, 57.59391933]), 'currentState': array([67., 74.,  0.]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 20.0}
episode index:683
target Thresh 31.972330032189355
target distance 23.0
model initialize at round 683
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7.23982579, 7.34119876]), 'previousTarget': array([6.65859887, 7.02547777]), 'currentState': array([25.5, 15.5,  0. ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12227945571585838
{'scaleFactor': 20, 'currentTarget': array([59.54918316, 52.30069849]), 'previousTarget': array([59.0398733 , 51.81203395]), 'currentState': array([75., 65.,  0.]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:684
target Thresh 31.972605352969225
target distance 10.0
model initialize at round 684
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([20.5       , 20.49999905,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 11.853268746334958}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12210094556152866
{'scaleFactor': 20, 'currentTarget': array([56.48550724, 55.25691582]), 'previousTarget': array([55.99112035, 54.75177243]), 'currentState': array([70.        , 69.99999845,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 20.0}
episode index:685
target Thresh 31.972877934261565
target distance 14.0
model initialize at round 685
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([ 8.5, 20.5,  0. ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 15.44344521148039}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12192295584496667
{'scaleFactor': 20, 'currentTarget': array([45.32846616, 42.81388778]), 'previousTarget': array([44.84500037, 42.30036245]), 'currentState': array([58.        , 58.28748563,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 20.0}
episode index:686
target Thresh 31.97314780332473
target distance 10.0
model initialize at round 686
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([5.5, 6.5, 0. ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 9.823441352194257}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.12297247584138438
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.        ,  3.36901453,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.1824308274123885}
episode index:687
target Thresh 31.973414987145848
target distance 4.0
model initialize at round 687
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 14.]), 'previousTarget': array([15., 14.]), 'currentState': array([11.5, 16.5,  0. ]), 'targetState': array([15, 14], dtype=int32), 'currentDistance': 4.301162633521202}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.12417598975716725
{'scaleFactor': 20, 'currentTarget': array([15., 14.]), 'previousTarget': array([15., 14.]), 'currentState': array([14.        , 14.00005859,  0.        ]), 'targetState': array([15, 14], dtype=int32), 'currentDistance': 1.0000000017159945}
episode index:688
target Thresh 31.97367951244353
target distance 5.0
model initialize at round 688
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  3.]), 'previousTarget': array([12.,  3.]), 'currentState': array([17.5       ,  2.49999997,  0.        ]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 5.5226805112918855}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12399576335693914
{'scaleFactor': 20, 'currentTarget': array([52.06680305, 38.6958786 ]), 'previousTarget': array([51.55937844, 38.20421703]), 'currentState': array([67.        , 51.99999934,  0.        ]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 20.0}
episode index:689
target Thresh 31.973941405670516
target distance 4.0
model initialize at round 689
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 24.]), 'previousTarget': array([17., 24.]), 'currentState': array([16.5, 20.5,  0. ]), 'targetState': array([17, 24], dtype=int32), 'currentDistance': 3.5355339059326485}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.123816059352074
{'scaleFactor': 20, 'currentTarget': array([51.41851008, 56.31111595]), 'previousTarget': array([50.91391655, 55.81601073]), 'currentState': array([66.        , 69.99980295,  0.        ]), 'targetState': array([17, 24], dtype=int32), 'currentDistance': 20.0}
episode index:690
target Thresh 31.974200693016357
target distance 3.0
model initialize at round 690
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([7.5, 8.5, 0. ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 2.549509756796313}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12363687547457462
{'scaleFactor': 20, 'currentTarget': array([42.42742106, 44.3016513 ]), 'previousTarget': array([41.92301274, 43.80634254]), 'currentState': array([57.        , 57.99982429,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 20.0}
episode index:691
target Thresh 31.974457400410003
target distance 21.0
model initialize at round 691
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.21472851, 25.13407074]), 'previousTarget': array([21.64677133, 25.25775784]), 'currentState': array([ 2.5, 28.5,  0. ]), 'targetState': array([23, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.12443469653466957
{'scaleFactor': 20, 'currentTarget': array([23., 25.]), 'previousTarget': array([23., 25.]), 'currentState': array([22.        , 24.32499999,  0.        ]), 'targetState': array([23, 25], dtype=int32), 'currentDistance': 1.2064928578692993}
episode index:692
target Thresh 31.9747115535224
target distance 8.0
model initialize at round 692
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 5.]), 'previousTarget': array([6., 5.]), 'currentState': array([10.5       , 13.49998266,  0.        ]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 9.617676701580512}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1242551370880106
{'scaleFactor': 20, 'currentTarget': array([46.37162583, 48.36209515]), 'previousTarget': array([45.87633019, 47.85771664]), 'currentState': array([60.        , 62.99997127,  0.        ]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 20.0}
episode index:693
target Thresh 31.974963177769077
target distance 17.0
model initialize at round 693
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.93335932, 20.95377336]), 'previousTarget': array([26.79140314, 20.86502556]), 'currentState': array([10.5       ,  9.55444437,  0.        ]), 'targetState': array([27, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.12513128727902845
{'scaleFactor': 20, 'currentTarget': array([27., 21.]), 'previousTarget': array([27., 21.]), 'currentState': array([26.        , 20.42209172,  0.        ]), 'targetState': array([27, 21], dtype=int32), 'currentDistance': 1.1549796436352433}
episode index:694
target Thresh 31.97521229831267
target distance 11.0
model initialize at round 694
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([18.5       , 24.49810204,  0.        ]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 11.595531488531678}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12495124226136078
{'scaleFactor': 20, 'currentTarget': array([55.63194357, 58.2807843 ]), 'previousTarget': array([55.14906009, 57.76733021]), 'currentState': array([68.        , 73.99800989,  0.        ]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 20.0}
episode index:695
target Thresh 31.975458940065433
target distance 3.0
model initialize at round 695
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 26.]), 'previousTarget': array([ 7., 26.]), 'currentState': array([ 4.5       , 24.50011742,  0.        ]), 'targetState': array([ 7, 26], dtype=int32), 'currentDistance': 2.915415536478024}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.12615188129546803
{'scaleFactor': 20, 'currentTarget': array([ 7., 26.]), 'previousTarget': array([ 7., 26.]), 'currentState': array([ 6.5       , 25.60334232,  0.        ]), 'targetState': array([ 7, 26], dtype=int32), 'currentDistance': 0.6382298265806482}
episode index:696
target Thresh 31.975703127691755
target distance 11.0
model initialize at round 696
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 10.]), 'previousTarget': array([20., 10.]), 'currentState': array([19.5       , 21.49994373,  0.        ]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 11.510808219544026}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12597088863937697
{'scaleFactor': 20, 'currentTarget': array([56.47494415, 55.40753914]), 'previousTarget': array([55.99037042, 54.89516008]), 'currentState': array([69.        , 70.99994037,  0.        ]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 20.0}
episode index:697
target Thresh 31.975944885610595
target distance 11.0
model initialize at round 697
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  3.]), 'previousTarget': array([21.,  3.]), 'currentState': array([10.5,  4.5,  0. ]), 'targetState': array([21,  3], dtype=int32), 'currentDistance': 10.606601717798135}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.12697403725713657
{'scaleFactor': 20, 'currentTarget': array([21.,  3.]), 'previousTarget': array([21.,  3.]), 'currentState': array([20.       ,  2.1715745,  0.       ]), 'targetState': array([21,  3], dtype=int32), 'currentDistance': 1.2985718323499238}
episode index:698
target Thresh 31.976184237997952
target distance 11.0
model initialize at round 698
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 17.]), 'previousTarget': array([ 4., 17.]), 'currentState': array([6.5, 6.5, 0. ]), 'targetState': array([ 4, 17], dtype=int32), 'currentDistance': 10.793516572461453}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1267923862739361
{'scaleFactor': 20, 'currentTarget': array([40., 44.]), 'previousTarget': array([39.48135685, 43.52489784]), 'currentState': array([56., 56.,  0.]), 'targetState': array([ 4, 17], dtype=int32), 'currentDistance': 20.0}
episode index:699
target Thresh 31.976421208789265
target distance 9.0
model initialize at round 699
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  7.]), 'previousTarget': array([17.,  7.]), 'currentState': array([26.5, 16.5,  0. ]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 13.435028842544517}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12661125429354478
{'scaleFactor': 20, 'currentTarget': array([61.85786438, 51.85786438]), 'previousTarget': array([61.35786438, 51.35786438]), 'currentState': array([76., 66.,  0.]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 20.0}
episode index:700
target Thresh 31.976655821681803
target distance 15.0
model initialize at round 700
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 25.]), 'previousTarget': array([ 3., 25.]), 'currentState': array([14.5, 10.5,  0. ]), 'targetState': array([ 3, 25], dtype=int32), 'currentDistance': 18.506755523321736}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12643063909483787
{'scaleFactor': 20, 'currentTarget': array([46.65266463, 50.04661085]), 'previousTarget': array([46.12630809, 49.59268808]), 'currentState': array([64., 60.,  0.]), 'targetState': array([ 3, 25], dtype=int32), 'currentDistance': 20.0}
episode index:701
target Thresh 31.97688810013706
target distance 23.0
model initialize at round 701
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5.61222174, 5.61572109]), 'previousTarget': array([5.16799178, 5.58678368]), 'currentState': array([25.5,  3.5,  0. ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12625053846934664
{'scaleFactor': 20, 'currentTarget': array([58.18391905, 42.17320816]), 'previousTarget': array([57.66512284, 41.7024581 ]), 'currentState': array([75., 53.,  0.]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 20.0}
episode index:702
target Thresh 31.977118067383067
target distance 14.0
model initialize at round 702
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 16.]), 'previousTarget': array([26., 16.]), 'currentState': array([26.5,  2.5,  0. ]), 'targetState': array([26, 16], dtype=int32), 'currentDistance': 13.509256086106305}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12607095022116832
{'scaleFactor': 20, 'currentTarget': array([59.76931317, 40.31390548]), 'previousTarget': array([59.24754484, 39.84419882]), 'currentState': array([76., 52.,  0.]), 'targetState': array([26, 16], dtype=int32), 'currentDistance': 20.0}
episode index:703
target Thresh 31.97734574641675
target distance 11.0
model initialize at round 703
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 16.]), 'previousTarget': array([13., 16.]), 'currentState': array([24.5, 17.5,  0. ]), 'targetState': array([13, 16], dtype=int32), 'currentDistance': 11.597413504743294}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1258918721668769
{'scaleFactor': 20, 'currentTarget': array([58.65621216, 54.17158721]), 'previousTarget': array([58.14598052, 53.68383498]), 'currentState': array([74., 67.,  0.]), 'targetState': array([13, 16], dtype=int32), 'currentDistance': 20.0}
episode index:704
target Thresh 31.97757116000619
target distance 23.0
model initialize at round 704
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.77400456,  6.54059105]), 'previousTarget': array([19.73259233,  6.07518824]), 'currentState': array([18.5       , 26.49997261,  0.        ]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12571330213543452
{'scaleFactor': 20, 'currentTarget': array([57.01187165, 59.28886552]), 'previousTarget': array([56.53947175, 58.77075012]), 'currentState': array([68.        , 75.99997067,  0.        ]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 20.0}
episode index:705
target Thresh 31.97779433069294
target distance 7.0
model initialize at round 705
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 26.]), 'previousTarget': array([11., 26.]), 'currentState': array([11.5, 19.5,  0. ]), 'targetState': array([11, 26], dtype=int32), 'currentDistance': 6.519202405202577}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12553523796810387
{'scaleFactor': 20, 'currentTarget': array([45.83629599, 55.95921416]), 'previousTarget': array([45.32569258, 55.47155385]), 'currentState': array([61.        , 68.99999943,  0.        ]), 'targetState': array([11, 26], dtype=int32), 'currentDistance': 20.0}
episode index:706
target Thresh 31.978015280794256
target distance 4.0
model initialize at round 706
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 19.]), 'previousTarget': array([23., 19.]), 'currentState': array([23.5, 15.5,  0. ]), 'targetState': array([23, 19], dtype=int32), 'currentDistance': 3.5355339059326703}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12535767751836116
{'scaleFactor': 20, 'currentTarget': array([58.28137021, 51.45881462]), 'previousTarget': array([57.77544283, 50.96526027]), 'currentState': array([73.        , 64.99993485,  0.        ]), 'targetState': array([23, 19], dtype=int32), 'currentDistance': 20.0}
episode index:707
target Thresh 31.978234032405332
target distance 4.0
model initialize at round 707
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.5       , 14.43367979,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 4.461784001281202}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1251806186518098
{'scaleFactor': 20, 'currentTarget': array([50.39864405, 49.23967271]), 'previousTarget': array([49.90398951, 48.73471608]), 'currentState': array([64.        , 63.90265727,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 20.0}
episode index:708
target Thresh 31.97845060740151
target distance 9.0
model initialize at round 708
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 13.]), 'previousTarget': array([17., 13.]), 'currentState': array([26.5       , 15.49999994,  0.        ]), 'targetState': array([17, 13], dtype=int32), 'currentDistance': 9.82344133702535}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12500405924609498
{'scaleFactor': 20, 'currentTarget': array([60.99582379, 51.7759802 ]), 'previousTarget': array([60.48827521, 51.28454988]), 'currentState': array([76.        , 64.99999988,  0.        ]), 'targetState': array([17, 13], dtype=int32), 'currentDistance': 20.0}
episode index:709
target Thresh 31.978665027440467
target distance 13.0
model initialize at round 709
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 12.]), 'previousTarget': array([26., 12.]), 'currentState': array([17.5, 24.5,  0. ]), 'targetState': array([26, 12], dtype=int32), 'currentDistance': 15.11621645783086}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12482799719081879
{'scaleFactor': 20, 'currentTarget': array([53.87441893, 44.04697442]), 'previousTarget': array([53.38642334, 43.53654138]), 'currentState': array([67.        , 59.13733961,  0.        ]), 'targetState': array([26, 12], dtype=int32), 'currentDistance': 20.0}
episode index:710
target Thresh 31.978877313964393
target distance 14.0
model initialize at round 710
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([16.5, 25.5,  0. ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 17.902513789968268}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12465243038745617
{'scaleFactor': 20, 'currentTarget': array([52.32117742, 60.40925592]), 'previousTarget': array([51.82500046, 59.90567276]), 'currentState': array([66., 75.,  0.]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 20.0}
episode index:711
target Thresh 31.97908748820211
target distance 21.0
model initialize at round 711
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.37163063, 25.28245704]), 'previousTarget': array([21.11990655, 24.6897547 ]), 'currentState': array([14.5,  6.5,  0. ]), 'targetState': array([22, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12447735674927154
{'scaleFactor': 20, 'currentTarget': array([47.54206199, 44.63618566]), 'previousTarget': array([47.01336798, 44.17785512]), 'currentState': array([64., 56.,  0.]), 'targetState': array([22, 27], dtype=int32), 'currentDistance': 20.0}
episode index:712
target Thresh 31.97929557117122
target distance 8.0
model initialize at round 712
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 15.]), 'previousTarget': array([17., 15.]), 'currentState': array([16.5,  7.5,  0. ]), 'targetState': array([17, 15], dtype=int32), 'currentDistance': 7.516648189186453}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12430277420123609
{'scaleFactor': 20, 'currentTarget': array([50.81486777, 43.98417155]), 'previousTarget': array([50.30381478, 43.4970778 ]), 'currentState': array([66.        , 56.99999881,  0.        ]), 'targetState': array([17, 15], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:713
target Thresh 31.979501583680197
target distance 13.0
model initialize at round 713
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 27.]), 'previousTarget': array([25., 27.]), 'currentState': array([13.5, 14.5,  0. ]), 'targetState': array([25, 27], dtype=int32), 'currentDistance': 16.985287751462916}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.12522907023289714
{'scaleFactor': 20, 'currentTarget': array([25., 27.]), 'previousTarget': array([25., 27.]), 'currentState': array([25.5       , 26.49965277,  0.        ]), 'targetState': array([25, 27], dtype=int32), 'currentDistance': 0.7073523502646775}
episode index:714
target Thresh 31.979705546330457
target distance 13.0
model initialize at round 714
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 20.]), 'previousTarget': array([23., 20.]), 'currentState': array([10.5, 20.5,  0. ]), 'targetState': array([23, 20], dtype=int32), 'currentDistance': 12.50999600319672}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.1261638747271779
{'scaleFactor': 20, 'currentTarget': array([23., 20.]), 'previousTarget': array([23., 20.]), 'currentState': array([22.        , 19.63995183,  0.        ]), 'targetState': array([23, 20], dtype=int32), 'currentDistance': 1.0628427391739965}
episode index:715
target Thresh 31.97990747951844
target distance 16.0
model initialize at round 715
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 26.]), 'previousTarget': array([11., 26.]), 'currentState': array([13.5, 10.5,  0. ]), 'targetState': array([11, 26], dtype=int32), 'currentDistance': 15.700318468107515}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12598766819822935
{'scaleFactor': 20, 'currentTarget': array([46.26060772, 49.05501274]), 'previousTarget': array([45.73484744, 48.59451241]), 'currentState': array([63., 60.,  0.]), 'targetState': array([11, 26], dtype=int32), 'currentDistance': 20.0}
episode index:716
target Thresh 31.98010740343763
target distance 13.0
model initialize at round 716
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([21.5, 16.5,  0. ]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 13.50925608610637}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12581195317982177
{'scaleFactor': 20, 'currentTarget': array([55.21295552, 53.7211868 ]), 'previousTarget': array([54.69935236, 53.23869665]), 'currentState': array([71.       , 65.9999989,  0.       ]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 20.0}
episode index:717
target Thresh 31.980305338080587
target distance 8.0
model initialize at round 717
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 29.]), 'previousTarget': array([ 9., 29.]), 'currentState': array([12.5, 21.5,  0. ]), 'targetState': array([ 9, 29], dtype=int32), 'currentDistance': 8.276472678623387}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12563672761828998
{'scaleFactor': 20, 'currentTarget': array([46.32508178, 58.57836331]), 'previousTarget': array([45.80999692, 58.09742284]), 'currentState': array([62.       , 70.9999952,  0.       ]), 'targetState': array([ 9, 29], dtype=int32), 'currentDistance': 20.0}
episode index:718
target Thresh 31.98050130324094
target distance 12.0
model initialize at round 718
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 14.]), 'previousTarget': array([27., 14.]), 'currentState': array([26.5       , 26.45388657,  0.        ]), 'targetState': array([27, 14], dtype=int32), 'currentDistance': 12.463919554547}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1254619894713939
{'scaleFactor': 20, 'currentTarget': array([63.59266487, 60.26305423]), 'previousTarget': array([63.10909717, 59.75007082]), 'currentState': array([76.        , 75.94929135,  0.        ]), 'targetState': array([27, 14], dtype=int32), 'currentDistance': 20.0}
episode index:719
target Thresh 31.980695318515366
target distance 14.0
model initialize at round 719
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  7.]), 'previousTarget': array([19.,  7.]), 'currentState': array([5.5       , 1.50000036, 0.        ]), 'targetState': array([19,  7], dtype=int32), 'currentDistance': 14.57737960218138}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.1263680441518491
{'scaleFactor': 20, 'currentTarget': array([19.,  7.]), 'previousTarget': array([19.,  7.]), 'currentState': array([18.        ,  6.49720346,  0.        ]), 'targetState': array([19,  7], dtype=int32), 'currentDistance': 1.119287436012639}
episode index:720
target Thresh 31.980887403305555
target distance 18.0
model initialize at round 720
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.48355773,  2.78589041]), 'previousTarget': array([23.09400392,  3.35899411]), 'currentState': array([12.5, 19.5,  0. ]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12619277640683962
{'scaleFactor': 20, 'currentTarget': array([49.16455657, 32.07076701]), 'previousTarget': array([48.68096484, 31.5570509 ]), 'currentState': array([62.        , 47.40867404,  0.        ]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 20.0}
episode index:721
target Thresh 31.98107757682015
target distance 17.0
model initialize at round 721
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 11.]), 'previousTarget': array([27., 11.]), 'currentState': array([10.5, 18.5,  0. ]), 'targetState': array([27, 11], dtype=int32), 'currentDistance': 18.124568960391755}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.1270322647631382
{'scaleFactor': 20, 'currentTarget': array([27., 11.]), 'previousTarget': array([27., 11.]), 'currentState': array([26.        , 10.53557449,  0.        ]), 'targetState': array([27, 11], dtype=int32), 'currentDistance': 1.1025838093461136}
episode index:722
target Thresh 31.98126585807666
target distance 20.0
model initialize at round 722
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6.42407644, 7.08785147]), 'previousTarget': array([5.85014149, 6.71008489]), 'currentState': array([23.5       , 17.49999988,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12685656315212418
{'scaleFactor': 20, 'currentTarget': array([58.02819318, 53.73893681]), 'previousTarget': array([57.52208254, 53.24584314]), 'currentState': array([73.        , 66.99959293,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 20.0}
episode index:723
target Thresh 31.981452265903364
target distance 9.0
model initialize at round 723
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 23.]), 'previousTarget': array([ 4., 23.]), 'currentState': array([13.5       , 25.49988955,  0.        ]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 9.823413244650034}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1266813469046765
{'scaleFactor': 20, 'currentTarget': array([47.99576378, 61.77557256]), 'previousTarget': array([47.48821478, 61.28414371]), 'currentState': array([63.        , 74.99952415,  0.        ]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 20.0}
episode index:724
target Thresh 31.9816368189412
target distance 24.0
model initialize at round 724
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.73855458, 25.14310384]), 'previousTarget': array([23.71202025, 24.72787848]), 'currentState': array([27.5,  5.5,  0. ]), 'targetState': array([23, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1265066140123942
{'scaleFactor': 20, 'currentTarget': array([58.97996644, 46.32368671]), 'previousTarget': array([58.44590951, 45.89477851]), 'currentState': array([77.        , 54.99999875,  0.        ]), 'targetState': array([23, 29], dtype=int32), 'currentDistance': 20.0}
episode index:725
target Thresh 31.98181953564563
target distance 4.0
model initialize at round 725
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([24.5, 10.5,  0. ]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 4.3011626335212965}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12633236247794186
{'scaleFactor': 20, 'currentTarget': array([59.01937701, 46.74446583]), 'previousTarget': array([58.51106616, 46.25404556]), 'currentState': array([74.        , 59.99516147,  0.        ]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 20.0}
episode index:726
target Thresh 31.982000434288473
target distance 10.0
model initialize at round 726
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([14.5       , 24.49746281,  0.        ]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 14.847448448408251}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1261585903149736
{'scaleFactor': 20, 'currentTarget': array([48.31883896, 49.08432731]), 'previousTarget': array([47.83532197, 48.79625944]), 'currentState': array([64.        , 61.49807729,  0.        ]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 20.0}
episode index:727
target Thresh 31.982179532959748
target distance 13.0
model initialize at round 727
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 24.]), 'previousTarget': array([23., 24.]), 'currentState': array([21.5, 11.5,  0. ]), 'targetState': array([23, 24], dtype=int32), 'currentDistance': 12.589678312014083}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12598529554805743
{'scaleFactor': 20, 'currentTarget': array([54.52026529, 45.67470425]), 'previousTarget': array([54.01707993, 45.31585078]), 'currentState': array([71.        , 57.00688604,  0.        ]), 'targetState': array([23, 24], dtype=int32), 'currentDistance': 20.0}
episode index:728
target Thresh 31.982356849569463
target distance 18.0
model initialize at round 728
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([7.5, 3.5, 0. ]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 18.668154702594475}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12581247621260055
{'scaleFactor': 20, 'currentTarget': array([40.68912688, 39.93813894]), 'previousTarget': array([40.17753707, 39.53571172]), 'currentState': array([57.        , 51.51205003,  0.        ]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 20.0}
episode index:729
target Thresh 31.982532401849436
target distance 11.0
model initialize at round 729
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 16.]), 'previousTarget': array([12., 16.]), 'currentState': array([23.5       , 27.08844896,  0.        ]), 'targetState': array([12, 16], dtype=int32), 'currentDistance': 15.975096252083166}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1256401303547751
{'scaleFactor': 20, 'currentTarget': array([57.22104358, 51.21973279]), 'previousTarget': array([56.71526384, 50.79202942]), 'currentState': array([73.        , 63.50893676,  0.        ]), 'targetState': array([12, 16], dtype=int32), 'currentDistance': 20.0}
episode index:730
target Thresh 31.982706207355037
target distance 19.0
model initialize at round 730
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3.52633404, 18.82455532]), 'previousTarget': array([ 3.23313766, 18.91410718]), 'currentState': array([22.5, 12.5,  0. ]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12546825603144435
{'scaleFactor': 20, 'currentTarget': array([55.0250372 , 51.41326656]), 'previousTarget': array([54.50417711, 50.94768315]), 'currentState': array([72.        , 61.98921275,  0.        ]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 20.0}
episode index:731
target Thresh 31.982878283466963
target distance 24.0
model initialize at round 731
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6.92543563, 24.4954746 ]), 'previousTarget': array([ 6.83261089, 23.98266146]), 'currentState': array([6.5, 4.5, 0. ]), 'targetState': array([ 7, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1252968513100899
{'scaleFactor': 20, 'currentTarget': array([38.33297812, 44.62548425]), 'previousTarget': array([37.79764289, 44.19242513]), 'currentState': array([56.        , 53.99972224,  0.        ]), 'targetState': array([ 7, 28], dtype=int32), 'currentDistance': 20.0}
episode index:732
target Thresh 31.983048647392966
target distance 7.0
model initialize at round 732
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  3.]), 'previousTarget': array([21.,  3.]), 'currentState': array([18.5       ,  9.50000069,  0.        ]), 'targetState': array([21,  3], dtype=int32), 'currentDistance': 6.9641947783569265}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12512591426873917
{'scaleFactor': 20, 'currentTarget': array([52.67013251, 29.53671045]), 'previousTarget': array([52.17137199, 29.12391399]), 'currentState': array([68.        , 42.38175473,  0.        ]), 'targetState': array([21,  3], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:733
target Thresh 31.983217316169583
target distance 20.0
model initialize at round 733
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.99719013,  7.01217609]), 'previousTarget': array([22.8507125,  7.59715  ]), 'currentState': array([18.5, 26.5,  0. ]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12495544299589349
{'scaleFactor': 20, 'currentTarget': array([52.68372084, 31.92573862]), 'previousTarget': array([52.18402808, 31.50733005]), 'currentState': array([68.        , 44.78698244,  0.        ]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 20.0}
episode index:734
target Thresh 31.98338430666383
target distance 7.0
model initialize at round 734
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  4.]), 'previousTarget': array([12.,  4.]), 'currentState': array([5.5       , 1.50331652, 0.        ]), 'targetState': array([12,  4], dtype=int32), 'currentDistance': 6.963004264761937}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.12600358287516264
{'scaleFactor': 20, 'currentTarget': array([12.,  4.]), 'previousTarget': array([12.,  4.]), 'currentState': array([11.        ,  3.72597074,  0.        ]), 'targetState': array([12,  4], dtype=int32), 'currentDistance': 1.0368664497364197}
episode index:735
target Thresh 31.9835496355749
target distance 3.0
model initialize at round 735
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  3.]), 'previousTarget': array([20.,  3.]), 'currentState': array([23.5       ,  6.20322701,  0.        ]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 4.744540367522742}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1258323823549518
{'scaleFactor': 20, 'currentTarget': array([57.68742049, 34.66501769]), 'previousTarget': array([57.18757784, 34.24582514]), 'currentState': array([73.        , 47.53066608,  0.        ]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 20.0}
episode index:736
target Thresh 31.983713319435815
target distance 18.0
model initialize at round 736
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 23.]), 'previousTarget': array([20., 23.]), 'currentState': array([13.5,  5.5,  0. ]), 'targetState': array([20, 23], dtype=int32), 'currentDistance': 18.66815470259445}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1256616464223128
{'scaleFactor': 20, 'currentTarget': array([46.938793  , 42.98948759]), 'previousTarget': array([46.41688828, 42.52693876]), 'currentState': array([63.        , 54.90744168,  0.        ]), 'targetState': array([20, 23], dtype=int32), 'currentDistance': 20.0}
episode index:737
target Thresh 31.983875374615103
target distance 12.0
model initialize at round 737
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 28.]), 'previousTarget': array([17., 28.]), 'currentState': array([19.5, 16.5,  0. ]), 'targetState': array([17, 28], dtype=int32), 'currentDistance': 11.768602295939758}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12549137318867823
{'scaleFactor': 20, 'currentTarget': array([52.85002023, 54.18803768]), 'previousTarget': array([52.33005539, 53.71652215]), 'currentState': array([69.        , 65.98541677,  0.        ]), 'targetState': array([17, 28], dtype=int32), 'currentDistance': 20.0}
episode index:738
target Thresh 31.984035817318414
target distance 12.0
model initialize at round 738
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 21.]), 'previousTarget': array([ 8., 21.]), 'currentState': array([20.5       , 24.49999908,  0.        ]), 'targetState': array([ 8, 21], dtype=int32), 'currentDistance': 12.980754736643744}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12532156077570303
{'scaleFactor': 20, 'currentTarget': array([54.74845341, 60.65687971]), 'previousTarget': array([54.24089929, 60.17989903]), 'currentState': array([70.        , 73.59482107,  0.        ]), 'targetState': array([ 8, 21], dtype=int32), 'currentDistance': 20.0}
episode index:739
target Thresh 31.984194663590156
target distance 15.0
model initialize at round 739
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 29.]), 'previousTarget': array([26., 29.]), 'currentState': array([21.5, 14.5,  0. ]), 'targetState': array([26, 29], dtype=int32), 'currentDistance': 15.182226450688876}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12515220731519533
{'scaleFactor': 20, 'currentTarget': array([55.21139851, 51.71403128]), 'previousTarget': array([54.69242433, 51.23934801]), 'currentState': array([71.        , 63.99084124,  0.        ]), 'targetState': array([26, 29], dtype=int32), 'currentDistance': 20.0}
episode index:740
target Thresh 31.984351929315082
target distance 14.0
model initialize at round 740
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([24.5       , 16.49999595,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 16.324825855220627}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12498331094904797
{'scaleFactor': 20, 'currentTarget': array([59.02839959, 52.42623065]), 'previousTarget': array([58.52270067, 51.94110747]), 'currentState': array([74.        , 65.68711981,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 20.0}
episode index:741
target Thresh 31.984507630219902
target distance 15.0
model initialize at round 741
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([21.5,  5.5,  0. ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 16.446884203398604}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12481486982917053
{'scaleFactor': 20, 'currentTarget': array([54.43778824, 43.78851608]), 'previousTarget': array([53.91853836, 43.31703492]), 'currentState': array([71.        , 54.99981135,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:742
target Thresh 31.984661781874834
target distance 6.0
model initialize at round 742
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 29.]), 'previousTarget': array([15., 29.]), 'currentState': array([21.5, 27.5,  0. ]), 'targetState': array([15, 29], dtype=int32), 'currentDistance': 6.670832032063224}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12464688211742199
{'scaleFactor': 20, 'currentTarget': array([55.80487212, 63.92125576]), 'previousTarget': array([55.29565728, 63.43600668]), 'currentState': array([71.        , 76.92541236,  0.        ]), 'targetState': array([15, 29], dtype=int32), 'currentDistance': 20.0}
episode index:743
target Thresh 31.98481439969517
target distance 11.0
model initialize at round 743
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  5.]), 'previousTarget': array([25.,  5.]), 'currentState': array([14.5, 12.5,  0. ]), 'targetState': array([25,  5], dtype=int32), 'currentDistance': 12.903487900563828}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.12558978768424747
{'scaleFactor': 20, 'currentTarget': array([25.,  5.]), 'previousTarget': array([25.,  5.]), 'currentState': array([24.        ,  4.46656059,  0.        ]), 'targetState': array([25,  5], dtype=int32), 'currentDistance': 1.1333832533986123}
episode index:744
target Thresh 31.984965498942824
target distance 20.0
model initialize at round 744
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  6.]), 'previousTarget': array([10.96680906,  6.22127294]), 'currentState': array([ 8.5       , 25.53989086,  0.        ]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 19.699170912676255}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.125421210788027
{'scaleFactor': 20, 'currentTarget': array([43.68419834, 37.88627963]), 'previousTarget': array([43.18538838, 37.40499999]), 'currentState': array([58.        , 51.85258991,  0.        ]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 20.0}
episode index:745
target Thresh 31.98511509472784
target distance 8.0
model initialize at round 745
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 27.]), 'previousTarget': array([21., 27.]), 'currentState': array([17.5, 19.5,  0. ]), 'targetState': array([21, 27], dtype=int32), 'currentDistance': 8.27647267862332}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12525308584058997
{'scaleFactor': 20, 'currentTarget': array([52.22555166, 55.49017018]), 'previousTarget': array([51.718679  , 54.99900614]), 'currentState': array([67.        , 68.97036588,  0.        ]), 'targetState': array([21, 27], dtype=int32), 'currentDistance': 20.0}
episode index:746
target Thresh 31.985263202009925
target distance 9.0
model initialize at round 746
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 19.]), 'previousTarget': array([ 5., 19.]), 'currentState': array([14.5, 11.5,  0. ]), 'targetState': array([ 5, 19], dtype=int32), 'currentDistance': 12.103718436910217}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12508541102688103
{'scaleFactor': 20, 'currentTarget': array([47.70669137, 49.40136875]), 'previousTarget': array([47.18773393, 48.92804629]), 'currentState': array([64.        , 60.99999341,  0.        ]), 'targetState': array([ 5, 19], dtype=int32), 'currentDistance': 20.0}
episode index:747
target Thresh 31.98540983559993
target distance 15.0
model initialize at round 747
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 18.]), 'previousTarget': array([ 8., 18.]), 'currentState': array([13.5,  3.5,  0. ]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 15.508062419270855}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12491818454155099
{'scaleFactor': 20, 'currentTarget': array([46.12677022, 42.26248999]), 'previousTarget': array([45.6012829 , 41.80264685]), 'currentState': array([63.        , 52.99999979,  0.        ]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 20.0}
episode index:748
target Thresh 31.985555010161338
target distance 21.0
model initialize at round 748
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.2347575 , 11.74523267]), 'previousTarget': array([14.90599608, 11.35899411]), 'currentState': array([26.5       , 28.27079806,  0.        ]), 'targetState': array([12,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12475140458889204
{'scaleFactor': 20, 'currentTarget': array([62.11896687, 58.98735144]), 'previousTarget': array([61.62147841, 58.48928729]), 'currentState': array([76.       , 73.3858555,  0.       ]), 'targetState': array([12,  7], dtype=int32), 'currentDistance': 20.0}
episode index:749
target Thresh 31.985698740211724
target distance 11.0
model initialize at round 749
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 11.]), 'previousTarget': array([23., 11.]), 'currentState': array([12.5       ,  8.50001484,  0.        ]), 'targetState': array([23, 11], dtype=int32), 'currentDistance': 10.793513134861875}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.12568662754788762
{'scaleFactor': 20, 'currentTarget': array([23., 11.]), 'previousTarget': array([23., 11.]), 'currentState': array([22.        , 10.85394399,  0.        ]), 'targetState': array([23, 11], dtype=int32), 'currentDistance': 1.0106098941566402}
episode index:750
target Thresh 31.985841040124214
target distance 13.0
model initialize at round 750
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  7.]), 'previousTarget': array([13.,  7.]), 'currentState': array([26.5,  5.5,  0. ]), 'targetState': array([13,  7], dtype=int32), 'currentDistance': 13.583077707206204}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12551926852319004
{'scaleFactor': 20, 'currentTarget': array([60.09135969, 42.8790949 ]), 'previousTarget': array([59.57675067, 42.39829747]), 'currentState': array([76.        , 54.99995145,  0.        ]), 'targetState': array([13,  7], dtype=int32), 'currentDistance': 20.0}
episode index:751
target Thresh 31.985981924128918
target distance 19.0
model initialize at round 751
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  5.]), 'previousTarget': array([24.76114  ,  5.5672925]), 'currentState': array([17.5       , 23.50019383,  0.        ]), 'targetState': array([25,  5], dtype=int32), 'currentDistance': 19.962644411671604}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12535235460228156
{'scaleFactor': 20, 'currentTarget': array([53.98911958, 38.84294354]), 'previousTarget': array([53.5064374 , 38.35630458]), 'currentState': array([67.        , 54.03231452,  0.        ]), 'targetState': array([25,  5], dtype=int32), 'currentDistance': 20.0}
episode index:752
target Thresh 31.986121406314354
target distance 23.0
model initialize at round 752
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.93421736,  5.98419248]), 'previousTarget': array([14.75140711,  5.45647272]), 'currentState': array([19.5       , 25.45606011,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12518588401184028
{'scaleFactor': 20, 'currentTarget': array([55.62729613, 48.2938032 ]), 'previousTarget': array([55.13444271, 47.78992812]), 'currentState': array([69.        , 63.16561517,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 20.0}
episode index:753
target Thresh 31.98625950062885
target distance 23.0
model initialize at round 753
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.77400662,  8.54055303]), 'previousTarget': array([20.73259233,  8.07518824]), 'currentState': array([19.5       , 28.49993446,  0.        ]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12501985498795187
{'scaleFactor': 20, 'currentTarget': array([56.4675689 , 49.11070249]), 'previousTarget': array([55.98451716, 48.60690593]), 'currentState': array([69.        , 64.69717646,  0.        ]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 20.0}
episode index:754
target Thresh 31.986396220881964
target distance 7.0
model initialize at round 754
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  6.]), 'previousTarget': array([23.,  6.]), 'currentState': array([19.5       , 12.55343491,  0.        ]), 'targetState': array([23,  6], dtype=int32), 'currentDistance': 7.4295026144759415}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1248542657760473
{'scaleFactor': 20, 'currentTarget': array([55.30854386, 40.40267395]), 'previousTarget': array([54.81354664, 39.8988923 ]), 'currentState': array([69.        , 54.98156378,  0.        ]), 'targetState': array([23,  6], dtype=int32), 'currentDistance': 20.0}
episode index:755
target Thresh 31.98653158074583
target distance 15.0
model initialize at round 755
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([23.5       , 28.25084406,  0.        ]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 15.647307902139783}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12468911463084088
{'scaleFactor': 20, 'currentTarget': array([59.38335237, 55.36859794]), 'previousTarget': array([58.88825915, 54.86408429]), 'currentState': array([73.        , 70.01738312,  0.        ]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 20.0}
episode index:756
target Thresh 31.986665593756545
target distance 19.0
model initialize at round 756
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 26.]), 'previousTarget': array([10., 26.]), 'currentState': array([13.5,  7.5,  0. ]), 'targetState': array([10, 26], dtype=int32), 'currentDistance': 18.82817038376275}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1245243998162691
{'scaleFactor': 20, 'currentTarget': array([45.73624794, 46.9023337 ]), 'previousTarget': array([45.2065286 , 46.45331662]), 'currentState': array([63., 57.,  0.]), 'targetState': array([10, 26], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:757
target Thresh 31.98679827331552
target distance 11.0
model initialize at round 757
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 18.]), 'previousTarget': array([ 8., 18.]), 'currentState': array([19.5, 13.5,  0. ]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 12.349089035228513}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1243601196054297
{'scaleFactor': 20, 'currentTarget': array([52.90551739, 51.12702103]), 'previousTarget': array([52.38884898, 50.64964925]), 'currentState': array([69., 63.,  0.]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 20.0}
episode index:758
target Thresh 31.98692963269083
target distance 17.0
model initialize at round 758
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  8.]), 'previousTarget': array([21.,  8.]), 'currentState': array([21.5       , 25.49969956,  0.        ]), 'targetState': array([21,  8], dtype=int32), 'currentDistance': 17.506841085354047}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12419627228052135
{'scaleFactor': 20, 'currentTarget': array([58.07978156, 51.81356518]), 'previousTarget': array([57.59156574, 51.30440761]), 'currentState': array([71.        , 67.08012849,  0.        ]), 'targetState': array([21,  8], dtype=int32), 'currentDistance': 20.0}
episode index:759
target Thresh 31.987059685018515
target distance 6.0
model initialize at round 759
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 19.]), 'previousTarget': array([ 5., 19.]), 'currentState': array([ 6.5       , 25.22303416,  0.        ]), 'targetState': array([ 5, 19], dtype=int32), 'currentDistance': 6.401261917464448}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12403285613278382
{'scaleFactor': 20, 'currentTarget': array([42.17813013, 57.88195347]), 'previousTarget': array([41.68126647, 57.37898481]), 'currentState': array([56.        , 72.33726092,  0.        ]), 'targetState': array([ 5, 19], dtype=int32), 'currentDistance': 20.0}
episode index:760
target Thresh 31.98718844330392
target distance 7.0
model initialize at round 760
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 28.]), 'previousTarget': array([11., 28.]), 'currentState': array([ 7.5, 21.5,  0. ]), 'targetState': array([11, 28], dtype=int32), 'currentDistance': 7.3824115301166335}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12386986946243851
{'scaleFactor': 20, 'currentTarget': array([42.38935978, 57.34179077]), 'previousTarget': array([41.88413453, 56.84738476]), 'currentState': array([57.        , 70.99935982,  0.        ]), 'targetState': array([11, 28], dtype=int32), 'currentDistance': 20.0}
episode index:761
target Thresh 31.98731592042298
target distance 21.0
model initialize at round 761
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.81711303,  8.68270279]), 'previousTarget': array([23.45612429,  8.63241055]), 'currentState': array([4.5      , 3.5010868, 0.       ]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.12459409410758003
{'scaleFactor': 20, 'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([24.       ,  8.5321616,  0.       ]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 1.1040257120679686}
episode index:762
target Thresh 31.987442129123508
target distance 20.0
model initialize at round 762
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.95951333,  4.55331764]), 'previousTarget': array([14.99007438,  4.0992562 ]), 'currentState': array([13.5       , 24.49999207,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1244307990956435
{'scaleFactor': 20, 'currentTarget': array([50.56669489, 48.81316655]), 'previousTarget': array([50.08348144, 48.30169479]), 'currentState': array([63.        , 64.47882722,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 20.0}
episode index:763
target Thresh 31.987567082026487
target distance 7.0
model initialize at round 763
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 20.]), 'previousTarget': array([ 9., 20.]), 'currentState': array([ 2.5       , 17.49999231,  0.        ]), 'targetState': array([ 9, 20], dtype=int32), 'currentDistance': 6.96419689878558}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.1254398402673229
{'scaleFactor': 20, 'currentTarget': array([ 9., 20.]), 'previousTarget': array([ 9., 20.]), 'currentState': array([ 8.        , 19.48972945,  0.        ]), 'targetState': array([ 9, 20], dtype=int32), 'currentDistance': 1.1226647028661407}
episode index:764
target Thresh 31.987690791627305
target distance 18.0
model initialize at round 764
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.7396427 , 24.90043122]), 'previousTarget': array([11.72118773, 24.78704435]), 'currentState': array([24.5,  9.5,  0. ]), 'targetState': array([10, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12527586661991466
{'scaleFactor': 20, 'currentTarget': array([56.11145618, 50.05572809]), 'previousTarget': array([55.58332996, 49.61220305]), 'currentState': array([74., 59.,  0.]), 'targetState': array([10, 27], dtype=int32), 'currentDistance': 20.0}
episode index:765
target Thresh 31.98781327029703
target distance 9.0
model initialize at round 765
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([12.5       , 23.49897793,  0.        ]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 12.74678711292974}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1251123211021341
{'scaleFactor': 20, 'currentTarget': array([47.97874645, 58.73384488]), 'previousTarget': array([47.47979325, 58.23281822]), 'currentState': array([62.        , 72.99583805,  0.        ]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 20.0}
episode index:766
target Thresh 31.987934530283628
target distance 9.0
model initialize at round 766
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 16.]), 'previousTarget': array([16., 16.]), 'currentState': array([20.5,  7.5,  0. ]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 9.617692030835713}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12494920203941945
{'scaleFactor': 20, 'currentTarget': array([54.0710735 , 44.90581423]), 'previousTarget': array([53.55381047, 44.42858466]), 'currentState': array([70.        , 56.99999881,  0.        ]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 20.0}
episode index:767
target Thresh 31.9880545837132
target distance 12.0
model initialize at round 767
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'previousTarget': array([7., 3.]), 'currentState': array([16.5       , 15.47286648,  0.        ]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 15.678724377885509}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12478650776593063
{'scaleFactor': 20, 'currentTarget': array([52.20693788, 50.46755603]), 'previousTarget': array([51.70988163, 49.9647531 ]), 'currentState': array([66.        , 64.95035401,  0.        ]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 20.0}
episode index:768
target Thresh 31.98817344259119
target distance 8.0
model initialize at round 768
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 14.]), 'previousTarget': array([25., 14.]), 'currentState': array([17.5,  6.5,  0. ]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 10.606601717798158}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.12575394509363289
{'scaleFactor': 20, 'currentTarget': array([25., 14.]), 'previousTarget': array([25., 14.]), 'currentState': array([24.5       , 13.49981907,  0.        ]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 0.7072347295155483}
episode index:769
target Thresh 31.988291118803584
target distance 19.0
model initialize at round 769
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.65581896, 11.7106186 ]), 'previousTarget': array([24.10111675, 12.13601924]), 'currentState': array([ 8.5, 23.5,  0. ]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.12650419671688734
{'scaleFactor': 20, 'currentTarget': array([27., 10.]), 'previousTarget': array([27., 10.]), 'currentState': array([26.        ,  9.79731075,  0.        ]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 1.0203347162537884}
episode index:770
target Thresh 31.988407624118096
target distance 14.0
model initialize at round 770
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 10.]), 'previousTarget': array([19., 10.]), 'currentState': array([15.5       , 24.49160805,  0.        ]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 14.908276358274687}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12634011864073055
{'scaleFactor': 20, 'currentTarget': array([51.86596371, 47.74297096]), 'previousTarget': array([51.37656537, 47.23374605]), 'currentState': array([65.        , 62.82597765,  0.        ]), 'targetState': array([19, 10], dtype=int32), 'currentDistance': 20.0}
episode index:771
target Thresh 31.988522970185365
target distance 5.0
model initialize at round 771
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 8.]), 'previousTarget': array([9., 8.]), 'currentState': array([ 7.5       , 12.58953357,  0.        ]), 'targetState': array([9, 8], dtype=int32), 'currentDistance': 4.828438501892059}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12617646563730991
{'scaleFactor': 20, 'currentTarget': array([43.04227319, 42.93601316]), 'previousTarget': array([42.54420105, 42.43413487]), 'currentState': array([57.        , 57.26018372,  0.        ]), 'targetState': array([9, 8], dtype=int32), 'currentDistance': 20.0}
episode index:772
target Thresh 31.988637168540087
target distance 4.0
model initialize at round 772
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 10.]), 'previousTarget': array([27., 10.]), 'currentState': array([26.5       , 13.64260447,  0.        ]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 3.6767604391239392}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1260132360569253
{'scaleFactor': 20, 'currentTarget': array([62.02791684, 45.87549183]), 'previousTarget': array([61.52965893, 45.37379111]), 'currentState': array([76.        , 60.18565927,  0.        ]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 20.0}
episode index:773
target Thresh 31.988750230602193
target distance 8.0
model initialize at round 773
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  7.]), 'previousTarget': array([13.,  7.]), 'currentState': array([19.5       , 15.49938586,  0.        ]), 'targetState': array([13,  7], dtype=int32), 'currentDistance': 10.69997944191429}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12585042825840215
{'scaleFactor': 20, 'currentTarget': array([55.10794263, 50.61078416]), 'previousTarget': array([54.61017445, 50.10862959]), 'currentState': array([69.        , 64.99865204,  0.        ]), 'targetState': array([13,  7], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:774
target Thresh 31.988862167677983
target distance 4.0
model initialize at round 774
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 25.]), 'previousTarget': array([19., 25.]), 'currentState': array([15.5       , 27.50399634,  0.        ]), 'targetState': array([19, 25], dtype=int32), 'currentDistance': 4.30348668905278}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.1269151245443913
{'scaleFactor': 20, 'currentTarget': array([19., 25.]), 'previousTarget': array([19., 25.]), 'currentState': array([18.       , 25.0158982,  0.       ]), 'targetState': array([19, 25], dtype=int32), 'currentDistance': 1.0001263683631028}
episode index:775
target Thresh 31.988972990961262
target distance 21.0
model initialize at round 775
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.33978252, 20.54483829]), 'previousTarget': array([11.16928442, 20.41826648]), 'currentState': array([26.5,  7.5,  0. ]), 'targetState': array([ 5, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12675157412616397
{'scaleFactor': 20, 'currentTarget': array([57.67093342, 48.99716811]), 'previousTarget': array([57.14414133, 48.55881291]), 'currentState': array([76., 57.,  0.]), 'targetState': array([ 5, 26], dtype=int32), 'currentDistance': 20.0}
episode index:776
target Thresh 31.989082711534447
target distance 25.0
model initialize at round 776
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.10618397,  8.39152188]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.5       , 28.38764423,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12658844468713415
{'scaleFactor': 20, 'currentTarget': array([53.24485217, 47.981642  ]), 'previousTarget': array([52.75799025, 47.47077337]), 'currentState': array([66.        , 63.38638815,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:777
target Thresh 31.989191340369686
target distance 13.0
model initialize at round 777
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 12.]), 'previousTarget': array([ 4., 12.]), 'currentState': array([17.5, 22.5,  0. ]), 'targetState': array([ 4, 12], dtype=int32), 'currentDistance': 17.102631376487185}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12642573460398873
{'scaleFactor': 20, 'currentTarget': array([52.51724138, 58.20689655]), 'previousTarget': array([52.0144858 , 57.70979049]), 'currentState': array([67., 72.,  0.]), 'targetState': array([ 4, 12], dtype=int32), 'currentDistance': 20.0}
episode index:778
target Thresh 31.98929888832996
target distance 19.0
model initialize at round 778
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 28.]), 'previousTarget': array([22., 28.]), 'currentState': array([ 3.5, 23.5,  0. ]), 'targetState': array([22, 28], dtype=int32), 'currentDistance': 19.039432764659793}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.12716645599088935
{'scaleFactor': 20, 'currentTarget': array([22., 28.]), 'previousTarget': array([22., 28.]), 'currentState': array([21.        , 27.72286045,  0.        ]), 'targetState': array([22, 28], dtype=int32), 'currentDistance': 1.0376927923254284}
episode index:779
target Thresh 31.989405366170143
target distance 17.0
model initialize at round 779
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 21.]), 'previousTarget': array([18., 21.]), 'currentState': array([24.5,  4.5,  0. ]), 'targetState': array([18, 21], dtype=int32), 'currentDistance': 17.734147850968235}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12700342207295232
{'scaleFactor': 20, 'currentTarget': array([56.76923077, 43.84615385]), 'previousTarget': array([56.24136419, 43.39359164]), 'currentState': array([74., 54.,  0.]), 'targetState': array([18, 21], dtype=int32), 'currentDistance': 20.0}
episode index:780
target Thresh 31.98951078453812
target distance 12.0
model initialize at round 780
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 22.]), 'previousTarget': array([ 8., 22.]), 'currentState': array([20.5, 27.5,  0. ]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 13.656500283747768}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12684080565544534
{'scaleFactor': 20, 'currentTarget': array([55.03850723, 63.72770803]), 'previousTarget': array([54.53168841, 63.23539868]), 'currentState': array([70., 77.,  0.]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 20.0}
episode index:781
target Thresh 31.989615153975805
target distance 12.0
model initialize at round 781
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 14.]), 'previousTarget': array([15., 14.]), 'currentState': array([7.5, 2.5, 0. ]), 'targetState': array([15, 14], dtype=int32), 'currentDistance': 13.729530217745987}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12667860513670434
{'scaleFactor': 20, 'currentTarget': array([42.16928432, 38.58173309]), 'previousTarget': array([41.66081615, 38.09109859]), 'currentState': array([57.        , 51.99999946,  0.        ]), 'targetState': array([15, 14], dtype=int32), 'currentDistance': 20.0}
episode index:782
target Thresh 31.989718484920235
target distance 12.0
model initialize at round 782
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([21.5, 23.5,  0. ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 17.6776695296638}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1265168189232475
{'scaleFactor': 20, 'currentTarget': array([56.85786438, 58.85786438]), 'previousTarget': array([56.35786438, 58.35786438]), 'currentState': array([71., 73.,  0.]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 20.0}
episode index:783
target Thresh 31.989820787704588
target distance 23.0
model initialize at round 783
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.56801137, 25.20944187]), 'previousTarget': array([18.28798697, 24.62485559]), 'currentState': array([11.5,  6.5,  0. ]), 'targetState': array([20, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12635544542972296
{'scaleFactor': 20, 'currentTarget': array([44.2965827 , 45.00018861]), 'previousTarget': array([43.76424848, 44.54944654]), 'currentState': array([61., 56.,  0.]), 'targetState': array([20, 29], dtype=int32), 'currentDistance': 20.0}
episode index:784
target Thresh 31.98992207255923
target distance 13.0
model initialize at round 784
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 28.]), 'previousTarget': array([19., 28.]), 'currentState': array([25.5, 15.5,  0. ]), 'targetState': array([19, 28], dtype=int32), 'currentDistance': 14.089002803605336}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12619448307885708
{'scaleFactor': 20, 'currentTarget': array([58.31329359, 53.97485469]), 'previousTarget': array([57.78983957, 53.51043503]), 'currentState': array([75., 65.,  0.]), 'targetState': array([19, 28], dtype=int32), 'currentDistance': 20.0}
episode index:785
target Thresh 31.99002234961273
target distance 12.0
model initialize at round 785
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 29.]), 'previousTarget': array([ 3., 29.]), 'currentState': array([ 9.5, 17.5,  0. ]), 'targetState': array([ 3, 29], dtype=int32), 'currentDistance': 13.209844813622874}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12603393030140306
{'scaleFactor': 20, 'currentTarget': array([42.45048451, 55.76997163]), 'previousTarget': array([41.92822063, 55.30285177]), 'currentState': array([59., 67.,  0.]), 'targetState': array([ 3, 29], dtype=int32), 'currentDistance': 20.0}
episode index:786
target Thresh 31.990121628892876
target distance 8.0
model initialize at round 786
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([10.5       , 14.49079034,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 9.609553608383532}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12587378553608997
{'scaleFactor': 20, 'currentTarget': array([47.40687536, 45.98380991]), 'previousTarget': array([46.92304451, 45.47072095]), 'currentState': array([60.        , 61.52128756,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 20.0}
episode index:787
target Thresh 31.99021992032768
target distance 15.0
model initialize at round 787
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 13.]), 'previousTarget': array([ 2., 13.]), 'currentState': array([17.5,  3.5,  0. ]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 18.179658962697914}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12571404722957208
{'scaleFactor': 20, 'currentTarget': array([49.96683367, 42.51805149]), 'previousTarget': array([49.44417563, 42.05496027]), 'currentState': array([67., 53.,  0.]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 20.0}
episode index:788
target Thresh 31.990317233746364
target distance 18.0
model initialize at round 788
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.2603573 , 17.90043122]), 'previousTarget': array([17.80368799, 17.36442559]), 'currentState': array([5.5, 2.5, 0. ]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1255547138363787
{'scaleFactor': 20, 'currentTarget': array([40.23942193, 38.50460928]), 'previousTarget': array([39.73028826, 38.01460603]), 'currentState': array([55.        , 51.99999127,  0.        ]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 20.0}
episode index:789
target Thresh 31.990413578880357
target distance 7.0
model initialize at round 789
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  5.]), 'previousTarget': array([11.,  5.]), 'currentState': array([4.5       , 2.48569101, 0.        ]), 'targetState': array([11,  5], dtype=int32), 'currentDistance': 6.969343562517752}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.1265291233812171
{'scaleFactor': 20, 'currentTarget': array([11.,  5.]), 'previousTarget': array([11.,  5.]), 'currentState': array([10.        ,  4.74926142,  0.        ]), 'targetState': array([11,  5], dtype=int32), 'currentDistance': 1.0309557885564389}
episode index:790
target Thresh 31.990508965364246
target distance 13.0
model initialize at round 790
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 15.]), 'previousTarget': array([25., 15.]), 'currentState': array([25.5       , 28.47516945,  0.        ]), 'targetState': array([25, 15], dtype=int32), 'currentDistance': 13.484442580522801}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12636916241613338
{'scaleFactor': 20, 'currentTarget': array([62.16711381, 59.42845143]), 'previousTarget': array([61.67956939, 58.91804025]), 'currentState': array([75.        , 74.76849811,  0.        ]), 'targetState': array([25, 15], dtype=int32), 'currentDistance': 20.0}
episode index:791
target Thresh 31.990603402736767
target distance 22.0
model initialize at round 791
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2.44004776, 7.12261428]), 'previousTarget': array([2.42229124, 7.3226018 ]), 'currentState': array([ 6.5       , 26.70619893,  0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1262096053928807
{'scaleFactor': 20, 'currentTarget': array([43.1907403 , 54.39245178]), 'previousTarget': array([42.7024578 , 53.88268759]), 'currentState': array([56.        , 69.75223257,  0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 20.0}
episode index:792
target Thresh 31.990696900441726
target distance 7.0
model initialize at round 792
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([ 7.5       , 18.50658855,  0.        ]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 6.961831703428431}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.1271795028063307
{'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([13.        , 20.64703752,  0.        ]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 1.0604633463476658}
episode index:793
target Thresh 31.99078946782898
target distance 14.0
model initialize at round 793
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([19.5       , 24.49994969,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 17.3348937441004}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12701932711010105
{'scaleFactor': 20, 'currentTarget': array([55.44392861, 59.29500745]), 'previousTarget': array([54.94882075, 58.790499  ]), 'currentState': array([69.        , 73.99986815,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 20.0}
episode index:794
target Thresh 31.99088111415534
target distance 4.0
model initialize at round 794
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([ 8.5       , 25.53140286,  0.        ]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 4.972002225678539}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.12804380613185062
{'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([11.5       , 22.60115391,  0.        ]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 0.7819117748823913}
episode index:795
target Thresh 31.990971848585517
target distance 5.0
model initialize at round 795
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 18.]), 'previousTarget': array([ 7., 18.]), 'currentState': array([12.5, 16.5,  0. ]), 'targetState': array([ 7, 18], dtype=int32), 'currentDistance': 5.7008771254957455}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12788294707892117
{'scaleFactor': 20, 'currentTarget': array([46.93150685, 52.84931507]), 'previousTarget': array([46.42278873, 52.35931127]), 'currentState': array([62., 66.,  0.]), 'targetState': array([ 7, 18], dtype=int32), 'currentDistance': 20.0}
episode index:796
target Thresh 31.991061680193027
target distance 13.0
model initialize at round 796
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 19.]), 'previousTarget': array([20., 19.]), 'currentState': array([23.5,  6.5,  0. ]), 'targetState': array([20, 19], dtype=int32), 'currentDistance': 12.980754985747195}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12772249168735414
{'scaleFactor': 20, 'currentTarget': array([56.60083833, 44.55152864]), 'previousTarget': array([56.07870897, 44.0832929 ]), 'currentState': array([73., 56.,  0.]), 'targetState': array([20, 19], dtype=int32), 'currentDistance': 20.0}
episode index:797
target Thresh 31.99115061796111
target distance 5.0
model initialize at round 797
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 26.]), 'previousTarget': array([14., 26.]), 'currentState': array([ 9.5      , 26.5036535,  0.       ]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 4.528097486200997}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.1287304401287321
{'scaleFactor': 20, 'currentTarget': array([14., 26.]), 'previousTarget': array([14., 26.]), 'currentState': array([13.        , 25.97863268,  0.        ]), 'targetState': array([14, 26], dtype=int32), 'currentDistance': 1.000228255028679}
episode index:798
target Thresh 31.991238670783616
target distance 18.0
model initialize at round 798
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.77466927,  3.87523626]), 'previousTarget': array([20.21358457,  4.29018892]), 'currentState': array([ 4.5       , 15.50000036,  0.        ]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.12946761170879412
{'scaleFactor': 20, 'currentTarget': array([22.,  3.]), 'previousTarget': array([22.,  3.]), 'currentState': array([21.        ,  2.69688453,  0.        ]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 1.044930134281439}
episode index:799
target Thresh 31.991325847465895
target distance 16.0
model initialize at round 799
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 25.]), 'previousTarget': array([24., 25.]), 'currentState': array([12.5,  9.5,  0. ]), 'targetState': array([24, 25], dtype=int32), 'currentDistance': 19.300259065618672}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12930577719415812
{'scaleFactor': 20, 'currentTarget': array([47.09517347, 45.66410178]), 'previousTarget': array([46.58477559, 45.17573207]), 'currentState': array([62.        , 58.99999869,  0.        ]), 'targetState': array([24, 25], dtype=int32), 'currentDistance': 20.0}
episode index:800
target Thresh 31.991412156725694
target distance 12.0
model initialize at round 800
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([ 3.5       , 25.50023639,  0.        ]), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 15.572570895273772}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.13015524297571504
{'scaleFactor': 20, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.        , 15.54589733,  0.        ]), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 1.1392997411370038}
episode index:801
target Thresh 31.99149760719401
target distance 13.0
model initialize at round 801
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 12.]), 'previousTarget': array([ 9., 12.]), 'currentState': array([20.5       , 25.49997601,  0.        ]), 'targetState': array([ 9, 12], dtype=int32), 'currentDistance': 17.734129588088074}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12999295464282762
{'scaleFactor': 20, 'currentTarget': array([56.08776398, 60.63156519]), 'previousTarget': array([55.58964767, 60.12974165]), 'currentState': array([70.        , 74.99992239,  0.        ]), 'targetState': array([ 9, 12], dtype=int32), 'currentDistance': 20.0}
episode index:802
target Thresh 31.99158220741596
target distance 11.0
model initialize at round 802
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([19.5, 11.5,  0. ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 11.597413504743288}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12983107051500342
{'scaleFactor': 20, 'currentTarget': array([53.65621216, 48.17158721]), 'previousTarget': array([53.14598052, 47.68383498]), 'currentState': array([69., 61.,  0.]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 20.0}
episode index:803
target Thresh 31.991665965851638
target distance 22.0
model initialize at round 803
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8.71295565, 16.77881227]), 'previousTarget': array([ 8.52085402, 16.66475581]), 'currentState': array([24.5,  4.5,  0. ]), 'targetState': array([ 2, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12966958908401463
{'scaleFactor': 20, 'currentTarget': array([55.72376903, 45.87723068]), 'previousTarget': array([55.19747586, 45.4366502 ]), 'currentState': array([74., 54.,  0.]), 'targetState': array([ 2, 22], dtype=int32), 'currentDistance': 20.0}
episode index:804
target Thresh 31.991748890876952
target distance 20.0
model initialize at round 804
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 29.]), 'previousTarget': array([19.00992562, 28.9007438 ]), 'currentState': array([21.5,  9.5,  0. ]), 'targetState': array([19, 29], dtype=int32), 'currentDistance': 19.659603251337433}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12950850884912765
{'scaleFactor': 20, 'currentTarget': array([53.67628828, 49.00555093]), 'previousTarget': array([53.14551526, 48.55908156]), 'currentState': array([71., 59.,  0.]), 'targetState': array([19, 29], dtype=int32), 'currentDistance': 20.0}
episode index:805
target Thresh 31.991830990784482
target distance 16.0
model initialize at round 805
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 24.]), 'previousTarget': array([25., 24.]), 'currentState': array([14.5,  8.5,  0. ]), 'targetState': array([25, 24], dtype=int32), 'currentDistance': 18.721645226848945}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12934782831705677
{'scaleFactor': 20, 'currentTarget': array([48.92454538, 44.85729592]), 'previousTarget': array([48.41210876, 44.3715751 ]), 'currentState': array([64.        , 57.99999991,  0.        ]), 'targetState': array([25, 24], dtype=int32), 'currentDistance': 20.0}
episode index:806
target Thresh 31.99191227378428
target distance 14.0
model initialize at round 806
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  6.]), 'previousTarget': array([24.,  6.]), 'currentState': array([13.5       , 20.26360846,  0.        ]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 17.711592988105686}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1291875460019179
{'scaleFactor': 20, 'currentTarget': array([50.44832932, 38.81049516]), 'previousTarget': array([49.96747065, 38.29509724]), 'currentState': array([63.        , 54.38147981,  0.        ]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 20.0}
episode index:807
target Thresh 31.99199274800472
target distance 15.0
model initialize at round 807
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 26.]), 'previousTarget': array([19., 26.]), 'currentState': array([26.5, 11.5,  0. ]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 16.324827717314474}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12902766042518285
{'scaleFactor': 20, 'currentTarget': array([58.95657549, 50.53473933]), 'previousTarget': array([58.43063044, 50.07711062]), 'currentState': array([76., 61.,  0.]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 20.0}
episode index:808
target Thresh 31.992072421493283
target distance 2.0
model initialize at round 808
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 12.]), 'previousTarget': array([18., 12.]), 'currentState': array([19.5       , 14.38124219,  0.        ]), 'targetState': array([18, 12], dtype=int32), 'currentDistance': 2.814305304614615}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12886817011563378
{'scaleFactor': 20, 'currentTarget': array([54.96754   , 49.54318207]), 'previousTarget': array([54.46862156, 49.04211721]), 'currentState': array([69.        , 63.79414928,  0.        ]), 'targetState': array([18, 12], dtype=int32), 'currentDistance': 20.0}
episode index:809
target Thresh 31.99215130221739
target distance 8.0
model initialize at round 809
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 15.]), 'previousTarget': array([10., 15.]), 'currentState': array([ 2.5       , 18.50000459,  0.        ]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 8.276474619481625}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.12979243289697123
{'scaleFactor': 20, 'currentTarget': array([10., 15.]), 'previousTarget': array([10., 15.]), 'currentState': array([ 9.        , 15.09836271,  0.        ]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 1.004825966780017}
episode index:810
target Thresh 31.99222939806518
target distance 21.0
model initialize at round 810
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6.12394591, 23.456665  ]), 'previousTarget': array([ 5.76952105, 23.49442256]), 'currentState': array([25.5, 18.5,  0. ]), 'targetState': array([ 4, 24], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1296323929057296
{'scaleFactor': 20, 'currentTarget': array([57.99980187, 57.46466595]), 'previousTarget': array([57.47927974, 56.99785346]), 'currentState': array([75., 68.,  0.]), 'targetState': array([ 4, 24], dtype=int32), 'currentDistance': 20.0}
episode index:811
target Thresh 31.9923067168463
target distance 13.0
model initialize at round 811
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([ 3.5, 10.5,  0. ]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 16.32482771731445}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1294727471016585
{'scaleFactor': 20, 'currentTarget': array([38.49065812, 46.23449738]), 'previousTarget': array([37.9858316 , 45.73958663]), 'currentState': array([53.        , 59.99963444,  0.        ]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 20.0}
episode index:812
target Thresh 31.992383266292695
target distance 10.0
model initialize at round 812
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([15.5       , 12.50000009,  0.        ]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 10.124228396566684}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.13035032452635992
{'scaleFactor': 20, 'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([24.        ,  8.92864138,  0.        ]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 1.0025427932381006}
episode index:813
target Thresh 31.99245905405937
target distance 13.0
model initialize at round 813
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 28.]), 'previousTarget': array([17., 28.]), 'currentState': array([ 4.5, 21.5,  0. ]), 'targetState': array([17, 28], dtype=int32), 'currentDistance': 14.089002803605338}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.1311651451149561
{'scaleFactor': 20, 'currentTarget': array([17., 28.]), 'previousTarget': array([17., 28.]), 'currentState': array([16.        , 27.61138957,  0.        ]), 'targetState': array([17, 28], dtype=int32), 'currentDistance': 1.0728550990021062}
episode index:814
target Thresh 31.99253408772517
target distance 9.0
model initialize at round 814
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 28.]), 'previousTarget': array([14., 28.]), 'currentState': array([14.5, 19.5,  0. ]), 'targetState': array([14, 28], dtype=int32), 'currentDistance': 8.514693182963128}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1310042062865942
{'scaleFactor': 20, 'currentTarget': array([48.53464419, 56.31840824]), 'previousTarget': array([48.02085402, 55.83524419]), 'currentState': array([64., 69.,  0.]), 'targetState': array([14, 28], dtype=int32), 'currentDistance': 20.0}
episode index:815
target Thresh 31.99260837479352
target distance 26.0
model initialize at round 815
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.1079185 , 22.49615643]), 'previousTarget': array([24., 22.]), 'currentState': array([24.5,  2.5,  0. ]), 'targetState': array([24, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13084366191614497
{'scaleFactor': 20, 'currentTarget': array([55.96953885, 43.34537865]), 'previousTarget': array([55.43268208, 42.92258644]), 'currentState': array([74., 52.,  0.]), 'targetState': array([24, 28], dtype=int32), 'currentDistance': 20.0}
episode index:816
target Thresh 31.992681922693187
target distance 23.0
model initialize at round 816
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.45570316,  2.16961979]), 'previousTarget': array([22.92481176,  2.26740767]), 'currentState': array([3.5, 3.5, 0. ]), 'targetState': array([26,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.13147800581633132
{'scaleFactor': 20, 'currentTarget': array([26.,  2.]), 'previousTarget': array([26.,  2.]), 'currentState': array([25.        ,  2.08352032,  0.        ]), 'targetState': array([26,  2], dtype=int32), 'currentDistance': 1.0034817604605994}
episode index:817
target Thresh 31.992754738779027
target distance 2.0
model initialize at round 817
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 3.]), 'previousTarget': array([6., 3.]), 'currentState': array([8.5       , 5.48429221, 0.        ]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 3.524444322220984}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13131727475787616
{'scaleFactor': 20, 'currentTarget': array([43.85410119, 40.83395809]), 'previousTarget': array([43.35406465, 40.33399465]), 'currentState': array([58.        , 54.97232953,  0.        ]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:818
target Thresh 31.992826830332707
target distance 5.0
model initialize at round 818
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 13.]), 'previousTarget': array([17., 13.]), 'currentState': array([22.5, 10.5,  0. ]), 'targetState': array([17, 13], dtype=int32), 'currentDistance': 6.041522986797327}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13115693620505825
{'scaleFactor': 20, 'currentTarget': array([56.79537106, 47.00695345]), 'previousTarget': array([56.28534815, 46.51869154]), 'currentState': array([72., 60.,  0.]), 'targetState': array([17, 13], dtype=int32), 'currentDistance': 20.0}
episode index:819
target Thresh 31.99289820456344
target distance 13.0
model initialize at round 819
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  4.]), 'previousTarget': array([24.,  4.]), 'currentState': array([12.5       , 16.50023827,  0.        ]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 16.98546310219285}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13099698872188134
{'scaleFactor': 20, 'currentTarget': array([49.64019667, 36.61857598]), 'previousTarget': array([49.16196206, 36.10149318]), 'currentState': array([62.        , 52.34229251,  0.        ]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 20.0}
episode index:820
target Thresh 31.992968868608713
target distance 23.0
model initialize at round 820
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4.17544468, 25.47366596]), 'previousTarget': array([ 4.17676768, 25.13347761]), 'currentState': array([10.5,  6.5,  0. ]), 'targetState': array([ 3, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13083743087934557
{'scaleFactor': 20, 'currentTarget': array([41.92524322, 47.43827311]), 'previousTarget': array([41.39274618, 47.00721723]), 'currentState': array([60., 56.,  0.]), 'targetState': array([ 3, 29], dtype=int32), 'currentDistance': 20.0}
episode index:821
target Thresh 31.99303882953499
target distance 4.0
model initialize at round 821
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([ 4.5, 21.5,  0. ]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 4.301162633521252}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.13183518345722958
{'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([ 7., 24.,  0.]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 0.9999999999995204}
episode index:822
target Thresh 31.993108094338414
target distance 20.0
model initialize at round 822
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6.55332553, 21.95951277]), 'previousTarget': array([ 6.0992562 , 21.99007438]), 'currentState': array([26.5, 20.5,  0. ]), 'targetState': array([ 6, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13167499489895834
{'scaleFactor': 20, 'currentTarget': array([59.50542296, 58.68943289]), 'previousTarget': array([58.98802948, 58.21484029]), 'currentState': array([76., 70.,  0.]), 'targetState': array([ 6, 22], dtype=int32), 'currentDistance': 20.0}
episode index:823
target Thresh 31.993176669945534
target distance 2.0
model initialize at round 823
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.5,  3.5,  0. ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.5811388300841687}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.13270463689544018
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.5       ,  4.49999571,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.7071098157663427}
episode index:824
target Thresh 31.99324456321396
target distance 17.0
model initialize at round 824
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  8.]), 'previousTarget': array([19.,  8.]), 'currentState': array([2.5, 4.5, 0. ]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 16.867127793433003}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.13343142323817833
{'scaleFactor': 20, 'currentTarget': array([19.,  8.]), 'previousTarget': array([19.,  8.]), 'currentState': array([18.        ,  7.75779567,  0.        ]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 1.0289134735634198}
episode index:825
target Thresh 31.993311780933077
target distance 7.0
model initialize at round 825
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([14.5       , 22.83235644,  0.        ]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 7.2307652247615755}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.13435382860260997
{'scaleFactor': 20, 'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([20.        , 25.81970002,  0.        ]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 1.0161240493471708}
episode index:826
target Thresh 31.993378329824715
target distance 16.0
model initialize at round 826
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 13.]), 'previousTarget': array([ 7., 13.]), 'currentState': array([12.5      , 29.3343952,  0.       ]), 'targetState': array([ 7, 13], dtype=int32), 'currentDistance': 17.235500182769442}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1341913693177217
{'scaleFactor': 20, 'currentTarget': array([49.14971693, 63.26850269]), 'previousTarget': array([48.66089081, 62.75914045]), 'currentState': array([62.        , 78.59397903,  0.        ]), 'targetState': array([ 7, 13], dtype=int32), 'currentDistance': 20.0}
episode index:827
target Thresh 31.993444216543818
target distance 21.0
model initialize at round 827
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6.61599637, 20.91255687]), 'previousTarget': array([ 6.02633404, 20.67544468]), 'currentState': array([25.5, 27.5,  0. ]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13402930244656502
{'scaleFactor': 20, 'currentTarget': array([59.40407712, 64.47932952]), 'previousTarget': array([58.89342817, 63.99260556]), 'currentState': array([75., 77.,  0.]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 20.0}
episode index:828
target Thresh 31.99350944767911
target distance 4.0
model initialize at round 828
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([13.5       , 11.49999994,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 4.5276925624865605}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13386762656906614
{'scaleFactor': 20, 'currentTarget': array([48.32478637, 47.4118384 ]), 'previousTarget': array([47.81972096, 46.91731109]), 'currentState': array([63.        , 60.99999887,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:829
target Thresh 31.993574029753763
target distance 14.0
model initialize at round 829
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 19.]), 'previousTarget': array([ 5., 19.]), 'currentState': array([19.5, 28.5,  0. ]), 'targetState': array([ 5, 19], dtype=int32), 'currentDistance': 17.334935823359825}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.133706340271995
{'scaleFactor': 20, 'currentTarget': array([54.29512538, 64.44394371]), 'previousTarget': array([53.79061682, 63.94883597]), 'currentState': array([69., 78.,  0.]), 'targetState': array([ 5, 19], dtype=int32), 'currentDistance': 20.0}
episode index:830
target Thresh 31.993637969226036
target distance 13.0
model initialize at round 830
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 28.]), 'previousTarget': array([26., 28.]), 'currentState': array([22.5, 15.5,  0. ]), 'targetState': array([26, 28], dtype=int32), 'currentDistance': 12.98075498574707}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.133545442148924
{'scaleFactor': 20, 'currentTarget': array([56.41573099, 52.4648271 ]), 'previousTarget': array([55.89936323, 51.98520347]), 'currentState': array([72., 65.,  0.]), 'targetState': array([26, 28], dtype=int32), 'currentDistance': 20.0}
episode index:831
target Thresh 31.99370127248993
target distance 22.0
model initialize at round 831
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7.543335  , 22.87605409]), 'previousTarget': array([ 7.56757793, 22.50265712]), 'currentState': array([12.5,  3.5,  0. ]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13338493080018732
{'scaleFactor': 20, 'currentTarget': array([44.17673769, 43.92633919]), 'previousTarget': array([43.64433887, 43.49026273]), 'currentState': array([62., 53.,  0.]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 20.0}
episode index:832
target Thresh 31.993763945875823
target distance 17.0
model initialize at round 832
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([17.5       , 25.78573367,  0.        ]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 18.81517618413502}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13322480483284016
{'scaleFactor': 20, 'currentTarget': array([54.87862076, 46.90064482]), 'previousTarget': array([54.40112871, 46.38352002]), 'currentState': array([67.        , 62.80888688,  0.        ]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 20.0}
episode index:833
target Thresh 31.993825995651108
target distance 13.0
model initialize at round 833
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 21.]), 'previousTarget': array([ 4., 21.]), 'currentState': array([3.5, 8.5, 0. ]), 'targetState': array([ 4, 21], dtype=int32), 'currentDistance': 12.509996003196724}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13306506286061853
{'scaleFactor': 20, 'currentTarget': array([37.03917262, 45.94794667]), 'previousTarget': array([36.51979104, 45.47365718]), 'currentState': array([53., 58.,  0.]), 'targetState': array([ 4, 21], dtype=int32), 'currentDistance': 20.0}
episode index:834
target Thresh 31.993887428020813
target distance 25.0
model initialize at round 834
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.64347812, 15.07790579]), 'previousTarget': array([13.14936344, 14.57225358]), 'currentState': array([27.5, 29.5,  0. ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13290570350389924
{'scaleFactor': 20, 'currentTarget': array([62.95309439, 64.76327134]), 'previousTarget': array([62.45373998, 64.2626344 ]), 'currentState': array([77., 79.,  0.]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 20.0}
episode index:835
target Thresh 31.993948249128227
target distance 11.0
model initialize at round 835
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 21.]), 'previousTarget': array([19., 21.]), 'currentState': array([ 8.5       , 17.50002217,  0.        ]), 'targetState': array([19, 21], dtype=int32), 'currentDistance': 11.067964798913833}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.1337349653703247
{'scaleFactor': 20, 'currentTarget': array([19., 21.]), 'previousTarget': array([19., 21.]), 'currentState': array([18.        , 21.41205733,  0.        ]), 'targetState': array([19, 21], dtype=int32), 'currentDistance': 1.081568878659252}
episode index:836
target Thresh 31.994008465055508
target distance 12.0
model initialize at round 836
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 21.]), 'previousTarget': array([ 2., 21.]), 'currentState': array([8.5, 9.5, 0. ]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 13.209844813622876}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13357518643917735
{'scaleFactor': 20, 'currentTarget': array([41.45048451, 47.76997163]), 'previousTarget': array([40.92822063, 47.30285177]), 'currentState': array([58., 59.,  0.]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 20.0}
episode index:837
target Thresh 31.994068081824306
target distance 3.0
model initialize at round 837
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 15.]), 'previousTarget': array([19., 15.]), 'currentState': array([22.5       , 16.49977908,  0.        ]), 'targetState': array([19, 15], dtype=int32), 'currentDistance': 3.80779953187673}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13341578884199454
{'scaleFactor': 20, 'currentTarget': array([57.58851954, 52.13210631]), 'previousTarget': array([57.08593114, 51.63479673]), 'currentState': array([72.        , 65.99966669,  0.        ]), 'targetState': array([19, 15], dtype=int32), 'currentDistance': 20.0}
episode index:838
target Thresh 31.994127105396338
target distance 6.0
model initialize at round 838
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([10.5       ,  9.50055391,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 5.701022892841494}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.13434558795837312
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.        ,  7.82228117,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.0156692294408283}
episode index:839
target Thresh 31.99418554167402
target distance 6.0
model initialize at round 839
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 17.]), 'previousTarget': array([19., 17.]), 'currentState': array([21.5, 11.5,  0. ]), 'targetState': array([19, 17], dtype=int32), 'currentDistance': 6.041522986797246}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13418565273461314
{'scaleFactor': 20, 'currentTarget': array([55.73227429, 48.08115517]), 'previousTarget': array([55.22102674, 47.59445948]), 'currentState': array([71., 61.,  0.]), 'targetState': array([19, 17], dtype=int32), 'currentDistance': 20.0}
episode index:840
target Thresh 31.994243396501023
target distance 3.0
model initialize at round 840
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([19.5       , 20.49950397,  0.        ]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 4.300874340965661}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1340260978562129
{'scaleFactor': 20, 'currentTarget': array([54.72372504, 55.99254742]), 'previousTarget': array([54.22244158, 55.4938557 ]), 'currentState': array([69.        , 69.99925914,  0.        ]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 20.0}
episode index:841
target Thresh 31.994300675662878
target distance 7.0
model initialize at round 841
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 26.]), 'previousTarget': array([16., 26.]), 'currentState': array([20.5, 19.5,  0. ]), 'targetState': array([16, 26], dtype=int32), 'currentDistance': 7.905694150420929}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1338669219680226
{'scaleFactor': 20, 'currentTarget': array([54.35439711, 56.54146436]), 'previousTarget': array([53.83987962, 56.05971746]), 'currentState': array([70., 69.,  0.]), 'targetState': array([16, 26], dtype=int32), 'currentDistance': 20.0}
episode index:842
target Thresh 31.994357384887547
target distance 18.0
model initialize at round 842
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5.51644227, 21.21410959]), 'previousTarget': array([ 5.57099981, 21.06563667]), 'currentState': array([16.5,  4.5,  0. ]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13370812372132268
{'scaleFactor': 20, 'currentTarget': array([48.28905549, 44.70901272]), 'previousTarget': array([47.76046713, 44.26371429]), 'currentState': array([66., 54.,  0.]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 20.0}
episode index:843
target Thresh 31.994413529846007
target distance 10.0
model initialize at round 843
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 22.]), 'previousTarget': array([21., 22.]), 'currentState': array([11.5       , 22.50000009,  0.        ]), 'targetState': array([21, 22], dtype=int32), 'currentDistance': 9.51314879991927}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.1345484496332452
{'scaleFactor': 20, 'currentTarget': array([21., 22.]), 'previousTarget': array([21., 22.]), 'currentState': array([20.        , 21.48614595,  0.        ]), 'targetState': array([21, 22], dtype=int32), 'currentDistance': 1.1242979960051205}
episode index:844
target Thresh 31.994469116152793
target distance 16.0
model initialize at round 844
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([7.5, 3.5, 0. ]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 16.140012391568973}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.13527344802933794
{'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([22.        ,  7.75660581,  0.        ]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 1.0291942144372772}
episode index:845
target Thresh 31.994524149366587
target distance 13.0
model initialize at round 845
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([20.5, 13.5,  0. ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 13.729530217746072}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13511355033663186
{'scaleFactor': 20, 'currentTarget': array([54.5755395 , 50.26869927]), 'previousTarget': array([54.06496027, 49.78152727]), 'currentState': array([70., 63.,  0.]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 20.0}
episode index:846
target Thresh 31.994578634990756
target distance 18.0
model initialize at round 846
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 23.]), 'previousTarget': array([25., 23.]), 'currentState': array([ 7.5, 17.5,  0. ]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 18.34393632784417}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.13580140981982153
{'scaleFactor': 20, 'currentTarget': array([25., 23.]), 'previousTarget': array([25., 23.]), 'currentState': array([24.       , 22.4059525,  0.       ]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 1.163139042673449}
episode index:847
target Thresh 31.994632578473905
target distance 5.0
model initialize at round 847
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 16.]), 'previousTarget': array([10., 16.]), 'currentState': array([10.5      , 20.9732846,  0.       ]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 4.998355703042019}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1356412666478642
{'scaleFactor': 20, 'currentTarget': array([46.26325364, 54.37377155]), 'previousTarget': array([45.76728429, 53.86996363]), 'currentState': array([60.        , 68.90999525,  0.        ]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 20.0}
episode index:848
target Thresh 31.994685985210428
target distance 4.0
model initialize at round 848
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([17.5,  7.5,  0. ]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 4.301162633521249}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.1366016303501635
{'scaleFactor': 20, 'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([20.        ,  9.32833454,  0.        ]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 1.204630437098006}
episode index:849
target Thresh 31.99473886054104
target distance 14.0
model initialize at round 849
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([3.5, 7.5, 0. ]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 18.398369492974105}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.1373468556624635
{'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([16.5       , 20.49985135,  0.        ]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 0.7072119032379326}
episode index:850
target Thresh 31.994791209753327
target distance 5.0
model initialize at round 850
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 17.]), 'previousTarget': array([11., 17.]), 'currentState': array([10.5, 12.5,  0. ]), 'targetState': array([11, 17], dtype=int32), 'currentDistance': 4.527692569068621}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1371854610024606
{'scaleFactor': 20, 'currentTarget': array([45.26940181, 48.47189766]), 'previousTarget': array([44.76322472, 47.97862692]), 'currentState': array([60.       , 61.9999972,  0.       ]), 'targetState': array([11, 17], dtype=int32), 'currentDistance': 20.0}
episode index:851
target Thresh 31.994843038082248
target distance 23.0
model initialize at round 851
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7.62144993, 8.14371212]), 'previousTarget': array([7.5731765, 8.2957649]), 'currentState': array([11.5       , 27.76402938,  0.        ]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13702444520316193
{'scaleFactor': 20, 'currentTarget': array([48.72535198, 58.67608807]), 'previousTarget': array([48.24125819, 58.16373645]), 'currentState': array([61.       , 74.4663704,  0.       ]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 20.0}
episode index:852
target Thresh 31.994894350710677
target distance 12.0
model initialize at round 852
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 18.]), 'previousTarget': array([ 7., 18.]), 'currentState': array([19.5, 14.5,  0. ]), 'targetState': array([ 7, 18], dtype=int32), 'currentDistance': 12.980754985747224}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13686380693211483
{'scaleFactor': 20, 'currentTarget': array([52.93803944, 52.08306152]), 'previousTarget': array([52.42191204, 51.60482923]), 'currentState': array([69., 64.,  0.]), 'targetState': array([ 7, 18], dtype=int32), 'currentDistance': 20.0}
episode index:853
target Thresh 31.994945152769926
target distance 22.0
model initialize at round 853
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7.07201987, 24.24906181]), 'previousTarget': array([ 6.70472358, 24.26234812]), 'currentState': array([26.5, 19.5,  0. ]), 'targetState': array([ 4, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13670354486310768
{'scaleFactor': 20, 'currentTarget': array([58.93436333, 58.57099981]), 'previousTarget': array([58.41372202, 58.10485186]), 'currentState': array([76., 69.,  0.]), 'targetState': array([ 4, 25], dtype=int32), 'currentDistance': 20.0}
episode index:854
target Thresh 31.994995449340237
target distance 16.0
model initialize at round 854
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 16.]), 'previousTarget': array([27., 16.]), 'currentState': array([11.5     ,  7.500229,  0.      ]), 'targetState': array([27, 16], dtype=int32), 'currentDistance': 17.677559419633425}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.13741754316657961
{'scaleFactor': 20, 'currentTarget': array([27., 16.]), 'previousTarget': array([27., 16.]), 'currentState': array([26.        , 15.17132895,  0.        ]), 'targetState': array([27, 16], dtype=int32), 'currentDistance': 1.2987284960464403}
episode index:855
target Thresh 31.995045245451312
target distance 5.0
model initialize at round 855
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 19.]), 'previousTarget': array([20., 19.]), 'currentState': array([25.5       , 21.49964711,  0.        ]), 'targetState': array([20, 19], dtype=int32), 'currentDistance': 6.041376968707941}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13725700865353455
{'scaleFactor': 20, 'currentTarget': array([60.46698473, 57.25923626]), 'previousTarget': array([59.96335301, 56.76307857]), 'currentState': array([75.        , 70.99937698,  0.        ]), 'targetState': array([20, 19], dtype=int32), 'currentDistance': 20.0}
episode index:856
target Thresh 31.995094546082804
target distance 3.0
model initialize at round 856
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([17.5, 17.5,  0. ]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 2.5495097567963283}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1370968487834604
{'scaleFactor': 20, 'currentTarget': array([52.42487057, 53.28683089]), 'previousTarget': array([51.9204358 , 52.79155203]), 'currentState': array([67.        , 66.98229006,  0.        ]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 20.0}
episode index:857
target Thresh 31.995143356164817
target distance 12.0
model initialize at round 857
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 17.]), 'previousTarget': array([20., 17.]), 'currentState': array([13.5, 28.5,  0. ]), 'targetState': array([20, 17], dtype=int32), 'currentDistance': 13.2098448136228}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13693706224641675
{'scaleFactor': 20, 'currentTarget': array([48.76160042, 45.37136087]), 'previousTarget': array([48.26046411, 44.87251305]), 'currentState': array([63.        , 59.41657277,  0.        ]), 'targetState': array([20, 17], dtype=int32), 'currentDistance': 20.0}
episode index:858
target Thresh 31.995191680578397
target distance 18.0
model initialize at round 858
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 29.]), 'previousTarget': array([26., 29.]), 'currentState': array([23.5, 11.5,  0. ]), 'targetState': array([26, 29], dtype=int32), 'currentDistance': 17.6776695296636}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13677764773856294
{'scaleFactor': 20, 'currentTarget': array([56.46801505, 49.74418046]), 'previousTarget': array([55.94162619, 49.2830371 ]), 'currentState': array([73., 61.,  0.]), 'targetState': array([26, 29], dtype=int32), 'currentDistance': 20.0}
episode index:859
target Thresh 31.99523952415603
target distance 13.0
model initialize at round 859
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  3.]), 'previousTarget': array([24.,  3.]), 'currentState': array([12.5, 15.5,  0. ]), 'targetState': array([24,  3], dtype=int32), 'currentDistance': 16.985287751462916}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.13753218319561952
{'scaleFactor': 20, 'currentTarget': array([24.,  3.]), 'previousTarget': array([24.,  3.]), 'currentState': array([24.5       ,  3.74539998,  0.        ]), 'targetState': array([24,  3], dtype=int32), 'currentDistance': 0.8975639992638511}
episode index:860
target Thresh 31.99528689168211
target distance 21.0
model initialize at round 860
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.20985627, 19.44759085]), 'previousTarget': array([ 9.63513716, 19.07722123]), 'currentState': array([27.5, 29.5,  0. ]), 'targetState': array([ 6, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13737244779121113
{'scaleFactor': 20, 'currentTarget': array([61.93532942, 65.84493271]), 'previousTarget': array([61.42861854, 65.35262232]), 'currentState': array([77.        , 78.99999639,  0.        ]), 'targetState': array([ 6, 17], dtype=int32), 'currentDistance': 20.0}
episode index:861
target Thresh 31.995333787893433
target distance 6.0
model initialize at round 861
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([19.5,  9.5,  0. ]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 5.5226805085935435}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13721308300259025
{'scaleFactor': 20, 'currentTarget': array([51.99020154, 34.78456011]), 'previousTarget': array([51.61523304, 35.07105074]), 'currentState': array([69.        , 45.30438692,  0.        ]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 20.0}
episode index:862
target Thresh 31.995380217479653
target distance 14.0
model initialize at round 862
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([13.5       , 24.50544748,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 15.44820738975147}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13705408754140533
{'scaleFactor': 20, 'currentTarget': array([48.88710907, 54.06466679]), 'previousTarget': array([48.38754769, 53.56567197]), 'currentState': array([63.        , 68.23598675,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 20.0}
episode index:863
target Thresh 31.995426185083772
target distance 22.0
model initialize at round 863
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.71211   ,  8.76925948]), 'previousTarget': array([10.57770876,  9.3226018 ]), 'currentState': array([ 7.5       , 28.50963306,  0.        ]), 'targetState': array([11,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13689546012526946
{'scaleFactor': 20, 'currentTarget': array([43.81236146, 44.41170228]), 'previousTarget': array([43.32247281, 43.90316131]), 'currentState': array([57.        , 59.44786501,  0.        ]), 'targetState': array([11,  7], dtype=int32), 'currentDistance': 20.0}
episode index:864
target Thresh 31.99547169530259
target distance 11.0
model initialize at round 864
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([ 3.5, 20.5,  0. ]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 12.34908903522843}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.13769230771337387
{'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'previousTarget': array([14., 14.]), 'currentState': array([13.       , 13.2460045,  0.       ]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 1.2524013792637727}
episode index:865
target Thresh 31.995516752687163
target distance 18.0
model initialize at round 865
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 27.]), 'previousTarget': array([ 6., 27.]), 'currentState': array([24.5, 22.5,  0. ]), 'targetState': array([ 6, 27], dtype=int32), 'currentDistance': 19.039432764659832}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13753330966751548
{'scaleFactor': 20, 'currentTarget': array([54.16972138, 33.31568212]), 'previousTarget': array([53.67113191, 33.27656495]), 'currentState': array([74.        , 35.91569168,  0.        ]), 'targetState': array([ 6, 27], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:866
target Thresh 31.99556136174327
target distance 9.0
model initialize at round 866
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([17.5,  9.5,  0. ]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 8.631338250815942}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13737467839915618
{'scaleFactor': 20, 'currentTarget': array([47.22780624, 22.29729182]), 'previousTarget': array([46.72926975, 22.23517038]), 'currentState': array([67.        , 25.30733397,  0.        ]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 20.0}
episode index:867
target Thresh 31.99560552693185
target distance 22.0
model initialize at round 867
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.13536265, 23.43779978]), 'previousTarget': array([17.73750984, 22.87322975]), 'currentState': array([7.5, 6.5, 0. ]), 'targetState': array([21, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1372164126406318
{'scaleFactor': 20, 'currentTarget': array([37.33707439, 31.03795544]), 'previousTarget': array([36.84086838, 30.96264501]), 'currentState': array([57.        , 34.69436849,  0.        ]), 'targetState': array([21, 28], dtype=int32), 'currentDistance': 20.0}
episode index:868
target Thresh 31.995649252669466
target distance 20.0
model initialize at round 868
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.98782391, 17.00280987]), 'previousTarget': array([24.40285  , 17.1492875]), 'currentState': array([ 5.5, 21.5,  0. ]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.13785189327725833
{'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([24.        , 16.68413989,  0.        ]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 1.0486980543971602}
episode index:869
target Thresh 31.99569254332872
target distance 18.0
model initialize at round 869
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  6.]), 'previousTarget': array([24.88854382,  6.05572809]), 'currentState': array([ 7.5, 14.5,  0. ]), 'targetState': array([25,  6], dtype=int32), 'currentDistance': 19.455076458343687}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.13851842044889168
{'scaleFactor': 20, 'currentTarget': array([25.,  6.]), 'previousTarget': array([25.,  6.]), 'currentState': array([24.        ,  5.73043394,  0.        ]), 'targetState': array([25,  6], dtype=int32), 'currentDistance': 1.035695833833885}
episode index:870
target Thresh 31.995735403238715
target distance 17.0
model initialize at round 870
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.11799019, 15.76006461]), 'previousTarget': array([11.00324289, 15.76756726]), 'currentState': array([26.5       , 28.54262203,  0.        ]), 'targetState': array([ 9, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13835938667110878
{'scaleFactor': 20, 'currentTarget': array([56.27871977, 21.97675237]), 'previousTarget': array([55.77921602, 21.89964967]), 'currentState': array([76.        , 25.30407955,  0.        ]), 'targetState': array([ 9, 14], dtype=int32), 'currentDistance': 20.0}
episode index:871
target Thresh 31.995777836685484
target distance 18.0
model initialize at round 871
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.21410959, 18.48355773]), 'previousTarget': array([19.64100589, 18.09400392]), 'currentState': array([3.5, 7.5, 0. ]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.13902380312286014
{'scaleFactor': 20, 'currentTarget': array([21., 19.]), 'previousTarget': array([21., 19.]), 'currentState': array([20.        , 18.55158033,  0.        ]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 1.0959380457482548}
episode index:872
target Thresh 31.9958198479124
target distance 16.0
model initialize at round 872
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.64213562, 17.64213562]), 'previousTarget': array([20.14213562, 17.14213562]), 'currentState': array([6.5, 3.5, 0. ]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.1397118633408045
{'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([21.5      , 18.4937216,  0.       ]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 0.7115601266942684}
episode index:873
target Thresh 31.995861441120628
target distance 8.0
model initialize at round 873
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 5.]), 'previousTarget': array([6., 5.]), 'currentState': array([14.5      , 11.5001438,  0.       ]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 10.700554629146577}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13955200995025438
{'scaleFactor': 20, 'currentTarget': array([44.20139304, 10.46261677]), 'previousTarget': array([43.7049047 , 10.43914941]), 'currentState': array([64.        , 13.29372302,  0.        ]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 20.0}
episode index:874
target Thresh 31.995902620469515
target distance 11.0
model initialize at round 874
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 13.]), 'previousTarget': array([ 3., 13.]), 'currentState': array([13.5      , 23.5002501,  0.       ]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 14.84941925414707}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13939252193888266
{'scaleFactor': 20, 'currentTarget': array([43.19123328, 18.59815596]), 'previousTarget': array([42.68964698, 18.50498328]), 'currentState': array([63.        , 21.35727919,  0.        ]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 20.0}
episode index:875
target Thresh 31.995943390077038
target distance 18.0
model initialize at round 875
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5.74045326, 7.10077208]), 'previousTarget': array([5.72118773, 7.21295565]), 'currentState': array([18.5       , 22.50187486,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13923339805539078
{'scaleFactor': 20, 'currentTarget': array([48.23120956, 11.78457736]), 'previousTarget': array([47.74427495, 11.9002933 ]), 'currentState': array([68.        , 14.81689073,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 20.0}
episode index:876
target Thresh 31.995983754020184
target distance 22.0
model initialize at round 876
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.71221616,  6.35579465]), 'previousTarget': array([10.73765188,  6.70472358]), 'currentState': array([16.5       , 25.50002506,  0.        ]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13907463705418738
{'scaleFactor': 20, 'currentTarget': array([48.08790692, 22.91843546]), 'previousTarget': array([47.5632785, 22.5280302]), 'currentState': array([66.        , 31.81545302,  0.        ]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 20.0}
episode index:877
target Thresh 31.996023716335387
target distance 14.0
model initialize at round 877
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.5       , 24.50000346,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 13.509259540806976}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13891623769535572
{'scaleFactor': 20, 'currentTarget': array([34.5102171 , 29.03215185]), 'previousTarget': array([34.00445672, 28.7267703 ]), 'currentState': array([52.        , 38.73305357,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 20.0}
episode index:878
target Thresh 31.996063281018913
target distance 4.0
model initialize at round 878
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 18.]), 'previousTarget': array([23., 18.]), 'currentState': array([20.5       , 21.50000086,  0.        ]), 'targetState': array([23, 18], dtype=int32), 'currentDistance': 4.3011633368045095}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.13982927968819492
{'scaleFactor': 20, 'currentTarget': array([23., 18.]), 'previousTarget': array([23., 18.]), 'currentState': array([23.5       , 18.58304632,  0.        ]), 'targetState': array([23, 18], dtype=int32), 'currentDistance': 0.7680774751838941}
episode index:879
target Thresh 31.996102452027255
target distance 24.0
model initialize at round 879
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8.41919652, 21.55318422]), 'previousTarget': array([ 8.4, 21.2]), 'currentState': array([14.5,  2.5,  0. ]), 'targetState': array([ 7, 26], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13967038277945834
{'scaleFactor': 20, 'currentTarget': array([45.59918814, 42.43846147]), 'previousTarget': array([45.07336626, 42.06594809]), 'currentState': array([64.        , 50.27492258,  0.        ]), 'targetState': array([ 7, 26], dtype=int32), 'currentDistance': 20.0}
episode index:880
target Thresh 31.996141233277555
target distance 3.0
model initialize at round 880
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 16.]), 'previousTarget': array([24., 16.]), 'currentState': array([25.5, 13.5,  0. ]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 2.915475947422622}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13951184659015134
{'scaleFactor': 20, 'currentTarget': array([58.16732196, 37.92403455]), 'previousTarget': array([57.64578598, 37.49464798]), 'currentState': array([75.        , 48.72500442,  0.        ]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 20.0}
episode index:881
target Thresh 31.99617962864797
target distance 6.0
model initialize at round 881
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 22.]), 'previousTarget': array([25., 22.]), 'currentState': array([23.5, 16.5,  0. ]), 'targetState': array([25, 22], dtype=int32), 'currentDistance': 5.700877125495589}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1393536698933371
{'scaleFactor': 20, 'currentTarget': array([56.19155211, 42.11333544]), 'previousTarget': array([55.66941084, 41.68791463]), 'currentState': array([73.        , 52.95197372,  0.        ]), 'targetState': array([25, 22], dtype=int32), 'currentDistance': 20.0}
episode index:882
target Thresh 31.99621764197806
target distance 11.0
model initialize at round 882
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 24.]), 'previousTarget': array([ 5., 24.]), 'currentState': array([14.5, 13.5,  0. ]), 'targetState': array([ 5, 24], dtype=int32), 'currentDistance': 14.15980225850629}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13919585146763683
{'scaleFactor': 20, 'currentTarget': array([47.30884665, 51.92954373]), 'previousTarget': array([46.78656026, 51.4633885 ]), 'currentState': array([64.        , 62.94795558,  0.        ]), 'targetState': array([ 5, 24], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:883
target Thresh 31.9962552770692
target distance 14.0
model initialize at round 883
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 15.]), 'previousTarget': array([11., 15.]), 'currentState': array([25.5       , 17.48505622,  0.        ]), 'targetState': array([11, 15], dtype=int32), 'currentDistance': 14.71140728903748}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13903839009719832
{'scaleFactor': 20, 'currentTarget': array([59.47269037, 54.35197099]), 'previousTarget': array([58.9614921 , 53.86628509]), 'currentState': array([75.        , 66.95763066,  0.        ]), 'targetState': array([11, 15], dtype=int32), 'currentDistance': 20.0}
episode index:884
target Thresh 31.996292537684923
target distance 4.0
model initialize at round 884
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 8.]), 'previousTarget': array([7., 8.]), 'currentState': array([11.5       ,  6.49936658,  0.        ]), 'targetState': array([7, 8], dtype=int32), 'currentDistance': 4.74361683285022}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13888128457166476
{'scaleFactor': 20, 'currentTarget': array([44.36774033, 32.95437704]), 'previousTarget': array([43.84390092, 32.49018881]), 'currentState': array([61.        , 44.06148909,  0.        ]), 'targetState': array([7, 8], dtype=int32), 'currentDistance': 20.0}
episode index:885
target Thresh 31.996329427551327
target distance 20.0
model initialize at round 885
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.47366596, 17.82455532]), 'previousTarget': array([21.1565257 , 17.74695771]), 'currentState': array([ 2.5, 11.5,  0. ]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.13950269292527356
{'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'previousTarget': array([22., 18.]), 'currentState': array([21.        , 17.51600659,  0.        ]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 1.1109678778236896}
episode index:886
target Thresh 31.996365950357426
target distance 22.0
model initialize at round 886
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.36618652, 13.3236243 ]), 'previousTarget': array([20.82570003, 13.77104998]), 'currentState': array([ 5.5, 25.5,  0. ]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.140092072122634
{'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([26.        ,  8.40448052,  0.        ]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 1.163891513302067}
episode index:887
target Thresh 31.996402109755536
target distance 8.0
model initialize at round 887
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([15.5,  3.5,  0. ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 7.9056941504209775}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13993431078015356
{'scaleFactor': 20, 'currentTarget': array([48.39328602, 34.75350962]), 'previousTarget': array([47.86875769, 34.2901612 ]), 'currentState': array([65.        , 45.89877994,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 20.0}
episode index:888
target Thresh 31.996437909361617
target distance 27.0
model initialize at round 888
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.26443056, 21.92421688]), 'previousTarget': array([16.01924318, 21.35993796]), 'currentState': array([11.5,  2.5,  0. ]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13977690435632886
{'scaleFactor': 20, 'currentTarget': array([43.31366665, 42.36471146]), 'previousTarget': array([42.7731896 , 41.94178564]), 'currentState': array([61.        , 51.70246348,  0.        ]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 20.0}
episode index:889
target Thresh 31.99647335275567
target distance 16.0
model initialize at round 889
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  6.]), 'previousTarget': array([11.,  6.]), 'currentState': array([ 9.5       , 21.50000006,  0.        ]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 15.572411561724882}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1396198516548049
{'scaleFactor': 20, 'currentTarget': array([43.79941319, 34.0462462 ]), 'previousTarget': array([43.28803516, 33.56005623]), 'currentState': array([59.        , 47.04402142,  0.        ]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 20.0}
episode index:890
target Thresh 31.99650844348206
target distance 8.0
model initialize at round 890
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  7.]), 'previousTarget': array([18.,  7.]), 'currentState': array([20.5       , 14.50060272,  0.        ]), 'targetState': array([18,  7], dtype=int32), 'currentDistance': 7.906265945172455}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13946315148459748
{'scaleFactor': 20, 'currentTarget': array([53.3237187, 30.3868636]), 'previousTarget': array([52.79854201, 29.92499932]), 'currentState': array([70.        , 41.42777126,  0.        ]), 'targetState': array([18,  7], dtype=int32), 'currentDistance': 20.0}
episode index:891
target Thresh 31.996543185049887
target distance 23.0
model initialize at round 891
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.92798013,  7.75093819]), 'previousTarget': array([22.35234545,  7.95156206]), 'currentState': array([ 3.5, 12.5,  0. ]), 'targetState': array([26,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.14003449618962416
{'scaleFactor': 20, 'currentTarget': array([26.,  7.]), 'previousTarget': array([26.,  7.]), 'currentState': array([25.        ,  6.25569283,  0.        ]), 'targetState': array([26,  7], dtype=int32), 'currentDistance': 1.2465926199349178}
episode index:892
target Thresh 31.99657758093334
target distance 9.0
model initialize at round 892
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 23.]), 'previousTarget': array([13., 23.]), 'currentState': array([ 4.5, 26.5,  0. ]), 'targetState': array([13, 23], dtype=int32), 'currentDistance': 9.19238815542509}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.1408407939034558
{'scaleFactor': 20, 'currentTarget': array([13., 23.]), 'previousTarget': array([13., 23.]), 'currentState': array([12.        , 22.38321093,  0.        ]), 'targetState': array([13, 23], dtype=int32), 'currentDistance': 1.1749164907625846}
episode index:893
target Thresh 31.996611634572034
target distance 5.0
model initialize at round 893
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([3.5, 5.5, 0. ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 5.700877125495698}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.14171540676757713
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([7.5       , 9.49899381, 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.7078186197173227}
episode index:894
target Thresh 31.99664534937136
target distance 20.0
model initialize at round 894
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.50829159, 25.83391929]), 'previousTarget': array([14.57218647, 25.56953382]), 'currentState': array([22.5,  7.5,  0. ]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1415570655309653
{'scaleFactor': 20, 'currentTarget': array([54.21850825, 47.70750809]), 'previousTarget': array([53.68802691, 47.26695383]), 'currentState': array([72.        , 56.86275527,  0.        ]), 'targetState': array([14, 27], dtype=int32), 'currentDistance': 20.0}
episode index:895
target Thresh 31.99667872870283
target distance 11.0
model initialize at round 895
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 13.]), 'previousTarget': array([10., 13.]), 'currentState': array([ 2.5, 23.5,  0. ]), 'targetState': array([10, 13], dtype=int32), 'currentDistance': 12.90348790056388}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14139907773461377
{'scaleFactor': 20, 'currentTarget': array([34.153432  , 25.21808019]), 'previousTarget': array([33.61073496, 24.80298757]), 'currentState': array([52.        , 34.24581582,  0.        ]), 'targetState': array([10, 13], dtype=int32), 'currentDistance': 20.0}
episode index:896
target Thresh 31.996711775904398
target distance 11.0
model initialize at round 896
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 23.]), 'previousTarget': array([ 4., 23.]), 'currentState': array([12.5, 12.5,  0. ]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 13.509256086106284}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14124144219644807
{'scaleFactor': 20, 'currentTarget': array([45.10403125, 49.03477453]), 'previousTarget': array([44.57969235, 48.57330263]), 'currentState': array([62.        , 59.73646785,  0.        ]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 20.0}
episode index:897
target Thresh 31.996744494280822
target distance 21.0
model initialize at round 897
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.01234015, 28.49405381]), 'previousTarget': array([12., 28.]), 'currentState': array([12.5,  8.5,  0. ]), 'targetState': array([12, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14108415773965916
{'scaleFactor': 20, 'currentTarget': array([44.55915291, 47.2736993 ]), 'previousTarget': array([44.02620029, 46.83264723]), 'currentState': array([62.        , 57.06230763,  0.        ]), 'targetState': array([12, 29], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:898
target Thresh 31.99677688710396
target distance 16.0
model initialize at round 898
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.64213562, 22.64213562]), 'previousTarget': array([21.14213562, 22.14213562]), 'currentState': array([7.5, 8.5, 0. ]), 'targetState': array([23, 24], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.14175002672258308
{'scaleFactor': 20, 'currentTarget': array([23., 24.]), 'previousTarget': array([23., 24.]), 'currentState': array([22.5       , 23.49594712,  0.        ]), 'targetState': array([23, 24], dtype=int32), 'currentDistance': 0.7099783822320193}
episode index:899
target Thresh 31.99680895761312
target distance 16.0
model initialize at round 899
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  9.]), 'previousTarget': array([18.,  9.]), 'currentState': array([19.5       , 24.50013998,  0.        ]), 'targetState': array([18,  9], dtype=int32), 'currentDistance': 15.572550832999255}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14159252669289132
{'scaleFactor': 20, 'currentTarget': array([53.99101642, 40.69765099]), 'previousTarget': array([53.48252261, 40.20933887]), 'currentState': array([69.        , 53.91621415,  0.        ]), 'targetState': array([18,  9], dtype=int32), 'currentDistance': 20.0}
episode index:900
target Thresh 31.99684070901538
target distance 19.0
model initialize at round 900
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7.83088269, 21.31927723]), 'previousTarget': array([ 7.23313766, 21.08589282]), 'currentState': array([26.5      , 28.4931227,  0.       ]), 'targetState': array([ 7, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1414353762748082
{'scaleFactor': 20, 'currentTarget': array([60.57738212, 65.23563872]), 'previousTarget': array([60.06776185, 64.74746317]), 'currentState': array([76.        , 77.96917152,  0.        ]), 'targetState': array([ 7, 21], dtype=int32), 'currentDistance': 20.0}
episode index:901
target Thresh 31.996872144485913
target distance 16.0
model initialize at round 901
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.92303856, 27.19350923]), 'previousTarget': array([10.85786438, 27.14213562]), 'currentState': array([25.5, 13.5,  0. ]), 'targetState': array([ 9, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14127857430554566
{'scaleFactor': 20, 'currentTarget': array([56.43293766, 10.00852238]), 'previousTarget': array([55.93756866, 10.17295192]), 'currentState': array([75.        ,  2.57453313,  0.        ]), 'targetState': array([ 9, 29], dtype=int32), 'currentDistance': 20.0}
episode index:902
target Thresh 31.996903267168285
target distance 16.0
model initialize at round 902
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3.41116334, 13.54190985]), 'previousTarget': array([ 3., 13.]), 'currentState': array([15.5       , 29.47489539,  0.        ]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14112211962746643
{'scaleFactor': 20, 'currentTarget': array([51.25649422, 64.01746355]), 'previousTarget': array([50.76017218, 63.51835067]), 'currentState': array([65.        , 78.54729661,  0.        ]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 20.0}
episode index:903
target Thresh 31.996934080174796
target distance 18.0
model initialize at round 903
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.5       , 20.51943943,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 17.526572908243416}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1409660110880555
{'scaleFactor': 20, 'currentTarget': array([48.97964771, 32.87433991]), 'previousTarget': array([48.47043201, 32.38524256]), 'currentState': array([64.        , 46.07998329,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 20.0}
episode index:904
target Thresh 31.99696458658677
target distance 13.0
model initialize at round 904
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 23.]), 'previousTarget': array([27., 23.]), 'currentState': array([14.5, 28.5,  0. ]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 13.656500283747558}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.14168716940027165
{'scaleFactor': 20, 'currentTarget': array([27., 23.]), 'previousTarget': array([27., 23.]), 'currentState': array([26.        , 22.20232493,  0.        ]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 1.2791737654899085}
episode index:905
target Thresh 31.99699478945487
target distance 11.0
model initialize at round 905
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([10.5, 19.5,  0. ]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 13.509256086106182}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.14244266769435038
{'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([20.        , 10.19506925,  0.        ]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 1.2837108345528518}
episode index:906
target Thresh 31.997024691799414
target distance 19.0
model initialize at round 906
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.37211039,  4.1062817 ]), 'previousTarget': array([25.02072541,  4.69147429]), 'currentState': array([15.5, 21.5,  0. ]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14228561954915264
{'scaleFactor': 20, 'currentTarget': array([45.95771875, -3.40882645]), 'previousTarget': array([45.46264525, -3.26712836]), 'currentState': array([65.        , -9.52368743,  0.        ]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 20.0}
episode index:907
target Thresh 31.997054296610656
target distance 20.0
model initialize at round 907
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4.64704681, 10.92110152]), 'previousTarget': array([ 4.0992562 , 10.99007438]), 'currentState': array([24.5       ,  8.50030628,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14212891732497956
{'scaleFactor': 20, 'currentTarget': array([54.65226041, -2.26291547]), 'previousTarget': array([54.153182  , -2.14198174]), 'currentState': array([74.        , -7.32897634,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 20.0}
episode index:908
target Thresh 31.997083606849106
target distance 7.0
model initialize at round 908
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 11.]), 'previousTarget': array([20., 11.]), 'currentState': array([14.5,  4.5,  0. ]), 'targetState': array([20, 11], dtype=int32), 'currentDistance': 8.514693182963152}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.14294768075115244
{'scaleFactor': 20, 'currentTarget': array([20., 11.]), 'previousTarget': array([20., 11.]), 'currentState': array([20.5       , 10.49786907,  0.        ]), 'targetState': array([20, 11], dtype=int32), 'currentDistance': 0.708615175183529}
episode index:909
target Thresh 31.997112625445812
target distance 17.0
model initialize at round 909
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 25.]), 'previousTarget': array([21., 25.]), 'currentState': array([ 4.5, 15.5,  0. ]), 'targetState': array([21, 25], dtype=int32), 'currentDistance': 19.039432764659733}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.14358727707555544
{'scaleFactor': 20, 'currentTarget': array([21., 25.]), 'previousTarget': array([21., 25.]), 'currentState': array([20.49999994, 24.51676314,  0.        ]), 'targetState': array([21, 25], dtype=int32), 'currentDistance': 0.6953545278001628}
episode index:910
target Thresh 31.997141355302652
target distance 12.0
model initialize at round 910
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([24.5       , 19.56492591,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 15.739688931026215}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14342966206230015
{'scaleFactor': 20, 'currentTarget': array([58.01675394, 44.61296302]), 'previousTarget': array([57.52867392, 44.31652529]), 'currentState': array([74.        , 56.63526919,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 20.0}
episode index:911
target Thresh 31.997169799292642
target distance 9.0
model initialize at round 911
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  5.]), 'previousTarget': array([17.,  5.]), 'currentState': array([26.5       ,  2.50274423,  0.        ]), 'targetState': array([17,  5], dtype=int32), 'currentDistance': 9.822743323153059}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14327239269600378
{'scaleFactor': 20, 'currentTarget': array([56.5315267 , -4.30035225]), 'previousTarget': array([56.02583921, -4.13011898]), 'currentState': array([76.        , -8.88058668,  0.        ]), 'targetState': array([17,  5], dtype=int32), 'currentDistance': 20.0}
episode index:912
target Thresh 31.997197960260205
target distance 14.0
model initialize at round 912
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 18.]), 'previousTarget': array([ 2., 18.]), 'currentState': array([6.5, 4.5, 0. ]), 'targetState': array([ 2, 18], dtype=int32), 'currentDistance': 14.230249470757709}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14311546784091506
{'scaleFactor': 20, 'currentTarget': array([36.67788455,  8.73395873]), 'previousTarget': array([36.17244295,  8.90766831]), 'currentState': array([56.       ,  3.5710279,  0.       ]), 'targetState': array([ 2, 18], dtype=int32), 'currentDistance': 20.0}
episode index:913
target Thresh 31.997225841021457
target distance 4.0
model initialize at round 913
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  8.]), 'previousTarget': array([19.,  8.]), 'currentState': array([17.5,  4.5,  0. ]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 3.8078865529319263}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14295888636625323
{'scaleFactor': 20, 'currentTarget': array([48.00175775, -1.54145863]), 'previousTarget': array([47.49705798, -1.3516463 ]), 'currentState': array([67.        , -7.79180194,  0.        ]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 20.0}
episode index:914
target Thresh 31.9972534443645
target distance 5.0
model initialize at round 914
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 23.]), 'previousTarget': array([22., 23.]), 'currentState': array([18.5       , 27.50000161,  0.        ]), 'targetState': array([22, 23], dtype=int32), 'currentDistance': 5.700878395820182}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.1438111112930966
{'scaleFactor': 20, 'currentTarget': array([22., 23.]), 'previousTarget': array([22., 23.]), 'currentState': array([22.5       , 23.98267931,  0.        ]), 'targetState': array([22, 23], dtype=int32), 'currentDistance': 1.1025691004699465}
episode index:915
target Thresh 31.99728077304969
target distance 1.0
model initialize at round 915
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 24.]), 'previousTarget': array([15., 24.]), 'currentState': array([14.5       , 24.15338401,  0.        ]), 'targetState': array([15, 24], dtype=int32), 'currentDistance': 0.5229977591218509}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.1447458153200692
{'scaleFactor': 20, 'currentTarget': array([15., 24.]), 'previousTarget': array([15., 24.]), 'currentState': array([14.5       , 24.15338401,  0.        ]), 'targetState': array([15, 24], dtype=int32), 'currentDistance': 0.5229977591218509}
episode index:916
target Thresh 31.99730782980992
target distance 4.0
model initialize at round 916
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  3.]), 'previousTarget': array([10.,  3.]), 'currentState': array([6.5       , 2.50013426, 0.        ]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 3.535514921275723}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.1456250347689023
{'scaleFactor': 20, 'currentTarget': array([10.,  3.]), 'previousTarget': array([10.,  3.]), 'currentState': array([9.        , 2.57038522, 0.        ]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 1.0883790062285095}
episode index:917
target Thresh 31.997334617350887
target distance 4.0
model initialize at round 917
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 16.]), 'previousTarget': array([19., 16.]), 'currentState': array([17.5       , 19.50138876,  0.        ]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 3.8091630626158732}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14546640183342416
{'scaleFactor': 20, 'currentTarget': array([47.66494677,  8.41790587]), 'previousTarget': array([47.16903814,  8.52499705]), 'currentState': array([66.99999973,  3.30363886,  0.        ]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 20.0}
episode index:918
target Thresh 31.99736113835137
target distance 5.0
model initialize at round 918
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 20.]), 'previousTarget': array([ 4., 20.]), 'currentState': array([ 7.5       , 24.51221135,  0.        ]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 5.710521104979162}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14530811412740302
{'scaleFactor': 20, 'currentTarget': array([37.37443965, 13.44955897]), 'previousTarget': array([36.87760598, 13.5190603 ]), 'currentState': array([57.        ,  9.59762685,  0.        ]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 20.0}
episode index:919
target Thresh 31.997387395463484
target distance 10.0
model initialize at round 919
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 27.]), 'previousTarget': array([11., 27.]), 'currentState': array([ 6.5, 17.5,  0. ]), 'targetState': array([11, 27], dtype=int32), 'currentDistance': 10.511898020814247}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14515017052509063
{'scaleFactor': 20, 'currentTarget': array([36.37745623, 21.99853476]), 'previousTarget': array([35.87689223, 22.10095759]), 'currentState': array([56.        , 18.13126493,  0.        ]), 'targetState': array([11, 27], dtype=int32), 'currentDistance': 20.0}
episode index:920
target Thresh 31.99741339131297
target distance 12.0
model initialize at round 920
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([21.5       , 21.50108564,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 12.74852818468111}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.144992569905628
{'scaleFactor': 20, 'currentTarget': array([55.1144137 , 39.91903478]), 'previousTarget': array([54.6010111 , 39.45884771]), 'currentState': array([71.       , 52.0700902,  0.       ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 20.0}
episode index:921
target Thresh 31.997439128499433
target distance 13.0
model initialize at round 921
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 28.]), 'previousTarget': array([22., 28.]), 'currentState': array([ 9.5, 24.5,  0. ]), 'targetState': array([22, 28], dtype=int32), 'currentDistance': 12.980754985747115}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.14569606417215514
{'scaleFactor': 20, 'currentTarget': array([22., 28.]), 'previousTarget': array([22., 28.]), 'currentState': array([21.        , 28.17456524,  0.        ]), 'targetState': array([22, 28], dtype=int32), 'currentDistance': 1.015122171598521}
episode index:922
target Thresh 31.99746460959661
target distance 9.0
model initialize at round 922
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([11.5,  6.5,  0. ]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 12.020815280171254}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.14647002114991153
{'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([19., 14.,  0.]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 1.414213562371481}
episode index:923
target Thresh 31.997489837152635
target distance 23.0
model initialize at round 923
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7.16961979, 22.45570316]), 'previousTarget': array([ 7.13125551, 21.98112317]), 'currentState': array([8.5, 2.5, 0. ]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1463115038110047
{'scaleFactor': 20, 'currentTarget': array([38.54870846, 17.45369106]), 'previousTarget': array([38.05157184, 17.55242345]), 'currentState': array([58.        , 12.80102975,  0.        ]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 20.0}
episode index:924
target Thresh 31.997514813690284
target distance 3.0
model initialize at round 924
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 18.]), 'previousTarget': array([27., 18.]), 'currentState': array([25.5, 15.5,  0. ]), 'targetState': array([27, 18], dtype=int32), 'currentDistance': 2.9154759474225416}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.147202301104182
{'scaleFactor': 20, 'currentTarget': array([27., 18.]), 'previousTarget': array([27., 18.]), 'currentState': array([27., 17.,  0.]), 'targetState': array([27, 18], dtype=int32), 'currentDistance': 0.9999999999996803}
episode index:925
target Thresh 31.99753954170723
target distance 4.0
model initialize at round 925
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 16.]), 'previousTarget': array([ 5., 16.]), 'currentState': array([ 9.5, 12.5,  0. ]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 5.700877125495704}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1470433353362509
{'scaleFactor': 20, 'currentTarget': array([39.70901778,  6.50351276]), 'previousTarget': array([39.20047477,  6.70230254]), 'currentState': array([59.        ,  1.22544446,  0.        ]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 20.0}
episode index:926
target Thresh 31.997564023676293
target distance 15.0
model initialize at round 926
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 26.]), 'previousTarget': array([11., 26.]), 'currentState': array([26.5, 22.5,  0. ]), 'targetState': array([11, 26], dtype=int32), 'currentDistance': 15.890248582070797}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14688471253653543
{'scaleFactor': 20, 'currentTarget': array([56.71651252, 13.42220764]), 'previousTarget': array([56.21527753, 13.5714363 ]), 'currentState': array([76.        ,  8.11682259,  0.        ]), 'targetState': array([11, 26], dtype=int32), 'currentDistance': 20.0}
episode index:927
target Thresh 31.9975882620457
target distance 6.0
model initialize at round 927
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'previousTarget': array([10., 12.]), 'currentState': array([15.5       , 17.50472772,  0.        ]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 7.781518314873148}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1467264315963021
{'scaleFactor': 20, 'currentTarget': array([45.43618578,  4.47548921]), 'previousTarget': array([44.93665742,  4.57741466]), 'currentState': array([65.       ,  0.3213138,  0.       ]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 20.0}
episode index:928
target Thresh 31.997612259239297
target distance 7.0
model initialize at round 928
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([11.5,  7.5,  0. ]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 7.905694150420906}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14656849141159134
{'scaleFactor': 20, 'currentTarget': array([41.47739038,  8.33138828]), 'previousTarget': array([40.96493777,  8.52096198]), 'currentState': array([61.        ,  3.98769011,  0.        ]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 20.0}
episode index:929
target Thresh 31.99763601765683
target distance 11.0
model initialize at round 929
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 5.]), 'previousTarget': array([8., 5.]), 'currentState': array([19.5       ,  9.50000003,  0.        ]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 12.349089046088467}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14641089088319176
{'scaleFactor': 20, 'currentTarget': array([49.69934165, -6.32665535]), 'previousTarget': array([49.20075829, -6.20317635]), 'currentState': array([ 69.        , -11.56922985,   0.        ]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 20.0}
episode index:930
target Thresh 31.99765953967416
target distance 10.0
model initialize at round 930
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 15.]), 'previousTarget': array([26., 15.]), 'currentState': array([22.5       , 25.48145956,  0.        ]), 'targetState': array([26, 15], dtype=int32), 'currentDistance': 11.050384358301915}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14625362891661475
{'scaleFactor': 20, 'currentTarget': array([52.85235042,  6.89918393]), 'previousTarget': array([52.35114237,  7.0564025 ]), 'currentState': array([72.        ,  1.12272173,  0.        ]), 'targetState': array([26, 15], dtype=int32), 'currentDistance': 20.0}
episode index:931
target Thresh 31.997682827643505
target distance 17.0
model initialize at round 931
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([24.5       , 24.52316219,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 18.581304815046664}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14609670442206904
{'scaleFactor': 20, 'currentTarget': array([54.6103194 , -1.76343315]), 'previousTarget': array([54.10510127, -1.59245192]), 'currentState': array([74.        , -6.66652261,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 20.0}
episode index:932
target Thresh 31.997705883893683
target distance 8.0
model initialize at round 932
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([21.5       ,  6.50000015,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 8.860022531288681}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14594011631443551
{'scaleFactor': 20, 'currentTarget': array([51.21608949,  3.33624234]), 'previousTarget': array([50.72074343,  3.34877626]), 'currentState': array([71.        ,  0.40419785,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 20.0}
episode index:933
target Thresh 31.99772871073034
target distance 16.0
model initialize at round 933
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([25.5,  2.5,  0. ]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 18.721645226849102}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14578386351324235
{'scaleFactor': 20, 'currentTarget': array([55.10520682, 13.87008248]), 'previousTarget': array([54.6051608 , 13.92247492]), 'currentState': array([75.        , 11.82137456,  0.        ]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 20.0}
episode index:934
target Thresh 31.997751310436175
target distance 13.0
model initialize at round 934
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 17.]), 'previousTarget': array([20., 17.]), 'currentState': array([14.5,  4.5,  0. ]), 'targetState': array([20, 17], dtype=int32), 'currentDistance': 13.656500283747635}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14562794494263995
{'scaleFactor': 20, 'currentTarget': array([44.05708857, 15.17841714]), 'previousTarget': array([43.55693158, 15.2187533 ]), 'currentState': array([64.        , 13.66835636,  0.        ]), 'targetState': array([20, 17], dtype=int32), 'currentDistance': 20.0}
episode index:935
target Thresh 31.997773685271184
target distance 9.0
model initialize at round 935
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([18.5       , 21.53363973,  0.        ]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 9.612502922980097}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14547235953137644
{'scaleFactor': 20, 'currentTarget': array([48.06152426, 19.92902563]), 'previousTarget': array([47.56187362, 19.95967235]), 'currentState': array([68.        , 18.36148445,  0.        ]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 20.0}
episode index:936
target Thresh 31.99779583747286
target distance 20.0
model initialize at round 936
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 26.]), 'previousTarget': array([24.03319094, 25.77872706]), 'currentState': array([27.5,  6.5,  0. ]), 'targetState': array([24, 26], dtype=int32), 'currentDistance': 19.811612756158965}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14531710621277305
{'scaleFactor': 20, 'currentTarget': array([57.04403765, 23.80353998]), 'previousTarget': array([56.54469603, 23.82056673]), 'currentState': array([77.        , 22.47705337,  0.        ]), 'targetState': array([24, 26], dtype=int32), 'currentDistance': 20.0}
episode index:937
target Thresh 31.99781776925645
target distance 7.0
model initialize at round 937
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 27.]), 'previousTarget': array([22., 27.]), 'currentState': array([22.5, 20.5,  0. ]), 'targetState': array([22, 27], dtype=int32), 'currentDistance': 6.519202405202577}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14516218392469973
{'scaleFactor': 20, 'currentTarget': array([52.04242498, 25.04008327]), 'previousTarget': array([51.54468486, 25.0217177 ]), 'currentState': array([72.      , 23.738085,  0.      ]), 'targetState': array([22, 27], dtype=int32), 'currentDistance': 20.0}
episode index:938
target Thresh 31.997839482815145
target distance 15.0
model initialize at round 938
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 21.]), 'previousTarget': array([24., 21.]), 'currentState': array([ 9.5, 14.5,  0. ]), 'targetState': array([24, 21], dtype=int32), 'currentDistance': 15.890248582070598}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.1458194581850005
{'scaleFactor': 20, 'currentTarget': array([24., 21.]), 'previousTarget': array([24., 21.]), 'currentState': array([23.        , 20.97836847,  0.        ]), 'targetState': array([24, 21], dtype=int32), 'currentDistance': 1.0002339342108708}
episode index:939
target Thresh 31.997860980320322
target distance 14.0
model initialize at round 939
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  9.]), 'previousTarget': array([23.,  9.]), 'currentState': array([21.5       , 23.49610564,  0.        ]), 'targetState': array([23,  9], dtype=int32), 'currentDistance': 14.573506055809103}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14566433110182495
{'scaleFactor': 20, 'currentTarget': array([51.00410714,  8.43237896]), 'previousTarget': array([50.50396643,  8.45215233]), 'currentState': array([71.        ,  8.02707807,  0.        ]), 'targetState': array([23,  9], dtype=int32), 'currentDistance': 20.0}
episode index:940
target Thresh 31.99788226392175
target distance 22.0
model initialize at round 940
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.27670597, 21.47135032]), 'previousTarget': array([18.94427191, 20.88854382]), 'currentState': array([10.5,  3.5,  0. ]), 'targetState': array([21, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14550953372552122
{'scaleFactor': 20, 'currentTarget': array([40.02817511, 23.9889115 ]), 'previousTarget': array([39.5260718 , 24.05312258]), 'currentState': array([60.        , 22.92768085,  0.        ]), 'targetState': array([21, 25], dtype=int32), 'currentDistance': 20.0}
episode index:941
target Thresh 31.997903335747804
target distance 22.0
model initialize at round 941
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6.5442973, 6.8303793]), 'previousTarget': array([6.08213587, 6.81071492]), 'currentState': array([26.5       ,  5.49999219,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14535506500606737
{'scaleFactor': 20, 'currentTarget': array([56.00581326,  8.25416995]), 'previousTarget': array([55.50579691,  8.24036334]), 'currentState': array([76.        ,  8.73634889,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 20.0}
episode index:942
target Thresh 31.997924197905686
target distance 6.0
model initialize at round 942
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  2.]), 'previousTarget': array([27.,  2.]), 'currentState': array([21.5       ,  6.50163007,  0.        ]), 'targetState': array([27,  2], dtype=int32), 'currentDistance': 7.107367534294844}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.14616965904899162
{'scaleFactor': 20, 'currentTarget': array([27.,  2.]), 'previousTarget': array([27.,  2.]), 'currentState': array([26.        ,  2.50917432,  0.        ]), 'targetState': array([27,  2], dtype=int32), 'currentDistance': 1.1221668704852312}
episode index:943
target Thresh 31.997944852481627
target distance 3.0
model initialize at round 943
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 24.]), 'previousTarget': array([ 2., 24.]), 'currentState': array([ 2.5       , 26.81084576,  0.        ]), 'targetState': array([ 2, 24], dtype=int32), 'currentDistance': 2.8549700349592317}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14601481830847363
{'scaleFactor': 20, 'currentTarget': array([32.0079575 , 24.84674826]), 'previousTarget': array([31.50767473, 24.81769549]), 'currentState': array([52.        , 25.41087287,  0.        ]), 'targetState': array([ 2, 24], dtype=int32), 'currentDistance': 20.0}
episode index:944
target Thresh 31.997965301541107
target distance 8.0
model initialize at round 944
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  2.]), 'previousTarget': array([13.,  2.]), 'currentState': array([5.5       , 6.50003859, 0.        ]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 8.746447698783582}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.14678889894835773
{'scaleFactor': 20, 'currentTarget': array([13.,  2.]), 'previousTarget': array([13.,  2.]), 'currentState': array([12.        ,  1.87764569,  0.        ]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 1.0074574820245785}
episode index:945
target Thresh 31.99798554712904
target distance 19.0
model initialize at round 945
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 23.]), 'previousTarget': array([27., 23.]), 'currentState': array([ 8.5, 25.5,  0. ]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 18.668154702594382}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.147377333193655
{'scaleFactor': 20, 'currentTarget': array([27., 23.]), 'previousTarget': array([27., 23.]), 'currentState': array([26.        , 22.63613343,  0.        ]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 1.064142322440436}
episode index:946
target Thresh 31.99800559127001
target distance 9.0
model initialize at round 946
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([17.5       ,  4.50000009,  0.        ]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 9.617692044779893}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.147221707709818
{'scaleFactor': 20, 'currentTarget': array([47.1072869,  7.0670901]), 'previousTarget': array([46.60579907,  6.98677622]), 'currentState': array([67.       ,  9.1358978,  0.       ]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 20.0}
episode index:947
target Thresh 31.998025435968447
target distance 16.0
model initialize at round 947
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 15.]), 'previousTarget': array([25., 15.]), 'currentState': array([ 9.5, 10.5,  0. ]), 'targetState': array([25, 15], dtype=int32), 'currentDistance': 16.140012391568874}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.14785456676743589
{'scaleFactor': 20, 'currentTarget': array([25., 15.]), 'previousTarget': array([25., 15.]), 'currentState': array([24.        , 14.81429515,  0.        ]), 'targetState': array([25, 15], dtype=int32), 'currentDistance': 1.0170969922782023}
episode index:948
target Thresh 31.998045083208837
target distance 14.0
model initialize at round 948
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([2.5, 5.5, 0. ]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 18.398369492974105}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.1485183884667317
{'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([15.        , 17.99886182,  0.        ]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 1.000000647726081}
episode index:949
target Thresh 31.998064534955912
target distance 12.0
model initialize at round 949
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([15.5,  5.5,  0. ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 12.747548783982042}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14836205332097724
{'scaleFactor': 20, 'currentTarget': array([45.45458153, 17.20944737]), 'previousTarget': array([44.94426205, 16.99130445]), 'currentState': array([65.        , 21.44933141,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 20.0}
episode index:950
target Thresh 31.998083793154876
target distance 2.0
model initialize at round 950
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([16.5       ,  9.83104658,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 2.367008995076822}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14820604695576065
{'scaleFactor': 20, 'currentTarget': array([46.87245219, 17.73572543]), 'previousTarget': array([46.37138477, 17.57640222]), 'currentState': array([66.        , 23.57840589,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 20.0}
episode index:951
target Thresh 31.998102859731556
target distance 18.0
model initialize at round 951
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9.5143228 , 2.94134162]), 'previousTarget': array([9.35899411, 2.90599608]), 'currentState': array([26.5      , 13.5000712,  0.       ]), 'targetState': array([8, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14805036833500881
{'scaleFactor': 20, 'currentTarget': array([57.20094213, 19.86524447]), 'previousTarget': array([56.69670216, 19.64789383]), 'currentState': array([76.        , 26.69132849,  0.        ]), 'targetState': array([8, 2], dtype=int32), 'currentDistance': 20.0}
episode index:952
target Thresh 31.998121736592626
target distance 9.0
model initialize at round 952
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 29.]), 'previousTarget': array([ 8., 29.]), 'currentState': array([ 7.5, 20.5,  0. ]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 8.514693182963123}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14789501642699726
{'scaleFactor': 20, 'currentTarget': array([37.92914882, 38.45569304]), 'previousTarget': array([37.43625472, 38.33809475]), 'currentState': array([57.        , 44.48085987,  0.        ]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 20.0}
episode index:953
target Thresh 31.99814042562579
target distance 8.0
model initialize at round 953
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 21.]), 'previousTarget': array([20., 21.]), 'currentState': array([12.5, 19.5,  0. ]), 'targetState': array([20, 21], dtype=int32), 'currentDistance': 7.6485292703890835}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.1486598235617687
{'scaleFactor': 20, 'currentTarget': array([20., 21.]), 'previousTarget': array([20., 21.]), 'currentState': array([19.        , 21.89393432,  0.        ]), 'targetState': array([20, 21], dtype=int32), 'currentDistance': 1.3413122534754989}
episode index:954
target Thresh 31.998158928699972
target distance 10.0
model initialize at round 954
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 17.]), 'previousTarget': array([22., 17.]), 'currentState': array([12.5       , 26.50008211,  0.        ]), 'targetState': array([22, 17], dtype=int32), 'currentDistance': 13.435086899953612}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14850415882505483
{'scaleFactor': 20, 'currentTarget': array([42.34969424, 20.85616923]), 'previousTarget': array([41.85092216, 20.76843003]), 'currentState': array([62.       , 24.5798077,  0.       ]), 'targetState': array([22, 17], dtype=int32), 'currentDistance': 20.0}
episode index:955
target Thresh 31.998177247665488
target distance 25.0
model initialize at round 955
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7.94957912, 21.71675745]), 'previousTarget': array([ 7.55225396, 21.66745905]), 'currentState': array([27.5, 17.5,  0. ]), 'targetState': array([ 2, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14834881974678593
{'scaleFactor': 20, 'currentTarget': array([57.36988431, 33.79931933]), 'previousTarget': array([56.87431939, 33.76846873]), 'currentState': array([77.        , 37.62796897,  0.        ]), 'targetState': array([ 2, 23], dtype=int32), 'currentDistance': 20.0}
episode index:956
target Thresh 31.99819538435425
target distance 10.0
model initialize at round 956
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([ 9.5       , 15.50004634,  0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 12.103754810461318}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1481938053060892
{'scaleFactor': 20, 'currentTarget': array([39.3991422 , 13.58582259]), 'previousTarget': array([38.88685815, 13.36241029]), 'currentState': array([59.        , 17.56154559,  0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 20.0}
episode index:957
target Thresh 31.998213340579944
target distance 20.0
model initialize at round 957
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9.17546874, 3.52640845]), 'previousTarget': array([9.25304229, 3.8434743 ]), 'currentState': array([15.5       , 22.50008243,  0.        ]), 'targetState': array([9, 3], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14803911448635423
{'scaleFactor': 20, 'currentTarget': array([45.51036416, 11.40996312]), 'previousTarget': array([45.01289186, 11.31670253]), 'currentState': array([65.        , 15.89929436,  0.        ]), 'targetState': array([9, 3], dtype=int32), 'currentDistance': 20.0}
episode index:958
target Thresh 31.998231118138207
target distance 14.0
model initialize at round 958
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'previousTarget': array([22., 18.]), 'currentState': array([21.5,  4.5,  0. ]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 13.509256086106296}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.147884746275211
{'scaleFactor': 20, 'currentTarget': array([51.55531657, 25.11376134]), 'previousTarget': array([51.04680677, 24.9353042 ]), 'currentState': array([71.        , 29.79396285,  0.        ]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 20.0}
episode index:959
target Thresh 31.99824871880681
target distance 21.0
model initialize at round 959
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.08785189,  7.42407727]), 'previousTarget': array([12.07722123,  7.63513716]), 'currentState': array([22.5       , 24.50000101,  0.        ]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14773069966450766
{'scaleFactor': 20, 'currentTarget': array([52.4341    , 12.98815568]), 'previousTarget': array([51.92646022, 12.79955149]), 'currentState': array([72.        , 17.13249608,  0.        ]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 20.0}
episode index:960
target Thresh 31.998266144345834
target distance 6.0
model initialize at round 960
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'previousTarget': array([22., 18.]), 'currentState': array([16.5       , 22.50005773,  0.        ]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 7.1063717568965865}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.14852756391822164
{'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'previousTarget': array([22., 18.]), 'currentState': array([21.        , 18.11372012,  0.        ]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 1.0064453613900737}
episode index:961
target Thresh 31.998283396497847
target distance 15.0
model initialize at round 961
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 16.]), 'previousTarget': array([ 3., 16.]), 'currentState': array([18.5       , 18.49999884,  0.        ]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 15.700318283033315}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1483731693611341
{'scaleFactor': 20, 'currentTarget': array([48.4909595, 26.2697592]), 'previousTarget': array([47.9829437 , 26.06871622]), 'currentState': array([68.        , 30.67400018,  0.        ]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 20.0}
episode index:962
target Thresh 31.99830047698808
target distance 18.0
model initialize at round 962
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 20.]), 'previousTarget': array([13., 20.]), 'currentState': array([13.5,  2.5,  0. ]), 'targetState': array([13, 20], dtype=int32), 'currentDistance': 17.5071414000116}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14821909545733228
{'scaleFactor': 20, 'currentTarget': array([43.60638701, 27.7133172 ]), 'previousTarget': array([43.10980464, 27.61053491]), 'currentState': array([63.        , 32.60082936,  0.        ]), 'targetState': array([13, 20], dtype=int32), 'currentDistance': 20.0}
episode index:963
target Thresh 31.998317387524597
target distance 11.0
model initialize at round 963
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 16.]), 'previousTarget': array([ 9., 16.]), 'currentState': array([17.5       , 26.50000539,  0.        ]), 'targetState': array([ 9, 16], dtype=int32), 'currentDistance': 13.509260278736978}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14806534120893255
{'scaleFactor': 20, 'currentTarget': array([47.61419933, 25.79688026]), 'previousTarget': array([47.11252261, 25.65576162]), 'currentState': array([67.        , 30.71528776,  0.        ]), 'targetState': array([ 9, 16], dtype=int32), 'currentDistance': 20.0}
episode index:964
target Thresh 31.99833412979846
target distance 5.0
model initialize at round 964
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([21.5       , 23.50270474,  0.        ]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 6.042642717674885}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14791190562218756
{'scaleFactor': 20, 'currentTarget': array([51.65261789, 30.33805922]), 'previousTarget': array([51.1455433 , 30.15269368]), 'currentState': array([71.        , 35.40548513,  0.        ]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:965
target Thresh 31.998350705483922
target distance 5.0
model initialize at round 965
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 12.]), 'previousTarget': array([14., 12.]), 'currentState': array([16.5       , 16.50554508,  0.        ]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 5.152663045967748}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14775878770746478
{'scaleFactor': 20, 'currentTarget': array([46.50031438, 19.40931854]), 'previousTarget': array([45.99586151, 19.26053353]), 'currentState': array([66.        , 23.85479499,  0.        ]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 20.0}
episode index:966
target Thresh 31.99836711623855
target distance 12.0
model initialize at round 966
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  9.]), 'previousTarget': array([20.,  9.]), 'currentState': array([24.5       , 20.50003782,  0.        ]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 12.349124254043968}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14760598647922543
{'scaleFactor': 20, 'currentTarget': array([54.5405468 , 17.19767616]), 'previousTarget': array([54.03415559, 17.02761103]), 'currentState': array([74.        , 21.81608294,  0.        ]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 20.0}
episode index:967
target Thresh 31.998383363703446
target distance 15.0
model initialize at round 967
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 20.]), 'previousTarget': array([ 8., 20.]), 'currentState': array([5.5, 5.5, 0. ]), 'targetState': array([ 8, 20], dtype=int32), 'currentDistance': 14.713938969562168}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1474535009560031
{'scaleFactor': 20, 'currentTarget': array([35.55984456, 26.66162333]), 'previousTarget': array([35.0688402 , 26.59759785]), 'currentState': array([55.        , 31.36059733,  0.        ]), 'targetState': array([ 8, 20], dtype=int32), 'currentDistance': 20.0}
episode index:968
target Thresh 31.998399449503363
target distance 18.0
model initialize at round 968
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([26.5, 18.5,  0. ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 19.962464777677173}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14730133016038285
{'scaleFactor': 20, 'currentTarget': array([56.4395137 , 21.32611949]), 'previousTarget': array([55.94360318, 21.26945789]), 'currentState': array([76.        , 25.49593672,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 20.0}
episode index:969
target Thresh 31.9984153752469
target distance 6.0
model initialize at round 969
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([23.5,  3.5,  0. ]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 6.519202405202731}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1471494731189804
{'scaleFactor': 20, 'currentTarget': array([53.84224079, 15.04405031]), 'previousTarget': array([53.34070218, 14.88309099]), 'currentState': array([73.        , 20.78689472,  0.        ]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 20.0}
episode index:970
target Thresh 31.998431142526638
target distance 11.0
model initialize at round 970
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  4.]), 'previousTarget': array([23.,  4.]), 'currentState': array([18.5       , 14.50003812,  0.        ]), 'targetState': array([23,  4], dtype=int32), 'currentDistance': 11.423694694013372}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1469979288624212
{'scaleFactor': 20, 'currentTarget': array([48.29973614,  8.43006602]), 'previousTarget': array([47.79668298,  8.31930586]), 'currentState': array([68.        , 11.87964625,  0.        ]), 'targetState': array([23,  4], dtype=int32), 'currentDistance': 20.0}
episode index:971
target Thresh 31.99844675291932
target distance 18.0
model initialize at round 971
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 3.]), 'previousTarget': array([9., 3.]), 'currentState': array([12.5       , 20.50004482,  0.        ]), 'targetState': array([9, 3], dtype=int32), 'currentDistance': 17.846612249843293}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14684669642531994
{'scaleFactor': 20, 'currentTarget': array([42.75878414, 12.57380627]), 'previousTarget': array([42.26337618, 12.46351759]), 'currentState': array([62.       , 18.0305097,  0.       ]), 'targetState': array([9, 3], dtype=int32), 'currentDistance': 20.0}
episode index:972
target Thresh 31.998462207986
target distance 11.0
model initialize at round 972
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 21.]), 'previousTarget': array([23., 21.]), 'currentState': array([12.5, 12.5,  0. ]), 'targetState': array([23, 21], dtype=int32), 'currentDistance': 13.509256086106184}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.14754486901258643
{'scaleFactor': 20, 'currentTarget': array([23., 21.]), 'previousTarget': array([23., 21.]), 'currentState': array([22.        , 20.45974623,  0.        ]), 'targetState': array([23, 21], dtype=int32), 'currentDistance': 1.1366064107920049}
episode index:973
target Thresh 31.998477509272202
target distance 12.0
model initialize at round 973
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 19.]), 'previousTarget': array([18., 19.]), 'currentState': array([10.5,  7.5,  0. ]), 'targetState': array([18, 19], dtype=int32), 'currentDistance': 13.72953021774594}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14739338557417514
{'scaleFactor': 20, 'currentTarget': array([40.71891287, 25.26160844]), 'previousTarget': array([40.21685879, 25.11399087]), 'currentState': array([60.        , 30.57571034,  0.        ]), 'targetState': array([18, 19], dtype=int32), 'currentDistance': 20.0}
episode index:974
target Thresh 31.99849265830806
target distance 18.0
model initialize at round 974
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  3.]), 'previousTarget': array([24.,  3.]), 'currentState': array([6.5, 8.5, 0. ]), 'targetState': array([24,  3], dtype=int32), 'currentDistance': 18.343936327844123}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.1479783467506101
{'scaleFactor': 20, 'currentTarget': array([24.,  3.]), 'previousTarget': array([24.,  3.]), 'currentState': array([23.        ,  2.47532802,  0.        ]), 'targetState': array([24,  3], dtype=int32), 'currentDistance': 1.1292832619869226}
episode index:975
target Thresh 31.99850765660849
target distance 6.0
model initialize at round 975
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 15.]), 'previousTarget': array([14., 15.]), 'currentState': array([13.5,  9.5,  0. ]), 'targetState': array([14, 15], dtype=int32), 'currentDistance': 5.5226805085935435}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14782672959205415
{'scaleFactor': 20, 'currentTarget': array([43.5169528 , 21.84455814]), 'previousTarget': array([43.01248234, 21.6972689 ]), 'currentState': array([63.        , 26.36239744,  0.        ]), 'targetState': array([14, 15], dtype=int32), 'currentDistance': 20.0}
episode index:976
target Thresh 31.99852250567334
target distance 4.0
model initialize at round 976
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 23.]), 'previousTarget': array([25., 23.]), 'currentState': array([23.5, 19.5,  0. ]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 3.80788655293185}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14767542280639187
{'scaleFactor': 20, 'currentTarget': array([53.54417279, 29.79816573]), 'previousTarget': array([53.05772501, 29.76856197]), 'currentState': array([73.        , 34.43182384,  0.        ]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 20.0}
episode index:977
target Thresh 31.998537206987525
target distance 5.0
model initialize at round 977
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([21.5       , 12.51950553,  0.        ]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 5.7162864029968805}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14752442544155916
{'scaleFactor': 20, 'currentTarget': array([51.22272453, 13.00000363]), 'previousTarget': array([50.72039949, 12.89820083]), 'currentState': array([71.        , 15.97647383,  0.        ]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 20.0}
episode index:978
target Thresh 31.998551762021187
target distance 12.0
model initialize at round 978
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 24.]), 'previousTarget': array([18., 24.]), 'currentState': array([11.5, 12.5,  0. ]), 'targetState': array([18, 24], dtype=int32), 'currentDistance': 13.209844813622796}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14737373654938188
{'scaleFactor': 20, 'currentTarget': array([41.48646011, 29.27689061]), 'previousTarget': array([40.99054871, 29.18795084]), 'currentState': array([61.        , 33.66115349,  0.        ]), 'targetState': array([18, 24], dtype=int32), 'currentDistance': 20.0}
episode index:979
target Thresh 31.998566172229847
target distance 12.0
model initialize at round 979
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([27.5, 14.5,  0. ]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 12.980754985747227}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.147223355185556
{'scaleFactor': 20, 'currentTarget': array([57.19692745, 23.96567626]), 'previousTarget': array([56.69429993, 23.85457595]), 'currentState': array([77.        , 26.76537584,  0.        ]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 20.0}
episode index:980
target Thresh 31.998580439054535
target distance 16.0
model initialize at round 980
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  6.]), 'previousTarget': array([22.,  6.]), 'currentState': array([20.5       , 21.51282275,  0.        ]), 'targetState': array([22,  6], dtype=int32), 'currentDistance': 15.58517467297037}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14707328040962778
{'scaleFactor': 20, 'currentTarget': array([50.42908242, 11.98563284]), 'previousTarget': array([49.91950793, 11.81025027]), 'currentState': array([70.        , 16.10621349,  0.        ]), 'targetState': array([22,  6], dtype=int32), 'currentDistance': 20.0}
episode index:981
target Thresh 31.998594563921948
target distance 12.0
model initialize at round 981
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 20.]), 'previousTarget': array([23., 20.]), 'currentState': array([11.5,  8.5,  0. ]), 'targetState': array([23, 20], dtype=int32), 'currentDistance': 16.26345596729048}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.14773983571424024
{'scaleFactor': 20, 'currentTarget': array([23., 20.]), 'previousTarget': array([23., 20.]), 'currentState': array([22.5       , 19.49938667,  0.        ]), 'targetState': array([23, 20], dtype=int32), 'currentDistance': 0.7075406051718591}
episode index:982
target Thresh 31.99860854824458
target distance 1.0
model initialize at round 982
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 6.]), 'previousTarget': array([8., 6.]), 'currentState': array([9.5       , 6.48123685, 0.        ]), 'targetState': array([8, 6], dtype=int32), 'currentDistance': 1.575305970670814}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14758954086610773
{'scaleFactor': 20, 'currentTarget': array([40.32384064, 18.38439832]), 'previousTarget': array([39.82136746, 18.17926611]), 'currentState': array([59.        , 25.53989073,  0.        ]), 'targetState': array([8, 6], dtype=int32), 'currentDistance': 20.0}
episode index:983
target Thresh 31.998622393420874
target distance 3.0
model initialize at round 983
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'previousTarget': array([11., 19.]), 'currentState': array([ 8.5      , 18.4956919,  0.       ]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 2.5503581443160184}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.14842562771482107
{'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'previousTarget': array([11., 19.]), 'currentState': array([10.        , 18.93896622,  0.        ]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 1.0018608299617147}
episode index:984
target Thresh 31.998636100835366
target distance 12.0
model initialize at round 984
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([ 5.5       , 22.50000003,  0.        ]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 11.76860230227071}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.14909700054782252
{'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([16.        , 20.04917173,  0.        ]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 1.0012081997298587}
episode index:985
target Thresh 31.998649671858804
target distance 14.0
model initialize at round 985
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 23.]), 'previousTarget': array([25., 23.]), 'currentState': array([11.5       , 20.49999991,  0.        ]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 13.72953023402595}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.14973465202738775
{'scaleFactor': 20, 'currentTarget': array([25., 23.]), 'previousTarget': array([25., 23.]), 'currentState': array([24.        , 23.02030624,  0.        ]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 1.0002061504967361}
episode index:986
target Thresh 31.9986631078483
target distance 12.0
model initialize at round 986
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  6.]), 'previousTarget': array([18.,  6.]), 'currentState': array([ 6.5, 14.5,  0. ]), 'targetState': array([18,  6], dtype=int32), 'currentDistance': 14.300349646075047}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.1504033381633491
{'scaleFactor': 20, 'currentTarget': array([18.,  6.]), 'previousTarget': array([18.,  6.]), 'currentState': array([17.        ,  6.03335085,  0.        ]), 'targetState': array([18,  6], dtype=int32), 'currentDistance': 1.0005559851450003}
episode index:987
target Thresh 31.99867641014747
target distance 13.0
model initialize at round 987
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 5.]), 'previousTarget': array([6., 5.]), 'currentState': array([11.5       , 17.55825084,  0.        ]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 13.709838229273439}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1502511080639935
{'scaleFactor': 20, 'currentTarget': array([42.77548491, 21.62386148]), 'previousTarget': array([42.26404126, 21.33194696]), 'currentState': array([61.       , 29.8620075,  0.       ]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 20.0}
episode index:988
target Thresh 31.998689580086552
target distance 18.0
model initialize at round 988
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.71606909, 19.33564511]), 'previousTarget': array([19.27881227, 18.78704435]), 'currentState': array([7.5, 3.5, 0. ]), 'targetState': array([21, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1500991858111482
{'scaleFactor': 20, 'currentTarget': array([38.95918896, 29.59396753]), 'previousTarget': array([38.4575857 , 29.34995163]), 'currentState': array([57.        , 38.22699348,  0.        ]), 'targetState': array([21, 21], dtype=int32), 'currentDistance': 20.0}
episode index:989
target Thresh 31.99870261898255
target distance 12.0
model initialize at round 989
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([ 4.5       , 14.50000006,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 12.349089056948353}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.15076547740954224
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.        , 10.04059441,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 1.0008236139155675}
episode index:990
target Thresh 31.99871552813936
target distance 5.0
model initialize at round 990
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([ 9.5, 18.5,  0. ]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 6.519202405202673}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15061334271992616
{'scaleFactor': 20, 'currentTarget': array([40.52021285, 37.11488566]), 'previousTarget': array([40.03962128, 37.02288503]), 'currentState': array([59.        , 44.76324934,  0.        ]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 20.0}
episode index:991
target Thresh 31.99872830884792
target distance 9.0
model initialize at round 991
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 24.]), 'previousTarget': array([25., 24.]), 'currentState': array([22.5, 15.5,  0. ]), 'targetState': array([25, 24], dtype=int32), 'currentDistance': 8.860022573334575}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1504615147534746
{'scaleFactor': 20, 'currentTarget': array([53.60062638, 36.18651595]), 'previousTarget': array([53.09452544, 35.94502258]), 'currentState': array([72.        , 44.02635334,  0.        ]), 'targetState': array([25, 24], dtype=int32), 'currentDistance': 20.0}
episode index:992
target Thresh 31.998740962386304
target distance 20.0
model initialize at round 992
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.88419958,  9.05615617]), 'previousTarget': array([17.46924689,  9.61536159]), 'currentState': array([ 6.5, 25.5,  0. ]), 'targetState': array([20,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15030999258353153
{'scaleFactor': 20, 'currentTarget': array([38.15562502, 15.18961125]), 'previousTarget': array([37.66647732, 14.96867498]), 'currentState': array([56.        , 24.22168086,  0.        ]), 'targetState': array([20,  6], dtype=int32), 'currentDistance': 20.0}
episode index:993
target Thresh 31.998753490019876
target distance 12.0
model initialize at round 993
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 24.]), 'previousTarget': array([ 6., 24.]), 'currentState': array([18.5, 14.5,  0. ]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 15.700318468107598}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15015877528716984
{'scaleFactor': 20, 'currentTarget': array([49.91820263, 44.7591191 ]), 'previousTarget': array([49.41266877, 44.48576319]), 'currentState': array([68.        , 53.30596671,  0.        ]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 20.0}
episode index:994
target Thresh 31.998765893001412
target distance 10.0
model initialize at round 994
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 13.]), 'previousTarget': array([16., 13.]), 'currentState': array([24.5,  3.5,  0. ]), 'targetState': array([16, 13], dtype=int32), 'currentDistance': 12.74754878398202}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15000786194517268
{'scaleFactor': 20, 'currentTarget': array([55.90662351, 31.79674156]), 'previousTarget': array([55.39973582, 31.51906245]), 'currentState': array([74.        , 40.31904919,  0.        ]), 'targetState': array([16, 13], dtype=int32), 'currentDistance': 20.0}
episode index:995
target Thresh 31.998778172571217
target distance 10.0
model initialize at round 995
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 22.]), 'previousTarget': array([19., 22.]), 'currentState': array([16.5, 12.5,  0. ]), 'targetState': array([19, 22], dtype=int32), 'currentDistance': 9.823441352194152}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14985725164201488
{'scaleFactor': 20, 'currentTarget': array([48.00568415, 36.07143408]), 'previousTarget': array([47.53160695, 35.94594497]), 'currentState': array([66.        , 44.80095854,  0.        ]), 'targetState': array([19, 22], dtype=int32), 'currentDistance': 20.0}
episode index:996
target Thresh 31.998790329957263
target distance 16.0
model initialize at round 996
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 24.]), 'previousTarget': array([ 2., 24.]), 'currentState': array([18.5, 20.5,  0. ]), 'targetState': array([ 2, 24], dtype=int32), 'currentDistance': 16.867127793433063}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14970694346584434
{'scaleFactor': 20, 'currentTarget': array([50.75790687, 52.65905768]), 'previousTarget': array([50.25954399, 52.37661719]), 'currentState': array([68.        , 62.79366298,  0.        ]), 'targetState': array([ 2, 24], dtype=int32), 'currentDistance': 20.0}
episode index:997
target Thresh 31.998802366375298
target distance 15.0
model initialize at round 997
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([ 3.5, 20.5,  0. ]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 19.144189719076596}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.15032080696372138
{'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([17.        ,  8.10916395,  0.        ]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 1.0059407377257925}
episode index:998
target Thresh 31.99881428302897
target distance 4.0
model initialize at round 998
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  7.]), 'previousTarget': array([11.,  7.]), 'currentState': array([15.5,  7.5,  0. ]), 'targetState': array([11,  7], dtype=int32), 'currentDistance': 4.527692569068788}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1501703356854794
{'scaleFactor': 20, 'currentTarget': array([47.14666131, 25.25082943]), 'previousTarget': array([46.62766928, 24.89448889]), 'currentState': array([65.        , 34.26516788,  0.        ]), 'targetState': array([11,  7], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:999
target Thresh 31.99882608110996
target distance 13.0
model initialize at round 999
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 10.]), 'previousTarget': array([22., 10.]), 'currentState': array([9.5       , 9.49999964, 0.        ]), 'targetState': array([22, 10], dtype=int32), 'currentDistance': 12.509996017490415}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.1508137796334376
{'scaleFactor': 20, 'currentTarget': array([22., 10.]), 'previousTarget': array([22., 10.]), 'currentState': array([21.        , 10.10961819,  0.        ]), 'targetState': array([22, 10], dtype=int32), 'currentDistance': 1.0059901334207497}
episode index:1000
target Thresh 31.99883776179808
target distance 2.0
model initialize at round 1000
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([17.5       ,  7.92993566,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 2.444310053610997}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15066311651692066
{'scaleFactor': 20, 'currentTarget': array([48.99315565, 21.94732184]), 'previousTarget': array([48.48330631, 21.65550933]), 'currentState': array([67.        , 30.65097375,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 20.0}
episode index:1001
target Thresh 31.998849326261414
target distance 18.0
model initialize at round 1001
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([26.5, 16.5,  0. ]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 18.506755523321825}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1505127541251872
{'scaleFactor': 20, 'currentTarget': array([58.16495276, 42.45642984]), 'previousTarget': array([57.66907082, 42.23320588]), 'currentState': array([76.       , 51.5069044,  0.       ]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 20.0}
episode index:1002
target Thresh 31.99886077565641
target distance 16.0
model initialize at round 1002
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.16314258,  6.89457173]), 'previousTarget': array([20.67882258,  7.40925592]), 'currentState': array([ 7.5, 21.5,  0. ]), 'targetState': array([22,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15036269155876128
{'scaleFactor': 20, 'currentTarget': array([39.20093263, 14.81451993]), 'previousTarget': array([38.73688007, 14.6598534 ]), 'currentState': array([57.        , 23.93555061,  0.        ]), 'targetState': array([22,  6], dtype=int32), 'currentDistance': 20.0}
episode index:1003
target Thresh 31.998872111128026
target distance 15.0
model initialize at round 1003
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 25.]), 'previousTarget': array([19., 25.]), 'currentState': array([ 4.5, 15.5,  0. ]), 'targetState': array([19, 25], dtype=int32), 'currentDistance': 17.334935823359675}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.1509722334141282
{'scaleFactor': 20, 'currentTarget': array([19., 25.]), 'previousTarget': array([19., 25.]), 'currentState': array([18.        , 25.04646987,  0.        ]), 'targetState': array([19, 25], dtype=int32), 'currentDistance': 1.001079142004889}
episode index:1004
target Thresh 31.998883333809815
target distance 18.0
model initialize at round 1004
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([27.5       , 20.50064561,  0.        ]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 17.507786744379192}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15082201228635292
{'scaleFactor': 20, 'currentTarget': array([59.28737299, 19.93018175]), 'previousTarget': array([58.77204123, 19.59304699]), 'currentState': array([77.        , 29.21796106,  0.        ]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:1005
target Thresh 31.998894444824057
target distance 6.0
model initialize at round 1005
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 8.]), 'previousTarget': array([7., 8.]), 'currentState': array([13.5       , 13.20503137,  0.        ]), 'targetState': array([7, 8], dtype=int32), 'currentDistance': 8.327205504385027}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1506720898089311
{'scaleFactor': 20, 'currentTarget': array([45.35956409, 28.49309537]), 'previousTarget': array([44.89511613, 28.42845355]), 'currentState': array([63.        , 37.91726752,  0.        ]), 'targetState': array([7, 8], dtype=int32), 'currentDistance': 20.0}
episode index:1006
target Thresh 31.998905445281856
target distance 7.0
model initialize at round 1006
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([21.5,  2.5,  0. ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 8.514693182963256}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1505224650921397
{'scaleFactor': 20, 'currentTarget': array([52.92282863, 26.47700935]), 'previousTarget': array([52.43576795, 26.31384826]), 'currentState': array([71.        , 35.03363691,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 20.0}
episode index:1007
target Thresh 31.998916336283273
target distance 17.0
model initialize at round 1007
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([ 7.5, 27.5,  0. ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 18.560711193270517}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1503731372497864
{'scaleFactor': 20, 'currentTarget': array([40.26699854, 26.88710965]), 'previousTarget': array([39.73502499, 26.43983805]), 'currentState': array([57.        , 37.84186487,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 20.0}
episode index:1008
target Thresh 31.998927118917415
target distance 11.0
model initialize at round 1008
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  4.]), 'previousTarget': array([25.,  4.]), 'currentState': array([19.5       , 14.50000018,  0.        ]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 11.853269749528739}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15022410539919195
{'scaleFactor': 20, 'currentTarget': array([52.89802801, 24.5533387 ]), 'previousTarget': array([52.39531141, 24.17329334]), 'currentState': array([69.        , 36.41615868,  0.        ]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:1009
target Thresh 31.998937794262552
target distance 17.0
model initialize at round 1009
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 10.]), 'previousTarget': array([25., 10.]), 'currentState': array([ 8.5, 13.5,  0. ]), 'targetState': array([25, 10], dtype=int32), 'currentDistance': 16.867127793432907}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.15080042150241493
{'scaleFactor': 20, 'currentTarget': array([25., 10.]), 'previousTarget': array([25., 10.]), 'currentState': array([24.        , 10.02467729,  0.        ]), 'targetState': array([25, 10], dtype=int32), 'currentDistance': 1.0003044379684312}
episode index:1010
target Thresh 31.998948363386233
target distance 5.0
model initialize at round 1010
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  6.]), 'previousTarget': array([10.,  6.]), 'currentState': array([15.5,  6.5,  0. ]), 'targetState': array([10,  6], dtype=int32), 'currentDistance': 5.52268050859371}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15065126183722957
{'scaleFactor': 20, 'currentTarget': array([48.78585741, 34.00919575]), 'previousTarget': array([48.29441915, 33.69690849]), 'currentState': array([65.        , 45.71823415,  0.        ]), 'targetState': array([10,  6], dtype=int32), 'currentDistance': 20.0}
episode index:1011
target Thresh 31.998958827345376
target distance 4.0
model initialize at round 1011
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  2.]), 'previousTarget': array([18.,  2.]), 'currentState': array([22.5       ,  4.49993888,  0.        ]), 'targetState': array([18,  2], dtype=int32), 'currentDistance': 5.1477853860588505}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1505023969539912
{'scaleFactor': 20, 'currentTarget': array([55.45076561, 27.4143886 ]), 'previousTarget': array([54.96223771, 27.13801172]), 'currentState': array([72.        , 38.64483121,  0.        ]), 'targetState': array([18,  2], dtype=int32), 'currentDistance': 20.0}
episode index:1012
target Thresh 31.998969187186383
target distance 16.0
model initialize at round 1012
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 13.]), 'previousTarget': array([ 3., 13.]), 'currentState': array([ 4.5       , 28.50183472,  0.        ]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 15.574237692348266}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15035382597970295
{'scaleFactor': 20, 'currentTarget': array([38.1133713 , 39.85381981]), 'previousTarget': array([37.61432209, 39.47645277]), 'currentState': array([54.        , 52.00351234,  0.        ]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 20.0}
episode index:1013
target Thresh 31.998979443945252
target distance 18.0
model initialize at round 1013
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5.52264154, 11.1174021 ]), 'previousTarget': array([ 5.05181363, 10.71272322]), 'currentState': array([20.5       , 24.37178755,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15020554804481173
{'scaleFactor': 20, 'currentTarget': array([52.19675534, 33.69400887]), 'previousTarget': array([51.68662005, 33.36311176]), 'currentState': array([70.        , 42.80688326,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 20.0}
episode index:1014
target Thresh 31.998989598647665
target distance 18.0
model initialize at round 1014
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 27.]), 'previousTarget': array([13., 27.]), 'currentState': array([16.5,  9.5,  0. ]), 'targetState': array([13, 27], dtype=int32), 'currentDistance': 17.846568297574688}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15005756228319123
{'scaleFactor': 20, 'currentTarget': array([47.37761248, 40.46552975]), 'previousTarget': array([46.87855488, 40.27509951]), 'currentState': array([66.        , 47.75982087,  0.        ]), 'targetState': array([13, 27], dtype=int32), 'currentDistance': 20.0}
episode index:1015
target Thresh 31.998999652309102
target distance 10.0
model initialize at round 1015
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 19.]), 'previousTarget': array([25., 19.]), 'currentState': array([15.5, 13.5,  0. ]), 'targetState': array([25, 19], dtype=int32), 'currentDistance': 10.977249200049965}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.1507395363295502
{'scaleFactor': 20, 'currentTarget': array([25., 19.]), 'previousTarget': array([25., 19.]), 'currentState': array([24.        , 18.80186969,  0.        ]), 'targetState': array([25, 19], dtype=int32), 'currentDistance': 1.019438874857986}
episode index:1016
target Thresh 31.999009605934933
target distance 6.0
model initialize at round 1016
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 11.]), 'previousTarget': array([24., 11.]), 'currentState': array([18.5      , 16.5000006,  0.       ]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 7.7781750145204995}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.1514805811070126
{'scaleFactor': 20, 'currentTarget': array([24., 11.]), 'previousTarget': array([24., 11.]), 'currentState': array([23.5      , 11.5353933,  0.       ]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 0.7325612487647118}
episode index:1017
target Thresh 31.999019460520536
target distance 18.0
model initialize at round 1017
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.76070491, 24.4717636 ]), 'previousTarget': array([15.28727678, 23.94818637]), 'currentState': array([2.5, 9.5, 0. ]), 'targetState': array([18, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15133177896447134
{'scaleFactor': 20, 'currentTarget': array([33.57351387, 33.57195699]), 'previousTarget': array([33.06894989, 33.34860885]), 'currentState': array([52.        , 41.34785622,  0.        ]), 'targetState': array([18, 27], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:1018
target Thresh 31.999029217051373
target distance 22.0
model initialize at round 1018
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9.57727252, 12.48703223]), 'previousTarget': array([ 9.26249016, 12.12677025]), 'currentState': array([20.5       , 29.24095787,  0.        ]), 'targetState': array([6, 7], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15118326887716568
{'scaleFactor': 20, 'currentTarget': array([51.43302337, 25.19134798]), 'previousTarget': array([50.92940445, 24.96431111]), 'currentState': array([70.        , 32.62555129,  0.        ]), 'targetState': array([6, 7], dtype=int32), 'currentDistance': 20.0}
episode index:1019
target Thresh 31.999038876503104
target distance 10.0
model initialize at round 1019
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([17.5,  5.5,  0. ]), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 9.823441352194276}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15103504998610962
{'scaleFactor': 20, 'currentTarget': array([48.42236649, 28.32663043]), 'previousTarget': array([47.92612123, 28.14809532]), 'currentState': array([67.        , 35.73416264,  0.        ]), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 20.0}
episode index:1020
target Thresh 31.999048439841687
target distance 15.0
model initialize at round 1020
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([21.5       , 17.50102645,  0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 17.678163103970846}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15088712143568248
{'scaleFactor': 20, 'currentTarget': array([52.18869727, 25.67752434]), 'previousTarget': array([51.67189878, 25.36291489]), 'currentState': array([71.        , 32.46979123,  0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 20.0}
episode index:1021
target Thresh 31.99905790802346
target distance 25.0
model initialize at round 1021
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7.80432882, 9.02429332]), 'previousTarget': array([7.25118736, 8.84018998]), 'currentState': array([27.5, 12.5,  0. ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15073948237361234
{'scaleFactor': 20, 'currentTarget': array([58.14051164, 27.81731717]), 'previousTarget': array([57.62699714, 27.5087778 ]), 'currentState': array([77.        , 34.47462135,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 20.0}
episode index:1022
target Thresh 31.99906728199525
target distance 16.0
model initialize at round 1022
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 26.]), 'previousTarget': array([27., 26.]), 'currentState': array([25.5, 10.5,  0. ]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 15.572411502397351}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15059213195095975
{'scaleFactor': 20, 'currentTarget': array([56.49106323, 38.07362695]), 'previousTarget': array([55.97230066, 37.77731471]), 'currentState': array([75.        , 45.65117666,  0.        ]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 20.0}
episode index:1023
target Thresh 31.999076562694462
target distance 16.0
model initialize at round 1023
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 28.]), 'previousTarget': array([14., 28.]), 'currentState': array([ 8.5, 12.5,  0. ]), 'targetState': array([14, 28], dtype=int32), 'currentDistance': 16.446884203398426}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1504450693221014
{'scaleFactor': 20, 'currentTarget': array([39.17340903, 37.0252593 ]), 'previousTarget': array([38.67386042, 36.8480214 ]), 'currentState': array([58.        , 43.77503502,  0.        ]), 'targetState': array([14, 28], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:1024
target Thresh 31.999085751049176
target distance 7.0
model initialize at round 1024
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'previousTarget': array([ 2., 17.]), 'currentState': array([ 5.5       , 23.87647342,  0.        ]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 7.715950148055613}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15029829364471398
{'scaleFactor': 20, 'currentTarget': array([36.08417872, 28.70418481]), 'previousTarget': array([35.57655322, 28.48574755]), 'currentState': array([55.        , 35.19969905,  0.        ]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 20.0}
episode index:1025
target Thresh 31.99909484797823
target distance 18.0
model initialize at round 1025
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([22.5,  2.5,  0. ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 18.828170383762814}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15015180407975812
{'scaleFactor': 20, 'currentTarget': array([53.4038766 , 25.55576994]), 'previousTarget': array([52.91358021, 25.43619205]), 'currentState': array([72.        , 32.91676134,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 20.0}
episode index:1026
target Thresh 31.99910385439133
target distance 2.0
model initialize at round 1026
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 17.]), 'previousTarget': array([17., 17.]), 'currentState': array([19.5, 16.5,  0. ]), 'targetState': array([17, 17], dtype=int32), 'currentDistance': 2.549509756796453}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15000559979146233
{'scaleFactor': 20, 'currentTarget': array([51.10501922, 34.02181792]), 'previousTarget': array([50.58424532, 33.664211  ]), 'currentState': array([69.        , 42.95320431,  0.        ]), 'targetState': array([17, 17], dtype=int32), 'currentDistance': 20.0}
episode index:1027
target Thresh 31.999112771189118
target distance 12.0
model initialize at round 1027
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 10.]), 'previousTarget': array([20., 10.]), 'currentState': array([ 8.5, 16.5,  0. ]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 13.209844813622798}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.15064735297086873
{'scaleFactor': 20, 'currentTarget': array([20., 10.]), 'previousTarget': array([20., 10.]), 'currentState': array([19.        ,  9.66612407,  0.        ]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 1.0542642631655699}
episode index:1028
target Thresh 31.99912159926329
target distance 21.0
model initialize at round 1028
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.81685674, 28.68257132]), 'previousTarget': array([23.23047895, 28.49442256]), 'currentState': array([ 4.5, 23.5,  0. ]), 'targetState': array([25, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.1511576364461743
{'scaleFactor': 20, 'currentTarget': array([25., 29.]), 'previousTarget': array([25., 29.]), 'currentState': array([24.        , 28.76250053,  0.        ]), 'targetState': array([25, 29], dtype=int32), 'currentDistance': 1.027816130537534}
episode index:1029
target Thresh 31.999130339496656
target distance 8.0
model initialize at round 1029
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([7.5, 7.5, 0. ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 8.27647267862343}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.1518628436175848
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.        , 10.60875462,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.0738123427320203}
episode index:1030
target Thresh 31.999138992763246
target distance 15.0
model initialize at round 1030
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 7.]), 'previousTarget': array([9., 7.]), 'currentState': array([24.5,  4.5,  0. ]), 'targetState': array([9, 7], dtype=int32), 'currentDistance': 15.700318468107664}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15171554697004105
{'scaleFactor': 20, 'currentTarget': array([55.38478844, 25.22126515]), 'previousTarget': array([54.86148505, 24.84651651]), 'currentState': array([74.       , 32.5338501,  0.       ]), 'targetState': array([9, 7], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:1031
target Thresh 31.999147559928396
target distance 10.0
model initialize at round 1031
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 22.]), 'previousTarget': array([15., 22.]), 'currentState': array([25.5, 20.5,  0. ]), 'targetState': array([15, 22], dtype=int32), 'currentDistance': 10.606601717798279}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1515685357811166
{'scaleFactor': 20, 'currentTarget': array([57.88509166, 47.92900005]), 'previousTarget': array([57.39926261, 47.71452543]), 'currentState': array([75.        , 58.27694248,  0.        ]), 'targetState': array([15, 22], dtype=int32), 'currentDistance': 20.0}
episode index:1032
target Thresh 31.999156041848824
target distance 7.0
model initialize at round 1032
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 21.]), 'previousTarget': array([10., 21.]), 'currentState': array([ 3.5       , 21.48175842,  0.        ]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 6.517828715998673}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.15228854518912976
{'scaleFactor': 20, 'currentTarget': array([10., 21.]), 'previousTarget': array([10., 21.]), 'currentState': array([ 9.       , 20.7893601,  0.       ]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 1.0219438176909614}
episode index:1033
target Thresh 31.999164439372738
target distance 8.0
model initialize at round 1033
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'previousTarget': array([26., 11.]), 'currentState': array([26.5,  3.5,  0. ]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 7.516648189186464}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15214126419765092
{'scaleFactor': 20, 'currentTarget': array([58.67256376, 29.83332597]), 'previousTarget': array([58.16016245, 29.48474657]), 'currentState': array([76.        , 39.82131642,  0.        ]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 20.0}
episode index:1034
target Thresh 31.99917275333989
target distance 17.0
model initialize at round 1034
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 25.]), 'previousTarget': array([27., 25.]), 'currentState': array([10.5       , 25.49978095,  0.        ]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 16.507567385926663}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.15270180729471058
{'scaleFactor': 20, 'currentTarget': array([27., 25.]), 'previousTarget': array([27., 25.]), 'currentState': array([26.        , 24.84262116,  0.        ]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 1.0123083025226878}
episode index:1035
target Thresh 31.99918098458169
target distance 6.0
model initialize at round 1035
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 26.]), 'previousTarget': array([18., 26.]), 'currentState': array([24.5, 25.5,  0. ]), 'targetState': array([18, 26], dtype=int32), 'currentDistance': 6.519202405202725}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15255441172782377
{'scaleFactor': 20, 'currentTarget': array([56.0857304 , 44.90573713]), 'previousTarget': array([55.60486714, 44.76779247]), 'currentState': array([74.        , 53.79837142,  0.        ]), 'targetState': array([18, 26], dtype=int32), 'currentDistance': 20.0}
episode index:1036
target Thresh 31.999189133921266
target distance 24.0
model initialize at round 1036
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.47321941, 15.32052197]), 'previousTarget': array([11.2569172 , 15.48550743]), 'currentState': array([26.5       , 28.51885009,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1524073004339686
{'scaleFactor': 20, 'currentTarget': array([56.91835632, 24.2422433 ]), 'previousTarget': array([56.40366944, 23.93366901]), 'currentState': array([76.        , 30.23314261,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 20.0}
episode index:1037
target Thresh 31.999197202173555
target distance 21.0
model initialize at round 1037
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.0401881 , 20.08023371]), 'previousTarget': array([15.58396457, 19.54489741]), 'currentState': array([3.5, 4.5, 0. ]), 'targetState': array([20, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15226047259154668
{'scaleFactor': 20, 'currentTarget': array([33.75075986, 28.87774507]), 'previousTarget': array([33.28479007, 28.8354258 ]), 'currentState': array([53.       , 34.3060739,  0.       ]), 'targetState': array([20, 25], dtype=int32), 'currentDistance': 20.0}
episode index:1038
target Thresh 31.999205190145396
target distance 13.0
model initialize at round 1038
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 20.]), 'previousTarget': array([ 4., 20.]), 'currentState': array([3.5, 7.5, 0. ]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 12.509996003196807}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15211392738212265
{'scaleFactor': 20, 'currentTarget': array([33.01589877, 21.15764901]), 'previousTarget': array([32.51626116, 21.15062374]), 'currentState': array([53.        , 21.95495587,  0.        ]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:1039
target Thresh 31.999213098635586
target distance 3.0
model initialize at round 1039
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 16.]), 'previousTarget': array([13., 16.]), 'currentState': array([16.5, 13.5,  0. ]), 'targetState': array([13, 16], dtype=int32), 'currentDistance': 4.301162633521333}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1519676639904091
{'scaleFactor': 20, 'currentTarget': array([46.00000739, 15.97163638]), 'previousTarget': array([45.50000312, 15.98183512]), 'currentState': array([66.        , 15.95444631,  0.        ]), 'targetState': array([13, 16], dtype=int32), 'currentDistance': 20.0}
episode index:1040
target Thresh 31.999220928434983
target distance 13.0
model initialize at round 1040
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 29.]), 'previousTarget': array([ 5., 29.]), 'currentState': array([18.5, 18.5,  0. ]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 17.102631376487086}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15182168160425114
{'scaleFactor': 20, 'currentTarget': array([48.01209853, 27.50323357]), 'previousTarget': array([47.51191084, 27.5321697 ]), 'currentState': array([68.        , 26.80767997,  0.        ]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 20.0}
episode index:1041
target Thresh 31.999228680326578
target distance 4.0
model initialize at round 1041
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 12.]), 'previousTarget': array([11., 12.]), 'currentState': array([15.5       , 12.53221208,  0.        ]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 4.531362896147013}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15167597941461175
{'scaleFactor': 20, 'currentTarget': array([45.05655471,  9.43340312]), 'previousTarget': array([44.55582291,  9.48762347]), 'currentState': array([65.       ,  7.9304095,  0.       ]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:1042
target Thresh 31.99923635508556
target distance 5.0
model initialize at round 1042
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([19.5       ,  8.53592995,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 6.538562578870958}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1515305566155565
{'scaleFactor': 20, 'currentTarget': array([49.05782097,  2.32840341]), 'previousTarget': array([48.55774527,  2.36824394]), 'currentState': array([69.        ,  0.80870093,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:1043
target Thresh 31.999243953479418
target distance 5.0
model initialize at round 1043
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([14.5, 16.5,  0. ]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 4.527692569068636}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15138541240423892
{'scaleFactor': 20, 'currentTarget': array([44.05471394, 18.77231347]), 'previousTarget': array([43.55242616, 18.85601222]), 'currentState': array([64.        , 17.29394821,  0.        ]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:1044
target Thresh 31.99925147626799
target distance 15.0
model initialize at round 1044
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([ 6.5, 13.5,  0. ]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 19.1441897190766}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.15197006054006942
{'scaleFactor': 20, 'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([20.       , 25.9625991,  0.       ]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 1.0006991692888099}
episode index:1045
target Thresh 31.999258924203566
target distance 11.0
model initialize at round 1045
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([26.5,  6.5,  0. ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 11.59741350474328}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15182477367530836
{'scaleFactor': 20, 'currentTarget': array([56.04156381,  2.34991816]), 'previousTarget': array([55.5424514 ,  2.35425345]), 'currentState': array([76.        ,  1.06118829,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 20.0}
episode index:1046
target Thresh 31.999266298030946
target distance 4.0
model initialize at round 1046
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([17.5       ,  9.51257288,  0.        ]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 4.747407369407608}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15167976434037495
{'scaleFactor': 20, 'currentTarget': array([47.04699096,  5.66195848]), 'previousTarget': array([46.54895368,  5.64836434]), 'currentState': array([67.        ,  4.29176504,  0.        ]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:1047
target Thresh 31.999273598487516
target distance 9.0
model initialize at round 1047
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  3.]), 'previousTarget': array([17.,  3.]), 'currentState': array([26.5       ,  2.52369028,  0.        ]), 'targetState': array([17,  3], dtype=int32), 'currentDistance': 9.511933081462766}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1515350317408135
{'scaleFactor': 20, 'currentTarget': array([56.04883121,  0.26628527]), 'previousTarget': array([55.54919753,  0.29112238]), 'currentState': array([76.        , -1.13044805,  0.        ]), 'targetState': array([17,  3], dtype=int32), 'currentDistance': 20.0}
episode index:1048
target Thresh 31.99928082630333
target distance 8.0
model initialize at round 1048
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 13.]), 'previousTarget': array([ 5., 13.]), 'currentState': array([ 9.5       , 21.09681834,  0.        ]), 'targetState': array([ 5, 13], dtype=int32), 'currentDistance': 9.263285986969336}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15139057508519785
{'scaleFactor': 20, 'currentTarget': array([39.04919586, 10.60738125]), 'previousTarget': array([38.54930312, 10.63993048]), 'currentState': array([59.        ,  9.20544894,  0.        ]), 'targetState': array([ 5, 13], dtype=int32), 'currentDistance': 20.0}
episode index:1049
target Thresh 31.999287982201174
target distance 11.0
model initialize at round 1049
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 24.]), 'previousTarget': array([25., 24.]), 'currentState': array([19.5, 13.5,  0. ]), 'targetState': array([25, 24], dtype=int32), 'currentDistance': 11.85326959112959}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1512463935851167
{'scaleFactor': 20, 'currentTarget': array([49.02222038, 22.86668396]), 'previousTarget': array([48.5217704, 22.9016064]), 'currentState': array([69.        , 21.92417583,  0.        ]), 'targetState': array([25, 24], dtype=int32), 'currentDistance': 20.0}
episode index:1050
target Thresh 31.999295066896646
target distance 15.0
model initialize at round 1050
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 23.]), 'previousTarget': array([ 8., 23.]), 'currentState': array([23.5       , 22.50048712,  0.        ]), 'targetState': array([ 8, 23], dtype=int32), 'currentDistance': 15.50804672156774}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15110248645515942
{'scaleFactor': 20, 'currentTarget': array([53.04981486, 19.81444641]), 'previousTarget': array([52.55097067, 19.813245  ]), 'currentState': array([73.        , 18.40373287,  0.        ]), 'targetState': array([ 8, 23], dtype=int32), 'currentDistance': 20.0}
episode index:1051
target Thresh 31.99930208109822
target distance 13.0
model initialize at round 1051
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 12.]), 'previousTarget': array([ 9., 12.]), 'currentState': array([22.5,  8.5,  0. ]), 'targetState': array([ 9, 12], dtype=int32), 'currentDistance': 13.946325680981412}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15095885291290168
{'scaleFactor': 20, 'currentTarget': array([52.03675547,  9.38723873]), 'previousTarget': array([51.53894617,  9.34139324]), 'currentState': array([72.        ,  8.17527023,  0.        ]), 'targetState': array([ 9, 12], dtype=int32), 'currentDistance': 20.0}
episode index:1052
target Thresh 31.999309025507323
target distance 13.0
model initialize at round 1052
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([19.5,  8.5,  0. ]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 16.3248277173145}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1508154921788913
{'scaleFactor': 20, 'currentTarget': array([49.04764048, 18.230879  ]), 'previousTarget': array([48.54276735, 18.40987581]), 'currentState': array([69.       , 16.8512597,  0.       ]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 20.0}
episode index:1053
target Thresh 31.9993159008184
target distance 6.0
model initialize at round 1053
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 14.]), 'previousTarget': array([ 7., 14.]), 'currentState': array([4.5, 8.5, 0. ]), 'targetState': array([ 7, 14], dtype=int32), 'currentDistance': 6.041522986797215}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1506724034766343
{'scaleFactor': 20, 'currentTarget': array([34.0166482 , 12.89697234]), 'previousTarget': array([33.5135227 , 13.02451747]), 'currentState': array([54.        , 12.08109802,  0.        ]), 'targetState': array([ 7, 14], dtype=int32), 'currentDistance': 20.0}
episode index:1054
target Thresh 31.99932270771899
target distance 6.0
model initialize at round 1054
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 21.]), 'previousTarget': array([19., 21.]), 'currentState': array([13.5       , 21.49697238,  0.        ]), 'targetState': array([19, 21], dtype=int32), 'currentDistance': 5.522407224077038}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.15139547915815754
{'scaleFactor': 20, 'currentTarget': array([19., 21.]), 'previousTarget': array([19., 21.]), 'currentState': array([18.        , 20.87939046,  0.        ]), 'targetState': array([19, 21], dtype=int32), 'currentDistance': 1.007247071057909}
episode index:1055
target Thresh 31.999329446889785
target distance 2.0
model initialize at round 1055
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 4.]), 'previousTarget': array([6., 4.]), 'currentState': array([4.5, 2.5, 0. ]), 'targetState': array([6, 4], dtype=int32), 'currentDistance': 2.1213203435596495}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.15218023722713656
{'scaleFactor': 20, 'currentTarget': array([6., 4.]), 'previousTarget': array([6., 4.]), 'currentState': array([5.5       , 3.49978548, 0.        ]), 'targetState': array([6, 4], dtype=int32), 'currentDistance': 0.7072584839605477}
episode index:1056
target Thresh 31.999336119004713
target distance 5.0
model initialize at round 1056
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 12.]), 'previousTarget': array([12., 12.]), 'currentState': array([11.5       , 16.50264269,  0.        ]), 'targetState': array([12, 12], dtype=int32), 'currentDistance': 4.530319106203865}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1520362634927684
{'scaleFactor': 20, 'currentTarget': array([41.12320328,  8.75238888]), 'previousTarget': array([40.62469136,  8.78857976]), 'currentState': array([61.        ,  6.53587083,  0.        ]), 'targetState': array([12, 12], dtype=int32), 'currentDistance': 20.0}
episode index:1057
target Thresh 31.999342724730987
target distance 11.0
model initialize at round 1057
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([21.5      ,  7.4999395,  0.       ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 11.597421329752796}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.151892561920469
{'scaleFactor': 20, 'currentTarget': array([51.11857414,  4.50250329]), 'previousTarget': array([50.6179435 ,  4.56919733]), 'currentState': array([71.        ,  2.32789853,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 20.0}
episode index:1058
target Thresh 31.999349264729187
target distance 12.0
model initialize at round 1058
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 22.]), 'previousTarget': array([10., 22.]), 'currentState': array([22.5, 23.5,  0. ]), 'targetState': array([10, 22], dtype=int32), 'currentDistance': 12.589678312014243}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15174913173924098
{'scaleFactor': 20, 'currentTarget': array([52.11140561, 17.5365205 ]), 'previousTarget': array([51.60949167, 17.62808361]), 'currentState': array([72.        , 15.42848511,  0.        ]), 'targetState': array([10, 22], dtype=int32), 'currentDistance': 20.0}
episode index:1059
target Thresh 31.999355739653314
target distance 11.0
model initialize at round 1059
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  2.]), 'previousTarget': array([25.,  2.]), 'currentState': array([14.5       , 11.50000003,  0.        ]), 'targetState': array([25,  2], dtype=int32), 'currentDistance': 14.159802278500974}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.15238537654310547
{'scaleFactor': 20, 'currentTarget': array([25.,  2.]), 'previousTarget': array([25.,  2.]), 'currentState': array([24.        ,  2.05611017,  0.        ]), 'targetState': array([25,  2], dtype=int32), 'currentDistance': 1.0015729387133663}
episode index:1060
target Thresh 31.999362150150873
target distance 7.0
model initialize at round 1060
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 24.]), 'previousTarget': array([ 6., 24.]), 'currentState': array([ 7.5, 17.5,  0. ]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 6.670832032063086}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15224175224853137
{'scaleFactor': 20, 'currentTarget': array([37.12004037, 20.57495985]), 'previousTarget': array([36.6229988 , 20.58800388]), 'currentState': array([57.        , 18.38699161,  0.        ]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:1061
target Thresh 31.999368496862914
target distance 4.0
model initialize at round 1061
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  2.]), 'previousTarget': array([23.,  2.]), 'currentState': array([19.5       ,  2.37494767,  0.        ]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 3.520026385281736}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.15299386928963446
{'scaleFactor': 20, 'currentTarget': array([23.,  2.]), 'previousTarget': array([23.,  2.]), 'currentState': array([22.        ,  1.89256936,  0.        ]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 1.005754115944887}
episode index:1062
target Thresh 31.999374780424116
target distance 20.0
model initialize at round 1062
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.47366596,  5.17544468]), 'previousTarget': array([22.87716713,  5.39299151]), 'currentState': array([ 4.5, 11.5,  0. ]), 'targetState': array([24,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.15349853082922002
{'scaleFactor': 20, 'currentTarget': array([24.,  5.]), 'previousTarget': array([24.,  5.]), 'currentState': array([23.        ,  4.91903266,  0.        ]), 'targetState': array([24,  5], dtype=int32), 'currentDistance': 1.0032725007919359}
episode index:1063
target Thresh 31.99938100146284
target distance 9.0
model initialize at round 1063
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 14.]), 'previousTarget': array([27., 14.]), 'currentState': array([18.5       , 20.50000054,  0.        ]), 'targetState': array([27, 14], dtype=int32), 'currentDistance': 10.700467605377927}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.15416259081400582
{'scaleFactor': 20, 'currentTarget': array([27., 14.]), 'previousTarget': array([27., 14.]), 'currentState': array([26.        , 14.00104654,  0.        ]), 'targetState': array([27, 14], dtype=int32), 'currentDistance': 1.000000547619834}
episode index:1064
target Thresh 31.999387160601195
target distance 18.0
model initialize at round 1064
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 24.]), 'previousTarget': array([16., 24.]), 'currentState': array([23.5,  6.5,  0. ]), 'targetState': array([16, 24], dtype=int32), 'currentDistance': 19.039432764659807}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15401783720760767
{'scaleFactor': 20, 'currentTarget': array([53.099519  , 20.28510306]), 'previousTarget': array([52.59739518, 20.37498665]), 'currentState': array([73.        , 18.29240239,  0.        ]), 'targetState': array([16, 24], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:1065
target Thresh 31.999393258455097
target distance 15.0
model initialize at round 1065
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 6.]), 'previousTarget': array([9., 6.]), 'currentState': array([24.5, 14.5,  0. ]), 'targetState': array([9, 6], dtype=int32), 'currentDistance': 17.67766952966372}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15387335518396078
{'scaleFactor': 20, 'currentTarget': array([54.09132271,  1.67611993]), 'previousTarget': array([53.58253867,  1.93705448]), 'currentState': array([74.        , -0.23295543,  0.        ]), 'targetState': array([9, 6], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:1066
target Thresh 31.99939929563434
target distance 13.0
model initialize at round 1066
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 2.5       , 23.49938548,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 13.945730824617165}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1537291439794772
{'scaleFactor': 20, 'currentTarget': array([35.04100282, 28.15431814]), 'previousTarget': array([34.67741391, 28.43916559]), 'currentState': array([52.        , 38.75584703,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 20.0}
episode index:1067
target Thresh 31.999405272742646
target distance 5.0
model initialize at round 1067
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 19.]), 'previousTarget': array([20., 19.]), 'currentState': array([15.5       , 19.49152544,  0.        ]), 'targetState': array([20, 19], dtype=int32), 'currentDistance': 4.526764546511211}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.1544579231966378
{'scaleFactor': 20, 'currentTarget': array([20., 19.]), 'previousTarget': array([20., 19.]), 'currentState': array([19.        , 18.96091428,  0.        ]), 'targetState': array([20, 19], dtype=int32), 'currentDistance': 1.0007635553801288}
episode index:1068
target Thresh 31.99941119037773
target distance 20.0
model initialize at round 1068
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.6597004 , 23.72731807]), 'previousTarget': array([21.28991511, 23.14985851]), 'currentState': array([11.5,  6.5,  0. ]), 'targetState': array([23, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15431343496165498
{'scaleFactor': 20, 'currentTarget': array([41.0956895 , 24.22348026]), 'previousTarget': array([40.59797436, 24.25168738]), 'currentState': array([61.       , 22.2694016,  0.       ]), 'targetState': array([23, 26], dtype=int32), 'currentDistance': 20.0}
episode index:1069
target Thresh 31.999417049131363
target distance 21.0
model initialize at round 1069
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6.20626792, 24.58327697]), 'previousTarget': array([ 5.94278962, 24.59867161]), 'currentState': array([24.5, 16.5,  0. ]), 'targetState': array([ 3, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1541692167981394
{'scaleFactor': 20, 'currentTarget': array([54.1101979 , 20.61240804]), 'previousTarget': array([53.61035237, 20.66132844]), 'currentState': array([74.        , 18.51579831,  0.        ]), 'targetState': array([ 3, 26], dtype=int32), 'currentDistance': 20.0}
episode index:1070
target Thresh 31.999422849589422
target distance 17.0
model initialize at round 1070
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 21.]), 'previousTarget': array([ 3., 21.]), 'currentState': array([20.5, 17.5,  0. ]), 'targetState': array([ 3, 21], dtype=int32), 'currentDistance': 17.84656829757481}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1540252679495884
{'scaleFactor': 20, 'currentTarget': array([50.09481117, 16.39795139]), 'previousTarget': array([49.59378834, 16.47171155]), 'currentState': array([70.        , 14.45284015,  0.        ]), 'targetState': array([ 3, 21], dtype=int32), 'currentDistance': 20.0}
episode index:1071
target Thresh 31.99942859233196
target distance 3.0
model initialize at round 1071
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([12.5       , 12.50245008,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 3.537266797474295}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.1547776660298593
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.5       , 10.60790119,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.7871110796049635}
episode index:1072
target Thresh 31.999434277933254
target distance 11.0
model initialize at round 1072
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([17.5       , 21.47924599,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 12.72882903140534}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15463341843803277
{'scaleFactor': 20, 'currentTarget': array([47.08717632,  6.71321681]), 'previousTarget': array([46.58876562,  6.73030798]), 'currentState': array([67.        ,  4.84788762,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 20.0}
episode index:1073
target Thresh 31.999439906961868
target distance 7.0
model initialize at round 1073
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 15.]), 'previousTarget': array([25., 15.]), 'currentState': array([18.5, 11.5,  0. ]), 'targetState': array([25, 15], dtype=int32), 'currentDistance': 7.382411530116591}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.15532308774512837
{'scaleFactor': 20, 'currentTarget': array([25., 15.]), 'previousTarget': array([25., 15.]), 'currentState': array([24.        , 15.02720336,  0.        ]), 'targetState': array([25, 15], dtype=int32), 'currentDistance': 1.0003699429327164}
episode index:1074
target Thresh 31.999445479980714
target distance 14.0
model initialize at round 1074
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 23.]), 'previousTarget': array([11., 23.]), 'currentState': array([25.5, 28.5,  0. ]), 'targetState': array([11, 23], dtype=int32), 'currentDistance': 15.508062419270868}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1551786011518771
{'scaleFactor': 20, 'currentTarget': array([55.06039862, 19.56799963]), 'previousTarget': array([54.56166928, 19.57117654]), 'currentState': array([75.        , 18.01484256,  0.        ]), 'targetState': array([11, 23], dtype=int32), 'currentDistance': 20.0}
episode index:1075
target Thresh 31.999450997547097
target distance 11.0
model initialize at round 1075
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 14.]), 'previousTarget': array([ 6., 14.]), 'currentState': array([14.5,  3.5,  0. ]), 'targetState': array([ 6, 14], dtype=int32), 'currentDistance': 13.50925608610635}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1550343831210668
{'scaleFactor': 20, 'currentTarget': array([44.05524045, 11.16571084]), 'previousTarget': array([43.55981045, 11.08869499]), 'currentState': array([64.       ,  9.6802593,  0.       ]), 'targetState': array([ 6, 14], dtype=int32), 'currentDistance': 20.0}
episode index:1076
target Thresh 31.999456460212773
target distance 3.0
model initialize at round 1076
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 18.]), 'previousTarget': array([13., 18.]), 'currentState': array([13.5       , 20.92385483,  0.        ]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 2.966298544402097}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15489043290461268
{'scaleFactor': 20, 'currentTarget': array([43.0830726 , 15.24952743]), 'previousTarget': array([42.58471533, 15.26830947]), 'currentState': array([63.        , 13.42853782,  0.        ]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 20.0}
episode index:1077
target Thresh 31.99946186852402
target distance 12.0
model initialize at round 1077
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 17.]), 'previousTarget': array([21., 17.]), 'currentState': array([22.5,  5.5,  0. ]), 'targetState': array([21, 17], dtype=int32), 'currentDistance': 11.597413504743217}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1547467497572058
{'scaleFactor': 20, 'currentTarget': array([52.057874  , 14.63213143]), 'previousTarget': array([51.55558861, 14.71708172]), 'currentState': array([72.       , 13.1117333,  0.       ]), 'targetState': array([21, 17], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:1078
target Thresh 31.99946722302167
target distance 19.0
model initialize at round 1078
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 27.]), 'previousTarget': array([24., 27.]), 'currentState': array([19.5,  8.5,  0. ]), 'targetState': array([24, 27], dtype=int32), 'currentDistance': 19.039432764659676}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15460333293630013
{'scaleFactor': 20, 'currentTarget': array([49.05792907, 25.08865756]), 'previousTarget': array([48.5584991 , 25.11751829]), 'currentState': array([69.        , 23.56753718,  0.        ]), 'targetState': array([24, 27], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:1079
target Thresh 31.99947252424118
target distance 16.0
model initialize at round 1079
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.64213475, 14.35786541]), 'previousTarget': array([24.14213562, 14.85786438]), 'currentState': array([10.5       , 28.50000191,  0.        ]), 'targetState': array([26, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.15514508945523714
{'scaleFactor': 20, 'currentTarget': array([26., 13.]), 'previousTarget': array([26., 13.]), 'currentState': array([25.5       , 13.66088027,  0.        ]), 'targetState': array([26, 13], dtype=int32), 'currentDistance': 0.8287114866633054}
episode index:1080
target Thresh 31.999477772712673
target distance 5.0
model initialize at round 1080
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 10.]), 'previousTarget': array([22., 10.]), 'currentState': array([17.5       , 10.49924025,  0.        ]), 'targetState': array([22, 10], dtype=int32), 'currentDistance': 4.527608731617119}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.15586379459719066
{'scaleFactor': 20, 'currentTarget': array([22., 10.]), 'previousTarget': array([22., 10.]), 'currentState': array([21.       , 10.0145029,  0.       ]), 'targetState': array([22, 10], dtype=int32), 'currentDistance': 1.0001051615470606}
episode index:1081
target Thresh 31.999482968961004
target distance 5.0
model initialize at round 1081
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  5.]), 'previousTarget': array([13.,  5.]), 'currentState': array([8.5       , 8.50037235, 0.        ]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 5.701105733991918}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.15658117126383558
{'scaleFactor': 20, 'currentTarget': array([13.,  5.]), 'previousTarget': array([13.,  5.]), 'currentState': array([12.        ,  5.13604534,  0.        ]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 1.0092117387560002}
episode index:1082
target Thresh 31.999488113505798
target distance 7.0
model initialize at round 1082
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 17.]), 'previousTarget': array([11., 17.]), 'currentState': array([12.5       , 24.49200767,  0.        ]), 'targetState': array([11, 17], dtype=int32), 'currentDistance': 7.640692309542174}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15643659031160673
{'scaleFactor': 20, 'currentTarget': array([42.05504522, 14.691181  ]), 'previousTarget': array([41.55593866, 14.70984868]), 'currentState': array([62.      , 13.208353,  0.      ]), 'targetState': array([11, 17], dtype=int32), 'currentDistance': 20.0}
episode index:1083
target Thresh 31.999493206861516
target distance 10.0
model initialize at round 1083
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 19.]), 'previousTarget': array([19., 19.]), 'currentState': array([ 9.5       , 23.50000578,  0.        ]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 10.511900495861237}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.1570698989860277
{'scaleFactor': 20, 'currentTarget': array([19., 19.]), 'previousTarget': array([19., 19.]), 'currentState': array([18.        , 19.17790471,  0.        ]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 1.0157017701565652}
episode index:1084
target Thresh 31.9994982495375
target distance 19.0
model initialize at round 1084
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 21.]), 'previousTarget': array([15., 21.]), 'currentState': array([21.5,  2.5,  0. ]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 19.608671551127607}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1569251341021696
{'scaleFactor': 20, 'currentTarget': array([51.05457217, 18.33107648]), 'previousTarget': array([50.55647198, 18.32233401]), 'currentState': array([71.        , 16.85462513,  0.        ]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 20.0}
episode index:1085
target Thresh 31.999503242038017
target distance 5.0
model initialize at round 1085
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 22.]), 'previousTarget': array([ 9., 22.]), 'currentState': array([ 4.5       , 25.50069761,  0.        ]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 5.701305444896025}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.15763889120512062
{'scaleFactor': 20, 'currentTarget': array([ 9., 22.]), 'previousTarget': array([ 9., 22.]), 'currentState': array([ 8.        , 22.13514528,  0.        ]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 1.0090908015818918}
episode index:1086
target Thresh 31.999508184862325
target distance 4.0
model initialize at round 1086
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 28.]), 'previousTarget': array([21., 28.]), 'currentState': array([23.5, 24.5,  0. ]), 'targetState': array([21, 28], dtype=int32), 'currentDistance': 4.301162633521295}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15749386922609107
{'scaleFactor': 20, 'currentTarget': array([53.06572318, 25.39400989]), 'previousTarget': array([52.56316809, 25.48545095]), 'currentState': array([73.        , 23.77394576,  0.        ]), 'targetState': array([21, 28], dtype=int32), 'currentDistance': 20.0}
episode index:1087
target Thresh 31.999513078504712
target distance 17.0
model initialize at round 1087
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 22.]), 'previousTarget': array([ 3., 22.]), 'currentState': array([8.5, 5.5, 0. ]), 'targetState': array([ 3, 22], dtype=int32), 'currentDistance': 17.39252713092612}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1573491138315818
{'scaleFactor': 20, 'currentTarget': array([38.04824948, 19.56106888]), 'previousTarget': array([37.55242826, 19.49321875]), 'currentState': array([58.        , 18.17267015,  0.        ]), 'targetState': array([ 3, 22], dtype=int32), 'currentDistance': 20.0}
episode index:1088
target Thresh 31.99951792345454
target distance 13.0
model initialize at round 1088
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  8.]), 'previousTarget': array([17.,  8.]), 'currentState': array([4.5, 3.5, 0. ]), 'targetState': array([17,  8], dtype=int32), 'currentDistance': 13.285330255586427}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.1579333793685993
{'scaleFactor': 20, 'currentTarget': array([17.,  8.]), 'previousTarget': array([17.,  8.]), 'currentState': array([16.        ,  8.00190598,  0.        ]), 'targetState': array([17,  8], dtype=int32), 'currentDistance': 1.000001816368599}
episode index:1089
target Thresh 31.999522720196314
target distance 3.0
model initialize at round 1089
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([18.5       ,  5.64832294,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 3.5595396660756258}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15778848636000425
{'scaleFactor': 20, 'currentTarget': array([48.06207104,  2.38911761]), 'previousTarget': array([47.56132016,  2.44433391]), 'currentState': array([68.        ,  0.81463707,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 20.0}
episode index:1090
target Thresh 31.999527469209706
target distance 6.0
model initialize at round 1090
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([13.5       , 19.46125323,  0.        ]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 5.6908049509643215}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.1584811799998976
{'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([18.        , 18.03411552,  0.        ]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 1.000581765104977}
episode index:1091
target Thresh 31.99953217096963
target distance 4.0
model initialize at round 1091
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  5.]), 'previousTarget': array([23.,  5.]), 'currentState': array([26.5       ,  9.27074134,  0.        ]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 5.5217054994870685}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1583360507141834
{'scaleFactor': 20, 'currentTarget': array([56.05766405,  2.48426181]), 'previousTarget': array([55.55676729,  2.54180448]), 'currentState': array([76.       ,  0.9666199,  0.       ]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 20.0}
episode index:1092
target Thresh 31.999536825946258
target distance 21.0
model initialize at round 1092
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6.9007856 , 10.14682221]), 'previousTarget': array([ 6.6170994 , 10.12161403]), 'currentState': array([25.5, 17.5,  0. ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15819118698983375
{'scaleFactor': 20, 'currentTarget': array([55.05226983,  5.30177629]), 'previousTarget': array([54.55122537,  5.37498564]), 'currentState': array([75.        ,  3.85676378,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 20.0}
episode index:1093
target Thresh 31.999541434605096
target distance 18.0
model initialize at round 1093
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.21410959, 10.51644227]), 'previousTarget': array([22.64100589, 10.90599608]), 'currentState': array([ 6.5, 21.5,  0. ]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.15870264891452152
{'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([23.        , 10.05650964,  0.        ]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 1.0015953970093532}
episode index:1094
target Thresh 31.99954599740701
target distance 5.0
model initialize at round 1094
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  4.]), 'previousTarget': array([24.,  4.]), 'currentState': array([25.5       ,  9.46019462,  0.        ]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 5.6624840186162055}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15855771498857218
{'scaleFactor': 20, 'currentTarget': array([55.05574215,  1.67649788]), 'previousTarget': array([54.54908516,  1.85575759]), 'currentState': array([75.        ,  0.18432535,  0.        ]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 20.0}
episode index:1095
target Thresh 31.99955051480829
target distance 2.0
model initialize at round 1095
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 17.]), 'previousTarget': array([24., 17.]), 'currentState': array([26.5       , 16.49999976,  0.        ]), 'targetState': array([24, 17], dtype=int32), 'currentDistance': 2.5495098035542316}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1584130455405899
{'scaleFactor': 20, 'currentTarget': array([56.05235154, 14.67630827]), 'previousTarget': array([55.55733157, 14.60540277]), 'currentState': array([76.        , 13.23016832,  0.        ]), 'targetState': array([24, 17], dtype=int32), 'currentDistance': 20.0}
episode index:1096
target Thresh 31.999554987260677
target distance 5.0
model initialize at round 1096
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 24.]), 'previousTarget': array([20., 24.]), 'currentState': array([15.5, 19.5,  0. ]), 'targetState': array([20, 24], dtype=int32), 'currentDistance': 6.363961030678816}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.1591182892072867
{'scaleFactor': 20, 'currentTarget': array([20., 24.]), 'previousTarget': array([20., 24.]), 'currentState': array([19., 23.,  0.]), 'targetState': array([20, 24], dtype=int32), 'currentDistance': 1.4142135623721908}
episode index:1097
target Thresh 31.999559415211415
target distance 7.0
model initialize at round 1097
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 17.]), 'previousTarget': array([11., 17.]), 'currentState': array([ 4.5       , 20.50004166,  0.        ]), 'targetState': array([11, 17], dtype=int32), 'currentDistance': 7.382431282935423}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.15978879919367237
{'scaleFactor': 20, 'currentTarget': array([11., 17.]), 'previousTarget': array([11., 17.]), 'currentState': array([10.        , 17.04149615,  0.        ]), 'targetState': array([11, 17], dtype=int32), 'currentDistance': 1.0008605949283498}
episode index:1098
target Thresh 31.99956379910331
target distance 3.0
model initialize at round 1098
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 25.]), 'previousTarget': array([17., 25.]), 'currentState': array([20.5       , 26.60203978,  0.        ]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 3.8492247882058472}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15964340447193107
{'scaleFactor': 20, 'currentTarget': array([50.04691315, 22.73252106]), 'previousTarget': array([49.54861621, 22.7263899 ]), 'currentState': array([70.        , 21.36346115,  0.        ]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:1099
target Thresh 31.99956813937475
target distance 12.0
model initialize at round 1099
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 18.]), 'previousTarget': array([13., 18.]), 'currentState': array([20.5,  6.5,  0. ]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 13.729530217746026}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15949827410422932
{'scaleFactor': 20, 'currentTarget': array([50.02147392, 16.28304359]), 'previousTarget': array([49.51825283, 16.43874964]), 'currentState': array([70.        , 15.35649327,  0.        ]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 20.0}
episode index:1100
target Thresh 31.99957243645977
target distance 12.0
model initialize at round 1100
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([27.5       , 17.50000089,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 14.089003216086935}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15935340737025636
{'scaleFactor': 20, 'currentTarget': array([57.04050514,  8.3203168 ]), 'previousTarget': array([56.54435563,  8.22853311]), 'currentState': array([77.        ,  7.04808843,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 20.0}
episode index:1101
target Thresh 31.99957669078808
target distance 9.0
model initialize at round 1101
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 11.]), 'previousTarget': array([27., 11.]), 'currentState': array([27.5,  2.5,  0. ]), 'targetState': array([27, 11], dtype=int32), 'currentDistance': 8.51469318296321}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.159208803552316
{'scaleFactor': 20, 'currentTarget': array([57.04046087,  9.08625556]), 'previousTarget': array([56.53971382,  9.13566206]), 'currentState': array([77.       ,  7.8147219,  0.       ]), 'targetState': array([27, 11], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:1102
target Thresh 31.999580902785112
target distance 12.0
model initialize at round 1102
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 15.]), 'previousTarget': array([ 8., 15.]), 'currentState': array([5.5, 3.5, 0. ]), 'targetState': array([ 8, 15], dtype=int32), 'currentDistance': 11.768602295939825}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15906446193531482
{'scaleFactor': 20, 'currentTarget': array([35.04469894, 13.18882803]), 'previousTarget': array([34.53841304, 13.35282177]), 'currentState': array([55.        , 11.85243002,  0.        ]), 'targetState': array([ 8, 15], dtype=int32), 'currentDistance': 20.0}
episode index:1103
target Thresh 31.999585072872076
target distance 18.0
model initialize at round 1103
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 14.]), 'previousTarget': array([19.48314552, 14.28714138]), 'currentState': array([ 2.5       , 23.50000003,  0.        ]), 'targetState': array([20, 14], dtype=int32), 'currentDistance': 19.9123077659583}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.15957050004279938
{'scaleFactor': 20, 'currentTarget': array([20., 14.]), 'previousTarget': array([20., 14.]), 'currentState': array([19.        , 14.58255373,  0.        ]), 'targetState': array([20, 14], dtype=int32), 'currentDistance': 1.1573110418777812}
episode index:1104
target Thresh 31.99958920146598
target distance 15.0
model initialize at round 1104
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  5.]), 'previousTarget': array([22.,  5.]), 'currentState': array([ 7.5       , 10.50002822,  0.        ]), 'targetState': array([22,  5], dtype=int32), 'currentDistance': 15.50807242862853}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.16011599525936437
{'scaleFactor': 20, 'currentTarget': array([22.,  5.]), 'previousTarget': array([22.,  5.]), 'currentState': array([21.        ,  5.66399713,  0.        ]), 'targetState': array([22,  5], dtype=int32), 'currentDistance': 1.2003716889946574}
episode index:1105
target Thresh 31.99959328897969
target distance 13.0
model initialize at round 1105
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 25.]), 'previousTarget': array([ 6., 25.]), 'currentState': array([19.5, 15.5,  0. ]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 16.507574019219195}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15997122492007018
{'scaleFactor': 20, 'currentTarget': array([49.03118372, 22.59422196]), 'previousTarget': array([48.53125832, 22.61932237]), 'currentState': array([69.        , 21.47780973,  0.        ]), 'targetState': array([ 6, 25], dtype=int32), 'currentDistance': 20.0}
episode index:1106
target Thresh 31.999597335821957
target distance 19.0
model initialize at round 1106
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 4.]), 'previousTarget': array([8., 4.]), 'currentState': array([ 7.5       , 23.49999979,  0.        ]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 19.506408994583364}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15982671613513788
{'scaleFactor': 20, 'currentTarget': array([45.4193513 , 56.68818105]), 'previousTarget': array([44.94234449, 56.17213493]), 'currentState': array([57.        , 72.99427119,  0.        ]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 20.0}
episode index:1107
target Thresh 31.999601342397472
target distance 17.0
model initialize at round 1107
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 28.]), 'previousTarget': array([13., 28.]), 'currentState': array([ 9.5, 11.5,  0. ]), 'targetState': array([13, 28], dtype=int32), 'currentDistance': 16.867127793432907}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15968246819638776
{'scaleFactor': 20, 'currentTarget': array([39.02342786, 26.73929752]), 'previousTarget': array([38.52380206, 26.75364859]), 'currentState': array([59.        , 25.77153438,  0.        ]), 'targetState': array([13, 28], dtype=int32), 'currentDistance': 20.0}
episode index:1108
target Thresh 31.999605309106894
target distance 23.0
model initialize at round 1108
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2.55377393, 7.71819641]), 'previousTarget': array([2.41321632, 7.16799178]), 'currentState': array([ 5.5       , 27.49999997,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15953848039819443
{'scaleFactor': 20, 'currentTarget': array([43.24962876, 60.8146143 ]), 'previousTarget': array([42.76968412, 60.30011708]), 'currentState': array([55.        , 76.99882808,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 20.0}
episode index:1109
target Thresh 31.9996092363469
target distance 13.0
model initialize at round 1109
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([23.5,  4.5,  0. ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 14.983324063771773}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15939475203747536
{'scaleFactor': 20, 'currentTarget': array([53.00690318,  9.86974787]), 'previousTarget': array([52.50739338,  9.84387075]), 'currentState': array([73.      ,  9.344315,  0.      ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 20.0}
episode index:1110
target Thresh 31.999613124510212
target distance 10.0
model initialize at round 1110
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 20.]), 'previousTarget': array([ 7., 20.]), 'currentState': array([17.5      , 23.5649927,  0.       ]), 'targetState': array([ 7, 20], dtype=int32), 'currentDistance': 11.088695726859354}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15925128241367925
{'scaleFactor': 20, 'currentTarget': array([47.00894745, 18.80283896]), 'previousTarget': array([46.50731522, 18.93116548]), 'currentState': array([67.        , 18.20466003,  0.        ]), 'targetState': array([ 7, 20], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:1111
target Thresh 31.999616973985653
target distance 6.0
model initialize at round 1111
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([ 3.5, 21.5,  0. ]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 6.041522986797258}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.15992957914485728
{'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([ 8.        , 24.58162764,  0.        ]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 1.1568451568649667}
episode index:1112
target Thresh 31.999620785158175
target distance 19.0
model initialize at round 1112
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.14999657, 25.67353688]), 'previousTarget': array([13.23886  , 25.4327075]), 'currentState': array([21.5,  7.5,  0. ]), 'targetState': array([13, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15978588680061212
{'scaleFactor': 20, 'currentTarget': array([51.00189708, 25.47654512]), 'previousTarget': array([50.50212343, 25.45347622]), 'currentState': array([71.        , 25.20108243,  0.        ]), 'targetState': array([13, 26], dtype=int32), 'currentDistance': 20.0}
episode index:1113
target Thresh 31.999624558408897
target distance 12.0
model initialize at round 1113
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([18.5       , 11.69970244,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 12.615030257208927}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15964245243185035
{'scaleFactor': 20, 'currentTarget': array([48.00352611,  9.2111555 ]), 'previousTarget': array([47.50686931,  8.91185083]), 'currentState': array([68.        ,  8.83561302,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 20.0}
episode index:1114
target Thresh 31.999628294115144
target distance 11.0
model initialize at round 1114
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([24.5       , 11.59943095,  0.        ]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 12.0501412088403}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15949927534446753
{'scaleFactor': 20, 'currentTarget': array([54.11836292,  3.5065699 ]), 'previousTarget': array([53.62253588,  3.4824658 ]), 'currentState': array([74.        ,  1.33389716,  0.        ]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 20.0}
episode index:1115
target Thresh 31.999631992650496
target distance 22.0
model initialize at round 1115
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.06378104, 12.41115241]), 'previousTarget': array([21.50265712, 12.56757793]), 'currentState': array([ 2.5       , 16.56548405,  0.        ]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.1599497975359008
{'scaleFactor': 20, 'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([23.        , 12.83764575,  0.        ]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 1.3044732276873958}
episode index:1116
target Thresh 31.999635654384804
target distance 2.0
model initialize at round 1116
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([24.5       , 11.58157694,  0.        ]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 2.5667550995550914}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15980660165628047
{'scaleFactor': 20, 'currentTarget': array([54.02818754,  9.29776166]), 'previousTarget': array([53.5260355 ,  9.38981183]), 'currentState': array([74.        ,  8.23629714,  0.        ]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 20.0}
episode index:1117
target Thresh 31.99963927968425
target distance 17.0
model initialize at round 1117
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 23.]), 'previousTarget': array([ 8., 23.]), 'currentState': array([9.5, 6.5, 0. ]), 'targetState': array([ 8, 23], dtype=int32), 'currentDistance': 16.568041525780902}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1596636619410244
{'scaleFactor': 20, 'currentTarget': array([39.01512858, 21.79296684]), 'previousTarget': array([38.51709738, 21.73733956]), 'currentState': array([59.        , 21.01520438,  0.        ]), 'targetState': array([ 8, 23], dtype=int32), 'currentDistance': 20.0}
episode index:1118
target Thresh 31.99964286891136
target distance 14.0
model initialize at round 1118
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([ 2.5       , 23.50000823,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 18.398375081405764}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.16021608168852944
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.        , 11.75123638,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.2507422185848838}
episode index:1119
target Thresh 31.999646422425066
target distance 21.0
model initialize at round 1119
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.35291755,  4.92108751]), 'previousTarget': array([23.79898987,  4.82842712]), 'currentState': array([4.5, 2.5, 0. ]), 'targetState': array([25,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.1606763611236828
{'scaleFactor': 20, 'currentTarget': array([25.,  5.]), 'previousTarget': array([25.,  5.]), 'currentState': array([24.        ,  4.76513273,  0.        ]), 'targetState': array([25,  5], dtype=int32), 'currentDistance': 1.027211094571749}
episode index:1120
target Thresh 31.999649940580717
target distance 17.0
model initialize at round 1120
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 8.5       , 27.49986166,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 17.50700311406336}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16053302806291236
{'scaleFactor': 20, 'currentTarget': array([46.15781594, 60.5716062 ]), 'previousTarget': array([45.68009658, 60.06626418]), 'currentState': array([58.        , 76.68876094,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 20.0}
episode index:1121
target Thresh 31.999653423730138
target distance 11.0
model initialize at round 1121
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([17.5       , 19.54190448,  0.        ]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 11.60290780033954}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16038995049779392
{'scaleFactor': 20, 'currentTarget': array([47.00169605, 17.46599108]), 'previousTarget': array([46.502382  , 17.37484164]), 'currentState': array([67.        , 17.20553178,  0.        ]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 20.0}
episode index:1122
target Thresh 31.999656872221642
target distance 10.0
model initialize at round 1122
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([13.5      , 14.8441975,  0.       ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 9.957822269725852}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1602471277457923
{'scaleFactor': 20, 'currentTarget': array([49.96970253, 45.71936261]), 'previousTarget': array([49.50777043, 45.38582696]), 'currentState': array([63.        , 60.89207983,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 20.0}
episode index:1123
target Thresh 31.999660286400083
target distance 5.0
model initialize at round 1123
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  4.]), 'previousTarget': array([25.,  4.]), 'currentState': array([25.5       ,  8.52245143,  0.        ]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 4.550007356232187}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1601045591268014
{'scaleFactor': 20, 'currentTarget': array([55.00041   ,  4.19209982]), 'previousTarget': array([54.50028894,  4.15857576]), 'currentState': array([75.        ,  4.32016199,  0.        ]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 20.0}
episode index:1124
target Thresh 31.999663666606878
target distance 11.0
model initialize at round 1124
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 29.]), 'previousTarget': array([14., 29.]), 'currentState': array([11.5, 18.5,  0. ]), 'targetState': array([14, 29], dtype=int32), 'currentDistance': 10.793516572461353}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15996224396313313
{'scaleFactor': 20, 'currentTarget': array([41.00080661, 29.24250482]), 'previousTarget': array([40.50094701, 29.25790197]), 'currentState': array([61.        , 29.42212541,  0.        ]), 'targetState': array([14, 29], dtype=int32), 'currentDistance': 20.0}
episode index:1125
target Thresh 31.999667013180055
target distance 7.0
model initialize at round 1125
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 12.]), 'previousTarget': array([ 9., 12.]), 'currentState': array([ 2.5       , 15.50161597,  0.        ]), 'targetState': array([ 9, 12], dtype=int32), 'currentDistance': 7.383177798932651}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.1606153310060244
{'scaleFactor': 20, 'currentTarget': array([ 9., 12.]), 'previousTarget': array([ 9., 12.]), 'currentState': array([ 8.       , 11.8507929,  0.       ]), 'targetState': array([ 9, 12], dtype=int32), 'currentDistance': 1.0110701057649196}
episode index:1126
target Thresh 31.999670326454275
target distance 13.0
model initialize at round 1126
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 27.]), 'previousTarget': array([11., 27.]), 'currentState': array([ 9.5, 14.5,  0. ]), 'targetState': array([11, 27], dtype=int32), 'currentDistance': 12.589678312014083}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16047281518436868
{'scaleFactor': 20, 'currentTarget': array([39.00167781, 27.36272939]), 'previousTarget': array([38.50217319, 27.40546263]), 'currentState': array([59.        , 27.62178455,  0.        ]), 'targetState': array([11, 27], dtype=int32), 'currentDistance': 20.0}
episode index:1127
target Thresh 31.99967360676086
target distance 9.0
model initialize at round 1127
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([ 5.5, 22.5,  0. ]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 8.514693182963201}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.16109301513069574
{'scaleFactor': 20, 'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([13.        , 22.87348488,  0.        ]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 1.0079712672764007}
episode index:1128
target Thresh 31.999676854427854
target distance 10.0
model initialize at round 1128
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([11.5,  8.5,  0. ]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 12.747548783981848}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.16169695682976856
{'scaleFactor': 20, 'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([20., 17.,  0.]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 0.9999999999985612}
episode index:1129
target Thresh 31.99968006978002
target distance 15.0
model initialize at round 1129
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 26.]), 'previousTarget': array([10., 26.]), 'currentState': array([25.5       , 28.58756307,  0.        ]), 'targetState': array([10, 26], dtype=int32), 'currentDistance': 15.714499121168219}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16155386217770681
{'scaleFactor': 20, 'currentTarget': array([55.44897524, 16.20409345]), 'previousTarget': array([54.95183535, 16.27936338]), 'currentState': array([75.        , 11.99013679,  0.        ]), 'targetState': array([10, 26], dtype=int32), 'currentDistance': 20.0}
episode index:1130
target Thresh 31.999683253138897
target distance 4.0
model initialize at round 1130
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 19.]), 'previousTarget': array([23., 19.]), 'currentState': array([27.5       , 19.54473481,  0.        ]), 'targetState': array([23, 19], dtype=int32), 'currentDistance': 4.532850759585524}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16141102056658593
{'scaleFactor': 20, 'currentTarget': array([57.41028296, 11.9206559 ]), 'previousTarget': array([56.91138975, 12.01359391]), 'currentState': array([77.        ,  7.89039704,  0.        ]), 'targetState': array([23, 19], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:1131
target Thresh 31.999686404822825
target distance 6.0
model initialize at round 1131
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 29.]), 'previousTarget': array([16., 29.]), 'currentState': array([22.5       , 28.52276641,  0.        ]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 6.517495830315524}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16126843132580274
{'scaleFactor': 20, 'currentTarget': array([52.30460806, 22.59029758]), 'previousTarget': array([51.80579037, 22.66582308]), 'currentState': array([72.        , 19.11300893,  0.        ]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 20.0}
episode index:1132
target Thresh 31.999689525146973
target distance 11.0
model initialize at round 1132
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 20.]), 'previousTarget': array([ 7., 20.]), 'currentState': array([18.5, 12.5,  0. ]), 'targetState': array([ 7, 20], dtype=int32), 'currentDistance': 13.729530217746005}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16112609378712153
{'scaleFactor': 20, 'currentTarget': array([48.26498054, 13.21518846]), 'previousTarget': array([47.76648485, 13.27776684]), 'currentState': array([68.        ,  9.97034534,  0.        ]), 'targetState': array([ 7, 20], dtype=int32), 'currentDistance': 20.0}
episode index:1133
target Thresh 31.999692614423378
target distance 12.0
model initialize at round 1133
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 13.]), 'previousTarget': array([11., 13.]), 'currentState': array([18.5       , 24.50686038,  0.        ]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 13.73527705214041}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16098400728466375
{'scaleFactor': 20, 'currentTarget': array([48.20652667,  7.61121175]), 'previousTarget': array([47.70428688,  7.71330778]), 'currentState': array([68.        ,  4.74443417,  0.        ]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:1134
target Thresh 31.999695672960968
target distance 6.0
model initialize at round 1134
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 16.]), 'previousTarget': array([ 7., 16.]), 'currentState': array([ 3.5       , 21.68188512,  0.        ]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 6.673366358431288}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16084217115489755
{'scaleFactor': 20, 'currentTarget': array([33.17324339, 12.53245844]), 'previousTarget': array([32.67353561, 12.59575673]), 'currentState': array([53.        ,  9.90572619,  0.        ]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 20.0}
episode index:1135
target Thresh 31.9996987010656
target distance 21.0
model initialize at round 1135
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.07892443,  3.6471836 ]), 'previousTarget': array([24.10381815,  4.09009055]), 'currentState': array([26.5       , 23.50010261,  0.        ]), 'targetState': array([24,  3], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16070058473662738
{'scaleFactor': 20, 'currentTarget': array([56.00447716,  2.32269393]), 'previousTarget': array([55.50396141,  2.37287468]), 'currentState': array([76.       ,  1.8995316,  0.       ]), 'targetState': array([24,  3], dtype=int32), 'currentDistance': 20.0}
episode index:1136
target Thresh 31.99970169904009
target distance 24.0
model initialize at round 1136
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.30272011,  9.69248087]), 'previousTarget': array([12.02246883, 10.27341645]), 'currentState': array([ 5.5       , 28.50000593,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1605592473709839
{'scaleFactor': 20, 'currentTarget': array([35.01375483,  4.22025006]), 'previousTarget': array([34.51427659,  4.22446572]), 'currentState': array([55.        ,  3.47862756,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 20.0}
episode index:1137
target Thresh 31.999704667184233
target distance 10.0
model initialize at round 1137
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([21.5       , 19.50000221,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 13.435030401977926}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16041815840141363
{'scaleFactor': 20, 'currentTarget': array([51.01393001,  8.54312459]), 'previousTarget': array([50.51393113,  8.56173778]), 'currentState': array([71.        ,  7.79679594,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 20.0}
episode index:1138
target Thresh 31.99970760579485
target distance 8.0
model initialize at round 1138
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 20.]), 'previousTarget': array([18., 20.]), 'currentState': array([22.5       , 27.50233549,  0.        ]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 8.748430589908802}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16027731717366875
{'scaleFactor': 20, 'currentTarget': array([52.01116771, 18.86293442]), 'previousTarget': array([51.51120478, 18.87778947]), 'currentState': array([72.       , 18.1946653,  0.       ]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 20.0}
episode index:1139
target Thresh 31.999710515165802
target distance 20.0
model initialize at round 1139
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.16691207, 20.32041844]), 'previousTarget': array([24.56953382, 20.57218647]), 'currentState': array([ 6.5       , 27.50000018,  0.        ]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.16072946781567457
{'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([26.        , 20.96333912,  0.        ]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 0.9633391200328703}
episode index:1140
target Thresh 31.999713395588028
target distance 17.0
model initialize at round 1140
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 11.]), 'previousTarget': array([23., 11.]), 'currentState': array([ 6.5       , 13.61410028,  0.        ]), 'targetState': array([23, 11], dtype=int32), 'currentDistance': 16.705793015012112}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.1612239909253522
{'scaleFactor': 20, 'currentTarget': array([23., 11.]), 'previousTarget': array([23., 11.]), 'currentState': array([22.5       , 11.93848057,  0.        ]), 'targetState': array([23, 11], dtype=int32), 'currentDistance': 1.0633653132420522}
episode index:1141
target Thresh 31.99971624734958
target distance 3.0
model initialize at round 1141
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 28.]), 'previousTarget': array([13., 28.]), 'currentState': array([14.5, 25.5,  0. ]), 'targetState': array([13, 28], dtype=int32), 'currentDistance': 2.915475947422624}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16108281405063649
{'scaleFactor': 20, 'currentTarget': array([44.01472392, 26.80925269]), 'previousTarget': array([43.51438163, 26.84217511]), 'currentState': array([64.        , 26.04195861,  0.        ]), 'targetState': array([13, 28], dtype=int32), 'currentDistance': 20.0}
episode index:1142
target Thresh 31.999719070735622
target distance 8.0
model initialize at round 1142
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 14.]), 'previousTarget': array([ 5., 14.]), 'currentState': array([11.5,  6.5,  0. ]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 9.92471662063966}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16094188420457292
{'scaleFactor': 20, 'currentTarget': array([41.00311261, 13.36473799]), 'previousTarget': array([40.50327023, 13.35788755]), 'currentState': array([61.        , 13.01190009,  0.        ]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 20.0}
episode index:1143
target Thresh 31.99972186602851
target distance 13.0
model initialize at round 1143
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 16.]), 'previousTarget': array([ 4., 16.]), 'currentState': array([17.5       , 27.50000003,  0.        ]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 17.73414787029403}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16080120073935913
{'scaleFactor': 20, 'currentTarget': array([47.01112548, 14.56476993]), 'previousTarget': array([46.51139401, 14.56441376]), 'currentState': array([67.        , 13.89776497,  0.        ]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 20.0}
episode index:1144
target Thresh 31.999724633507764
target distance 4.0
model initialize at round 1144
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 18.]), 'previousTarget': array([14., 18.]), 'currentState': array([18.5       , 18.49999958,  0.        ]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 4.5276925229931955}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16066076300945575
{'scaleFactor': 20, 'currentTarget': array([48.00526856, 17.21931085]), 'previousTarget': array([47.50502098, 17.24909367]), 'currentState': array([68.        , 16.76027405,  0.        ]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 20.0}
episode index:1145
target Thresh 31.999727373450142
target distance 8.0
model initialize at round 1145
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([22.5,  3.5,  0. ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 8.631338250816116}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16052057037157666
{'scaleFactor': 20, 'currentTarget': array([52.02645891,  3.04204318]), 'previousTarget': array([51.52642943,  3.06886809]), 'currentState': array([72.        ,  2.01361897,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 20.0}
episode index:1146
target Thresh 31.999730086129638
target distance 18.0
model initialize at round 1146
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.64213562, 22.64213562]), 'previousTarget': array([23.14213562, 22.14213562]), 'currentState': array([9.5, 8.5, 0. ]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.1610063680718615
{'scaleFactor': 20, 'currentTarget': array([27., 26.]), 'previousTarget': array([27., 26.]), 'currentState': array([26., 25.,  0.]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 1.4142135623692516}
episode index:1147
target Thresh 31.99973277181752
target distance 15.0
model initialize at round 1147
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 20.]), 'previousTarget': array([17.64636501, 19.62110536]), 'currentState': array([4.5, 5.5, 0. ]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 19.811612756158954}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.1615301802201849
{'scaleFactor': 20, 'currentTarget': array([18., 20.]), 'previousTarget': array([18., 20.]), 'currentState': array([18., 19.,  0.]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 0.9999999999982094}
episode index:1148
target Thresh 31.999735430782362
target distance 11.0
model initialize at round 1148
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 20.]), 'previousTarget': array([24., 20.]), 'currentState': array([13.5       , 25.50001171,  0.        ]), 'targetState': array([24, 20], dtype=int32), 'currentDistance': 11.853275025729213}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.16210862969243503
{'scaleFactor': 20, 'currentTarget': array([24., 20.]), 'previousTarget': array([24., 20.]), 'currentState': array([23.       , 20.7078046,  0.       ]), 'targetState': array([24, 20], dtype=int32), 'currentDistance': 1.22514789151205}
episode index:1149
target Thresh 31.999738063290064
target distance 6.0
model initialize at round 1149
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 16.]), 'previousTarget': array([24., 16.]), 'currentState': array([18.5       , 19.50089797,  0.        ]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 6.519684549331173}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.16276202849051433
{'scaleFactor': 20, 'currentTarget': array([24., 16.]), 'previousTarget': array([24., 16.]), 'currentState': array([23.        , 16.86903418,  0.        ]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 1.324847313276296}
episode index:1150
target Thresh 31.999740669603874
target distance 27.0
model initialize at round 1150
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.26255776, 12.6808684 ]), 'previousTarget': array([11.09544899, 12.92885613]), 'currentState': array([23.5       , 28.50000262,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16262061925637836
{'scaleFactor': 20, 'currentTarget': array([53.00019614,  1.7785588 ]), 'previousTarget': array([52.50010979,  1.83598337]), 'currentState': array([73.        ,  1.68998354,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 20.0}
episode index:1151
target Thresh 31.99974324998443
target distance 16.0
model initialize at round 1151
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 21.]), 'previousTarget': array([25., 21.]), 'currentState': array([ 9.5, 16.5,  0. ]), 'targetState': array([25, 21], dtype=int32), 'currentDistance': 16.140012391568874}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16247945552438497
{'scaleFactor': 20, 'currentTarget': array([39.08958197, 19.66195363]), 'previousTarget': array([38.60141596, 19.62502805]), 'currentState': array([59.       , 17.7711195,  0.       ]), 'targetState': array([25, 21], dtype=int32), 'currentDistance': 20.0}
episode index:1152
target Thresh 31.99974580468977
target distance 6.0
model initialize at round 1152
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.5,  5.5,  0. ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 5.522680508593643}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16233853665576017
{'scaleFactor': 20, 'currentTarget': array([46.02031937,  9.64574149]), 'previousTarget': array([45.52078443,  9.65309894]), 'currentState': array([66.        ,  8.74443021,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 20.0}
episode index:1153
target Thresh 31.99974833397537
target distance 10.0
model initialize at round 1153
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([25.5       , 10.50573912,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 10.512172566133007}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1621978620139441
{'scaleFactor': 20, 'currentTarget': array([55.0068294 ,  8.95422875]), 'previousTarget': array([54.50661033,  8.98401104]), 'currentState': array([75.       ,  8.4316109,  0.       ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 20.0}
episode index:1154
target Thresh 31.99975083809415
target distance 9.0
model initialize at round 1154
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 14.]), 'previousTarget': array([ 5., 14.]), 'currentState': array([2.5, 5.5, 0. ]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 8.860022573334682}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16205743096458136
{'scaleFactor': 20, 'currentTarget': array([32.05812521, 11.93258235]), 'previousTarget': array([31.55839866, 11.96597619]), 'currentState': array([52.        , 10.40889275,  0.        ]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 20.0}
episode index:1155
target Thresh 31.999753317296538
target distance 9.0
model initialize at round 1155
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 29.]), 'previousTarget': array([18., 29.]), 'currentState': array([ 9.5, 21.5,  0. ]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 11.335784048754523}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.16266123799198337
{'scaleFactor': 20, 'currentTarget': array([18., 29.]), 'previousTarget': array([18., 29.]), 'currentState': array([17.        , 28.99667534,  0.        ]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 1.000005526657785}
episode index:1156
target Thresh 31.99975577183045
target distance 10.0
model initialize at round 1156
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 11.]), 'previousTarget': array([23., 11.]), 'currentState': array([13.5,  3.5,  0. ]), 'targetState': array([23, 11], dtype=int32), 'currentDistance': 12.103718436910144}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.16324920856708444
{'scaleFactor': 20, 'currentTarget': array([23., 11.]), 'previousTarget': array([23., 11.]), 'currentState': array([22.        , 11.48857056,  0.        ]), 'targetState': array([23, 11], dtype=int32), 'currentDistance': 1.1129695360750416}
episode index:1157
target Thresh 31.99975820194134
target distance 13.0
model initialize at round 1157
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([ 8.5       , 26.49999914,  0.        ]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 12.509995968653687}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.16379356528131292
{'scaleFactor': 20, 'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([20.       , 26.3858961,  0.       ]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 1.0718748981845696}
episode index:1158
target Thresh 31.999760607872222
target distance 13.0
model initialize at round 1158
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([16.5,  9.5,  0. ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 15.443445211480533}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16365224210160514
{'scaleFactor': 20, 'currentTarget': array([46.02507286, -0.15641424]), 'previousTarget': array([45.5260482 , -0.17254318]), 'currentState': array([66.        , -1.15755647,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 20.0}
episode index:1159
target Thresh 31.999762989863694
target distance 18.0
model initialize at round 1159
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 29.]), 'previousTarget': array([25., 29.]), 'currentState': array([25.5, 11.5,  0. ]), 'targetState': array([25, 29], dtype=int32), 'currentDistance': 17.50714140001152}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16351116258255205
{'scaleFactor': 20, 'currentTarget': array([55.02972191, 27.36101811]), 'previousTarget': array([54.53057158, 27.36533356]), 'currentState': array([75.        , 26.27106715,  0.        ]), 'targetState': array([25, 29], dtype=int32), 'currentDistance': 20.0}
episode index:1160
target Thresh 31.999765348153954
target distance 4.0
model initialize at round 1160
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 13.]), 'previousTarget': array([15., 13.]), 'currentState': array([17.5       , 16.50018871,  0.        ]), 'targetState': array([15, 13], dtype=int32), 'currentDistance': 4.301316193184105}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16337032609453953
{'scaleFactor': 20, 'currentTarget': array([47.00002068, 13.04601442]), 'previousTarget': array([46.50005919, 13.07663412]), 'currentState': array([67.        , 13.07477338,  0.        ]), 'targetState': array([15, 13], dtype=int32), 'currentDistance': 20.0}
episode index:1161
target Thresh 31.99976768297883
target distance 9.0
model initialize at round 1161
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 29.]), 'previousTarget': array([11., 29.]), 'currentState': array([ 2.5, 26.5,  0. ]), 'targetState': array([11, 29], dtype=int32), 'currentDistance': 8.860022573334657}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.1639698854994851
{'scaleFactor': 20, 'currentTarget': array([11., 29.]), 'previousTarget': array([11., 29.]), 'currentState': array([10.        , 29.63023111,  0.        ]), 'targetState': array([11, 29], dtype=int32), 'currentDistance': 1.182028446271199}
episode index:1162
target Thresh 31.999769994571814
target distance 16.0
model initialize at round 1162
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7.92303856, 5.80649077]), 'previousTarget': array([7.85786438, 5.85786438]), 'currentState': array([22.5, 19.5,  0. ]), 'targetState': array([6, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1638288967759258
{'scaleFactor': 20, 'currentTarget': array([52.00005431,  3.89280351]), 'previousTarget': array([51.50005254,  3.89570671]), 'currentState': array([72.        ,  3.84619652,  0.        ]), 'targetState': array([6, 4], dtype=int32), 'currentDistance': 20.0}
episode index:1163
target Thresh 31.999772283164063
target distance 8.0
model initialize at round 1163
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 17.]), 'previousTarget': array([11., 17.]), 'currentState': array([ 3.5, 10.5,  0. ]), 'targetState': array([11, 17], dtype=int32), 'currentDistance': 9.924716620639556}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.1644420343414095
{'scaleFactor': 20, 'currentTarget': array([11., 17.]), 'previousTarget': array([11., 17.]), 'currentState': array([10.        , 16.99835849,  0.        ]), 'targetState': array([11, 17], dtype=int32), 'currentDistance': 1.000001347279442}
episode index:1164
target Thresh 31.99977454898444
target distance 13.0
model initialize at round 1164
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 14.]), 'previousTarget': array([11., 14.]), 'currentState': array([24.5,  4.5,  0. ]), 'targetState': array([11, 14], dtype=int32), 'currentDistance': 16.50757401921925}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16430088238060142
{'scaleFactor': 20, 'currentTarget': array([54.00113167, 14.45746532]), 'previousTarget': array([53.50093344, 14.41063484]), 'currentState': array([74.        , 14.67022224,  0.        ]), 'targetState': array([11, 14], dtype=int32), 'currentDistance': 20.0}
episode index:1165
target Thresh 31.999776792259524
target distance 15.0
model initialize at round 1165
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([18.5       , 12.50000012,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 16.807736360073104}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16415997253293368
{'scaleFactor': 20, 'currentTarget': array([48.00062786,  6.35658379]), 'previousTarget': array([47.50064099,  6.35629011]), 'currentState': array([68.        ,  6.51505828,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 20.0}
episode index:1166
target Thresh 31.99977901321365
target distance 24.0
model initialize at round 1166
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7.50199965, 15.06366706]), 'previousTarget': array([ 7., 15.]), 'currentState': array([27.5       , 15.34647781,  0.        ]), 'targetState': array([ 3, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16401930417600744
{'scaleFactor': 20, 'currentTarget': array([57.00005886, 14.86899077]), 'previousTarget': array([56.50001167, 14.94220311]), 'currentState': array([77.        , 14.82046903,  0.        ]), 'targetState': array([ 3, 15], dtype=int32), 'currentDistance': 20.0}
episode index:1167
target Thresh 31.999781212068914
target distance 7.0
model initialize at round 1167
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 29.]), 'previousTarget': array([ 5., 29.]), 'currentState': array([12.5, 28.5,  0. ]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 7.516648189186529}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1638788766895554
{'scaleFactor': 20, 'currentTarget': array([42.00015936, 29.14770627]), 'previousTarget': array([41.50011805, 29.12540853]), 'currentState': array([62.        , 29.22754652,  0.        ]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 20.0}
episode index:1168
target Thresh 31.999783389045202
target distance 8.0
model initialize at round 1168
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([12.5       , 10.50000462,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 7.648533800044362}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16373868945543257
{'scaleFactor': 20, 'currentTarget': array([42.00169349,  3.36442043]), 'previousTarget': array([41.50174001,  3.36279736]), 'currentState': array([62.        ,  3.62468295,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 20.0}
episode index:1169
target Thresh 31.999785544360215
target distance 22.0
model initialize at round 1169
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.19603151,  9.27647243]), 'previousTarget': array([14.76343395,  9.82527831]), 'currentState': array([ 3.5, 25.5,  0. ]), 'targetState': array([19,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16359874185760742
{'scaleFactor': 20, 'currentTarget': array([33.04928963,  3.01182005]), 'previousTarget': array([32.53781411,  3.16633418]), 'currentState': array([53.        ,  1.60855393,  0.        ]), 'targetState': array([19,  4], dtype=int32), 'currentDistance': 20.0}
episode index:1170
target Thresh 31.999787678229485
target distance 15.0
model initialize at round 1170
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 25.]), 'previousTarget': array([ 9., 25.]), 'currentState': array([12.5, 10.5,  0. ]), 'targetState': array([ 9, 25], dtype=int32), 'currentDistance': 14.916433890176238}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16345903328215258
{'scaleFactor': 20, 'currentTarget': array([42.04881248, 22.68677765]), 'previousTarget': array([41.5491259 , 22.71442322]), 'currentState': array([62.        , 21.29031196,  0.        ]), 'targetState': array([ 9, 25], dtype=int32), 'currentDistance': 20.0}
episode index:1171
target Thresh 31.9997897908664
target distance 15.0
model initialize at round 1171
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([23.5, 17.5,  0. ]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 14.916433890176203}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16331956311723606
{'scaleFactor': 20, 'currentTarget': array([53.04905433,  1.17219163]), 'previousTarget': array([52.55067321,  1.17770764]), 'currentState': array([73.        , -0.22772505,  0.        ]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 20.0}
episode index:1172
target Thresh 31.999791882482228
target distance 12.0
model initialize at round 1172
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 12.]), 'previousTarget': array([ 6., 12.]), 'currentState': array([18.5,  7.5,  0. ]), 'targetState': array([ 6, 12], dtype=int32), 'currentDistance': 13.285330255586501}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16318033075311225
{'scaleFactor': 20, 'currentTarget': array([48.00555995, 12.99067842]), 'previousTarget': array([47.50569493, 12.99070548]), 'currentState': array([68.        , 13.46223647,  0.        ]), 'targetState': array([ 6, 12], dtype=int32), 'currentDistance': 20.0}
episode index:1173
target Thresh 31.99979395328613
target distance 23.0
model initialize at round 1173
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8.75093819, 21.92798013]), 'previousTarget': array([ 8.75140711, 21.54352728]), 'currentState': array([13.5,  2.5,  0. ]), 'targetState': array([ 8, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.163041335582113
{'scaleFactor': 20, 'currentTarget': array([43.00580751, 25.84378096]), 'previousTarget': array([42.50579248, 25.83065091]), 'currentState': array([63.        , 26.32572153,  0.        ]), 'targetState': array([ 8, 25], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:1174
target Thresh 31.999796003485187
target distance 9.0
model initialize at round 1174
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 21.]), 'previousTarget': array([10., 21.]), 'currentState': array([13.5, 12.5,  0. ]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 9.192388155425073}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16290257699863886
{'scaleFactor': 20, 'currentTarget': array([43.03603228, 22.98573444]), 'previousTarget': array([42.53426839, 22.90698175]), 'currentState': array([63.       , 24.1857314,  0.       ]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 20.0}
episode index:1175
target Thresh 31.999798033284424
target distance 8.0
model initialize at round 1175
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([12.5,  8.5,  0. ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 8.86002257333473}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16276405439915023
{'scaleFactor': 20, 'currentTarget': array([42.03704231, 13.3182469 ]), 'previousTarget': array([41.53678372, 13.27973629]), 'currentState': array([62.      , 14.534931,  0.      ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 20.0}
episode index:1176
target Thresh 31.99980004288682
target distance 14.0
model initialize at round 1176
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 21.]), 'previousTarget': array([ 5., 21.]), 'currentState': array([19.5, 13.5,  0. ]), 'targetState': array([ 5, 21], dtype=int32), 'currentDistance': 16.32482771731454}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1626257671821586
{'scaleFactor': 20, 'currentTarget': array([49.01808063, 22.87297738]), 'previousTarget': array([48.5189761 , 22.89710486]), 'currentState': array([69.        , 23.72321171,  0.        ]), 'targetState': array([ 5, 21], dtype=int32), 'currentDistance': 20.0}
episode index:1177
target Thresh 31.99980203249334
target distance 11.0
model initialize at round 1177
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 2.]), 'previousTarget': array([7., 2.]), 'currentState': array([ 4.5, 12.5,  0. ]), 'targetState': array([7, 2], dtype=int32), 'currentDistance': 10.793516572461375}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16248771474821788
{'scaleFactor': 20, 'currentTarget': array([34.00004247,  2.05564151]), 'previousTarget': array([33.50017452,  2.11070797]), 'currentState': array([54.        ,  2.09685729,  0.        ]), 'targetState': array([7, 2], dtype=int32), 'currentDistance': 20.0}
episode index:1178
target Thresh 31.999804002302945
target distance 9.0
model initialize at round 1178
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 12.]), 'previousTarget': array([14., 12.]), 'currentState': array([23.5, 20.5,  0. ]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 12.74754878398197}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16234989649991574
{'scaleFactor': 20, 'currentTarget': array([53.00201302, 12.55340664]), 'previousTarget': array([52.50256575, 12.6167916 ]), 'currentState': array([73.        , 12.83716171,  0.        ]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 20.0}
episode index:1179
target Thresh 31.999805952512613
target distance 2.0
model initialize at round 1179
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 21.]), 'previousTarget': array([21., 21.]), 'currentState': array([21.5      , 22.5022167,  0.       ]), 'targetState': array([21, 21], dtype=int32), 'currentDistance': 1.5832419284373291}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16221231184186496
{'scaleFactor': 20, 'currentTarget': array([51.01520487, 22.1710633 ]), 'previousTarget': array([50.51608089, 22.18433823]), 'currentState': array([71.        , 22.95078345,  0.        ]), 'targetState': array([21, 21], dtype=int32), 'currentDistance': 20.0}
episode index:1180
target Thresh 31.999807883317374
target distance 11.0
model initialize at round 1180
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 19.]), 'previousTarget': array([12., 19.]), 'currentState': array([23.5       , 20.03152665,  0.        ]), 'targetState': array([12, 19], dtype=int32), 'currentDistance': 11.546170241248495}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1620749601806949
{'scaleFactor': 20, 'currentTarget': array([53.0006585, 19.332721 ]), 'previousTarget': array([52.50054883, 19.30004603]), 'currentState': array([73.        , 19.49501597,  0.        ]), 'targetState': array([12, 19], dtype=int32), 'currentDistance': 20.0}
episode index:1181
target Thresh 31.999809794910306
target distance 17.0
model initialize at round 1181
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.93585323,  6.09203666]), 'previousTarget': array([16.53366395,  6.66064273]), 'currentState': array([ 5.5, 22.5,  0. ]), 'targetState': array([17,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16193784092504285
{'scaleFactor': 20, 'currentTarget': array([35.02269101,  5.14075632]), 'previousTarget': array([34.51700648,  5.27715664]), 'currentState': array([55.        ,  4.18832494,  0.        ]), 'targetState': array([17,  6], dtype=int32), 'currentDistance': 20.0}
episode index:1182
target Thresh 31.99981168748257
target distance 10.0
model initialize at round 1182
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 28.]), 'previousTarget': array([22., 28.]), 'currentState': array([12.5, 18.5,  0. ]), 'targetState': array([22, 28], dtype=int32), 'currentDistance': 13.43502884254429}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.16251350056363867
{'scaleFactor': 20, 'currentTarget': array([22., 28.]), 'previousTarget': array([22., 28.]), 'currentState': array([21., 27.,  0.]), 'targetState': array([22, 28], dtype=int32), 'currentDistance': 1.41421356237106}
episode index:1183
target Thresh 31.999813561223426
target distance 11.0
model initialize at round 1183
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([ 4.5, 12.5,  0. ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 10.606601717798133}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16237624253951397
{'scaleFactor': 20, 'currentTarget': array([34.00680915,  2.80930918]), 'previousTarget': array([33.50718119,  2.81774388]), 'currentState': array([54.        ,  3.33115174,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 20.0}
episode index:1184
target Thresh 31.99981541632025
target distance 16.0
model initialize at round 1184
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9.02085402, 27.16475581]), 'previousTarget': array([ 8.94846611, 27.17009216]), 'currentState': array([24.5, 14.5,  0. ]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1622392161745017
{'scaleFactor': 20, 'currentTarget': array([54.01581346, 29.83095738]), 'previousTarget': array([53.51649597, 29.84980729]), 'currentState': array([74.        , 30.62612302,  0.        ]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 20.0}
episode index:1185
target Thresh 31.99981725295855
target distance 3.0
model initialize at round 1185
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 26.]), 'previousTarget': array([19., 26.]), 'currentState': array([19.5       , 28.50000939,  0.        ]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 2.5495189622251373}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16210242088261764
{'scaleFactor': 20, 'currentTarget': array([49.0085726 , 26.87890355]), 'previousTarget': array([48.50802759, 26.83630294]), 'currentState': array([69.        , 27.46442078,  0.        ]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 20.0}
episode index:1186
target Thresh 31.999819071321994
target distance 20.0
model initialize at round 1186
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.98782391, 18.99719013]), 'previousTarget': array([24.40285  , 18.8507125]), 'currentState': array([ 5.5, 14.5,  0. ]), 'targetState': array([25, 19], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.16254668934511676
{'scaleFactor': 20, 'currentTarget': array([25., 19.]), 'previousTarget': array([25., 19.]), 'currentState': array([24.        , 18.85213239,  0.        ]), 'targetState': array([25, 19], dtype=int32), 'currentDistance': 1.0108733008910953}
episode index:1187
target Thresh 31.99982087159242
target distance 19.0
model initialize at round 1187
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7.52020262, 15.25938847]), 'previousTarget': array([ 7.30163555, 15.31492866]), 'currentState': array([25.5,  6.5,  0. ]), 'targetState': array([ 6, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1624098655325367
{'scaleFactor': 20, 'currentTarget': array([55.00431366, 14.98204726]), 'previousTarget': array([54.50534755, 14.87810016]), 'currentState': array([75.        , 14.56668253,  0.        ]), 'targetState': array([ 6, 16], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:1188
target Thresh 31.999822653949856
target distance 9.0
model initialize at round 1188
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 19.]), 'previousTarget': array([10., 19.]), 'currentState': array([14.5, 27.5,  0. ]), 'targetState': array([10, 19], dtype=int32), 'currentDistance': 9.617692030835642}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.162273271869347
{'scaleFactor': 20, 'currentTarget': array([44.00949151, 20.04814768]), 'previousTarget': array([43.50954315, 20.03554728]), 'currentState': array([64.        , 20.66424055,  0.        ]), 'targetState': array([10, 19], dtype=int32), 'currentDistance': 20.0}
episode index:1189
target Thresh 31.999824418572537
target distance 8.0
model initialize at round 1189
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 17.]), 'previousTarget': array([18., 17.]), 'currentState': array([23.5,  9.5,  0. ]), 'targetState': array([18, 17], dtype=int32), 'currentDistance': 9.300537618869122}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16213690777533915
{'scaleFactor': 20, 'currentTarget': array([53.00130703, 17.40017307]), 'previousTarget': array([52.50118073, 17.37491116]), 'currentState': array([73.        , 17.62881992,  0.        ]), 'targetState': array([18, 17], dtype=int32), 'currentDistance': 20.0}
episode index:1190
target Thresh 31.999826165636932
target distance 15.0
model initialize at round 1190
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([23.5,  7.5,  0. ]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 16.807736313971688}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1620007726722532
{'scaleFactor': 20, 'currentTarget': array([53.05596807, 17.37781202]), 'previousTarget': array([52.55679894, 17.36519833]), 'currentState': array([73.        , 18.87300109,  0.        ]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 20.0}
episode index:1191
target Thresh 31.999827895317743
target distance 17.0
model initialize at round 1191
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.83524419,  7.02085402]), 'previousTarget': array([12.85099785,  7.11284334]), 'currentState': array([25.5, 22.5,  0. ]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16186486598376978
{'scaleFactor': 20, 'currentTarget': array([55.04998064,  9.04922207]), 'previousTarget': array([54.55080433,  9.03869248]), 'currentState': array([75.        , 10.46227821,  0.        ]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:1192
target Thresh 31.999829607787944
target distance 13.0
model initialize at round 1192
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 23.]), 'previousTarget': array([27., 23.]), 'currentState': array([14.5, 15.5,  0. ]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 14.577379737113143}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.16239441285523656
{'scaleFactor': 20, 'currentTarget': array([27., 23.]), 'previousTarget': array([27., 23.]), 'currentState': array([26.        , 23.01674883,  0.        ]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 1.0001402518897111}
episode index:1193
target Thresh 31.999831303218784
target distance 9.0
model initialize at round 1193
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 17.]), 'previousTarget': array([26., 17.]), 'currentState': array([17.5,  9.5,  0. ]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 11.335784048754519}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.16297872101418637
{'scaleFactor': 20, 'currentTarget': array([26., 17.]), 'previousTarget': array([26., 17.]), 'currentState': array([25.        , 16.97043547,  0.        ]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 1.0004369352494094}
episode index:1194
target Thresh 31.9998329817798
target distance 10.0
model initialize at round 1194
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 16.]), 'previousTarget': array([12., 16.]), 'currentState': array([18.5,  6.5,  0. ]), 'targetState': array([12, 16], dtype=int32), 'currentDistance': 11.510864433221387}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1628423371472289
{'scaleFactor': 20, 'currentTarget': array([48.06703455, 18.96042217]), 'previousTarget': array([47.55676666, 18.68469513]), 'currentState': array([68.        , 20.59654207,  0.        ]), 'targetState': array([12, 16], dtype=int32), 'currentDistance': 20.0}
episode index:1195
target Thresh 31.99983464363886
target distance 14.0
model initialize at round 1195
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([13.5, 27.5,  0. ]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 14.57737973711321}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16270618134693857
{'scaleFactor': 20, 'currentTarget': array([43.04528762, 16.36242438]), 'previousTarget': array([42.54362817, 16.28540343]), 'currentState': array([63.        , 17.70758381,  0.        ]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:1196
target Thresh 31.999836288962143
target distance 18.0
model initialize at round 1196
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.4717636 ,  4.23929509]), 'previousTarget': array([17.94818637,  4.71272322]), 'currentState': array([ 3.5, 17.5,  0. ]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.1631698608383766
{'scaleFactor': 20, 'currentTarget': array([21.,  2.]), 'previousTarget': array([21.,  2.]), 'currentState': array([20.        ,  1.31496499,  0.        ]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 1.2121357012991312}
episode index:1197
target Thresh 31.99983791791419
target distance 20.0
model initialize at round 1197
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.38484567, 22.85819484]), 'previousTarget': array([20.86588292, 22.37929463]), 'currentState': array([6.5, 9.5, 0. ]), 'targetState': array([26, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.16360915902287637
{'scaleFactor': 20, 'currentTarget': array([26., 27.]), 'previousTarget': array([26., 27.]), 'currentState': array([25.        , 26.44582338,  0.        ]), 'targetState': array([26, 27], dtype=int32), 'currentDistance': 1.143289868440909}
episode index:1198
target Thresh 31.999839530657887
target distance 1.0
model initialize at round 1198
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 24.]), 'previousTarget': array([24., 24.]), 'currentState': array([23.5       , 23.49855173,  0.        ]), 'targetState': array([24, 24], dtype=int32), 'currentDistance': 0.708131605815723}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.16430673270175639
{'scaleFactor': 20, 'currentTarget': array([24., 24.]), 'previousTarget': array([24., 24.]), 'currentState': array([23.5       , 23.49855173,  0.        ]), 'targetState': array([24, 24], dtype=int32), 'currentDistance': 0.708131605815723}
episode index:1199
target Thresh 31.999841127354518
target distance 13.0
model initialize at round 1199
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 26.]), 'previousTarget': array([20., 26.]), 'currentState': array([ 7.5, 18.5,  0. ]), 'targetState': array([20, 26], dtype=int32), 'currentDistance': 14.577379737113212}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.16483115566087464
{'scaleFactor': 20, 'currentTarget': array([20., 26.]), 'previousTarget': array([20., 26.]), 'currentState': array([19.        , 25.57391398,  0.        ]), 'targetState': array([20, 26], dtype=int32), 'currentDistance': 1.086990938037383}
episode index:1200
target Thresh 31.999842708163754
target distance 9.0
model initialize at round 1200
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 24.]), 'previousTarget': array([13., 24.]), 'currentState': array([22.5, 21.5,  0. ]), 'targetState': array([13, 24], dtype=int32), 'currentDistance': 9.82344135219431}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1646939107352619
{'scaleFactor': 20, 'currentTarget': array([52.03261875, 26.23199441]), 'previousTarget': array([51.53086588, 26.14314326]), 'currentState': array([72.        , 27.37378516,  0.        ]), 'targetState': array([13, 24], dtype=int32), 'currentDistance': 20.0}
episode index:1201
target Thresh 31.999844273243674
target distance 2.0
model initialize at round 1201
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 14.]), 'previousTarget': array([20., 14.]), 'currentState': array([18.5       , 14.50002784,  0.        ]), 'targetState': array([20, 14], dtype=int32), 'currentDistance': 1.5811476326212084}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.1653805214584439
{'scaleFactor': 20, 'currentTarget': array([20., 14.]), 'previousTarget': array([20., 14.]), 'currentState': array([19.        , 14.00089025,  0.        ]), 'targetState': array([20, 14], dtype=int32), 'currentDistance': 1.0000003962767212}
episode index:1202
target Thresh 31.99984582275079
target distance 6.0
model initialize at round 1202
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  6.]), 'previousTarget': array([25.,  6.]), 'currentState': array([21.5, 11.5,  0. ]), 'targetState': array([25,  6], dtype=int32), 'currentDistance': 6.51920240520254}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1652430480407727
{'scaleFactor': 20, 'currentTarget': array([51.04031979,  7.6560097 ]), 'previousTarget': array([50.53788489,  7.57411192]), 'currentState': array([71.        ,  8.92532684,  0.        ]), 'targetState': array([25,  6], dtype=int32), 'currentDistance': 20.0}
episode index:1203
target Thresh 31.999847356840053
target distance 6.0
model initialize at round 1203
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 22.]), 'previousTarget': array([ 9., 22.]), 'currentState': array([15.5       , 23.49990055,  0.        ]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 6.67080967041051}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16510580298426045
{'scaleFactor': 20, 'currentTarget': array([45.03880839, 24.24836077]), 'previousTarget': array([44.53836637, 24.20444034]), 'currentState': array([65.        , 25.49368386,  0.        ]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 20.0}
episode index:1204
target Thresh 31.99984887566487
target distance 18.0
model initialize at round 1204
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.90043122, 19.2603573 ]), 'previousTarget': array([23.36442559, 18.80368799]), 'currentState': array([8.5, 6.5, 0. ]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.16556441271837996
{'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([25.        , 20.47705025,  0.        ]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 1.1284841348250885}
episode index:1205
target Thresh 31.99985037937713
target distance 21.0
model initialize at round 1205
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7.7881459 , 7.96511314]), 'previousTarget': array([7.63241055, 8.54387571]), 'currentState': array([ 3.5, 27.5,  0. ]), 'targetState': array([8, 7], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16542712879406954
{'scaleFactor': 20, 'currentTarget': array([33.06124962,  8.96586515]), 'previousTarget': array([32.55902669,  8.89102962]), 'currentState': array([53.        , 10.52990904,  0.        ]), 'targetState': array([8, 7], dtype=int32), 'currentDistance': 20.0}
episode index:1206
target Thresh 31.9998518681272
target distance 4.0
model initialize at round 1206
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 25.]), 'previousTarget': array([10., 25.]), 'currentState': array([ 9.5, 21.5,  0. ]), 'targetState': array([10, 25], dtype=int32), 'currentDistance': 3.535533905932649}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16529007234933543
{'scaleFactor': 20, 'currentTarget': array([39.04626367, 23.02091341]), 'previousTarget': array([38.5438822 , 23.10603128]), 'currentState': array([59.        , 21.66135225,  0.        ]), 'targetState': array([10, 25], dtype=int32), 'currentDistance': 20.0}
episode index:1207
target Thresh 31.999853342063965
target distance 10.0
model initialize at round 1207
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  8.]), 'previousTarget': array([21.,  8.]), 'currentState': array([11.5,  2.5,  0. ]), 'targetState': array([21,  8], dtype=int32), 'currentDistance': 10.977249200050007}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.16585104347601967
{'scaleFactor': 20, 'currentTarget': array([21.,  8.]), 'previousTarget': array([21.,  8.]), 'currentState': array([20.       ,  8.0762324,  0.       ]), 'targetState': array([21,  8], dtype=int32), 'currentDistance': 1.002901479952586}
episode index:1208
target Thresh 31.99985480133481
target distance 10.0
model initialize at round 1208
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 29.]), 'previousTarget': array([14., 29.]), 'currentState': array([ 4.5, 26.5,  0. ]), 'targetState': array([14, 29], dtype=int32), 'currentDistance': 9.823441352194234}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.16641108661076567
{'scaleFactor': 20, 'currentTarget': array([14., 29.]), 'previousTarget': array([14., 29.]), 'currentState': array([13.        , 28.89796551,  0.        ]), 'targetState': array([14, 29], dtype=int32), 'currentDistance': 1.0051920401327523}
episode index:1209
target Thresh 31.999856246085667
target distance 11.0
model initialize at round 1209
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 16.]), 'previousTarget': array([15., 16.]), 'currentState': array([26.5, 17.5,  0. ]), 'targetState': array([15, 16], dtype=int32), 'currentDistance': 11.597413504743269}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1662735567871204
{'scaleFactor': 20, 'currentTarget': array([56.07515187, 12.42911647]), 'previousTarget': array([55.57547111, 12.46502966]), 'currentState': array([76.        , 10.69694243,  0.        ]), 'targetState': array([15, 16], dtype=int32), 'currentDistance': 20.0}
episode index:1210
target Thresh 31.999857676461012
target distance 13.0
model initialize at round 1210
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([ 6.5, 20.5,  0. ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 15.116216457830909}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1661362540977834
{'scaleFactor': 20, 'currentTarget': array([36.05058534,  6.49996436]), 'previousTarget': array([35.54284885,  6.65311975]), 'currentState': array([56.        ,  5.07839662,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 20.0}
episode index:1211
target Thresh 31.999859092603888
target distance 6.0
model initialize at round 1211
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 15.]), 'previousTarget': array([25., 15.]), 'currentState': array([24.5,  9.5,  0. ]), 'targetState': array([25, 15], dtype=int32), 'currentDistance': 5.5226805085935435}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.165999177980541
{'scaleFactor': 20, 'currentTarget': array([54.02848578, 13.44903227]), 'previousTarget': array([53.52860858, 13.47245129]), 'currentState': array([74.        , 12.38197096,  0.        ]), 'targetState': array([25, 15], dtype=int32), 'currentDistance': 20.0}
episode index:1212
target Thresh 31.999860494655906
target distance 9.0
model initialize at round 1212
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 20.]), 'previousTarget': array([16., 20.]), 'currentState': array([18.5, 28.5,  0. ]), 'targetState': array([16, 20], dtype=int32), 'currentDistance': 8.860022573334623}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16586232787503355
{'scaleFactor': 20, 'currentTarget': array([48.00560388, 20.75781235]), 'previousTarget': array([47.50576382, 20.7565527 ]), 'currentState': array([68.        , 21.23122945,  0.        ]), 'targetState': array([16, 20], dtype=int32), 'currentDistance': 20.0}
episode index:1213
target Thresh 31.999861882757273
target distance 9.0
model initialize at round 1213
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 21.]), 'previousTarget': array([25., 21.]), 'currentState': array([22.5, 12.5,  0. ]), 'targetState': array([25, 21], dtype=int32), 'currentDistance': 8.860022573334575}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16572570322274768
{'scaleFactor': 20, 'currentTarget': array([52.0502069 , 22.92030919]), 'previousTarget': array([51.5483439, 22.8492547]), 'currentState': array([72.        , 24.33655606,  0.        ]), 'targetState': array([25, 21], dtype=int32), 'currentDistance': 20.0}
episode index:1214
target Thresh 31.999863257046798
target distance 10.0
model initialize at round 1214
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 28.]), 'previousTarget': array([16., 28.]), 'currentState': array([ 6.5, 24.5,  0. ]), 'targetState': array([16, 28], dtype=int32), 'currentDistance': 10.124228365658269}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.1662830838730861
{'scaleFactor': 20, 'currentTarget': array([16., 28.]), 'previousTarget': array([16., 28.]), 'currentState': array([15.        , 27.46220434,  0.        ]), 'targetState': array([16, 28], dtype=int32), 'currentDistance': 1.1354400781207135}
episode index:1215
target Thresh 31.999864617661917
target distance 18.0
model initialize at round 1215
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3.92822063, 17.69714823]), 'previousTarget': array([ 3.78641543, 17.70981108]), 'currentState': array([20.5,  6.5,  0. ]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16614633791595362
{'scaleFactor': 20, 'currentTarget': array([50.03062114, 21.66089679]), 'previousTarget': array([49.53005908, 21.60883251]), 'currentState': array([70.        , 22.76720053,  0.        ]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 20.0}
episode index:1216
target Thresh 31.99986596473869
target distance 12.0
model initialize at round 1216
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 17.]), 'previousTarget': array([11., 17.]), 'currentState': array([18.5, 28.5,  0. ]), 'targetState': array([11, 17], dtype=int32), 'currentDistance': 13.72953021774596}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16600981668512704
{'scaleFactor': 20, 'currentTarget': array([48.08005833, 20.32774889]), 'previousTarget': array([47.57901678, 20.26123071]), 'currentState': array([68.        , 22.11546355,  0.        ]), 'targetState': array([11, 17], dtype=int32), 'currentDistance': 20.0}
episode index:1217
target Thresh 31.99986729841182
target distance 11.0
model initialize at round 1217
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([25.5, 21.5,  0. ]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 11.510864433221412}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16587351962709326
{'scaleFactor': 20, 'currentTarget': array([55.14552977, 26.99089924]), 'previousTarget': array([54.64682342, 26.9525144 ]), 'currentState': array([75.        , 29.39922065,  0.        ]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 20.0}
episode index:1218
target Thresh 31.999868618814688
target distance 13.0
model initialize at round 1218
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([21.5       ,  8.52502143,  0.        ]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 14.586838651479132}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1657374461901555
{'scaleFactor': 20, 'currentTarget': array([51.15297518,  8.36813918]), 'previousTarget': array([50.65400223,  8.3240572 ]), 'currentState': array([71.        , 10.83706725,  0.        ]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 20.0}
episode index:1219
target Thresh 31.999869926079324
target distance 11.0
model initialize at round 1219
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 15.]), 'previousTarget': array([13., 15.]), 'currentState': array([ 2.5, 21.5,  0. ]), 'targetState': array([13, 15], dtype=int32), 'currentDistance': 12.349089035228433}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.16627878322101244
{'scaleFactor': 20, 'currentTarget': array([13., 15.]), 'previousTarget': array([13., 15.]), 'currentState': array([12.        , 14.59751598,  0.        ]), 'targetState': array([13, 15], dtype=int32), 'currentDistance': 1.0779579713651715}
episode index:1220
target Thresh 31.99987122033646
target distance 20.0
model initialize at round 1220
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.28870996, 15.4852476 ]), 'previousTarget': array([12.13411708, 15.37929463]), 'currentState': array([27.5,  2.5,  0. ]), 'targetState': array([ 7, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16614260076137197
{'scaleFactor': 20, 'currentTarget': array([57.07775996, 24.42886229]), 'previousTarget': array([56.57512543, 24.30906946]), 'currentState': array([77.        , 26.19077931,  0.        ]), 'targetState': array([ 7, 20], dtype=int32), 'currentDistance': 20.0}
episode index:1221
target Thresh 31.99987250171552
target distance 20.0
model initialize at round 1221
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.76772571, 14.58115517]), 'previousTarget': array([20.23878636, 15.0470316 ]), 'currentState': array([ 5.5, 27.5,  0. ]), 'targetState': array([25, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.16657083847422607
{'scaleFactor': 20, 'currentTarget': array([25., 11.]), 'previousTarget': array([25., 11.]), 'currentState': array([24.        , 10.59663107,  0.        ]), 'targetState': array([25, 11], dtype=int32), 'currentDistance': 1.078288686623429}
episode index:1222
target Thresh 31.999873770344646
target distance 14.0
model initialize at round 1222
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 26.]), 'previousTarget': array([ 8., 26.]), 'currentState': array([ 8.5, 12.5,  0. ]), 'targetState': array([ 8, 26], dtype=int32), 'currentDistance': 13.50925608610622}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16643463991455787
{'scaleFactor': 20, 'currentTarget': array([38.1410861, 29.5992188]), 'previousTarget': array([37.64360067, 29.57155813]), 'currentState': array([58.        , 31.97061896,  0.        ]), 'targetState': array([ 8, 26], dtype=int32), 'currentDistance': 20.0}
episode index:1223
target Thresh 31.999875026350704
target distance 14.0
model initialize at round 1223
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([14.5, 17.5,  0. ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 13.583077707206039}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16629866390155576
{'scaleFactor': 20, 'currentTarget': array([44.18791115,  7.89149414]), 'previousTarget': array([43.68798117,  7.8231984 ]), 'currentState': array([64.        , 10.62666054,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 20.0}
episode index:1224
target Thresh 31.999876269859293
target distance 18.0
model initialize at round 1224
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  4.]), 'previousTarget': array([19.48314552,  4.28714138]), 'currentState': array([ 2.5, 13.5,  0. ]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 19.912307751739842}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.16674881236579797
{'scaleFactor': 20, 'currentTarget': array([20.,  4.]), 'previousTarget': array([20.,  4.]), 'currentState': array([19.        ,  3.60323025,  0.        ]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 1.0758374562361745}
episode index:1225
target Thresh 31.99987750099476
target distance 13.0
model initialize at round 1225
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 24.]), 'previousTarget': array([23., 24.]), 'currentState': array([10.5       , 24.49999154,  0.        ]), 'targetState': array([23, 24], dtype=int32), 'currentDistance': 12.509995664915726}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.16726012188560047
{'scaleFactor': 20, 'currentTarget': array([23., 24.]), 'previousTarget': array([23., 24.]), 'currentState': array([22.        , 23.61458206,  0.        ]), 'targetState': array([23, 24], dtype=int32), 'currentDistance': 1.071702845932208}
episode index:1226
target Thresh 31.99987871988023
target distance 19.0
model initialize at round 1226
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 25.]), 'previousTarget': array([ 2., 25.]), 'currentState': array([2.5, 6.5, 0. ]), 'targetState': array([ 2, 25], dtype=int32), 'currentDistance': 18.50675552332175}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1671238055678453
{'scaleFactor': 20, 'currentTarget': array([32.07785341, 27.66168379]), 'previousTarget': array([31.57626865, 27.59036701]), 'currentState': array([52.        , 29.42465716,  0.        ]), 'targetState': array([ 2, 25], dtype=int32), 'currentDistance': 20.0}
episode index:1227
target Thresh 31.999879926637583
target distance 6.0
model initialize at round 1227
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([14.5,  7.5,  0. ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 7.382411530116772}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16698771126363698
{'scaleFactor': 20, 'currentTarget': array([44.04883267, 13.52372976]), 'previousTarget': array([43.56180671, 13.80227045]), 'currentState': array([64.        , 14.92048386,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 20.0}
episode index:1228
target Thresh 31.999881121387503
target distance 17.0
model initialize at round 1228
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 20.]), 'previousTarget': array([ 6., 20.]), 'currentState': array([23.5, 19.5,  0. ]), 'targetState': array([ 6, 20], dtype=int32), 'currentDistance': 17.507141400011676}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1668518384310384
{'scaleFactor': 20, 'currentTarget': array([53.17920975, 26.35865768]), 'previousTarget': array([52.68206767, 26.3423106 ]), 'currentState': array([73.        , 29.03003817,  0.        ]), 'targetState': array([ 6, 20], dtype=int32), 'currentDistance': 20.0}
episode index:1229
target Thresh 31.99988230424946
target distance 13.0
model initialize at round 1229
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([ 7.5, 20.5,  0. ]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 13.65650028374759}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16671618652987494
{'scaleFactor': 20, 'currentTarget': array([37.28314268, 12.13006898]), 'previousTarget': array([36.81591301, 12.28394525]), 'currentState': array([57.        , 15.48350563,  0.        ]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 20.0}
episode index:1230
target Thresh 31.999883475341743
target distance 6.0
model initialize at round 1230
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 23.]), 'previousTarget': array([11., 23.]), 'currentState': array([17.5       , 26.64611828,  0.        ]), 'targetState': array([11, 23], dtype=int32), 'currentDistance': 7.452796692222993}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1665807550217272
{'scaleFactor': 20, 'currentTarget': array([47.14667282, 27.4019199 ]), 'previousTarget': array([46.6405287 , 27.24741541]), 'currentState': array([67.        , 29.81964605,  0.        ]), 'targetState': array([11, 23], dtype=int32), 'currentDistance': 20.0}
episode index:1231
target Thresh 31.999884634781466
target distance 21.0
model initialize at round 1231
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8.01234015, 5.50594619]), 'previousTarget': array([8., 6.]), 'currentState': array([ 8.5, 25.5,  0. ]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16644554336992384
{'scaleFactor': 20, 'currentTarget': array([38.05490811,  7.23166671]), 'previousTarget': array([37.55746313,  7.24542839]), 'currentState': array([58.        ,  8.71264936,  0.        ]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:1232
target Thresh 31.999885782684572
target distance 9.0
model initialize at round 1232
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 19.]), 'previousTarget': array([19., 19.]), 'currentState': array([18.5, 27.5,  0. ]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 8.514693182963118}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16631055103953463
{'scaleFactor': 20, 'currentTarget': array([48.10121927, 21.93897656]), 'previousTarget': array([47.59821927, 21.84473325]), 'currentState': array([68.       , 23.9485848,  0.       ]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 20.0}
episode index:1233
target Thresh 31.99988691916585
target distance 9.0
model initialize at round 1233
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 6.5       , 11.44646347,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 8.622195576393391}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.16687274536984398
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.        ,  9.53804054,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.1015473397244355}
episode index:1234
target Thresh 31.99988804433895
target distance 12.0
model initialize at round 1234
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([ 3.5, 11.5,  0. ]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 15.572411502397381}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.16739327583369126
{'scaleFactor': 20, 'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([14., 22.,  0.]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 0.9999999999982414}
episode index:1235
target Thresh 31.99988915831639
target distance 19.0
model initialize at round 1235
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([22.5,  6.5,  0. ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 19.50640920313124}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1672578443807514
{'scaleFactor': 20, 'currentTarget': array([52.0475432 , 10.38794882]), 'previousTarget': array([51.5463514, 10.3108837]), 'currentState': array([72.        , 11.76616061,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 20.0}
episode index:1236
target Thresh 31.99989026120957
target distance 13.0
model initialize at round 1236
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 24.]), 'previousTarget': array([16., 24.]), 'currentState': array([ 9.5, 11.5,  0. ]), 'targetState': array([16, 24], dtype=int32), 'currentDistance': 14.089002803605265}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16712263189539914
{'scaleFactor': 20, 'currentTarget': array([39.12229482, 26.56882072]), 'previousTarget': array([38.62155039, 26.50545828]), 'currentState': array([59.        , 28.77717682,  0.        ]), 'targetState': array([16, 24], dtype=int32), 'currentDistance': 20.0}
episode index:1237
target Thresh 31.999891353128778
target distance 23.0
model initialize at round 1237
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.6022212 ,  7.69130636]), 'previousTarget': array([20.25132013,  8.26830308]), 'currentState': array([11.5, 25.5,  0. ]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16698763784701837
{'scaleFactor': 20, 'currentTarget': array([41.22181603,  5.73668102]), 'previousTarget': array([40.70474858,  5.55302353]), 'currentState': array([61.        ,  8.70710837,  0.        ]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 20.0}
episode index:1238
target Thresh 31.99989243418321
target distance 19.0
model initialize at round 1238
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  5.]), 'previousTarget': array([9.91410718, 5.23313766]), 'currentState': array([ 3.5, 23.5,  0. ]), 'targetState': array([10,  5], dtype=int32), 'currentDistance': 19.608671551127504}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.166852861706706
{'scaleFactor': 20, 'currentTarget': array([33.0574487 ,  6.75141295]), 'previousTarget': array([32.56256887,  6.78891028]), 'currentState': array([53.        ,  8.26622245,  0.        ]), 'targetState': array([10,  5], dtype=int32), 'currentDistance': 20.0}
episode index:1239
target Thresh 31.99989350448097
target distance 5.0
model initialize at round 1239
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 8.]), 'previousTarget': array([7., 8.]), 'currentState': array([ 7.5, 12.5,  0. ]), 'targetState': array([7, 8], dtype=int32), 'currentDistance': 4.527692569068629}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1667183029472651
{'scaleFactor': 20, 'currentTarget': array([37.13488757, 11.51771789]), 'previousTarget': array([36.62458591, 11.32218014]), 'currentState': array([57.        , 13.83662023,  0.        ]), 'targetState': array([7, 8], dtype=int32), 'currentDistance': 20.0}
episode index:1240
target Thresh 31.999894564129086
target distance 4.0
model initialize at round 1240
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([16.5, 18.5,  0. ]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 3.5355339059326685}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16658396104319803
{'scaleFactor': 20, 'currentTarget': array([46.70357222, 23.3664817 ]), 'previousTarget': array([46.20041961, 23.20990657]), 'currentState': array([66.        , 28.62460635,  0.        ]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 20.0}
episode index:1241
target Thresh 31.99989561323353
target distance 6.0
model initialize at round 1241
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 8.]), 'previousTarget': array([6., 8.]), 'currentState': array([ 4.5, 13.5,  0. ]), 'targetState': array([6, 8], dtype=int32), 'currentDistance': 5.700877125495614}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16644983547069947
{'scaleFactor': 20, 'currentTarget': array([34.12717855, 11.18722011]), 'previousTarget': array([33.6306349 , 11.17362892]), 'currentState': array([54.        , 13.43910101,  0.        ]), 'targetState': array([6, 8], dtype=int32), 'currentDistance': 20.0}
episode index:1242
target Thresh 31.999896651899213
target distance 6.0
model initialize at round 1242
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 12.]), 'previousTarget': array([20., 12.]), 'currentState': array([21.5,  6.5,  0. ]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 5.7008771254957145}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16631592570764983
{'scaleFactor': 20, 'currentTarget': array([51.10179572, 15.15001778]), 'previousTarget': array([50.60216098, 15.10501279]), 'currentState': array([71.        , 17.16532577,  0.        ]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:1243
target Thresh 31.999897680229996
target distance 8.0
model initialize at round 1243
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([15.5       , 13.50057247,  0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 9.617959897141576}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1661822312336083
{'scaleFactor': 20, 'currentTarget': array([45.07736845, 12.35901362]), 'previousTarget': array([44.58014627, 12.37449242]), 'currentState': array([65.        , 14.11649827,  0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 20.0}
episode index:1244
target Thresh 31.999898698328717
target distance 15.0
model initialize at round 1244
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 18.]), 'previousTarget': array([25., 18.]), 'currentState': array([10.5,  6.5,  0. ]), 'targetState': array([25, 18], dtype=int32), 'currentDistance': 18.506755523321686}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.16666107499514526
{'scaleFactor': 20, 'currentTarget': array([25., 18.]), 'previousTarget': array([25., 18.]), 'currentState': array([24.        , 17.15333132,  0.        ]), 'targetState': array([25, 18], dtype=int32), 'currentDistance': 1.3102854063666318}
episode index:1245
target Thresh 31.99989970629719
target distance 13.0
model initialize at round 1245
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 20.]), 'previousTarget': array([13., 20.]), 'currentState': array([21.5,  7.5,  0. ]), 'targetState': array([13, 20], dtype=int32), 'currentDistance': 15.11621645783102}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1665273181131267
{'scaleFactor': 20, 'currentTarget': array([51.05370721, 22.79440477]), 'previousTarget': array([50.55436787, 22.7747154 ]), 'currentState': array([71.       , 24.2591245,  0.       ]), 'targetState': array([13, 20], dtype=int32), 'currentDistance': 20.0}
episode index:1246
target Thresh 31.999900704236207
target distance 16.0
model initialize at round 1246
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([18.5, 24.5,  0. ]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 16.807736313971503}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16639377575698147
{'scaleFactor': 20, 'currentTarget': array([48.07925253, 11.06073433]), 'previousTarget': array([47.57085735, 10.90500839]), 'currentState': array([68.        , 12.83944739,  0.        ]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 20.0}
episode index:1247
target Thresh 31.999901692245565
target distance 15.0
model initialize at round 1247
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([5.5, 9.5, 0. ]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 14.916433890176284}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.16687129894495434
{'scaleFactor': 20, 'currentTarget': array([20., 13.]), 'previousTarget': array([20., 13.]), 'currentState': array([19.        , 12.13722229,  0.        ]), 'targetState': array([20, 13], dtype=int32), 'currentDistance': 1.3207518249958967}
episode index:1248
target Thresh 31.999902670424063
target distance 6.0
model initialize at round 1248
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 21.]), 'previousTarget': array([19., 21.]), 'currentState': array([25.5, 20.5,  0. ]), 'targetState': array([19, 21], dtype=int32), 'currentDistance': 6.51920240520272}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16673769502266053
{'scaleFactor': 20, 'currentTarget': array([55.13083137, 25.15310382]), 'previousTarget': array([54.62652693, 25.02655128]), 'currentState': array([75.        , 27.43699038,  0.        ]), 'targetState': array([19, 21], dtype=int32), 'currentDistance': 20.0}
episode index:1249
target Thresh 31.999903638869526
target distance 19.0
model initialize at round 1249
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 25.]), 'previousTarget': array([22., 25.]), 'currentState': array([ 3.5       , 22.50104254,  0.        ]), 'targetState': array([22, 25], dtype=int32), 'currentDistance': 18.66801511577199}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.16716706302264206
{'scaleFactor': 20, 'currentTarget': array([22., 25.]), 'previousTarget': array([22., 25.]), 'currentState': array([21.        , 24.07324387,  0.        ]), 'targetState': array([22, 25], dtype=int32), 'currentDistance': 1.3634063671635617}
episode index:1250
target Thresh 31.999904597678796
target distance 1.0
model initialize at round 1250
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 13.]), 'previousTarget': array([17., 13.]), 'currentState': array([18.5       , 12.49999878,  0.        ]), 'targetState': array([17, 13], dtype=int32), 'currentDistance': 1.5811392164819}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16703343627362316
{'scaleFactor': 20, 'currentTarget': array([48.17414042, 17.14090088]), 'previousTarget': array([47.67185826, 17.04704751]), 'currentState': array([68.        , 19.77439513,  0.        ]), 'targetState': array([17, 13], dtype=int32), 'currentDistance': 20.0}
episode index:1251
target Thresh 31.99990554694775
target distance 12.0
model initialize at round 1251
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 20.]), 'previousTarget': array([22., 20.]), 'currentState': array([10.5, 28.5,  0. ]), 'targetState': array([22, 20], dtype=int32), 'currentDistance': 14.30034964607498}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.16754677048444394
{'scaleFactor': 20, 'currentTarget': array([22., 20.]), 'previousTarget': array([22., 20.]), 'currentState': array([21.        , 19.28012896,  0.        ]), 'targetState': array([22, 20], dtype=int32), 'currentDistance': 1.2321583989962948}
episode index:1252
target Thresh 31.999906486771323
target distance 16.0
model initialize at round 1252
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6.09203666, 9.06414677]), 'previousTarget': array([6., 9.]), 'currentState': array([22.5, 20.5,  0. ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1674130539876487
{'scaleFactor': 20, 'currentTarget': array([52.07773475, 13.0744369 ]), 'previousTarget': array([51.57716743, 13.01535486]), 'currentState': array([72.        , 14.83606892,  0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 20.0}
episode index:1253
target Thresh 31.999907417243495
target distance 10.0
model initialize at round 1253
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'previousTarget': array([11., 19.]), 'currentState': array([7.5, 9.5, 0. ]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 10.12422836565822}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1672795507548037
{'scaleFactor': 20, 'currentTarget': array([37.03512368, 20.54501338]), 'previousTarget': array([36.54079107, 20.63373657]), 'currentState': array([57.       , 21.7297975,  0.       ]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 20.0}
episode index:1254
target Thresh 31.999908338457317
target distance 13.0
model initialize at round 1254
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([16.5       ,  2.49999997,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 13.509256085003345}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1671462602761146
{'scaleFactor': 20, 'currentTarget': array([46.09849555,  6.29318035]), 'previousTarget': array([45.59305637,  6.1231726 ]), 'currentState': array([66.        ,  8.27563349,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 20.0}
episode index:1255
target Thresh 31.999909250504903
target distance 4.0
model initialize at round 1255
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 13.]), 'previousTarget': array([ 7., 13.]), 'currentState': array([9.5, 9.5, 0. ]), 'targetState': array([ 7, 13], dtype=int32), 'currentDistance': 4.301162633521295}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16701318204341067
{'scaleFactor': 20, 'currentTarget': array([39.00741064, 13.87156358]), 'previousTarget': array([38.50640286, 13.79742542]), 'currentState': array([59.       , 14.4159629,  0.       ]), 'targetState': array([ 7, 13], dtype=int32), 'currentDistance': 20.0}
episode index:1256
target Thresh 31.99991015347747
target distance 17.0
model initialize at round 1256
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.83524419,  5.02085402]), 'previousTarget': array([13.85099785,  5.11284334]), 'currentState': array([26.5, 20.5,  0. ]), 'targetState': array([13,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16688031555013827
{'scaleFactor': 20, 'currentTarget': array([56.00469511,  4.93199863]), 'previousTarget': array([55.50468034,  4.91971209]), 'currentState': array([76.        ,  5.36533729,  0.        ]), 'targetState': array([13,  4], dtype=int32), 'currentDistance': 20.0}
episode index:1257
target Thresh 31.999911047465304
target distance 13.0
model initialize at round 1257
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 25.]), 'previousTarget': array([12., 25.]), 'currentState': array([25.5, 14.5,  0. ]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 17.102631376487082}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16674766029135438
{'scaleFactor': 20, 'currentTarget': array([55.00246901, 25.67576425]), 'previousTarget': array([54.50240027, 25.65854057]), 'currentState': array([75.        , 25.99001635,  0.        ]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 20.0}
episode index:1258
target Thresh 31.99991193255781
target distance 4.0
model initialize at round 1258
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 11.]), 'previousTarget': array([25., 11.]), 'currentState': array([21.5       , 10.51177904,  0.        ]), 'targetState': array([25, 11], dtype=int32), 'currentDistance': 3.533887336303582}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.167370569258478
{'scaleFactor': 20, 'currentTarget': array([25., 11.]), 'previousTarget': array([25., 11.]), 'currentState': array([24.        , 10.14433324,  0.        ]), 'targetState': array([25, 11], dtype=int32), 'currentDistance': 1.31611762815905}
episode index:1259
target Thresh 31.999912808843504
target distance 7.0
model initialize at round 1259
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 18.]), 'previousTarget': array([21., 18.]), 'currentState': array([17.5, 11.5,  0. ]), 'targetState': array([21, 18], dtype=int32), 'currentDistance': 7.382411530116592}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16723773547335222
{'scaleFactor': 20, 'currentTarget': array([47.04962547, 19.83849808]), 'previousTarget': array([46.55362188, 19.8749864 ]), 'currentState': array([67.        , 21.24653082,  0.        ]), 'targetState': array([21, 18], dtype=int32), 'currentDistance': 20.0}
episode index:1260
target Thresh 31.999913676410007
target distance 18.0
model initialize at round 1260
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6.70865964, 11.3639063 ]), 'previousTarget': array([ 6.51685448, 11.28714138]), 'currentState': array([24.5, 20.5,  0. ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16710511236829803
{'scaleFactor': 20, 'currentTarget': array([54.00042942, 10.68544677]), 'previousTarget': array([53.50041221, 10.69502505]), 'currentState': array([74.        , 10.55438691,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 20.0}
episode index:1261
target Thresh 31.99991453534408
target distance 12.0
model initialize at round 1261
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  3.]), 'previousTarget': array([26.,  3.]), 'currentState': array([25.5, 14.5,  0. ]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 11.510864433221254}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16697269944249113
{'scaleFactor': 20, 'currentTarget': array([55.00063589,  2.76873527]), 'previousTarget': array([54.5014896 ,  2.65212273]), 'currentState': array([75.        ,  2.60925092,  0.        ]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 20.0}
episode index:1262
target Thresh 31.99991538573161
target distance 14.0
model initialize at round 1262
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 27.]), 'previousTarget': array([25., 27.]), 'currentState': array([26.5, 13.5,  0. ]), 'targetState': array([25, 27], dtype=int32), 'currentDistance': 13.583077707206053}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16684049619669344
{'scaleFactor': 20, 'currentTarget': array([56.00008096, 26.91179521]), 'previousTarget': array([55.50032574, 26.82592196]), 'currentState': array([76.        , 26.85488927,  0.        ]), 'targetState': array([25, 27], dtype=int32), 'currentDistance': 20.0}
episode index:1263
target Thresh 31.99991622765765
target distance 22.0
model initialize at round 1263
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.79373208, 19.41672303]), 'previousTarget': array([23.20732955, 19.72394111]), 'currentState': array([ 5.5, 27.5,  0. ]), 'targetState': array([27, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.16723245944415177
{'scaleFactor': 20, 'currentTarget': array([27., 18.]), 'previousTarget': array([27., 18.]), 'currentState': array([26.        , 17.09460825,  0.        ]), 'targetState': array([27, 18], dtype=int32), 'currentDistance': 1.3489752502881702}
episode index:1264
target Thresh 31.999917061206386
target distance 9.0
model initialize at round 1264
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([23.5, 13.5,  0. ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 11.51086443322136}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16710025987146865
{'scaleFactor': 20, 'currentTarget': array([53.01077196,  8.28087685]), 'previousTarget': array([52.51051692,  8.24938097]), 'currentState': array([73.        ,  8.93720171,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 20.0}
episode index:1265
target Thresh 31.99991788646117
target distance 20.0
model initialize at round 1265
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 26.]), 'previousTarget': array([11.00992562, 25.9007438 ]), 'currentState': array([13.5,  6.5,  0. ]), 'targetState': array([11, 26], dtype=int32), 'currentDistance': 19.65960325133751}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1669682691448719
{'scaleFactor': 20, 'currentTarget': array([43.02923805, 27.73379343]), 'previousTarget': array([42.52629811, 27.61831799]), 'currentState': array([63.        , 28.81484244,  0.        ]), 'targetState': array([11, 26], dtype=int32), 'currentDistance': 20.0}
episode index:1266
target Thresh 31.99991870350453
target distance 10.0
model initialize at round 1266
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  3.]), 'previousTarget': array([12.,  3.]), 'currentState': array([2.5       , 2.50121599, 0.        ]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 9.513084961477235}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.16750179315768882
{'scaleFactor': 20, 'currentTarget': array([12.,  3.]), 'previousTarget': array([12.,  3.]), 'currentState': array([11.       ,  2.0987902,  0.       ]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 1.346172019543698}
episode index:1267
target Thresh 31.999919512418177
target distance 16.0
model initialize at round 1267
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([11.5, 24.5,  0. ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 15.700318468107488}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1673696939517285
{'scaleFactor': 20, 'currentTarget': array([41.10881142, 11.83939275]), 'previousTarget': array([40.60822742, 11.77941105]), 'currentState': array([61.        , 13.92280747,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 20.0}
episode index:1268
target Thresh 31.999920313283
target distance 20.0
model initialize at round 1268
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8.00883877, 9.71572468]), 'previousTarget': array([7.85786438, 9.85786438]), 'currentState': array([22.5, 23.5,  0. ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1672378029399462
{'scaleFactor': 20, 'currentTarget': array([52.18511581, 10.87587344]), 'previousTarget': array([51.68511686, 10.80738802]), 'currentState': array([72.        , 13.59071496,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 20.0}
episode index:1269
target Thresh 31.99992110617908
target distance 12.0
model initialize at round 1269
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 17.]), 'previousTarget': array([18., 17.]), 'currentState': array([ 9.5, 28.5,  0. ]), 'targetState': array([18, 17], dtype=int32), 'currentDistance': 14.30034964607498}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16710611963054467
{'scaleFactor': 20, 'currentTarget': array([39.31805386, 20.84792266]), 'previousTarget': array([38.82122263, 20.77737695]), 'currentState': array([59.        , 24.40052681,  0.        ]), 'targetState': array([18, 17], dtype=int32), 'currentDistance': 20.0}
episode index:1270
target Thresh 31.999921891185714
target distance 24.0
model initialize at round 1270
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4.69727837, 21.30752452]), 'previousTarget': array([ 4.67544468, 20.97366596]), 'currentState': array([11.5,  2.5,  0. ]), 'targetState': array([ 3, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16697464353327438
{'scaleFactor': 20, 'currentTarget': array([41.1683841 , 30.98436886]), 'previousTarget': array([40.6674494 , 30.90510709]), 'currentState': array([61.        , 33.57415858,  0.        ]), 'targetState': array([ 3, 26], dtype=int32), 'currentDistance': 20.0}
episode index:1271
target Thresh 31.9999226683814
target distance 14.0
model initialize at round 1271
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 25.]), 'previousTarget': array([24., 25.]), 'currentState': array([19.5, 11.5,  0. ]), 'targetState': array([24, 25], dtype=int32), 'currentDistance': 14.230249470757606}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16684337415942746
{'scaleFactor': 20, 'currentTarget': array([49.19961304, 28.58722235]), 'previousTarget': array([48.69905422, 28.51096725]), 'currentState': array([69.        , 31.40585257,  0.        ]), 'targetState': array([24, 25], dtype=int32), 'currentDistance': 20.0}
episode index:1272
target Thresh 31.999923437843865
target distance 13.0
model initialize at round 1272
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 22.]), 'previousTarget': array([21., 22.]), 'currentState': array([ 8.5, 27.5,  0. ]), 'targetState': array([21, 22], dtype=int32), 'currentDistance': 13.656500283747558}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.16733573151173242
{'scaleFactor': 20, 'currentTarget': array([21., 22.]), 'previousTarget': array([21., 22.]), 'currentState': array([20.        , 21.14338308,  0.        ]), 'targetState': array([21, 22], dtype=int32), 'currentDistance': 1.3167355651704558}
episode index:1273
target Thresh 31.999924199650046
target distance 20.0
model initialize at round 1273
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.88419958,  9.05615617]), 'previousTarget': array([19.46924689,  9.61536159]), 'currentState': array([ 8.5, 25.5,  0. ]), 'targetState': array([22,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16720438478370123
{'scaleFactor': 20, 'currentTarget': array([38.16743922,  8.10528138]), 'previousTarget': array([37.66715102,  8.03835645]), 'currentState': array([58.        , 10.68782524,  0.        ]), 'targetState': array([22,  6], dtype=int32), 'currentDistance': 20.0}
episode index:1274
target Thresh 31.99992495387613
target distance 3.0
model initialize at round 1274
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  3.]), 'previousTarget': array([20.,  3.]), 'currentState': array([17.5       ,  4.50000003,  0.        ]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 2.915475962755763}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.16783426291328266
{'scaleFactor': 20, 'currentTarget': array([20.,  3.]), 'previousTarget': array([20.,  3.]), 'currentState': array([19.        ,  3.00011808,  0.        ]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 1.0000000069707458}
episode index:1275
target Thresh 31.99992570059754
target distance 15.0
model initialize at round 1275
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 15.]), 'previousTarget': array([23., 15.]), 'currentState': array([ 8.5, 10.5,  0. ]), 'targetState': array([23, 15], dtype=int32), 'currentDistance': 15.182226450688876}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.1683001786275725
{'scaleFactor': 20, 'currentTarget': array([23., 15.]), 'previousTarget': array([23., 15.]), 'currentState': array([22.        , 14.05784318,  0.        ]), 'targetState': array([23, 15], dtype=int32), 'currentDistance': 1.3739212058135561}
episode index:1276
target Thresh 31.999926439888945
target distance 9.0
model initialize at round 1276
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 26.]), 'previousTarget': array([19., 26.]), 'currentState': array([18.5, 17.5,  0. ]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 8.514693182963114}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1681683852222259
{'scaleFactor': 20, 'currentTarget': array([48.237772  , 30.54908527]), 'previousTarget': array([47.73263275, 30.42105013]), 'currentState': array([68.        , 33.62387702,  0.        ]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 20.0}
episode index:1277
target Thresh 31.99992717182428
target distance 14.0
model initialize at round 1277
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 26.]), 'previousTarget': array([18., 26.]), 'currentState': array([ 4.5, 15.5,  0. ]), 'targetState': array([18, 26], dtype=int32), 'currentDistance': 17.10263137648702}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.1686454219782329
{'scaleFactor': 20, 'currentTarget': array([18., 26.]), 'previousTarget': array([18., 26.]), 'currentState': array([17.        , 25.02974191,  0.        ]), 'targetState': array([18, 26], dtype=int32), 'currentDistance': 1.3933415839939198}
episode index:1278
target Thresh 31.999927896476734
target distance 11.0
model initialize at round 1278
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 18.]), 'previousTarget': array([ 9., 18.]), 'currentState': array([12.5, 28.5,  0. ]), 'targetState': array([ 9, 18], dtype=int32), 'currentDistance': 11.06797181058928}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1685135647288363
{'scaleFactor': 20, 'currentTarget': array([42.42403811, 24.99445683]), 'previousTarget': array([41.92520028, 24.89981157]), 'currentState': array([62.        , 29.09100614,  0.        ]), 'targetState': array([ 9, 18], dtype=int32), 'currentDistance': 20.0}
episode index:1279
target Thresh 31.99992861391878
target distance 15.0
model initialize at round 1279
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 27.]), 'previousTarget': array([19., 27.]), 'currentState': array([21.5, 12.5,  0. ]), 'targetState': array([19, 27], dtype=int32), 'currentDistance': 14.713938969562095}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1683819135063919
{'scaleFactor': 20, 'currentTarget': array([51.36917115, 33.30700818]), 'previousTarget': array([50.86766853, 33.19628596]), 'currentState': array([71.        , 37.13199948,  0.        ]), 'targetState': array([19, 27], dtype=int32), 'currentDistance': 20.0}
episode index:1280
target Thresh 31.999929324222155
target distance 16.0
model initialize at round 1280
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9.35786438, 8.35786438]), 'previousTarget': array([9.32117742, 8.40925592]), 'currentState': array([23.5, 22.5,  0. ]), 'targetState': array([8, 7], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16825046782840095
{'scaleFactor': 20, 'currentTarget': array([53.22396209, 13.82538783]), 'previousTarget': array([52.7259768 , 13.78104279]), 'currentState': array([73.        , 16.81006946,  0.        ]), 'targetState': array([8, 7], dtype=int32), 'currentDistance': 20.0}
episode index:1281
target Thresh 31.999930027457896
target distance 16.0
model initialize at round 1281
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 12.]), 'previousTarget': array([ 7., 12.]), 'currentState': array([23.5       , 17.50000024,  0.        ]), 'targetState': array([ 7, 12], dtype=int32), 'currentDistance': 17.392527206320775}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1681192272138702
{'scaleFactor': 20, 'currentTarget': array([53.26705072, 19.63751531]), 'previousTarget': array([52.76641203, 19.54564929]), 'currentState': array([73.       , 22.8949242,  0.       ]), 'targetState': array([ 7, 12], dtype=int32), 'currentDistance': 20.0}
episode index:1282
target Thresh 31.999930723696323
target distance 16.0
model initialize at round 1282
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  8.]), 'previousTarget': array([12.,  8.]), 'currentState': array([24.5, 23.5,  0. ]), 'targetState': array([12,  8], dtype=int32), 'currentDistance': 19.912307751739867}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16798819118330602
{'scaleFactor': 20, 'currentTarget': array([54.47354737, 17.41067585]), 'previousTarget': array([53.95482238, 17.10353245]), 'currentState': array([74.        , 21.73706551,  0.        ]), 'targetState': array([12,  8], dtype=int32), 'currentDistance': 20.0}
episode index:1283
target Thresh 31.99993141300706
target distance 24.0
model initialize at round 1283
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.46433444, 16.29105728]), 'previousTarget': array([20.88854382, 15.94427191]), 'currentState': array([3.5, 7.5, 0. ]), 'targetState': array([27, 19], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.1683429709369726
{'scaleFactor': 20, 'currentTarget': array([27., 19.]), 'previousTarget': array([27., 19.]), 'currentState': array([27.        , 18.09313822,  0.        ]), 'targetState': array([27, 19], dtype=int32), 'currentDistance': 0.906861783004075}
episode index:1284
target Thresh 31.999932095459044
target distance 19.0
model initialize at round 1284
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8.66241722, 17.29332229]), 'previousTarget': array([ 8.49385478, 17.29367831]), 'currentState': array([25.5,  6.5,  0. ]), 'targetState': array([ 6, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16821196473390881
{'scaleFactor': 20, 'currentTarget': array([55.85174347, 34.03355652]), 'previousTarget': array([55.32692205, 33.64260388]), 'currentState': array([75.        , 39.80800646,  0.        ]), 'targetState': array([ 6, 19], dtype=int32), 'currentDistance': 20.0}
episode index:1285
target Thresh 31.999932771120516
target distance 1.0
model initialize at round 1285
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([26.5       ,  9.50001195,  0.        ]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 0.7071152316802809}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.16885876724966783
{'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([26.5       ,  9.50001195,  0.        ]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 0.7071152316802809}
episode index:1286
target Thresh 31.999933440059042
target distance 13.0
model initialize at round 1286
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 14.]), 'previousTarget': array([ 2., 14.]), 'currentState': array([15.5,  8.5,  0. ]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 14.577379737113295}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16872756385631146
{'scaleFactor': 20, 'currentTarget': array([45.22709956, 20.57033466]), 'previousTarget': array([44.7549336 , 20.89259546]), 'currentState': array([65.        , 23.57573114,  0.        ]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 20.0}
episode index:1287
target Thresh 31.99993410234152
target distance 7.0
model initialize at round 1287
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([21.5,  8.5,  0. ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 8.746427842267977}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16859656419493232
{'scaleFactor': 20, 'currentTarget': array([51.00117464,  4.40103895]), 'previousTarget': array([50.50049128,  4.25584044]), 'currentState': array([71.        ,  4.61779714,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 20.0}
episode index:1288
target Thresh 31.99993475803418
target distance 25.0
model initialize at round 1288
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7.5954295 , 19.45142848]), 'previousTarget': array([ 7.14246323, 19.38290441]), 'currentState': array([27.5, 17.5,  0. ]), 'targetState': array([ 2, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1684657677913676
{'scaleFactor': 20, 'currentTarget': array([57.00672268, 18.57341917]), 'previousTarget': array([56.50666196, 18.5927894 ]), 'currentState': array([77.        , 18.05490026,  0.        ]), 'targetState': array([ 2, 20], dtype=int32), 'currentDistance': 20.0}
episode index:1289
target Thresh 31.999935407202585
target distance 11.0
model initialize at round 1289
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([13.5, 17.5,  0. ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 13.209844813622938}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16833517417292468
{'scaleFactor': 20, 'currentTarget': array([43.00910367,  9.76223904]), 'previousTarget': array([42.50854859,  9.81523251]), 'currentState': array([63.        ,  9.15886201,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 20.0}
episode index:1290
target Thresh 31.999936049911657
target distance 15.0
model initialize at round 1290
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 17.]), 'previousTarget': array([ 7., 17.]), 'currentState': array([14.5,  2.5,  0. ]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 16.324827717314548}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16820478286837554
{'scaleFactor': 20, 'currentTarget': array([44.01474725, 15.57776757]), 'previousTarget': array([43.51185485, 15.74230592]), 'currentState': array([64.        , 14.80986622,  0.        ]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 20.0}
episode index:1291
target Thresh 31.99993668622567
target distance 19.0
model initialize at round 1291
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([8.5       , 6.50000215, 0.        ]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 18.668154415237883}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.1686028864381449
{'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([27.5       ,  8.05816046,  0.        ]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 1.0663309649623833}
episode index:1292
target Thresh 31.99993731620825
target distance 5.0
model initialize at round 1292
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 22.]), 'previousTarget': array([10., 22.]), 'currentState': array([ 5.5, 19.5,  0. ]), 'targetState': array([10, 22], dtype=int32), 'currentDistance': 5.147815070493467}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.16917193453448726
{'scaleFactor': 20, 'currentTarget': array([10., 22.]), 'previousTarget': array([10., 22.]), 'currentState': array([10.5       , 21.02574499,  0.        ]), 'targetState': array([10, 22], dtype=int32), 'currentDistance': 1.0950674948140613}
episode index:1293
target Thresh 31.999937939922397
target distance 7.0
model initialize at round 1293
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'previousTarget': array([ 8., 17.]), 'currentState': array([ 3.5, 10.5,  0. ]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 7.905694150420885}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1690411988818331
{'scaleFactor': 20, 'currentTarget': array([33.01006316, 16.20631894]), 'previousTarget': array([32.51690113, 15.99144688]), 'currentState': array([53.        , 15.57194893,  0.        ]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:1294
target Thresh 31.99993855743049
target distance 20.0
model initialize at round 1294
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2.78527149, 14.86592926]), 'previousTarget': array([ 2.38838649, 14.9223227 ]), 'currentState': array([22.5, 11.5,  0. ]), 'targetState': array([ 2, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16891066513752281
{'scaleFactor': 20, 'currentTarget': array([52.0447934 , 11.64497027]), 'previousTarget': array([51.54489797, 11.67459573]), 'currentState': array([72.        , 10.30716254,  0.        ]), 'targetState': array([ 2, 15], dtype=int32), 'currentDistance': 20.0}
episode index:1295
target Thresh 31.99993916879427
target distance 14.0
model initialize at round 1295
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 13.]), 'previousTarget': array([19., 13.]), 'currentState': array([ 6.5, 26.5,  0. ]), 'targetState': array([19, 13], dtype=int32), 'currentDistance': 18.398369492974044}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.1693745019281614
{'scaleFactor': 20, 'currentTarget': array([19., 13.]), 'previousTarget': array([19., 13.]), 'currentState': array([19.5       , 13.50000134,  0.        ]), 'targetState': array([19, 13], dtype=int32), 'currentDistance': 0.7071077294912309}
episode index:1296
target Thresh 31.99993977407488
target distance 2.0
model initialize at round 1296
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 19.]), 'previousTarget': array([18., 19.]), 'currentState': array([16.5, 20.5,  0. ]), 'targetState': array([18, 19], dtype=int32), 'currentDistance': 2.121320343559527}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.16999957941318208
{'scaleFactor': 20, 'currentTarget': array([18., 19.]), 'previousTarget': array([18., 19.]), 'currentState': array([17.5       , 19.50000063,  0.        ]), 'targetState': array([18, 19], dtype=int32), 'currentDistance': 0.7071072237283668}
episode index:1297
target Thresh 31.999940373332848
target distance 10.0
model initialize at round 1297
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([ 4.5, 20.5,  0. ]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 12.103718436910157}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.17051802595707327
{'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([13.        , 12.06968454,  0.        ]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 1.3658282702908044}
episode index:1298
target Thresh 31.9999409666281
target distance 15.0
model initialize at round 1298
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([10.5, 18.5,  0. ]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 18.506755523321633}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.1709619474877696
{'scaleFactor': 20, 'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([25.       ,  6.1344302,  0.       ]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 0.8655698001400953}
episode index:1299
target Thresh 31.999941554019962
target distance 21.0
model initialize at round 1299
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7.12394591, 24.456665  ]), 'previousTarget': array([ 6.76952105, 24.49442256]), 'currentState': array([26.5, 19.5,  0. ]), 'targetState': array([ 5, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17083043829739442
{'scaleFactor': 20, 'currentTarget': array([56.05293119, 21.27831281]), 'previousTarget': array([55.55224785, 21.3387709 ]), 'currentState': array([76.        , 19.82419943,  0.        ]), 'targetState': array([ 5, 25], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:1300
target Thresh 31.99994213556718
target distance 18.0
model initialize at round 1300
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 17.]), 'previousTarget': array([ 6., 17.]), 'currentState': array([24.5       , 15.49999857,  0.        ]), 'targetState': array([ 6, 17], dtype=int32), 'currentDistance': 18.56071130887875}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17069913127333802
{'scaleFactor': 20, 'currentTarget': array([54.07855226, 12.72621209]), 'previousTarget': array([53.57842266, 12.77418055]), 'currentState': array([74.        , 10.95535922,  0.        ]), 'targetState': array([ 6, 17], dtype=int32), 'currentDistance': 20.0}
episode index:1301
target Thresh 31.999942711327908
target distance 9.0
model initialize at round 1301
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([12.5, 17.5,  0. ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 8.514693182963128}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1705680259497794
{'scaleFactor': 20, 'currentTarget': array([42.0807545,  6.2886201]), 'previousTarget': array([41.59126558,  6.16333945]), 'currentState': array([62.        ,  4.49316507,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 20.0}
episode index:1302
target Thresh 31.99994328135972
target distance 18.0
model initialize at round 1302
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.05178812, 13.77974263]), 'previousTarget': array([20.54026305, 14.26752934]), 'currentState': array([ 6.5, 27.5,  0. ]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.1709879511275603
{'scaleFactor': 20, 'currentTarget': array([24., 11.]), 'previousTarget': array([24., 11.]), 'currentState': array([23.        , 11.00000036,  0.        ]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 0.9999999999976232}
episode index:1303
target Thresh 31.99994384571962
target distance 15.0
model initialize at round 1303
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 27.]), 'previousTarget': array([25., 27.]), 'currentState': array([10.5, 16.5,  0. ]), 'targetState': array([25, 27], dtype=int32), 'currentDistance': 17.902513789968044}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17085682539816802
{'scaleFactor': 20, 'currentTarget': array([40.12892377, 25.27383574]), 'previousTarget': array([39.60341965, 25.50911035]), 'currentState': array([60.        , 23.00660628,  0.        ]), 'targetState': array([25, 27], dtype=int32), 'currentDistance': 20.0}
episode index:1304
target Thresh 31.999944404464046
target distance 12.0
model initialize at round 1304
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 29.]), 'previousTarget': array([ 8., 29.]), 'currentState': array([17.5, 17.5,  0. ]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 14.916433890176286}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17072590062774798
{'scaleFactor': 20, 'currentTarget': array([47.02374327, 27.09679249]), 'previousTarget': array([46.52752397, 26.97663008]), 'currentState': array([67.        , 26.12254044,  0.        ]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 20.0}
episode index:1305
target Thresh 31.99994495764887
target distance 11.0
model initialize at round 1305
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 16.]), 'previousTarget': array([24., 16.]), 'currentState': array([17.5, 26.5,  0. ]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 12.34908903522836}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17059517635467925
{'scaleFactor': 20, 'currentTarget': array([47.01699151, 15.05061722]), 'previousTarget': array([46.50966121, 15.30009022]), 'currentState': array([67.        , 14.22637726,  0.        ]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 20.0}
episode index:1306
target Thresh 31.999945505329414
target distance 11.0
model initialize at round 1306
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([12.5,  4.5,  0. ]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 12.903487900563896}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17046465211875372
{'scaleFactor': 20, 'currentTarget': array([42.00988774, 14.3076471 ]), 'previousTarget': array([41.50446208, 14.54567137]), 'currentState': array([62.        , 13.67882916,  0.        ]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 20.0}
episode index:1307
target Thresh 31.999946047560446
target distance 10.0
model initialize at round 1307
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([24.5, 19.5,  0. ]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 11.06797181058938}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17033432746117058
{'scaleFactor': 20, 'currentTarget': array([54.0390091 , 20.49560793]), 'previousTarget': array([53.53749837, 20.57547455]), 'currentState': array([74.        , 19.24707185,  0.        ]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 20.0}
episode index:1308
target Thresh 31.99994658439619
target distance 12.0
model initialize at round 1308
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 22.]), 'previousTarget': array([19., 22.]), 'currentState': array([ 7.5, 15.5,  0. ]), 'targetState': array([19, 22], dtype=int32), 'currentDistance': 13.209844813622869}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.17081047716031686
{'scaleFactor': 20, 'currentTarget': array([19., 22.]), 'previousTarget': array([19., 22.]), 'currentState': array([19.        , 21.32580622,  0.        ]), 'targetState': array([19, 22], dtype=int32), 'currentDistance': 0.6741937771428574}
episode index:1309
target Thresh 31.99994711589033
target distance 1.0
model initialize at round 1309
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([18.5       ,  2.39527896,  0.        ]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 0.7846575913445943}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.1714434462617212
{'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([18.5       ,  2.39527896,  0.        ]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 0.7846575913445943}
episode index:1310
target Thresh 31.99994764209601
target distance 16.0
model initialize at round 1310
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.64213562, 22.64213562]), 'previousTarget': array([24.14213562, 22.14213562]), 'currentState': array([10.5,  8.5,  0. ]), 'targetState': array([26, 24], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.17187689929537991
{'scaleFactor': 20, 'currentTarget': array([26., 24.]), 'previousTarget': array([26., 24.]), 'currentState': array([25.5       , 23.07544971,  0.        ]), 'targetState': array([26, 24], dtype=int32), 'currentDistance': 1.0510914554636506}
episode index:1311
target Thresh 31.999948163065863
target distance 9.0
model initialize at round 1311
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  6.]), 'previousTarget': array([24.,  6.]), 'currentState': array([26.5, 14.5,  0. ]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 8.860022573334621}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17174589556116088
{'scaleFactor': 20, 'currentTarget': array([56.05972801,  3.51673368]), 'previousTarget': array([55.56445111,  3.45981651]), 'currentState': array([76.        ,  1.97221009,  0.        ]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 20.0}
episode index:1312
target Thresh 31.999948678851975
target distance 14.0
model initialize at round 1312
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 25.]), 'previousTarget': array([20., 25.]), 'currentState': array([ 6.5, 12.5,  0. ]), 'targetState': array([20, 25], dtype=int32), 'currentDistance': 18.398369492974048}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.17220156749584786
{'scaleFactor': 20, 'currentTarget': array([20., 25.]), 'previousTarget': array([20., 25.]), 'currentState': array([19.5       , 24.00403517,  0.        ]), 'targetState': array([20, 25], dtype=int32), 'currentDistance': 1.1144262863818022}
episode index:1313
target Thresh 31.99994918950593
target distance 21.0
model initialize at round 1313
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.21472851, 27.86592926]), 'previousTarget': array([23.79898987, 27.82842712]), 'currentState': array([ 4.5, 24.5,  0. ]), 'targetState': array([25, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.17257962700199234
{'scaleFactor': 20, 'currentTarget': array([25., 28.]), 'previousTarget': array([25., 28.]), 'currentState': array([24.5       , 27.17191614,  0.        ]), 'targetState': array([25, 28], dtype=int32), 'currentDistance': 0.9673276981976198}
episode index:1314
target Thresh 31.999949695078794
target distance 17.0
model initialize at round 1314
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  2.]), 'previousTarget': array([26.,  2.]), 'currentState': array([ 9.5, 11.5,  0. ]), 'targetState': array([26,  2], dtype=int32), 'currentDistance': 19.03943276465966}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.17300527243366717
{'scaleFactor': 20, 'currentTarget': array([26.,  2.]), 'previousTarget': array([26.,  2.]), 'currentState': array([25.        ,  1.60482968,  0.        ]), 'targetState': array([26,  2], dtype=int32), 'currentDistance': 1.0752486125180845}
episode index:1315
target Thresh 31.99995019562112
target distance 25.0
model initialize at round 1315
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.86839057, 27.67883155]), 'previousTarget': array([21.44774604, 27.66745905]), 'currentState': array([ 2.5       , 22.69230503,  0.        ]), 'targetState': array([27, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.17334761295225193
{'scaleFactor': 20, 'currentTarget': array([27., 29.]), 'previousTarget': array([27., 29.]), 'currentState': array([26.        , 28.38692536,  0.        ]), 'targetState': array([27, 29], dtype=int32), 'currentDistance': 1.1729708093922617}
episode index:1316
target Thresh 31.99995069118297
target distance 11.0
model initialize at round 1316
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([13.5, 14.5,  0. ]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 11.423659658795758}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.17384330088762273
{'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([23.        ,  9.26759592,  0.        ]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 1.2395223841514058}
episode index:1317
target Thresh 31.9999511818139
target distance 12.0
model initialize at round 1317
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 29.]), 'previousTarget': array([18., 29.]), 'currentState': array([13.5, 17.5,  0. ]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 12.349089035228364}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17371140156980208
{'scaleFactor': 20, 'currentTarget': array([43.1377646, 26.0341569]), 'previousTarget': array([42.62613853, 26.22104127]), 'currentState': array([63.        , 23.69073954,  0.        ]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 20.0}
episode index:1318
target Thresh 31.999951667562968
target distance 17.0
model initialize at round 1318
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([22.5, 22.5,  0. ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 17.734147850968156}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17357970225094704
{'scaleFactor': 20, 'currentTarget': array([52.16717022,  1.29423888]), 'previousTarget': array([51.66654645,  1.36815102]), 'currentState': array([72.        , -1.28623836,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 20.0}
episode index:1319
target Thresh 31.99995214847875
target distance 12.0
model initialize at round 1319
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 25.]), 'previousTarget': array([15., 25.]), 'currentState': array([27.5       , 22.49999997,  0.        ]), 'targetState': array([15, 25], dtype=int32), 'currentDistance': 12.747548789826748}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17344820247651452
{'scaleFactor': 20, 'currentTarget': array([57.27152191, 17.96267349]), 'previousTarget': array([56.77214783, 18.03763187]), 'currentState': array([77.        , 14.67829347,  0.        ]), 'targetState': array([15, 25], dtype=int32), 'currentDistance': 20.0}
episode index:1320
target Thresh 31.999952624609342
target distance 10.0
model initialize at round 1320
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([12.5, 14.5,  0. ]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 9.513148795220149}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17331690179333775
{'scaleFactor': 20, 'currentTarget': array([42.07905814,  2.31757213]), 'previousTarget': array([41.576282 ,  2.4094039]), 'currentState': array([62.        ,  0.54103745,  0.        ]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 20.0}
episode index:1321
target Thresh 31.999953096002354
target distance 5.0
model initialize at round 1321
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 12.]), 'previousTarget': array([ 2., 12.]), 'currentState': array([ 3.5, 16.5,  0. ]), 'targetState': array([ 2, 12], dtype=int32), 'currentDistance': 4.743416490252494}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17318579974962117
{'scaleFactor': 20, 'currentTarget': array([33.03792766, 10.08579205]), 'previousTarget': array([32.53398956, 10.2175784 ]), 'currentState': array([53.        ,  8.85466741,  0.        ]), 'targetState': array([ 2, 12], dtype=int32), 'currentDistance': 20.0}
episode index:1322
target Thresh 31.99995356270493
target distance 12.0
model initialize at round 1322
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([27.5, 25.5,  0. ]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 14.57737973711328}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17305489589493514
{'scaleFactor': 20, 'currentTarget': array([57.07756133, 14.28345648]), 'previousTarget': array([56.57562664, 14.3741437 ]), 'currentState': array([77.        , 12.52378676,  0.        ]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:1323
target Thresh 31.999954024763735
target distance 7.0
model initialize at round 1323
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 11.]), 'previousTarget': array([23., 11.]), 'currentState': array([25.5, 17.5,  0. ]), 'targetState': array([23, 11], dtype=int32), 'currentDistance': 6.964194138592015}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17292418978021087
{'scaleFactor': 20, 'currentTarget': array([55.02528278,  9.3881767 ]), 'previousTarget': array([54.5214755,  9.5380648]), 'currentState': array([75.        ,  8.38285491,  0.        ]), 'targetState': array([23, 11], dtype=int32), 'currentDistance': 20.0}
episode index:1324
target Thresh 31.99995448222498
target distance 4.0
model initialize at round 1324
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([18.5,  2.5,  0. ]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 4.30116263352127}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.17350423201388696
{'scaleFactor': 20, 'currentTarget': array([21.,  6.]), 'previousTarget': array([21.,  6.]), 'currentState': array([21.5       ,  5.24005541,  0.        ]), 'targetState': array([21,  6], dtype=int32), 'currentDistance': 0.9096789415771677}
episode index:1325
target Thresh 31.999954935134408
target distance 18.0
model initialize at round 1325
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([20.5,  5.5,  0. ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 18.82817038376281}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1733733841767724
{'scaleFactor': 20, 'currentTarget': array([50.03131869,  6.30885656]), 'previousTarget': array([49.53125955,  6.33939594]), 'currentState': array([70.        ,  5.19003269,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 20.0}
episode index:1326
target Thresh 31.999955383537312
target distance 14.0
model initialize at round 1326
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 24.]), 'previousTarget': array([10., 24.]), 'currentState': array([21.5, 10.5,  0. ]), 'targetState': array([10, 24], dtype=int32), 'currentDistance': 17.73414785096819}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1732427335481539
{'scaleFactor': 20, 'currentTarget': array([51.04599349, 21.21151039]), 'previousTarget': array([50.54745185, 21.20189485]), 'currentState': array([71.        , 19.85592035,  0.        ]), 'targetState': array([10, 24], dtype=int32), 'currentDistance': 20.0}
episode index:1327
target Thresh 31.999955827478534
target distance 11.0
model initialize at round 1327
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([5.5, 3.5, 0. ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 12.903487900563947}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.17373439461011733
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.        , 10.31979652,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.2094117443942927}
episode index:1328
target Thresh 31.999956267002467
target distance 19.0
model initialize at round 1328
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([ 9.5, 24.5,  0. ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 19.039432764659672}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17360366895578314
{'scaleFactor': 20, 'currentTarget': array([39.00306344,  5.56232919]), 'previousTarget': array([38.50639247,  5.38024788]), 'currentState': array([59.        ,  5.21228907,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 20.0}
episode index:1329
target Thresh 31.999956702153064
target distance 13.0
model initialize at round 1329
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  9.]), 'previousTarget': array([23.,  9.]), 'currentState': array([25.5, 21.5,  0. ]), 'targetState': array([23,  9], dtype=int32), 'currentDistance': 12.7475487839819}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17347313988138027
{'scaleFactor': 20, 'currentTarget': array([55.02755344,  7.31709034]), 'previousTarget': array([54.52761275,  7.34157454]), 'currentState': array([75.        ,  6.26762449,  0.        ]), 'targetState': array([23,  9], dtype=int32), 'currentDistance': 20.0}
episode index:1330
target Thresh 31.99995713297384
target distance 15.0
model initialize at round 1330
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 29.]), 'previousTarget': array([ 8., 29.]), 'currentState': array([23.5, 23.5,  0. ]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 16.44688420339858}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17334280694382853
{'scaleFactor': 20, 'currentTarget': array([53.02888622, 26.57725653]), 'previousTarget': array([52.52788688, 26.64610767]), 'currentState': array([73.        , 25.50272661,  0.        ]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 20.0}
episode index:1331
target Thresh 31.999957559507877
target distance 9.0
model initialize at round 1331
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 12.]), 'previousTarget': array([23., 12.]), 'currentState': array([15.5, 20.5,  0. ]), 'targetState': array([23, 12], dtype=int32), 'currentDistance': 11.335784048754519}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.17385190226226024
{'scaleFactor': 20, 'currentTarget': array([23., 12.]), 'previousTarget': array([23., 12.]), 'currentState': array([23.5       , 12.50002804,  0.        ]), 'targetState': array([23, 12], dtype=int32), 'currentDistance': 0.7071266115569785}
episode index:1332
target Thresh 31.999957981797827
target distance 14.0
model initialize at round 1332
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 19.]), 'previousTarget': array([ 7., 19.]), 'currentState': array([21.5, 20.5,  0. ]), 'targetState': array([ 7, 19], dtype=int32), 'currentDistance': 14.57737973711332}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1737214807301805
{'scaleFactor': 20, 'currentTarget': array([51.01081615, 17.55198839]), 'previousTarget': array([50.51039607, 17.59654984]), 'currentState': array([71.        , 16.89431892,  0.        ]), 'targetState': array([ 7, 19], dtype=int32), 'currentDistance': 20.0}
episode index:1333
target Thresh 31.999958399885926
target distance 3.0
model initialize at round 1333
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  6.]), 'previousTarget': array([26.,  6.]), 'currentState': array([25.5       ,  3.49999994,  0.        ]), 'targetState': array([26,  6], dtype=int32), 'currentDistance': 2.549509815243549}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17359125473263165
{'scaleFactor': 20, 'currentTarget': array([55.00042475,  5.81099366]), 'previousTarget': array([54.50000113,  5.99040577]), 'currentState': array([75.        ,  5.68064914,  0.        ]), 'targetState': array([26,  6], dtype=int32), 'currentDistance': 20.0}
episode index:1334
target Thresh 31.999958813813976
target distance 4.0
model initialize at round 1334
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([19.5       ,  2.51962736,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 4.737246367447016}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1734612238302102
{'scaleFactor': 20, 'currentTarget': array([49.0043032 ,  4.70550445]), 'previousTarget': array([48.50659122,  4.86044013]), 'currentState': array([69.        ,  5.12036526,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 20.0}
episode index:1335
target Thresh 31.999959223623375
target distance 2.0
model initialize at round 1335
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 15.]), 'previousTarget': array([10., 15.]), 'currentState': array([12.5       , 14.74513727,  0.        ]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 2.512957422832573}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17333138758482833
{'scaleFactor': 20, 'currentTarget': array([42.00986547, 16.00578108]), 'previousTarget': array([41.50667654, 15.81430496]), 'currentState': array([62.        , 16.63389053,  0.        ]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 20.0}
episode index:1336
target Thresh 31.9999596293551
target distance 4.0
model initialize at round 1336
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'previousTarget': array([10., 17.]), 'currentState': array([ 6.5       , 16.50992572,  0.        ]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 3.534143856282961}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.17391303205926
{'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'previousTarget': array([10., 17.]), 'currentState': array([ 9.        , 16.40795255,  0.        ]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 1.1621188311416923}
episode index:1337
target Thresh 31.999960031049728
target distance 9.0
model initialize at round 1337
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 10.]), 'previousTarget': array([22., 10.]), 'currentState': array([25.5, 18.5,  0. ]), 'targetState': array([22, 10], dtype=int32), 'currentDistance': 9.192388155425071}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1737830522146716
{'scaleFactor': 20, 'currentTarget': array([55.01432574, 11.25024279]), 'previousTarget': array([54.5109301 , 11.07527476]), 'currentState': array([75.        , 12.00709438,  0.        ]), 'targetState': array([22, 10], dtype=int32), 'currentDistance': 20.0}
episode index:1338
target Thresh 31.99996042874743
target distance 12.0
model initialize at round 1338
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([20.5, 23.5,  0. ]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 11.510864433221256}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17365326651473534
{'scaleFactor': 20, 'currentTarget': array([50.00498192, 12.64751834]), 'previousTarget': array([49.50782374, 12.79762497]), 'currentState': array([70.        , 13.09389479,  0.        ]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 20.0}
episode index:1339
target Thresh 31.99996082248797
target distance 6.0
model initialize at round 1339
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 25.]), 'previousTarget': array([10., 25.]), 'currentState': array([ 4.5, 28.5,  0. ]), 'targetState': array([10, 25], dtype=int32), 'currentDistance': 6.519202405202611}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.17420540381396588
{'scaleFactor': 20, 'currentTarget': array([10., 25.]), 'previousTarget': array([10., 25.]), 'currentState': array([ 9.        , 24.13938248,  0.        ]), 'targetState': array([10, 25], dtype=int32), 'currentDistance': 1.3193416968398277}
episode index:1340
target Thresh 31.99996121231073
target distance 18.0
model initialize at round 1340
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 28.]), 'previousTarget': array([13., 28.]), 'currentState': array([18.5, 10.5,  0. ]), 'targetState': array([13, 28], dtype=int32), 'currentDistance': 18.34393632784409}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1740754967268563
{'scaleFactor': 20, 'currentTarget': array([48.01360861, 29.29230652]), 'previousTarget': array([47.51105469, 29.14791996]), 'currentState': array([68.       , 30.0299781,  0.       ]), 'targetState': array([13, 28], dtype=int32), 'currentDistance': 20.0}
episode index:1341
target Thresh 31.999961598254686
target distance 11.0
model initialize at round 1341
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 12.]), 'previousTarget': array([17., 12.]), 'currentState': array([ 6.5, 18.5,  0. ]), 'targetState': array([17, 12], dtype=int32), 'currentDistance': 12.34908903522843}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.17456140814795074
{'scaleFactor': 20, 'currentTarget': array([17., 12.]), 'previousTarget': array([17., 12.]), 'currentState': array([16.        , 11.73089188,  0.        ]), 'targetState': array([17, 12], dtype=int32), 'currentDistance': 1.035576739040696}
episode index:1342
target Thresh 31.999961980358435
target distance 14.0
model initialize at round 1342
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 18.]), 'previousTarget': array([12., 18.]), 'currentState': array([16.5,  4.5,  0. ]), 'targetState': array([12, 18], dtype=int32), 'currentDistance': 14.230249470757736}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1744314294374906
{'scaleFactor': 20, 'currentTarget': array([46.00031288, 18.19018508]), 'previousTarget': array([45.50034743, 18.19746343]), 'currentState': array([66.        , 18.30205587,  0.        ]), 'targetState': array([12, 18], dtype=int32), 'currentDistance': 20.0}
episode index:1343
target Thresh 31.99996235866019
target distance 2.0
model initialize at round 1343
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 13.]), 'previousTarget': array([13., 13.]), 'currentState': array([15.5       , 12.51648432,  0.        ]), 'targetState': array([13, 13], dtype=int32), 'currentDistance': 2.5463282217043317}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17430164414773058
{'scaleFactor': 20, 'currentTarget': array([45.00004111, 13.06488238]), 'previousTarget': array([44.50001534, 13.03901586]), 'currentState': array([65.        , 13.10543374,  0.        ]), 'targetState': array([13, 13], dtype=int32), 'currentDistance': 20.0}
episode index:1344
target Thresh 31.99996273319778
target distance 9.0
model initialize at round 1344
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 15.]), 'previousTarget': array([13., 15.]), 'currentState': array([22.5, 21.5,  0. ]), 'targetState': array([13, 15], dtype=int32), 'currentDistance': 11.510864433221363}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17417205184724902
{'scaleFactor': 20, 'currentTarget': array([52.00003506, 14.92697819]), 'previousTarget': array([51.50002847, 14.93503733]), 'currentState': array([72.        , 14.88953121,  0.        ]), 'targetState': array([13, 15], dtype=int32), 'currentDistance': 20.0}
episode index:1345
target Thresh 31.999963104008657
target distance 15.0
model initialize at round 1345
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'previousTarget': array([ 5., 22.]), 'currentState': array([13.5,  7.5,  0. ]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 16.807736313971656}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17404265210590633
{'scaleFactor': 20, 'currentTarget': array([43.00037307, 21.76789112]), 'previousTarget': array([42.50020001, 21.8322898 ]), 'currentState': array([63.        , 21.64573202,  0.        ]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 20.0}
episode index:1346
target Thresh 31.999963471129906
target distance 15.0
model initialize at round 1346
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  9.]), 'previousTarget': array([23.,  9.]), 'currentState': array([ 8.5, 10.5,  0. ]), 'targetState': array([23,  9], dtype=int32), 'currentDistance': 14.577379737113164}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17391344449484034
{'scaleFactor': 20, 'currentTarget': array([38.00394771,  9.29815523]), 'previousTarget': array([37.51085975,  9.47838767]), 'currentState': array([58.       ,  9.6955125,  0.       ]), 'targetState': array([23,  9], dtype=int32), 'currentDistance': 20.0}
episode index:1347
target Thresh 31.999963834598237
target distance 13.0
model initialize at round 1347
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([17.5,  7.5,  0. ]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 15.11621645783093}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17378442858646137
{'scaleFactor': 20, 'currentTarget': array([47.00692666, 20.55301538]), 'previousTarget': array([46.5030988, 20.3609665]), 'currentState': array([67.        , 21.07934069,  0.        ]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 20.0}
episode index:1348
target Thresh 31.999964194449994
target distance 14.0
model initialize at round 1348
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([11.5       , 16.50000131,  0.        ]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 13.50925603757281}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.17423219502887255
{'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([24.       , 16.4875894,  0.       ]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 1.1236390090773152}
episode index:1349
target Thresh 31.99996455072117
target distance 8.0
model initialize at round 1349
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 20.]), 'previousTarget': array([ 6., 20.]), 'currentState': array([ 4.5, 12.5,  0. ]), 'targetState': array([ 6, 20], dtype=int32), 'currentDistance': 7.6485292703890995}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.174103134143666
{'scaleFactor': 20, 'currentTarget': array([34.00851003, 20.81732406]), 'previousTarget': array([33.50977508, 20.86041201]), 'currentState': array([54.        , 21.40070125,  0.        ]), 'targetState': array([ 6, 20], dtype=int32), 'currentDistance': 20.0}
episode index:1350
target Thresh 31.99996490344739
target distance 11.0
model initialize at round 1350
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([18.5, 16.5,  0. ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 12.747548783982003}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1739742643182451
{'scaleFactor': 20, 'currentTarget': array([48.01997833, 12.83485004]), 'previousTarget': array([47.52066437, 12.84342044]), 'currentState': array([68.        , 13.72856927,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 20.0}
episode index:1351
target Thresh 31.999965252663923
target distance 25.0
model initialize at round 1351
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.14722618, 11.26944291]), 'previousTarget': array([20.56953382, 11.57218647]), 'currentState': array([ 2.5, 18.5,  0. ]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.1743067725509174
{'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([26.        ,  8.21351841,  0.        ]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 1.2722237578180469}
episode index:1352
target Thresh 31.999965598405694
target distance 7.0
model initialize at round 1352
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([ 2.5, 12.5,  0. ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 6.67083203206309}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17417794271163364
{'scaleFactor': 20, 'currentTarget': array([32.03100469,  7.56263576]), 'previousTarget': array([31.53377306,  7.60214382]), 'currentState': array([52.        ,  8.67584118,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 20.0}
episode index:1353
target Thresh 31.999965940707277
target distance 20.0
model initialize at round 1353
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9.05511749, 24.1564651 ]), 'previousTarget': array([ 9., 24.]), 'currentState': array([21.5,  8.5,  0. ]), 'targetState': array([ 6, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17404930316753348
{'scaleFactor': 20, 'currentTarget': array([51.04790107, 31.12340558]), 'previousTarget': array([50.54787175, 31.08778722]), 'currentState': array([71.       , 32.5067885,  0.       ]), 'targetState': array([ 6, 28], dtype=int32), 'currentDistance': 20.0}
episode index:1354
target Thresh 31.9999662796029
target distance 8.0
model initialize at round 1354
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([23.5, 11.5,  0. ]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 10.700467279516365}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17392085349729913
{'scaleFactor': 20, 'currentTarget': array([53.06871689, 21.16388993]), 'previousTarget': array([52.57259717, 21.2100802 ]), 'currentState': array([73.        , 22.82037828,  0.        ]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 20.0}
episode index:1355
target Thresh 31.999966615126457
target distance 21.0
model initialize at round 1355
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.97484213, 18.16030035]), 'previousTarget': array([23.3829006 , 17.87838597]), 'currentState': array([ 5.5       , 10.49999955,  0.        ]), 'targetState': array([26, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.1742909185382748
{'scaleFactor': 20, 'currentTarget': array([26., 19.]), 'previousTarget': array([26., 19.]), 'currentState': array([25.        , 18.19039828,  0.        ]), 'targetState': array([26, 19], dtype=int32), 'currentDistance': 1.2866448428910264}
episode index:1356
target Thresh 31.9999669473115
target distance 2.0
model initialize at round 1356
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 16.]), 'previousTarget': array([ 4., 16.]), 'currentState': array([ 5.5       , 14.49860483,  0.        ]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 2.1223071041318566}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17416248013109847
{'scaleFactor': 20, 'currentTarget': array([35.08844003, 18.93337402]), 'previousTarget': array([34.58392036, 18.81058628]), 'currentState': array([55.        , 20.81214481,  0.        ]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 20.0}
episode index:1357
target Thresh 31.999967276191246
target distance 3.0
model initialize at round 1357
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  8.]), 'previousTarget': array([21.,  8.]), 'currentState': array([24.5       ,  7.50693965,  0.        ]), 'targetState': array([21,  8], dtype=int32), 'currentDistance': 3.5345591675844092}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1740342308821065
{'scaleFactor': 20, 'currentTarget': array([54.04938529, 10.3268461 ]), 'previousTarget': array([53.54991803, 10.30405467]), 'currentState': array([74.        , 11.73147162,  0.        ]), 'targetState': array([21,  8], dtype=int32), 'currentDistance': 20.0}
episode index:1358
target Thresh 31.999967601798584
target distance 11.0
model initialize at round 1358
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 18.]), 'previousTarget': array([ 5., 18.]), 'currentState': array([16.5, 15.5,  0. ]), 'targetState': array([ 5, 18], dtype=int32), 'currentDistance': 11.768602295939882}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17390617037373116
{'scaleFactor': 20, 'currentTarget': array([46.10800007, 22.28946192]), 'previousTarget': array([45.60821831, 22.24162574]), 'currentState': array([66.        , 24.36511571,  0.        ]), 'targetState': array([ 5, 18], dtype=int32), 'currentDistance': 20.0}
episode index:1359
target Thresh 31.99996792416607
target distance 4.0
model initialize at round 1359
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([8.5, 7.5, 0. ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 3.5355339059327533}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17377829818963283
{'scaleFactor': 20, 'currentTarget': array([38.09414708, 13.93037348]), 'previousTarget': array([37.59350037, 13.87163955]), 'currentState': array([58.        , 15.86867674,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 20.0}
episode index:1360
target Thresh 31.999968243325952
target distance 14.0
model initialize at round 1360
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 15.]), 'previousTarget': array([22., 15.]), 'currentState': array([8.5, 2.5, 0. ]), 'targetState': array([22, 15], dtype=int32), 'currentDistance': 18.398369492974044}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.17421640608648478
{'scaleFactor': 20, 'currentTarget': array([22., 15.]), 'previousTarget': array([22., 15.]), 'currentState': array([21.5       , 14.02120368,  0.        ]), 'targetState': array([22, 15], dtype=int32), 'currentDistance': 1.0991097454157237}
episode index:1361
target Thresh 31.99996855931014
target distance 2.0
model initialize at round 1361
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  5.]), 'previousTarget': array([18.,  5.]), 'currentState': array([18.5       ,  3.48779446,  0.        ]), 'targetState': array([18,  5], dtype=int32), 'currentDistance': 1.592722699855865}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17408849389405712
{'scaleFactor': 20, 'currentTarget': array([48.10485696,  8.09491083]), 'previousTarget': array([47.60633208,  8.06516511]), 'currentState': array([68.        , 10.14021846,  0.        ]), 'targetState': array([18,  5], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:1362
target Thresh 31.99996887215023
target distance 15.0
model initialize at round 1362
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 5.]), 'previousTarget': array([6., 5.]), 'currentState': array([11.5, 19.5,  0. ]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 15.508062419270775}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.173960769393768
{'scaleFactor': 20, 'currentTarget': array([41.10383435,  8.59104859]), 'previousTarget': array([40.60974499,  8.64069092]), 'currentState': array([61.        , 10.62638459,  0.        ]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 20.0}
episode index:1363
target Thresh 31.99996918187751
target distance 12.0
model initialize at round 1363
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  4.]), 'previousTarget': array([27.,  4.]), 'currentState': array([15.5,  8.5,  0. ]), 'targetState': array([27,  4], dtype=int32), 'currentDistance': 12.349089035228365}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.1744209378836106
{'scaleFactor': 20, 'currentTarget': array([27.,  4.]), 'previousTarget': array([27.,  4.]), 'currentState': array([26.5       ,  3.27623994,  0.        ]), 'targetState': array([27,  4], dtype=int32), 'currentDistance': 0.8796752956904159}
episode index:1364
target Thresh 31.999969488522954
target distance 17.0
model initialize at round 1364
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 2.]), 'previousTarget': array([8., 2.]), 'currentState': array([25.5       ,  6.50000018,  0.        ]), 'targetState': array([8, 2], dtype=int32), 'currentDistance': 18.06931104412473}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17429315697673617
{'scaleFactor': 20, 'currentTarget': array([55.15014673,  7.81027975]), 'previousTarget': array([54.64860775,  7.71860615]), 'currentState': array([75.        , 10.25636335,  0.        ]), 'targetState': array([8, 2], dtype=int32), 'currentDistance': 20.0}
episode index:1365
target Thresh 31.999969792117223
target distance 20.0
model initialize at round 1365
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8.17168721, 21.04929441]), 'previousTarget': array([ 8., 21.]), 'currentState': array([24.5,  9.5,  0. ]), 'targetState': array([ 4, 24], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1741655631575731
{'scaleFactor': 20, 'currentTarget': array([54.10893794, 29.25151488]), 'previousTarget': array([53.60921796, 29.20587612]), 'currentState': array([74.        , 31.33613716,  0.        ]), 'targetState': array([ 4, 24], dtype=int32), 'currentDistance': 20.0}
episode index:1366
target Thresh 31.999970092690678
target distance 10.0
model initialize at round 1366
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([17.5, 20.5,  0. ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 9.617692030835604}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1740381560155412
{'scaleFactor': 20, 'currentTarget': array([47.15125369, 14.85303337]), 'previousTarget': array([46.65199884, 14.80071615]), 'currentState': array([67.        , 17.30808326,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 20.0}
episode index:1367
target Thresh 31.99997039027338
target distance 19.0
model initialize at round 1367
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  3.]), 'previousTarget': array([12.,  3.]), 'currentState': array([13.5, 21.5,  0. ]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 18.560711193270514}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17391093514126085
{'scaleFactor': 20, 'currentTarget': array([43.1579478 ,  6.93921697]), 'previousTarget': array([42.66322615,  6.94170173]), 'currentState': array([63.        ,  9.44779517,  0.        ]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 20.0}
episode index:1368
target Thresh 31.99997068489508
target distance 21.0
model initialize at round 1368
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9.95070559, 18.82831279]), 'previousTarget': array([ 9.90599608, 18.64100589]), 'currentState': array([21.5,  2.5,  0. ]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17378390012654849
{'scaleFactor': 20, 'currentTarget': array([51.40799939, 32.10994806]), 'previousTarget': array([50.92140486, 32.1616702 ]), 'currentState': array([71.        , 36.12909124,  0.        ]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 20.0}
episode index:1369
target Thresh 31.99997097658525
target distance 10.0
model initialize at round 1369
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 18.]), 'previousTarget': array([ 8., 18.]), 'currentState': array([16.5,  8.5,  0. ]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 12.747548783981957}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17365705056441233
{'scaleFactor': 20, 'currentTarget': array([46.50388966, 26.81051949]), 'previousTarget': array([45.9980081 , 26.64190641]), 'currentState': array([66.        , 31.27164957,  0.        ]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 20.0}
episode index:1370
target Thresh 31.99997126537305
target distance 15.0
model initialize at round 1370
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 16.]), 'previousTarget': array([10., 16.]), 'currentState': array([25.5       , 22.50001329,  0.        ]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 16.80774145428361}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17353038604904805
{'scaleFactor': 20, 'currentTarget': array([55.27409159, 23.57351533]), 'previousTarget': array([54.7669135 , 23.38794327]), 'currentState': array([75.        , 26.87329374,  0.        ]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 20.0}
episode index:1371
target Thresh 31.999971551287366
target distance 11.0
model initialize at round 1371
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  9.]), 'previousTarget': array([20.,  9.]), 'currentState': array([9.5, 4.5, 0. ]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 11.423659658795788}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.17398818503118363
{'scaleFactor': 20, 'currentTarget': array([20.,  9.]), 'previousTarget': array([20.,  9.]), 'currentState': array([20.5       ,  8.34411193,  0.        ]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 0.8247358090467026}
episode index:1372
target Thresh 31.999971834356785
target distance 4.0
model initialize at round 1372
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 13.]), 'previousTarget': array([10., 13.]), 'currentState': array([11.5, 16.5,  0. ]), 'targetState': array([10, 13], dtype=int32), 'currentDistance': 3.807886552931914}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17386146384762122
{'scaleFactor': 20, 'currentTarget': array([41.3083039 , 18.56180498]), 'previousTarget': array([40.79274334, 18.32722039]), 'currentState': array([61.        , 22.05996232,  0.        ]), 'targetState': array([10, 13], dtype=int32), 'currentDistance': 20.0}
episode index:1373
target Thresh 31.99997211460962
target distance 8.0
model initialize at round 1373
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 18.]), 'previousTarget': array([14., 18.]), 'currentState': array([ 6.5       , 15.50000092,  0.        ]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 7.905693858267032}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.17436087934310426
{'scaleFactor': 20, 'currentTarget': array([14., 18.]), 'previousTarget': array([14., 18.]), 'currentState': array([14.        , 17.13998178,  0.        ]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 0.8600182235240368}
episode index:1374
target Thresh 31.99997239207389
target distance 10.0
model initialize at round 1374
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 12.]), 'previousTarget': array([ 4., 12.]), 'currentState': array([10.5, 21.5,  0. ]), 'targetState': array([ 4, 12], dtype=int32), 'currentDistance': 11.510864433221318}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17423407143085473
{'scaleFactor': 20, 'currentTarget': array([40.5255915 , 20.54308967]), 'previousTarget': array([40.01393353, 20.32571982]), 'currentState': array([60.        , 25.09802256,  0.        ]), 'targetState': array([ 4, 12], dtype=int32), 'currentDistance': 20.0}
episode index:1375
target Thresh 31.999972666777342
target distance 21.0
model initialize at round 1375
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.07891249, 26.35291755]), 'previousTarget': array([16.10381815, 25.90990945]), 'currentState': array([18.5,  6.5,  0. ]), 'targetState': array([16, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17410744783243112
{'scaleFactor': 20, 'currentTarget': array([48.7460249 , 36.20357733]), 'previousTarget': array([48.22792345, 35.94105159]), 'currentState': array([68.        , 41.61508757,  0.        ]), 'targetState': array([16, 27], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:1376
target Thresh 31.99997293874745
target distance 12.0
model initialize at round 1376
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  7.]), 'previousTarget': array([11.,  7.]), 'currentState': array([ 7.5, 18.5,  0. ]), 'targetState': array([11,  7], dtype=int32), 'currentDistance': 12.020815280171231}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1739810081462783
{'scaleFactor': 20, 'currentTarget': array([38.1820842 , 16.78479082]), 'previousTarget': array([37.64700952, 16.43547931]), 'currentState': array([57.        , 23.55871471,  0.        ]), 'targetState': array([11,  7], dtype=int32), 'currentDistance': 20.0}
episode index:1377
target Thresh 31.999973208011415
target distance 9.0
model initialize at round 1377
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 19.]), 'previousTarget': array([ 2., 19.]), 'currentState': array([11.5, 26.5,  0. ]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 12.103718436910219}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1738547519720067
{'scaleFactor': 20, 'currentTarget': array([41.91198518, 31.48412945]), 'previousTarget': array([41.38032492, 31.08698282]), 'currentState': array([61.        , 37.45469811,  0.        ]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 20.0}
episode index:1378
target Thresh 31.999973474596157
target distance 21.0
model initialize at round 1378
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.04048723,  4.55332553]), 'previousTarget': array([24.04869701,  5.02263725]), 'currentState': array([25.5, 24.5,  0. ]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17372867891038815
{'scaleFactor': 20, 'currentTarget': array([58.00711861, 25.10751807]), 'previousTarget': array([57.48145702, 24.6683549 ]), 'currentState': array([75.        , 35.65464954,  0.        ]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 20.0}
episode index:1379
target Thresh 31.999973738528332
target distance 18.0
model initialize at round 1379
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([ 2.5, 21.5,  0. ]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 18.668154702594396}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17360278856335165
{'scaleFactor': 20, 'currentTarget': array([35.3526374 , 21.54661321]), 'previousTarget': array([34.87128619, 21.28892566]), 'currentState': array([52.        , 32.63107614,  0.        ]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 20.0}
episode index:1380
target Thresh 31.999973999834342
target distance 8.0
model initialize at round 1380
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  9.]), 'previousTarget': array([22.,  9.]), 'currentState': array([14.5       ,  2.27530852,  0.        ]), 'targetState': array([22,  9], dtype=int32), 'currentDistance': 10.073305093490337}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.17409985993632626
{'scaleFactor': 20, 'currentTarget': array([22.,  9.]), 'previousTarget': array([22.,  9.]), 'currentState': array([22.        ,  8.09674293,  0.        ]), 'targetState': array([22,  9], dtype=int32), 'currentDistance': 0.9032570719718258}
episode index:1381
target Thresh 31.999974258540313
target distance 4.0
model initialize at round 1381
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 17.]), 'previousTarget': array([18., 17.]), 'currentState': array([19.5       , 13.49999994,  0.        ]), 'targetState': array([18, 17], dtype=int32), 'currentDistance': 3.807886607717269}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17397388319252285
{'scaleFactor': 20, 'currentTarget': array([51.90053472, 37.56586016]), 'previousTarget': array([51.37361647, 37.127662  ]), 'currentState': array([69.       , 47.9393016,  0.       ]), 'targetState': array([18, 17], dtype=int32), 'currentDistance': 20.0}
episode index:1382
target Thresh 31.999974514672118
target distance 4.0
model initialize at round 1382
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 15.]), 'previousTarget': array([11., 15.]), 'currentState': array([ 7.5, 18.5,  0. ]), 'targetState': array([11, 15], dtype=int32), 'currentDistance': 4.9497474683057785}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.17452884072412697
{'scaleFactor': 20, 'currentTarget': array([11., 15.]), 'previousTarget': array([11., 15.]), 'currentState': array([10.5       , 15.50005522,  0.        ]), 'targetState': array([11, 15], dtype=int32), 'currentDistance': 0.7071458313194431}
episode index:1383
target Thresh 31.999974768255367
target distance 19.0
model initialize at round 1383
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 26.]), 'previousTarget': array([ 7., 26.]), 'currentState': array([7.5, 7.5, 0. ]), 'targetState': array([ 7, 26], dtype=int32), 'currentDistance': 18.50675552332175}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17440273607042456
{'scaleFactor': 20, 'currentTarget': array([39.55469557, 44.25170101]), 'previousTarget': array([39.11040859, 44.24257157]), 'currentState': array([57.        , 54.03236321,  0.        ]), 'targetState': array([ 7, 26], dtype=int32), 'currentDistance': 20.0}
episode index:1384
target Thresh 31.99997501931542
target distance 4.0
model initialize at round 1384
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 25.]), 'previousTarget': array([17., 25.]), 'currentState': array([18.5       , 21.49999967,  0.        ]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 3.807886854251063}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17427681351730512
{'scaleFactor': 20, 'currentTarget': array([51.2536793 , 47.36568097]), 'previousTarget': array([50.73001471, 46.9196102 ]), 'currentState': array([68.        , 58.30006449,  0.        ]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 20.0}
episode index:1385
target Thresh 31.999975267877385
target distance 10.0
model initialize at round 1385
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 27.]), 'previousTarget': array([ 4., 27.]), 'currentState': array([14.5, 19.5,  0. ]), 'targetState': array([ 4, 27], dtype=int32), 'currentDistance': 12.90348790056396}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17415107267061153
{'scaleFactor': 20, 'currentTarget': array([44.64308221, 37.56316879]), 'previousTarget': array([44.15268681, 37.51729145]), 'currentState': array([64.        , 42.59404682,  0.        ]), 'targetState': array([ 4, 27], dtype=int32), 'currentDistance': 20.0}
episode index:1386
target Thresh 31.999975513966117
target distance 17.0
model initialize at round 1386
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.64213562, 11.35786438]), 'previousTarget': array([23.14213562, 11.85786438]), 'currentState': array([ 9.5, 25.5,  0. ]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.17454820984673788
{'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([25.5       ,  9.50000387,  0.        ]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 0.7071095207334153}
episode index:1387
target Thresh 31.999975757606226
target distance 25.0
model initialize at round 1387
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.46972624,  7.60331715]), 'previousTarget': array([13.40509555,  8.06369443]), 'currentState': array([15.5, 27.5,  0. ]), 'targetState': array([13,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1744224546523238
{'scaleFactor': 20, 'currentTarget': array([49.16212043, 30.88594242]), 'previousTarget': array([48.67274096, 30.55803672]), 'currentState': array([65.        , 43.09911444,  0.        ]), 'targetState': array([13,  3], dtype=int32), 'currentDistance': 20.0}
episode index:1388
target Thresh 31.999975998822073
target distance 3.0
model initialize at round 1388
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 12.]), 'previousTarget': array([ 6., 12.]), 'currentState': array([ 6.5, 14.5,  0. ]), 'targetState': array([ 6, 12], dtype=int32), 'currentDistance': 2.549509756796313}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17429688053090386
{'scaleFactor': 20, 'currentTarget': array([36.54203014, 19.25904455]), 'previousTarget': array([36.04977143, 19.19502502]), 'currentState': array([56.        , 23.88369686,  0.        ]), 'targetState': array([ 6, 12], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:1389
target Thresh 31.99997623763778
target distance 16.0
model initialize at round 1389
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([25.5, 22.5,  0. ]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 16.507574019219252}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.174171487091673
{'scaleFactor': 20, 'currentTarget': array([55.70937108, 35.7831788 ]), 'previousTarget': array([55.19288971, 35.48606146]), 'currentState': array([75.        , 41.06253822,  0.        ]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:1390
target Thresh 31.999976474077236
target distance 11.0
model initialize at round 1390
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([15.5       , 12.50002655,  0.        ]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 12.903472466458982}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.17462257199637995
{'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([26.5       , 19.12480743,  0.        ]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 1.0079494186558038}
episode index:1391
target Thresh 31.99997670816408
target distance 17.0
model initialize at round 1391
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 11.]), 'previousTarget': array([22.86502556, 11.20859686]), 'currentState': array([12.5, 27.5,  0. ]), 'targetState': array([23, 11], dtype=int32), 'currentDistance': 19.557607215607838}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17449712474638257
{'scaleFactor': 20, 'currentTarget': array([42.61180089, 15.96555867]), 'previousTarget': array([42.14177584, 15.96965386]), 'currentState': array([62.        , 20.87450308,  0.        ]), 'targetState': array([23, 11], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:1392
target Thresh 31.99997693992172
target distance 10.0
model initialize at round 1392
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 12.]), 'previousTarget': array([13., 12.]), 'currentState': array([6.5, 2.5, 0. ]), 'targetState': array([13, 12], dtype=int32), 'currentDistance': 11.510864433221345}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17437185760729687
{'scaleFactor': 20, 'currentTarget': array([39.81635307, 31.47169515]), 'previousTarget': array([39.2916198 , 31.00617253]), 'currentState': array([56.        , 43.22284709,  0.        ]), 'targetState': array([13, 12], dtype=int32), 'currentDistance': 20.0}
episode index:1393
target Thresh 31.999977169373334
target distance 5.0
model initialize at round 1393
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'previousTarget': array([11., 19.]), 'currentState': array([13.5       , 23.50000003,  0.        ]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 5.1478150965454095}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17424677019150972
{'scaleFactor': 20, 'currentTarget': array([43.646683  , 27.50981358]), 'previousTarget': array([43.13394261, 27.28912585]), 'currentState': array([63.        , 32.55452577,  0.        ]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 20.0}
episode index:1394
target Thresh 31.999977396541865
target distance 18.0
model initialize at round 1394
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7.7396427 , 10.09956878]), 'previousTarget': array([ 7.72118773, 10.21295565]), 'currentState': array([20.5, 25.5,  0. ]), 'targetState': array([6, 8], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1741218621125194
{'scaleFactor': 20, 'currentTarget': array([51.14493729, 23.96966219]), 'previousTarget': array([50.62753251, 23.65524183]), 'currentState': array([70.        , 30.63949053,  0.        ]), 'targetState': array([6, 8], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:1395
target Thresh 31.99997762145003
target distance 5.0
model initialize at round 1395
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 8.]), 'previousTarget': array([8., 8.]), 'currentState': array([3.5       , 5.50231645, 0.        ]), 'targetState': array([8, 8], dtype=int32), 'currentDistance': 5.146690503649606}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.17465151496736978
{'scaleFactor': 20, 'currentTarget': array([8., 8.]), 'previousTarget': array([8., 8.]), 'currentState': array([8.        , 7.42976338, 0.        ]), 'targetState': array([8, 8], dtype=int32), 'currentDistance': 0.5702366232872533}
episode index:1396
target Thresh 31.999977844120323
target distance 27.0
model initialize at round 1396
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.75077408,  9.67321192]), 'previousTarget': array([20.67544468, 10.02633404]), 'currentState': array([27.5, 28.5,  0. ]), 'targetState': array([18,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17452649598743608
{'scaleFactor': 20, 'currentTarget': array([58.19120333, 16.52874279]), 'previousTarget': array([57.70107821, 16.41666586]), 'currentState': array([77.        , 23.32794626,  0.        ]), 'targetState': array([18,  2], dtype=int32), 'currentDistance': 20.0}
episode index:1397
target Thresh 31.99997806457501
target distance 8.0
model initialize at round 1397
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([ 5.5, 14.5,  0. ]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 9.924716620639556}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.17502935330289499
{'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([12.        ,  8.33998351,  0.        ]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 1.056214365412599}
episode index:1398
target Thresh 31.999978282836135
target distance 13.0
model initialize at round 1398
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 16.]), 'previousTarget': array([ 4., 16.]), 'currentState': array([17.5, 10.5,  0. ]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 14.577379737113297}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1749042429717278
{'scaleFactor': 20, 'currentTarget': array([48.36639203, 33.29925707]), 'previousTarget': array([47.8678187 , 33.11477608]), 'currentState': array([67.        , 40.56483716,  0.        ]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 20.0}
episode index:1399
target Thresh 31.999978498925525
target distance 14.0
model initialize at round 1399
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 12.]), 'previousTarget': array([14., 12.]), 'currentState': array([ 3.5, 25.5,  0. ]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 17.102631376487007}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17477931136960512
{'scaleFactor': 20, 'currentTarget': array([34.47672733, 20.33785948]), 'previousTarget': array([33.94064926, 20.0079343 ]), 'currentState': array([53.        , 27.88029739,  0.        ]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 20.0}
episode index:1400
target Thresh 31.99997871286479
target distance 18.0
model initialize at round 1400
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([17.5, 24.5,  0. ]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 19.03943276465973}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17465455811380953
{'scaleFactor': 20, 'currentTarget': array([49.1497109, 26.783719 ]), 'previousTarget': array([48.62921958, 26.41041268]), 'currentState': array([67.        , 35.80409477,  0.        ]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 20.0}
episode index:1401
target Thresh 31.999978924675325
target distance 14.0
model initialize at round 1401
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([ 2.5, 12.5,  0. ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 14.983324063771667}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.17507922900374628
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.5       ,  5.27628502,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.8796382094055102}
episode index:1402
target Thresh 31.99997913437831
target distance 13.0
model initialize at round 1402
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 16.]), 'previousTarget': array([ 3., 16.]), 'currentState': array([ 9.5, 28.5,  0. ]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 14.08900280360534}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17495443981700093
{'scaleFactor': 20, 'currentTarget': array([41.63225384, 38.06068121]), 'previousTarget': array([41.13004973, 37.76266298]), 'currentState': array([59.        , 47.97841246,  0.        ]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 20.0}
episode index:1403
target Thresh 31.999979341994713
target distance 18.0
model initialize at round 1403
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([ 2.5, 21.5,  0. ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 17.56416807025022}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17482982839262984
{'scaleFactor': 20, 'currentTarget': array([34.82937945, 22.41298907]), 'previousTarget': array([34.33936281, 22.16038563]), 'currentState': array([52.        , 32.66822138,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 20.0}
episode index:1404
target Thresh 31.9999795475453
target distance 1.0
model initialize at round 1404
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.5       , 10.46643293,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.731227611085571}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.17541713812331125
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.5       , 10.46643293,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.731227611085571}
episode index:1405
target Thresh 31.999979751050624
target distance 11.0
model initialize at round 1405
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 14.]), 'previousTarget': array([13., 14.]), 'currentState': array([24.5       , 22.50000012,  0.        ]), 'targetState': array([13, 14], dtype=int32), 'currentDistance': 14.300349716932102}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17529237486717802
{'scaleFactor': 20, 'currentTarget': array([56.77588055, 39.83511895]), 'previousTarget': array([56.28161017, 39.57630462]), 'currentState': array([74.        , 50.00024114,  0.        ]), 'targetState': array([13, 14], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:1406
target Thresh 31.99997995253104
target distance 14.0
model initialize at round 1406
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([11.5, 21.5,  0. ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 15.953056133543793}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1751677889575354
{'scaleFactor': 20, 'currentTarget': array([44.06367095, 33.79208271]), 'previousTarget': array([43.54423816, 33.36250987]), 'currentState': array([61.        , 44.42978726,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 20.0}
episode index:1407
target Thresh 31.99998015200669
target distance 7.0
model initialize at round 1407
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 29.]), 'previousTarget': array([16., 29.]), 'currentState': array([ 9.5       , 23.65212566,  0.        ]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 8.417229947716903}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.1756729147265401
{'scaleFactor': 20, 'currentTarget': array([16., 29.]), 'previousTarget': array([16., 29.]), 'currentState': array([15.5      , 28.2604287,  0.       ]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 0.8927293612348397}
episode index:1408
target Thresh 31.99998034949752
target distance 24.0
model initialize at round 1408
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.55318422, 12.41919652]), 'previousTarget': array([21.97366596, 12.67544468]), 'currentState': array([ 3.5, 18.5,  0. ]), 'targetState': array([27, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.17599523602993022
{'scaleFactor': 20, 'currentTarget': array([27., 11.]), 'previousTarget': array([27., 11.]), 'currentState': array([26.5       , 10.27276468,  0.        ]), 'targetState': array([27, 11], dtype=int32), 'currentDistance': 0.8825368035805284}
episode index:1409
target Thresh 31.999980545023288
target distance 19.0
model initialize at round 1409
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 22.]), 'previousTarget': array([27., 22.]), 'currentState': array([ 8.5, 16.5,  0. ]), 'targetState': array([27, 22], dtype=int32), 'currentDistance': 19.30025906561873}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.17636432679732006
{'scaleFactor': 20, 'currentTarget': array([27., 22.]), 'previousTarget': array([27., 22.]), 'currentState': array([26.5       , 21.27928683,  0.        ]), 'targetState': array([27, 22], dtype=int32), 'currentDistance': 0.8771701494452334}
episode index:1410
target Thresh 31.99998073860354
target distance 10.0
model initialize at round 1410
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 7.]), 'previousTarget': array([5., 7.]), 'currentState': array([10.5, 16.5,  0. ]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 10.977249200050048}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1762393343616026
{'scaleFactor': 20, 'currentTarget': array([43.07699469, 30.98264439]), 'previousTarget': array([42.55488486, 30.54495398]), 'currentState': array([60.       , 41.6415323,  0.       ]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 20.0}
episode index:1411
target Thresh 31.99998093025764
target distance 22.0
model initialize at round 1411
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.28648796,  6.7598546 ]), 'previousTarget': array([16.29773591,  7.18339664]), 'currentState': array([19.5, 26.5,  0. ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17611451896899524
{'scaleFactor': 20, 'currentTarget': array([52.23837179, 28.58919913]), 'previousTarget': array([51.74605759, 28.30457613]), 'currentState': array([69.        , 39.50010284,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:1412
target Thresh 31.999981120004744
target distance 21.0
model initialize at round 1412
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.56461275,  6.45512279]), 'previousTarget': array([24.97366596,  6.67544468]), 'currentState': array([ 6.5, 12.5,  0. ]), 'targetState': array([27,  6], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.17645389950799395
{'scaleFactor': 20, 'currentTarget': array([27.,  6.]), 'previousTarget': array([27.,  6.]), 'currentState': array([27.5       ,  5.10418949,  0.        ]), 'targetState': array([27,  6], dtype=int32), 'currentDistance': 1.0259027625613748}
episode index:1413
target Thresh 31.999981307863834
target distance 14.0
model initialize at round 1413
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 18.]), 'previousTarget': array([25., 18.]), 'currentState': array([11.5, 24.5,  0. ]), 'targetState': array([25, 18], dtype=int32), 'currentDistance': 14.98332406377159}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.17686824803333986
{'scaleFactor': 20, 'currentTarget': array([25., 18.]), 'previousTarget': array([25., 18.]), 'currentState': array([25.        , 17.40175578,  0.        ]), 'targetState': array([25, 18], dtype=int32), 'currentDistance': 0.5982442200192288}
episode index:1414
target Thresh 31.9999814938537
target distance 3.0
model initialize at round 1414
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 17.]), 'previousTarget': array([ 4., 17.]), 'currentState': array([ 7.5       , 18.50000149,  0.        ]), 'targetState': array([ 4, 17], dtype=int32), 'currentDistance': 3.8078871399177268}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17674325280504777
{'scaleFactor': 20, 'currentTarget': array([40.28048034, 40.81586978]), 'previousTarget': array([39.77711409, 40.46973909]), 'currentState': array([57.        , 51.79119037,  0.        ]), 'targetState': array([ 4, 17], dtype=int32), 'currentDistance': 20.0}
episode index:1415
target Thresh 31.99998167799293
target distance 7.0
model initialize at round 1415
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([16.5, 16.5,  0. ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 9.192388155425121}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17661843412368827
{'scaleFactor': 20, 'currentTarget': array([49.79061516, 38.75938142]), 'previousTarget': array([49.27829948, 38.32623833]), 'currentState': array([66.       , 50.4750053,  0.       ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 20.0}
episode index:1416
target Thresh 31.999981860299947
target distance 16.0
model initialize at round 1416
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  7.]), 'previousTarget': array([24.,  7.]), 'currentState': array([ 8.5, 14.5,  0. ]), 'targetState': array([24,  7], dtype=int32), 'currentDistance': 17.219175357722456}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.17700542205723394
{'scaleFactor': 20, 'currentTarget': array([24.,  7.]), 'previousTarget': array([24.,  7.]), 'currentState': array([24.5      ,  6.4443869,  0.       ]), 'targetState': array([24,  7], dtype=int32), 'currentDistance': 0.7474663320063855}
episode index:1417
target Thresh 31.99998204079298
target distance 24.0
model initialize at round 1417
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.30752452, 24.30272163]), 'previousTarget': array([21.97366596, 24.32455532]), 'currentState': array([ 3.5, 17.5,  0. ]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1768805945381527
{'scaleFactor': 20, 'currentTarget': array([36.39422281, 32.30589361]), 'previousTarget': array([35.89085782, 31.96411528]), 'currentState': array([53.        , 43.45255965,  0.        ]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 20.0}
episode index:1418
target Thresh 31.999982219490075
target distance 9.0
model initialize at round 1418
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 22.]), 'previousTarget': array([ 9., 22.]), 'currentState': array([ 3.5, 13.5,  0. ]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 10.12422836565823}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17675594295637811
{'scaleFactor': 20, 'currentTarget': array([36.54641623, 41.03573137]), 'previousTarget': array([36.03762279, 40.65323696]), 'currentState': array([53.        , 52.40584928,  0.        ]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 20.0}
episode index:1419
target Thresh 31.999982396409102
target distance 12.0
model initialize at round 1419
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 19.]), 'previousTarget': array([25., 19.]), 'currentState': array([13.5       ,  8.50000316,  0.        ]), 'targetState': array([25, 19], dtype=int32), 'currentDistance': 15.572409372349462}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17663146694021165
{'scaleFactor': 20, 'currentTarget': array([46.88071257, 35.07092771]), 'previousTarget': array([46.35435224, 34.61114061]), 'currentState': array([63.        , 46.91020863,  0.        ]), 'targetState': array([25, 19], dtype=int32), 'currentDistance': 20.0}
episode index:1420
target Thresh 31.99998257156776
target distance 7.0
model initialize at round 1420
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 11.]), 'previousTarget': array([18., 11.]), 'currentState': array([15.5,  4.5,  0. ]), 'targetState': array([18, 11], dtype=int32), 'currentDistance': 6.964194138592036}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17650716611900108
{'scaleFactor': 20, 'currentTarget': array([48.73114657, 32.97403872]), 'previousTarget': array([48.25699506, 32.73661081]), 'currentState': array([65.        , 44.60694068,  0.        ]), 'targetState': array([18, 11], dtype=int32), 'currentDistance': 20.0}
episode index:1421
target Thresh 31.99998274498356
target distance 17.0
model initialize at round 1421
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.16475581, 24.97914598]), 'previousTarget': array([14.71414506, 24.43860471]), 'currentState': array([2.5, 9.5, 0. ]), 'targetState': array([16, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1763830401231368
{'scaleFactor': 20, 'currentTarget': array([34.79994858, 37.15510634]), 'previousTarget': array([34.30006379, 36.85877484]), 'currentState': array([52.        , 47.36090035,  0.        ]), 'targetState': array([16, 26], dtype=int32), 'currentDistance': 20.0}
episode index:1422
target Thresh 31.99998291667384
target distance 6.0
model initialize at round 1422
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([11.5,  5.5,  0. ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 6.041522986797324}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17625908858404815
{'scaleFactor': 20, 'currentTarget': array([43.76445139, 31.46414603]), 'previousTarget': array([43.26629369, 31.17928261]), 'currentState': array([61.        , 41.60987736,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 20.0}
episode index:1423
target Thresh 31.999983086655774
target distance 15.0
model initialize at round 1423
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 20.]), 'previousTarget': array([25., 20.]), 'currentState': array([10.5, 22.5,  0. ]), 'targetState': array([25, 20], dtype=int32), 'currentDistance': 14.713938969562069}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1761353111341998
{'scaleFactor': 20, 'currentTarget': array([42.56951463, 29.88527056]), 'previousTarget': array([42.06836639, 29.60067453]), 'currentState': array([60.        , 39.69231801,  0.        ]), 'targetState': array([25, 20], dtype=int32), 'currentDistance': 20.0}
episode index:1424
target Thresh 31.99998325494636
target distance 15.0
model initialize at round 1424
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([16.5, 23.5,  0. ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 14.577379737113182}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17601170740708807
{'scaleFactor': 20, 'currentTarget': array([49.01908912, 30.16842111]), 'previousTarget': array([48.52730281, 29.89855471]), 'currentState': array([66.        , 40.73481433,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 20.0}
episode index:1425
target Thresh 31.99998342156243
target distance 15.0
model initialize at round 1425
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 22.]), 'previousTarget': array([21., 22.]), 'currentState': array([ 6.5, 26.5,  0. ]), 'targetState': array([21, 22], dtype=int32), 'currentDistance': 15.182226450688956}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.17641224063775043
{'scaleFactor': 20, 'currentTarget': array([21., 22.]), 'previousTarget': array([21., 22.]), 'currentState': array([21.        , 21.27239257,  0.        ]), 'targetState': array([21, 22], dtype=int32), 'currentDistance': 0.7276074290286338}
episode index:1426
target Thresh 31.99998358652064
target distance 2.0
model initialize at round 1426
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([11.5       ,  9.50000057,  0.        ]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 2.121320743954706}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.17697544159035186
{'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([12.5       ,  8.50064135,  0.        ]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 0.7075604266084609}
episode index:1427
target Thresh 31.999983749837487
target distance 18.0
model initialize at round 1427
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([ 8.5, 21.5,  0. ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 18.668154702594368}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17685150920828577
{'scaleFactor': 20, 'currentTarget': array([41.06026155, 20.35679648]), 'previousTarget': array([40.55516618, 20.02269897]), 'currentState': array([58.        , 30.98907098,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:1428
target Thresh 31.999983911529306
target distance 14.0
model initialize at round 1428
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 23.]), 'previousTarget': array([20., 23.]), 'currentState': array([ 6.5, 15.5,  0. ]), 'targetState': array([20, 23], dtype=int32), 'currentDistance': 15.443445211480542}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1767277502795186
{'scaleFactor': 20, 'currentTarget': array([38.72801782, 33.93363265]), 'previousTarget': array([38.23801393, 33.67179828]), 'currentState': array([56.        , 44.01721493,  0.        ]), 'targetState': array([20, 23], dtype=int32), 'currentDistance': 20.0}
episode index:1429
target Thresh 31.999984071612264
target distance 10.0
model initialize at round 1429
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  2.]), 'previousTarget': array([25.,  2.]), 'currentState': array([15.5,  8.5,  0. ]), 'targetState': array([25,  2], dtype=int32), 'currentDistance': 11.510864433221228}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.17719363520476641
{'scaleFactor': 20, 'currentTarget': array([25.,  2.]), 'previousTarget': array([25.,  2.]), 'currentState': array([24.        ,  1.10417807,  0.        ]), 'targetState': array([25,  2], dtype=int32), 'currentDistance': 1.3425710142987963}
episode index:1430
target Thresh 31.99998423010237
target distance 23.0
model initialize at round 1430
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.92798013,  2.75093819]), 'previousTarget': array([22.35234545,  2.95156206]), 'currentState': array([3.5, 7.5, 0. ]), 'targetState': array([26,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.17751438422702995
{'scaleFactor': 20, 'currentTarget': array([26.,  2.]), 'previousTarget': array([26.,  2.]), 'currentState': array([26.        ,  1.35721457,  0.        ]), 'targetState': array([26,  2], dtype=int32), 'currentDistance': 0.6427854299545821}
episode index:1431
target Thresh 31.999984387015473
target distance 17.0
model initialize at round 1431
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6.22533058, 14.12476387]), 'previousTarget': array([ 6.11284334, 14.14900215]), 'currentState': array([22.5,  2.5,  0. ]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1773904216682122
{'scaleFactor': 20, 'currentTarget': array([54.49064268, 42.31985643]), 'previousTarget': array([53.96649284, 41.87074502]), 'currentState': array([72.        , 51.98538312,  0.        ]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 20.0}
episode index:1432
target Thresh 31.999984542367265
target distance 16.0
model initialize at round 1432
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([7.5, 6.5, 0. ]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 15.57241150239744}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17726663212064192
{'scaleFactor': 20, 'currentTarget': array([40.90817478, 21.21713813]), 'previousTarget': array([40.40882655, 20.85006971]), 'currentState': array([57.        , 33.09371848,  0.        ]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:1433
target Thresh 31.999984696173282
target distance 20.0
model initialize at round 1433
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.83391929, 17.50829159]), 'previousTarget': array([21.2384301 , 17.79270645]), 'currentState': array([ 3.5, 25.5,  0. ]), 'targetState': array([23, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.17760952272486022
{'scaleFactor': 20, 'currentTarget': array([23., 17.]), 'previousTarget': array([23., 17.]), 'currentState': array([23.5       , 16.01316844,  0.        ]), 'targetState': array([23, 17], dtype=int32), 'currentDistance': 1.106271453695003}
episode index:1434
target Thresh 31.9999848484489
target distance 20.0
model initialize at round 1434
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7.18314326, 9.68257132]), 'previousTarget': array([6.8434743 , 9.74695771]), 'currentState': array([26.5,  4.5,  0. ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17748575302261294
{'scaleFactor': 20, 'currentTarget': array([59.06726902, 43.3565691 ]), 'previousTarget': array([58.54688942, 42.88906028]), 'currentState': array([76., 54.,  0.]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:1435
target Thresh 31.999984999209353
target distance 22.0
model initialize at round 1435
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.89196526, 23.45150202]), 'previousTarget': array([13.81071492, 22.91786413]), 'currentState': array([12.5,  3.5,  0. ]), 'targetState': array([14, 25], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17736215570156655
{'scaleFactor': 20, 'currentTarget': array([44.72442198, 42.92257949]), 'previousTarget': array([44.19147429, 42.47927459]), 'currentState': array([62., 53.,  0.]), 'targetState': array([14, 25], dtype=int32), 'currentDistance': 20.0}
episode index:1436
target Thresh 31.999985148469715
target distance 18.0
model initialize at round 1436
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 25.]), 'previousTarget': array([ 3., 25.]), 'currentState': array([21.5, 18.5,  0. ]), 'targetState': array([ 3, 25], dtype=int32), 'currentDistance': 19.608671551127625}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17723873040184382
{'scaleFactor': 20, 'currentTarget': array([54.09613153, 57.31078905]), 'previousTarget': array([53.5753432 , 56.84373461]), 'currentState': array([71., 68.,  0.]), 'targetState': array([ 3, 25], dtype=int32), 'currentDistance': 20.0}
episode index:1437
target Thresh 31.999985296244912
target distance 10.0
model initialize at round 1437
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 26.]), 'previousTarget': array([ 2., 26.]), 'currentState': array([ 6.5, 16.5,  0. ]), 'targetState': array([ 2, 26], dtype=int32), 'currentDistance': 10.511898020814243}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17711547676456854
{'scaleFactor': 20, 'currentTarget': array([39.92885524, 54.09544393]), 'previousTarget': array([39.41022865, 53.62063329]), 'currentState': array([56.        , 65.99999374,  0.        ]), 'targetState': array([ 2, 26], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:1438
target Thresh 31.99998544254972
target distance 22.0
model initialize at round 1438
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.03500945, 27.49459386]), 'previousTarget': array([10., 27.]), 'currentState': array([10.5,  7.5,  0. ]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1769923944318621
{'scaleFactor': 20, 'currentTarget': array([42.54987968, 47.22793262]), 'previousTarget': array([42.01685448, 46.78714138]), 'currentState': array([60., 57.,  0.]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 20.0}
episode index:1439
target Thresh 31.99998558739877
target distance 6.0
model initialize at round 1439
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([23.5       , 22.49999568,  0.        ]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 6.5192020737727585}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17686948304683997
{'scaleFactor': 20, 'currentTarget': array([57.94794164, 57.82656986]), 'previousTarget': array([57.44029331, 57.34095571]), 'currentState': array([73.        , 70.99606261,  0.        ]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 20.0}
episode index:1440
target Thresh 31.999985730806547
target distance 4.0
model initialize at round 1440
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 19.]), 'previousTarget': array([14., 19.]), 'currentState': array([17.5, 15.5,  0. ]), 'targetState': array([14, 19], dtype=int32), 'currentDistance': 4.9497474683058345}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17674674225360829
{'scaleFactor': 20, 'currentTarget': array([51.74026937, 50.97415929]), 'previousTarget': array([51.23847512, 50.5401538 ]), 'currentState': array([67.        , 63.90244693,  0.        ]), 'targetState': array([14, 19], dtype=int32), 'currentDistance': 20.0}
episode index:1441
target Thresh 31.999985872787395
target distance 15.0
model initialize at round 1441
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  9.]), 'previousTarget': array([17.,  9.]), 'currentState': array([ 2.5, 12.5,  0. ]), 'targetState': array([17,  9], dtype=int32), 'currentDistance': 14.916433890176284}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.17714755539157642
{'scaleFactor': 20, 'currentTarget': array([17.,  9.]), 'previousTarget': array([17.,  9.]), 'currentState': array([16.5       ,  8.20458093,  0.        ]), 'targetState': array([17,  9], dtype=int32), 'currentDistance': 0.9395166269827131}
episode index:1442
target Thresh 31.999986013355507
target distance 16.0
model initialize at round 1442
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 27.]), 'previousTarget': array([11., 27.]), 'currentState': array([27.5, 18.5,  0. ]), 'targetState': array([11, 27], dtype=int32), 'currentDistance': 18.56071119327062}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17702479201292667
{'scaleFactor': 20, 'currentTarget': array([60.01117498, 57.44633597]), 'previousTarget': array([59.48916467, 56.98184991]), 'currentState': array([77., 68.,  0.]), 'targetState': array([11, 27], dtype=int32), 'currentDistance': 20.0}
episode index:1443
target Thresh 31.999986152524947
target distance 9.0
model initialize at round 1443
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([17.5, 15.5,  0. ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 8.63133825081597}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17690219866665732
{'scaleFactor': 20, 'currentTarget': array([50.27476322, 29.47369292]), 'previousTarget': array([49.76472826, 29.09508302]), 'currentState': array([67.        , 40.44029926,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 20.0}
episode index:1444
target Thresh 31.999986290309625
target distance 10.0
model initialize at round 1444
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 20.]), 'previousTarget': array([21., 20.]), 'currentState': array([11.5, 21.5,  0. ]), 'targetState': array([21, 20], dtype=int32), 'currentDistance': 9.617692030835581}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.1773572931737739
{'scaleFactor': 20, 'currentTarget': array([21., 20.]), 'previousTarget': array([21., 20.]), 'currentState': array([20.5       , 19.45767246,  0.        ]), 'targetState': array([21, 20], dtype=int32), 'currentDistance': 0.7376443293322287}
episode index:1445
target Thresh 31.999986426723325
target distance 23.0
model initialize at round 1445
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.94433475,  5.50493644]), 'previousTarget': array([17.86874449,  6.01887683]), 'currentState': array([17.5, 25.5,  0. ]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17723463944405482
{'scaleFactor': 20, 'currentTarget': array([49.89252109, 22.31398231]), 'previousTarget': array([49.41471271, 22.11654958]), 'currentState': array([67.        , 32.67420264,  0.        ]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 20.0}
episode index:1446
target Thresh 31.999986561779682
target distance 17.0
model initialize at round 1446
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 23.]), 'previousTarget': array([22., 23.]), 'currentState': array([25.5,  6.5,  0. ]), 'targetState': array([22, 23], dtype=int32), 'currentDistance': 16.86712779343302}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1771121552426422
{'scaleFactor': 20, 'currentTarget': array([58.02206182, 45.42883095]), 'previousTarget': array([57.49469707, 44.97290771]), 'currentState': array([75., 56.,  0.]), 'targetState': array([22, 23], dtype=int32), 'currentDistance': 20.0}
episode index:1447
target Thresh 31.999986695492208
target distance 5.0
model initialize at round 1447
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 18.]), 'previousTarget': array([ 2., 18.]), 'currentState': array([ 3.5, 22.5,  0. ]), 'targetState': array([ 2, 18], dtype=int32), 'currentDistance': 4.743416490252494}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17698984021830338
{'scaleFactor': 20, 'currentTarget': array([36.00956334, 39.11995501]), 'previousTarget': array([35.49419322, 38.73228313]), 'currentState': array([53.        , 49.67102425,  0.        ]), 'targetState': array([ 2, 18], dtype=int32), 'currentDistance': 20.0}
episode index:1448
target Thresh 31.999986827874274
target distance 25.0
model initialize at round 1448
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.58940683,  8.68476261]), 'previousTarget': array([15.22705473,  9.25566398]), 'currentState': array([ 6.5, 26.5,  0. ]), 'targetState': array([19,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1768676940207752
{'scaleFactor': 20, 'currentTarget': array([40.10036048, 18.10121542]), 'previousTarget': array([39.60553968, 17.73754727]), 'currentState': array([56.        , 30.23387644,  0.        ]), 'targetState': array([19,  2], dtype=int32), 'currentDistance': 20.0}
episode index:1449
target Thresh 31.999986958939115
target distance 14.0
model initialize at round 1449
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 6.]), 'previousTarget': array([8., 6.]), 'currentState': array([22.5       , 17.50002542,  0.        ]), 'targetState': array([8, 6], dtype=int32), 'currentDistance': 18.506771320044287}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17674571630076089
{'scaleFactor': 20, 'currentTarget': array([55.4310896 , 38.06573445]), 'previousTarget': array([54.92861076, 37.71090238]), 'currentState': array([72.        , 49.26712757,  0.        ]), 'targetState': array([8, 6], dtype=int32), 'currentDistance': 20.0}
episode index:1450
target Thresh 31.99998708869984
target distance 21.0
model initialize at round 1450
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.64621213, 11.09077974]), 'previousTarget': array([24.05721038, 11.40132839]), 'currentState': array([ 6.5, 19.5,  0. ]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.17708960557213202
{'scaleFactor': 20, 'currentTarget': array([27., 10.]), 'previousTarget': array([27., 10.]), 'currentState': array([26.        ,  9.50183658,  0.        ]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 1.1172138515140078}
episode index:1451
target Thresh 31.999987217169423
target distance 7.0
model initialize at round 1451
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([27.5, 15.5,  0. ]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 7.9056941504210005}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1769676430338592
{'scaleFactor': 20, 'currentTarget': array([60.31232938, 44.62986731]), 'previousTarget': array([59.79831128, 44.21761024]), 'currentState': array([77.        , 55.65355314,  0.        ]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 20.0}
episode index:1452
target Thresh 31.99998734436071
target distance 20.0
model initialize at round 1452
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.98199646, 20.109422  ]), 'previousTarget': array([16.638375  , 19.52431817]), 'currentState': array([7.5, 2.5, 0. ]), 'targetState': array([18, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17684584837244569
{'scaleFactor': 20, 'currentTarget': array([39.61614257, 34.29715944]), 'previousTarget': array([39.14078776, 34.09641721]), 'currentState': array([57.        , 44.18662357,  0.        ]), 'targetState': array([18, 22], dtype=int32), 'currentDistance': 20.0}
episode index:1453
target Thresh 31.999987470286424
target distance 12.0
model initialize at round 1453
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 25.]), 'previousTarget': array([24., 25.]), 'currentState': array([12.5       , 13.49999636,  0.        ]), 'targetState': array([24, 25], dtype=int32), 'currentDistance': 16.263458538248447}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.17727554901974046
{'scaleFactor': 20, 'currentTarget': array([24., 25.]), 'previousTarget': array([24., 25.]), 'currentState': array([23.5       , 24.44004256,  0.        ]), 'targetState': array([24, 25], dtype=int32), 'currentDistance': 0.7507012320734981}
episode index:1454
target Thresh 31.99998759495916
target distance 10.0
model initialize at round 1454
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 26.]), 'previousTarget': array([ 3., 26.]), 'currentState': array([12.5, 16.5,  0. ]), 'targetState': array([ 3, 26], dtype=int32), 'currentDistance': 13.435028842544401}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1771537101544348
{'scaleFactor': 20, 'currentTarget': array([45.35112965, 54.19071813]), 'previousTarget': array([44.86061812, 53.91592993]), 'currentState': array([62.        , 65.27291629,  0.        ]), 'targetState': array([ 3, 26], dtype=int32), 'currentDistance': 20.0}
episode index:1455
target Thresh 31.999987718391377
target distance 18.0
model initialize at round 1455
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([ 4.5, 18.5,  0. ]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 19.03943276465974}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.17752498544457482
{'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([21.       , 10.6849059,  0.       ]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 1.0484675919935162}
episode index:1456
target Thresh 31.999987840595423
target distance 24.0
model initialize at round 1456
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4.73855458, 6.85689616]), 'previousTarget': array([4.71202025, 7.27212152]), 'currentState': array([ 8.5, 26.5,  0. ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1774031426268366
{'scaleFactor': 20, 'currentTarget': array([40.6997343 , 24.28759613]), 'previousTarget': array([40.20071782, 24.00288437]), 'currentState': array([58.       , 34.3225753,  0.       ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:1457
target Thresh 31.99998796158352
target distance 6.0
model initialize at round 1457
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([21.5, 16.5,  0. ]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 7.106335201775933}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1772814669460226
{'scaleFactor': 20, 'currentTarget': array([54.29862252, 46.57242356]), 'previousTarget': array([53.78325364, 46.15920844]), 'currentState': array([71.        , 57.57533182,  0.        ]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 20.0}
episode index:1458
target Thresh 31.999988081367768
target distance 1.0
model initialize at round 1458
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  4.]), 'previousTarget': array([13.,  4.]), 'currentState': array([13.5       ,  3.49806875,  0.        ]), 'targetState': array([13,  4], dtype=int32), 'currentDistance': 0.7084736973398393}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.17784535901802667
{'scaleFactor': 20, 'currentTarget': array([13.,  4.]), 'previousTarget': array([13.,  4.]), 'currentState': array([13.5       ,  3.49806875,  0.        ]), 'targetState': array([13,  4], dtype=int32), 'currentDistance': 0.7084736973398393}
episode index:1459
target Thresh 31.99998819996014
target distance 5.0
model initialize at round 1459
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 18.]), 'previousTarget': array([ 2., 18.]), 'currentState': array([ 3.5, 22.5,  0. ]), 'targetState': array([ 2, 18], dtype=int32), 'currentDistance': 4.743416490252494}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1777235471282883
{'scaleFactor': 20, 'currentTarget': array([37.03093922, 44.41435929]), 'previousTarget': array([36.53943271, 44.08196651]), 'currentState': array([53.        , 56.45550117,  0.        ]), 'targetState': array([ 2, 18], dtype=int32), 'currentDistance': 20.0}
episode index:1460
target Thresh 31.999988317372498
target distance 17.0
model initialize at round 1460
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 15.]), 'previousTarget': array([19., 15.]), 'currentState': array([ 2.5, 21.5,  0. ]), 'targetState': array([19, 15], dtype=int32), 'currentDistance': 17.734147850968174}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.178103136329196
{'scaleFactor': 20, 'currentTarget': array([19., 15.]), 'previousTarget': array([19., 15.]), 'currentState': array([18.        , 15.33079144,  0.        ]), 'targetState': array([19, 15], dtype=int32), 'currentDistance': 1.0532914977090115}
episode index:1461
target Thresh 31.999988433616583
target distance 21.0
model initialize at round 1461
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.71128986, 16.48524731]), 'previousTarget': array([21.18513205, 16.01582747]), 'currentState': array([6.5       , 3.49999949, 0.        ]), 'targetState': array([27, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.17844350973051684
{'scaleFactor': 20, 'currentTarget': array([27., 21.]), 'previousTarget': array([27., 21.]), 'currentState': array([26.        , 21.41889015,  0.        ]), 'targetState': array([27, 21], dtype=int32), 'currentDistance': 1.084190461322332}
episode index:1462
target Thresh 31.99998854870402
target distance 14.0
model initialize at round 1462
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([13.5, 17.5,  0. ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 16.50757401921916}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17832153877376325
{'scaleFactor': 20, 'currentTarget': array([46.34984139, 32.18275625]), 'previousTarget': array([45.87357262, 31.99518796]), 'currentState': array([63.       , 43.2630188,  0.       ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 20.0}
episode index:1463
target Thresh 31.99998866264632
target distance 8.0
model initialize at round 1463
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 14.]), 'previousTarget': array([15., 14.]), 'currentState': array([14.5,  6.5,  0. ]), 'targetState': array([15, 14], dtype=int32), 'currentDistance': 7.516648189186453}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17819973444399975
{'scaleFactor': 20, 'currentTarget': array([47.43513362, 35.94479699]), 'previousTarget': array([46.95143519, 35.68528134]), 'currentState': array([64.        , 47.15216964,  0.        ]), 'targetState': array([15, 14], dtype=int32), 'currentDistance': 20.0}
episode index:1464
target Thresh 31.999988775454874
target distance 7.0
model initialize at round 1464
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 12.]), 'previousTarget': array([11., 12.]), 'currentState': array([10.5, 18.5,  0. ]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 6.519202405202561}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17807809640001065
{'scaleFactor': 20, 'currentTarget': array([43.52798288, 34.40028137]), 'previousTarget': array([43.02406284, 34.03694329]), 'currentState': array([60.        , 45.74367821,  0.        ]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 20.0}
episode index:1465
target Thresh 31.999988887140965
target distance 10.0
model initialize at round 1465
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 15.]), 'previousTarget': array([11., 15.]), 'currentState': array([ 7.5, 24.5,  0. ]), 'targetState': array([11, 15], dtype=int32), 'currentDistance': 10.124228365658219}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17795662430151135
{'scaleFactor': 20, 'currentTarget': array([40.61815401, 35.74345235]), 'previousTarget': array([40.1460241, 35.5183149]), 'currentState': array([57.        , 47.21668737,  0.        ]), 'targetState': array([11, 15], dtype=int32), 'currentDistance': 20.0}
episode index:1466
target Thresh 31.99998899771576
target distance 11.0
model initialize at round 1466
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 29.]), 'previousTarget': array([ 7., 29.]), 'currentState': array([18.5, 18.5,  0. ]), 'targetState': array([ 7, 29], dtype=int32), 'currentDistance': 15.572411502397442}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17783531780914494
{'scaleFactor': 20, 'currentTarget': array([51.14955702, 57.22676597]), 'previousTarget': array([50.62677025, 56.76249016]), 'currentState': array([68., 68.,  0.]), 'targetState': array([ 7, 29], dtype=int32), 'currentDistance': 20.0}
episode index:1467
target Thresh 31.999989107190316
target distance 17.0
model initialize at round 1467
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5.35786438, 14.35786438]), 'previousTarget': array([ 5.29270602, 14.43600015]), 'currentState': array([19.5, 28.5,  0. ]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1777141765844793
{'scaleFactor': 20, 'currentTarget': array([53.56479089, 53.66425703]), 'previousTarget': array([53.06243433, 53.2347532 ]), 'currentState': array([69.        , 66.38252419,  0.        ]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 20.0}
episode index:1468
target Thresh 31.999989215575585
target distance 14.0
model initialize at round 1468
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([ 2.5       , 16.77148627,  0.        ]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 13.615732209314158}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.17812269066399916
{'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([15.        , 15.04206979,  0.        ]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 1.0008845423288002}
episode index:1469
target Thresh 31.9999893228824
target distance 19.0
model initialize at round 1469
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3.52020262, 11.74061153]), 'previousTarget': array([ 2.92524322, 11.43827311]), 'currentState': array([21.5, 20.5,  0. ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17800151876558826
{'scaleFactor': 20, 'currentTarget': array([55.54489318, 54.97919591]), 'previousTarget': array([55.04262529, 54.5507884 ]), 'currentState': array([71.        , 67.67327616,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 20.0}
episode index:1470
target Thresh 31.999989429121495
target distance 21.0
model initialize at round 1470
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.04929441, 11.17168721]), 'previousTarget': array([14.62476387, 11.72533058]), 'currentState': array([ 3.5, 27.5,  0. ]), 'targetState': array([18,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17788051161482987
{'scaleFactor': 20, 'currentTarget': array([37.69484481, 23.56702427]), 'previousTarget': array([37.18842396, 23.12469092]), 'currentState': array([53.        , 36.44150385,  0.        ]), 'targetState': array([18,  7], dtype=int32), 'currentDistance': 20.0}
episode index:1471
target Thresh 31.999989534303495
target distance 7.0
model initialize at round 1471
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 29.]), 'previousTarget': array([14., 29.]), 'currentState': array([14.5, 22.5,  0. ]), 'targetState': array([14, 29], dtype=int32), 'currentDistance': 6.519202405202574}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17775966887596112
{'scaleFactor': 20, 'currentTarget': array([48.82077561, 58.87379856]), 'previousTarget': array([48.31182282, 58.39620954]), 'currentState': array([64.        , 71.89651513,  0.        ]), 'targetState': array([14, 29], dtype=int32), 'currentDistance': 20.0}
episode index:1472
target Thresh 31.999989638438915
target distance 8.0
model initialize at round 1472
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 20.]), 'previousTarget': array([12., 20.]), 'currentState': array([19.5       , 27.50000042,  0.        ]), 'targetState': array([12, 20], dtype=int32), 'currentDistance': 10.606602012826151}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17763899021413085
{'scaleFactor': 20, 'currentTarget': array([53.83338095, 55.96044092]), 'previousTarget': array([53.33607118, 55.5477811 ]), 'currentState': array([69.        , 68.99783584,  0.        ]), 'targetState': array([12, 20], dtype=int32), 'currentDistance': 20.0}
episode index:1473
target Thresh 31.99998974153817
target distance 11.0
model initialize at round 1473
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 29.]), 'previousTarget': array([ 2., 29.]), 'currentState': array([13.5, 19.5,  0. ]), 'targetState': array([ 2, 29], dtype=int32), 'currentDistance': 14.916433890176311}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17751847529539672
{'scaleFactor': 20, 'currentTarget': array([46.27511265, 58.03286076]), 'previousTarget': array([45.75329692, 57.56620212]), 'currentState': array([63., 69.,  0.]), 'targetState': array([ 2, 29], dtype=int32), 'currentDistance': 20.0}
episode index:1474
target Thresh 31.99998984361157
target distance 21.0
model initialize at round 1474
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.4852476 , 10.28870996]), 'previousTarget': array([18.01582747, 10.81486795]), 'currentState': array([ 5.5, 25.5,  0. ]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17739812378672187
{'scaleFactor': 20, 'currentTarget': array([39.84725247, 19.51326599]), 'previousTarget': array([39.34800334, 19.08482015]), 'currentState': array([55.        , 32.56678055,  0.        ]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 20.0}
episode index:1475
target Thresh 31.999989944669323
target distance 3.0
model initialize at round 1475
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  9.]), 'previousTarget': array([22.,  9.]), 'currentState': array([20.5       , 11.50020924,  0.        ]), 'targetState': array([22,  9], dtype=int32), 'currentDistance': 2.9156553730353223}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.17792874566085012
{'scaleFactor': 20, 'currentTarget': array([22.,  9.]), 'previousTarget': array([22.,  9.]), 'currentState': array([22.5       ,  9.52291688,  0.        ]), 'targetState': array([22,  9], dtype=int32), 'currentDistance': 0.7234929624868056}
episode index:1476
target Thresh 31.999990044721535
target distance 21.0
model initialize at round 1476
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7.35577145, 8.71220997]), 'previousTarget': array([6.76952105, 8.50557744]), 'currentState': array([26.5, 14.5,  0. ]), 'targetState': array([5, 8], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17780827934692944
{'scaleFactor': 20, 'currentTarget': array([60.29668489, 51.61428667]), 'previousTarget': array([59.78524243, 51.12880787]), 'currentState': array([76., 64.,  0.]), 'targetState': array([5, 8], dtype=int32), 'currentDistance': 20.0}
episode index:1477
target Thresh 31.999990143778213
target distance 24.0
model initialize at round 1477
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.00214654,  9.47927974]), 'previousTarget': array([14.5999788 , 10.04003392]), 'currentState': array([ 4.5, 26.5,  0. ]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17768797604561215
{'scaleFactor': 20, 'currentTarget': array([39.19574406, 21.34481452]), 'previousTarget': array([38.68657383, 20.8578137 ]), 'currentState': array([54.        , 34.79226804,  0.        ]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 20.0}
episode index:1478
target Thresh 31.999990241849257
target distance 10.0
model initialize at round 1478
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 21.]), 'previousTarget': array([20., 21.]), 'currentState': array([10.5       , 17.49999911,  0.        ]), 'targetState': array([20, 21], dtype=int32), 'currentDistance': 10.124228674742913}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.17813777673346767
{'scaleFactor': 20, 'currentTarget': array([20., 21.]), 'previousTarget': array([20., 21.]), 'currentState': array([19.        , 21.46589774,  0.        ]), 'targetState': array([20, 21], dtype=int32), 'currentDistance': 1.1032047421672793}
episode index:1479
target Thresh 31.999990338944478
target distance 7.0
model initialize at round 1479
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 13.]), 'previousTarget': array([ 3., 13.]), 'currentState': array([10.5,  7.5,  0. ]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 9.300537618869205}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17801741337080992
{'scaleFactor': 20, 'currentTarget': array([44.16819358, 44.77895645]), 'previousTarget': array([43.6527351 , 44.29900844]), 'currentState': array([60., 57.,  0.]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 20.0}
episode index:1480
target Thresh 31.999990435073588
target distance 14.0
model initialize at round 1480
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 21.]), 'previousTarget': array([19., 21.]), 'currentState': array([ 5.5, 14.5,  0. ]), 'targetState': array([19, 21], dtype=int32), 'currentDistance': 14.983324063771667}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.17842241265914774
{'scaleFactor': 20, 'currentTarget': array([19., 21.]), 'previousTarget': array([19., 21.]), 'currentState': array([18.        , 21.62846345,  0.        ]), 'targetState': array([19, 21], dtype=int32), 'currentDistance': 1.1810869165016864}
episode index:1481
target Thresh 31.999990530246194
target distance 5.0
model initialize at round 1481
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([19.5, 11.5,  0. ]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 5.147815070493393}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17830201966814968
{'scaleFactor': 20, 'currentTarget': array([54.55153316, 47.15595738]), 'previousTarget': array([54.04820565, 46.65943478]), 'currentState': array([69.        , 60.98497781,  0.        ]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 20.0}
episode index:1482
target Thresh 31.99999062447182
target distance 4.0
model initialize at round 1482
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 25.]), 'previousTarget': array([19., 25.]), 'currentState': array([23.5       , 29.49924782,  0.        ]), 'targetState': array([19, 25], dtype=int32), 'currentDistance': 6.363429180751867}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17818178904126622
{'scaleFactor': 20, 'currentTarget': array([58.85773504, 64.85700598]), 'previousTarget': array([58.35773383, 64.35700719]), 'currentState': array([73.        , 78.99901226,  0.        ]), 'targetState': array([19, 25], dtype=int32), 'currentDistance': 20.0}
episode index:1483
target Thresh 31.999990717759886
target distance 22.0
model initialize at round 1483
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7.07201987, 18.24906181]), 'previousTarget': array([ 6.70472358, 18.26234812]), 'currentState': array([26.5, 13.5,  0. ]), 'targetState': array([ 4, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17806172045026808
{'scaleFactor': 20, 'currentTarget': array([58.93436333, 52.57099981]), 'previousTarget': array([58.41372202, 52.10485186]), 'currentState': array([76., 63.,  0.]), 'targetState': array([ 4, 19], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:1484
target Thresh 31.999990810119716
target distance 6.0
model initialize at round 1484
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 15.]), 'previousTarget': array([17., 15.]), 'currentState': array([20.5,  9.5,  0. ]), 'targetState': array([17, 15], dtype=int32), 'currentDistance': 6.519202405202623}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17794181356780997
{'scaleFactor': 20, 'currentTarget': array([54.61180231, 46.22489249]), 'previousTarget': array([54.09956878, 45.7396427 ]), 'currentState': array([70., 59.,  0.]), 'targetState': array([17, 15], dtype=int32), 'currentDistance': 20.0}
episode index:1485
target Thresh 31.999990901560555
target distance 17.0
model initialize at round 1485
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([27.5,  3.5,  0. ]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 19.55760721560799}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17782206806742787
{'scaleFactor': 20, 'currentTarget': array([59.47568183, 43.361625  ]), 'previousTarget': array([58.94772845, 42.91262478]), 'currentState': array([77., 53.,  0.]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 20.0}
episode index:1486
target Thresh 31.99999099209154
target distance 3.0
model initialize at round 1486
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 12.]), 'previousTarget': array([27., 12.]), 'currentState': array([26.5       , 14.50285324,  0.        ]), 'targetState': array([27, 12], dtype=int32), 'currentDistance': 2.5523076545981116}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17770248362353586
{'scaleFactor': 20, 'currentTarget': array([61.73585605, 46.1390442 ]), 'previousTarget': array([61.23459289, 45.64032955]), 'currentState': array([76.        , 60.15810969,  0.        ]), 'targetState': array([27, 12], dtype=int32), 'currentDistance': 20.0}
episode index:1487
target Thresh 31.999991081721724
target distance 14.0
model initialize at round 1487
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 15.]), 'previousTarget': array([27., 15.]), 'currentState': array([13.5       , 22.50002119,  0.        ]), 'targetState': array([27, 15], dtype=int32), 'currentDistance': 15.4434555019988}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.1781057893196216
{'scaleFactor': 20, 'currentTarget': array([27., 15.]), 'previousTarget': array([27., 15.]), 'currentState': array([26.        , 15.15814027,  0.        ]), 'targetState': array([27, 15], dtype=int32), 'currentDistance': 1.0124269572792592}
episode index:1488
target Thresh 31.999991170460078
target distance 4.0
model initialize at round 1488
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 23.]), 'previousTarget': array([18., 23.]), 'currentState': array([22.5, 21.5,  0. ]), 'targetState': array([18, 23], dtype=int32), 'currentDistance': 4.7434164902526215}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17798617495473268
{'scaleFactor': 20, 'currentTarget': array([57.05181363, 57.71272322]), 'previousTarget': array([56.54410443, 57.22140113]), 'currentState': array([72., 71.,  0.]), 'targetState': array([18, 23], dtype=int32), 'currentDistance': 20.0}
episode index:1489
target Thresh 31.999991258315468
target distance 11.0
model initialize at round 1489
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 17.]), 'previousTarget': array([ 3., 17.]), 'currentState': array([11.5, 28.5,  0. ]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 14.300349646075205}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17786672114603821
{'scaleFactor': 20, 'currentTarget': array([47.21876743, 63.50594505]), 'previousTarget': array([46.72186234, 63.00300298]), 'currentState': array([61., 78.,  0.]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 20.0}
episode index:1490
target Thresh 31.99999134529668
target distance 21.0
model initialize at round 1490
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8.20985627, 21.55240915]), 'previousTarget': array([ 7.99469707, 21.52709229]), 'currentState': array([25.5, 11.5,  0. ]), 'targetState': array([ 4, 24], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17774742757048756
{'scaleFactor': 20, 'currentTarget': array([57.26385096, 51.7572181 ]), 'previousTarget': array([56.73919339, 51.30468878]), 'currentState': array([75., 61.,  0.]), 'targetState': array([ 4, 24], dtype=int32), 'currentDistance': 20.0}
episode index:1491
target Thresh 31.99999143141242
target distance 14.0
model initialize at round 1491
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([19.5, 24.5,  0. ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 16.80773631397172}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17762829390589607
{'scaleFactor': 20, 'currentTarget': array([55.56961151, 59.18026097]), 'previousTarget': array([55.07562142, 58.6748167 ]), 'currentState': array([69., 74.,  0.]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 20.0}
episode index:1492
target Thresh 31.99999151667129
target distance 17.0
model initialize at round 1492
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 20.]), 'previousTarget': array([ 8., 20.]), 'currentState': array([18.5,  3.5,  0. ]), 'targetState': array([ 8, 20], dtype=int32), 'currentDistance': 19.55760721560799}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17750931983094237
{'scaleFactor': 20, 'currentTarget': array([50.47568183, 43.361625  ]), 'previousTarget': array([49.94772845, 42.91262478]), 'currentState': array([68., 53.,  0.]), 'targetState': array([ 8, 20], dtype=int32), 'currentDistance': 20.0}
episode index:1493
target Thresh 31.99999160108182
target distance 10.0
model initialize at round 1493
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 22.]), 'previousTarget': array([11., 22.]), 'currentState': array([21.5, 13.5,  0. ]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 13.509256086106307}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1773905050251653
{'scaleFactor': 20, 'currentTarget': array([54.48710717, 51.7161899 ]), 'previousTarget': array([53.96664   , 51.24620033]), 'currentState': array([71., 63.,  0.]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 20.0}
episode index:1494
target Thresh 31.999991684652453
target distance 5.0
model initialize at round 1494
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 2.]), 'previousTarget': array([9., 2.]), 'currentState': array([4.5       , 2.49713498, 0.        ]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 4.5273770763941155}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.1778953042511732
{'scaleFactor': 20, 'currentTarget': array([9., 2.]), 'previousTarget': array([9., 2.]), 'currentState': array([8.        , 2.37759876, 0.        ]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 1.0689157242040421}
episode index:1495
target Thresh 31.999991767391542
target distance 8.0
model initialize at round 1495
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 17.]), 'previousTarget': array([ 6., 17.]), 'currentState': array([12.5, 25.5,  0. ]), 'targetState': array([ 6, 17], dtype=int32), 'currentDistance': 10.70046727951646}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17777639027774328
{'scaleFactor': 20, 'currentTarget': array([48.10810973, 60.61197079]), 'previousTarget': array([47.61034302, 60.10981484]), 'currentState': array([62., 75.,  0.]), 'targetState': array([ 6, 17], dtype=int32), 'currentDistance': 20.0}
episode index:1496
target Thresh 31.999991849307367
target distance 1.0
model initialize at round 1496
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  2.]), 'previousTarget': array([17.,  2.]), 'currentState': array([18.5       ,  2.53167695,  0.        ]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 1.5914397183399147}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1776576351740173
{'scaleFactor': 20, 'currentTarget': array([53.71860065, 37.99892346]), 'previousTarget': array([53.21721515, 37.5003368 ]), 'currentState': array([68.        , 52.00041026,  0.        ]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 20.0}
episode index:1497
target Thresh 31.99999193040811
target distance 22.0
model initialize at round 1497
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.09921439, 21.85317778]), 'previousTarget': array([21.51093912, 21.57265691]), 'currentState': array([ 3.5       , 14.49999997,  0.        ]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.1779811494636101
{'scaleFactor': 20, 'currentTarget': array([25., 23.]), 'previousTarget': array([25., 23.]), 'currentState': array([24.        , 23.06242477,  0.        ]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 1.001946531552564}
episode index:1498
target Thresh 31.99999201070189
target distance 10.0
model initialize at round 1498
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([2.5, 4.5, 0. ]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 10.97724920005008}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17786241620846427
{'scaleFactor': 20, 'currentTarget': array([37.20119853, 40.54654412]), 'previousTarget': array([36.69350007, 40.0550173 ]), 'currentState': array([52., 54.,  0.]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 20.0}
episode index:1499
target Thresh 31.999992090196738
target distance 16.0
model initialize at round 1499
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([ 5.5       , 13.80175559,  0.        ]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 15.751185174410685}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.17824195599387965
{'scaleFactor': 20, 'currentTarget': array([21., 11.]), 'previousTarget': array([21., 11.]), 'currentState': array([20.        , 10.97371781,  0.        ]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 1.0003453171652343}
episode index:1500
target Thresh 31.999992168900594
target distance 16.0
model initialize at round 1500
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.64213562,  5.35786438]), 'previousTarget': array([20.14213562,  5.85786438]), 'currentState': array([ 6.5, 19.5,  0. ]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.17861601223464876
{'scaleFactor': 20, 'currentTarget': array([22.,  4.]), 'previousTarget': array([22.,  4.]), 'currentState': array([21.5       ,  4.50040287,  0.        ]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 0.707391709092999}
episode index:1501
target Thresh 31.999992246821336
target distance 15.0
model initialize at round 1501
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 16.]), 'previousTarget': array([ 2., 16.]), 'currentState': array([17.5,  8.5,  0. ]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 17.2191753577226}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1784970934515365
{'scaleFactor': 20, 'currentTarget': array([50.20166663, 47.14569228]), 'previousTarget': array([49.68066803, 46.67825928]), 'currentState': array([67., 58.,  0.]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 20.0}
episode index:1502
target Thresh 31.999992323966755
target distance 9.0
model initialize at round 1502
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 12.]), 'previousTarget': array([19., 12.]), 'currentState': array([25.5,  3.5,  0. ]), 'targetState': array([19, 12], dtype=int32), 'currentDistance': 10.700467279516399}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1783783329103179
{'scaleFactor': 20, 'currentTarget': array([58.8627436 , 41.18522299]), 'previousTarget': array([58.34418104, 40.7106186 ]), 'currentState': array([75., 53.,  0.]), 'targetState': array([19, 12], dtype=int32), 'currentDistance': 20.0}
episode index:1503
target Thresh 31.99999240034456
target distance 7.0
model initialize at round 1503
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([9.5       , 6.49894544, 0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 6.519283370682955}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.17885503498568253
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.        ,  7.02399166,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.0002877585435161}
episode index:1504
target Thresh 31.999992475962394
target distance 9.0
model initialize at round 1504
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 16.]), 'previousTarget': array([14., 16.]), 'currentState': array([ 6.5       , 24.50000021,  0.        ]), 'targetState': array([14, 16], dtype=int32), 'currentDistance': 11.335784205183069}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.17930194710269862
{'scaleFactor': 20, 'currentTarget': array([14., 16.]), 'previousTarget': array([14., 16.]), 'currentState': array([14.5      , 16.5014849,  0.       ]), 'targetState': array([14, 16], dtype=int32), 'currentDistance': 0.7081575429554814}
episode index:1505
target Thresh 31.99999255082782
target distance 13.0
model initialize at round 1505
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 10.]), 'previousTarget': array([26., 10.]), 'currentState': array([16.5       , 22.50000006,  0.        ]), 'targetState': array([26, 10], dtype=int32), 'currentDistance': 15.700318515562472}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17918288870488805
{'scaleFactor': 20, 'currentTarget': array([51.94003122, 36.24234402]), 'previousTarget': array([51.44106819, 35.74131906]), 'currentState': array([66.        , 50.46617183,  0.        ]), 'targetState': array([26, 10], dtype=int32), 'currentDistance': 20.0}
episode index:1506
target Thresh 31.999992624948323
target distance 12.0
model initialize at round 1506
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 12.]), 'previousTarget': array([11., 12.]), 'currentState': array([23.5, 11.5,  0. ]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 12.509996003196882}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17906398831424114
{'scaleFactor': 20, 'currentTarget': array([57.30882508, 48.59891014]), 'previousTarget': array([56.79581054, 48.11539531]), 'currentState': array([73., 61.,  0.]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 20.0}
episode index:1507
target Thresh 31.99999269833131
target distance 7.0
model initialize at round 1507
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 13.]), 'previousTarget': array([25., 13.]), 'currentState': array([22.5,  6.5,  0. ]), 'targetState': array([25, 13], dtype=int32), 'currentDistance': 6.964194138592034}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17894524561642003
{'scaleFactor': 20, 'currentTarget': array([57.24389176, 42.49973076]), 'previousTarget': array([56.73716533, 42.00708659]), 'currentState': array([72., 56.,  0.]), 'targetState': array([25, 13], dtype=int32), 'currentDistance': 20.0}
episode index:1508
target Thresh 31.99999277098413
target distance 24.0
model initialize at round 1508
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6.64571456, 10.37600089]), 'previousTarget': array([6.4, 9.8]), 'currentState': array([12.5, 29.5,  0. ]), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17882666029792008
{'scaleFactor': 20, 'currentTarget': array([49.79543981, 63.15548326]), 'previousTarget': array([49.31099836, 62.64351114]), 'currentState': array([62., 79.,  0.]), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:1509
target Thresh 31.99999284291404
target distance 15.0
model initialize at round 1509
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 27.]), 'previousTarget': array([18., 27.]), 'currentState': array([ 3.5, 20.5,  0. ]), 'targetState': array([18, 27], dtype=int32), 'currentDistance': 15.890248582070674}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.17921309477080033
{'scaleFactor': 20, 'currentTarget': array([18., 27.]), 'previousTarget': array([18., 27.]), 'currentState': array([17.        , 27.42098542,  0.        ]), 'targetState': array([18, 27], dtype=int32), 'currentDistance': 1.085001714342807}
episode index:1510
target Thresh 31.999992914128235
target distance 11.0
model initialize at round 1510
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 12.]), 'previousTarget': array([13., 12.]), 'currentState': array([24.5,  4.5,  0. ]), 'targetState': array([13, 12], dtype=int32), 'currentDistance': 13.729530217746051}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17909448914884746
{'scaleFactor': 20, 'currentTarget': array([57.5270449 , 42.65796534]), 'previousTarget': array([57.00723955, 42.18678415]), 'currentState': array([74., 54.,  0.]), 'targetState': array([13, 12], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:1511
target Thresh 31.999992984633835
target distance 15.0
model initialize at round 1511
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 20.]), 'previousTarget': array([ 3., 20.]), 'currentState': array([4.5, 5.5, 0. ]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 14.577379737113256}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1789760404126379
{'scaleFactor': 20, 'currentTarget': array([37.50973295, 43.68315006]), 'previousTarget': array([36.98583786, 43.21804765]), 'currentState': array([54., 55.,  0.]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 20.0}
episode index:1512
target Thresh 31.999993054437894
target distance 10.0
model initialize at round 1512
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 28.]), 'previousTarget': array([12., 28.]), 'currentState': array([ 2.5, 23.5,  0. ]), 'targetState': array([12, 28], dtype=int32), 'currentDistance': 10.51189802081429}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.17941488188849464
{'scaleFactor': 20, 'currentTarget': array([12., 28.]), 'previousTarget': array([12., 28.]), 'currentState': array([11.        , 28.41671251,  0.        ]), 'targetState': array([12, 28], dtype=int32), 'currentDistance': 1.0833509652896407}
episode index:1513
target Thresh 31.999993123547394
target distance 5.0
model initialize at round 1513
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  7.]), 'previousTarget': array([22.,  7.]), 'currentState': array([27.5,  5.5,  0. ]), 'targetState': array([22,  7], dtype=int32), 'currentDistance': 5.70087712549577}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17929637800349565
{'scaleFactor': 20, 'currentTarget': array([61.93150685, 41.84931507]), 'previousTarget': array([61.42278873, 41.35931127]), 'currentState': array([77., 55.,  0.]), 'targetState': array([22,  7], dtype=int32), 'currentDistance': 20.0}
episode index:1514
target Thresh 31.99999319196924
target distance 13.0
model initialize at round 1514
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 10.]), 'previousTarget': array([27., 10.]), 'currentState': array([20.5       , 22.50004968,  0.        ]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 14.08904688097294}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17917803055926892
{'scaleFactor': 20, 'currentTarget': array([56.64441815, 43.04401844]), 'previousTarget': array([56.15337119, 42.53599136]), 'currentState': array([70.        , 57.93120869,  0.        ]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 20.0}
episode index:1515
target Thresh 31.999993259710276
target distance 9.0
model initialize at round 1515
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 22.]), 'previousTarget': array([ 8., 22.]), 'currentState': array([17.5, 19.5,  0. ]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 9.82344135219431}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1790598392462351
{'scaleFactor': 20, 'currentTarget': array([51.35679005, 56.53845987]), 'previousTarget': array([50.8435349 , 56.05511749]), 'currentState': array([67., 69.,  0.]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 20.0}
episode index:1516
target Thresh 31.999993326777282
target distance 5.0
model initialize at round 1516
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 14.]), 'previousTarget': array([18., 14.]), 'currentState': array([18.5,  9.5,  0. ]), 'targetState': array([18, 14], dtype=int32), 'currentDistance': 4.5276925690686385}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17894180375563112
{'scaleFactor': 20, 'currentTarget': array([53.13411708, 45.62070537]), 'previousTarget': array([52.6266491 , 45.12900778]), 'currentState': array([68., 59.,  0.]), 'targetState': array([18, 14], dtype=int32), 'currentDistance': 20.0}
episode index:1517
target Thresh 31.999993393176958
target distance 15.0
model initialize at round 1517
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 14.]), 'previousTarget': array([17., 14.]), 'currentState': array([2.5, 5.5, 0. ]), 'targetState': array([17, 14], dtype=int32), 'currentDistance': 16.807736313971617}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.17932612583111956
{'scaleFactor': 20, 'currentTarget': array([17., 14.]), 'previousTarget': array([17., 14.]), 'currentState': array([16.        , 14.71558708,  0.        ]), 'targetState': array([17, 14], dtype=int32), 'currentDistance': 1.2296604687395196}
episode index:1518
target Thresh 31.999993458915945
target distance 15.0
model initialize at round 1518
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 25.]), 'previousTarget': array([14., 25.]), 'currentState': array([10.5, 10.5,  0. ]), 'targetState': array([14, 25], dtype=int32), 'currentDistance': 14.9164338901762}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17920807044874226
{'scaleFactor': 20, 'currentTarget': array([44.08342612, 47.88956335]), 'previousTarget': array([43.56326724, 47.41610373]), 'currentState': array([60., 60.,  0.]), 'targetState': array([14, 25], dtype=int32), 'currentDistance': 20.0}
episode index:1519
target Thresh 31.999993524000818
target distance 9.0
model initialize at round 1519
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 29.]), 'previousTarget': array([14., 29.]), 'currentState': array([22.5, 20.5,  0. ]), 'targetState': array([14, 29], dtype=int32), 'currentDistance': 12.02081528017131}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17909017040239442
{'scaleFactor': 20, 'currentTarget': array([55.66845003, 58.45528364]), 'previousTarget': array([55.14883001, 57.98308896]), 'currentState': array([72., 70.,  0.]), 'targetState': array([14, 29], dtype=int32), 'currentDistance': 20.0}
episode index:1520
target Thresh 31.999993588438087
target distance 15.0
model initialize at round 1520
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 19.]), 'previousTarget': array([17., 19.]), 'currentState': array([19.5,  4.5,  0. ]), 'targetState': array([17, 19], dtype=int32), 'currentDistance': 14.713938969562179}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1789724253856933
{'scaleFactor': 20, 'currentTarget': array([52.40824505, 42.83247263]), 'previousTarget': array([51.88385449, 42.36879573]), 'currentState': array([69., 54.,  0.]), 'targetState': array([17, 19], dtype=int32), 'currentDistance': 20.0}
episode index:1521
target Thresh 31.999993652234195
target distance 17.0
model initialize at round 1521
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 6.]), 'previousTarget': array([5., 6.]), 'currentState': array([22.5, 10.5,  0. ]), 'targetState': array([5, 6], dtype=int32), 'currentDistance': 18.06931099959275}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17885483509306144
{'scaleFactor': 20, 'currentTarget': array([56.4280763 , 47.44949434]), 'previousTarget': array([55.91697501, 46.96328065]), 'currentState': array([72., 60.,  0.]), 'targetState': array([5, 6], dtype=int32), 'currentDistance': 20.0}
episode index:1522
target Thresh 31.99999371539552
target distance 25.0
model initialize at round 1522
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9.28324255, 9.94957912]), 'previousTarget': array([9.0776773 , 9.38838649]), 'currentState': array([13.5, 29.5,  0. ]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1787373992197239
{'scaleFactor': 20, 'currentTarget': array([51.17272673, 62.87190008]), 'previousTarget': array([50.69152111, 62.35813436]), 'currentState': array([63., 79.,  0.]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 20.0}
episode index:1523
target Thresh 31.999993777928378
target distance 14.0
model initialize at round 1523
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 24.]), 'previousTarget': array([23., 24.]), 'currentState': array([21.5, 10.5,  0. ]), 'targetState': array([23, 24], dtype=int32), 'currentDistance': 13.583077707206037}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1786201174617057
{'scaleFactor': 20, 'currentTarget': array([55., 48.]), 'previousTarget': array([54.47978669, 47.52699847]), 'currentState': array([71., 60.,  0.]), 'targetState': array([23, 24], dtype=int32), 'currentDistance': 20.0}
episode index:1524
target Thresh 31.999993839839025
target distance 9.0
model initialize at round 1524
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 16.]), 'previousTarget': array([ 8., 16.]), 'currentState': array([17.5, 12.5,  0. ]), 'targetState': array([ 8, 16], dtype=int32), 'currentDistance': 10.124228365658343}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17850298951582919
{'scaleFactor': 20, 'currentTarget': array([51.22736135, 49.70268851]), 'previousTarget': array([50.71295565, 49.22118773]), 'currentState': array([67., 62.,  0.]), 'targetState': array([ 8, 16], dtype=int32), 'currentDistance': 20.0}
episode index:1525
target Thresh 31.999993901133653
target distance 7.0
model initialize at round 1525
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  4.]), 'previousTarget': array([12.,  4.]), 'currentState': array([5.5       , 7.47326243, 0.        ]), 'targetState': array([12,  4], dtype=int32), 'currentDistance': 7.369772852832841}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.17897273739573932
{'scaleFactor': 20, 'currentTarget': array([12.,  4.]), 'previousTarget': array([12.,  4.]), 'currentState': array([11.       ,  4.1443125,  0.       ]), 'targetState': array([12,  4], dtype=int32), 'currentDistance': 1.0103593904796777}
episode index:1526
target Thresh 31.999993961818387
target distance 21.0
model initialize at round 1526
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.69849333, 14.0252491 ]), 'previousTarget': array([12.20689655, 13.51724138]), 'currentState': array([26.5, 28.5,  0. ]), 'targetState': array([6, 7], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1788555319357552
{'scaleFactor': 20, 'currentTarget': array([61.95851634, 63.75792372]), 'previousTarget': array([61.45923781, 63.25721245]), 'currentState': array([76., 78.,  0.]), 'targetState': array([6, 7], dtype=int32), 'currentDistance': 20.0}
episode index:1527
target Thresh 31.999994021899298
target distance 1.0
model initialize at round 1527
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 23.]), 'previousTarget': array([22., 23.]), 'currentState': array([22.5       , 22.49841422,  0.        ]), 'targetState': array([22, 23], dtype=int32), 'currentDistance': 0.7082289857647814}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.17939293014783914
{'scaleFactor': 20, 'currentTarget': array([22., 23.]), 'previousTarget': array([22., 23.]), 'currentState': array([22.5       , 22.49841422,  0.        ]), 'targetState': array([22, 23], dtype=int32), 'currentDistance': 0.7082289857647814}
episode index:1528
target Thresh 31.999994081382393
target distance 15.0
model initialize at round 1528
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 20.]), 'previousTarget': array([ 6., 20.]), 'currentState': array([21.5, 22.5,  0. ]), 'targetState': array([ 6, 20], dtype=int32), 'currentDistance': 15.700318468107671}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17927560318240562
{'scaleFactor': 20, 'currentTarget': array([55.38262381, 59.50609905]), 'previousTarget': array([54.87081065, 59.02087982]), 'currentState': array([71., 72.,  0.]), 'targetState': array([ 6, 20], dtype=int32), 'currentDistance': 20.0}
episode index:1529
target Thresh 31.99999414027362
target distance 19.0
model initialize at round 1529
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 12.]), 'previousTarget': array([25., 12.]), 'currentState': array([ 6.5       , 17.50000095,  0.        ]), 'targetState': array([25, 12], dtype=int32), 'currentDistance': 19.300259337387633}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.17961819932084822
{'scaleFactor': 20, 'currentTarget': array([25., 12.]), 'previousTarget': array([25., 12.]), 'currentState': array([24.        , 12.64717498,  0.        ]), 'targetState': array([25, 12], dtype=int32), 'currentDistance': 1.1911487985092406}
episode index:1530
target Thresh 31.99999419857887
target distance 7.0
model initialize at round 1530
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 21.]), 'previousTarget': array([13., 21.]), 'currentState': array([18.5, 28.5,  0. ]), 'targetState': array([13, 21], dtype=int32), 'currentDistance': 9.300537618869251}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17950087848523696
{'scaleFactor': 20, 'currentTarget': array([54.11261652, 63.60762076]), 'previousTarget': array([53.61493133, 63.10538753]), 'currentState': array([68., 78.,  0.]), 'targetState': array([13, 21], dtype=int32), 'currentDistance': 20.0}
episode index:1531
target Thresh 31.999994256303978
target distance 13.0
model initialize at round 1531
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  3.]), 'previousTarget': array([10.,  3.]), 'currentState': array([23.5,  5.5,  0. ]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 13.72953021774606}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1793837108099855
{'scaleFactor': 20, 'currentTarget': array([57.5755395 , 42.26869927]), 'previousTarget': array([57.06496027, 41.78152727]), 'currentState': array([73., 55.,  0.]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 20.0}
episode index:1532
target Thresh 31.999994313454707
target distance 8.0
model initialize at round 1532
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'previousTarget': array([16., 14.]), 'currentState': array([24.5, 14.5,  0. ]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 8.514693182963287}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1792666959953671
{'scaleFactor': 20, 'currentTarget': array([58.85180077, 50.94120756]), 'previousTarget': array([58.3428138, 50.4516397]), 'currentState': array([74., 64.,  0.]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 20.0}
episode index:1533
target Thresh 31.999994370036777
target distance 13.0
model initialize at round 1533
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([8.5       , 7.49998847, 0.        ]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 12.747551045892104}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.17966718334063977
{'scaleFactor': 20, 'currentTarget': array([21., 10.]), 'previousTarget': array([21., 10.]), 'currentState': array([20.        , 10.13076974,  0.        ]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 1.0085141179362795}
episode index:1534
target Thresh 31.99999442605585
target distance 4.0
model initialize at round 1534
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 29.]), 'previousTarget': array([23., 29.]), 'currentState': array([25.5, 25.5,  0. ]), 'targetState': array([23, 29], dtype=int32), 'currentDistance': 4.301162633521296}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17955013631566216
{'scaleFactor': 20, 'currentTarget': array([60.02006875, 61.74852236]), 'previousTarget': array([59.5117392 , 61.25794434]), 'currentState': array([75., 75.,  0.]), 'targetState': array([23, 29], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:1535
target Thresh 31.99999448151752
target distance 12.0
model initialize at round 1535
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  6.]), 'previousTarget': array([20.,  6.]), 'currentState': array([13.5       , 17.50006562,  0.        ]), 'targetState': array([20,  6], dtype=int32), 'currentDistance': 13.209901944099679}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.179433241695665
{'scaleFactor': 20, 'currentTarget': array([50.17524517, 42.10944837]), 'previousTarget': array([49.68983426, 41.59726862]), 'currentState': array([63.        , 57.45629377,  0.        ]), 'targetState': array([20,  6], dtype=int32), 'currentDistance': 20.0}
episode index:1536
target Thresh 31.999994536427337
target distance 23.0
model initialize at round 1536
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.2092914 ,  9.43213769]), 'previousTarget': array([22.62485559,  9.71201303]), 'currentState': array([ 4.5       , 16.50054735,  0.        ]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.17973881709363032
{'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'previousTarget': array([27.,  8.]), 'currentState': array([26.        ,  8.83126477,  0.        ]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 1.300384985120639}
episode index:1537
target Thresh 31.999994590790795
target distance 1.0
model initialize at round 1537
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 13.]), 'previousTarget': array([16., 13.]), 'currentState': array([16.5       , 13.50161767,  0.        ]), 'targetState': array([16, 13], dtype=int32), 'currentDistance': 0.7082515703587547}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.18027214686144982
{'scaleFactor': 20, 'currentTarget': array([16., 13.]), 'previousTarget': array([16., 13.]), 'currentState': array([16.5       , 13.50161767,  0.        ]), 'targetState': array([16, 13], dtype=int32), 'currentDistance': 0.7082515703587547}
episode index:1538
target Thresh 31.999994644613324
target distance 4.0
model initialize at round 1538
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  5.]), 'previousTarget': array([23.,  5.]), 'currentState': array([24.5       ,  9.43783441,  0.        ]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 4.684482283783784}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18015501096355413
{'scaleFactor': 20, 'currentTarget': array([60.25915713, 44.40538216]), 'previousTarget': array([59.76306874, 43.9016846 ]), 'currentState': array([74.        , 58.93773356,  0.        ]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 20.0}
episode index:1539
target Thresh 31.999994697900313
target distance 17.0
model initialize at round 1539
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4.09956878, 11.7396427 ]), 'previousTarget': array([ 3.56139529, 11.28585494]), 'currentState': array([19.5, 24.5,  0. ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18003802719020118
{'scaleFactor': 20, 'currentTarget': array([54.53779049, 60.18535211]), 'previousTarget': array([54.03535842, 59.68789864]), 'currentState': array([69., 74.,  0.]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 20.0}
episode index:1540
target Thresh 31.999994750657088
target distance 17.0
model initialize at round 1540
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 24.]), 'previousTarget': array([ 6., 24.]), 'currentState': array([23.5, 26.5,  0. ]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 17.67766952966378}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17992119524523673
{'scaleFactor': 20, 'currentTarget': array([57.20026626, 63.73752008]), 'previousTarget': array([56.68738251, 63.25413833]), 'currentState': array([73., 76.,  0.]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 20.0}
episode index:1541
target Thresh 31.99999480288892
target distance 15.0
model initialize at round 1541
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([21.64636501,  8.37889464]), 'currentState': array([ 8.5, 22.5,  0. ]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 19.811612756158837}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.18029395665377004
{'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([22.5      ,  8.5085139,  0.       ]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 0.7131524269832965}
episode index:1542
target Thresh 31.99999485460104
target distance 5.0
model initialize at round 1542
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 10.]), 'previousTarget': array([17., 10.]), 'currentState': array([22.5,  6.5,  0. ]), 'targetState': array([17, 10], dtype=int32), 'currentDistance': 6.51920240520272}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1801771102787514
{'scaleFactor': 20, 'currentTarget': array([56.6584626 , 43.16889599]), 'previousTarget': array([56.1471259 , 42.68246291]), 'currentState': array([72., 56.,  0.]), 'targetState': array([17, 10], dtype=int32), 'currentDistance': 20.0}
episode index:1543
target Thresh 31.999994905798616
target distance 10.0
model initialize at round 1543
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 22.]), 'previousTarget': array([20., 22.]), 'currentState': array([10.5, 14.5,  0. ]), 'targetState': array([20, 22], dtype=int32), 'currentDistance': 12.103718436910091}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.18060636292324958
{'scaleFactor': 20, 'currentTarget': array([20., 22.]), 'previousTarget': array([20., 22.]), 'currentState': array([19.        , 22.89213589,  0.        ]), 'targetState': array([20, 22], dtype=int32), 'currentDistance': 1.3401143396216393}
episode index:1544
target Thresh 31.999994956486766
target distance 11.0
model initialize at round 1544
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'previousTarget': array([26., 11.]), 'currentState': array([15.5       ,  8.49999517,  0.        ]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 10.793517690720801}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.18102420257432553
{'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'previousTarget': array([26., 11.]), 'currentState': array([25.        , 11.92229971,  0.        ]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 1.3603811085154}
episode index:1545
target Thresh 31.999995006670563
target distance 17.0
model initialize at round 1545
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([ 4.5       , 25.52797878,  0.        ]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 16.506750256610978}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.1813760500085969
{'scaleFactor': 20, 'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([20.5      , 26.7580619,  0.       ]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 0.9081067367572313}
episode index:1546
target Thresh 31.99999505635502
target distance 21.0
model initialize at round 1546
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8.61599637, 8.91255687]), 'previousTarget': array([8.02633404, 8.67544468]), 'currentState': array([27.5, 15.5,  0. ]), 'targetState': array([6, 8], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18125880627879173
{'scaleFactor': 20, 'currentTarget': array([61.40407712, 52.47932952]), 'previousTarget': array([60.89342817, 51.99260556]), 'currentState': array([77., 65.,  0.]), 'targetState': array([6, 8], dtype=int32), 'currentDistance': 20.0}
episode index:1547
target Thresh 31.999995105545107
target distance 6.0
model initialize at round 1547
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 25.]), 'previousTarget': array([21., 25.]), 'currentState': array([27.5, 22.5,  0. ]), 'targetState': array([21, 25], dtype=int32), 'currentDistance': 6.964194138592107}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18114171402667364
{'scaleFactor': 20, 'currentTarget': array([61.68051492, 59.14257502]), 'previousTarget': array([61.16959038, 58.65560275]), 'currentState': array([77., 72.,  0.]), 'targetState': array([21, 25], dtype=int32), 'currentDistance': 20.0}
episode index:1548
target Thresh 31.99999515424575
target distance 7.0
model initialize at round 1548
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 15.]), 'previousTarget': array([10., 15.]), 'currentState': array([10.5,  8.5,  0. ]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 6.519202405202575}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18102477295887076
{'scaleFactor': 20, 'currentTarget': array([44.83629607, 44.95921462]), 'previousTarget': array([44.32569267, 44.47155431]), 'currentState': array([60., 58.,  0.]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 20.0}
episode index:1549
target Thresh 31.999995202461808
target distance 11.0
model initialize at round 1549
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([14.5, 25.5,  0. ]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 13.209844813623015}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18090798278276826
{'scaleFactor': 20, 'currentTarget': array([50.47457415, 60.26694684]), 'previousTarget': array([49.97999282, 59.76197415]), 'currentState': array([64., 75.,  0.]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 20.0}
episode index:1550
target Thresh 31.99999525019811
target distance 15.0
model initialize at round 1550
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([ 7.5, 22.5,  0. ]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 14.916433890176284}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.18128286010808375
{'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([21.        , 19.87981601,  0.        ]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 1.3319445242842285}
episode index:1551
target Thresh 31.99999529745943
target distance 4.0
model initialize at round 1551
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 21.]), 'previousTarget': array([24., 21.]), 'currentState': array([26.5, 17.5,  0. ]), 'targetState': array([24, 21], dtype=int32), 'currentDistance': 4.301162633521292}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18116605414151926
{'scaleFactor': 20, 'currentTarget': array([61.02006875, 53.74852236]), 'previousTarget': array([60.5117392 , 53.25794434]), 'currentState': array([76., 67.,  0.]), 'targetState': array([24, 21], dtype=int32), 'currentDistance': 20.0}
episode index:1552
target Thresh 31.99999534425049
target distance 17.0
model initialize at round 1552
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.35786438, 22.64213562]), 'previousTarget': array([13.29270602, 22.56399985]), 'currentState': array([27.5,  8.5,  0. ]), 'targetState': array([11, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18104939860118344
{'scaleFactor': 20, 'currentTarget': array([59.11145618, 49.05572809]), 'previousTarget': array([58.58418744, 48.61047468]), 'currentState': array([77., 58.,  0.]), 'targetState': array([11, 25], dtype=int32), 'currentDistance': 20.0}
episode index:1553
target Thresh 31.99999539057597
target distance 24.0
model initialize at round 1553
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7.60486776, 14.05545404]), 'previousTarget': array([ 7.27341645, 14.02246883]), 'currentState': array([26.5,  7.5,  0. ]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18093289319667819
{'scaleFactor': 20, 'currentTarget': array([58.50571462, 47.30722026]), 'previousTarget': array([57.98323487, 46.84790493]), 'currentState': array([76., 57.,  0.]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 20.0}
episode index:1554
target Thresh 31.999995436440507
target distance 14.0
model initialize at round 1554
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 12.]), 'previousTarget': array([18., 12.]), 'currentState': array([ 4.5, 25.5,  0. ]), 'targetState': array([18, 12], dtype=int32), 'currentDistance': 19.091883092036728}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.18131174223372545
{'scaleFactor': 20, 'currentTarget': array([18., 12.]), 'previousTarget': array([18., 12.]), 'currentState': array([17.5       , 12.61470222,  0.        ]), 'targetState': array([18, 12], dtype=int32), 'currentDistance': 0.792375431904741}
episode index:1555
target Thresh 31.999995481848682
target distance 11.0
model initialize at round 1555
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  5.]), 'previousTarget': array([13.,  5.]), 'currentState': array([ 2.5, 15.5,  0. ]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 14.849242404917446}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.18172086510992308
{'scaleFactor': 20, 'currentTarget': array([13.,  5.]), 'previousTarget': array([13.,  5.]), 'currentState': array([12.5       ,  5.59600028,  0.        ]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 0.7779565145139559}
episode index:1556
target Thresh 31.99999552680504
target distance 6.0
model initialize at round 1556
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  5.]), 'previousTarget': array([27.,  5.]), 'currentState': array([21.5       ,  6.51620576,  0.        ]), 'targetState': array([27,  5], dtype=int32), 'currentDistance': 5.705162565620365}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.18219086920907124
{'scaleFactor': 20, 'currentTarget': array([27.,  5.]), 'previousTarget': array([27.,  5.]), 'currentState': array([26.        ,  5.80240656,  0.        ]), 'targetState': array([27,  5], dtype=int32), 'currentDistance': 1.2821295926216927}
episode index:1557
target Thresh 31.999995571314074
target distance 9.0
model initialize at round 1557
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([23.5, 11.5,  0. ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 10.977249200050185}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18207393026862898
{'scaleFactor': 20, 'currentTarget': array([58.37065177, 47.36247199]), 'previousTarget': array([57.86642313, 46.86700958]), 'currentState': array([73., 61.,  0.]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:1558
target Thresh 31.999995615380236
target distance 20.0
model initialize at round 1558
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3.55332553, 17.04048723]), 'previousTarget': array([ 3.02495322, 17.00124766]), 'currentState': array([23.5, 18.5,  0. ]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18195714134607052
{'scaleFactor': 20, 'currentTarget': array([56.83527008, 56.22283963]), 'previousTarget': array([56.32024623, 55.74348827]), 'currentState': array([73., 68.,  0.]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 20.0}
episode index:1559
target Thresh 31.99999565900793
target distance 10.0
model initialize at round 1559
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 19.]), 'previousTarget': array([ 5., 19.]), 'currentState': array([15.5, 25.5,  0. ]), 'targetState': array([ 5, 19], dtype=int32), 'currentDistance': 12.349089035228578}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18184050215289996
{'scaleFactor': 20, 'currentTarget': array([50.37889464, 61.35363499]), 'previousTarget': array([49.87480833, 60.85801449]), 'currentState': array([65., 75.,  0.]), 'targetState': array([ 5, 19], dtype=int32), 'currentDistance': 20.0}
episode index:1560
target Thresh 31.999995702201524
target distance 21.0
model initialize at round 1560
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.6175174,  5.6991327]), 'previousTarget': array([15.28013989,  6.28336929]), 'currentState': array([ 6.5, 23.5,  0. ]), 'targetState': array([17,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18172401240136063
{'scaleFactor': 20, 'currentTarget': array([44.56452979, 42.5510761 ]), 'previousTarget': array([44.0947635, 42.0300465]), 'currentState': array([56.        , 58.95930639,  0.        ]), 'targetState': array([17,  3], dtype=int32), 'currentDistance': 20.0}
episode index:1561
target Thresh 31.999995744965332
target distance 20.0
model initialize at round 1561
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4.18314326, 18.31742868]), 'previousTarget': array([ 3.59715  , 18.1492875]), 'currentState': array([23.5, 23.5,  0. ]), 'targetState': array([ 3, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18160767180443274
{'scaleFactor': 20, 'currentTarget': array([57.27363322, 60.64356896]), 'previousTarget': array([56.76185365, 60.15857588]), 'currentState': array([73., 73.,  0.]), 'targetState': array([ 3, 18], dtype=int32), 'currentDistance': 20.0}
episode index:1562
target Thresh 31.999995787303636
target distance 7.0
model initialize at round 1562
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  5.]), 'previousTarget': array([23.,  5.]), 'currentState': array([16.5       ,  5.51824313,  0.        ]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 6.520626959576789}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.18206431325194028
{'scaleFactor': 20, 'currentTarget': array([23.,  5.]), 'previousTarget': array([23.,  5.]), 'currentState': array([22.       ,  5.5134567,  0.       ]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 1.1241164446977436}
episode index:1563
target Thresh 31.999995829220666
target distance 23.0
model initialize at round 1563
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.77400308,  7.54061834]), 'previousTarget': array([19.73259233,  7.07518824]), 'currentState': array([18.5, 27.5,  0. ]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1819479038444902
{'scaleFactor': 20, 'currentTarget': array([57.01187473, 60.28889282]), 'previousTarget': array([56.53947485, 59.77077741]), 'currentState': array([68., 77.,  0.]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 20.0}
episode index:1564
target Thresh 31.99999587072061
target distance 14.0
model initialize at round 1564
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'previousTarget': array([22., 18.]), 'currentState': array([22.5,  4.5,  0. ]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 13.509256086106303}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.181831643203056
{'scaleFactor': 20, 'currentTarget': array([55.76931317, 42.31390548]), 'previousTarget': array([55.24754484, 41.84419882]), 'currentState': array([72., 54.,  0.]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 20.0}
episode index:1565
target Thresh 31.999995911807627
target distance 9.0
model initialize at round 1565
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([11.5, 19.5,  0. ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 12.747548783982074}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18171553104264537
{'scaleFactor': 20, 'currentTarget': array([46.97924944, 54.73751236]), 'previousTarget': array([46.48030027, 54.23647941]), 'currentState': array([61., 69.,  0.]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 20.0}
episode index:1566
target Thresh 31.999995952485822
target distance 8.0
model initialize at round 1566
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([ 3.5, 16.5,  0. ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 9.924716620639556}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.18215956773183256
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([10.        , 10.09243566,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 1.0042630884709023}
episode index:1567
target Thresh 31.99999599275926
target distance 11.0
model initialize at round 1567
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  5.]), 'previousTarget': array([18.,  5.]), 'currentState': array([23.5, 16.5,  0. ]), 'targetState': array([18,  5], dtype=int32), 'currentDistance': 12.74754878398207}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18204339453812604
{'scaleFactor': 20, 'currentTarget': array([59.60725756, 51.14623112]), 'previousTarget': array([59.11392179, 50.6402251 ]), 'currentState': array([73., 66.,  0.]), 'targetState': array([18,  5], dtype=int32), 'currentDistance': 20.0}
episode index:1568
target Thresh 31.999996032631973
target distance 11.0
model initialize at round 1568
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 4.]), 'previousTarget': array([8., 4.]), 'currentState': array([ 9.5       , 15.49926165,  0.        ]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 11.596681354457052}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18192736943007112
{'scaleFactor': 20, 'currentTarget': array([46.17146757, 49.65534564]), 'previousTarget': array([45.68371431, 49.1451147 ]), 'currentState': array([59.        , 64.99903345,  0.        ]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 20.0}
episode index:1569
target Thresh 31.999996072107944
target distance 4.0
model initialize at round 1569
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 15.]), 'previousTarget': array([26., 15.]), 'currentState': array([22.5       , 14.53069243,  0.        ]), 'targetState': array([26, 15], dtype=int32), 'currentDistance': 3.531324057178405}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.18241721827113477
{'scaleFactor': 20, 'currentTarget': array([26., 15.]), 'previousTarget': array([26., 15.]), 'currentState': array([25.        , 14.78171672,  0.        ]), 'targetState': array([26, 15], dtype=int32), 'currentDistance': 1.023546573937438}
episode index:1570
target Thresh 31.999996111191123
target distance 16.0
model initialize at round 1570
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 28.]), 'previousTarget': array([25., 28.]), 'currentState': array([ 9.5, 28.5,  0. ]), 'targetState': array([25, 28], dtype=int32), 'currentDistance': 15.50806241927074}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.1827767057797665
{'scaleFactor': 20, 'currentTarget': array([25., 28.]), 'previousTarget': array([25., 28.]), 'currentState': array([24.        , 27.78297947,  0.        ]), 'targetState': array([25, 28], dtype=int32), 'currentDistance': 1.0232780213264732}
episode index:1571
target Thresh 31.99999614988542
target distance 21.0
model initialize at round 1571
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.91214849, 25.57592352]), 'previousTarget': array([19.52709229, 25.00530293]), 'currentState': array([9.5, 8.5, 0. ]), 'targetState': array([22, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1826604356106954
{'scaleFactor': 20, 'currentTarget': array([43.2588817 , 45.66236674]), 'previousTarget': array([42.73623925, 45.1913101 ]), 'currentState': array([59., 58.,  0.]), 'targetState': array([22, 29], dtype=int32), 'currentDistance': 20.0}
episode index:1572
target Thresh 31.999996188194697
target distance 16.0
model initialize at round 1572
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 28.]), 'previousTarget': array([20., 28.]), 'currentState': array([27.5, 12.5,  0. ]), 'targetState': array([20, 28], dtype=int32), 'currentDistance': 17.219175357722527}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1825443132740071
{'scaleFactor': 20, 'currentTarget': array([59.8236099 , 51.75443398]), 'previousTarget': array([59.2966428 , 51.29977936]), 'currentState': array([77., 62.,  0.]), 'targetState': array([20, 28], dtype=int32), 'currentDistance': 20.0}
episode index:1573
target Thresh 31.999996226122793
target distance 12.0
model initialize at round 1573
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 23.]), 'previousTarget': array([19., 23.]), 'currentState': array([ 7.5      , 20.5431532,  0.       ]), 'targetState': array([19, 23], dtype=int32), 'currentDistance': 11.759510883344957}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.18294277804843356
{'scaleFactor': 20, 'currentTarget': array([19., 23.]), 'previousTarget': array([19., 23.]), 'currentState': array([18.        , 22.69923418,  0.        ]), 'targetState': array([19, 23], dtype=int32), 'currentDistance': 1.044250966303901}
episode index:1574
target Thresh 31.9999962636735
target distance 22.0
model initialize at round 1574
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.28648796, 23.2401454 ]), 'previousTarget': array([24.29773591, 22.81660336]), 'currentState': array([27.5,  3.5,  0. ]), 'targetState': array([24, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1828266239036409
{'scaleFactor': 20, 'currentTarget': array([59.31613232, 43.65757934]), 'previousTarget': array([58.78336929, 43.21986011]), 'currentState': array([77., 53.,  0.]), 'targetState': array([24, 25], dtype=int32), 'currentDistance': 20.0}
episode index:1575
target Thresh 31.99999630085057
target distance 9.0
model initialize at round 1575
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 20.]), 'previousTarget': array([23., 20.]), 'currentState': array([23.5, 28.5,  0. ]), 'targetState': array([23, 20], dtype=int32), 'currentDistance': 8.514693182963128}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1827106171625853
{'scaleFactor': 20, 'currentTarget': array([58.99290308, 56.68361696]), 'previousTarget': array([58.49426035, 56.18228538]), 'currentState': array([73.        , 70.95951399,  0.        ]), 'targetState': array([23, 20], dtype=int32), 'currentDistance': 20.0}
episode index:1576
target Thresh 31.999996337657723
target distance 9.0
model initialize at round 1576
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 19.]), 'previousTarget': array([ 7., 19.]), 'currentState': array([ 7.5       , 27.50000003,  0.        ]), 'targetState': array([ 7, 19], dtype=int32), 'currentDistance': 8.514693212714038}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1825947575448538
{'scaleFactor': 20, 'currentTarget': array([43.03681735, 55.95474569]), 'previousTarget': array([42.53861297, 55.45299489]), 'currentState': array([57.        , 70.27359796,  0.        ]), 'targetState': array([ 7, 19], dtype=int32), 'currentDistance': 20.0}
episode index:1577
target Thresh 31.999996374098636
target distance 22.0
model initialize at round 1577
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3.543335  , 23.87605409]), 'previousTarget': array([ 3.56757793, 23.50265712]), 'currentState': array([8.5, 4.5, 0. ]), 'targetState': array([ 3, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18247904477074425
{'scaleFactor': 20, 'currentTarget': array([40.17673769, 44.92633919]), 'previousTarget': array([39.64433887, 44.49026273]), 'currentState': array([58., 54.,  0.]), 'targetState': array([ 3, 26], dtype=int32), 'currentDistance': 20.0}
episode index:1578
target Thresh 31.99999641017696
target distance 12.0
model initialize at round 1578
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 27.]), 'previousTarget': array([21., 27.]), 'currentState': array([ 9.5, 28.5,  0. ]), 'targetState': array([21, 27], dtype=int32), 'currentDistance': 11.597413504743113}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.18287628911745138
{'scaleFactor': 20, 'currentTarget': array([21., 27.]), 'previousTarget': array([21., 27.]), 'currentState': array([20.        , 26.68615855,  0.        ]), 'targetState': array([21, 27], dtype=int32), 'currentDistance': 1.0480918178648408}
episode index:1579
target Thresh 31.999996445896294
target distance 17.0
model initialize at round 1579
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 10.]), 'previousTarget': array([23., 10.]), 'currentState': array([26.5       , 26.50000083,  0.        ]), 'targetState': array([23, 10], dtype=int32), 'currentDistance': 16.86712860973509}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18276054463066818
{'scaleFactor': 20, 'currentTarget': array([62.75689315, 54.99320018]), 'previousTarget': array([62.26513268, 54.4859236 ]), 'currentState': array([76.       , 69.9805322,  0.       ]), 'targetState': array([23, 10], dtype=int32), 'currentDistance': 20.0}
episode index:1580
target Thresh 31.999996481260215
target distance 17.0
model initialize at round 1580
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.16475581, 24.97914598]), 'previousTarget': array([25.71414506, 24.43860471]), 'currentState': array([13.5,  9.5,  0. ]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1826449465632231
{'scaleFactor': 20, 'currentTarget': array([48.25691448, 45.4854971 ]), 'previousTarget': array([47.74829301, 44.99490837]), 'currentState': array([63.        , 58.99998671,  0.        ]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 20.0}
episode index:1581
target Thresh 31.99999651627226
target distance 5.0
model initialize at round 1581
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([19.5,  2.5,  0. ]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 5.700877125495646}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.18311277194114012
{'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([23.5      ,  6.4902522,  0.       ]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 0.7140327883783235}
episode index:1582
target Thresh 31.999996550935933
target distance 17.0
model initialize at round 1582
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 21.]), 'previousTarget': array([ 8., 21.]), 'currentState': array([12.5,  4.5,  0. ]), 'targetState': array([ 8, 21], dtype=int32), 'currentDistance': 17.102631376487096}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18299709741685638
{'scaleFactor': 20, 'currentTarget': array([44.93436333, 43.57099981]), 'previousTarget': array([44.40678119, 43.11626895]), 'currentState': array([62., 54.,  0.]), 'targetState': array([ 8, 21], dtype=int32), 'currentDistance': 20.0}
episode index:1583
target Thresh 31.999996585254692
target distance 14.0
model initialize at round 1583
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 5.]), 'previousTarget': array([6., 5.]), 'currentState': array([ 4.5, 18.5,  0. ]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 13.583077707206042}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18288156894626492
{'scaleFactor': 20, 'currentTarget': array([40.52744454, 42.88178953]), 'previousTarget': array([40.03430174, 42.37554243]), 'currentState': array([54.        , 57.66320521,  0.        ]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 20.0}
episode index:1584
target Thresh 31.999996619231975
target distance 18.0
model initialize at round 1584
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 15.]), 'previousTarget': array([ 6., 15.]), 'currentState': array([24.5, 13.5,  0. ]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 18.56071119327066}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18276618625292343
{'scaleFactor': 20, 'currentTarget': array([57.66064273, 51.46633605]), 'previousTarget': array([57.14387161, 50.99013188]), 'currentState': array([74., 63.,  0.]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 20.0}
episode index:1585
target Thresh 31.999996652871182
target distance 24.0
model initialize at round 1585
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9.04508385, 11.85151561]), 'previousTarget': array([ 8.58594111, 11.31908351]), 'currentState': array([21.5, 27.5,  0. ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18265094906108678
{'scaleFactor': 20, 'currentTarget': array([57.3606795 , 62.37232294]), 'previousTarget': array([56.86427729, 61.86896906]), 'currentState': array([71., 77.,  0.]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 20.0}
episode index:1586
target Thresh 31.99999668617567
target distance 18.0
model initialize at round 1586
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 12.]), 'previousTarget': array([ 7., 12.]), 'currentState': array([25.5, 17.5,  0. ]), 'targetState': array([ 7, 12], dtype=int32), 'currentDistance': 19.30025906561888}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18253585709570488
{'scaleFactor': 20, 'currentTarget': array([59.44977856, 54.42261501]), 'previousTarget': array([58.93900957, 53.93594106]), 'currentState': array([75., 67.,  0.]), 'targetState': array([ 7, 12], dtype=int32), 'currentDistance': 20.0}
episode index:1587
target Thresh 31.999996719148772
target distance 17.0
model initialize at round 1587
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.35786438, 14.35786438]), 'previousTarget': array([10.85786438, 13.85786438]), 'currentState': array([25.5, 28.5,  0. ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18242091008242042
{'scaleFactor': 20, 'currentTarget': array([60.85786438, 63.85786438]), 'previousTarget': array([60.35786438, 63.35786438]), 'currentState': array([75., 78.,  0.]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 20.0}
episode index:1588
target Thresh 31.999996751793788
target distance 11.0
model initialize at round 1588
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 24.]), 'previousTarget': array([14., 24.]), 'currentState': array([ 7.5, 13.5,  0. ]), 'targetState': array([14, 24], dtype=int32), 'currentDistance': 12.349089035228403}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1823061077475668
{'scaleFactor': 20, 'currentTarget': array([42.18561931, 49.56370123]), 'previousTarget': array([41.67754971, 49.07260385]), 'currentState': array([57., 63.,  0.]), 'targetState': array([14, 24], dtype=int32), 'currentDistance': 20.0}
episode index:1589
target Thresh 31.99999678411398
target distance 5.0
model initialize at round 1589
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 15.]), 'previousTarget': array([ 7., 15.]), 'currentState': array([ 7.5      , 19.5004468,  0.       ]), 'targetState': array([ 7, 15], dtype=int32), 'currentDistance': 4.528136633029284}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18219144981816582
{'scaleFactor': 20, 'currentTarget': array([43.13973219, 52.59527073]), 'previousTarget': array([42.64254901, 52.0925635 ]), 'currentState': array([57.        , 67.01376498,  0.        ]), 'targetState': array([ 7, 15], dtype=int32), 'currentDistance': 20.0}
episode index:1590
target Thresh 31.999996816112578
target distance 9.0
model initialize at round 1590
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([18.5      ,  8.5002487,  0.       ]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 8.514678582393474}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.18261751324043052
{'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([26.        ,  8.62468742,  0.        ]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 1.068110261701548}
episode index:1591
target Thresh 31.99999684779279
target distance 8.0
model initialize at round 1591
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 3.]), 'previousTarget': array([6., 3.]), 'currentState': array([14.5,  8.5,  0. ]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 10.124228365658404}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18250280374718905
{'scaleFactor': 20, 'currentTarget': array([49.48752298, 44.23816834]), 'previousTarget': array([48.98426357, 43.74160634]), 'currentState': array([64., 58.,  0.]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 20.0}
episode index:1592
target Thresh 31.999996879157774
target distance 13.0
model initialize at round 1592
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 23.]), 'previousTarget': array([22., 23.]), 'currentState': array([ 9.5      , 18.4999541,  0.       ]), 'targetState': array([22, 23], dtype=int32), 'currentDistance': 13.285345801381814}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.18288642677286165
{'scaleFactor': 20, 'currentTarget': array([22., 23.]), 'previousTarget': array([22., 23.]), 'currentState': array([21.        , 22.59735907,  0.        ]), 'targetState': array([22, 23], dtype=int32), 'currentDistance': 1.0780165654737348}
episode index:1593
target Thresh 31.999996910210672
target distance 21.0
model initialize at round 1593
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.35291755,  8.07891249]), 'previousTarget': array([23.79898987,  8.17157288]), 'currentState': array([ 4.5, 10.5,  0. ]), 'targetState': array([25,  8], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.1831956128596166
{'scaleFactor': 20, 'currentTarget': array([25.,  8.]), 'previousTarget': array([25.,  8.]), 'currentState': array([24.        ,  7.63235301,  0.        ]), 'targetState': array([25,  8], dtype=int32), 'currentDistance': 1.0654408984709287}
episode index:1594
target Thresh 31.999996940954592
target distance 19.0
model initialize at round 1594
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.85000343,  6.32646312]), 'previousTarget': array([25.56172689,  6.92524322]), 'currentState': array([17.5, 24.5,  0. ]), 'targetState': array([26,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18308075667600557
{'scaleFactor': 20, 'currentTarget': array([54.67470709, 42.64408487]), 'previousTarget': array([54.19520809, 42.12806393]), 'currentState': array([67.        , 58.39486754,  0.        ]), 'targetState': array([26,  6], dtype=int32), 'currentDistance': 20.0}
episode index:1595
target Thresh 31.9999969713926
target distance 6.0
model initialize at round 1595
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([6.5, 4.5, 0. ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 5.522680508593634}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18296604442244918
{'scaleFactor': 20, 'currentTarget': array([40.98572966, 40.7874421 ]), 'previousTarget': array([40.47670203, 40.29770785]), 'currentState': array([56., 54.,  0.]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 20.0}
episode index:1596
target Thresh 31.99999700152775
target distance 15.0
model initialize at round 1596
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 27.]), 'previousTarget': array([18., 27.]), 'currentState': array([11.5, 12.5,  0. ]), 'targetState': array([18, 27], dtype=int32), 'currentDistance': 15.890248582070598}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18285147582857164
{'scaleFactor': 20, 'currentTarget': array([45.48875993, 49.37457203]), 'previousTarget': array([44.97213364, 48.89502613]), 'currentState': array([61., 62.,  0.]), 'targetState': array([18, 27], dtype=int32), 'currentDistance': 20.0}
episode index:1597
target Thresh 31.999997031363044
target distance 8.0
model initialize at round 1597
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  7.]), 'previousTarget': array([18.,  7.]), 'currentState': array([22.5, 15.5,  0. ]), 'targetState': array([18,  7], dtype=int32), 'currentDistance': 9.617692030835782}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1827370506246739
{'scaleFactor': 20, 'currentTarget': array([58.37162944, 50.36212051]), 'previousTarget': array([57.87633383, 49.85774197]), 'currentState': array([72., 65.,  0.]), 'targetState': array([18,  7], dtype=int32), 'currentDistance': 20.0}
episode index:1598
target Thresh 31.999997060901478
target distance 15.0
model initialize at round 1598
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([18.5       , 23.49999592,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 15.700314437282792}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18262276854173165
{'scaleFactor': 20, 'currentTarget': array([55.50609842, 57.38261898]), 'previousTarget': array([55.02087919, 56.87080582]), 'currentState': array([68.        , 72.99999467,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 20.0}
episode index:1599
target Thresh 31.999997090145996
target distance 11.0
model initialize at round 1599
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 15.]), 'previousTarget': array([ 3., 15.]), 'currentState': array([12.5,  4.5,  0. ]), 'targetState': array([ 3, 15], dtype=int32), 'currentDistance': 14.159802258506351}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18250862931139306
{'scaleFactor': 20, 'currentTarget': array([45.31561651, 42.97133973]), 'previousTarget': array([44.79338393, 42.50504754]), 'currentState': array([62., 54.,  0.]), 'targetState': array([ 3, 15], dtype=int32), 'currentDistance': 20.0}
episode index:1600
target Thresh 31.99999711909953
target distance 15.0
model initialize at round 1600
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 15.]), 'previousTarget': array([11., 15.]), 'currentState': array([26.5, 23.5,  0. ]), 'targetState': array([11, 15], dtype=int32), 'currentDistance': 17.677669529663795}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18239463266597683
{'scaleFactor': 20, 'currentTarget': array([61.07717397, 59.68424755]), 'previousTarget': array([60.57098427, 59.19118753]), 'currentState': array([76., 73.,  0.]), 'targetState': array([11, 15], dtype=int32), 'currentDistance': 20.0}
episode index:1601
target Thresh 31.999997147764965
target distance 8.0
model initialize at round 1601
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 14.]), 'previousTarget': array([23., 14.]), 'currentState': array([15.5       , 13.21435542,  0.        ]), 'targetState': array([23, 14], dtype=int32), 'currentDistance': 7.541036891550392}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.18282854427042938
{'scaleFactor': 20, 'currentTarget': array([23., 14.]), 'previousTarget': array([23., 14.]), 'currentState': array([22.        , 13.62474619,  0.        ]), 'targetState': array([23, 14], dtype=int32), 'currentDistance': 1.0680896139259832}
episode index:1602
target Thresh 31.99999717614518
target distance 18.0
model initialize at round 1602
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4.32646312, 18.14999657]), 'previousTarget': array([ 4., 18.]), 'currentState': array([22.5, 26.5,  0. ]), 'targetState': array([ 4, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18271449028148962
{'scaleFactor': 20, 'currentTarget': array([56.78332096, 63.02106788]), 'previousTarget': array([56.27513539, 62.53067089]), 'currentState': array([72., 76.,  0.]), 'targetState': array([ 4, 18], dtype=int32), 'currentDistance': 20.0}
episode index:1603
target Thresh 31.999997204243005
target distance 17.0
model initialize at round 1603
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 27.]), 'previousTarget': array([ 7., 27.]), 'currentState': array([17.5, 10.5,  0. ]), 'targetState': array([ 7, 27], dtype=int32), 'currentDistance': 19.557607215607923}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18260057850450614
{'scaleFactor': 20, 'currentTarget': array([49.47568183, 50.361625  ]), 'previousTarget': array([48.94772845, 49.91262478]), 'currentState': array([67., 60.,  0.]), 'targetState': array([ 7, 27], dtype=int32), 'currentDistance': 20.0}
episode index:1604
target Thresh 31.99999723206125
target distance 8.0
model initialize at round 1604
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 17.]), 'previousTarget': array([15., 17.]), 'currentState': array([23.5, 22.5,  0. ]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 10.124228365658407}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1824868086736622
{'scaleFactor': 20, 'currentTarget': array([58.48752298, 58.23816834]), 'previousTarget': array([57.98426357, 57.74160634]), 'currentState': array([73., 72.,  0.]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 20.0}
episode index:1605
target Thresh 31.9999972596027
target distance 15.0
model initialize at round 1605
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([ 2.5, 14.5,  0. ]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 16.324827717314474}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.18284786465477892
{'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([16.        , 21.94951681,  0.        ]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 1.0012734655417168}
episode index:1606
target Thresh 31.99999728687011
target distance 12.0
model initialize at round 1606
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 24.]), 'previousTarget': array([21., 24.]), 'currentState': array([11.5, 12.5,  0. ]), 'targetState': array([21, 24], dtype=int32), 'currentDistance': 14.916433890176185}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18273408253613874
{'scaleFactor': 20, 'currentTarget': array([46.49496155, 48.20243508]), 'previousTarget': array([45.99031296, 47.70733359]), 'currentState': array([61.        , 61.97210681,  0.        ]), 'targetState': array([21, 24], dtype=int32), 'currentDistance': 20.0}
episode index:1607
target Thresh 31.999997313866203
target distance 12.0
model initialize at round 1607
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 2.]), 'previousTarget': array([9., 2.]), 'currentState': array([21.5,  4.5,  0. ]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 12.747548783982039}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18262044193754662
{'scaleFactor': 20, 'currentTarget': array([55.67616914, 41.14775476]), 'previousTarget': array([55.16627364, 40.65956248]), 'currentState': array([71., 54.,  0.]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 20.0}
episode index:1608
target Thresh 31.99999734059368
target distance 18.0
model initialize at round 1608
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 23.]), 'previousTarget': array([10., 23.]), 'currentState': array([5.5, 5.5, 0. ]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 18.06931099959266}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1825069425951367
{'scaleFactor': 20, 'currentTarget': array([38.70089351, 43.40952427]), 'previousTarget': array([38.17590645, 42.94474277]), 'currentState': array([55., 55.,  0.]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 20.0}
episode index:1609
target Thresh 31.999997367055215
target distance 19.0
model initialize at round 1609
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.7106186 , 18.65581896]), 'previousTarget': array([11.70632169, 18.50614522]), 'currentState': array([23.5,  2.5,  0. ]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1823935842456987
{'scaleFactor': 20, 'currentTarget': array([55.05485031, 43.16984698]), 'previousTarget': array([54.52601326, 42.72869447]), 'currentState': array([73., 52.,  0.]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 20.0}
episode index:1610
target Thresh 31.999997393253455
target distance 4.0
model initialize at round 1610
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  2.]), 'previousTarget': array([17.,  2.]), 'currentState': array([21.5,  4.5,  0. ]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 5.147815070493566}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18228036662667593
{'scaleFactor': 20, 'currentTarget': array([56.59359573, 40.12716626]), 'previousTarget': array([56.09110399, 39.6297543 ]), 'currentState': array([71., 54.,  0.]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 20.0}
episode index:1611
target Thresh 31.999997419191015
target distance 13.0
model initialize at round 1611
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 21.]), 'previousTarget': array([ 2., 21.]), 'currentState': array([12.5,  8.5,  0. ]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 16.3248277173145}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18216728947616312
{'scaleFactor': 20, 'currentTarget': array([44.97658261, 47.50222594]), 'previousTarget': array([44.45209418, 47.04204097]), 'currentState': array([62., 58.,  0.]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:1612
target Thresh 31.999997444870495
target distance 7.0
model initialize at round 1612
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 18.]), 'previousTarget': array([ 4., 18.]), 'currentState': array([11.5, 12.5,  0. ]), 'targetState': array([ 4, 18], dtype=int32), 'currentDistance': 9.300537618869155}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1820543525329045
{'scaleFactor': 20, 'currentTarget': array([45.16819358, 49.77895645]), 'previousTarget': array([44.6527351 , 49.29900844]), 'currentState': array([61., 62.,  0.]), 'targetState': array([ 4, 18], dtype=int32), 'currentDistance': 20.0}
episode index:1613
target Thresh 31.999997470294456
target distance 4.0
model initialize at round 1613
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 23.]), 'previousTarget': array([25., 23.]), 'currentState': array([21.5       , 21.49613938,  0.        ]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 3.809408979400845}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.1825307687022769
{'scaleFactor': 20, 'currentTarget': array([25., 23.]), 'previousTarget': array([25., 23.]), 'currentState': array([24.        , 22.70414066,  0.        ]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 1.0428483827510204}
episode index:1614
target Thresh 31.999997495465447
target distance 7.0
model initialize at round 1614
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 13.]), 'previousTarget': array([ 8., 13.]), 'currentState': array([15.5, 19.5,  0. ]), 'targetState': array([ 8, 13], dtype=int32), 'currentDistance': 9.924716620639716}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18241774655447363
{'scaleFactor': 20, 'currentTarget': array([50.73327137, 54.98356485]), 'previousTarget': array([50.23216401, 54.48469208]), 'currentState': array([65., 69.,  0.]), 'targetState': array([ 8, 13], dtype=int32), 'currentDistance': 20.0}
episode index:1615
target Thresh 31.999997520385985
target distance 16.0
model initialize at round 1615
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 19.]), 'previousTarget': array([ 8., 19.]), 'currentState': array([20.5,  3.5,  0. ]), 'targetState': array([ 8, 19], dtype=int32), 'currentDistance': 19.912307751739927}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18230486428556616
{'scaleFactor': 20, 'currentTarget': array([52.46375183, 43.38334778]), 'previousTarget': array([51.93663439, 42.93296345]), 'currentState': array([70., 53.,  0.]), 'targetState': array([ 8, 19], dtype=int32), 'currentDistance': 20.0}
episode index:1616
target Thresh 31.999997545058555
target distance 12.0
model initialize at round 1616
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 27.]), 'previousTarget': array([19., 27.]), 'currentState': array([13.5, 15.5,  0. ]), 'targetState': array([19, 27], dtype=int32), 'currentDistance': 12.747548783981856}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18219212163603893
{'scaleFactor': 20, 'currentTarget': array([47.86353984, 51.92760259]), 'previousTarget': array([47.35180077, 51.44120756]), 'currentState': array([63., 65.,  0.]), 'targetState': array([19, 27], dtype=int32), 'currentDistance': 20.0}
episode index:1617
target Thresh 31.999997569485632
target distance 5.0
model initialize at round 1617
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 8.]), 'previousTarget': array([9., 8.]), 'currentState': array([11.5,  3.5,  0. ]), 'targetState': array([9, 8], dtype=int32), 'currentDistance': 5.147815070493543}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18207951834701788
{'scaleFactor': 20, 'currentTarget': array([45.87662635, 39.91246511]), 'previousTarget': array([45.36684394, 39.42377777]), 'currentState': array([61., 53.,  0.]), 'targetState': array([9, 8], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:1618
target Thresh 31.999997593669654
target distance 11.0
model initialize at round 1618
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 27.]), 'previousTarget': array([ 2., 27.]), 'currentState': array([13.5, 20.5,  0. ]), 'targetState': array([ 2, 27], dtype=int32), 'currentDistance': 13.209844813622938}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18196705416026865
{'scaleFactor': 20, 'currentTarget': array([46.65321942, 58.47685959]), 'previousTarget': array([46.13444665, 58.00353691]), 'currentState': array([63., 70.,  0.]), 'targetState': array([ 2, 27], dtype=int32), 'currentDistance': 20.0}
episode index:1619
target Thresh 31.99999761761304
target distance 16.0
model initialize at round 1619
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([10.5, 18.5,  0. ]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 15.57241150239735}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.1823159461603744
{'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([25.        , 20.57204212,  0.        ]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 1.1520556366299262}
episode index:1620
target Thresh 31.999997641318185
target distance 19.0
model initialize at round 1620
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9.66241722, 5.70667771]), 'previousTarget': array([9.09022194, 5.32014017]), 'currentState': array([26.5, 16.5,  0. ]), 'targetState': array([7, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18220347487958452
{'scaleFactor': 20, 'currentTarget': array([61.12340126, 52.63262142]), 'previousTarget': array([60.61792376, 52.13871987]), 'currentState': array([76., 66.,  0.]), 'targetState': array([7, 4], dtype=int32), 'currentDistance': 20.0}
episode index:1621
target Thresh 31.99999766478746
target distance 4.0
model initialize at round 1621
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 12.]), 'previousTarget': array([22., 12.]), 'currentState': array([22.5,  8.5,  0. ]), 'targetState': array([22, 12], dtype=int32), 'currentDistance': 3.5355339059326703}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1820911422810151
{'scaleFactor': 20, 'currentTarget': array([57.28137976, 44.45886938]), 'previousTarget': array([56.77545248, 43.96531491]), 'currentState': array([72., 58.,  0.]), 'targetState': array([22, 12], dtype=int32), 'currentDistance': 20.0}
episode index:1622
target Thresh 31.999997688023214
target distance 9.0
model initialize at round 1622
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 23.]), 'previousTarget': array([20., 23.]), 'currentState': array([11.5       , 28.50123695,  0.        ]), 'targetState': array([20, 23], dtype=int32), 'currentDistance': 10.124900391081875}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.18250886699596292
{'scaleFactor': 20, 'currentTarget': array([20., 23.]), 'previousTarget': array([20., 23.]), 'currentState': array([19.        , 23.22702946,  0.        ]), 'targetState': array([20, 23], dtype=int32), 'currentDistance': 1.0254474021888478}
episode index:1623
target Thresh 31.99999771102777
target distance 18.0
model initialize at round 1623
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.33564511, 26.71606909]), 'previousTarget': array([23.78704435, 26.27881227]), 'currentState': array([ 8.5, 14.5,  0. ]), 'targetState': array([26, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.18283843698709734
{'scaleFactor': 20, 'currentTarget': array([26., 28.]), 'previousTarget': array([26., 28.]), 'currentState': array([25.        , 28.04944433,  0.        ]), 'targetState': array([26, 28], dtype=int32), 'currentDistance': 1.0012216248327395}
episode index:1624
target Thresh 31.999997733803422
target distance 22.0
model initialize at round 1624
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.08744313,  6.61599637]), 'previousTarget': array([21.83486126,  7.20413153]), 'currentState': array([15.5, 25.5,  0. ]), 'targetState': array([23,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1827259210258745
{'scaleFactor': 20, 'currentTarget': array([53.50165034, 47.40935632]), 'previousTarget': array([53.02917802, 46.8900485 ]), 'currentState': array([65.        , 63.77358423,  0.        ]), 'targetState': array([23,  4], dtype=int32), 'currentDistance': 20.0}
episode index:1625
target Thresh 31.999997756352457
target distance 18.0
model initialize at round 1625
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([27.5, 19.5,  0. ]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 18.828170383762796}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18261354346066797
{'scaleFactor': 20, 'currentTarget': array([60.43433174, 57.79381265]), 'previousTarget': array([59.91590918, 57.32109434]), 'currentState': array([77., 69.,  0.]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 20.0}
episode index:1626
target Thresh 31.999997778677123
target distance 7.0
model initialize at round 1626
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 20.]), 'previousTarget': array([21., 20.]), 'currentState': array([14.5      , 18.4963989,  0.       ]), 'targetState': array([21, 20], dtype=int32), 'currentDistance': 6.671642697268236}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.18305160413110313
{'scaleFactor': 20, 'currentTarget': array([21., 20.]), 'previousTarget': array([21., 20.]), 'currentState': array([20.        , 19.74067922,  0.        ]), 'targetState': array([21, 20], dtype=int32), 'currentDistance': 1.0330766018718076}
episode index:1627
target Thresh 31.999997800779653
target distance 13.0
model initialize at round 1627
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([3.5       , 1.96066277, 0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 12.500061896561844}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.18342664263203223
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.        ,  2.15294851,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.0116290070072502}
episode index:1628
target Thresh 31.999997822662262
target distance 9.0
model initialize at round 1628
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  8.]), 'previousTarget': array([24.,  8.]), 'currentState': array([15.5,  2.5,  0. ]), 'targetState': array([24,  8], dtype=int32), 'currentDistance': 10.12422836565823}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.18384200893774694
{'scaleFactor': 20, 'currentTarget': array([24.,  8.]), 'previousTarget': array([24.,  8.]), 'currentState': array([23.        ,  8.05287551,  0.        ]), 'targetState': array([24,  8], dtype=int32), 'currentDistance': 1.0013969341363902}
episode index:1629
target Thresh 31.999997844327133
target distance 12.0
model initialize at round 1629
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 25.]), 'previousTarget': array([13., 25.]), 'currentState': array([25.5, 24.5,  0. ]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 12.509996003196878}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18372922242919618
{'scaleFactor': 20, 'currentTarget': array([59.30882508, 61.59891014]), 'previousTarget': array([58.79581054, 61.11539531]), 'currentState': array([75., 74.,  0.]), 'targetState': array([13, 25], dtype=int32), 'currentDistance': 20.0}
episode index:1630
target Thresh 31.99999786577644
target distance 6.0
model initialize at round 1630
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 8.]), 'previousTarget': array([9., 8.]), 'currentState': array([5.5, 2.5, 0. ]), 'targetState': array([9, 8], dtype=int32), 'currentDistance': 6.519202405202656}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18361657422415067
{'scaleFactor': 20, 'currentTarget': array([40.54665029, 38.17281705]), 'previousTarget': array([40.04319492, 37.67643065]), 'currentState': array([55.        , 51.99673408,  0.        ]), 'targetState': array([9, 8], dtype=int32), 'currentDistance': 20.0}
episode index:1631
target Thresh 31.999997887012317
target distance 17.0
model initialize at round 1631
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([10.5, 22.5,  0. ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 16.867127793432907}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18350406406837608
{'scaleFactor': 20, 'currentTarget': array([46.57825877, 41.99059446]), 'previousTarget': array([46.08594234, 41.483643  ]), 'currentState': array([60.        , 56.81816548,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 20.0}
episode index:1632
target Thresh 31.999997908036896
target distance 14.0
model initialize at round 1632
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  8.]), 'previousTarget': array([20.,  8.]), 'currentState': array([ 6.5, 20.5,  0. ]), 'targetState': array([20,  8], dtype=int32), 'currentDistance': 18.398369492974048}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.18386800607408996
{'scaleFactor': 20, 'currentTarget': array([20.,  8.]), 'previousTarget': array([20.,  8.]), 'currentState': array([19.       ,  8.1441099,  0.       ]), 'targetState': array([20,  8], dtype=int32), 'currentDistance': 1.010330472987812}
episode index:1633
target Thresh 31.999997928852277
target distance 22.0
model initialize at round 1633
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.71351204, 26.2401454 ]), 'previousTarget': array([19.57770876, 25.6773982 ]), 'currentState': array([16.5,  6.5,  0. ]), 'targetState': array([20, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18375547975458315
{'scaleFactor': 20, 'currentTarget': array([48.91602889, 45.60106106]), 'previousTarget': array([48.38342382, 45.15481659]), 'currentState': array([66., 56.,  0.]), 'targetState': array([20, 28], dtype=int32), 'currentDistance': 20.0}
episode index:1634
target Thresh 31.99999794946054
target distance 18.0
model initialize at round 1634
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([27.5, 17.5,  0. ]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 18.828170383762796}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1836430910819504
{'scaleFactor': 20, 'currentTarget': array([60.43433174, 55.79381265]), 'previousTarget': array([59.91590918, 55.32109434]), 'currentState': array([77., 67.,  0.]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 20.0}
episode index:1635
target Thresh 31.99999796986375
target distance 16.0
model initialize at round 1635
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 26.]), 'previousTarget': array([19., 26.]), 'currentState': array([ 3.5       , 17.48668283,  0.        ]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 17.684076713214612}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.18398754646290982
{'scaleFactor': 20, 'currentTarget': array([19., 26.]), 'previousTarget': array([19., 26.]), 'currentState': array([18.        , 25.03330274,  0.        ]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 1.3908643333958708}
episode index:1636
target Thresh 31.999997990063942
target distance 13.0
model initialize at round 1636
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 4.]), 'previousTarget': array([5., 4.]), 'currentState': array([10.5       , 17.49943516,  0.        ]), 'targetState': array([5, 4], dtype=int32), 'currentDistance': 14.576856641498837}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18387515333739796
{'scaleFactor': 20, 'currentTarget': array([46.84677941, 51.93314948]), 'previousTarget': array([46.35547106, 51.42556588]), 'currentState': array([60.        , 66.99942932,  0.        ]), 'targetState': array([5, 4], dtype=int32), 'currentDistance': 20.0}
episode index:1637
target Thresh 31.99999801006314
target distance 7.0
model initialize at round 1637
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 20.]), 'previousTarget': array([20., 20.]), 'currentState': array([19.5, 13.5,  0. ]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 6.519202405202563}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18376289744402957
{'scaleFactor': 20, 'currentTarget': array([53.96748622, 49.80820219]), 'previousTarget': array([53.4580763 , 49.31893284]), 'currentState': array([69., 63.,  0.]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 20.0}
episode index:1638
target Thresh 31.99999802986334
target distance 13.0
model initialize at round 1638
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([17.5       , 14.50025472,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 12.589931218085358}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18365077853161713
{'scaleFactor': 20, 'currentTarget': array([53.76199818, 44.76490476]), 'previousTarget': array([53.27061037, 44.25730449]), 'currentState': array([67.        , 59.75674614,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 20.0}
episode index:1639
target Thresh 31.99999804946653
target distance 18.0
model initialize at round 1639
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 13.]), 'previousTarget': array([25., 13.]), 'currentState': array([ 7.5, 13.5,  0. ]), 'targetState': array([25, 13], dtype=int32), 'currentDistance': 17.507141400011598}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.18397643691824314
{'scaleFactor': 20, 'currentTarget': array([25., 13.]), 'previousTarget': array([25., 13.]), 'currentState': array([24.        , 12.29796816,  0.        ]), 'targetState': array([25, 13], dtype=int32), 'currentDistance': 1.2218218810827572}
episode index:1640
target Thresh 31.999998068874664
target distance 12.0
model initialize at round 1640
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 28.]), 'previousTarget': array([12., 28.]), 'currentState': array([ 3.5, 16.5,  0. ]), 'targetState': array([12, 28], dtype=int32), 'currentDistance': 14.300349646075029}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18386432452523996
{'scaleFactor': 20, 'currentTarget': array([38.33139406, 52.40470665]), 'previousTarget': array([37.82478651, 51.91183932]), 'currentState': array([53.        , 65.99999994,  0.        ]), 'targetState': array([12, 28], dtype=int32), 'currentDistance': 20.0}
episode index:1641
target Thresh 31.99999808808968
target distance 18.0
model initialize at round 1641
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 13.]), 'previousTarget': array([ 9., 13.]), 'currentState': array([27.5,  5.5,  0. ]), 'targetState': array([ 9, 13], dtype=int32), 'currentDistance': 19.962464777677205}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18375234868813567
{'scaleFactor': 20, 'currentTarget': array([59.98404005, 44.49014239]), 'previousTarget': array([59.46250195, 44.02509379]), 'currentState': array([77., 55.,  0.]), 'targetState': array([ 9, 13], dtype=int32), 'currentDistance': 20.0}
episode index:1642
target Thresh 31.999998107113505
target distance 20.0
model initialize at round 1642
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.17544468, 22.47366596]), 'previousTarget': array([13.25304229, 22.1565257 ]), 'currentState': array([19.5,  3.5,  0. ]), 'targetState': array([13, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.183640509157589
{'scaleFactor': 20, 'currentTarget': array([51.3704006 , 43.55557175]), 'previousTarget': array([50.83974599, 43.11301814]), 'currentState': array([69., 53.,  0.]), 'targetState': array([13, 23], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:1643
target Thresh 31.999998125948043
target distance 17.0
model initialize at round 1643
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 8.]), 'previousTarget': array([5., 8.]), 'currentState': array([14.5, 25.5,  0. ]), 'targetState': array([5, 8], dtype=int32), 'currentDistance': 19.912307751739984}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18352880568486543
{'scaleFactor': 20, 'currentTarget': array([50.78239264, 59.99017469]), 'previousTarget': array([50.28998751, 59.48349008]), 'currentState': array([64., 75.,  0.]), 'targetState': array([5, 8], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:1644
target Thresh 31.99999814459517
target distance 17.0
model initialize at round 1644
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  4.]), 'previousTarget': array([13.,  4.]), 'currentState': array([ 3.5, 20.5,  0. ]), 'targetState': array([13,  4], dtype=int32), 'currentDistance': 19.0394327646597}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1834172380218351
{'scaleFactor': 20, 'currentTarget': array([39.93668115, 35.22767471]), 'previousTarget': array([39.44970114, 34.71645355]), 'currentState': array([53.        , 50.37197069,  0.        ]), 'targetState': array([13,  4], dtype=int32), 'currentDistance': 20.0}
episode index:1645
target Thresh 31.999998163056755
target distance 14.0
model initialize at round 1645
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([26.5, 19.5,  0. ]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 15.890248582070807}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1833058059209713
{'scaleFactor': 20, 'currentTarget': array([62.82990784, 53.94846611]), 'previousTarget': array([62.3383045 , 53.44112317]), 'currentState': array([76., 69.,  0.]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 20.0}
episode index:1646
target Thresh 31.999998181334647
target distance 15.0
model initialize at round 1646
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4.89457173, 20.16314258]), 'previousTarget': array([ 4.85786438, 20.14213562]), 'currentState': array([19.5,  6.5,  0. ]), 'targetState': array([ 4, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18319450913534835
{'scaleFactor': 20, 'currentTarget': array([51.390578  , 46.51800354]), 'previousTarget': array([50.86430716, 46.06695499]), 'currentState': array([69., 56.,  0.]), 'targetState': array([ 4, 21], dtype=int32), 'currentDistance': 20.0}
episode index:1647
target Thresh 31.99999819943067
target distance 11.0
model initialize at round 1647
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([15.5, 19.5,  0. ]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 14.159802258506181}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.1835796501720364
{'scaleFactor': 20, 'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([25.5       ,  9.50026786,  0.        ]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 0.7072962144856147}
episode index:1648
target Thresh 31.999998217346633
target distance 5.0
model initialize at round 1648
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([21.5, 13.5,  0. ]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 6.519202405202671}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18346832230655913
{'scaleFactor': 20, 'currentTarget': array([55.6584626 , 50.16889599]), 'previousTarget': array([55.1471259 , 49.68246291]), 'currentState': array([71., 63.,  0.]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 20.0}
episode index:1649
target Thresh 31.99999823508433
target distance 8.0
model initialize at round 1649
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([18.5       , 20.50069299,  0.        ]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 7.516694318225348}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.18388896030697877
{'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([25.        , 19.20611899,  0.        ]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 1.2768112876892506}
episode index:1650
target Thresh 31.999998252645536
target distance 17.0
model initialize at round 1650
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 10.]), 'previousTarget': array([25., 10.]), 'currentState': array([17.5       , 26.50020772,  0.        ]), 'targetState': array([25, 10], dtype=int32), 'currentDistance': 18.12475806391139}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18377757995549057
{'scaleFactor': 20, 'currentTarget': array([55.80167838, 55.5795264 ]), 'previousTarget': array([55.33164614, 55.05931424]), 'currentState': array([67.        , 72.15051287,  0.        ]), 'targetState': array([25, 10], dtype=int32), 'currentDistance': 20.0}
episode index:1651
target Thresh 31.999998270032002
target distance 16.0
model initialize at round 1651
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 13.]), 'previousTarget': array([12., 13.]), 'currentState': array([ 2.5       , 28.50000048,  0.        ]), 'targetState': array([12, 13], dtype=int32), 'currentDistance': 18.179659369249773}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18366633444704294
{'scaleFactor': 20, 'currentTarget': array([40.62668575, 54.40818743]), 'previousTarget': array([40.156693  , 53.88748294]), 'currentState': array([52.        , 70.85956194,  0.        ]), 'targetState': array([12, 13], dtype=int32), 'currentDistance': 20.0}
episode index:1652
target Thresh 31.99999828724547
target distance 20.0
model initialize at round 1652
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.25831869,  5.52262855]), 'previousTarget': array([18.94427191,  6.11145618]), 'currentState': array([10.5       , 23.50294706,  0.        ]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18355522353691164
{'scaleFactor': 20, 'currentTarget': array([49.45739974, 51.48818221]), 'previousTarget': array([48.99388779, 50.96560248]), 'currentState': array([60.        , 68.48387519,  0.        ]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 20.0}
episode index:1653
target Thresh 31.999998304287665
target distance 14.0
model initialize at round 1653
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 12.]), 'previousTarget': array([13., 12.]), 'currentState': array([19.5, 26.5,  0. ]), 'targetState': array([13, 12], dtype=int32), 'currentDistance': 15.890248582070807}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1834442469809643
{'scaleFactor': 20, 'currentTarget': array([55.82990784, 60.94846611]), 'previousTarget': array([55.3383045 , 60.44112317]), 'currentState': array([69., 76.,  0.]), 'targetState': array([13, 12], dtype=int32), 'currentDistance': 20.0}
episode index:1654
target Thresh 31.999998321160284
target distance 14.0
model initialize at round 1654
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 21.]), 'previousTarget': array([ 6., 21.]), 'currentState': array([17.5,  7.5,  0. ]), 'targetState': array([ 6, 21], dtype=int32), 'currentDistance': 17.734147850968252}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18333340453565858
{'scaleFactor': 20, 'currentTarget': array([49.77585075, 46.83492831]), 'previousTarget': array([49.25033154, 46.37829372]), 'currentState': array([67., 57.,  0.]), 'targetState': array([ 6, 21], dtype=int32), 'currentDistance': 20.0}
episode index:1655
target Thresh 31.99999833786502
target distance 21.0
model initialize at round 1655
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.21472857, 12.86592928]), 'previousTarget': array([21.79898987, 12.82842712]), 'currentState': array([2.5       , 9.50000036, 0.        ]), 'targetState': array([23, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.18362666441128297
{'scaleFactor': 20, 'currentTarget': array([23., 13.]), 'previousTarget': array([23., 13.]), 'currentState': array([22.5       , 12.03894423,  0.        ]), 'targetState': array([23, 13], dtype=int32), 'currentDistance': 1.0833412177337982}
episode index:1656
target Thresh 31.999998354403537
target distance 18.0
model initialize at round 1656
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  9.]), 'previousTarget': array([22.,  9.]), 'currentState': array([18.5       , 27.49999952,  0.        ]), 'targetState': array([22,  9], dtype=int32), 'currentDistance': 18.82816991523675}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18351584566390142
{'scaleFactor': 20, 'currentTarget': array([56.79381259, 60.43433124]), 'previousTarget': array([56.32109428, 59.91590869]), 'currentState': array([68.        , 76.99999946,  0.        ]), 'targetState': array([22,  9], dtype=int32), 'currentDistance': 20.0}
episode index:1657
target Thresh 31.999998370777497
target distance 18.0
model initialize at round 1657
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.14999657,  8.32646312]), 'previousTarget': array([11.,  8.]), 'currentState': array([19.5, 26.5,  0. ]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1834051605941403
{'scaleFactor': 20, 'currentTarget': array([56.02106788, 60.78332096]), 'previousTarget': array([55.53067089, 60.27513539]), 'currentState': array([69., 76.,  0.]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 20.0}
episode index:1658
target Thresh 31.99999838698853
target distance 22.0
model initialize at round 1658
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.66021748,  8.45516171]), 'previousTarget': array([17.13646016,  8.92760259]), 'currentState': array([ 2.5, 21.5,  0. ]), 'targetState': array([24,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.18368587033963413
{'scaleFactor': 20, 'currentTarget': array([24.,  3.]), 'previousTarget': array([24.,  3.]), 'currentState': array([24.        ,  2.19865981,  0.        ]), 'targetState': array([24,  3], dtype=int32), 'currentDistance': 0.801340185107807}
episode index:1659
target Thresh 31.999998403038262
target distance 21.0
model initialize at round 1659
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.84743117, 14.06603695]), 'previousTarget': array([24.3829006 , 13.87838597]), 'currentState': array([6.5       , 6.10539819, 0.        ]), 'targetState': array([27, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.18397418128580542
{'scaleFactor': 20, 'currentTarget': array([27., 15.]), 'previousTarget': array([27., 15.]), 'currentState': array([27.        , 14.04889113,  0.        ]), 'targetState': array([27, 15], dtype=int32), 'currentDistance': 0.9511088654403146}
episode index:1660
target Thresh 31.9999984189283
target distance 7.0
model initialize at round 1660
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 20.]), 'previousTarget': array([24., 20.]), 'currentState': array([17.5      , 15.4953731,  0.       ]), 'targetState': array([24, 20], dtype=int32), 'currentDistance': 7.908328743029153}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.1843970655064137
{'scaleFactor': 20, 'currentTarget': array([24., 20.]), 'previousTarget': array([24., 20.]), 'currentState': array([23.5       , 19.00235304,  0.        ]), 'targetState': array([24, 20], dtype=int32), 'currentDistance': 1.115929859838095}
episode index:1661
target Thresh 31.999998434660224
target distance 16.0
model initialize at round 1661
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([ 3.5, 24.5,  0. ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 19.300259065618718}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18428611661020045
{'scaleFactor': 20, 'currentTarget': array([41.06886754, 44.07146855]), 'previousTarget': array([40.59512835, 43.55198201]), 'currentState': array([53.        , 60.12288837,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 20.0}
episode index:1662
target Thresh 31.999998450235616
target distance 3.0
model initialize at round 1662
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([9.5       , 7.49999997, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 2.5495097860200175}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18417530114621355
{'scaleFactor': 20, 'currentTarget': array([44.42744661, 43.30179975]), 'previousTarget': array([43.92303854, 42.80649069]), 'currentState': array([59.        , 56.99999991,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 20.0}
episode index:1663
target Thresh 31.999998465656027
target distance 7.0
model initialize at round 1663
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 11.]), 'previousTarget': array([19., 11.]), 'currentState': array([26.5,  7.5,  0. ]), 'targetState': array([19, 11], dtype=int32), 'currentDistance': 8.276472678623497}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1840646188738901
{'scaleFactor': 20, 'currentTarget': array([60.43604422, 44.43961463]), 'previousTarget': array([59.92305132, 43.95573159]), 'currentState': array([76., 57.,  0.]), 'targetState': array([19, 11], dtype=int32), 'currentDistance': 20.0}
episode index:1664
target Thresh 31.999998480923008
target distance 16.0
model initialize at round 1664
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 13.]), 'previousTarget': array([26., 13.]), 'currentState': array([17.5       , 28.50000063,  0.        ]), 'targetState': array([26, 13], dtype=int32), 'currentDistance': 17.677670078415687}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18395406955324511
{'scaleFactor': 20, 'currentTarget': array([55.49920325, 54.96934921]), 'previousTarget': array([55.02738832, 54.44957486]), 'currentState': array([67.       , 71.3318574,  0.       ]), 'targetState': array([26, 13], dtype=int32), 'currentDistance': 20.0}
episode index:1665
target Thresh 31.999998496038074
target distance 6.0
model initialize at round 1665
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  7.]), 'previousTarget': array([21.,  7.]), 'currentState': array([27.5,  2.5,  0. ]), 'targetState': array([21,  7], dtype=int32), 'currentDistance': 7.905694150421019}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18384365294486982
{'scaleFactor': 20, 'currentTarget': array([61.4098213 , 39.47217783]), 'previousTarget': array([60.8963477 , 38.98896347]), 'currentState': array([77., 52.,  0.]), 'targetState': array([21,  7], dtype=int32), 'currentDistance': 20.0}
episode index:1666
target Thresh 31.999998511002747
target distance 7.0
model initialize at round 1666
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([11.5       , 28.50001845,  0.        ]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 8.514707265644986}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.18426509338804395
{'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'previousTarget': array([17., 22.]), 'currentState': array([17.5       , 22.50107664,  0.        ]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 0.7078684890929637}
episode index:1667
target Thresh 31.999998525818516
target distance 7.0
model initialize at round 1667
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 16.]), 'previousTarget': array([26., 16.]), 'currentState': array([20.5       , 22.50002173,  0.        ]), 'targetState': array([26, 16], dtype=int32), 'currentDistance': 8.514709768223389}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.18468602850694568
{'scaleFactor': 20, 'currentTarget': array([26., 16.]), 'previousTarget': array([26., 16.]), 'currentState': array([26.5       , 16.50129572,  0.        ]), 'targetState': array([26, 16], dtype=int32), 'currentDistance': 0.7080235832587463}
episode index:1668
target Thresh 31.999998540486867
target distance 3.0
model initialize at round 1668
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([ 4.5       , 26.50079063,  0.        ]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 2.9158827998602703}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.18515092364265154
{'scaleFactor': 20, 'currentTarget': array([ 7., 25.]), 'previousTarget': array([ 7., 25.]), 'currentState': array([ 6.5      , 24.6895811,  0.       ]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 0.5885234879370305}
episode index:1669
target Thresh 31.999998555009267
target distance 12.0
model initialize at round 1669
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  4.]), 'previousTarget': array([22.,  4.]), 'currentState': array([21.5       , 16.49993166,  0.        ]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 12.509927721075806}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18504005482609906
{'scaleFactor': 20, 'currentTarget': array([58.59889795, 50.30873571]), 'previousTarget': array([58.11538301, 49.79572123]), 'currentState': array([71.      , 65.999901,  0.      ]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 20.0}
episode index:1670
target Thresh 31.999998569387163
target distance 15.0
model initialize at round 1670
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 14.]), 'previousTarget': array([24., 14.]), 'currentState': array([ 9.5, 18.5,  0. ]), 'targetState': array([24, 14], dtype=int32), 'currentDistance': 15.182226450688875}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.18538553816513018
{'scaleFactor': 20, 'currentTarget': array([24., 14.]), 'previousTarget': array([24., 14.]), 'currentState': array([23.        , 13.42079553,  0.        ]), 'targetState': array([24, 14], dtype=int32), 'currentDistance': 1.1556287543886565}
episode index:1671
target Thresh 31.999998583622
target distance 10.0
model initialize at round 1671
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 17.]), 'previousTarget': array([22., 17.]), 'currentState': array([12.5, 26.5,  0. ]), 'targetState': array([22, 17], dtype=int32), 'currentDistance': 13.435028842544291}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.18577377274843454
{'scaleFactor': 20, 'currentTarget': array([22., 17.]), 'previousTarget': array([22., 17.]), 'currentState': array([21.5       , 17.50032166,  0.        ]), 'targetState': array([22, 17], dtype=int32), 'currentDistance': 0.7073342632207833}
episode index:1672
target Thresh 31.999998597715194
target distance 18.0
model initialize at round 1672
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.2603573 ,  5.09956878]), 'previousTarget': array([15.80368799,  5.63557441]), 'currentState': array([ 3.5, 20.5,  0. ]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1856627304455365
{'scaleFactor': 20, 'currentTarget': array([38.93200844, 24.15207355]), 'previousTarget': array([38.43308009, 23.65101313]), 'currentState': array([53.        , 38.36796655,  0.        ]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 20.0}
episode index:1673
target Thresh 31.99999861166816
target distance 9.0
model initialize at round 1673
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 24.]), 'previousTarget': array([ 8., 24.]), 'currentState': array([17.5, 28.5,  0. ]), 'targetState': array([ 8, 24], dtype=int32), 'currentDistance': 10.511898020814428}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.185551820809667
{'scaleFactor': 20, 'currentTarget': array([52.24654175, 64.49683482]), 'previousTarget': array([51.74121873, 64.00265302]), 'currentState': array([67., 78.,  0.]), 'targetState': array([ 8, 24], dtype=int32), 'currentDistance': 20.0}
episode index:1674
target Thresh 31.999998625482295
target distance 4.0
model initialize at round 1674
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  8.]), 'previousTarget': array([17.,  8.]), 'currentState': array([15.5       , 11.50000009,  0.        ]), 'targetState': array([17,  8], dtype=int32), 'currentDistance': 3.807886635109884}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18544104360321348
{'scaleFactor': 20, 'currentTarget': array([50.71067685, 41.01254937]), 'previousTarget': array([50.20911962, 40.5141397 ]), 'currentState': array([65.        , 55.00594938,  0.        ]), 'targetState': array([17,  8], dtype=int32), 'currentDistance': 20.0}
episode index:1675
target Thresh 31.999998639158974
target distance 11.0
model initialize at round 1675
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 16.]), 'previousTarget': array([19., 16.]), 'currentState': array([12.5, 26.5,  0. ]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 12.34908903522836}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18533039858913042
{'scaleFactor': 20, 'currentTarget': array([47.97849878, 45.47502472]), 'previousTarget': array([47.47991173, 44.97363571]), 'currentState': array([62.        , 59.73677438,  0.        ]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 20.0}
episode index:1676
target Thresh 31.99999865269957
target distance 20.0
model initialize at round 1676
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3.64708245, 10.07891249]), 'previousTarget': array([ 3.0992562 , 10.00992562]), 'currentState': array([23.5, 12.5,  0. ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18521988553093774
{'scaleFactor': 20, 'currentTarget': array([56.94512304, 50.07351997]), 'previousTarget': array([56.43090429, 49.59268447]), 'currentState': array([73., 62.,  0.]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 20.0}
episode index:1677
target Thresh 31.999998666105434
target distance 1.0
model initialize at round 1677
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 12.]), 'previousTarget': array([20., 12.]), 'currentState': array([19.5       , 12.50058833,  0.        ]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 0.707522913958043}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.18570545174933406
{'scaleFactor': 20, 'currentTarget': array([20., 12.]), 'previousTarget': array([20., 12.]), 'currentState': array([19.5       , 12.50058833,  0.        ]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 0.707522913958043}
episode index:1678
target Thresh 31.999998679377907
target distance 9.0
model initialize at round 1678
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([13.5, 24.5,  0. ]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 12.020815280171194}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.18610196891392344
{'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([21.5       , 16.50028089,  0.        ]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 0.7073054260952557}
episode index:1679
target Thresh 31.999998692518314
target distance 17.0
model initialize at round 1679
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 21.]), 'previousTarget': array([ 7., 21.]), 'currentState': array([24.5, 20.5,  0. ]), 'targetState': array([ 7, 21], dtype=int32), 'currentDistance': 17.507141400011676}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18599119393242705
{'scaleFactor': 20, 'currentTarget': array([57.85659332, 58.19362795]), 'previousTarget': array([57.34105445, 57.71490438]), 'currentState': array([74., 70.,  0.]), 'targetState': array([ 7, 21], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:1680
target Thresh 31.999998705527975
target distance 11.0
model initialize at round 1680
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([ 4.5, 24.5,  0. ]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 12.34908903522843}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.18637202524111426
{'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'previousTarget': array([15., 18.]), 'currentState': array([14.        , 17.77570389,  0.        ]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 1.0248457174301362}
episode index:1681
target Thresh 31.999998718408186
target distance 1.0
model initialize at round 1681
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 19.]), 'previousTarget': array([13., 19.]), 'currentState': array([14.5       , 18.49999157,  0.        ]), 'targetState': array([13, 19], dtype=int32), 'currentDistance': 1.5811414971875357}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1862612214211136
{'scaleFactor': 20, 'currentTarget': array([49.57790451, 54.14347036]), 'previousTarget': array([49.07510662, 53.64638304]), 'currentState': array([64.        , 67.99999091,  0.        ]), 'targetState': array([13, 19], dtype=int32), 'currentDistance': 20.0}
episode index:1682
target Thresh 31.99999873116024
target distance 3.0
model initialize at round 1682
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([13.5       , 21.46495005,  0.        ]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 2.5566146451447023}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.1867270786870547
{'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([15.        , 21.71777783,  0.        ]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 1.0390617649361567}
episode index:1683
target Thresh 31.999998743785405
target distance 12.0
model initialize at round 1683
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 15.]), 'previousTarget': array([17., 15.]), 'currentState': array([ 6.5, 26.5,  0. ]), 'targetState': array([17, 15], dtype=int32), 'currentDistance': 15.572411502397383}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.1870922232896984
{'scaleFactor': 20, 'currentTarget': array([17., 15.]), 'previousTarget': array([17., 15.]), 'currentState': array([17.5       , 15.50002435,  0.        ]), 'targetState': array([17, 15], dtype=int32), 'currentDistance': 0.7071239983845267}
episode index:1684
target Thresh 31.99999875628495
target distance 18.0
model initialize at round 1684
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 12.]), 'previousTarget': array([20., 12.]), 'currentState': array([2.5       , 8.49996287, 0.        ]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 17.846575580128402}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.18740714216762636
{'scaleFactor': 20, 'currentTarget': array([20., 12.]), 'previousTarget': array([20., 12.]), 'currentState': array([19.        , 12.10269586,  0.        ]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 1.0052593895234776}
episode index:1685
target Thresh 31.99999876866012
target distance 8.0
model initialize at round 1685
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([10.5,  9.5,  0. ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 11.335784048754746}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18729598727903346
{'scaleFactor': 20, 'currentTarget': array([45.73542858, 44.98136946]), 'previousTarget': array([45.23435939, 44.48245749]), 'currentState': array([60., 59.,  0.]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 20.0}
episode index:1686
target Thresh 31.999998780912158
target distance 4.0
model initialize at round 1686
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 21.]), 'previousTarget': array([ 5., 21.]), 'currentState': array([ 7.5      , 25.4951314,  0.       ]), 'targetState': array([ 5, 21], dtype=int32), 'currentDistance': 5.143559694618604}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18718496416861316
{'scaleFactor': 20, 'currentTarget': array([43.1265072 , 60.58928629]), 'previousTarget': array([42.62908898, 60.08680035]), 'currentState': array([57.        , 74.99505588,  0.        ]), 'targetState': array([ 5, 21], dtype=int32), 'currentDistance': 20.0}
episode index:1687
target Thresh 31.999998793042284
target distance 10.0
model initialize at round 1687
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 15.]), 'previousTarget': array([14., 15.]), 'currentState': array([24.5, 12.5,  0. ]), 'targetState': array([14, 15], dtype=int32), 'currentDistance': 10.793516572461513}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18707407260216258
{'scaleFactor': 20, 'currentTarget': array([58.25543893, 49.66676049]), 'previousTarget': array([57.74152038, 49.18454954]), 'currentState': array([74., 62.,  0.]), 'targetState': array([14, 15], dtype=int32), 'currentDistance': 20.0}
episode index:1688
target Thresh 31.999998805051714
target distance 15.0
model initialize at round 1688
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 18.]), 'previousTarget': array([10., 18.]), 'currentState': array([18.5,  3.5,  0. ]), 'targetState': array([10, 18], dtype=int32), 'currentDistance': 16.807736313971656}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18696331234603342
{'scaleFactor': 20, 'currentTarget': array([50.87624678, 42.66670064]), 'previousTarget': array([50.35014149, 42.21008489]), 'currentState': array([68., 53.,  0.]), 'targetState': array([10, 18], dtype=int32), 'currentDistance': 20.0}
episode index:1689
target Thresh 31.99999881694165
target distance 7.0
model initialize at round 1689
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 18.]), 'previousTarget': array([17., 18.]), 'currentState': array([17.5, 11.5,  0. ]), 'targetState': array([17, 18], dtype=int32), 'currentDistance': 6.519202405202575}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18685268316713044
{'scaleFactor': 20, 'currentTarget': array([51.83629607, 47.95921462]), 'previousTarget': array([51.32569267, 47.47155431]), 'currentState': array([67., 61.,  0.]), 'targetState': array([17, 18], dtype=int32), 'currentDistance': 20.0}
episode index:1690
target Thresh 31.999998828713274
target distance 19.0
model initialize at round 1690
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8.25100648, 20.0707967 ]), 'previousTarget': array([ 8., 20.]), 'currentState': array([27.5, 25.5,  0. ]), 'targetState': array([ 8, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18674218483290977
{'scaleFactor': 20, 'currentTarget': array([61.36053521, 62.53375995]), 'previousTarget': array([60.8492439 , 62.04793858]), 'currentState': array([77., 75.,  0.]), 'targetState': array([ 8, 20], dtype=int32), 'currentDistance': 20.0}
episode index:1691
target Thresh 31.999998840367773
target distance 8.0
model initialize at round 1691
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([11.5, 14.5,  0. ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 10.700467279516461}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18663181711137733
{'scaleFactor': 20, 'currentTarget': array([46.61197079, 50.10810973]), 'previousTarget': array([46.10981484, 49.61034302]), 'currentState': array([61., 64.,  0.]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 20.0}
episode index:1692
target Thresh 31.999998851906305
target distance 6.0
model initialize at round 1692
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([7.5, 3.5, 0. ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 5.700877125495696}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18652157977108708
{'scaleFactor': 20, 'currentTarget': array([42.2569172 , 39.48550743]), 'previousTarget': array([41.75047419, 38.99253952]), 'currentState': array([57., 53.,  0.]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 20.0}
episode index:1693
target Thresh 31.999998863330028
target distance 15.0
model initialize at round 1693
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 20.]), 'previousTarget': array([22., 20.]), 'currentState': array([ 7.5       , 17.49994132,  0.        ]), 'targetState': array([22, 20], dtype=int32), 'currentDistance': 14.71394893994497}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.1868614977962205
{'scaleFactor': 20, 'currentTarget': array([22., 20.]), 'previousTarget': array([22., 20.]), 'currentState': array([21.        , 20.46927683,  0.        ]), 'targetState': array([22, 20], dtype=int32), 'currentDistance': 1.1046360247554852}
episode index:1694
target Thresh 31.999998874640085
target distance 13.0
model initialize at round 1694
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([ 4.5, 17.5,  0. ]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 14.577379737113185}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1867512550246593
{'scaleFactor': 20, 'currentTarget': array([40.1486619 , 34.31866905]), 'previousTarget': array([39.65212684, 33.81534318]), 'currentState': array([54.        , 48.74574196,  0.        ]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 20.0}
episode index:1695
target Thresh 31.999998885837602
target distance 22.0
model initialize at round 1695
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.38621834, 17.59861077]), 'previousTarget': array([11.20119853, 17.45345588]), 'currentState': array([26.5,  4.5,  0. ]), 'targetState': array([ 4, 24], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18664114225636647
{'scaleFactor': 20, 'currentTarget': array([57.53846154, 46.30769231]), 'previousTarget': array([57.01179773, 45.87200046]), 'currentState': array([76., 54.,  0.]), 'targetState': array([ 4, 24], dtype=int32), 'currentDistance': 20.0}
episode index:1696
target Thresh 31.999998896923703
target distance 19.0
model initialize at round 1696
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.67353688, 25.85000343]), 'previousTarget': array([21.07475678, 25.56172689]), 'currentState': array([ 3.5, 17.5,  0. ]), 'targetState': array([22, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.18694568353671012
{'scaleFactor': 20, 'currentTarget': array([22., 26.]), 'previousTarget': array([22., 26.]), 'currentState': array([21.        , 26.54502247,  0.        ]), 'targetState': array([22, 26], dtype=int32), 'currentDistance': 1.1388808084199273}
episode index:1697
target Thresh 31.999998907899496
target distance 15.0
model initialize at round 1697
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 14.]), 'previousTarget': array([11., 14.]), 'currentState': array([26.5, 23.5,  0. ]), 'targetState': array([11, 14], dtype=int32), 'currentDistance': 18.179658962697953}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18683558596101124
{'scaleFactor': 20, 'currentTarget': array([61.19089372, 59.55788814]), 'previousTarget': array([60.68561931, 59.06370123]), 'currentState': array([76., 73.,  0.]), 'targetState': array([11, 14], dtype=int32), 'currentDistance': 20.0}
episode index:1698
target Thresh 31.999998918766078
target distance 16.0
model initialize at round 1698
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 20.]), 'previousTarget': array([14., 20.]), 'currentState': array([11.5,  4.5,  0. ]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 15.700318468107573}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18672561798810894
{'scaleFactor': 20, 'currentTarget': array([44.79551178, 42.27760427]), 'previousTarget': array([44.27262339, 41.80930933]), 'currentState': array([61., 54.,  0.]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 20.0}
episode index:1699
target Thresh 31.999998929524534
target distance 20.0
model initialize at round 1699
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.72731807, 18.3402996 ]), 'previousTarget': array([20.14985851, 18.71008489]), 'currentState': array([ 3.5, 28.5,  0. ]), 'targetState': array([23, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.18702133767509774
{'scaleFactor': 20, 'currentTarget': array([23., 17.]), 'previousTarget': array([23., 17.]), 'currentState': array([22.        , 17.63642392,  0.        ]), 'targetState': array([23, 17], dtype=int32), 'currentDistance': 1.1853418947653231}
episode index:1700
target Thresh 31.999998940175946
target distance 17.0
model initialize at round 1700
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([ 5.5       , 11.50964561,  0.        ]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 16.507868991752403}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.1873419032435747
{'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([21.        , 11.64190368,  0.        ]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 1.1882930317804798}
episode index:1701
target Thresh 31.99999895072137
target distance 8.0
model initialize at round 1701
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([26.5,  5.5,  0. ]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 8.86002257333475}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1872318316200473
{'scaleFactor': 20, 'currentTarget': array([61.10861119, 41.64909968]), 'previousTarget': array([60.60195202, 41.15653094]), 'currentState': array([76., 55.,  0.]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 20.0}
episode index:1702
target Thresh 31.999998961161868
target distance 20.0
model initialize at round 1702
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.25938847,  9.52020262]), 'previousTarget': array([11.94427191, 10.11145618]), 'currentState': array([ 3.5, 27.5,  0. ]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1871218892644278
{'scaleFactor': 20, 'currentTarget': array([41.25412878, 46.93820528]), 'previousTarget': array([40.78080901, 46.41887924]), 'currentState': array([53.       , 63.1256853,  0.       ]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 20.0}
episode index:1703
target Thresh 31.999998971498478
target distance 5.0
model initialize at round 1703
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 18.]), 'previousTarget': array([ 3., 18.]), 'currentState': array([ 2.5, 13.5,  0. ]), 'targetState': array([ 3, 18], dtype=int32), 'currentDistance': 4.52769256906863}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18701207594913177
{'scaleFactor': 20, 'currentTarget': array([37.26940223, 49.47190001]), 'previousTarget': array([36.76322514, 48.97862926]), 'currentState': array([52., 63.,  0.]), 'targetState': array([ 3, 18], dtype=int32), 'currentDistance': 20.0}
episode index:1704
target Thresh 31.99999898173224
target distance 15.0
model initialize at round 1704
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  7.]), 'previousTarget': array([17.,  7.]), 'currentState': array([12.5       , 21.51218084,  0.        ]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 15.193860358315636}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18690239144710882
{'scaleFactor': 20, 'currentTarget': array([50.33241686, 53.40650426]), 'previousTarget': array([49.85674318, 52.88905897]), 'currentState': array([62.        , 69.65050326,  0.        ]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 20.0}
episode index:1705
target Thresh 31.999998991864175
target distance 9.0
model initialize at round 1705
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 19.]), 'previousTarget': array([14., 19.]), 'currentState': array([21.5, 28.5,  0. ]), 'targetState': array([14, 19], dtype=int32), 'currentDistance': 12.103718436910317}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18679283553184087
{'scaleFactor': 20, 'currentTarget': array([57.10375958, 63.6161722 ]), 'previousTarget': array([56.60591559, 63.11408959]), 'currentState': array([71., 78.,  0.]), 'targetState': array([14, 19], dtype=int32), 'currentDistance': 20.0}
episode index:1706
target Thresh 31.999999001895294
target distance 2.0
model initialize at round 1706
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 23.]), 'previousTarget': array([27., 23.]), 'currentState': array([25.5       , 23.28726119,  0.        ]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 1.5272586520129436}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.1872633728279558
{'scaleFactor': 20, 'currentTarget': array([27., 23.]), 'previousTarget': array([27., 23.]), 'currentState': array([26.        , 23.24537905,  0.        ]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 1.029665421230874}
episode index:1707
target Thresh 31.9999990118266
target distance 10.0
model initialize at round 1707
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'previousTarget': array([10., 17.]), 'currentState': array([20.5, 17.5,  0. ]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 10.511898020814401}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18715373385089024
{'scaleFactor': 20, 'currentTarget': array([54.63557441, 54.19631201]), 'previousTarget': array([54.12498935, 53.70902476]), 'currentState': array([70., 67.,  0.]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 20.0}
episode index:1708
target Thresh 31.99999902165909
target distance 22.0
model initialize at round 1708
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.31438069, 16.93629877]), 'previousTarget': array([18.79880147, 16.45345588]), 'currentState': array([4.5, 3.5, 0. ]), 'targetState': array([26, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.18743174924418055
{'scaleFactor': 20, 'currentTarget': array([26., 23.]), 'previousTarget': array([26., 23.]), 'currentState': array([25.        , 23.63673888,  0.        ]), 'targetState': array([26, 23], dtype=int32), 'currentDistance': 1.1855110303877543}
episode index:1709
target Thresh 31.999999031393745
target distance 19.0
model initialize at round 1709
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3.52020262, 18.74061153]), 'previousTarget': array([ 2.92524322, 18.43827311]), 'currentState': array([21.5, 27.5,  0. ]), 'targetState': array([ 2, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18732214003409622
{'scaleFactor': 20, 'currentTarget': array([55.79932823, 64.00232414]), 'previousTarget': array([55.29138434, 63.5116202 ]), 'currentState': array([71., 77.,  0.]), 'targetState': array([ 2, 18], dtype=int32), 'currentDistance': 20.0}
episode index:1710
target Thresh 31.999999041031536
target distance 3.0
model initialize at round 1710
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 19.]), 'previousTarget': array([20., 19.]), 'currentState': array([23.5, 16.5,  0. ]), 'targetState': array([20, 19], dtype=int32), 'currentDistance': 4.301162633521332}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1872126589469927
{'scaleFactor': 20, 'currentTarget': array([58.0362468 , 52.73025659]), 'previousTarget': array([57.5282364 , 52.23929509]), 'currentState': array([73., 66.,  0.]), 'targetState': array([20, 19], dtype=int32), 'currentDistance': 20.0}
episode index:1711
target Thresh 31.999999050573432
target distance 3.0
model initialize at round 1711
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 16.]), 'previousTarget': array([24., 16.]), 'currentState': array([21.5       , 18.50024128,  0.        ]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 3.5357045204916626}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.1876644015585891
{'scaleFactor': 20, 'currentTarget': array([24., 16.]), 'previousTarget': array([24., 16.]), 'currentState': array([23.5       , 16.50195408,  0.        ]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 0.7084898708502384}
episode index:1712
target Thresh 31.999999060020386
target distance 23.0
model initialize at round 1712
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.16827891,  9.22377759]), 'previousTarget': array([12.95156206,  8.64765455]), 'currentState': array([18.5, 28.5,  0. ]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18755484849288065
{'scaleFactor': 20, 'currentTarget': array([55.82680605, 62.13137217]), 'previousTarget': array([55.34286886, 61.61906292]), 'currentState': array([68., 78.,  0.]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 20.0}
episode index:1713
target Thresh 31.999999069373338
target distance 10.0
model initialize at round 1713
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 19.]), 'previousTarget': array([ 3., 19.]), 'currentState': array([13.5, 24.5,  0. ]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 11.853269591129807}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1874454232603877
{'scaleFactor': 20, 'currentTarget': array([48.2569172 , 60.48550743]), 'previousTarget': array([47.75177381, 59.99112054]), 'currentState': array([63., 74.,  0.]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 20.0}
episode index:1714
target Thresh 31.99999907863323
target distance 20.0
model initialize at round 1714
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.3402996 , 19.72731807]), 'previousTarget': array([17.361625  , 19.52431817]), 'currentState': array([27.5,  2.5,  0. ]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18733612563749538
{'scaleFactor': 20, 'currentTarget': array([59.05300068, 43.17360689]), 'previousTarget': array([58.52320347, 42.73445458]), 'currentState': array([77., 52.,  0.]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 20.0}
episode index:1715
target Thresh 31.99999908780098
target distance 6.0
model initialize at round 1715
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'previousTarget': array([13., 22.]), 'currentState': array([11.5, 16.5,  0. ]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 5.70087712549559}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18722695540110987
{'scaleFactor': 20, 'currentTarget': array([46.2569172 , 52.48550743]), 'previousTarget': array([45.75047419, 51.99253952]), 'currentState': array([61., 66.,  0.]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 20.0}
episode index:1716
target Thresh 31.999999096877513
target distance 22.0
model initialize at round 1716
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9.81001716, 27.36614761]), 'previousTarget': array([ 9.70226409, 26.81660336]), 'currentState': array([7.5, 7.5, 0. ]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1871179123286573
{'scaleFactor': 20, 'currentTarget': array([39.81796912, 46.7638965 ]), 'previousTarget': array([39.28515567, 46.31917809]), 'currentState': array([57., 57.,  0.]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 20.0}
episode index:1717
target Thresh 31.999999105863733
target distance 7.0
model initialize at round 1717
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 26.]), 'previousTarget': array([ 3., 26.]), 'currentState': array([ 9.5, 19.5,  0. ]), 'targetState': array([ 3, 26], dtype=int32), 'currentDistance': 9.19238815542512}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1870089961980818
{'scaleFactor': 20, 'currentTarget': array([43.13699572, 56.81947886]), 'previousTarget': array([42.62096788, 56.34038081]), 'currentState': array([59., 69.,  0.]), 'targetState': array([ 3, 26], dtype=int32), 'currentDistance': 20.0}
episode index:1718
target Thresh 31.999999114760538
target distance 2.0
model initialize at round 1718
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  5.]), 'previousTarget': array([12.,  5.]), 'currentState': array([13.5,  3.5,  0. ]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 2.1213203435597023}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18690020678784441
{'scaleFactor': 20, 'currentTarget': array([48.43600015, 39.29270602]), 'previousTarget': array([47.93176627, 38.7972059 ]), 'currentState': array([63., 53.,  0.]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 20.0}
episode index:1719
target Thresh 31.999999123568816
target distance 8.0
model initialize at round 1719
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.5       , 18.47806072,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 8.492791862036263}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18679154387692126
{'scaleFactor': 20, 'currentTarget': array([52.93494257, 52.80871386]), 'previousTarget': array([52.44531777, 52.29976844]), 'currentState': array([66.        , 67.95150998,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 20.0}
episode index:1720
target Thresh 31.999999132289453
target distance 8.0
model initialize at round 1720
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'previousTarget': array([15., 19.]), 'currentState': array([18.5, 11.5,  0. ]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 8.276472678623387}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1866830072448022
{'scaleFactor': 20, 'currentTarget': array([52.32508247, 48.57836724]), 'previousTarget': array([51.80999761, 48.09742668]), 'currentState': array([68., 61.,  0.]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 20.0}
episode index:1721
target Thresh 31.999999140923318
target distance 21.0
model initialize at round 1721
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.0252491 , 20.30150667]), 'previousTarget': array([ 9.85786438, 20.14213562]), 'currentState': array([24.5,  6.5,  0. ]), 'targetState': array([ 3, 27], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1865745966714893
{'scaleFactor': 20, 'currentTarget': array([55.48490971, 48.43749833]), 'previousTarget': array([54.95779869, 48.00421649]), 'currentState': array([74., 56.,  0.]), 'targetState': array([ 3, 27], dtype=int32), 'currentDistance': 20.0}
episode index:1722
target Thresh 31.99999914947127
target distance 8.0
model initialize at round 1722
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 15.]), 'previousTarget': array([16., 15.]), 'currentState': array([14.5,  7.5,  0. ]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 7.648529270389165}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18646631193749538
{'scaleFactor': 20, 'currentTarget': array([48.94846611, 43.82990784]), 'previousTarget': array([48.4386492 , 43.34113561]), 'currentState': array([64., 57.,  0.]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 20.0}
episode index:1723
target Thresh 31.999999157934173
target distance 10.0
model initialize at round 1723
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 15.]), 'previousTarget': array([12., 15.]), 'currentState': array([19.5, 25.5,  0. ]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 12.903487900564048}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18635815282384255
{'scaleFactor': 20, 'currentTarget': array([55.22501076, 60.50001133]), 'previousTarget': array([54.7282141 , 59.99696883]), 'currentState': array([69., 75.,  0.]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 20.0}
episode index:1724
target Thresh 31.99999916631287
target distance 13.0
model initialize at round 1724
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  3.]), 'previousTarget': array([13.,  3.]), 'currentState': array([26.5,  9.5,  0. ]), 'targetState': array([13,  3], dtype=int32), 'currentDistance': 14.983324063771802}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1862501191120606
{'scaleFactor': 20, 'currentTarget': array([61.05181363, 45.71272322]), 'previousTarget': array([60.54521474, 45.22015069]), 'currentState': array([76., 59.,  0.]), 'targetState': array([13,  3], dtype=int32), 'currentDistance': 20.0}
episode index:1725
target Thresh 31.999999174608195
target distance 21.0
model initialize at round 1725
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.04048723, 23.44667447]), 'previousTarget': array([26.04869701, 22.97736275]), 'currentState': array([27.5,  3.5,  0. ]), 'targetState': array([26, 24], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18614221058418573
{'scaleFactor': 20, 'currentTarget': array([59.61419762, 43.11395551]), 'previousTarget': array([59.08232321, 42.67022201]), 'currentState': array([77., 53.,  0.]), 'targetState': array([26, 24], dtype=int32), 'currentDistance': 20.0}
episode index:1726
target Thresh 31.999999182820982
target distance 16.0
model initialize at round 1726
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8.02085402, 4.83524419]), 'previousTarget': array([7.47772  , 4.3881475]), 'currentState': array([23.5, 17.5,  0. ]), 'targetState': array([7, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18603442702275885
{'scaleFactor': 20, 'currentTarget': array([58.53288912, 53.19048507]), 'previousTarget': array([58.03038173, 52.69311233]), 'currentState': array([73., 67.,  0.]), 'targetState': array([7, 4], dtype=int32), 'currentDistance': 20.0}
episode index:1727
target Thresh 31.99999919095205
target distance 7.0
model initialize at round 1727
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 22.]), 'previousTarget': array([ 7., 22.]), 'currentState': array([11.5, 15.5,  0. ]), 'targetState': array([ 7, 22], dtype=int32), 'currentDistance': 7.905694150420929}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1859267682108244
{'scaleFactor': 20, 'currentTarget': array([45.35439711, 52.54146436]), 'previousTarget': array([44.83987962, 52.05971746]), 'currentState': array([61., 65.,  0.]), 'targetState': array([ 7, 22], dtype=int32), 'currentDistance': 20.0}
episode index:1728
target Thresh 31.99999919900221
target distance 14.0
model initialize at round 1728
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 20.]), 'previousTarget': array([11., 20.]), 'currentState': array([25.5,  8.5,  0. ]), 'targetState': array([11, 20], dtype=int32), 'currentDistance': 18.50675552332176}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18581923393192862
{'scaleFactor': 20, 'currentTarget': array([57.80290512, 47.78922492]), 'previousTarget': array([57.2787705 , 47.32998258]), 'currentState': array([75., 58.,  0.]), 'targetState': array([11, 20], dtype=int32), 'currentDistance': 20.0}
episode index:1729
target Thresh 31.99999920697227
target distance 16.0
model initialize at round 1729
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 20.]), 'previousTarget': array([ 3., 20.]), 'currentState': array([19.5, 17.5,  0. ]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 16.6883192682787}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18571182397011823
{'scaleFactor': 20, 'currentTarget': array([52.70866906, 55.39859766]), 'previousTarget': array([52.19175268, 54.92238931]), 'currentState': array([69., 67.,  0.]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 20.0}
episode index:1730
target Thresh 31.999999214863028
target distance 5.0
model initialize at round 1730
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([ 6.5       , 15.50613645,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 6.368301632359016}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.1861376084129015
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.5       , 11.61905065,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.795753548204241}
episode index:1731
target Thresh 31.99999922267527
target distance 9.0
model initialize at round 1731
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  9.]), 'previousTarget': array([19.,  9.]), 'currentState': array([10.5,  2.5,  0. ]), 'targetState': array([19,  9], dtype=int32), 'currentDistance': 10.700467279516287}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.18652670815090866
{'scaleFactor': 20, 'currentTarget': array([19.,  9.]), 'previousTarget': array([19.,  9.]), 'currentState': array([18.        ,  9.65434301,  0.        ]), 'targetState': array([19,  9], dtype=int32), 'currentDistance': 1.1950584811707137}
episode index:1732
target Thresh 31.99999923040978
target distance 12.0
model initialize at round 1732
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 16.]), 'previousTarget': array([ 9., 16.]), 'currentState': array([21.5, 17.5,  0. ]), 'targetState': array([ 9, 16], dtype=int32), 'currentDistance': 12.589678312014259}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18641907589000217
{'scaleFactor': 20, 'currentTarget': array([55.5542065 , 54.29458922]), 'previousTarget': array([55.04327386, 53.80789154]), 'currentState': array([71., 67.,  0.]), 'targetState': array([ 9, 16], dtype=int32), 'currentDistance': 20.0}
episode index:1733
target Thresh 31.999999238067332
target distance 12.0
model initialize at round 1733
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 27.]), 'previousTarget': array([17., 27.]), 'currentState': array([26.5, 15.5,  0. ]), 'targetState': array([17, 27], dtype=int32), 'currentDistance': 14.916433890176286}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18631156777241856
{'scaleFactor': 20, 'currentTarget': array([59.1856962, 54.1704484]), 'previousTarget': array([58.66241722, 53.70667771]), 'currentState': array([76., 65.,  0.]), 'targetState': array([17, 27], dtype=int32), 'currentDistance': 20.0}
episode index:1734
target Thresh 31.999999245648688
target distance 9.0
model initialize at round 1734
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 25.]), 'previousTarget': array([ 8., 25.]), 'currentState': array([ 9.5, 16.5,  0. ]), 'targetState': array([ 8, 25], dtype=int32), 'currentDistance': 8.631338250815968}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18620418358350074
{'scaleFactor': 20, 'currentTarget': array([43.41248658, 53.46886176]), 'previousTarget': array([42.89770621, 52.98726934]), 'currentState': array([59., 66.,  0.]), 'targetState': array([ 8, 25], dtype=int32), 'currentDistance': 20.0}
episode index:1735
target Thresh 31.99999925315461
target distance 14.0
model initialize at round 1735
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 24.]), 'previousTarget': array([16., 24.]), 'currentState': array([ 2.5, 13.5,  0. ]), 'targetState': array([16, 24], dtype=int32), 'currentDistance': 17.102631376487025}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.186544976887542
{'scaleFactor': 20, 'currentTarget': array([16., 24.]), 'previousTarget': array([16., 24.]), 'currentState': array([15.        , 24.69825891,  0.        ]), 'targetState': array([16, 24], dtype=int32), 'currentDistance': 1.2196579462501371}
episode index:1736
target Thresh 31.999999260585845
target distance 16.0
model initialize at round 1736
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 29.]), 'previousTarget': array([15., 29.]), 'currentState': array([ 3.5, 13.5,  0. ]), 'targetState': array([15, 29], dtype=int32), 'currentDistance': 19.30025906561872}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18643758196705407
{'scaleFactor': 20, 'currentTarget': array([38.09517373, 49.66410281]), 'previousTarget': array([37.58477585, 49.17573309]), 'currentState': array([53., 63.,  0.]), 'targetState': array([15, 29], dtype=int32), 'currentDistance': 20.0}
episode index:1737
target Thresh 31.99999926794314
target distance 24.0
model initialize at round 1737
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.80716575, 12.59842328]), 'previousTarget': array([13.4000212 , 12.04003392]), 'currentState': array([24.5, 29.5,  0. ]), 'targetState': array([9, 5], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18633031063105462
{'scaleFactor': 20, 'currentTarget': array([60.80118019, 63.97365129]), 'previousTarget': array([60.30820219, 63.46748622]), 'currentState': array([74., 79.,  0.]), 'targetState': array([9, 5], dtype=int32), 'currentDistance': 20.0}
episode index:1738
target Thresh 31.99999927522723
target distance 17.0
model initialize at round 1738
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([26.5, 14.5,  0. ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 18.069310999592748}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1862231626663444
{'scaleFactor': 20, 'currentTarget': array([60.4280763 , 51.44949434]), 'previousTarget': array([59.91697501, 50.96328065]), 'currentState': array([76., 64.,  0.]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 20.0}
episode index:1739
target Thresh 31.999999282438836
target distance 4.0
model initialize at round 1739
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 16.]), 'previousTarget': array([17., 16.]), 'currentState': array([13.5       , 15.49538666,  0.        ]), 'targetState': array([17, 16], dtype=int32), 'currentDistance': 3.5361892798329584}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.1866572184058471
{'scaleFactor': 20, 'currentTarget': array([17., 16.]), 'previousTarget': array([17., 16.]), 'currentState': array([16.5       , 16.55891782,  0.        ]), 'targetState': array([17, 16], dtype=int32), 'currentDistance': 0.749926083037444}
episode index:1740
target Thresh 31.99999928957869
target distance 10.0
model initialize at round 1740
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 24.]), 'previousTarget': array([13., 24.]), 'currentState': array([23.5, 25.5,  0. ]), 'targetState': array([13, 24], dtype=int32), 'currentDistance': 10.606601717798306}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18655000575885922
{'scaleFactor': 20, 'currentTarget': array([57.76121364, 62.0470316 ]), 'previousTarget': array([57.25173301, 61.55819356]), 'currentState': array([73., 75.,  0.]), 'targetState': array([13, 24], dtype=int32), 'currentDistance': 20.0}
episode index:1741
target Thresh 31.9999992966475
target distance 15.0
model initialize at round 1741
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 22.]), 'previousTarget': array([22., 22.]), 'currentState': array([21.5,  7.5,  0. ]), 'targetState': array([22, 22], dtype=int32), 'currentDistance': 14.50861812854691}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18644291620331455
{'scaleFactor': 20, 'currentTarget': array([54.72533058, 45.37523613]), 'previousTarget': array([54.20265704, 44.9070447 ]), 'currentState': array([71., 57.,  0.]), 'targetState': array([22, 22], dtype=int32), 'currentDistance': 20.0}
episode index:1742
target Thresh 31.999999303645975
target distance 15.0
model initialize at round 1742
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([21.5,  3.5,  0. ]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 14.713938969562179}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18633594952735164
{'scaleFactor': 20, 'currentTarget': array([54.40824505, 41.83247263]), 'previousTarget': array([53.88385449, 41.36879573]), 'currentState': array([71., 53.,  0.]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 20.0}
episode index:1743
target Thresh 31.99999931057481
target distance 4.0
model initialize at round 1743
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 26.]), 'previousTarget': array([23., 26.]), 'currentState': array([27.5, 26.5,  0. ]), 'targetState': array([23, 26], dtype=int32), 'currentDistance': 4.527692569068798}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18622910551959512
{'scaleFactor': 20, 'currentTarget': array([62.32478652, 62.41183937]), 'previousTarget': array([61.81972111, 61.91731206]), 'currentState': array([77., 76.,  0.]), 'targetState': array([23, 26], dtype=int32), 'currentDistance': 20.0}
episode index:1744
target Thresh 31.999999317434707
target distance 22.0
model initialize at round 1744
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.42233749,  9.88830123]), 'previousTarget': array([21.43242207,  9.49734288]), 'currentState': array([17.5       , 29.49991179,  0.        ]), 'targetState': array([22,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18612238396915412
{'scaleFactor': 20, 'currentTarget': array([54.40425106, 46.96676923]), 'previousTarget': array([53.92039499, 46.45369394]), 'currentState': array([67.        , 62.50211952,  0.        ]), 'targetState': array([22,  7], dtype=int32), 'currentDistance': 20.0}
episode index:1745
target Thresh 31.999999324226348
target distance 18.0
model initialize at round 1745
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8.97063166, 24.24399708]), 'previousTarget': array([ 8.85786438, 24.14213562]), 'currentState': array([23.5, 10.5,  0. ]), 'targetState': array([ 5, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1860157846656208
{'scaleFactor': 20, 'currentTarget': array([54.90362596, 51.48405927]), 'previousTarget': array([54.3763372 , 51.04229069]), 'currentState': array([73., 60.,  0.]), 'targetState': array([ 5, 28], dtype=int32), 'currentDistance': 20.0}
episode index:1746
target Thresh 31.999999330950406
target distance 10.0
model initialize at round 1746
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([14.5     , 23.495233,  0.      ]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 10.507136418173895}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18590930739906922
{'scaleFactor': 20, 'currentTarget': array([50.71445235, 54.31338163]), 'previousTarget': array([50.22279591, 53.80597103]), 'currentState': array([64.        , 69.26310483,  0.        ]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 20.0}
episode index:1747
target Thresh 31.999999337607562
target distance 17.0
model initialize at round 1747
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 19.]), 'previousTarget': array([ 6., 19.]), 'currentState': array([23.5, 11.5,  0. ]), 'targetState': array([ 6, 19], dtype=int32), 'currentDistance': 19.039432764659814}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18580295196005372
{'scaleFactor': 20, 'currentTarget': array([56.05425823, 50.3772962 ]), 'previousTarget': array([55.53287081, 49.9114908 ]), 'currentState': array([73., 61.,  0.]), 'targetState': array([ 6, 19], dtype=int32), 'currentDistance': 20.0}
episode index:1748
target Thresh 31.999999344198475
target distance 13.0
model initialize at round 1748
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([2.5       , 2.60403332, 0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 12.577707381255193}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.18615047130349774
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.        ,  4.90745724,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.35036240958531}
episode index:1749
target Thresh 31.99999935072381
target distance 9.0
model initialize at round 1749
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 17.]), 'previousTarget': array([14., 17.]), 'currentState': array([8.5, 8.5, 0. ]), 'targetState': array([14, 17], dtype=int32), 'currentDistance': 10.124228365658183}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18604409960561002
{'scaleFactor': 20, 'currentTarget': array([43.3678408 , 44.36548802]), 'previousTarget': array([42.86212051, 43.87162944]), 'currentState': array([58., 58.,  0.]), 'targetState': array([14, 17], dtype=int32), 'currentDistance': 20.0}
episode index:1750
target Thresh 31.999999357184215
target distance 10.0
model initialize at round 1750
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 12.]), 'previousTarget': array([17., 12.]), 'currentState': array([27.5, 21.5,  0. ]), 'targetState': array([17, 12], dtype=int32), 'currentDistance': 14.15980225850641}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18593784940594948
{'scaleFactor': 20, 'currentTarget': array([62.7395264 , 56.97720096]), 'previousTarget': array([62.23852788, 56.47821649]), 'currentState': array([77., 71.,  0.]), 'targetState': array([17, 12], dtype=int32), 'currentDistance': 20.0}
episode index:1751
target Thresh 31.99999936358034
target distance 12.0
model initialize at round 1751
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.5       , 16.49925414,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 12.509250736485003}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1858317204964712
{'scaleFactor': 20, 'currentTarget': array([52.82774647, 46.07665866]), 'previousTarget': array([52.33714055, 45.56844164]), 'currentState': array([66.        , 61.12630108,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 20.0}
episode index:1752
target Thresh 31.99999936991282
target distance 6.0
model initialize at round 1752
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 16.]), 'previousTarget': array([ 7., 16.]), 'currentState': array([ 3.5, 10.5,  0. ]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 6.519202405202583}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.185725712669605
{'scaleFactor': 20, 'currentTarget': array([38.54716276, 46.17554656]), 'previousTarget': array([38.04371313, 45.67915389]), 'currentState': array([53.        , 59.99999937,  0.        ]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 20.0}
episode index:1753
target Thresh 31.999999376182295
target distance 11.0
model initialize at round 1753
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 21.]), 'previousTarget': array([ 8., 21.]), 'currentState': array([18.5, 10.5,  0. ]), 'targetState': array([ 8, 21], dtype=int32), 'currentDistance': 14.8492424049175}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18561982571825403
{'scaleFactor': 20, 'currentTarget': array([51.23112767, 49.10023299]), 'previousTarget': array([50.70859686, 48.63497444]), 'currentState': array([68., 60.,  0.]), 'targetState': array([ 8, 21], dtype=int32), 'currentDistance': 20.0}
episode index:1754
target Thresh 31.999999382389383
target distance 12.0
model initialize at round 1754
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 10.]), 'previousTarget': array([20., 10.]), 'currentState': array([8.5       , 9.51572073, 0.        ]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 11.510192284064372}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.18597544283648937
{'scaleFactor': 20, 'currentTarget': array([20., 10.]), 'previousTarget': array([20., 10.]), 'currentState': array([19.        , 10.77044328,  0.        ]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 1.2623719140883483}
episode index:1755
target Thresh 31.99999938853471
target distance 17.0
model initialize at round 1755
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 10.]), 'previousTarget': array([18., 10.]), 'currentState': array([11.5       , 27.24016429,  0.        ]), 'targetState': array([18, 10], dtype=int32), 'currentDistance': 18.424800267928}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18586953426995378
{'scaleFactor': 20, 'currentTarget': array([49.01270155, 51.41870786]), 'previousTarget': array([48.53537379, 50.90175681]), 'currentState': array([61.        , 67.42822615,  0.        ]), 'targetState': array([18, 10], dtype=int32), 'currentDistance': 20.0}
episode index:1756
target Thresh 31.999999394618893
target distance 9.0
model initialize at round 1756
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 14.]), 'previousTarget': array([24., 14.]), 'currentState': array([26.5       , 23.49204004,  0.        ]), 'targetState': array([24, 14], dtype=int32), 'currentDistance': 9.81574368476796}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1857637462595554
{'scaleFactor': 20, 'currentTarget': array([62.77017063, 57.95490364]), 'previousTarget': array([62.27868854, 57.44739477]), 'currentState': array([76.        , 72.95395744,  0.        ]), 'targetState': array([24, 14], dtype=int32), 'currentDistance': 20.0}
episode index:1757
target Thresh 31.999999400642537
target distance 17.0
model initialize at round 1757
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.97914598,  2.83524419]), 'previousTarget': array([24.43860471,  3.28585494]), 'currentState': array([ 9.5, 15.5,  0. ]), 'targetState': array([26,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.18607463341734543
{'scaleFactor': 20, 'currentTarget': array([26.,  2.]), 'previousTarget': array([26.,  2.]), 'currentState': array([25.        ,  2.73957704,  0.        ]), 'targetState': array([26,  2], dtype=int32), 'currentDistance': 1.2437741756134952}
episode index:1758
target Thresh 31.999999406606243
target distance 14.0
model initialize at round 1758
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 23.]), 'previousTarget': array([20., 23.]), 'currentState': array([25.5,  9.5,  0. ]), 'targetState': array([20, 23], dtype=int32), 'currentDistance': 14.577379737113207}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18596884908908087
{'scaleFactor': 20, 'currentTarget': array([58.26596966, 48.04681651]), 'previousTarget': array([57.74167347, 47.58402584]), 'currentState': array([75., 59.,  0.]), 'targetState': array([20, 23], dtype=int32), 'currentDistance': 20.0}
episode index:1759
target Thresh 31.999999412510608
target distance 22.0
model initialize at round 1759
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.07581251, 20.40273692]), 'previousTarget': array([23.50265712, 20.56757793]), 'currentState': array([ 4.5, 24.5,  0. ]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.1862394815844757
{'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([25.        , 20.97709228,  0.        ]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 1.398109199179739}
episode index:1760
target Thresh 31.999999418356225
target distance 11.0
model initialize at round 1760
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 18.]), 'previousTarget': array([23., 18.]), 'currentState': array([23.5       , 29.49104258,  0.        ]), 'targetState': array([23, 18], dtype=int32), 'currentDistance': 11.501915478365122}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18613372378686951
{'scaleFactor': 20, 'currentTarget': array([59.89086472, 60.50652198]), 'previousTarget': array([59.40083803, 59.99787204]), 'currentState': array([73.        , 75.61117597,  0.        ]), 'targetState': array([23, 18], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:1761
target Thresh 31.999999424143677
target distance 11.0
model initialize at round 1761
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([12.5       , 22.48495266,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 11.753898824051102}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18602808603216642
{'scaleFactor': 20, 'currentTarget': array([49.12110137, 51.53932666]), 'previousTarget': array([48.63392642, 51.02854126]), 'currentState': array([62.        , 66.84076352,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:1762
target Thresh 31.999999429873544
target distance 12.0
model initialize at round 1762
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([12.5, 10.5,  0. ]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 11.510864433221261}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.185922568116096
{'scaleFactor': 20, 'currentTarget': array([46.07675612, 47.89833465]), 'previousTarget': array([45.55815704, 47.42284624]), 'currentState': array([62., 60.,  0.]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 20.0}
episode index:1763
target Thresh 31.999999435546396
target distance 14.0
model initialize at round 1763
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 5.]), 'previousTarget': array([7., 5.]), 'currentState': array([21.5, 11.5,  0. ]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 15.890248582070807}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18581716983485105
{'scaleFactor': 20, 'currentTarget': array([55.94846611, 47.82990784]), 'previousTarget': array([55.44112317, 47.3383045 ]), 'currentState': array([71., 61.,  0.]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 20.0}
episode index:1764
target Thresh 31.999999441162803
target distance 12.0
model initialize at round 1764
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 7.5       , 22.49492398,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 12.504924039836931}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18571189098508625
{'scaleFactor': 20, 'currentTarget': array([44.24548755, 54.9861864 ]), 'previousTarget': array([43.75863114, 54.47531385]), 'currentState': array([57.        , 70.39145862,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 20.0}
episode index:1765
target Thresh 31.999999446723326
target distance 25.0
model initialize at round 1765
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.95181452,  8.27997224]), 'previousTarget': array([13.84018998,  8.25118736]), 'currentState': array([17.5       , 27.96271544,  0.        ]), 'targetState': array([13,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1856067313639169
{'scaleFactor': 20, 'currentTarget': array([54.64768689, 56.03472384]), 'previousTarget': array([54.16300543, 55.52270638]), 'currentState': array([67.        , 71.76432526,  0.        ]), 'targetState': array([13,  3], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:1766
target Thresh 31.999999452228522
target distance 15.0
model initialize at round 1766
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 21.]), 'previousTarget': array([13., 21.]), 'currentState': array([6.5, 6.5, 0. ]), 'targetState': array([13, 21], dtype=int32), 'currentDistance': 15.89024858207071}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1855016907689175
{'scaleFactor': 20, 'currentTarget': array([40.48875993, 43.37457203]), 'previousTarget': array([39.97213364, 42.89502613]), 'currentState': array([56., 56.,  0.]), 'targetState': array([13, 21], dtype=int32), 'currentDistance': 20.0}
episode index:1767
target Thresh 31.99999945767894
target distance 20.0
model initialize at round 1767
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.88419958,  8.05615617]), 'previousTarget': array([22.46924689,  8.61536159]), 'currentState': array([11.5, 24.5,  0. ]), 'targetState': array([25,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1853967689981206
{'scaleFactor': 20, 'currentTarget': array([48.97324704, 36.85316089]), 'previousTarget': array([48.49995584, 36.3330943 ]), 'currentState': array([61.        , 52.83306118,  0.        ]), 'targetState': array([25,  5], dtype=int32), 'currentDistance': 20.0}
episode index:1768
target Thresh 31.999999463075124
target distance 18.0
model initialize at round 1768
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 29.]), 'previousTarget': array([ 9., 29.]), 'currentState': array([ 6.5, 11.5,  0. ]), 'targetState': array([ 9, 29], dtype=int32), 'currentDistance': 17.67766952966361}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1852919658500154
{'scaleFactor': 20, 'currentTarget': array([39.46801505, 49.74418046]), 'previousTarget': array([38.94162619, 49.2830371 ]), 'currentState': array([56., 61.,  0.]), 'targetState': array([ 9, 29], dtype=int32), 'currentDistance': 20.0}
episode index:1769
target Thresh 31.999999468417617
target distance 14.0
model initialize at round 1769
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([ 3.5, 15.5,  0. ]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 14.983324063771628}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18518728112354646
{'scaleFactor': 20, 'currentTarget': array([37.66484654, 52.16126687]), 'previousTarget': array([37.15038662, 51.67855823]), 'currentState': array([53., 65.,  0.]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 20.0}
episode index:1770
target Thresh 31.99999947370695
target distance 21.0
model initialize at round 1770
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9.27647243, 14.19603151]), 'previousTarget': array([ 9.0913656 , 14.12086431]), 'currentState': array([25.5,  2.5,  0. ]), 'targetState': array([ 4, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1850827146181125
{'scaleFactor': 20, 'currentTarget': array([56.96161731, 43.36190125]), 'previousTarget': array([56.43569141, 42.91625053]), 'currentState': array([75., 52.,  0.]), 'targetState': array([ 4, 18], dtype=int32), 'currentDistance': 20.0}
episode index:1771
target Thresh 31.999999478943653
target distance 13.0
model initialize at round 1771
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  8.]), 'previousTarget': array([25.,  8.]), 'currentState': array([12.5, 15.5,  0. ]), 'targetState': array([25,  8], dtype=int32), 'currentDistance': 14.577379737113143}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.18542165108887382
{'scaleFactor': 20, 'currentTarget': array([25.,  8.]), 'previousTarget': array([25.,  8.]), 'currentState': array([24.5       ,  8.67468405,  0.        ]), 'targetState': array([25,  8], dtype=int32), 'currentDistance': 0.8397610191261087}
episode index:1772
target Thresh 31.99999948412825
target distance 15.0
model initialize at round 1772
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 8.]), 'previousTarget': array([9., 8.]), 'currentState': array([ 4.5       , 23.07368179,  0.        ]), 'targetState': array([9, 8], dtype=int32), 'currentDistance': 15.73104836320123}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.185317070349399
{'scaleFactor': 20, 'currentTarget': array([41.49031933, 48.52884942]), 'previousTarget': array([41.00727526, 48.01527166]), 'currentState': array([54.        , 64.13358875,  0.        ]), 'targetState': array([9, 8], dtype=int32), 'currentDistance': 20.0}
episode index:1773
target Thresh 31.99999948926126
target distance 19.0
model initialize at round 1773
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  7.]), 'previousTarget': array([22.,  7.]), 'currentState': array([25.5       , 25.82024784,  0.        ]), 'targetState': array([22,  7], dtype=int32), 'currentDistance': 19.14292895312471}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1852126075138018
{'scaleFactor': 20, 'currentTarget': array([62.26242334, 55.73905951]), 'previousTarget': array([61.77495562, 55.22871539]), 'currentState': array([75.        , 71.15833773,  0.        ]), 'targetState': array([22,  7], dtype=int32), 'currentDistance': 20.0}
episode index:1774
target Thresh 31.999999494343196
target distance 19.0
model initialize at round 1774
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 23.]), 'previousTarget': array([ 5., 23.]), 'currentState': array([24.5, 20.5,  0. ]), 'targetState': array([ 5, 23], dtype=int32), 'currentDistance': 19.659603251337572}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18510826238280814
{'scaleFactor': 20, 'currentTarget': array([57.47038656, 58.74069809]), 'previousTarget': array([56.95248906, 58.26701812]), 'currentState': array([74., 70.,  0.]), 'targetState': array([ 5, 23], dtype=int32), 'currentDistance': 20.0}
episode index:1775
target Thresh 31.999999499374564
target distance 5.0
model initialize at round 1775
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 14.]), 'previousTarget': array([19., 14.]), 'currentState': array([24.5, 12.5,  0. ]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 5.700877125495748}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18500403475759258
{'scaleFactor': 20, 'currentTarget': array([58.93150685, 48.84931507]), 'previousTarget': array([58.42278873, 48.35931127]), 'currentState': array([74., 62.,  0.]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 20.0}
episode index:1776
target Thresh 31.99999950435587
target distance 5.0
model initialize at round 1776
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 26.]), 'previousTarget': array([ 5., 26.]), 'currentState': array([10.5, 25.5,  0. ]), 'targetState': array([ 5, 26], dtype=int32), 'currentDistance': 5.522680508593703}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1848999244397774
{'scaleFactor': 20, 'currentTarget': array([45.06680314, 61.69587916]), 'previousTarget': array([44.55937853, 61.20421759]), 'currentState': array([60., 75.,  0.]), 'targetState': array([ 5, 26], dtype=int32), 'currentDistance': 20.0}
episode index:1777
target Thresh 31.999999509287612
target distance 17.0
model initialize at round 1777
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 6.]), 'previousTarget': array([7., 6.]), 'currentState': array([24.5, 11.5,  0. ]), 'targetState': array([7, 6], dtype=int32), 'currentDistance': 18.343936327844244}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18479593123143107
{'scaleFactor': 20, 'currentTarget': array([58.54143405, 48.31013243]), 'previousTarget': array([58.03122315, 47.82258138]), 'currentState': array([74., 61.,  0.]), 'targetState': array([7, 6], dtype=int32), 'currentDistance': 20.0}
episode index:1778
target Thresh 31.99999951417028
target distance 22.0
model initialize at round 1778
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.36618652, 23.6763757 ]), 'previousTarget': array([19.82570003, 23.22895002]), 'currentState': array([ 4.5, 11.5,  0. ]), 'targetState': array([26, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.18506060986512568
{'scaleFactor': 20, 'currentTarget': array([26., 28.]), 'previousTarget': array([26., 28.]), 'currentState': array([25.5       , 28.78347603,  0.        ]), 'targetState': array([26, 28], dtype=int32), 'currentDistance': 0.929427077944611}
episode index:1779
target Thresh 31.99999951900437
target distance 17.0
model initialize at round 1779
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'previousTarget': array([ 5., 22.]), 'currentState': array([22.5, 18.5,  0. ]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 17.846568297574812}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18495664323036998
{'scaleFactor': 20, 'currentTarget': array([55.51198449, 56.67986995]), 'previousTarget': array([54.99385477, 56.20632169]), 'currentState': array([72., 68.,  0.]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 20.0}
episode index:1780
target Thresh 31.999999523790354
target distance 7.0
model initialize at round 1780
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'previousTarget': array([14., 21.]), 'currentState': array([12.5, 14.5,  0. ]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 6.670832032063071}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18485279334646748
{'scaleFactor': 20, 'currentTarget': array([47.10329329, 50.65503357]), 'previousTarget': array([46.59517373, 50.16410281]), 'currentState': array([62., 64.,  0.]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 20.0}
episode index:1781
target Thresh 31.99999952852872
target distance 15.0
model initialize at round 1781
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  8.]), 'previousTarget': array([24.,  8.]), 'currentState': array([24.5       , 23.34334785,  0.        ]), 'targetState': array([24,  8], dtype=int32), 'currentDistance': 15.351492538779798}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1847490600168679
{'scaleFactor': 20, 'currentTarget': array([61.42266345, 54.26831504]), 'previousTarget': array([60.93733492, 53.75645993]), 'currentState': array([74.        , 69.81857567,  0.        ]), 'targetState': array([24,  8], dtype=int32), 'currentDistance': 20.0}
episode index:1782
target Thresh 31.99999953321994
target distance 18.0
model initialize at round 1782
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 26.]), 'previousTarget': array([15., 26.]), 'currentState': array([10.5,  8.5,  0. ]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 18.069310999592556}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1846454430454619
{'scaleFactor': 20, 'currentTarget': array([43.70089351, 46.40952427]), 'previousTarget': array([43.17590645, 45.94474277]), 'currentState': array([60., 58.,  0.]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 20.0}
episode index:1783
target Thresh 31.999999537864475
target distance 9.0
model initialize at round 1783
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([25.5, 19.5,  0. ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 13.435028842544513}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1845419422365799
{'scaleFactor': 20, 'currentTarget': array([60.85786438, 54.85786438]), 'previousTarget': array([60.35786438, 54.35786438]), 'currentState': array([75., 69.,  0.]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 20.0}
episode index:1784
target Thresh 31.999999542462803
target distance 13.0
model initialize at round 1784
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([16.5,  6.5,  0. ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 13.946325680981433}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1844385573949908
{'scaleFactor': 20, 'currentTarget': array([49.84748125, 44.20609742]), 'previousTarget': array([49.33087379, 43.72887612]), 'currentState': array([66., 56.,  0.]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 20.0}
episode index:1785
target Thresh 31.999999547015374
target distance 23.0
model initialize at round 1785
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6.72055926, 24.92499123]), 'previousTarget': array([ 6.71201303, 24.62485559]), 'currentState': array([14.5,  6.5,  0. ]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18433528832590065
{'scaleFactor': 20, 'currentTarget': array([45.81384202, 47.67752092]), 'previousTarget': array([45.28202055, 47.24741102]), 'currentState': array([64., 56.,  0.]), 'targetState': array([ 5, 29], dtype=int32), 'currentDistance': 20.0}
episode index:1786
target Thresh 31.999999551522645
target distance 12.0
model initialize at round 1786
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 12.]), 'previousTarget': array([ 6., 12.]), 'currentState': array([12.5, 24.5,  0. ]), 'targetState': array([ 6, 12], dtype=int32), 'currentDistance': 14.08900280360548}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18423213483495163
{'scaleFactor': 20, 'currentTarget': array([48.59427484, 59.15794714]), 'previousTarget': array([48.10070959, 58.65213765]), 'currentState': array([62., 74.,  0.]), 'targetState': array([ 6, 12], dtype=int32), 'currentDistance': 20.0}
episode index:1787
target Thresh 31.99999955598507
target distance 10.0
model initialize at round 1787
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([23.5,  9.5,  0. ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 10.606601717798283}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18412909672822067
{'scaleFactor': 20, 'currentTarget': array([57.38262381, 46.50609905]), 'previousTarget': array([56.86981777, 46.02212343]), 'currentState': array([73., 59.,  0.]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 20.0}
episode index:1788
target Thresh 31.999999560403094
target distance 9.0
model initialize at round 1788
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 2.]), 'previousTarget': array([5., 2.]), 'currentState': array([14.5,  3.5,  0. ]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 9.61769203083575}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1840261738122183
{'scaleFactor': 20, 'currentTarget': array([48.8693077, 39.920927 ]), 'previousTarget': array([48.36063072, 39.43097182]), 'currentState': array([64., 53.,  0.]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 20.0}
episode index:1789
target Thresh 31.999999564777156
target distance 9.0
model initialize at round 1789
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 23.]), 'previousTarget': array([10., 23.]), 'currentState': array([ 8.5, 14.5,  0. ]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 8.631338250815942}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18392336589388747
{'scaleFactor': 20, 'currentTarget': array([42.79252644, 51.010283  ]), 'previousTarget': array([42.28099481, 50.52379558]), 'currentState': array([58., 64.,  0.]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 20.0}
episode index:1790
target Thresh 31.999999569107693
target distance 2.0
model initialize at round 1790
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 27.]), 'previousTarget': array([ 9., 27.]), 'currentState': array([ 9.5, 25.5,  0. ]), 'targetState': array([ 9, 27], dtype=int32), 'currentDistance': 1.5811388300841407}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1838206727806022
{'scaleFactor': 20, 'currentTarget': array([44.57225358, 61.14936344]), 'previousTarget': array([44.06934104, 60.65239797]), 'currentState': array([59., 75.,  0.]), 'targetState': array([ 9, 27], dtype=int32), 'currentDistance': 20.0}
episode index:1791
target Thresh 31.999999573395144
target distance 12.0
model initialize at round 1791
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'previousTarget': array([27.,  8.]), 'currentState': array([18.5       , 19.50012666,  0.        ]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 14.300451503118955}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1837180942801666
{'scaleFactor': 20, 'currentTarget': array([55.7381592 , 45.03110531]), 'previousTarget': array([55.25929194, 44.51472777]), 'currentState': array([68.        , 60.83133506,  0.        ]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 20.0}
episode index:1792
target Thresh 31.999999577639933
target distance 18.0
model initialize at round 1792
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9.65560275, 13.16959038]), 'previousTarget': array([ 9.19631201, 12.63557441]), 'currentState': array([22.5, 28.5,  0. ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18361563020081348
{'scaleFactor': 20, 'currentTarget': array([58.18037512, 63.54254628]), 'previousTarget': array([57.68284457, 63.04018618]), 'currentState': array([72., 78.,  0.]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:1793
target Thresh 31.999999581842488
target distance 17.0
model initialize at round 1793
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  7.]), 'previousTarget': array([13.,  7.]), 'currentState': array([16.5       , 24.37082541,  0.        ]), 'targetState': array([13,  7], dtype=int32), 'currentDistance': 17.71992029950632}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1835132803512032
{'scaleFactor': 20, 'currentTarget': array([53.43717083, 57.09083282]), 'previousTarget': array([52.9511209 , 56.57958157]), 'currentState': array([66.        , 72.65281609,  0.        ]), 'targetState': array([13,  7], dtype=int32), 'currentDistance': 20.0}
episode index:1794
target Thresh 31.999999586003224
target distance 16.0
model initialize at round 1794
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 5.]), 'previousTarget': array([5., 5.]), 'currentState': array([13.5, 21.5,  0. ]), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 18.56071119327069}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1834110445404226
{'scaleFactor': 20, 'currentTarget': array([49.79770785, 55.97670203]), 'previousTarget': array([49.30555676, 55.46980813]), 'currentState': array([63., 71.,  0.]), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 20.0}
episode index:1795
target Thresh 31.99999959012256
target distance 6.0
model initialize at round 1795
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'previousTarget': array([ 5., 22.]), 'currentState': array([11.5, 18.5,  0. ]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 7.382411530116732}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18330892257798362
{'scaleFactor': 20, 'currentTarget': array([45.54547491, 55.30521153]), 'previousTarget': array([45.03327788, 54.82007466]), 'currentState': array([61., 68.,  0.]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 20.0}
episode index:1796
target Thresh 31.999999594200908
target distance 8.0
model initialize at round 1796
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 16.]), 'previousTarget': array([ 6., 16.]), 'currentState': array([14.5, 12.5,  0. ]), 'targetState': array([ 6, 16], dtype=int32), 'currentDistance': 9.192388155425162}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18320691427382224
{'scaleFactor': 20, 'currentTarget': array([48.33004642, 49.57210578]), 'previousTarget': array([47.81631846, 49.08943461]), 'currentState': array([64., 62.,  0.]), 'targetState': array([ 6, 16], dtype=int32), 'currentDistance': 20.0}
episode index:1797
target Thresh 31.999999598238677
target distance 3.0
model initialize at round 1797
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([ 5.5       , 10.49999994,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 4.301162585019042}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18310501943829732
{'scaleFactor': 20, 'currentTarget': array([40.99318659, 45.72382475]), 'previousTarget': array([40.49449387, 45.22254225]), 'currentState': array([55.        , 59.99999994,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 20.0}
episode index:1798
target Thresh 31.99999960223627
target distance 21.0
model initialize at round 1798
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8.12394591, 18.543335  ]), 'previousTarget': array([ 7.54387571, 18.36758945]), 'currentState': array([27.5, 23.5,  0. ]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1830032378821893
{'scaleFactor': 20, 'currentTarget': array([61.18900375, 60.75204516]), 'previousTarget': array([60.6767683 , 60.26785634]), 'currentState': array([77., 73.,  0.]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 20.0}
episode index:1799
target Thresh 31.999999606194084
target distance 19.0
model initialize at round 1799
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([6.5, 2.5, 0. ]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 18.66815470259447}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18290156941669922
{'scaleFactor': 20, 'currentTarget': array([39.30454642, 40.98810508]), 'previousTarget': array([38.77646458, 40.53079935]), 'currentState': array([56., 52.,  0.]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 20.0}
episode index:1800
target Thresh 31.999999610112518
target distance 23.0
model initialize at round 1800
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8.10774054, 8.84255222]), 'previousTarget': array([7.74867987, 8.26830308]), 'currentState': array([17.5, 26.5,  0. ]), 'targetState': array([5, 3], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1828000138534473
{'scaleFactor': 20, 'currentTarget': array([54.05309419, 60.75606252]), 'previousTarget': array([53.56230436, 60.24824497]), 'currentState': array([67., 76.,  0.]), 'targetState': array([5, 3], dtype=int32), 'currentDistance': 20.0}
episode index:1801
target Thresh 31.999999613991964
target distance 2.0
model initialize at round 1801
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([ 5.5, 20.5,  0. ]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 2.1213203435595833}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.182698571004472
{'scaleFactor': 20, 'currentTarget': array([40.43600015, 56.29270602]), 'previousTarget': array([39.93176627, 55.7972059 ]), 'currentState': array([55., 70.,  0.]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:1802
target Thresh 31.99999961783281
target distance 4.0
model initialize at round 1802
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 24.]), 'previousTarget': array([ 2., 24.]), 'currentState': array([ 2.5       , 27.92145859,  0.        ]), 'targetState': array([ 2, 24], dtype=int32), 'currentDistance': 3.9532059784920146}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18259724068222882
{'scaleFactor': 20, 'currentTarget': array([38.17996748, 61.84767558]), 'previousTarget': array([37.68318114, 61.34460421]), 'currentState': array([52.        , 76.30473964,  0.        ]), 'targetState': array([ 2, 24], dtype=int32), 'currentDistance': 20.0}
episode index:1803
target Thresh 31.999999621635435
target distance 25.0
model initialize at round 1803
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9.54158555, 15.69685566]), 'previousTarget': array([ 8.96953885, 15.34537865]), 'currentState': array([27.5, 24.5,  0. ]), 'targetState': array([ 2, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.182496022699589
{'scaleFactor': 20, 'currentTarget': array([61.58515363, 61.25706033]), 'previousTarget': array([61.07634618, 60.76772201]), 'currentState': array([77., 74.,  0.]), 'targetState': array([ 2, 12], dtype=int32), 'currentDistance': 20.0}
episode index:1804
target Thresh 31.999999625400225
target distance 11.0
model initialize at round 1804
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  8.]), 'previousTarget': array([11.,  8.]), 'currentState': array([22.5,  3.5,  0. ]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 12.349089035228547}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18239491686983855
{'scaleFactor': 20, 'currentTarget': array([55.90551739, 41.12702103]), 'previousTarget': array([55.38884898, 40.64964925]), 'currentState': array([72., 53.,  0.]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 20.0}
episode index:1805
target Thresh 31.999999629127554
target distance 7.0
model initialize at round 1805
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 28.]), 'previousTarget': array([21., 28.]), 'currentState': array([22.5, 21.5,  0. ]), 'targetState': array([21, 28], dtype=int32), 'currentDistance': 6.670832032063109}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18229392300667693
{'scaleFactor': 20, 'currentTarget': array([56.70955354, 58.10805495]), 'previousTarget': array([56.19784789, 57.6219512 ]), 'currentState': array([72., 71.,  0.]), 'targetState': array([21, 28], dtype=int32), 'currentDistance': 20.0}
episode index:1806
target Thresh 31.999999632817797
target distance 12.0
model initialize at round 1806
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 26.]), 'previousTarget': array([11., 26.]), 'currentState': array([ 6.5, 14.5,  0. ]), 'targetState': array([11, 26], dtype=int32), 'currentDistance': 12.349089035228396}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18219304092421615
{'scaleFactor': 20, 'currentTarget': array([40.71940168, 51.09638364]), 'previousTarget': array([40.2062331 , 50.61199418]), 'currentState': array([56., 64.,  0.]), 'targetState': array([11, 26], dtype=int32), 'currentDistance': 20.0}
episode index:1807
target Thresh 31.999999636471323
target distance 7.0
model initialize at round 1807
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  9.]), 'previousTarget': array([23.,  9.]), 'currentState': array([27.5, 16.5,  0. ]), 'targetState': array([23,  9], dtype=int32), 'currentDistance': 8.746427842268062}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1820922704369793
{'scaleFactor': 20, 'currentTarget': array([63.24510704, 51.48094632]), 'previousTarget': array([62.74867216, 50.97756967]), 'currentState': array([77., 66.,  0.]), 'targetState': array([23,  9], dtype=int32), 'currentDistance': 20.0}
episode index:1808
target Thresh 31.999999640088493
target distance 21.0
model initialize at round 1808
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8.52864968, 26.27670597]), 'previousTarget': array([ 8.28336929, 26.28013989]), 'currentState': array([26.5, 17.5,  0. ]), 'targetState': array([ 5, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18199161135989972
{'scaleFactor': 20, 'currentTarget': array([58.47047164, 57.37110414]), 'previousTarget': array([57.9468483 , 56.91423631]), 'currentState': array([76., 67.,  0.]), 'targetState': array([ 5, 28], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:1809
target Thresh 31.99999964366967
target distance 16.0
model initialize at round 1809
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 24.]), 'previousTarget': array([12., 24.]), 'currentState': array([19.5,  8.5,  0. ]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 17.219175357722527}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18189106350831966
{'scaleFactor': 20, 'currentTarget': array([51.8236099 , 47.75443398]), 'previousTarget': array([51.2966428 , 47.29977936]), 'currentState': array([69., 58.,  0.]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 20.0}
episode index:1810
target Thresh 31.99999964721522
target distance 20.0
model initialize at round 1810
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.72731807, 10.3402996 ]), 'previousTarget': array([21.14985851, 10.71008489]), 'currentState': array([ 4.5, 20.5,  0. ]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18179062669798926
{'scaleFactor': 20, 'currentTarget': array([41.29066377, 30.00913621]), 'previousTarget': array([40.81336723, 29.49047907]), 'currentState': array([54.        , 45.45169988,  0.        ]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 20.0}
episode index:1811
target Thresh 31.999999650725485
target distance 7.0
model initialize at round 1811
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([24.5       , 14.46112889,  0.        ]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 7.477863620103562}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18169030074506543
{'scaleFactor': 20, 'currentTarget': array([60.82985567, 47.94812009]), 'previousTarget': array([60.33946318, 47.43971887]), 'currentState': array([74.        , 62.99960834,  0.        ]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 20.0}
episode index:1812
target Thresh 31.999999654200824
target distance 21.0
model initialize at round 1812
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.82831279,  6.95070559]), 'previousTarget': array([19.27466942,  7.37523613]), 'currentState': array([ 3.5, 18.5,  0. ]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.18196279867574122
{'scaleFactor': 20, 'currentTarget': array([24.,  4.]), 'previousTarget': array([24.,  4.]), 'currentState': array([23.        ,  4.76579472,  0.        ]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 1.2595402152928228}
episode index:1813
target Thresh 31.999999657641585
target distance 17.0
model initialize at round 1813
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 24.]), 'previousTarget': array([23., 24.]), 'currentState': array([21.5,  7.5,  0. ]), 'targetState': array([23, 24], dtype=int32), 'currentDistance': 16.568041525780888}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18186248842288802
{'scaleFactor': 20, 'currentTarget': array([54.51916152, 45.66942354]), 'previousTarget': array([53.99385477, 45.20632169]), 'currentState': array([71., 57.,  0.]), 'targetState': array([23, 24], dtype=int32), 'currentDistance': 20.0}
episode index:1814
target Thresh 31.999999661048108
target distance 9.0
model initialize at round 1814
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 24.]), 'previousTarget': array([27., 24.]), 'currentState': array([18.5, 16.5,  0. ]), 'targetState': array([27, 24], dtype=int32), 'currentDistance': 11.335784048754519}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.18223615005716812
{'scaleFactor': 20, 'currentTarget': array([27., 24.]), 'previousTarget': array([27., 24.]), 'currentState': array([26.        , 23.96865594,  0.        ]), 'targetState': array([27, 24], dtype=int32), 'currentDistance': 1.0004911043342883}
episode index:1815
target Thresh 31.999999664420734
target distance 7.0
model initialize at round 1815
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  9.]), 'previousTarget': array([18.,  9.]), 'currentState': array([19.5,  2.5,  0. ]), 'targetState': array([18,  9], dtype=int32), 'currentDistance': 6.67083203206319}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18213579975427321
{'scaleFactor': 20, 'currentTarget': array([49.76641407, 18.05667259]), 'previousTarget': array([49.26843556, 17.92715507]), 'currentState': array([69.        , 23.54020908,  0.        ]), 'targetState': array([18,  9], dtype=int32), 'currentDistance': 20.0}
episode index:1816
target Thresh 31.999999667759806
target distance 3.0
model initialize at round 1816
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 15.]), 'previousTarget': array([14., 15.]), 'currentState': array([11.5       , 13.49998426,  0.        ]), 'targetState': array([14, 15], dtype=int32), 'currentDistance': 2.9154840433667237}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.182569571466021
{'scaleFactor': 20, 'currentTarget': array([14., 15.]), 'previousTarget': array([14., 15.]), 'currentState': array([13.        , 14.98634547,  0.        ]), 'targetState': array([14, 15], dtype=int32), 'currentDistance': 1.0000932187502345}
episode index:1817
target Thresh 31.99999967106565
target distance 6.0
model initialize at round 1817
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 23.]), 'previousTarget': array([18., 23.]), 'currentState': array([12.5       , 20.49999994,  0.        ]), 'targetState': array([18, 23], dtype=int32), 'currentDistance': 6.041523011461778}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.1829666080466276
{'scaleFactor': 20, 'currentTarget': array([18., 23.]), 'previousTarget': array([18., 23.]), 'currentState': array([17.5       , 23.75620778,  0.        ]), 'targetState': array([18, 23], dtype=int32), 'currentDistance': 0.9065595430501965}
episode index:1818
target Thresh 31.9999996743386
target distance 13.0
model initialize at round 1818
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 20.]), 'previousTarget': array([19., 20.]), 'currentState': array([ 6.5, 10.5,  0. ]), 'targetState': array([19, 20], dtype=int32), 'currentDistance': 15.700318468107536}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.18329795028563836
{'scaleFactor': 20, 'currentTarget': array([19., 20.]), 'previousTarget': array([19., 20.]), 'currentState': array([18.5       , 20.79811274,  0.        ]), 'targetState': array([19, 20], dtype=int32), 'currentDistance': 0.941798246978452}
episode index:1819
target Thresh 31.999999677578987
target distance 7.0
model initialize at round 1819
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 20.]), 'previousTarget': array([ 6., 20.]), 'currentState': array([13.5, 15.5,  0. ]), 'targetState': array([ 6, 20], dtype=int32), 'currentDistance': 8.746427842267979}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18319723712614075
{'scaleFactor': 20, 'currentTarget': array([44.31865715, 34.64936866]), 'previousTarget': array([43.81250294, 34.41848893]), 'currentState': array([63.        , 41.79131723,  0.        ]), 'targetState': array([ 6, 20], dtype=int32), 'currentDistance': 20.0}
episode index:1820
target Thresh 31.999999680787127
target distance 16.0
model initialize at round 1820
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'previousTarget': array([19.,  3.]), 'currentState': array([17.5, 18.5,  0. ]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 15.572411502397348}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1830966345796684
{'scaleFactor': 20, 'currentTarget': array([49.81788829, 21.35926328]), 'previousTarget': array([49.29242918, 20.94408335]), 'currentState': array([67.       , 31.5952311,  0.       ]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 20.0}
episode index:1821
target Thresh 31.99999968396335
target distance 5.0
model initialize at round 1821
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 19.]), 'previousTarget': array([12., 19.]), 'currentState': array([12.5, 14.5,  0. ]), 'targetState': array([12, 19], dtype=int32), 'currentDistance': 4.527692569068638}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1829961424640923
{'scaleFactor': 20, 'currentTarget': array([44.73192028, 38.12625573]), 'previousTarget': array([44.218704  , 37.76973847]), 'currentState': array([62.        , 48.21651949,  0.        ]), 'targetState': array([12, 19], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:1822
target Thresh 31.999999687107966
target distance 6.0
model initialize at round 1822
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.5       , 14.50000557,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 5.522686058740582}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18289576059768303
{'scaleFactor': 20, 'currentTarget': array([48.96331985, 29.26988408]), 'previousTarget': array([48.44785257, 28.8868708 ]), 'currentState': array([66.       , 39.7461205,  0.       ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 20.0}
episode index:1823
target Thresh 31.999999690221294
target distance 10.0
model initialize at round 1823
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 26.]), 'previousTarget': array([11., 26.]), 'currentState': array([12.5, 16.5,  0. ]), 'targetState': array([11, 26], dtype=int32), 'currentDistance': 9.617692030835604}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18279548879910976
{'scaleFactor': 20, 'currentTarget': array([45.12058468, 47.68548126]), 'previousTarget': array([44.6460876 , 47.49616781]), 'currentState': array([62.        , 58.41326474,  0.        ]), 'targetState': array([11, 26], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:1824
target Thresh 31.999999693303643
target distance 17.0
model initialize at round 1824
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'previousTarget': array([25.86502556, 11.20859686]), 'currentState': array([15.5, 27.5,  0. ]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 19.557607215607838}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.182695326887439
{'scaleFactor': 20, 'currentTarget': array([48.09178118, 24.95721573]), 'previousTarget': array([47.59682814, 24.65878804]), 'currentState': array([65.        , 35.63954395,  0.        ]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:1825
target Thresh 31.999999696355324
target distance 23.0
model initialize at round 1825
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3.43198863, 7.79055813]), 'previousTarget': array([3.4295875 , 8.11006407]), 'currentState': array([10.5, 26.5,  0. ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1825952746821337
{'scaleFactor': 20, 'currentTarget': array([43.0620728 , 29.78247608]), 'previousTarget': array([42.55509317, 29.42703586]), 'currentState': array([60.       , 40.4176358,  0.       ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 20.0}
episode index:1826
target Thresh 31.99999969937664
target distance 12.0
model initialize at round 1826
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 25.]), 'previousTarget': array([20., 25.]), 'currentState': array([27.5, 13.5,  0. ]), 'targetState': array([20, 25], dtype=int32), 'currentDistance': 13.729530217745959}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1824953320030521
{'scaleFactor': 20, 'currentTarget': array([59.98624526, 49.70897704]), 'previousTarget': array([59.47196085, 49.31705409]), 'currentState': array([77.        , 60.22240415,  0.        ]), 'targetState': array([20, 25], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:1827
target Thresh 31.99999970236789
target distance 25.0
model initialize at round 1827
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.84790493, 22.01676513]), 'previousTarget': array([10.77294527, 21.74433602]), 'currentState': array([20.5,  4.5,  0. ]), 'targetState': array([ 7, 29], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1823954986704465
{'scaleFactor': 20, 'currentTarget': array([51.41018578, 46.62308959]), 'previousTarget': array([50.87954344, 46.20078103]), 'currentState': array([70., 54.,  0.]), 'targetState': array([ 7, 29], dtype=int32), 'currentDistance': 20.0}
episode index:1828
target Thresh 31.99999970532938
target distance 17.0
model initialize at round 1828
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.42284624, 22.44184296]), 'previousTarget': array([10.46633605, 22.33935727]), 'currentState': array([22.5,  6.5,  0. ]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18229577450496237
{'scaleFactor': 20, 'currentTarget': array([54.34506482, 46.60301837]), 'previousTarget': array([53.81725277, 46.15545878]), 'currentState': array([72., 56.,  0.]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 20.0}
episode index:1829
target Thresh 31.999999708261402
target distance 17.0
model initialize at round 1829
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 24.]), 'previousTarget': array([ 6., 24.]), 'currentState': array([8.5, 7.5, 0. ]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 16.68831926827865}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18219615932763725
{'scaleFactor': 20, 'currentTarget': array([40.90327242, 45.1867748 ]), 'previousTarget': array([40.41632039, 44.95038677]), 'currentState': array([58.        , 55.56472769,  0.        ]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 20.0}
episode index:1830
target Thresh 31.99999971116425
target distance 16.0
model initialize at round 1830
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 28.]), 'previousTarget': array([ 7., 28.]), 'currentState': array([ 9.5, 12.5,  0. ]), 'targetState': array([ 7, 28], dtype=int32), 'currentDistance': 15.700318468107515}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1820966529598996
{'scaleFactor': 20, 'currentTarget': array([41.66372487, 47.94020481]), 'previousTarget': array([41.16294211, 47.64856214]), 'currentState': array([59.        , 57.91284561,  0.        ]), 'targetState': array([ 7, 28], dtype=int32), 'currentDistance': 20.0}
episode index:1831
target Thresh 31.999999714038214
target distance 24.0
model initialize at round 1831
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.46401955, 25.21238788]), 'previousTarget': array([21.98266146, 25.16738911]), 'currentState': array([ 2.5       , 26.41152227,  0.        ]), 'targetState': array([26, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.18233760751335557
{'scaleFactor': 20, 'currentTarget': array([26., 25.]), 'previousTarget': array([26., 25.]), 'currentState': array([26.        , 25.92397811,  0.        ]), 'targetState': array([26, 25], dtype=int32), 'currentDistance': 0.9239781079812666}
episode index:1832
target Thresh 31.99999971688358
target distance 15.0
model initialize at round 1832
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'previousTarget': array([19.,  3.]), 'currentState': array([ 4.5, 13.5,  0. ]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 17.90251378996811}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.18264575507845004
{'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'previousTarget': array([19.,  3.]), 'currentState': array([19.        ,  3.95515764,  0.        ]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 0.955157644813609}
episode index:1833
target Thresh 31.999999719700636
target distance 5.0
model initialize at round 1833
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 11.]), 'previousTarget': array([20., 11.]), 'currentState': array([15.5,  8.5,  0. ]), 'targetState': array([20, 11], dtype=int32), 'currentDistance': 5.147815070493392}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.18305438081063574
{'scaleFactor': 20, 'currentTarget': array([20., 11.]), 'previousTarget': array([20., 11.]), 'currentState': array([19.        , 11.98917532,  0.        ]), 'targetState': array([20, 11], dtype=int32), 'currentDistance': 1.4065801836534177}
episode index:1834
target Thresh 31.99999972248966
target distance 5.0
model initialize at round 1834
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  7.]), 'previousTarget': array([12.,  7.]), 'currentState': array([17.5       ,  8.49999946,  0.        ]), 'targetState': array([12,  7], dtype=int32), 'currentDistance': 5.700876984348615}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1829546236548806
{'scaleFactor': 20, 'currentTarget': array([49.58006823, 28.19718579]), 'previousTarget': array([49.08870397, 27.96298532]), 'currentState': array([67.        , 38.02296704,  0.        ]), 'targetState': array([12,  7], dtype=int32), 'currentDistance': 20.0}
episode index:1835
target Thresh 31.999999725250934
target distance 8.0
model initialize at round 1835
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([13.5,  7.5,  0. ]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 9.924716620639556}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.18332814826768784
{'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([20.5       , 14.49995109,  0.        ]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 0.7071413635215184}
episode index:1836
target Thresh 31.999999727984733
target distance 3.0
model initialize at round 1836
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([5.5     , 3.234106, 0.      ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 3.7112016422846823}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18322835069105872
{'scaleFactor': 20, 'currentTarget': array([38.06899233, 24.68008529]), 'previousTarget': array([37.55175564, 24.27450784]), 'currentState': array([55.        , 35.32625734,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:1837
target Thresh 31.999999730691332
target distance 20.0
model initialize at round 1837
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5.64708245, 18.07891249]), 'previousTarget': array([ 5.0992562 , 18.00992562]), 'currentState': array([25.5, 20.5,  0. ]), 'targetState': array([ 5, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18312866170809297
{'scaleFactor': 20, 'currentTarget': array([58.94512304, 58.07351997]), 'previousTarget': array([58.43090429, 57.59268447]), 'currentState': array([75., 70.,  0.]), 'targetState': array([ 5, 18], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:1838
target Thresh 31.999999733371
target distance 8.0
model initialize at round 1838
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 18.]), 'previousTarget': array([14., 18.]), 'currentState': array([ 7.5       , 25.50000003,  0.        ]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 9.924716643160853}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.1835014823448852
{'scaleFactor': 20, 'currentTarget': array([14., 18.]), 'previousTarget': array([14., 18.]), 'currentState': array([14.5       , 18.50373825,  0.        ]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 0.7097550484886241}
episode index:1839
target Thresh 31.999999736024
target distance 17.0
model initialize at round 1839
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9.94821188, 22.22025737]), 'previousTarget': array([ 9.85786438, 22.14213562]), 'currentState': array([24.5,  8.5,  0. ]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1834017532783934
{'scaleFactor': 20, 'currentTarget': array([56.05821933, 49.16300355]), 'previousTarget': array([55.53112829, 48.71822059]), 'currentState': array([74., 58.,  0.]), 'targetState': array([ 7, 25], dtype=int32), 'currentDistance': 20.0}
episode index:1840
target Thresh 31.999999738650605
target distance 15.0
model initialize at round 1840
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 11.]), 'previousTarget': array([17., 11.]), 'currentState': array([ 2.5, 23.5,  0. ]), 'targetState': array([17, 11], dtype=int32), 'currentDistance': 19.1441897190766}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.18371208328052552
{'scaleFactor': 20, 'currentTarget': array([17., 11.]), 'previousTarget': array([17., 11.]), 'currentState': array([16.5     , 11.931519,  0.      ]), 'targetState': array([17, 11], dtype=int32), 'currentDistance': 1.0572263951337573}
episode index:1841
target Thresh 31.999999741251074
target distance 10.0
model initialize at round 1841
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 22.]), 'previousTarget': array([15., 22.]), 'currentState': array([ 5.5, 14.5,  0. ]), 'targetState': array([15, 22], dtype=int32), 'currentDistance': 12.103718436910158}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.1840699720482255
{'scaleFactor': 20, 'currentTarget': array([15., 22.]), 'previousTarget': array([15., 22.]), 'currentState': array([14.        , 22.92743635,  0.        ]), 'targetState': array([15, 22], dtype=int32), 'currentDistance': 1.3638688304215814}
episode index:1842
target Thresh 31.99999974382567
target distance 11.0
model initialize at round 1842
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 11.]), 'previousTarget': array([27., 11.]), 'currentState': array([23.5, 21.5,  0. ]), 'targetState': array([27, 11], dtype=int32), 'currentDistance': 11.067971810589228}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18397009685991936
{'scaleFactor': 20, 'currentTarget': array([56.59326719, 31.63034158]), 'previousTarget': array([56.09986898, 31.31133817]), 'currentState': array([73.        , 43.06796014,  0.        ]), 'targetState': array([27, 11], dtype=int32), 'currentDistance': 20.0}
episode index:1843
target Thresh 31.999999746374648
target distance 20.0
model initialize at round 1843
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.74899309,  2.07079684]), 'previousTarget': array([23.1565257 ,  2.25304229]), 'currentState': array([4.5       , 7.50000167, 0.        ]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.18423677741968095
{'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([24.       ,  2.8788052,  0.       ]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 0.8788052024319484}
episode index:1844
target Thresh 31.99999974889826
target distance 6.0
model initialize at round 1844
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 12.]), 'previousTarget': array([19., 12.]), 'currentState': array([24.5,  6.5,  0. ]), 'targetState': array([19, 12], dtype=int32), 'currentDistance': 7.778174593052084}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18413692008774618
{'scaleFactor': 20, 'currentTarget': array([56.68116984, 33.76366327]), 'previousTarget': array([56.19376322, 33.54457398]), 'currentState': array([74.        , 43.76656894,  0.        ]), 'targetState': array([19, 12], dtype=int32), 'currentDistance': 20.0}
episode index:1845
target Thresh 31.999999751396764
target distance 21.0
model initialize at round 1845
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.68561931, 18.93629877]), 'previousTarget': array([10.51724138, 18.79310345]), 'currentState': array([25.5,  5.5,  0. ]), 'targetState': array([ 4, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1840371709436033
{'scaleFactor': 20, 'currentTarget': array([56.57707567, 47.21566578]), 'previousTarget': array([56.05009652, 46.77982762]), 'currentState': array([75., 55.,  0.]), 'targetState': array([ 4, 25], dtype=int32), 'currentDistance': 20.0}
episode index:1846
target Thresh 31.99999975387041
target distance 20.0
model initialize at round 1846
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9.65213765, 12.10070959]), 'previousTarget': array([ 9.50001133, 12.22501076]), 'currentState': array([24.5, 25.5,  0. ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18393752981152772
{'scaleFactor': 20, 'currentTarget': array([56.62537581, 37.00294929]), 'previousTarget': array([56.12603691, 36.72287411]), 'currentState': array([74.        , 46.90862617,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 20.0}
episode index:1847
target Thresh 31.99999975631944
target distance 12.0
model initialize at round 1847
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([21.5, 10.5,  0. ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 12.589678312014259}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18383799651617516
{'scaleFactor': 20, 'currentTarget': array([53.74894275, 35.24938981]), 'previousTarget': array([53.24668646, 34.94150902]), 'currentState': array([71.        , 45.36872891,  0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 20.0}
episode index:1848
target Thresh 31.999999758744103
target distance 12.0
model initialize at round 1848
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 15.]), 'previousTarget': array([22., 15.]), 'currentState': array([13.5,  3.5,  0. ]), 'targetState': array([22, 15], dtype=int32), 'currentDistance': 14.300349646075047}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1837385708825807
{'scaleFactor': 20, 'currentTarget': array([45.74845224, 28.92911437]), 'previousTarget': array([45.25055622, 28.64358216]), 'currentState': array([63.        , 39.04761722,  0.        ]), 'targetState': array([22, 15], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:1849
target Thresh 31.99999976114464
target distance 17.0
model initialize at round 1849
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([22.5,  4.5,  0. ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 18.668154702594546}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18363925273615767
{'scaleFactor': 20, 'currentTarget': array([54.42983943, 37.87966068]), 'previousTarget': array([53.93119472, 37.61749337]), 'currentState': array([72.        , 47.43421234,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 20.0}
episode index:1850
target Thresh 31.999999763521288
target distance 24.0
model initialize at round 1850
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.27836   , 13.55871662]), 'previousTarget': array([21.72787848, 13.71202025]), 'currentState': array([ 2.5       , 16.52797168,  0.        ]), 'targetState': array([26, 13], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.1838735319842431
{'scaleFactor': 20, 'currentTarget': array([26., 13.]), 'previousTarget': array([26., 13.]), 'currentState': array([26.5       , 13.95024504,  0.        ]), 'targetState': array([26, 13], dtype=int32), 'currentDistance': 1.0737623720484268}
episode index:1851
target Thresh 31.999999765874293
target distance 7.0
model initialize at round 1851
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'previousTarget': array([ 5., 22.]), 'currentState': array([ 2.5, 15.5,  0. ]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 6.964194138591988}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18377424821967278
{'scaleFactor': 20, 'currentTarget': array([34.02937323, 36.18009962]), 'previousTarget': array([33.52637266, 35.92231706]), 'currentState': array([52.      , 44.958287,  0.      ]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 20.0}
episode index:1852
target Thresh 31.99999976820388
target distance 8.0
model initialize at round 1852
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([ 6.5       , 18.36502123,  0.        ]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 9.22135851083607}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.184139215357515
{'scaleFactor': 20, 'currentTarget': array([14., 13.]), 'previousTarget': array([14., 13.]), 'currentState': array([14.        , 13.86344542,  0.        ]), 'targetState': array([14, 13], dtype=int32), 'currentDistance': 0.863445423543844}
episode index:1853
target Thresh 31.99999977051029
target distance 22.0
model initialize at round 1853
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.08744313,  8.61599637]), 'previousTarget': array([9.83486126, 9.20413153]), 'currentState': array([ 3.5, 27.5,  0. ]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1840398953923815
{'scaleFactor': 20, 'currentTarget': array([34.7607354 , 16.68942557]), 'previousTarget': array([34.25641563, 16.44781689]), 'currentState': array([53.        , 24.89486443,  0.        ]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 20.0}
episode index:1854
target Thresh 31.999999772793753
target distance 19.0
model initialize at round 1854
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.37211039, 11.1062817 ]), 'previousTarget': array([12.02072541, 11.69147429]), 'currentState': array([ 2.5, 28.5,  0. ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18394068251076837
{'scaleFactor': 20, 'currentTarget': array([33.64710835, 18.94159259]), 'previousTarget': array([33.12578462, 18.651577  ]), 'currentState': array([52.        , 26.88963437,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 20.0}
episode index:1855
target Thresh 31.99999977505449
target distance 21.0
model initialize at round 1855
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.95951277,  5.55332553]), 'previousTarget': array([15.89618185,  6.09009055]), 'currentState': array([14.5, 25.5,  0. ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.183841576539588
{'scaleFactor': 20, 'currentTarget': array([46.29497461, 20.91705073]), 'previousTarget': array([45.78476082, 20.60723252]), 'currentState': array([64.       , 30.2193126,  0.       ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 20.0}
episode index:1856
target Thresh 31.999999777292736
target distance 7.0
model initialize at round 1856
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([6.5       , 9.50000006, 0.        ]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 6.670832090141376}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18374257730612564
{'scaleFactor': 20, 'currentTarget': array([38.26629709, 18.78276157]), 'previousTarget': array([37.76856069, 18.53247753]), 'currentState': array([56.        , 28.03023588,  0.        ]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 20.0}
episode index:1857
target Thresh 31.999999779508713
target distance 14.0
model initialize at round 1857
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 12.]), 'previousTarget': array([ 6., 12.]), 'currentState': array([ 3.5, 25.5,  0. ]), 'targetState': array([ 6, 12], dtype=int32), 'currentDistance': 13.729530217745902}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18364368463803837
{'scaleFactor': 20, 'currentTarget': array([35.54404647, 28.52149525]), 'previousTarget': array([35.03837295, 28.21655517]), 'currentState': array([53.        , 38.28313889,  0.        ]), 'targetState': array([ 6, 12], dtype=int32), 'currentDistance': 20.0}
episode index:1858
target Thresh 31.999999781702638
target distance 7.0
model initialize at round 1858
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  4.]), 'previousTarget': array([11.,  4.]), 'currentState': array([ 6.5       , 10.50015563,  0.        ]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 7.905822106819651}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1835448983633541
{'scaleFactor': 20, 'currentTarget': array([38.77777537, 20.40053202]), 'previousTarget': array([38.28875233, 20.15152057]), 'currentState': array([56.        , 30.56886417,  0.        ]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 20.0}
episode index:1859
target Thresh 31.99999978387473
target distance 12.0
model initialize at round 1859
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([3.5, 7.5, 0. ]), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 13.729530217745985}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.18387289265651557
{'scaleFactor': 20, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([15.        , 15.98300296,  0.        ]), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.9830029606828532}
episode index:1860
target Thresh 31.999999786025214
target distance 7.0
model initialize at round 1860
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 13.]), 'previousTarget': array([10., 13.]), 'currentState': array([17.5, 13.5,  0. ]), 'targetState': array([10, 13], dtype=int32), 'currentDistance': 7.516648189186541}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1837740893826539
{'scaleFactor': 20, 'currentTarget': array([49.40614028, 34.30206493]), 'previousTarget': array([48.92655411, 34.15068978]), 'currentState': array([67.        , 43.81290612,  0.        ]), 'targetState': array([10, 13], dtype=int32), 'currentDistance': 20.0}
episode index:1861
target Thresh 31.999999788154298
target distance 19.0
model initialize at round 1861
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9.3639063 , 24.29134036]), 'previousTarget': array([ 9.43827311, 24.07475678]), 'currentState': array([18.5,  6.5,  0. ]), 'targetState': array([ 9, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18367539223475776
{'scaleFactor': 20, 'currentTarget': array([50.29503836, 46.69687116]), 'previousTarget': array([49.76557496, 46.25369999]), 'currentState': array([68.        , 55.99925438,  0.        ]), 'targetState': array([ 9, 25], dtype=int32), 'currentDistance': 20.0}
episode index:1862
target Thresh 31.999999790262198
target distance 5.0
model initialize at round 1862
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 24.]), 'previousTarget': array([16., 24.]), 'currentState': array([13.5, 19.5,  0. ]), 'targetState': array([16, 24], dtype=int32), 'currentDistance': 5.147815070493393}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1835768010419318
{'scaleFactor': 20, 'currentTarget': array([45.46429622, 40.16000937]), 'previousTarget': array([44.97442141, 39.93099637]), 'currentState': array([63.        , 49.77765425,  0.        ]), 'targetState': array([16, 24], dtype=int32), 'currentDistance': 20.0}
episode index:1863
target Thresh 31.999999792349126
target distance 8.0
model initialize at round 1863
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([23.5, 17.5,  0. ]), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 8.860022573334778}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1834783156336475
{'scaleFactor': 20, 'currentTarget': array([55.00272482, 34.38963728]), 'previousTarget': array([54.51538866, 34.2242077 ]), 'currentState': array([73.        , 43.11305898,  0.        ]), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:1864
target Thresh 31.999999794415285
target distance 12.0
model initialize at round 1864
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 29.]), 'previousTarget': array([17., 29.]), 'currentState': array([16.5, 17.5,  0. ]), 'targetState': array([17, 29], dtype=int32), 'currentDistance': 11.510864433221252}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18337993583974208
{'scaleFactor': 20, 'currentTarget': array([48.09508997, 44.47635731]), 'previousTarget': array([47.60039677, 44.25287308]), 'currentState': array([66.        , 53.38782165,  0.        ]), 'targetState': array([17, 29], dtype=int32), 'currentDistance': 20.0}
episode index:1865
target Thresh 31.999999796460887
target distance 14.0
model initialize at round 1865
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 29.]), 'previousTarget': array([19., 29.]), 'currentState': array([ 5.5       , 26.49999779,  0.        ]), 'targetState': array([19, 29], dtype=int32), 'currentDistance': 13.729530619320682}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.18369433198656168
{'scaleFactor': 20, 'currentTarget': array([19., 29.]), 'previousTarget': array([19., 29.]), 'currentState': array([18.5       , 29.86413068,  0.        ]), 'targetState': array([19, 29], dtype=int32), 'currentDistance': 0.9983595733599137}
episode index:1866
target Thresh 31.999999798486137
target distance 9.0
model initialize at round 1866
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  4.]), 'previousTarget': array([25.,  4.]), 'currentState': array([24.5, 12.5,  0. ]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 8.514693182963116}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18359594187837391
{'scaleFactor': 20, 'currentTarget': array([55.96095571, 18.82345792]), 'previousTarget': array([55.46370789, 18.59731672]), 'currentState': array([74.        , 27.46017496,  0.        ]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 20.0}
episode index:1867
target Thresh 31.99999980049123
target distance 10.0
model initialize at round 1867
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([20.5       , 12.49999994,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 11.067971791740769}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1834976571129144
{'scaleFactor': 20, 'currentTarget': array([51.8076933 , 28.09496016]), 'previousTarget': array([51.30967085, 27.8793776 ]), 'currentState': array([70.     , 36.40399,  0.     ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 20.0}
episode index:1868
target Thresh 31.999999802476378
target distance 6.0
model initialize at round 1868
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 7.]), 'previousTarget': array([8., 7.]), 'currentState': array([10.5, 12.5,  0. ]), 'targetState': array([8, 7], dtype=int32), 'currentDistance': 6.041522986797246}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1833994775210937
{'scaleFactor': 20, 'currentTarget': array([42.16935689, 24.36029746]), 'previousTarget': array([41.66413314, 24.07918119]), 'currentState': array([60.        , 33.41944567,  0.        ]), 'targetState': array([8, 7], dtype=int32), 'currentDistance': 20.0}
episode index:1869
target Thresh 31.99999980444177
target distance 21.0
model initialize at round 1869
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.68257132,  5.18314326]), 'previousTarget': array([19.49442256,  5.76952105]), 'currentState': array([14.5, 24.5,  0. ]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.183301402934184
{'scaleFactor': 20, 'currentTarget': array([45.64789927, 15.11030352]), 'previousTarget': array([45.15771088, 14.93480391]), 'currentState': array([64.        , 23.06017136,  0.        ]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 20.0}
episode index:1870
target Thresh 31.999999806387606
target distance 20.0
model initialize at round 1870
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 7.]), 'previousTarget': array([8.9223227 , 7.38838649]), 'currentState': array([ 5.5, 26.5,  0. ]), 'targetState': array([9, 7], dtype=int32), 'currentDistance': 19.811612756158873}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18320343318381835
{'scaleFactor': 20, 'currentTarget': array([36.81031451, 19.71249421]), 'previousTarget': array([36.31363514, 19.49863898]), 'currentState': array([55.       , 28.0272607,  0.       ]), 'targetState': array([9, 7], dtype=int32), 'currentDistance': 20.0}
episode index:1871
target Thresh 31.999999808314083
target distance 7.0
model initialize at round 1871
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'previousTarget': array([ 5., 22.]), 'currentState': array([12.5, 20.5,  0. ]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 7.648529270389241}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18310556810198939
{'scaleFactor': 20, 'currentTarget': array([44.32313026, 42.81214812]), 'previousTarget': array([43.81330897, 42.49013899]), 'currentState': array([62.        , 52.16780289,  0.        ]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 20.0}
episode index:1872
target Thresh 31.99999981022139
target distance 23.0
model initialize at round 1872
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.15788915, 12.53751079]), 'previousTarget': array([ 9.91602889, 12.60106106]), 'currentState': array([27.5, 22.5,  0. ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18300780752104864
{'scaleFactor': 20, 'currentTarget': array([59.21236691, 37.38060343]), 'previousTarget': array([58.70305731, 37.04835729]), 'currentState': array([77.        , 46.52391296,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 20.0}
episode index:1873
target Thresh 31.999999812109717
target distance 21.0
model initialize at round 1873
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.37163063,  6.71754296]), 'previousTarget': array([22.11990655,  7.3102453 ]), 'currentState': array([15.5, 25.5,  0. ]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1829101512737055
{'scaleFactor': 20, 'currentTarget': array([47.7306013 , 19.44650522]), 'previousTarget': array([47.21594521, 19.09866079]), 'currentState': array([65.        , 29.53451139,  0.        ]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 20.0}
episode index:1874
target Thresh 31.999999813979258
target distance 17.0
model initialize at round 1874
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4.42284624, 10.55815704]), 'previousTarget': array([ 4.46633605, 10.66064273]), 'currentState': array([16.5, 26.5,  0. ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1828125991930262
{'scaleFactor': 20, 'currentTarget': array([48.97185247, 37.70467046]), 'previousTarget': array([48.47844791, 37.43928445]), 'currentState': array([66.        , 48.19477016,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:1875
target Thresh 31.999999815830193
target distance 10.0
model initialize at round 1875
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 17.]), 'previousTarget': array([13., 17.]), 'currentState': array([3.5, 7.5, 0. ]), 'targetState': array([13, 17], dtype=int32), 'currentDistance': 13.435028842544408}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.1831599878722677
{'scaleFactor': 20, 'currentTarget': array([13., 17.]), 'previousTarget': array([13., 17.]), 'currentState': array([12.5       , 16.49933964,  0.        ]), 'targetState': array([13, 17], dtype=int32), 'currentDistance': 0.7075738801940381}
episode index:1876
target Thresh 31.999999817662715
target distance 9.0
model initialize at round 1876
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  6.]), 'previousTarget': array([18.,  6.]), 'currentState': array([20.5, 14.5,  0. ]), 'targetState': array([18,  6], dtype=int32), 'currentDistance': 8.860022573334621}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1830624066320587
{'scaleFactor': 20, 'currentTarget': array([52.42809657, 24.71366202]), 'previousTarget': array([51.93415378, 24.47304728]), 'currentState': array([70.      , 34.265008,  0.      ]), 'targetState': array([18,  6], dtype=int32), 'currentDistance': 20.0}
episode index:1877
target Thresh 31.999999819477
target distance 1.0
model initialize at round 1877
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([24.5       ,  9.49999586,  0.        ]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 0.7071097103986351}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.1834974106753856
{'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([24.5       ,  9.49999586,  0.        ]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 0.7071097103986351}
episode index:1878
target Thresh 31.999999821273235
target distance 11.0
model initialize at round 1878
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  8.]), 'previousTarget': array([24.,  8.]), 'currentState': array([13.5,  2.5,  0. ]), 'targetState': array([24,  8], dtype=int32), 'currentDistance': 11.85326959112963}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.1838263799030938
{'scaleFactor': 20, 'currentTarget': array([24.,  8.]), 'previousTarget': array([24.,  8.]), 'currentState': array([24.5       ,  8.81629498,  0.        ]), 'targetState': array([24,  8], dtype=int32), 'currentDistance': 0.9572551902814561}
episode index:1879
target Thresh 31.999999823051596
target distance 1.0
model initialize at round 1879
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 8.]), 'previousTarget': array([8., 8.]), 'currentState': array([8.5       , 7.49999252, 0.        ]), 'targetState': array([8, 8], dtype=int32), 'currentDistance': 0.7071120706358824}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.18426051480740066
{'scaleFactor': 20, 'currentTarget': array([8., 8.]), 'previousTarget': array([8., 8.]), 'currentState': array([8.5       , 7.49999252, 0.        ]), 'targetState': array([8, 8], dtype=int32), 'currentDistance': 0.7071120706358824}
episode index:1880
target Thresh 31.99999982481226
target distance 22.0
model initialize at round 1880
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7.89196526, 26.45150202]), 'previousTarget': array([ 7.81071492, 25.91786413]), 'currentState': array([6.5, 6.5, 0. ]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18416255600101714
{'scaleFactor': 20, 'currentTarget': array([38.72442198, 45.92257949]), 'previousTarget': array([38.19147429, 45.47927459]), 'currentState': array([56., 56.,  0.]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 20.0}
episode index:1881
target Thresh 31.99999982655541
target distance 16.0
model initialize at round 1881
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.16314258,  2.89457173]), 'previousTarget': array([25.67882258,  3.40925592]), 'currentState': array([12.5, 17.5,  0. ]), 'targetState': array([27,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1840647012953843
{'scaleFactor': 20, 'currentTarget': array([44.65926782, 12.14793592]), 'previousTarget': array([44.14651046, 11.82406727]), 'currentState': array([62.        , 22.11282465,  0.        ]), 'targetState': array([27,  2], dtype=int32), 'currentDistance': 20.0}
episode index:1882
target Thresh 31.99999982828121
target distance 9.0
model initialize at round 1882
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 20.]), 'previousTarget': array([19., 20.]), 'currentState': array([10.5, 17.5,  0. ]), 'targetState': array([19, 20], dtype=int32), 'currentDistance': 8.860022573334575}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18396695052464856
{'scaleFactor': 20, 'currentTarget': array([42.2204953, 31.9620298]), 'previousTarget': array([41.70605449, 31.65168282]), 'currentState': array([60.        , 41.12113525,  0.        ]), 'targetState': array([19, 20], dtype=int32), 'currentDistance': 20.0}
episode index:1883
target Thresh 31.999999829989843
target distance 19.0
model initialize at round 1883
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6.27268193, 15.6597004 ]), 'previousTarget': array([ 6.09022194, 15.67985983]), 'currentState': array([23.5,  5.5,  0. ]), 'targetState': array([ 4, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18386930352330852
{'scaleFactor': 20, 'currentTarget': array([55.48104193, 45.35185584]), 'previousTarget': array([54.95678935, 44.89604556]), 'currentState': array([73.        , 54.99997008,  0.        ]), 'targetState': array([ 4, 17], dtype=int32), 'currentDistance': 20.0}
episode index:1884
target Thresh 31.99999983168147
target distance 10.0
model initialize at round 1884
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([11.5, 11.5,  0. ]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 10.511898020814213}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18377176012621393
{'scaleFactor': 20, 'currentTarget': array([45.17295811, 43.53762793]), 'previousTarget': array([44.66722536, 43.12546177]), 'currentState': array([61.        , 55.76484123,  0.        ]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 20.0}
episode index:1885
target Thresh 31.99999983335627
target distance 8.0
model initialize at round 1885
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 13.]), 'previousTarget': array([16., 13.]), 'currentState': array([ 8.5       , 18.41099435,  0.        ]), 'targetState': array([16, 13], dtype=int32), 'currentDistance': 9.248181435589439}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18367432016856483
{'scaleFactor': 20, 'currentTarget': array([42.57804853, 34.94624362]), 'previousTarget': array([42.06816865, 34.4912125 ]), 'currentState': array([58.        , 47.68058353,  0.        ]), 'targetState': array([16, 13], dtype=int32), 'currentDistance': 20.0}
episode index:1886
target Thresh 31.9999998350144
target distance 15.0
model initialize at round 1886
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([21.5, 24.5,  0. ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 19.811612756158944}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18357698348591056
{'scaleFactor': 20, 'currentTarget': array([55.95236851, 51.9834119 ]), 'previousTarget': array([55.46018035, 51.60221183]), 'currentState': array([71.        , 65.15796262,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 20.0}
episode index:1887
target Thresh 31.999999836656034
target distance 17.0
model initialize at round 1887
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 19.]), 'previousTarget': array([ 4., 19.]), 'currentState': array([21.5, 20.5,  0. ]), 'targetState': array([ 4, 19], dtype=int32), 'currentDistance': 17.56416807025038}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18347974991414898
{'scaleFactor': 20, 'currentTarget': array([55.08591473, 57.8862933 ]), 'previousTarget': array([54.57214184, 57.40440847]), 'currentState': array([71., 70.,  0.]), 'targetState': array([ 4, 19], dtype=int32), 'currentDistance': 20.0}
episode index:1888
target Thresh 31.999999838281337
target distance 6.0
model initialize at round 1888
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 16.]), 'previousTarget': array([17., 16.]), 'currentState': array([11.5       , 20.14160457,  0.        ]), 'targetState': array([17, 16], dtype=int32), 'currentDistance': 6.884975558071247}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.18385659401385493
{'scaleFactor': 20, 'currentTarget': array([17., 16.]), 'previousTarget': array([17., 16.]), 'currentState': array([17.        , 16.98425255,  0.        ]), 'targetState': array([17, 16], dtype=int32), 'currentDistance': 0.9842525497075201}
episode index:1889
target Thresh 31.999999839890464
target distance 19.0
model initialize at round 1889
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 12.]), 'previousTarget': array([23.4327075, 11.76114  ]), 'currentState': array([5.5, 4.5, 0. ]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 19.962464777677138}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.18412045539004357
{'scaleFactor': 20, 'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([24.5       , 12.70766357,  0.        ]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 0.8664800822947696}
episode index:1890
target Thresh 31.99999984148358
target distance 15.0
model initialize at round 1890
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 6.]), 'previousTarget': array([7., 6.]), 'currentState': array([22.5, 16.5,  0. ]), 'targetState': array([7, 6], dtype=int32), 'currentDistance': 18.72164522684917}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18402308867645814
{'scaleFactor': 20, 'currentTarget': array([57.30386356, 52.43387165]), 'previousTarget': array([56.79950064, 51.93865089]), 'currentState': array([72.        , 65.99940053,  0.        ]), 'targetState': array([7, 6], dtype=int32), 'currentDistance': 20.0}
episode index:1891
target Thresh 31.999999843060845
target distance 10.0
model initialize at round 1891
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  5.]), 'previousTarget': array([17.,  5.]), 'currentState': array([ 7.5       , 10.67780119,  0.        ]), 'targetState': array([17,  5], dtype=int32), 'currentDistance': 11.067403777475684}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1839258248875171
{'scaleFactor': 20, 'currentTarget': array([42.39015268, 28.73668194]), 'previousTarget': array([41.88701692, 28.25560522]), 'currentState': array([57.        , 42.39509917,  0.        ]), 'targetState': array([17,  5], dtype=int32), 'currentDistance': 20.0}
episode index:1892
target Thresh 31.999999844622415
target distance 9.0
model initialize at round 1892
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([14.5,  3.5,  0. ]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 9.617692030835604}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.18426950578374668
{'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([23.5       ,  8.78033502,  0.        ]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 0.9267808531044134}
episode index:1893
target Thresh 31.999999846168446
target distance 17.0
model initialize at round 1893
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 28.]), 'previousTarget': array([14., 28.]), 'currentState': array([12.5, 11.5,  0. ]), 'targetState': array([14, 28], dtype=int32), 'currentDistance': 16.568041525780806}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18417221459801078
{'scaleFactor': 20, 'currentTarget': array([45.51916152, 49.66942354]), 'previousTarget': array([44.99385477, 49.20632169]), 'currentState': array([62., 61.,  0.]), 'targetState': array([14, 28], dtype=int32), 'currentDistance': 20.0}
episode index:1894
target Thresh 31.999999847699097
target distance 15.0
model initialize at round 1894
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  8.]), 'previousTarget': array([11.,  8.]), 'currentState': array([19.5, 22.5,  0. ]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 16.807736313971585}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18407502609426513
{'scaleFactor': 20, 'currentTarget': array([54.53490453, 49.5681888 ]), 'previousTarget': array([54.03425391, 49.08628012]), 'currentState': array([69.       , 63.3798148,  0.       ]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 20.0}
episode index:1895
target Thresh 31.999999849214518
target distance 7.0
model initialize at round 1895
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 23.]), 'previousTarget': array([ 3., 23.]), 'currentState': array([10.5, 28.5,  0. ]), 'targetState': array([ 3, 23], dtype=int32), 'currentDistance': 9.30053761886925}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1839779401100382
{'scaleFactor': 20, 'currentTarget': array([45.59605601, 64.03298463]), 'previousTarget': array([45.09441407, 63.54014751]), 'currentState': array([60.        , 77.90837282,  0.        ]), 'targetState': array([ 3, 23], dtype=int32), 'currentDistance': 20.0}
episode index:1896
target Thresh 31.999999850714858
target distance 15.0
model initialize at round 1896
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.6421334 , 12.35786671]), 'previousTarget': array([21.14213562, 12.85786438]), 'currentState': array([ 7.5       , 26.50000456,  0.        ]), 'targetState': array([22, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18388095648320107
{'scaleFactor': 20, 'currentTarget': array([42.55016033, 31.6652084 ]), 'previousTarget': array([42.04665074, 31.17363519]), 'currentState': array([57.        , 45.49279436,  0.        ]), 'targetState': array([22, 12], dtype=int32), 'currentDistance': 20.0}
episode index:1897
target Thresh 31.99999985220027
target distance 5.0
model initialize at round 1897
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 17.]), 'previousTarget': array([26., 17.]), 'currentState': array([25.5       , 21.50007311,  0.        ]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 4.52776522704252}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1837840750519665
{'scaleFactor': 20, 'currentTarget': array([60.56899535, 50.17061062]), 'previousTarget': array([60.06708124, 49.67995558]), 'currentState': array([75.       , 64.0178524,  0.       ]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 20.0}
episode index:1898
target Thresh 31.9999998536709
target distance 8.0
model initialize at round 1898
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  7.]), 'previousTarget': array([24.,  7.]), 'currentState': array([22.5       , 14.50000009,  0.        ]), 'targetState': array([24,  7], dtype=int32), 'currentDistance': 7.648529358059897}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18368729565488806
{'scaleFactor': 20, 'currentTarget': array([57.67433773, 39.80585855]), 'previousTarget': array([57.17382185, 39.3158612 ]), 'currentState': array([72.        , 53.76205432,  0.        ]), 'targetState': array([24,  7], dtype=int32), 'currentDistance': 20.0}
episode index:1899
target Thresh 31.999999855126898
target distance 5.0
model initialize at round 1899
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 12.]), 'previousTarget': array([ 5., 12.]), 'currentState': array([10.5,  7.5,  0. ]), 'targetState': array([ 5, 12], dtype=int32), 'currentDistance': 7.106335201776012}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18359061813085917
{'scaleFactor': 20, 'currentTarget': array([44.52085402, 44.33524419]), 'previousTarget': array([44.00819701, 43.85072967]), 'currentState': array([60., 57.,  0.]), 'targetState': array([ 5, 12], dtype=int32), 'currentDistance': 20.0}
episode index:1900
target Thresh 31.99999985656841
target distance 3.0
model initialize at round 1900
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  5.]), 'previousTarget': array([10.,  5.]), 'currentState': array([7.5       , 5.45306677, 0.        ]), 'targetState': array([10,  5], dtype=int32), 'currentDistance': 2.54072223879179}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.1840044573638256
{'scaleFactor': 20, 'currentTarget': array([10.,  5.]), 'previousTarget': array([10.,  5.]), 'currentState': array([9.       , 5.4550381, 0.       ]), 'targetState': array([10,  5], dtype=int32), 'currentDistance': 1.0986626747500339}
episode index:1901
target Thresh 31.99999985799558
target distance 14.0
model initialize at round 1901
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 12.]), 'previousTarget': array([ 2., 12.]), 'currentState': array([16.5, 25.5,  0. ]), 'targetState': array([ 2, 12], dtype=int32), 'currentDistance': 19.81161275615906}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18390771474691506
{'scaleFactor': 20, 'currentTarget': array([51.74692486, 60.96944791]), 'previousTarget': array([51.24604853, 60.47034345]), 'currentState': array([66.        , 74.99976683,  0.        ]), 'targetState': array([ 2, 12], dtype=int32), 'currentDistance': 20.0}
episode index:1902
target Thresh 31.999999859408547
target distance 11.0
model initialize at round 1902
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([9.5       , 9.54181793, 0.        ]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 11.440197119780528}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.1842452139109133
{'scaleFactor': 20, 'currentTarget': array([20.,  5.]), 'previousTarget': array([20.,  5.]), 'currentState': array([19.        ,  5.16516614,  0.        ]), 'targetState': array([20,  5], dtype=int32), 'currentDistance': 1.013548150641156}
episode index:1903
target Thresh 31.999999860807456
target distance 12.0
model initialize at round 1903
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([15.5       , 16.50070906,  0.        ]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 13.729917565505378}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.18457372370834524
{'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([26.        ,  9.49305026,  0.        ]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 1.1149432995368622}
episode index:1904
target Thresh 31.999999862192443
target distance 10.0
model initialize at round 1904
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  2.]), 'previousTarget': array([11.,  2.]), 'currentState': array([ 4.5       , 11.50000069,  0.        ]), 'targetState': array([11,  2], dtype=int32), 'currentDistance': 11.510864998930991}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18447683461453507
{'scaleFactor': 20, 'currentTarget': array([39.69258201, 30.0257363 ]), 'previousTarget': array([39.19062643, 29.52773871]), 'currentState': array([54.        , 44.00063488,  0.        ]), 'targetState': array([11,  2], dtype=int32), 'currentDistance': 20.0}
episode index:1905
target Thresh 31.99999986356365
target distance 16.0
model initialize at round 1905
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 27.]), 'previousTarget': array([13., 27.]), 'currentState': array([ 2.5, 11.5,  0. ]), 'targetState': array([13, 27], dtype=int32), 'currentDistance': 18.72164522684899}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18438004718818957
{'scaleFactor': 20, 'currentTarget': array([36.9245454 , 47.85729599]), 'previousTarget': array([36.41210878, 47.37157517]), 'currentState': array([52., 61.,  0.]), 'targetState': array([13, 27], dtype=int32), 'currentDistance': 20.0}
episode index:1906
target Thresh 31.999999864921218
target distance 15.0
model initialize at round 1906
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([25.5, 14.5,  0. ]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 15.508062419270903}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1842833612693704
{'scaleFactor': 20, 'currentTarget': array([59.14752022, 51.80578478]), 'previousTarget': array([58.63381348, 51.3236243 ]), 'currentState': array([75., 64.,  0.]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 20.0}
episode index:1907
target Thresh 31.999999866265274
target distance 22.0
model initialize at round 1907
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.28779003, 22.64422855]), 'previousTarget': array([11.06407315, 22.0585156 ]), 'currentState': array([5.5, 3.5, 0. ]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1841867766984745
{'scaleFactor': 20, 'currentTarget': array([38.24003831, 42.08653657]), 'previousTarget': array([37.70859686, 41.63497444]), 'currentState': array([55., 53.,  0.]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 20.0}
episode index:1908
target Thresh 31.999999867595957
target distance 23.0
model initialize at round 1908
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9.53566556, 11.70894272]), 'previousTarget': array([ 8.95731557, 11.37089005]), 'currentState': array([27.5, 20.5,  0. ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18409029331623328
{'scaleFactor': 20, 'currentTarget': array([61.65282073, 57.17564472]), 'previousTarget': array([61.1442584 , 56.68589839]), 'currentState': array([77., 70.,  0.]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 20.0}
episode index:1909
target Thresh 31.9999998689134
target distance 6.0
model initialize at round 1909
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'previousTarget': array([13., 22.]), 'currentState': array([19.5       , 28.49983943,  0.        ]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 9.192274612516599}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18399391096371168
{'scaleFactor': 20, 'currentTarget': array([54.8578356 , 63.85766523]), 'previousTarget': array([54.35783534, 63.35766549]), 'currentState': array([69.        , 77.99977207,  0.        ]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 20.0}
episode index:1910
target Thresh 31.99999987021773
target distance 12.0
model initialize at round 1910
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 24.]), 'previousTarget': array([26., 24.]), 'currentState': array([16.5, 12.5,  0. ]), 'targetState': array([26, 24], dtype=int32), 'currentDistance': 14.916433890176183}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18389762948230734
{'scaleFactor': 20, 'currentTarget': array([51.50001133, 48.22501076]), 'previousTarget': array([50.99542811, 47.72983681]), 'currentState': array([66., 62.,  0.]), 'targetState': array([26, 24], dtype=int32), 'currentDistance': 20.0}
episode index:1911
target Thresh 31.999999871509086
target distance 6.0
model initialize at round 1911
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 18.]), 'previousTarget': array([18., 18.]), 'currentState': array([20.5, 12.5,  0. ]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 6.041522986797246}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18380144871374965
{'scaleFactor': 20, 'currentTarget': array([54.73227429, 49.08115517]), 'previousTarget': array([54.22102674, 48.59445948]), 'currentState': array([70., 62.,  0.]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 20.0}
episode index:1912
target Thresh 31.999999872787594
target distance 9.0
model initialize at round 1912
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 20.]), 'previousTarget': array([13., 20.]), 'currentState': array([20.5, 11.5,  0. ]), 'targetState': array([13, 20], dtype=int32), 'currentDistance': 11.335784048754626}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18370536850009897
{'scaleFactor': 20, 'currentTarget': array([53.76391218, 49.32141052]), 'previousTarget': array([53.24479271, 48.84803725]), 'currentState': array([70., 61.,  0.]), 'targetState': array([13, 20], dtype=int32), 'currentDistance': 20.0}
episode index:1913
target Thresh 31.999999874053376
target distance 15.0
model initialize at round 1913
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 25.]), 'previousTarget': array([12., 25.]), 'currentState': array([ 3.5, 10.5,  0. ]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 16.807736313971542}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18360938868374574
{'scaleFactor': 20, 'currentTarget': array([37.78870996, 47.0147524 ]), 'previousTarget': array([37.27513539, 46.53067089]), 'currentState': array([53., 60.,  0.]), 'targetState': array([12, 25], dtype=int32), 'currentDistance': 20.0}
episode index:1914
target Thresh 31.999999875306568
target distance 8.0
model initialize at round 1914
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 15.]), 'previousTarget': array([ 5., 15.]), 'currentState': array([2.5, 7.5, 0. ]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 7.9056941504209535}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18351350910740957
{'scaleFactor': 20, 'currentTarget': array([37.08687799, 43.67338033]), 'previousTarget': array([36.57840028, 43.18287337]), 'currentState': array([52., 57.,  0.]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 20.0}
episode index:1915
target Thresh 31.99999987654729
target distance 16.0
model initialize at round 1915
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.55815704, 12.42284624]), 'previousTarget': array([10., 12.]), 'currentState': array([26.5, 24.5,  0. ]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18341772961413846
{'scaleFactor': 20, 'currentTarget': array([61.42303856, 60.30649077]), 'previousTarget': array([60.91967285, 59.8100745 ]), 'currentState': array([76., 74.,  0.]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 20.0}
episode index:1916
target Thresh 31.99999987777566
target distance 7.0
model initialize at round 1916
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 29.]), 'previousTarget': array([ 3., 29.]), 'currentState': array([ 7.5, 22.5,  0. ]), 'targetState': array([ 3, 29], dtype=int32), 'currentDistance': 7.905694150420879}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18332205004730795
{'scaleFactor': 20, 'currentTarget': array([41.35439711, 59.54146436]), 'previousTarget': array([40.83987962, 59.05971746]), 'currentState': array([57., 72.,  0.]), 'targetState': array([ 3, 29], dtype=int32), 'currentDistance': 20.0}
episode index:1917
target Thresh 31.999999878991815
target distance 4.0
model initialize at round 1917
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  7.]), 'previousTarget': array([18.,  7.]), 'currentState': array([18.5,  3.5,  0. ]), 'targetState': array([18,  7], dtype=int32), 'currentDistance': 3.5355339059327537}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18322647025062008
{'scaleFactor': 20, 'currentTarget': array([53.28137976, 39.45886938]), 'previousTarget': array([52.77545248, 38.96531491]), 'currentState': array([68., 53.,  0.]), 'targetState': array([18,  7], dtype=int32), 'currentDistance': 20.0}
episode index:1918
target Thresh 31.999999880195865
target distance 26.0
model initialize at round 1918
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2.1079185 , 22.49615643]), 'previousTarget': array([ 2., 22.]), 'currentState': array([2.5, 2.5, 0. ]), 'targetState': array([ 2, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18313099006810282
{'scaleFactor': 20, 'currentTarget': array([33.96953885, 43.34537865]), 'previousTarget': array([33.43268208, 42.92258644]), 'currentState': array([52., 52.,  0.]), 'targetState': array([ 2, 28], dtype=int32), 'currentDistance': 20.0}
episode index:1919
target Thresh 31.999999881387936
target distance 12.0
model initialize at round 1919
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 14.]), 'previousTarget': array([ 7., 14.]), 'currentState': array([19.5,  7.5,  0. ]), 'targetState': array([ 7, 14], dtype=int32), 'currentDistance': 14.089002803605442}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18303560934410903
{'scaleFactor': 20, 'currentTarget': array([52.56570444, 45.60202082]), 'previousTarget': array([52.04653075, 45.12971637]), 'currentState': array([69., 57.,  0.]), 'targetState': array([ 7, 14], dtype=int32), 'currentDistance': 20.0}
episode index:1920
target Thresh 31.999999882568147
target distance 14.0
model initialize at round 1920
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([10.5, 19.5,  0. ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 15.443445211480467}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18294032792331563
{'scaleFactor': 20, 'currentTarget': array([45.9734779 , 49.67902287]), 'previousTarget': array([45.47449671, 49.17802059]), 'currentState': array([60.       , 63.9358345,  0.       ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 20.0}
episode index:1921
target Thresh 31.999999883736614
target distance 16.0
model initialize at round 1921
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 19.]), 'previousTarget': array([18., 19.]), 'currentState': array([7.5, 3.5, 0. ]), 'targetState': array([18, 19], dtype=int32), 'currentDistance': 18.72164522684906}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18284514565072285
{'scaleFactor': 20, 'currentTarget': array([41.9245454 , 39.85729599]), 'previousTarget': array([41.41210878, 39.37157517]), 'currentState': array([57., 53.,  0.]), 'targetState': array([18, 19], dtype=int32), 'currentDistance': 20.0}
episode index:1922
target Thresh 31.999999884893455
target distance 6.0
model initialize at round 1922
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 23.]), 'previousTarget': array([11., 23.]), 'currentState': array([ 8.5       , 28.50003716,  0.        ]), 'targetState': array([11, 23], dtype=int32), 'currentDistance': 6.041556819217717}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1827500623716533
{'scaleFactor': 20, 'currentTarget': array([43.28378237, 52.71165687]), 'previousTarget': array([42.7774993 , 52.21848704]), 'currentState': array([58.        , 66.25539854,  0.        ]), 'targetState': array([11, 23], dtype=int32), 'currentDistance': 20.0}
episode index:1923
target Thresh 31.999999886038783
target distance 6.0
model initialize at round 1923
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  5.]), 'previousTarget': array([23.,  5.]), 'currentState': array([17.5      ,  6.4990131,  0.       ]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 5.7006175334134515}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.1831298789959319
{'scaleFactor': 20, 'currentTarget': array([23.,  5.]), 'previousTarget': array([23.,  5.]), 'currentState': array([22.        ,  5.82962033,  0.        ]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 1.2993344044083948}
episode index:1924
target Thresh 31.999999887172716
target distance 7.0
model initialize at round 1924
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 11.]), 'previousTarget': array([24., 11.]), 'currentState': array([17.5       , 15.48547935,  0.        ]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 7.89743787837932}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.18349985737269178
{'scaleFactor': 20, 'currentTarget': array([24., 11.]), 'previousTarget': array([24., 11.]), 'currentState': array([23.        , 11.96712179,  0.        ]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 1.3911594258028495}
episode index:1925
target Thresh 31.999999888295367
target distance 20.0
model initialize at round 1925
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 3.]), 'previousTarget': array([5.00124766, 3.02495322]), 'currentState': array([ 6.5, 22.5,  0. ]), 'targetState': array([5, 3], dtype=int32), 'currentDistance': 19.557607215607867}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18340458226502165
{'scaleFactor': 20, 'currentTarget': array([41.35358336, 36.80393734]), 'previousTarget': array([40.85898488, 36.37055063]), 'currentState': array([56.       , 50.4231326,  0.       ]), 'targetState': array([5, 3], dtype=int32), 'currentDistance': 20.0}
episode index:1926
target Thresh 31.999999889406848
target distance 2.0
model initialize at round 1926
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([16.5       ,  3.12379102,  0.        ]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 1.5050994039597834}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.18382315798776944
{'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'previousTarget': array([18.,  3.]), 'currentState': array([17.        ,  2.82448722,  0.        ]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 1.0152855446014137}
episode index:1927
target Thresh 31.999999890507265
target distance 10.0
model initialize at round 1927
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 13.]), 'previousTarget': array([ 6., 13.]), 'currentState': array([16.5, 14.5,  0. ]), 'targetState': array([ 6, 13], dtype=int32), 'currentDistance': 10.606601717798306}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1837278140261575
{'scaleFactor': 20, 'currentTarget': array([50.76121364, 51.0470316 ]), 'previousTarget': array([50.25173301, 50.55819356]), 'currentState': array([66., 64.,  0.]), 'targetState': array([ 6, 13], dtype=int32), 'currentDistance': 20.0}
episode index:1928
target Thresh 31.999999891596737
target distance 8.0
model initialize at round 1928
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 21.]), 'previousTarget': array([21., 21.]), 'currentState': array([15.5       , 28.50004733,  0.        ]), 'targetState': array([21, 21], dtype=int32), 'currentDistance': 9.300575782904842}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18363256891779767
{'scaleFactor': 20, 'currentTarget': array([50.32408522, 48.14911984]), 'previousTarget': array([49.82201236, 47.67612019]), 'currentState': array([65.        , 61.73652303,  0.        ]), 'targetState': array([21, 21], dtype=int32), 'currentDistance': 20.0}
episode index:1929
target Thresh 31.99999989267537
target distance 26.0
model initialize at round 1929
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.94637968, 12.79288008]), 'previousTarget': array([11.74852236, 13.02006875]), 'currentState': array([25.5, 27.5,  0. ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18353742250903196
{'scaleFactor': 20, 'currentTarget': array([60.29637371, 55.75190048]), 'previousTarget': array([59.79377201, 55.26796067]), 'currentState': array([75.        , 69.30931078,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 20.0}
episode index:1930
target Thresh 31.999999893743265
target distance 11.0
model initialize at round 1930
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([ 4.5, 16.5,  0. ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 10.606601717798135}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1834423746465208
{'scaleFactor': 20, 'currentTarget': array([39.21272232, 38.97732799]), 'previousTarget': array([38.70638208, 38.48547814]), 'currentState': array([54.        , 52.44344914,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 20.0}
episode index:1931
target Thresh 31.99999989480054
target distance 6.0
model initialize at round 1931
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'previousTarget': array([ 9., 21.]), 'currentState': array([11.5, 15.5,  0. ]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 6.041522986797246}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18334742517724206
{'scaleFactor': 20, 'currentTarget': array([45.73227429, 52.08115517]), 'previousTarget': array([45.22102674, 51.59445948]), 'currentState': array([61., 65.,  0.]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 20.0}
episode index:1932
target Thresh 31.99999989584729
target distance 8.0
model initialize at round 1932
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 13.]), 'previousTarget': array([ 4., 13.]), 'currentState': array([12.5, 17.5,  0. ]), 'targetState': array([ 4, 13], dtype=int32), 'currentDistance': 9.617692030835782}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18325257394849026
{'scaleFactor': 20, 'currentTarget': array([47.32994539, 53.15087862]), 'previousTarget': array([46.82527942, 52.65592186]), 'currentState': array([62.       , 66.7446087,  0.       ]), 'targetState': array([ 4, 13], dtype=int32), 'currentDistance': 20.0}
episode index:1933
target Thresh 31.999999896883626
target distance 11.0
model initialize at round 1933
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 18.]), 'previousTarget': array([ 2., 18.]), 'currentState': array([13.5,  9.5,  0. ]), 'targetState': array([ 2, 18], dtype=int32), 'currentDistance': 14.300349646075109}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18315782080787574
{'scaleFactor': 20, 'currentTarget': array([46.40097646, 47.84327926]), 'previousTarget': array([45.8801557, 47.3743191]), 'currentState': array([63., 59.,  0.]), 'targetState': array([ 2, 18], dtype=int32), 'currentDistance': 20.0}
episode index:1934
target Thresh 31.999999897909653
target distance 12.0
model initialize at round 1934
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  5.]), 'previousTarget': array([21.,  5.]), 'currentState': array([ 9.5       , 14.54145971,  0.        ]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 14.942872996389047}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.18347744497776267
{'scaleFactor': 20, 'currentTarget': array([21.,  5.]), 'previousTarget': array([21.,  5.]), 'currentState': array([20.5       ,  5.63514209,  0.        ]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 0.8083349997781935}
episode index:1935
target Thresh 31.999999898925466
target distance 17.0
model initialize at round 1935
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7.22533058, 14.87523613]), 'previousTarget': array([ 6.66064273, 14.46633605]), 'currentState': array([23.5, 26.5,  0. ]), 'targetState': array([ 6, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18338267357023283
{'scaleFactor': 20, 'currentTarget': array([58.32074187, 62.4162089 ]), 'previousTarget': array([57.81663545, 61.92064782]), 'currentState': array([73., 76.,  0.]), 'targetState': array([ 6, 14], dtype=int32), 'currentDistance': 20.0}
episode index:1936
target Thresh 31.999999899931176
target distance 16.0
model initialize at round 1936
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  5.]), 'previousTarget': array([10.,  5.]), 'currentState': array([26.5,  4.5,  0. ]), 'targetState': array([10,  5], dtype=int32), 'currentDistance': 16.507574019219255}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1832880000165053
{'scaleFactor': 20, 'currentTarget': array([59.94179597, 42.07800003]), 'previousTarget': array([59.42668285, 41.59838348]), 'currentState': array([76., 54.,  0.]), 'targetState': array([10,  5], dtype=int32), 'currentDistance': 20.0}
episode index:1937
target Thresh 31.999999900926877
target distance 8.0
model initialize at round 1937
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 28.]), 'previousTarget': array([14., 28.]), 'currentState': array([ 8.5, 20.5,  0. ]), 'targetState': array([14, 28], dtype=int32), 'currentDistance': 9.300537618869027}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1831934241651036
{'scaleFactor': 20, 'currentTarget': array([43.53258208, 56.18893711]), 'previousTarget': array([43.02881598, 55.6929629 ]), 'currentState': array([58.        , 69.99813038,  0.        ]), 'targetState': array([14, 28], dtype=int32), 'currentDistance': 20.0}
episode index:1938
target Thresh 31.999999901912673
target distance 5.0
model initialize at round 1938
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 29.]), 'previousTarget': array([18., 29.]), 'currentState': array([19.5, 24.5,  0. ]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 4.743416490252516}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18309894586486372
{'scaleFactor': 20, 'currentTarget': array([54.00324289, 60.76756726]), 'previousTarget': array([53.4945748 , 60.27739759]), 'currentState': array([69., 74.,  0.]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 20.0}
episode index:1939
target Thresh 31.999999902888657
target distance 27.0
model initialize at round 1939
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.62154811,  8.58840995]), 'previousTarget': array([13.52256629,  9.05464491]), 'currentState': array([15.5, 28.5,  0. ]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18300456496493336
{'scaleFactor': 20, 'currentTarget': array([50.6957145 , 38.83613918]), 'previousTarget': array([50.19468835, 38.34120018]), 'currentState': array([65.        , 52.81424408,  0.        ]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 20.0}
episode index:1940
target Thresh 31.99999990385493
target distance 14.0
model initialize at round 1940
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 21.]), 'previousTarget': array([25., 21.]), 'currentState': array([11.5       , 22.49999651,  0.        ]), 'targetState': array([25, 21], dtype=int32), 'currentDistance': 13.583077322145925}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.18331101359679025
{'scaleFactor': 20, 'currentTarget': array([25., 21.]), 'previousTarget': array([25., 21.]), 'currentState': array([24.        , 21.94930059,  0.        ]), 'targetState': array([25, 21], dtype=int32), 'currentDistance': 1.3788297954470585}
episode index:1941
target Thresh 31.99999990481159
target distance 11.0
model initialize at round 1941
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 28.]), 'previousTarget': array([ 8., 28.]), 'currentState': array([13.5, 17.5,  0. ]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 11.853269591129662}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18321662069586503
{'scaleFactor': 20, 'currentTarget': array([46.68534218, 55.43142445]), 'previousTarget': array([46.16479708, 54.96045298]), 'currentState': array([63., 67.,  0.]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 20.0}
episode index:1942
target Thresh 31.99999990575873
target distance 7.0
model initialize at round 1942
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 16.]), 'previousTarget': array([ 4., 16.]), 'currentState': array([11.5, 16.5,  0. ]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 7.516648189186539}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18312232495695824
{'scaleFactor': 20, 'currentTarget': array([45.96481774, 52.81124363]), 'previousTarget': array([45.45671583, 52.32048555]), 'currentState': array([61., 66.,  0.]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 20.0}
episode index:1943
target Thresh 31.99999990669645
target distance 18.0
model initialize at round 1943
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([13.5, 27.5,  0. ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 17.67766952966362}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18302812623012854
{'scaleFactor': 20, 'currentTarget': array([48.38779361, 44.94090559]), 'previousTarget': array([47.88741282, 44.47134585]), 'currentState': array([63.        , 58.59679899,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 20.0}
episode index:1944
target Thresh 31.999999907624833
target distance 15.0
model initialize at round 1944
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 14.]), 'previousTarget': array([25., 14.]), 'currentState': array([10.5      , 22.4999522,  0.       ]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 16.807712139150816}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.18332205484759562
{'scaleFactor': 20, 'currentTarget': array([25., 14.]), 'previousTarget': array([25., 14.]), 'currentState': array([24.5       , 14.81506691,  0.        ]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 0.956208171280936}
episode index:1945
target Thresh 31.99999990854398
target distance 1.0
model initialize at round 1945
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  7.]), 'previousTarget': array([24.,  7.]), 'currentState': array([25.5       ,  7.49493513,  0.        ]), 'targetState': array([24,  7], dtype=int32), 'currentDistance': 1.5795444843594657}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18322785029731423
{'scaleFactor': 20, 'currentTarget': array([60.30294553, 40.50552653]), 'previousTarget': array([59.79751898, 40.01253113]), 'currentState': array([75.        , 54.07006079,  0.        ]), 'targetState': array([24,  7], dtype=int32), 'currentDistance': 20.0}
episode index:1946
target Thresh 31.999999909453983
target distance 16.0
model initialize at round 1946
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 10.]), 'previousTarget': array([25., 10.]), 'currentState': array([9.5, 6.5, 0. ]), 'targetState': array([25, 10], dtype=int32), 'currentDistance': 15.890248582070626}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.1835136605300266
{'scaleFactor': 20, 'currentTarget': array([25., 10.]), 'previousTarget': array([25., 10.]), 'currentState': array([24.5       , 10.71219194,  0.        ]), 'targetState': array([25, 10], dtype=int32), 'currentDistance': 0.8701823708157909}
episode index:1947
target Thresh 31.99999991035493
target distance 15.0
model initialize at round 1947
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 3.]), 'previousTarget': array([6., 3.]), 'currentState': array([10.5, 17.5,  0. ]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 15.182226450688924}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18341945433878942
{'scaleFactor': 20, 'currentTarget': array([45.34571641, 39.54384887]), 'previousTarget': array([44.84821395, 39.09505328]), 'currentState': array([60.        , 53.15457892,  0.        ]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 20.0}
episode index:1948
target Thresh 31.999999911246913
target distance 25.0
model initialize at round 1948
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.72220127,  8.53737933]), 'previousTarget': array([18.59490445,  9.06369443]), 'currentState': array([17.5, 28.5,  0. ]), 'targetState': array([19,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18332534481886187
{'scaleFactor': 20, 'currentTarget': array([52.22659299, 34.32062235]), 'previousTarget': array([51.72001992, 33.82912094]), 'currentState': array([67.        , 47.80195928,  0.        ]), 'targetState': array([19,  4], dtype=int32), 'currentDistance': 20.0}
episode index:1949
target Thresh 31.999999912130022
target distance 13.0
model initialize at round 1949
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  3.]), 'previousTarget': array([21.,  3.]), 'currentState': array([19.5, 15.5,  0. ]), 'targetState': array([21,  3], dtype=int32), 'currentDistance': 12.589678312014083}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18323133182151888
{'scaleFactor': 20, 'currentTarget': array([54.24777337, 33.43573844]), 'previousTarget': array([53.7417074 , 32.94544774]), 'currentState': array([69.        , 46.94024915,  0.        ]), 'targetState': array([21,  3], dtype=int32), 'currentDistance': 20.0}
episode index:1950
target Thresh 31.999999913004345
target distance 17.0
model initialize at round 1950
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.97914598, 15.16475581]), 'previousTarget': array([19.43860471, 14.71414506]), 'currentState': array([4.5, 2.5, 0. ]), 'targetState': array([21, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.18351276290190477
{'scaleFactor': 20, 'currentTarget': array([21., 16.]), 'previousTarget': array([21., 16.]), 'currentState': array([20.        , 16.21635461,  0.        ]), 'targetState': array([21, 16], dtype=int32), 'currentDistance': 1.0231369979779108}
episode index:1951
target Thresh 31.999999913869964
target distance 17.0
model initialize at round 1951
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.70229215,  8.47670203]), 'previousTarget': array([15.23243274,  9.00324289]), 'currentState': array([ 2.5, 23.5,  0. ]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1834187502159919
{'scaleFactor': 20, 'currentTarget': array([37.24895618, 25.53959695]), 'previousTarget': array([36.75252952, 25.09468521]), 'currentState': array([52.        , 39.04539964,  0.        ]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 20.0}
episode index:1952
target Thresh 31.999999914726974
target distance 8.0
model initialize at round 1952
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 15.]), 'previousTarget': array([18., 15.]), 'currentState': array([10.5       , 21.48217463,  0.        ]), 'targetState': array([18, 15], dtype=int32), 'currentDistance': 9.913051396909642}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.18376966013025353
{'scaleFactor': 20, 'currentTarget': array([18., 15.]), 'previousTarget': array([18., 15.]), 'currentState': array([17.5       , 15.66205951,  0.        ]), 'targetState': array([18, 15], dtype=int32), 'currentDistance': 0.8296522102752895}
episode index:1953
target Thresh 31.999999915575454
target distance 22.0
model initialize at round 1953
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.89196526, 27.45150202]), 'previousTarget': array([15.81071492, 26.91786413]), 'currentState': array([14.5,  7.5,  0. ]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18367561219774062
{'scaleFactor': 20, 'currentTarget': array([46.72442198, 46.92257949]), 'previousTarget': array([46.19147429, 46.47927459]), 'currentState': array([64., 57.,  0.]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 20.0}
episode index:1954
target Thresh 31.999999916415494
target distance 10.0
model initialize at round 1954
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  8.]), 'previousTarget': array([26.,  8.]), 'currentState': array([18.5       , 17.50000858,  0.        ]), 'targetState': array([26,  8], dtype=int32), 'currentDistance': 12.1037251736141}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18358166047794638
{'scaleFactor': 20, 'currentTarget': array([53.32030957, 33.27986654]), 'previousTarget': array([52.82319436, 32.83045384]), 'currentState': array([68.        , 46.86319047,  0.        ]), 'targetState': array([26,  8], dtype=int32), 'currentDistance': 20.0}
episode index:1955
target Thresh 31.999999917247173
target distance 14.0
model initialize at round 1955
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 19.]), 'previousTarget': array([16., 19.]), 'currentState': array([ 2.5       , 25.49999517,  0.        ]), 'targetState': array([16, 19], dtype=int32), 'currentDistance': 14.983321969320828}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.18388148741318525
{'scaleFactor': 20, 'currentTarget': array([16., 19.]), 'previousTarget': array([16., 19.]), 'currentState': array([15.5      , 19.8752757,  0.       ]), 'targetState': array([16, 19], dtype=int32), 'currentDistance': 1.0080216035673566}
episode index:1956
target Thresh 31.999999918070575
target distance 3.0
model initialize at round 1956
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([21.5       ,  8.49976742,  0.        ]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 2.915595614289363}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.18428333591220764
{'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'previousTarget': array([24., 10.]), 'currentState': array([23.        ,  9.29798193,  0.        ]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 1.2218139699611752}
episode index:1957
target Thresh 31.999999918885788
target distance 7.0
model initialize at round 1957
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([11.5,  2.5,  0. ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 7.382411530116667}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1841892177631207
{'scaleFactor': 20, 'currentTarget': array([46.29054965, 37.82644011]), 'previousTarget': array([45.7857416 , 37.34119455]), 'currentState': array([61.        , 51.37753123,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 20.0}
episode index:1958
target Thresh 31.999999919692886
target distance 6.0
model initialize at round 1958
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([10.5,  9.5,  0. ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 6.519202405202722}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18409519570198588
{'scaleFactor': 20, 'currentTarget': array([44.94846599, 45.82990708]), 'previousTarget': array([44.44006441, 45.33951516]), 'currentState': array([60.        , 58.99999911,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 20.0}
episode index:1959
target Thresh 31.999999920491955
target distance 4.0
model initialize at round 1959
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 18.]), 'previousTarget': array([18., 18.]), 'currentState': array([14.5       , 19.48483065,  0.        ]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 3.8019366186681394}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.18448646858678078
{'scaleFactor': 20, 'currentTarget': array([18., 18.]), 'previousTarget': array([18., 18.]), 'currentState': array([17.        , 18.50241042,  0.        ]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 1.119114037712657}
episode index:1960
target Thresh 31.999999921283074
target distance 4.0
model initialize at round 1960
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 19.]), 'previousTarget': array([14., 19.]), 'currentState': array([10.5       , 19.47793317,  0.        ]), 'targetState': array([14, 19], dtype=int32), 'currentDistance': 3.53248073079911}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.18487734241712916
{'scaleFactor': 20, 'currentTarget': array([14., 19.]), 'previousTarget': array([14., 19.]), 'currentState': array([13.        , 19.34090245,  0.        ]), 'targetState': array([14, 19], dtype=int32), 'currentDistance': 1.0565105199888898}
episode index:1961
target Thresh 31.999999922066323
target distance 20.0
model initialize at round 1961
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.00280987, 27.98782391]), 'previousTarget': array([13.0776773 , 27.61161351]), 'currentState': array([17.5,  8.5,  0. ]), 'targetState': array([13, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18478311339449047
{'scaleFactor': 20, 'currentTarget': array([49.51685448, 48.28714138]), 'previousTarget': array([48.98605952, 47.84278048]), 'currentState': array([67., 58.,  0.]), 'targetState': array([13, 28], dtype=int32), 'currentDistance': 20.0}
episode index:1962
target Thresh 31.999999922841774
target distance 10.0
model initialize at round 1962
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 4.]), 'previousTarget': array([8., 4.]), 'currentState': array([18.5,  8.5,  0. ]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 11.42365965879597}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1846889803769691
{'scaleFactor': 20, 'currentTarget': array([53.13411019, 44.62065714]), 'previousTarget': array([52.62789937, 44.12757651]), 'currentState': array([68.        , 57.99994412,  0.        ]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 20.0}
episode index:1963
target Thresh 31.99999992360951
target distance 17.0
model initialize at round 1963
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  7.]), 'previousTarget': array([27.,  7.]), 'currentState': array([10.5       , 15.49712977,  0.        ]), 'targetState': array([27,  7], dtype=int32), 'currentDistance': 18.559396927068835}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.18496780644075597
{'scaleFactor': 20, 'currentTarget': array([27.,  7.]), 'previousTarget': array([27.,  7.]), 'currentState': array([26.        ,  7.87178773,  0.        ]), 'targetState': array([27,  7], dtype=int32), 'currentDistance': 1.32665513500228}
episode index:1964
target Thresh 31.99999992436961
target distance 7.0
model initialize at round 1964
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 15.]), 'previousTarget': array([20., 15.]), 'currentState': array([27.5,  9.5,  0. ]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 9.300537618869157}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18487367524154946
{'scaleFactor': 20, 'currentTarget': array([61.16819358, 46.77895645]), 'previousTarget': array([60.6527351 , 46.29900844]), 'currentState': array([77., 59.,  0.]), 'targetState': array([20, 15], dtype=int32), 'currentDistance': 20.0}
episode index:1965
target Thresh 31.999999925122143
target distance 7.0
model initialize at round 1965
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 14.]), 'previousTarget': array([18., 14.]), 'currentState': array([25.5,  7.5,  0. ]), 'targetState': array([18, 14], dtype=int32), 'currentDistance': 9.924716620639666}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18477963980144696
{'scaleFactor': 20, 'currentTarget': array([59.03367608, 44.95522933]), 'previousTarget': array([58.51698911, 44.47738119]), 'currentState': array([75., 57.,  0.]), 'targetState': array([18, 14], dtype=int32), 'currentDistance': 20.0}
episode index:1966
target Thresh 31.999999925867193
target distance 19.0
model initialize at round 1966
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.69714823,  5.92822063]), 'previousTarget': array([20.29367831,  6.49385478]), 'currentState': array([ 9.5, 22.5,  0. ]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18468569997439996
{'scaleFactor': 20, 'currentTarget': array([44.53122287, 25.50188096]), 'previousTarget': array([44.0334934 , 25.03381364]), 'currentState': array([59.        , 39.30965009,  0.        ]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 20.0}
episode index:1967
target Thresh 31.999999926604826
target distance 7.0
model initialize at round 1967
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  3.]), 'previousTarget': array([22.,  3.]), 'currentState': array([15.5       ,  7.48990858,  0.        ]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 7.89995436828228}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.18504680391458508
{'scaleFactor': 20, 'currentTarget': array([22.,  3.]), 'previousTarget': array([22.,  3.]), 'currentState': array([21.        ,  3.80453253,  0.        ]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 1.2834611752932565}
episode index:1968
target Thresh 31.99999992733512
target distance 21.0
model initialize at round 1968
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6.56220022, 23.13536265]), 'previousTarget': array([ 6.35899411, 23.09400392]), 'currentState': array([23.5, 12.5,  0. ]), 'targetState': array([ 2, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18495282382118
{'scaleFactor': 20, 'currentTarget': array([55.1619864 , 52.95537338]), 'previousTarget': array([54.63686574, 52.50508842]), 'currentState': array([73., 62.,  0.]), 'targetState': array([ 2, 26], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:1969
target Thresh 31.99999992805815
target distance 7.0
model initialize at round 1969
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  2.]), 'previousTarget': array([20.,  2.]), 'currentState': array([27.5       ,  8.88594315,  0.        ]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 10.181660625493256}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18485893913903728
{'scaleFactor': 20, 'currentTarget': array([62.57675477, 42.90027686]), 'previousTarget': array([62.07493192, 42.40757042]), 'currentState': array([77.        , 56.75560064,  0.        ]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 20.0}
episode index:1970
target Thresh 31.99999992877398
target distance 26.0
model initialize at round 1970
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.96886176, 11.91248658]), 'previousTarget': array([12.80578478, 12.14752022]), 'currentState': array([25.5, 27.5,  0. ]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18476514972293426
{'scaleFactor': 20, 'currentTarget': array([60.59801886, 55.57300215]), 'previousTarget': array([60.09925223, 55.10184593]), 'currentState': array([75.        , 69.45042768,  0.        ]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 20.0}
episode index:1971
target Thresh 31.99999992948269
target distance 17.0
model initialize at round 1971
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 28.]), 'previousTarget': array([13., 28.]), 'currentState': array([ 3.5, 11.5,  0. ]), 'targetState': array([13, 28], dtype=int32), 'currentDistance': 19.039432764659704}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18467145542794292
{'scaleFactor': 20, 'currentTarget': array([37.57253443, 48.2723409 ]), 'previousTarget': array([37.0557536 , 47.79270866]), 'currentState': array([53., 61.,  0.]), 'targetState': array([13, 28], dtype=int32), 'currentDistance': 20.0}
episode index:1972
target Thresh 31.99999993018435
target distance 7.0
model initialize at round 1972
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 18.]), 'previousTarget': array([ 7., 18.]), 'currentState': array([13.5       , 24.50000009,  0.        ]), 'targetState': array([ 7, 18], dtype=int32), 'currentDistance': 9.19238821864544}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.184577856109429
{'scaleFactor': 20, 'currentTarget': array([48.55976678, 57.82540706]), 'previousTarget': array([48.06044939, 57.35081259]), 'currentState': array([63.       , 71.6630248,  0.       ]), 'targetState': array([ 7, 18], dtype=int32), 'currentDistance': 20.0}
episode index:1973
target Thresh 31.999999930879028
target distance 8.0
model initialize at round 1973
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  9.]), 'previousTarget': array([25.,  9.]), 'currentState': array([25.5, 16.5,  0. ]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 7.516648189186381}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18448435162305138
{'scaleFactor': 20, 'currentTarget': array([60.53068944, 42.90484248]), 'previousTarget': array([60.03045835, 42.42638127]), 'currentState': array([75.        , 56.71205261,  0.        ]), 'targetState': array([25,  9], dtype=int32), 'currentDistance': 20.0}
episode index:1974
target Thresh 31.999999931566794
target distance 21.0
model initialize at round 1974
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.25867395,  7.64388062]), 'previousTarget': array([24.6897547 ,  7.88009345]), 'currentState': array([ 6.5       , 14.58017299,  0.        ]), 'targetState': array([27,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.18473308311542466
{'scaleFactor': 20, 'currentTarget': array([27.,  7.]), 'previousTarget': array([27.,  7.]), 'currentState': array([26.        ,  7.67479326,  0.        ]), 'targetState': array([27,  7], dtype=int32), 'currentDistance': 1.2063771993493737}
episode index:1975
target Thresh 31.999999932247714
target distance 11.0
model initialize at round 1975
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 21.]), 'previousTarget': array([15., 21.]), 'currentState': array([26.5, 16.5,  0. ]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 12.349089035228511}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1846395947130383
{'scaleFactor': 20, 'currentTarget': array([59.90551739, 54.12702103]), 'previousTarget': array([59.38884898, 53.64964925]), 'currentState': array([76., 66.,  0.]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 20.0}
episode index:1976
target Thresh 31.99999993292186
target distance 1.0
model initialize at round 1976
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 24.]), 'previousTarget': array([ 8., 24.]), 'currentState': array([ 9.5, 23.5,  0. ]), 'targetState': array([ 8, 24], dtype=int32), 'currentDistance': 1.5811388300842397}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18454620088667867
{'scaleFactor': 20, 'currentTarget': array([44.52019875, 58.79607561]), 'previousTarget': array([44.01894414, 58.31223648]), 'currentState': array([59.        , 72.59228364,  0.        ]), 'targetState': array([ 8, 24], dtype=int32), 'currentDistance': 20.0}
episode index:1977
target Thresh 31.9999999335893
target distance 10.0
model initialize at round 1977
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 25.]), 'previousTarget': array([17., 25.]), 'currentState': array([24.5, 15.5,  0. ]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 12.103718436910192}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1844529014929038
{'scaleFactor': 20, 'currentTarget': array([57.62886615, 53.51148502]), 'previousTarget': array([57.10854972, 53.04049051]), 'currentState': array([74., 65.,  0.]), 'targetState': array([17, 25], dtype=int32), 'currentDistance': 20.0}
episode index:1978
target Thresh 31.9999999342501
target distance 13.0
model initialize at round 1978
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 28.]), 'previousTarget': array([23., 28.]), 'currentState': array([10.5, 15.5,  0. ]), 'targetState': array([23, 28], dtype=int32), 'currentDistance': 17.677669529663575}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.1847567040392981
{'scaleFactor': 20, 'currentTarget': array([23., 28.]), 'previousTarget': array([23., 28.]), 'currentState': array([22.5       , 27.47562915,  0.        ]), 'targetState': array([23, 28], dtype=int32), 'currentDistance': 0.7245445379106829}
episode index:1979
target Thresh 31.99999993490432
target distance 12.0
model initialize at round 1979
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 17.]), 'previousTarget': array([13., 17.]), 'currentState': array([25.5, 27.5,  0. ]), 'targetState': array([13, 17], dtype=int32), 'currentDistance': 16.32482771731462}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18466339257261158
{'scaleFactor': 20, 'currentTarget': array([60.47027896, 61.90200975]), 'previousTarget': array([59.97210923, 61.4426452 ]), 'currentState': array([75.        , 75.64563397,  0.        ]), 'targetState': array([13, 17], dtype=int32), 'currentDistance': 20.0}
episode index:1980
target Thresh 31.999999935552033
target distance 5.0
model initialize at round 1980
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 28.]), 'previousTarget': array([ 8., 28.]), 'currentState': array([10.5, 23.5,  0. ]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 5.1478150704934675}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1845701753123528
{'scaleFactor': 20, 'currentTarget': array([44.87662635, 59.91246511]), 'previousTarget': array([44.36684394, 59.42377777]), 'currentState': array([60., 73.,  0.]), 'targetState': array([ 8, 28], dtype=int32), 'currentDistance': 20.0}
episode index:1981
target Thresh 31.9999999361933
target distance 12.0
model initialize at round 1981
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 20.]), 'previousTarget': array([12., 20.]), 'currentState': array([24.5, 24.5,  0. ]), 'targetState': array([12, 20], dtype=int32), 'currentDistance': 13.285330255586528}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18447705211592882
{'scaleFactor': 20, 'currentTarget': array([58.91836791, 60.8643839 ]), 'previousTarget': array([58.41053083, 60.37338782]), 'currentState': array([74.       , 73.9999986,  0.       ]), 'targetState': array([12, 20], dtype=int32), 'currentDistance': 20.0}
episode index:1982
target Thresh 31.99999993682819
target distance 16.0
model initialize at round 1982
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([17.5,  2.5,  0. ]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 15.57241150239745}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18438402284103425
{'scaleFactor': 20, 'currentTarget': array([50.35899411, 40.90599608]), 'previousTarget': array([49.83365101, 40.44410513]), 'currentState': array([67., 52.,  0.]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 20.0}
episode index:1983
target Thresh 31.999999937456757
target distance 8.0
model initialize at round 1983
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 13.]), 'previousTarget': array([17., 13.]), 'currentState': array([25.5,  6.5,  0. ]), 'targetState': array([17, 13], dtype=int32), 'currentDistance': 10.700467279516413}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18429108734565067
{'scaleFactor': 20, 'currentTarget': array([58.93376457, 44.08882546]), 'previousTarget': array([58.41647717, 43.61217878]), 'currentState': array([75., 56.,  0.]), 'targetState': array([17, 13], dtype=int32), 'currentDistance': 20.0}
episode index:1984
target Thresh 31.999999938079075
target distance 12.0
model initialize at round 1984
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 29.]), 'previousTarget': array([13., 29.]), 'currentState': array([17.5, 17.5,  0. ]), 'targetState': array([13, 29], dtype=int32), 'currentDistance': 12.349089035228422}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1841982454880458
{'scaleFactor': 20, 'currentTarget': array([50.64387161, 55.49013188]), 'previousTarget': array([50.12255556, 55.02048287]), 'currentState': array([67., 67.,  0.]), 'targetState': array([13, 29], dtype=int32), 'currentDistance': 20.0}
episode index:1985
target Thresh 31.999999938695197
target distance 6.0
model initialize at round 1985
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 23.]), 'previousTarget': array([ 2., 23.]), 'currentState': array([ 5.5, 28.5,  0. ]), 'targetState': array([ 2, 23], dtype=int32), 'currentDistance': 6.51920240520258}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18410549712677288
{'scaleFactor': 20, 'currentTarget': array([40.26287939, 58.10537593]), 'previousTarget': array([39.76184575, 57.64037162]), 'currentState': array([55.        , 71.62636983,  0.        ]), 'targetState': array([ 2, 23], dtype=int32), 'currentDistance': 20.0}
episode index:1986
target Thresh 31.99999993930519
target distance 14.0
model initialize at round 1986
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([9.5       , 3.51980042, 0.        ]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 13.94136969902099}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.184404297258767
{'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([22.        ,  7.77054279,  0.        ]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 1.262432647733053}
episode index:1987
target Thresh 31.999999939909113
target distance 9.0
model initialize at round 1987
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([21.5, 15.5,  0. ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 10.977249200050183}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1843115385579326
{'scaleFactor': 20, 'currentTarget': array([56.24891479, 50.51329807]), 'previousTarget': array([55.74952427, 50.05969684]), 'currentState': array([71.        , 64.01905555,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 20.0}
episode index:1988
target Thresh 31.999999940507028
target distance 8.0
model initialize at round 1988
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 10.]), 'previousTarget': array([22., 10.]), 'currentState': array([14.5       ,  8.46408626,  0.        ]), 'targetState': array([22, 10], dtype=int32), 'currentDistance': 7.655653532082689}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.1846600601690141
{'scaleFactor': 20, 'currentTarget': array([22., 10.]), 'previousTarget': array([22., 10.]), 'currentState': array([21.        , 10.78334296,  0.        ]), 'targetState': array([22, 10], dtype=int32), 'currentDistance': 1.2702858690300427}
episode index:1989
target Thresh 31.99999994109899
target distance 7.0
model initialize at round 1989
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 22.]), 'previousTarget': array([19., 22.]), 'currentState': array([24.5, 28.5,  0. ]), 'targetState': array([19, 22], dtype=int32), 'currentDistance': 8.514693182963194}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18456726616892916
{'scaleFactor': 20, 'currentTarget': array([59.25321903, 58.87851536]), 'previousTarget': array([58.75142211, 58.40906328]), 'currentState': array([74.        , 72.38897246,  0.        ]), 'targetState': array([19, 22], dtype=int32), 'currentDistance': 20.0}
episode index:1990
target Thresh 31.99999994168507
target distance 11.0
model initialize at round 1990
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  4.]), 'previousTarget': array([24.,  4.]), 'currentState': array([16.5, 14.5,  0. ]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 12.903487900563828}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1844745653823049
{'scaleFactor': 20, 'currentTarget': array([51.23620878, 28.88976542]), 'previousTarget': array([50.73496924, 28.42720117]), 'currentState': array([66.        , 42.38163219,  0.        ]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 20.0}
episode index:1991
target Thresh 31.999999942265312
target distance 8.0
model initialize at round 1991
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([13.5       , 16.49999738,  0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 11.335782313583088}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18438195766875953
{'scaleFactor': 20, 'currentTarget': array([48.203986  , 48.29270174]), 'previousTarget': array([47.71671761, 47.92341585]), 'currentState': array([63.       , 61.7492232,  0.       ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 20.0}
episode index:1992
target Thresh 31.999999942839782
target distance 4.0
model initialize at round 1992
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 13.]), 'previousTarget': array([12., 13.]), 'currentState': array([16.5, 16.5,  0. ]), 'targetState': array([12, 13], dtype=int32), 'currentDistance': 5.700877125495804}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18428944288819318
{'scaleFactor': 20, 'currentTarget': array([51.20354256, 48.65206051]), 'previousTarget': array([50.69874492, 48.16778736]), 'currentState': array([66.        , 62.10809436,  0.        ]), 'targetState': array([12, 13], dtype=int32), 'currentDistance': 20.0}
episode index:1993
target Thresh 31.999999943408536
target distance 22.0
model initialize at round 1993
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5.50493644, 20.05566525]), 'previousTarget': array([ 5., 20.]), 'currentState': array([25.5, 20.5,  0. ]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18419702090078688
{'scaleFactor': 20, 'currentTarget': array([58.57260069, 58.59208381]), 'previousTarget': array([58.05615617, 58.11580042]), 'currentState': array([75., 70.,  0.]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 20.0}
episode index:1994
target Thresh 31.99999994397163
target distance 22.0
model initialize at round 1994
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.14682221, 23.0992144 ]), 'previousTarget': array([17.16513874, 22.79586847]), 'currentState': array([24.5,  4.5,  0. ]), 'targetState': array([16, 26], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18410469156700202
{'scaleFactor': 20, 'currentTarget': array([55.98896727, 45.30501868]), 'previousTarget': array([55.45731557, 44.87089005]), 'currentState': array([74., 54.,  0.]), 'targetState': array([16, 26], dtype=int32), 'currentDistance': 20.0}
episode index:1995
target Thresh 31.99999994452912
target distance 13.0
model initialize at round 1995
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 29.]), 'previousTarget': array([16., 29.]), 'currentState': array([ 3.5, 18.5,  0. ]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 16.324827717314456}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.18441005709409453
{'scaleFactor': 20, 'currentTarget': array([16., 29.]), 'previousTarget': array([16., 29.]), 'currentState': array([15.        , 29.68895108,  0.        ]), 'targetState': array([16, 29], dtype=int32), 'currentDistance': 1.214353154502316}
episode index:1996
target Thresh 31.999999945081065
target distance 16.0
model initialize at round 1996
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([6.5, 2.5, 0. ]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 18.17965896269785}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1843177135502317
{'scaleFactor': 20, 'currentTarget': array([40.76121364, 39.0470316 ]), 'previousTarget': array([40.24693131, 38.56385314]), 'currentState': array([56., 52.,  0.]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 20.0}
episode index:1997
target Thresh 31.99999994562752
target distance 5.0
model initialize at round 1997
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  7.]), 'previousTarget': array([17.,  7.]), 'currentState': array([20.5,  2.5,  0. ]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 5.700877125495742}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1842254624423487
{'scaleFactor': 20, 'currentTarget': array([54.75411842, 39.05538356]), 'previousTarget': array([54.24330275, 38.56813281]), 'currentState': array([70., 52.,  0.]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 20.0}
episode index:1998
target Thresh 31.999999946168533
target distance 22.0
model initialize at round 1998
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9.69130636, 20.3977788 ]), 'previousTarget': array([ 9.11145618, 20.05572809]), 'currentState': array([27.5, 29.5,  0. ]), 'targetState': array([ 5, 18], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1841333036317222
{'scaleFactor': 20, 'currentTarget': array([61.74031635, 66.07165691]), 'previousTarget': array([61.23227429, 65.58115517]), 'currentState': array([77., 79.,  0.]), 'targetState': array([ 5, 18], dtype=int32), 'currentDistance': 20.0}
episode index:1999
target Thresh 31.999999946704165
target distance 9.0
model initialize at round 1999
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 16.]), 'previousTarget': array([ 3., 16.]), 'currentState': array([ 6.5, 24.5,  0. ]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 9.192388155425041}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18404123697990632
{'scaleFactor': 20, 'currentTarget': array([41.37504679, 51.79646614]), 'previousTarget': array([40.87350252, 51.32060552]), 'currentState': array([56.        , 65.43870729,  0.        ]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 20.0}

Process finished with exit code 0
